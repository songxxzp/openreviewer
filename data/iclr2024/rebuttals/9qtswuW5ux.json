[
    {
        "title": "Unsupervised graph neural networks with recurrent features for solving combinatorial optimization problems"
    },
    {
        "review": {
            "id": "ktoxokPBRk",
            "forum": "9qtswuW5ux",
            "replyto": "9qtswuW5ux",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7813/Reviewer_vpKg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7813/Reviewer_vpKg"
            ],
            "content": {
                "summary": {
                    "value": "The paper discusses the emergence of graph neural networks (GNNs) as a solution for combinatorial optimization problems. The authors introduce a new algorithm called QRF-GNN, which employs GNNs to efficiently address problems with quadratic unconstrained binary optimization (QUBO) formulations. QRF-GNN employs unsupervised learning and minimizes a loss function derived from QUBO relaxation. Its architecture includes recurrent use of intermediate GNN predictions, parallel convolutional layers, and a combination of artificial node features as input."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is well written and well presented. \n\n2. The paper also focuses on an important problem QUBO problems via unsupervised learning (which may have certain benefits)\n\n3. The paper shows experimental evidence that suggests that their method indeed works well."
                },
                "weaknesses": {
                    "value": "Please refer to questions."
                },
                "questions": {
                    "value": "1. I'm uncertain about the primary contributions of the paper. Are the key contributions limited to the QRF-GNN architecture and the incorporation of random node features combined with pagerank? Or do they also encompass the formulation of the problem as an unsupervised learning approach?\n\n2. If the primary contributions are solely related to the architecture and node features, it's worth noting that while the results appear promising, the method might be viewed as somewhat incremental, especially when compared to existing methods like spectral clustering, which can be considered a basic form of unsupervised learning. In such a case, could you provide insights into any theoretical guarantees, if any exist for your work?\n\n3. I have concerns about the reproducibility of this work, as it appears the authors haven't made the code base available. Additionally, they mention that the method lacks stability. How do you determine the best run under these circumstances?\n\nAs it stands, I am inclined to recommend rejecting this paper. However, since I am not an expert in this field, I am open to revising my evaluation if the authors address these questions or if other reviewers support the work."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7813/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7813/Reviewer_vpKg",
                        "ICLR.cc/2024/Conference/Submission7813/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7813/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698151632214,
            "cdate": 1698151632214,
            "tmdate": 1700670954184,
            "mdate": 1700670954184,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "S7DQrZLs63",
                "forum": "9qtswuW5ux",
                "replyto": "ktoxokPBRk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7813/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7813/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer vpKg"
                    },
                    "comment": {
                        "value": "We thank the reviewer vpKg for their comment and for showing interest to our work.\n\n$ \\\\ $\n\n#### \u2022  **Q1. I'm uncertain about the primary contributions of the paper. Are the key contributions limited to the QRF-GNN architecture and the incorporation of random node features combined with pagerank? Or do they also encompass the formulation of the problem as an unsupervised learning approach?**\n\nThe main contribution of the paper is that the use of recurrent features drastically improves the performance of GNNs in solving Combinatorial Optimization (CO) problems. As a result, GNN obtains highly accurate solutions and can even compete  with the best heuristic algorithms for solving MAX-CUT and graph coloring problems. Please see the general comment where we discuss the novelty and contribution of our work in more detail.\n\n#### \u2022  **Q2. If the primary contributions are solely related to the architecture and node features, it's worth noting that while the results appear promising, the method might be viewed as somewhat incremental, especially when compared to existing methods like spectral clustering, which can be considered a basic form of unsupervised learning**\n\nWe can not agree that contributions might be viewed as somewhat incremental only because they are related to architecture and node features, especially if it contains novelty which allows to significantly outperform existing ones. Generally speaking, the development of a novel unsupervised neural network comes down to the selection of the appropriate architecture, features and loss function. Our work is dedicated to the research in the field of unsupervised graph neural networks, therefore in our experiments we compare QRF-GNN with  1) SOTA unsupervised GNNs, 2) SOTA non-learning based conventional heuristic methods that provide the best solutions on the considered benchmark datasets of the MAX-CUT and graph coloring problems. It would also be interesting for us to compare QRF-GNN with any other unsupervised learning method. However, to the best of our knowledge, there is no a successful application of the spectral clustering method which demonstrates high quality results solving CO problems such as MAX-CUT and coloring.\n\n\n\n**In such a case, could you provide insights into any theoretical guarantees, if any exist for your work?**\n\nAs well as many other heuristic algorithms for solving hard combinatorial optimization problems, graph neural networks do not provide theoretical guarantees. The performance of the proposed method is evaluated by numerical experiments and comparative analysis with the SOTA methods.\n\n#### \u2022  **Q3. I have concerns about the reproducibility of this work, as it appears the authors haven't made the code base available. Additionally, they mention that the method lacks stability. How do you determine the best run under these circumstances?**\n\nIn the paper we mention the stochasticity of our method, but we do not mean that it lacks stability. The results can vary with the different seed initializations, and this is the common behavior of any other heuristic algorithm that contains some randomness. This is the case for all the algorithms that appear in our experiments section. Therefore, we follow the conventional experimental design for heuristic algorithms with a stochastic component. It is common to report the best value out of several runs and use it in the comparison with other algorithms [1-3]. In addition, in section B of the appendix, you can see the distribution of results for d-regular graphs and each graph from Gset (\"default\" refers to QRF-GNN). Speaking about stability, in contrast, we can say that GRF-GNN is more robust and stable than, for example, PI-GNN [1], since it provides high quality results for all problems with the same set of hyperparameters.\n\n$ \\\\ $\n\nWe hope that our answers will bring more understanding about the use of GNNs in combinatorial optimization and about the contribution of our paper. We will be glad to answer other questions of the reviewer if they have them. \n\n$ \\\\ $\n\n[1] Martin J. A. Schuetz, J. Kyle Brubaker, and Helmut G. Katzgraber. Combinatorial optimization with physics-inspired graph neural networks. Nature Machine Intelligence, 4(4):367\u2013377, April 2022a.\n\n[2] W. H. Tonshoff J, Ritzert M and G. M.  Graph neural networks for maximum constraint satisfaction. Frontiers in Artificial Intelligence 2021.\n\n[3] Una Benlic and Jin-Kao Hao. Breakout local search for the max-cutproblem. Engineering Applications of Artificial Intelligence, 26(3):1162\u20131173, 2013."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7813/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700082373314,
                "cdate": 1700082373314,
                "tmdate": 1700082373314,
                "mdate": 1700082373314,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HWaHoNonap",
                "forum": "9qtswuW5ux",
                "replyto": "S7DQrZLs63",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7813/Reviewer_vpKg"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7813/Reviewer_vpKg"
                ],
                "content": {
                    "comment": {
                        "value": "I've thoroughly reviewed all the responses and reviews, and I have additional questions:\n\nIs it feasible to assess performance by employing different types of GNNs, or even by incorporating deeper GNN layers? This consideration becomes particularly crucial in the context of unsupervised learning, as certain architectures may impart an implicit regularization effect. Exploring the impact of varying GNN types or layer depths could offer valuable insights.\n\nI'm keen to gain a deeper understanding of how the hyperparameters are chosen. While I acknowledge that there is a section in the paper addressing hyperparameters, especially in the realm of unsupervised learning, it is conceivable that the results might vary significantly with different learning rates and other hyperparameters. Conducting experiments to explore the influence of various hyperparameter settings, including learning rates, could enhance the robustness and completeness of the findings."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7813/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700389412906,
                "cdate": 1700389412906,
                "tmdate": 1700389412906,
                "mdate": 1700389412906,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "IsYZ6HmnM9",
                "forum": "9qtswuW5ux",
                "replyto": "ktoxokPBRk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7813/Reviewer_vpKg"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7813/Reviewer_vpKg"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the additional experiments and comments. I have raised the score accordingly. While the method works, I don't think the method is particularly novel, which is why the score is not higher. I am not from this field so I was hoping that some of the other reviewers would advocate for this method."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7813/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700657591094,
                "cdate": 1700657591094,
                "tmdate": 1700657800068,
                "mdate": 1700657800068,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "0upWXyYbgL",
            "forum": "9qtswuW5ux",
            "replyto": "9qtswuW5ux",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7813/Reviewer_Nnir"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7813/Reviewer_Nnir"
            ],
            "content": {
                "summary": {
                    "value": "The authors introduce QRF-GNN to efficiently solve problems that have a quadratic unconstrained binary optimization (QUBO) formulation.\nThe relaxed QUBO objective can be used to perform unsupervised learning for the GNNs. \n\nFurther the GNN can be applied repeatedly by including the output probabilities from the previous round as node features in the next round."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The method is simple - relax the QUBO formulation with probability predictions and then use these to augment the node features in the next GNN step. Finally repeat until convergence.\n\nFor the loss simply minimize (or maximize) the QUBO formulation itself wrt the parameters.\n\nTo the best of my knowledge, this is the first time a QUBO probelm has been solved like this.\n\nThe results are competitive against other benchmarks algorithms to solve max cut and coloring problems."
                },
                "weaknesses": {
                    "value": "Minor nit - The authors could have elaborated a little on the QUBO formulation and the training details before section 3."
                },
                "questions": {
                    "value": "Did you'll try to solve QUBO problems other than max cut and coloring?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7813/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698729249346,
            "cdate": 1698729249346,
            "tmdate": 1699636956347,
            "mdate": 1699636956347,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "J2TIKcO5kn",
                "forum": "9qtswuW5ux",
                "replyto": "0upWXyYbgL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7813/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7813/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Nnir"
                    },
                    "comment": {
                        "value": "We thank the reviewer Nnir for reading our work and considering the results of it worthy of attention.\n\n#### \u2022  **W1.Minor nit - The authors could have elaborated a little on the QUBO formulation and the training details before section 3.**\n\nUnfortunately, the page limit does not allow for much detail to be included in the main body of the article. We have tried to provide a sufficient number of references to the literature so that the interested reader can gain a good understanding of the issues discussed. We hope that the reader will find sufficient technical details placed in the section with experiments and in the appendix, and we are ready to answer any questions regarding the  reproducibility of the results.\n\n#### \u2022  **Q1.Did you'll try to solve QUBO problems other than max cut and coloring?**\n\nWe have tried to solve other problems and found the results promising, but the work is not yet finished and this is the subject of further research."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7813/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700082357114,
                "cdate": 1700082357114,
                "tmdate": 1700082357114,
                "mdate": 1700082357114,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4TU0uKWCmF",
                "forum": "9qtswuW5ux",
                "replyto": "0upWXyYbgL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7813/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7813/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Updates in the paper"
                    },
                    "comment": {
                        "value": "Dear reviewer Nnir,\n\nFollowing your recommendations, we have added additional explanations of the QUBO formulation to the introduction. Please let us know if you have any further questions or suggestions.\n\nWe greatly appreciate your time and effort.\n\nSincerely, the authors"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7813/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700656148999,
                "cdate": 1700656148999,
                "tmdate": 1700656148999,
                "mdate": 1700656148999,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6UXLBFp29g",
                "forum": "9qtswuW5ux",
                "replyto": "0upWXyYbgL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7813/Reviewer_Nnir"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7813/Reviewer_Nnir"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "I thank the authors for their response and elaborating on QUBO in the paper. I retain my score"
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7813/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700675653042,
                "cdate": 1700675653042,
                "tmdate": 1700675653042,
                "mdate": 1700675653042,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Olm6BdndSI",
            "forum": "9qtswuW5ux",
            "replyto": "9qtswuW5ux",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7813/Reviewer_VZaD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7813/Reviewer_VZaD"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces an unsupervised QRF-GNN method for solving CO problems, characterized by two fundamental attributes: the general quadratic unconstrained binary optimization (QUBO) formulation and its recurrent design. Built upon the QUBO formulation, the proposed method compromises various CO problems including the maximum cut problem and graph coloring problem. Compared to the previous baseline PI-GNN, the main improvements lie in the refinement of the message-passing paradigm within graph neural networks, including introducing the recurrent feature, artificial input features, and parallel graph convolutional layers. Empirical results show that the proposed method outperforms existing unsupervised learning methods and is competitive with conventional heuristics."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.The empirical results show advantages over previous methods built on QUBO formulation and are competitive with conventional heuristics.\n\n2.The framework provides a general solution to a range of CO problems, though this property is derived from the existing QUBO formulation."
                },
                "weaknesses": {
                    "value": "1.The novelty appears to be somewhat limited. It builds upon the foundation laid by PI-GNN [1], inheriting similarities in terms of problem formulation and experimental design. The primary enhancements are centered around the refinement of the message-passing mechanism within the graph neural networks. Moreover, the key characteristics of the proposed QRF-GNN, i.e., the unsupervised approach based on QUBO and the recurrent design, have previously been explored in methods like PI-GNN and RUN-CSP [2]. The methodological innovations do not appear compelling enough to warrant my vote for acceptance.\n\n2.The rationale behind certain design choices remains unclear. Questions arise regarding the decision to incorporate the recurrent feature, artificial input features, and parallel graph convolutional layers in lieu of the raw GNN. If the objective is to enhance representation capacity, why not consider a more advanced GNN or GraphFormer design? It is also necessary to establish the relationship between these design choices and their relevance to CO problems.\n\n3.The evaluation presented in this paper does not convincingly demonstrate QRF-GNN's superiority over heuristic methods. In PI-GNN, Fig. 4 and 5 show its advantages in scalability and computational complexity. Yet these properties are not well verified in this paper.\n\n[1] Combinatorial Optimization with Physics-Inspired Graph Neural Networks. Nature Machine Intelligence 2022.\n\n[2] Graph neural networks for maximum constraint satisfaction. Frontiers in Artificial Intelligence 2021."
                },
                "questions": {
                    "value": "1.Why are recurrent features important for solving CO problems?\n\n2.Since the methodology mainly focuses on the design of the graph networks, have you tried other advanced GNNs or GraphFormers in hand for better performance of this formulation?\n\n3.In Table. 5, why is QRF-GNN more efficient than PI-GNN [1] while the main algorithm pipelines seem similar? It probably comes from the setting of PI-GNN in experiments: \"In order to obtain the results of PI-GNN, the authors applied hyperparameter optimization for each graph.\" However, in my understanding, hyperparameter optimization is not a mandatory request of PI-GNN. It is worth investigating the performance of PI-GNN under the same conditions as QRF-GNN to determine if there is still a speed advantage.\n\n4.In Section 5, the statement mentions, \"...considering that most of the mentioned methods are specialized for solving specific problems...\". In fact, many methods try to propose a general solving framework for solving broad CO problems, such as [2] [3] [4]. Additionally, solving algorithms for NP-complete (NPC) problems inherently possess the potential to solve other CO problems since any NP problem can be reduced to an NPC problem.\n\n[1] Combinatorial Optimization with Physics-Inspired Graph Neural Networks. Nature Machine Intelligence 2022.\n\n[2] Revisiting Sampling for Combinatorial Optimization. ICML 2023.\n\n[3] DIFUSCO: Graph-based Diffusion Solvers for Combinatorial Optimization. NeurIPS 2023.\n\n[4] From Distribution Learning in Training to Gradient Search in Testing for Combinatorial Optimization. NeurIPS 2023."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No ethics concerns."
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7813/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7813/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7813/Reviewer_VZaD"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7813/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698744670300,
            "cdate": 1698744670300,
            "tmdate": 1699636956242,
            "mdate": 1699636956242,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8TelYCobKS",
                "forum": "9qtswuW5ux",
                "replyto": "Olm6BdndSI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7813/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7813/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer VZaD, Part 1."
                    },
                    "comment": {
                        "value": "We are grateful to the reviewer for carefully studying not only our article, but also related works in the field.\n\n\n#### \u2022 **W1. The key characteristics of the proposed QRF-GNN, i.e., the unsupervised approach based on QUBO and the recurrent design, have previously been explored in methods like PI-GNN and RUN-CSP [3].    Q1.Why are recurrent features important for solving CO problems?**\n\nThe authors of PI-GNN indeed proposed to use QUBO as a loss function, but their results without hyperparameter optimization are worse than solutions of greedy based algorithms with linear time complexity, as shown by in [1][2]. Optimizing the hyperparameters of GNN for each individual example is quite time-consuming and still does not allow the original PI-GNN to achieve results close to QRF-GNN.  \n\nThe RUN-CSP algorithm exploits recurrence, but it is different from the one  proposed in our paper. The RUN-CSP uses  the hidden state to update the current state of a node, but the message-passing protocol does not incorporate hidden states of neighbors. It is assumed that the desired information from neighbors of a node was aggregated by the neural network from their hidden states to their current states and thus contributing to the update of the current state of the considered node. As we see from the results in the experiments section, this approach does not yield good quality and the ability of the neural network to aggregate the necessary information is limited.\n\nA key feature of our approach is that we use already predicted nodes classes as hidden recurrent states and then  pass them directly through the message-passing protocol as additional vertex properties. The intuition behind this is as follows. Imagine a Max-Cut task and all vertices at the current step belong to one of the classes. The GNN processes the current vertex and finds that it has the same class as its neighbors. In this case GNN forces a vertex to change its class in order to cut edges with the neighbors and improve the loss value. To summarize, the proposed recurrence is important for solving CO problems, because states of vertices depend on states of nearest neighbors. We will add more details to the article to highlight this point. We have also pointed out this issue in our general comment.\n\n\n#### \u2022  **W2. The rationale behind certain design choices remains unclear. Questions arise regarding the decision to incorporate the recurrent feature, artificial input features, and parallel graph convolutional layers in lieu of the raw GNN.**\n\nWe have tried to answer the question about the importance of recurrence above. Specific features and architecture are probably not optimal and indeed are intended to enhance representation capacity without oversmoothing issues. We tried different combinations and, as can be seen from the ablation study in the appendix, recurrence makes the fundamental contribution, while other modifications are incremental. We also tried different convolutional layers such as GCN and GAT, which showed no additional benefits, so we have reported the results for GraphSAGE. \n\n#### \u2022  **Q2. Since the methodology mainly focuses on the design of the graph networks, have you tried other advanced GNNs or GraphFormers in hand for better performance of this formulation?**\n\nIt is possible that more modern architectures like GraphFormers are able to improve expressive power, but we believe that the recurrent feature should make a significant contribution in this case as well, as it changes the behavior of the iteration process. This could be a subject for future research, but the computational complexity of training must also be taken into account.\n\n$ \\\\ $\n\n[1] S. Boettcher, Inability of a graph neural network heuristic to outperform greedy algorithms in solving combinatorial optimization problems, Nature Machine Intelligence 5, 24 (2023)\n\n[2] M. C. Angelini and F. Ricci-Tersenghi, Modern graph neural networks do worse than classical greedy algorithms in solving combinatorial optimization problems like maximum independent set, Nature Machine Intelligence 5, 29 (2023).\n\n[3] W. H. Tonshoff J, Ritzert M and G. M.  Graph neural networks for maximum constraint satisfaction. Frontiers in Artificial Intelligence 2021."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7813/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700082681324,
                "cdate": 1700082681324,
                "tmdate": 1700082681324,
                "mdate": 1700082681324,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bYlOsbC3cW",
                "forum": "9qtswuW5ux",
                "replyto": "Olm6BdndSI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7813/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7813/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer VZaD, Part 2."
                    },
                    "comment": {
                        "value": "#### \u2022  **Q3. In Table. 5, why is QRF-GNN more efficient than PI-GNN [4] while the main algorithm pipelines seem similar? It probably comes from the setting of PI-GNN in experiments: \"In order to obtain the results of PI-GNN, the authors applied hyperparameter optimization for each graph.\" However, in my understanding, hyperparameter optimization is not a mandatory request of PI-GNN. It is worth investigating the performance of PI-GNN under the same conditions as QRF-GNN to determine if there is still a speed advantage.**\n\nIndeed, the difference in runtime arises mainly due to per instance hyperparameters optimization of PI-GNN. It is not mandatory to perform such the optimization. However, the quality of PI-GNN results without optimization may suffer greatly. There are cases where even greedy algorithms perform better [1][2]. We have investigated performance of PI-GNN without per instance optimization also for the coloring problem. We ran the code of PI-GNN presented by the authors on COLOR dataset and citation graphs. We set the hyperparameters in two ways: similar to the  hyperparameters of QRF-GNN (PI-GNN 1) and as an average of reported optimal values obtained during optimization (PI-GNN 2). The averaging was carried out separately for the COLOR dataset and citations graphs since their optimal parameters vary greatly. The results in columns \"QRF-GNN\" and \"PI-GNN\" of the following table refer to the Table 3 in the paper. The results in \"PI-GNN 1\"  and \"PI-GNN 2\" columns are taken under the same conditions as QRF-GNN (the best value out of 10 runs is presented).\n\n|Graph|QRF-GNN|PI-GNN|PI-GNN 1|PI-GNN 2|\n|-----|-------|------|--------|--------|\n|anna| 0 | 0 | 1 | 1 |\n|jean| 0 | 0 | 0 | 1 |\n|myciel5| 0 | 0 | 0 | 0 |\n|myciel6| 0 | 0 | 0 | 0 |\n|queen5_5| 0 | 0 | 0 | 4 |\n|queen6_6| 0 | 0 | 3 | 1 |\n|queen7_7| 0 | 0 | 9 | 8 |\n|queen8_8| 0 | 1 | 4 | 1 |\n|queen9_9| 0 | 1 | 6 | 2 |\n|queen8_12| 0 | 0 | 1 | 2 |\n|queen11_11| 7 | 17 | 19 | 15 |\n|queen13_13| 15 | 26 | 31 | 27 |\n|Cora| 0 | 0 | 0 | 90 |\n|Citeseer| 0 | 0 | 0 | 50 |\n|Pubmed| 0 | 17 | 3 | 1250 |\n\nAs can be seen from the table, there is a noticeable drop in overall quality. The algorithm fails to find a solution without violations for graphs, which were successfully colored by using per instance optimization. Therefore we decided to consider the optimization process as part of the PI-GNN algorithm. In order to keep the results of PI-GNN competitive, we place the runtime reported by the authors in [3] in Table 5. The QRF-GNN method does not require optimization of hyperparameters for each instance to obtain accurate results, therefore the table shows its runtime without optimization. Perhaps it is possible to find general hyperparameters that allow PI-GNN to work properly, but the authors of [3] do not provide them and the corresponding runtimes, which forces us to compare with what is given in [3].\n\n$ \\\\ $\n\n[1] S. Boettcher, Inability of a graph neural network heuristic to outperform greedy algorithms in solving combinatorial optimization problems, Nature Machine Intelligence 5, 24 (2023)\n\n[2] M. C. Angelini and F. Ricci-Tersenghi, Modern graph neural networks do worse than classical greedy algorithms in solving combinatorial optimization problems like maximum independent set, Nature Machine Intelligence 5, 29 (2023).\n\n[3] M. J. A. Schuetz, J. K. Brubaker, Z. Zhu, and H. G. Katzgraber, Graph coloring with physics-inspired graph neural networks, Phys. Rev. Res. 4, 043131 (2022)\n\n[4] Martin J. A. Schuetz, J. Kyle Brubaker, and Helmut G. Katzgraber. Combinatorial optimization with physics-inspired graph neural networks. Nature Machine Intelligence, 4(4):367\u2013377, April 2022a."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7813/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700082990104,
                "cdate": 1700082990104,
                "tmdate": 1700082990104,
                "mdate": 1700082990104,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hT8nwQlX0P",
                "forum": "9qtswuW5ux",
                "replyto": "Olm6BdndSI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7813/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7813/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer VZaD, Part 3."
                    },
                    "comment": {
                        "value": "#### \u2022  **W3.The evaluation presented in this paper does not convincingly demonstrate QRF-GNN's superiority over heuristic methods.**\n\nIn our experiments we   focused mainly on the comparison between QRF-GNN and existing unsupervised GNNs. As for the heuristics, our intention was to put QRF-GNN in the context of the algorithms that provide the best solutions on the considered benchmark datasets (BLS and TSHEA for Gset, HybridEA for COLOR). In fact, there are dozens of heuristics against which QRF-GNN does demonstrate superiority  (e.g. the heuristics presented in Table 1 in [4] , Table 2 in [5] and Table 1 in [3]). However, we present the comparison with the state-of-the-art methods and show that our algorithm is able to outperform them on some instances. In addition, we also compare our GNN with the strong EO heuristic for Max-Cut, which is able to achieve almost optimal result for d-regular graphs [1], and show that QRF-GNN outperforms it. \nWe agree that QRF-GNN does not show the best results among all existing methods on the considered benchmark instances. But we would like to question whether this is a weakness taking into account the ability to solve a wide range of CO problems, the linear scalability and the large gap between the results of best conventional heuristics and previously known unsupervised learning methods. Please see also the general comment explaining the results.\n\n**In PI-GNN, Fig. 4 and 5 show its advantages in scalability and computational complexity. Yet these properties are not well verified in this paper.**\n\nWe investigated scalability and computational complexity for large random d-regular graphs with vertex counts ranging from 50 thousand to 1 million and found dependence similar to that in PI-GNN (please see Fig. 2 in the appendix). Unlike in Fig. 4 in [4], we calculated the P-value, since the plot scale for the number of cuts does not allow to see the difference in quality for graphs with different numbers of vertices. Additionally, we raised the question of convergence, which has not been done for PI-GNN. Despite the fact that there were not enough iterations for convergence on large graphs, the P-value was still competitive (please compare Fig. 1 in [1] and Fig. 2b in the appendix) given the near-linear computational complexity. It should also be noted that Figs. 4 and 5 in  [4] as well as Fig. 2 in the appendix refer to the scalability of the algorithms without hyperparameter optimization. Under such conditions PI-GNN performs worse then the simple greedy search, while QRF-GNN provides results close to the global optimal bounds (see Fig.1 in [1] and the answer to Q3).\n\n$ \\\\ $\n\n[1] S. Boettcher, Inability of a graph neural network heuristic to outperform greedy algorithms in solving combinatorial optimization problems, Nature Machine Intelligence 5, 24 (2023)\n\n[2] M. C. Angelini and F. Ricci-Tersenghi, Modern graph neural networks do worse than classical greedy algorithms in solving combinatorial optimization problems like maximum independent set, Nature Machine Intelligence 5, 29 (2023).\n\n[3] M. J. A. Schuetz, J. K. Brubaker, Z. Zhu, and H. G. Katzgraber, Graph coloring with physics-inspired graph neural networks, Phys. Rev. Res. 4, 043131 (2022)\n\n[4] Martin J. A. Schuetz, J. Kyle Brubaker, and Helmut G. Katzgraber. Combinatorial optimization with physics-inspired graph neural networks. Nature Machine Intelligence, 4(4):367\u2013377, April 2022a.\n\n[5] W. H. Tonshoff J, Ritzert M and G. M.  Graph neural networks for maximum constraint satisfaction. Frontiers in Artificial Intelligence 2021."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7813/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700084046574,
                "cdate": 1700084046574,
                "tmdate": 1700084046574,
                "mdate": 1700084046574,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VdxRvfHdhh",
                "forum": "9qtswuW5ux",
                "replyto": "Olm6BdndSI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7813/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7813/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer VZaD, Part 4."
                    },
                    "comment": {
                        "value": "#### \u2022  **Q4. In Section 5, the statement mentions, \"...considering that most of the mentioned methods are specialized for solving specific problems...\".**\n\nIn the text of our paper, the quote was referring to the heuristics mentioned in the previous sentence: BLS and TSHEA for Max-Cut and HybridEA for graph coloring. Our aim was to highlight that general GNN solver can compete with the best specialized algorithms. \n\n**In fact, many methods try to propose a general solving framework for solving broad CO problems, such as [1] [2] [3].**\n \nThe algorithms mentioned by the reviewer are very interesting for study and practical use. However, the methods [2],[3] are supervised, which means that the training set of already labeled (i.e. solved) problems is required. Although the supervised learning framework for solving CO problems is very popular, these methods cannot be considered as autonomous algorithms and the comparison between supervised and unsupervised approaches is methodologically incorrect. The method [1] does not need a training set, and we would be interested in comparing QRF-GNN with this method. However, it is not learning-based algorithm, and in our comparative analysis we mainly focused on unsupervised GNNs. \n\n**Additionally, solving algorithms for NP-complete (NPC) problems inherently possess the potential to solve other CO problems since any NP problem can be reduced to an NPC problem.**\n\nWe doubt the practical application of this statement and have not seen any algorithm based on this in the literature. We would like to ask the reviewer to share examples if they know some. Sometimes it makes sense to formulate one problem in terms of another. For instance, Mixed-Integer Linear Programming  and QUBO formulations have proven its effectiveness in practice. However, this is not directly related to polynomial-time reductions in NP-completeness theory.\n\n$ \\\\ $\n\n[1] Haoran Sun,  Katayoon Goshvadi, Azade Nova, Dale Schuurmans, Hanjun Dai. Revisiting Sampling for Combinatorial Optimization. ICML 2023.\n\n[2] Zhiqing Sun, Yiming Yang. DIFUSCO: Graph-based Diffusion Solvers for Combinatorial Optimization. NeurIPS 2023.\n\n[3] Yang Li, Jinpei Guo, Runzhong Wang, Junchi Yan. From Distribution Learning in Training to Gradient Search in Testing for Combinatorial Optimization. NeurIPS 2023."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7813/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700084461485,
                "cdate": 1700084461485,
                "tmdate": 1700084461485,
                "mdate": 1700084461485,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pStdGwmwFa",
                "forum": "9qtswuW5ux",
                "replyto": "Olm6BdndSI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7813/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7813/Authors"
                ],
                "content": {
                    "title": {
                        "value": "to Reviewer VZaD"
                    },
                    "comment": {
                        "value": "Dear Reviewer VZaD,\n\nGiven that time is very short, we look forward to your feedback. We hope that our answers and explanations were convincing. Please let us know if we were able to answer your questions.\n\nWe greatly appreciate your time and effort.\n\nSincerely, the authors"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7813/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700656126662,
                "cdate": 1700656126662,
                "tmdate": 1700656126662,
                "mdate": 1700656126662,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DlyDhBgnuu",
                "forum": "9qtswuW5ux",
                "replyto": "Olm6BdndSI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7813/Reviewer_VZaD"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7813/Reviewer_VZaD"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the response"
                    },
                    "comment": {
                        "value": "Thanks for the reply. I appreciate the effort you've put into this. However, I have remaining concerns.\n\n1. The method is still not novel to me. I understand that QRF-GNN maintains some differences from PI-GNN and RUN-CSP, but it still seems to be a close effort with similar designs, since RUN-CSP has already considered the recurrent features. If your difference to RUN-CSP lies in some specific message-passing design like the use of hidden states of neighbors, you should take this as your core method novelty to explain. However, such novelty may still not be enough for me.\n\n2. How are the scalability and computational complexity of QRF-GNN compared to other baselines?"
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7813/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700711432516,
                "cdate": 1700711432516,
                "tmdate": 1700711512355,
                "mdate": 1700711512355,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "s7AS0yUfMC",
                "forum": "9qtswuW5ux",
                "replyto": "Olm6BdndSI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7813/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7813/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer VZaD"
                    },
                    "comment": {
                        "value": "**A1.** We apologize that our previous explanations and experiments have not emphasized novelty to the extent necessary to influence your decision. In conclusion, we would like to highlight some points. The RUN-CSP work is not a pioneer article in the study of recurrence for graph neural networks. Attempts to apply recurrence have existed since at least 2017, when the authors of GraphSAGE proposed using the LSTM aggregation function [1]. In this sense, the RUN-CSP architecture is more similar to the GraphSAGE with LSTM aggregation than to QRF-GNN. However, previous recurrent architectures failed to produce high quality solutions for combinatorial problems. We propose a different type of recurrence, which we described in detail in our previous answers (Part 1) and in Section 3, in particular in the scheme of Algorithm 1. The recurrence mechanism is generally a widely used technique in deep learning, but can take a variety of significantly different forms that can fundamentally change the behavior of the entire architecture. In addition, the difference between RUN-CSP and QRF-GNN is not only in the different implementation of recurrence, but also in the problem formulation, the loss function used, the method of aggregation of neighboring states, and the method of training (RUN-CSP is additionally pretrained on synthetic data in order to be further trained on other instances). We still believe that the results obtained using our architecture deserve the attention of the community, since they are qualitatively superior to the results from other unsupervised graph neural networks.\n\n$\n\\\\\n$\n\n**A2.** The main baselines in our paper are unsupervised graph neural networks, in particular PI-GNN and RUN-CSP. As we have written above, in the RUN-CSP paper the authors pretrain the model on synthetic instances, so it is challenging to make a fair comparison with it. As another baseline, we present the results of the PI-GNN with per-instance hyperparameter optimization. As we discussed in parts 2,3 of our previous response as well as in the section 4 of the paper, the authors of the algorithm give scalability for the algorithm without hyperparameter optimization, which obtains results of very low quality. If compared with the setting of the PI-GNN, whose results are used as a baseline, we have done it terms of computational time (Table 5.) and demonstrated the advantage. \n\nBy reporting Fig. 2 we aimed to show the scalability and linear ~$O(n)$ complexity of QRF-GNN on d-regular graphs, which is optimal in the context of approximate solution methods for combinatorial optimization problems. Hence, we considered this result to be self-sufficient, and it allows the authors of other papers to compare with our algorithm on this criterion. For d-regular graphs we also compare with non-learning EO [2] heuristic baseline which has the theoretical complexity of $O(n^4)$.\n\n$\n\\\\\n$\n\n[1] William L. Hamilton, Rex Ying, and Jure Leskovec. Inductive representation learning on large graphs. In Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS\u201917, pp. 1025\u20131035, Red Hook, NY, USA, 2017b. Curran Associates Inc. ISBN 9781510860964.\n\n[2]Stefan Boettcher and Allon G. Percus. Optimization with extremal dynamics. Physical Review Letters, 86(23):5211\u20135214, June 2001. doi: 10.1103/physrevlett.86.5211.\n\n$\n\\\\\n$\n\nWe hope that we have been able to address the reviewer's concerns. \n\nBest regards, authors."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7813/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700737732479,
                "cdate": 1700737732479,
                "tmdate": 1700738307777,
                "mdate": 1700738307777,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "GMmzOpF9pp",
            "forum": "9qtswuW5ux",
            "replyto": "9qtswuW5ux",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7813/Reviewer_oXir"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7813/Reviewer_oXir"
            ],
            "content": {
                "summary": {
                    "value": "This paper leverages the GNNs to solve the quadratic unconstrained binary optimization problems, especially for the maximum cut and graph coloring problems.  Experimental results demonstrate the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Using Neural Networks to solve combinatorial optimization problems is an interesting topic.\n2. The proposed QRF-GNN can outperform existing GNNs in solving Max-Cut and graph coloring problems.\n3. The paper is well-written and easy to follow."
                },
                "weaknesses": {
                    "value": "1. The paper's novelty appears constrained, utilizing a GNN with QUBO's continuous relaxation as a loss function for combinatorial optimization. Clarification on how this differs from prior work would be beneficial.\n\n2. The motivation behind the specific design of the GNN, particularly the use of two different convolutional layers (SAGEConv Pool and SAGEConv mean) in the first layer, is not explained. A rationale for these choices should be provided.\n\n3. An ablation study is needed to substantiate the proposed GNN's superiority over existing methods. Without this, the advantage of the proposed model remains unclear."
                },
                "questions": {
                    "value": "Please refer to the weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7813/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7813/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7813/Reviewer_oXir"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7813/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699474608405,
            "cdate": 1699474608405,
            "tmdate": 1699636956131,
            "mdate": 1699636956131,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wkITXFNKdX",
                "forum": "9qtswuW5ux",
                "replyto": "GMmzOpF9pp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7813/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7813/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer oXir"
                    },
                    "comment": {
                        "value": "We thank the reviewer oXir for their comment.\n\n\n#### \u2022  **Q1. The paper's novelty appears constrained, utilizing a GNN with QUBO's continuous relaxation as a loss function for combinatorial optimization. Clarification on how this differs from prior work would be beneficial.** \nThe main difference from the previous architectures is the use of recurrent node features within the message-passing protocol. We have written a more detailed answer in the general comment, please find additional information there.\n\n#### \u2022  **Q2. The motivation behind the specific design of the GNN, particularly the use of two different convolutional layers (SAGEConv Pool and SAGEConv mean) in the first layer, is not explained. A rationale for these choices should be provided.**\nThe use of different parallel layers was intended to improve representation capacity while preventing the oversmoothing effect. It should be noted, that the particular design of the architecture plays minor role, since the main gain in quality comes from the recurrence, which is supported by the ablation study in the section B of the appendix.\n\n\n#### \u2022  **Q3. An ablation study is needed to substantiate the proposed GNN's superiority over existing methods. Without this, the advantage of the proposed model remains unclear.**\nIn the appendix of the paper we provide a detailed ablation study of our method, showing the contribution of each part of the algorithm.  To demonstrate the advantage of the proposed GNN over existing methods, numerical experiments are given in section 4 of the main body of the paper. Could you please give us more specific request, what other analyses would be helpful for us to illustrate the advantage of the proposed model?"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7813/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700082345087,
                "cdate": 1700082345087,
                "tmdate": 1700082345087,
                "mdate": 1700082345087,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "v79ThIeFTK",
                "forum": "9qtswuW5ux",
                "replyto": "GMmzOpF9pp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7813/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7813/Authors"
                ],
                "content": {
                    "title": {
                        "value": "to Reviewer oXir"
                    },
                    "comment": {
                        "value": "Dear Reviewer oXir,\n\nGiven that time is very short, we look forward to your feedback. Please also let us know if you have any additional questions or concerns about our work.\n\nWe greatly appreciate your time and effort.\n\nSincerely, the authors"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7813/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700656111800,
                "cdate": 1700656111800,
                "tmdate": 1700656111800,
                "mdate": 1700656111800,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XHHThzwyKO",
                "forum": "9qtswuW5ux",
                "replyto": "v79ThIeFTK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7813/Reviewer_oXir"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7813/Reviewer_oXir"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the response from the authors. I still have questions about the novelty and motivation of the proposed designs. It seems the novelty lies in the use of recurrent node features. However, I didn't see much emphasis on the recurrent node feature in the paper, and the motivation for using recurrent node features is unclear to me. Also, authors said, \"The use of different parallel layers was intended to improve representation capacity while preventing the oversmoothing effect\"; \"Other modifications of the architecture (i.e. parallel convolutional layers) and different static features were used to increase the expressive power of GNN. \" Where do these conclusions come from?"
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7813/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700670833680,
                "cdate": 1700670833680,
                "tmdate": 1700670833680,
                "mdate": 1700670833680,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]