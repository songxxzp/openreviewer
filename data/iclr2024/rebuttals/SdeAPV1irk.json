[
    {
        "title": "Incremental Randomized Smoothing Certification"
    },
    {
        "review": {
            "id": "dwNQ1BCnmw",
            "forum": "SdeAPV1irk",
            "replyto": "SdeAPV1irk",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2086/Reviewer_qzcN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2086/Reviewer_qzcN"
            ],
            "content": {
                "summary": {
                    "value": "This paper study certified robustness with randomized smoothing. The authors present a method that decreases the sample complexity of randomized smoothing in the setting where there is a classifier $f$ and an approximation of the same classifier $f^p$ (for example, $f^p$ is a quantized version of $f$). The method, called Incremental Randomized Smoothing, proposes to compute the certification of $f^p$ via the certificate of $f$. The method relies on estimating the disparity $\\zeta_x$ which is the upper bound on the probability that outputs of $f$ and $f^p$ are distinct."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Randomized smoothing is an important method, and currently the state-of-the-art approach, for certified robustness. Given the computational cost of this method, it is important to investigate how to make randomized more efficient. This paper investigates how to reduce the number of samples necessary for computing the certificate via Monte Carlo sampling. \n- The paper is well-written, the theorems and algorithm are clear."
                },
                "weaknesses": {
                    "value": "**Main Comment**.\nI don't understand the main premise and setting used in this paper. I find one of the assumptions very strong and the practical implications of the method very limited. More detail below. \n\nThe authors state the following sentence in the abstract:\n\n_``[...] when the smoothed model is modified (e.g., quantized or pruned), certification guarantees may not hold for the modified DNN, and recertifying from scratch can be prohibitively expensive.``_\n\nLet $f$ be a base classifier, $f^p$ be a quantized version of $f$, and let $g$ be the smooth version of $f$ and $g^p$ be the smooth version of $f^p$. \nIt is true that a certificate computed from the _base model_ $f$ will not hold for the quantized version $f^p$. However, it would be possible to apply randomized smoothing directly to the quantized version $f^p$ via:\n$$\ng^p(x) = \\underset{c \\in \\mathcal{Y}}{\\operatorname{argmax}} \\ \\mathbb{P}_\\epsilon [\\ f^p(x + \\epsilon) = c\\ ]\n$$\n\nInstead, the authors propose to compute the certificate of $f^p$ by first computing the certificate for $f$ (which is the unquantized model and therefore expensive to run) and then computing the disparity $\\zeta_x$, which is an upper bound on the probability that the outputs of $f$ and $f^p$ are different.  \n$\\rightarrow$ It seems to me that this method is more expensive than computing the certificate directly on the quantized version $f^p$. \n\nTo claim that the approach is more efficient, the authors **assume** that the certificates for $f$ are available **for all $x$**, and therefore only the disparity $\\zeta_x$ is needed to compute the new certificate. The authors state:  \n\n_``The IRS algorithm utilizes a cache $C_f$, which stores information obtained from the RS execution of the classifier $f$ for each input $x$. The cached information is crucial for the operation of IRS. $C_f$ stores the top predicted class index and its lower confidence bound $\\underline{p_A}$ for $f$ on input $x$.``_\n\n$\\rightarrow$ The authors assume that the test data is already available and that the certificates have already been computed. I don't see how this can be realistic, especially since the authors mention that the quantized version $f^p$ can be used on edge devices, except perhaps if the model is only used for a limited set of inputs that are known in advance.\n\nCan the authors comment on this and provide a practical use case that I may have missed?"
                },
                "questions": {
                    "value": "- Why the authors use the same Gaussian samples (same seed) in Algorithm 3? Is there any benefit?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2086/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2086/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2086/Reviewer_qzcN"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2086/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698253650036,
            "cdate": 1698253650036,
            "tmdate": 1699930980442,
            "mdate": 1699930980442,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "deqz3cgUnV",
                "forum": "SdeAPV1irk",
                "replyto": "dwNQ1BCnmw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2086/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2086/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for insightful and constructive feedback.\n\n> It seems to me that this method is more expensive than computing the certificate directly on the quantized version.\n> To claim that the approach is more efficient, the authors assume that the certificates for \n are available for all $x$\n> The authors assume that the test data is already available and that the certificates have already been computed. I don't see how this can be realistic.\n\nI think there is a misunderstanding with the motivation of our setup. Please see common response C1 for the motivation of the setup that we focus on in this work. \n\nIRS is not applicable while certifying individual approximate networks. We focus on the scenarios where we intend to certify multiple similar networks on a fixed test set to compute the certified radii. Computing certified radii for similar networks is a common occurrence when a user is comparing and selecting the best approximation for a network using techniques such as approximation tuning. For certifying the first network we cannot use IRS, however, for all subsequent network certifications, we can use IRS for faster certification.\n\n\n> Why do the authors use the same Gaussian samples (same seed) in Algorithm 3? Is there any benefit?\n\nYes, using the same seed is crucial for making our algorithm 2x faster. The estimation of $\\zeta_{x}$ involves computing $f(x+\\epsilon)$ and $f^p(x+\\epsilon)$ on Gaussian-corrupted versions of the input $x$. Given that we have already conducted this computation for $f$, we store the results for those samples in the cache. Consequently, we only need to compute $f^p$ on Gaussian-corrupted inputs. To ensure the soundness of this process, we utilize the same Gaussian corruptions for $f^p$ by retaining and reusing the seed. This approach incurs small memory and computational overhead.\n\nThanks for pointing it out, we will further clarify our reasoning for using the same Gaussian samples in the paper."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2086/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699930338272,
                "cdate": 1699930338272,
                "tmdate": 1699930338272,
                "mdate": 1699930338272,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9Xkjgu8DNW",
                "forum": "SdeAPV1irk",
                "replyto": "deqz3cgUnV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2086/Reviewer_qzcN"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2086/Reviewer_qzcN"
                ],
                "content": {
                    "title": {
                        "value": "Response to Rebuttal"
                    },
                    "comment": {
                        "value": "Thank you for providing a detailed explanation of the motivation and setup. The addition of various scenarios to the paper would strengthen it and improve reader comprehension. \nThe method is interesting and useful in these specific contexts. I lean toward acceptance and increase the score accordingly."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2086/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699930959221,
                "cdate": 1699930959221,
                "tmdate": 1699930959221,
                "mdate": 1699930959221,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "kBSjxfv0oV",
            "forum": "SdeAPV1irk",
            "replyto": "SdeAPV1irk",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2086/Reviewer_6uEd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2086/Reviewer_6uEd"
            ],
            "content": {
                "summary": {
                    "value": "The paper deals with the problem of providing randomized smoothing-based certificates for modified neural networks. Given a modified version of a base model $f_p$ for some original base model $f$, the task is to provide a robustness certificate for the prediction of smoothed model $g_p$ at a point $x$ by efficiently reusing the values observed when calculating the certificate for smoothed model $g$ at the same $x$. The authors propose to do this using the fact that the difference between the value $g(x + \\epsilon)$ and $g_p(x + \\epsilon)$ around any point $x$ is very small (close to $0$) and the fact that the number of binomial samples required to estimate a parameter close to 0 is much smaller than the number of samples needed to estimate a binomial parameter close to 0.5. Given the difference in the value of $g$ and $g_p$ around a point $x$ and the certificate for $g(x)$, the authors give a formula to bound the certificate for $g_p$ at $x$."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The idea of reusing the observations for calculating the certificate for $g$ to calculate the certificate of $g_p$ is novel and interesting. \n- The authors also use a great insight that it is more efficient to estimate binomial parameters at extreme ends than near the middle.\n- The paper is well-written and easy to understand."
                },
                "weaknesses": {
                    "value": "- The practical usefulness of the proposed method is not clear. As randomized smoothing produces certificates at inference time, in order to calculate the certificate around a given point in this approach, the edge device would need access to both the original as well as the modified neural network models, which is not feasible."
                },
                "questions": {
                    "value": "Please refer to the weaknesses section for questions."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2086/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2086/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2086/Reviewer_6uEd"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2086/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698787317695,
            "cdate": 1698787317695,
            "tmdate": 1700093018884,
            "mdate": 1700093018884,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "XWZfY8aJ4B",
                "forum": "SdeAPV1irk",
                "replyto": "kBSjxfv0oV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2086/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2086/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for insightful and constructive comments.\n\n> The practical usefulness of the proposed method is not clear. As randomized smoothing produces certificates at inference time, in order to calculate the certificate around a given point in this approach, the edge device would need access to both the original as well as the modified neural network models, which is not feasible.\n\nPlease see common response C1 for motivation. IRS is not applicable for improving the inference time efficiency of RS. We focus on the common occurrences where the goal is to compute certified radius for multiple similar networks offline for comparison. For instance, when the user is selecting the best approximation for a network using the approximation tuning techniques as described in C1. We are happy to provide additional clarification to ensure a better understanding of our motivation."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2086/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699930277662,
                "cdate": 1699930277662,
                "tmdate": 1699930277662,
                "mdate": 1699930277662,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "0MiEYTecC0",
            "forum": "SdeAPV1irk",
            "replyto": "SdeAPV1irk",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2086/Reviewer_VbzH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2086/Reviewer_VbzH"
            ],
            "content": {
                "summary": {
                    "value": "This work proposed how to certify a similar neural network via randomized smoothing by re-using the certification result from the original neural network. An IRS certification algorithm is provided in Algorithm 2 and its theory is provided in Theorem 2. The experiments on Cifar10 and ImageNet dataset showed the efficiency of the proposed algorithm for certifying different quantization models (fp16, bf16 and int8) and pruned models."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This work proposed a first incremental approach for randomized smoothing to certify a similar (compressed) version of the original neural network with improved efficiency by re-using the certification results. \n2. The experiments results seem to be promising."
                },
                "weaknesses": {
                    "value": "1. Demanding prerequisite: I am not sure how likely IRS algorithm is applicable in practice. It seems like IRS will require many prerequisite. For example, IRS needs to know the certification cache from the original neural network, which makes the requirement more demanding. If there is no such information, regular RS is still needed. As another requirement, IRS needs the modified network to be a good approximation of the original neural network. Otherwise, the accuracy might be reduced per theorem 2.\n2. Novelty issue with the theory: for the theory part, most of the theorems are built upon theorem 1 in [Cohen et al 2019] and are direct application of that theorem, hence raising a novelty issue."
                },
                "questions": {
                    "value": "See weakness above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2086/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2086/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2086/Reviewer_VbzH"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2086/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698800079818,
            "cdate": 1698800079818,
            "tmdate": 1699636140740,
            "mdate": 1699636140740,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "C5SHAwDi4u",
                "forum": "SdeAPV1irk",
                "replyto": "0MiEYTecC0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2086/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2086/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for insightful and constructive feedback.\n\n> Demanding prerequisite: I am not sure how likely the IRS algorithm is applicable in practice. It seems like the IRS will require many prerequisites. For example, IRS needs to know the certification cache from the original neural network, which makes the requirement more demanding.\n\nPlease see common response C1 for motivation. The prerequisite is common in the scenario where the objective is to compute a certified radius for multiple similar networks offline. For example, when the user aims to select the best approximation for a network by utilizing techniques such as approximation tuning.\n\n> Novelty issue with the theory: for the theory part, most of the theorems are built upon theorem 1 in [Cohen et al 2019] and are direct applications of that theorem, hence raising a novelty issue.\n\n\nTheorems 2, 3, and 4 combined are crucial to prove the soundness of our Algorithm. Theorem 1 by [Cohen et al 2019] considers standard RS for a single network. Our theorems show how to use the estimated value of $\\zeta_x$ to transfer the certification guarantees across networks. We believe that this result is non-trivial and a novel theoretical contribution."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2086/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699930227186,
                "cdate": 1699930227186,
                "tmdate": 1699930227186,
                "mdate": 1699930227186,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6ntLZrPtFX",
                "forum": "SdeAPV1irk",
                "replyto": "C5SHAwDi4u",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2086/Reviewer_VbzH"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2086/Reviewer_VbzH"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your response. I have read the authors' reply and would like to keep my rating mainly due to weakness 1."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2086/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700434240417,
                "cdate": 1700434240417,
                "tmdate": 1700434240417,
                "mdate": 1700434240417,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "UHYhbidiT5",
            "forum": "SdeAPV1irk",
            "replyto": "SdeAPV1irk",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2086/Reviewer_7muz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2086/Reviewer_7muz"
            ],
            "content": {
                "summary": {
                    "value": "This work studies the efficiency of robustness certification in the case of approximated models by reusing the robustness guarantees in the original models. Specifically, the disparity between the original smoothed classifier and the approximated smoothed one is estimated to speed up the whole certification as it is relatively close to 0. The experiments show that the speed-up is obvious on different datasets with different models and smoothing parameters."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper is well-written and easy to follow. The motivation is clear and important.\n- The methodology is sound and it is friendly to read although it can be formally expressed with more complicated notations.\n- The experiment is extensive and validates the effectiveness and efficiency of the method."
                },
                "weaknesses": {
                    "value": "- Insight 1 in Section 3.1 is not very convincing in the sense of a single setting of n=1k and $\\sigma=1$, where it usually costs 10k-100k samples for Monte Carlo sampling in estimation. More examples can be given to show $\\zeta$ is small.\n- For insight 2 and Figure 2, although the needed samples are much less compared to 0.5, it still needs 41.5k and there is no significant reduction compred to naive Monte Carlo randomized smoothing (10k-100k). A better way is to use an example of current estimation of $p_A$ and to show the needed samples are much less when estimating $\\zeta$ compared to $p_A$.\n- The choice of threshold $\\gamma$ seems to be critical from the experiment results and the authors use grid search to optimize it. If I understand it correctly, whether to estimate $\\zeta$ actually depends on whether $\\zeta$ is closer to 0 than $p_A$ is to 1. So ideally, there can be some theoretical analysis for choosing $\\gamma$ in terms of $\\zeta$ and $p_A$.\n- I think there is a missing ablation study of directly using naive Monte Carlo to estimate $\\zeta$ instead of reusing seeds in terms of both certified radius and certification time.\n- There are some typos and text messed up in the last two paragraphs in Section 3.2, e.g. In case, ... are correct with..."
                },
                "questions": {
                    "value": "See Weakness"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2086/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2086/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2086/Reviewer_7muz"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2086/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698811568542,
            "cdate": 1698811568542,
            "tmdate": 1699636140669,
            "mdate": 1699636140669,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "pmsCputqFm",
                "forum": "SdeAPV1irk",
                "replyto": "UHYhbidiT5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2086/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2086/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for insightful and constructive comments.\n\n> Insight 1 in Section 3.1 is not very convincing in the sense of a single setting of $n=1k$ and, where it usually costs $10k-100k$ samples for Monte Carlo sampling in estimation. More examples can be given to show is $\\zeta_{x}$ is small.\n\nIn Appendix A4, we show $\\zeta_{x}$ for all networks and $\\sigma$ values. We will point to this section in our insight 1. By considering $n=1000$ (smaller than $n=10k,100k$) we show that $\\zeta_{x}$ is small with high confidence. The choice of smaller value $n$ for these experiments gives a strong argument that the IRS algorithm that uses the $\\zeta_{x}$ estimate is likely to work well as it needs fewer samples to show that $\\zeta_{x}$ is small. If the reviewer still thinks that experiments with larger values of $n$ would help strengthen this argument, we will add it to the paper.\n\n\n> For insight 2 and Figure 2, although the needed samples are much less compared to 0.5, it still needs 41.5k and there is no significant reduction compared to naive Monte Carlo randomized smoothing (10k-100k). A better way is to use an example of the current estimation of $p_A$ and to show the needed samples are much less when estimating $\\zeta_{x}$ compared to $p_A$.\n\nFirstly, we appreciate your suggestion and we will make the change accordingly. We agree with the reviewer that the evidence on the distribution of $p_A$ strengthens the insight to motivate that the IRS algorithm will improve the sample efficiency. In appendix section A5, we show our observations on the distribution $p_A$ for different $\\sigma$s. These observations show that $p_A$ is not close to $1$, especially when the $\\sigma$ is larger. We will be happy to put these observations as one of our insights to make our presentation stronger.\n\n\n> The choice of threshold $\\gamma$ seems to be critical from the experiment results and the authors use grid search to optimize it. If I understand it correctly, whether to estimate $\\zeta_{x}$ actually depends on whether is closer $\\zeta_{x}$ to 0 than $p_A$ is to 1. So ideally, there can be some theoretical analysis for choosing $\\gamma$ in terms of $\\zeta_{x}$ and $p_A$.\n\nYes, your understanding is correct, in the IRS algorithm the decision to estimate $\\zeta_{x}$ depends on whether $\\zeta_{x}$  is closer to 0 than $p_A$ is to 1.\n\nWhile this analysis can be theoretically interesting, practically, we do not anticipate the analysis to help with the efficiency of the search compared to grid search for the following reasons. We found out that the final ACR is not too sensitive around the best choice of $\\gamma$, and a simple grid search works quite well for this search. Since $\\zeta_{x}$ is a function of each input $x$, for such an analysis to work it should rely on the distribution of $\\zeta_{x}$ over different $x$s - computing that can be potentially expensive.\n\n> I think there is a missing ablation study of directly using naive Monte Carlo to estimate \n instead of reusing seeds in terms of both certified radius and certification time.\n\nIn theory, using naive Monte Carlo without seed reuse will not change the certified radius and it will be 2x slower than reusing seeds. We are happy to emphasize this in the paper. If the reviewer thinks that additional experiments will benefit the paper, we will add it in the revised version."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2086/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699930124041,
                "cdate": 1699930124041,
                "tmdate": 1699930124041,
                "mdate": 1699930124041,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]