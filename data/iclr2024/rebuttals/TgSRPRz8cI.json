[
    {
        "title": "Patched Denoising Diffusion Models For High-Resolution Image Synthesis"
    },
    {
        "review": {
            "id": "mKTMC6nhvk",
            "forum": "TgSRPRz8cI",
            "replyto": "TgSRPRz8cI",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1444/Reviewer_oyqb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1444/Reviewer_oyqb"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new feature collage strategy for the generative diffusion model to avoid boundary artifacts when synthesizing large-size images, termed Patch-DM. Feature collage systematically crops and combines partial features of the neighboring patches to predict the features of a shifted image patch, allowing the seamless generation of the entire image due to the overlap in the patch feature space. Experiments reveal the superiority of 1K resolution generation results on several datasets with 64\u00d764 patches."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper proposes a new feature collage strategy for generative diffusion model to avoid boundary artifact when synthesizing large-size images."
                },
                "weaknesses": {
                    "value": "1.\tThe novelty is relatively small and the impact of this paper may be limited since the proposed method only focuses on the boundary artifacts produced by the patch collage.\n2.\tLacking comparisons with aggregation sampling strategies proposed in StableSR [1], the sampling strategy in StableSR can be performed without more parameters and training. \n3.\tThe experiments only measure models with FID, lacking results measured under other metrics, e.g., CLIPScore.\n4.\tFigures 5 and 6 look confusing. (Which pictures are from which datasets or methods seem to be unclear)\n\n[1] Wang, J., Yue, Z., Zhou, S., Chan, K. C., & Loy, C. C. (2023). Exploiting Diffusion Prior for Real-World Image Super-Resolution. arXiv preprint arXiv:2305.07015."
                },
                "questions": {
                    "value": "Can the model be performed to generate images of more than 1K or arbitrary resolutions?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1444/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1444/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1444/Reviewer_oyqb"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1444/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698320462119,
            "cdate": 1698320462119,
            "tmdate": 1699636073068,
            "mdate": 1699636073068,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ne6XE9QISh",
                "forum": "TgSRPRz8cI",
                "replyto": "mKTMC6nhvk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1444/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1444/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer oyqb"
                    },
                    "comment": {
                        "value": "We thank reviewer oyqb for the valuable feedback.\n\n**The novelty is relatively small and the impact of this paper may be limited since the proposed method only focuses on the boundary artifacts produced by the patch collage.**\n\nThe main motivation of ours is to build a patch-based diffusion model for high-resolution image synthesis as diffusion models are hard to directly optimize in image space and previous diffusion methods working on high-resolution image generation usually utilize a VQ-GAN to encode the image to latent space(Stable Diffusion) or a super-resolution model that upsamples the generated image to a higher resolution(DALLE-2, Imagen). Patch-based image generation has two main challenges, one is global consistency and the other is the boundary artifacts between patches. We utilize a global embedding for global image consistency and propose the mechanism that does the patch collage in the feature space to solve the boundary artifacts problem which we believe could benefit the community by providing another solution for generating high-resolution images using diffusion models (also more efficient as it could utilize a small model e.g., 64x64 to generate high-resolution images e.g., 1024x1024).  \n\n**Lacking comparisons with aggregation sampling strategies proposed in StableSR [1], the sampling strategy in StableSR can be performed without more parameters and training.**\n\nThanks for raising this paper to us. We\u2019re not aware of this paper which was posted in May this year on Arxiv. There are some major differences between ours and StableSR. First, StableSR is a paper for the image super-resolution task while ours is to generate high-resolution images directly. Second, StableSR adds another module to the pretrained Stable Diffusion model and further train this added module for the image super-resolution task while ours train a patch-based diffusion model from scratch. Third, the aggregation sampling strategies utilize overlapping patches over pretrained stable diffusion models which train on complete images, with a feature map of the low-resolution input but our model does both training and inference on the partial image patches without such structured feature maps. We attempted to perform aggregation sampling but found the results to be blur e.g., an average blurred face. We assume the reasons behind this are that we trained on partial patch images and there are no structured feature maps to serve as conditions so the nearby patches will be largely different, unlike the ones with structured feature maps as conditions that can enforce the overall content and structure to be consistent.\n\n**The experiments only measure models with FID, lacking results measured under other metrics, e.g., CLIPScore.**\n\nThanks for the suggestion. However, our paper focuses on unconditional image generation. We also measure the models with sFID besides the most used FID in the unconditional image generation domain. CLIPScore might not be a direct fit for us as it\u2019s a metric for measuring the consistency of the generated images with the given texts or images which we don\u2019t utilize for our unconditional generation.\n\n**Figures 5 and 6 look confusing. (Which pictures are from which datasets or methods seem to be unclear)**\n\nThanks. In Figure 5, we label the pictures from the left to right as FFHQ, LSUN-Bedroom and LSUN-Church and they\u2019re all generated by our method. In Figure 6, we utilize two datasets here: LSUN-Bedroom and LSUN-Church where the left half of each group is LSUN-Bedroom and the right half is LSUN-Church. The methods have been labeled accordingly as COCO-GAN, InfinityGAN and ours. We\u2019ve updated the captions to add more clarifications on this.\n\n**Can the model be performed to generate images of more than 1K or arbitrary resolutions?**\n\nIn our paper, we show an example of generating a 2048x1024 image in Figure 4 by adding more patches during inference time when only trains on 1024x512 images. Also, our method can be utilized to train on higher-resolutions by adding more patches or increasing the patch size."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1444/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700682360212,
                "cdate": 1700682360212,
                "tmdate": 1700697426811,
                "mdate": 1700697426811,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "0Z4K6x4fOr",
            "forum": "TgSRPRz8cI",
            "replyto": "TgSRPRz8cI",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1444/Reviewer_Tdt5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1444/Reviewer_Tdt5"
            ],
            "content": {
                "summary": {
                    "value": "This work aims to resolve the limitation of the existing diffusion models towards generating high-resolution images. To this end, the authors present the ideas of feature collage, position embedding, and global conditioning to develop a unified approach, namely Patch-DM, to generate high-resolution images. Moreover, potential applications of outpainting and inpainting are also demonstrated. Comparisons on low-resolution images are conducted to reveal that the proposed method doesn't underperform by a large margin compared to other methods. Comparison of high-resolution images reveals that the proposed method achieves state-of-the-art results."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The ideas of feature collage, position embedding, and global conditioning and their impact on image generation is interesting.\n2. The proposed method achieves state-of-the-art results on high-resolution image synthesis"
                },
                "weaknesses": {
                    "value": "1. On Low resolution image synthesis as in Table 2, the proposed method is not comparable to that of state-of-the-art. \n2. Some of the implementation details, reasoning behind choices, are missing in the description. Please refer to the comments under Questions for details."
                },
                "questions": {
                    "value": "1. Section 5.2: Since every image is segmented into smaller patches, the total number of model parameters is much smaller than other large diffusion models. => The computations might be lesser, why should the number of model parameters be lesser? Authors argue that they use light weight models, but it\u2019s unclear what architectural changes they made for it as compared to existing diffusion models, and how those changes are justifiable. In fact, authors might be able to generate better quality images with heavier architectures and make Table 2 performance comparable to that of previous diffusion models. \n\n2. Beyond patch generation: The first one is to add patches inside the original images so that the generated images can have a 2\u00d7 resolution compared to the ones in the training dataset. => Why do we need to add patches to original images? During test time the images are supposed to be generated from random noise and hence there is no need for adding patches to original images. This sentence is confusing and should be rewritten for clarity. Also the position embedding adaptations is not well explained, so is the reason for different choices with respect to the two methods mentioned under this category.\n\n3. Image inpainting: No details of the position embedding and the related changes is mentioned in this case. \n\n4. Image inpainting: The details could be incomplete. It is unclear how the original contents in the non-masked region is maintained in the output. Why is the global embedding not used?\n\nMinor Fix: \n- Section 5.2, still be \u201drecovered\u201d during => still be ``recovered'' during"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1444/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698480294040,
            "cdate": 1698480294040,
            "tmdate": 1699636072979,
            "mdate": 1699636072979,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "M3ckbCTOVX",
                "forum": "TgSRPRz8cI",
                "replyto": "0Z4K6x4fOr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1444/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1444/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Tdt5"
                    },
                    "comment": {
                        "value": "We thank reviewer Tdt5 for the valuable feedback.\n\n**On low-resolution image synthesis as in Table 2, the proposed method is not comparable to that of state-of-the-art.**\n\nThanks for raising this concern. The main motivation of ours is to build a patch-based diffusion model for high-resolution image synthesis as diffusion models are hard to directly optimize in image space and previous diffusion methods working on high-resolution image generation usually utilize a VQ-GAN to encode the image to latent space(Stable Diffusion) or a super-resolution model that upsamples the generated image to a higher resolution. Our method does not out-perform the state-of-the-art results on low-resolution image synthesis but still achieve good results both quantitatively and qualitatively. Nevertheless, our model contains much fewer parameters compared to the previous diffusion methods as shown in Table 2. \n\nOn the other hand, our method outperforms all previous patch-based methods. We believe patch-based methods won\u2019t have advantages on low-resolution images compared to non-patch-based methods as the current computation resources for these tasks are sufficient. But our method could offer potential values for high-resolution image generation as using diffusion models to directly generate high-resolution images is still a hard problem which our method avoids optimizing on high-resolution images spaces by utilizing a patch-based method with Feature Space Patch Collage.\n\n**Section 5.2: ...The computations might be lesser, why should the number of model parameters be lesser? ... In fact, authors might be able to generate better quality images with heavier architectures and make Table 2 performance comparable to that of previous diffusion models.**\n\nThe previous methods on diffusion models such as the well-known ADM usually utilize different architectures for different resolutions. As our method takes 64x64 as input, we utilize the 64x64 model architecture which has much fewer parameters than the 256x256 model. Thus when generating 256x256 images, our model has fewer parameters than other non-patch-based diffusion models. Yes, we agree that with larger model size, ours might offer better performance due to the increase of model capability. We didn\u2019t try it as it\u2019s not our main focus. But it might be a good future direction as we could enlarge the model or patch size to gain much better image quality. Here we mostly would like to demonstrate that we could utilize a smaller model (e.g., for 64x64 image generation) to generate good quality higher-resolution images (e.g., 256x256 or 1k-resolution). \n\n**Beyond patch generation: ...Why do we need to add patches to original images? During test time the images are supposed to be generated from random noise ... Also the position embedding adaptations is not well explained, so is the reason for different choices with respect to the two methods mentioned under this category.**\n\nWhen we generate 512x1024-resolution images same as the training dataset, we need 8x16 patches (random gaussian noises) to start with. By adding more patches we mean that if we\u2019d like to generate a 2x resolution image, we need another 8x16 patches (random gaussian noise maps) to do the generation leading to a total patch number of 16x32. Regarding the position embedding, the reason we chose differently is based on the following intuition: when adding patches internally, the interpolated positional embedding can offer position information. However, when adding patches outside, the positional embeddings could not be interpolated thus we chose to not use position embedding.  We\u2019ve updated the manuscript to clarify more on this. Thanks for raising this confusion.\n\n**Image inpainting: No details of the position embedding and the related changes is mentioned in this case. The details could be incomplete. It is unclear how the original contents in the non-masked region is maintained in the output. Why is the global embedding not used?**\n\nFor the image inpainting, we use the position embedding but no global embedding is used. In each diffusion step, the non-masked region is replaced by the input image (with noise added). Thus the original contents in the non-masked region are maintained in the output. The global embedding is not used since we don\u2019t have the complete image to obtain the global embedding. Therefore, in the image inpainting application, we fully rely on the nearby image content and the position embedding to recover the missing image content."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1444/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700682288202,
                "cdate": 1700682288202,
                "tmdate": 1700697415247,
                "mdate": 1700697415247,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "aQCy0Oy3ff",
            "forum": "TgSRPRz8cI",
            "replyto": "TgSRPRz8cI",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1444/Reviewer_3rts"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1444/Reviewer_3rts"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a patch-based denoising diffusion model called Patch-DM for generating high-resolution images. The key contributions are: 1. Introduces a feature collage strategy to avoid boundary artifacts when synthesizing images from patches. It combines partial features from shifted patches to predict features for a new patch. 2. Achieves state-of-the-art FID scores on 1024x512 natural images and 1024x1024 LSUN/FFHQ images using a lightweight model. 3. Demonstrates Patch-DM can directly generate high-fidelity 1K resolution images with minimal patch boundary effects. 4.Reduces memory complexity compared to full-image diffusion models for high-res synthesis. 5.Shows applications like image outpainting, inpainting, super-resolution without any post-training. 6. Validates through ablation studies that feature collage is better than pixel collage for spatial consistency. 7. Provides an effective patch-based generative modeling approach using diffusion models for high-resolution image synthesis with reduced costs."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "\u2022\tProposes Patch-DM, a novel patch-based denoising diffusion model that can generate high-resolution images directly without relying on hierarchical sampling. This simplifies the sampling procedure.\n\u2022\tIntroduces a new feature collage strategy to avoid boundary artifacts when synthesizing images from patches. It forces consistency by combining partial features from shifted patches.\n\u2022\tAchieves state-of-the-art FID scores on generating natural images and LSUN/FFHQ images using a lightweight model, outperforming prior patch-based methods.\n\u2022\tQualitative results show Patch-DM can produce high-fidelity 1K resolution images with minimal patch boundary effects."
                },
                "weaknesses": {
                    "value": "\u2022\tBased on my experience and recent related publications (e.g., \"Weather Diffusion-PAMI'23\"), patch-based diffusion models often lead to reduced inference efficiency. I hope the authors can provide specific comparisons of inference time and overall efficiency, especially compared to previous GAN methods.\n\u2022\tThe proposed method has limited technical contributions. The authors did not provide detailed explanations or theoretical justifications to explain why the Patch Collage in Feature Space strategy can avoid artifacts. Furthermore, the research and exploration of Semantic Code are not sufficiently in-depth.\n\u2022\tThe authors should provide a quantitative comparison of image inpainting and image outpainting results. Quantitative results would better demonstrate the superiority of the proposed method."
                },
                "questions": {
                    "value": "\u2022\tWhy does the Patch Collage in Feature Space strategy avoid artifacts? Can a detailed analysis and explanation be provided? This is crucial for future work.\n\n\u2022\tWhat is the running speed of the proposed method? How much slower does the Patch Collage in Feature Space strategy make the model inference speed?\n\n\u2022\tWhat are the limitations or further areas of exploration for the proposed method?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1444/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698762839789,
            "cdate": 1698762839789,
            "tmdate": 1699636072910,
            "mdate": 1699636072910,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "t85SMXbP1k",
                "forum": "TgSRPRz8cI",
                "replyto": "aQCy0Oy3ff",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1444/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1444/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 3rts"
                    },
                    "comment": {
                        "value": "We thank reviewer 3rts for the valuable feedback.\n\n**...I hope the authors can provide specific comparisons of inference time and overall efficiency, especially compared to previous GAN methods...** \n\nThanks for the suggestions. We provide our inference efficiency compared with ADM and Diff-AE on 256x256 image generation below (all using NFE=50). As we utilize a 64x64 model to generate 256x256 images. Our model is much smaller and thus can have better efficiency. \n\nWhen compared to GAN methods (single-step), diffusion models won\u2019t have advantages due to diffusion models\u2019 iterative generation mechanism. For example, we did a test on StyleGAN3 of which the inference time is just 0.031s while most diffusion models will require several seconds or more on a single TITAN Xp GPU.  And ours is not exception as we\u2019re developed based on diffusion models and requires multiple steps e.g.,50  to generate real images. Though GAN-based methods run considerably faster than diffusion-based methods, the exploding application of diffusion models suggests that the superior scalability and stability gain of diffusion models over GANs outweigh the compromise of the speed. Nevertheless, the main motivation of our method is trying to generate high-resolution images with small diffusion models. Right now diffusion models are hard to directly train on a high-resolution dataset while previous methods usually utilize a super-resolution way (Imagen or DALLE2) or latent-space way(Stable Diffusion) to generate high-resolution images. Patch-based methods might provide another direction for solving this problem which can directly generate high-resolution images by only operating on low-resolution patches. \n\n|                                  | GFLOPS | Inference time |\n|----------------------------------|--------|----------------|\n| ADM                              | 2227.5 | 17.77s         |\n| DiffAE                           | 2215.1 | 4.81s          |\n| Ours                        | 961.3  | 3.76s          |\n\n**..did not provide detailed explanations or theoretical justifications to explain why the Patch Collage in Feature Space strategy can avoid artifacts. Furthermore, the research and exploration of Semantic Code are not sufficiently in-depth. Why does the Patch Collage in Feature Space strategy avoid artifacts? Can a detailed analysis and explanation be provided? This is crucial for future work.**\n\nWe provide ablations on using patch collage in pixel space in the ablation section of the paper(Figure 9 & Table 4) and show our method\u2019s superiority over the pixel space. The main reason behind this is that it lacks in-depth feature interaction as patches can only interact with other patches in the input image level using the collage in the pixel space. In our method, as the patches can interact with other patches in deep feature levels, the interaction enables better surrounding awareness. Furthermore, patch collage in pixel space can only be done in the original image level while patch collage in feature space can be done in multiple feature levels as the network has many hidden features and they represent different information of the images.  On the other hand,  lots of works on computer vision such as FPN utilizes multiple levels of features from CNN to enable better semantic understanding. The interaction between different patches on multiple levels of features have similar effects here. The generation also requires good representation and the shifted patches in feature space will provide better representation than the shifted patches in pixel space.  \n\nFor the semantic code, we also provided ablations in the paper (Figure 9 & Table 4). The main reason for the semantic code is to enforce global awareness of the generated images since the patch collage mechanism in the feature space is more focused on making the nearby patches consistent which can be seen in Figure 9 where images lack global consistency if no semantic code is provided."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1444/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700681658594,
                "cdate": 1700681658594,
                "tmdate": 1700697395211,
                "mdate": 1700697395211,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CbqGDc3nRF",
                "forum": "TgSRPRz8cI",
                "replyto": "aQCy0Oy3ff",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1444/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1444/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 3rts (Continued)"
                    },
                    "comment": {
                        "value": "**The authors should provide a quantitative comparison of image inpainting and image outpainting results. Quantitative results would better demonstrate the superiority of the proposed method.**\n\nThanks for the suggestion. For image inpainting, we compare our method with SDEdit which can use vanilla diffusion models for image inpainting. When it comes to image outpainting, as vanilla diffusion models that work on fixed-resolution images is hard to directly perform zero-shot outpainting, we compare our method with previous patch-based methods such as COCO-GAN and Inifinity-GAN. Notice that for outpainting with given input, as both COCO-GAN and Infinity-GAN needs to do GAN-inversion and cannot preserve the input faithfully, therefore we compare our method on loosely outpainting(no given input) which is to add patches to unconditionally generate larger images (e.g., generate 384x384 images while trained on 256x256 images) in a zero-shot manner. We provide all the results below. And we can see that our method outperforms the previous methods list in the table on inpainting and outpainting(no given input). On the other hand, our method can support all three tasks in a zero-shot manner. While vanilla diffusion models only support inpainting and previous patched GAN-based methods need to do GAN inversion for image outpainting and does not support inpainting in a zero-shot manner.\n\n|                                 | Inpainting |        | Outpainting|(w/ Given input)        | Outpainting|(w/o given input)         |\n|---------------------------------|------------|--------|-----------------------------|--------|------------------------------|--------|\n|                                 | bedroom    | church | bedroom                     | church | bedroom                      | church |\n| SDEdit(Vanilla Diffusion Model) | 42.67      | 40.85  | -                           | -      | -                            | -      |\n| COCOGAN                         | -          | -      | -                           | -      | 58.38                        | 86.48  |\n| InifinityGAN                    | -          | -      | -                           | -      | 55.01                        | 38.01  |\n| ours                            | **41.23**      | **39.32**  | 53.19                       | 55.49  | **48.73**                        | **31.05**  |\n\n\u201c-\u201d denotes the method does not support this task in a zero-shot manner. COCOGAN and ours both utilize a patch size of 64x64 while InifinityGAN utilizes a patch size of 101x101. We use the validation dataset for inpainting and outpainting(w/ given input). For outpainting(w/o given input), we randomly generate 1000 images.\n\n**What is the running speed of the proposed method? How much slower does the Patch Collage in Feature Space strategy make the model inference speed?**\n\nWe compare the speed of the proposed method above and show that our method has advantages when generating 256x256 images over ADM and Diff-AE. The main reason is that we utilize a much smaller model. For Patch Collage in Feature Space strategy, though it needs to do split and merge operations on the vectors, it does not contain any numeric operations. To evaluate the effects on the efficiency of this operation, we did a test on this specific operation and found that it only accounts for 4% of total inference time.\n\n**What are the limitations or further areas of exploration for the proposed method?**\n\nThe limitations of our method should be similar to other patch-based methods which might not be as good as non-patch-based methods in global consistency as they optimize the original pixel space directly. A simple solution to solve this might be to increase the patch size. For example, in our 1024x1024 experiments there are 16x16 patches but increasing the patch size could lead to fewer patches which could alleviate this problem as like in our 256x256 results where there are only 4x4 patches. For future exploration, please see our general response above."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1444/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700681923324,
                "cdate": 1700681923324,
                "tmdate": 1700697406587,
                "mdate": 1700697406587,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "4YYTO2QybW",
            "forum": "TgSRPRz8cI",
            "replyto": "TgSRPRz8cI",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1444/Reviewer_7jCc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1444/Reviewer_7jCc"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a denoising diffusion model, Patch-DM, for generating high-resolution images (e.g., 1024\u00d7512), trained on small-size image patches (e.g., 64\u00d764). The major contribution of the paper is a new feature collage strategy, which is designed to avoid the boundary artifact when synthesizing large-size images. The authors demonstrate the effectiveness of  Patch-DM on mage synthesis results on their newly collected dataset of nature images (1024\u00d7512), as well as on standard benchmarks of LHQ(1024\u00d7 1024), FFHQ(1024\u00d7 1024) and on other datasets with smaller sizes (256\u00d7256), including LSUN-Bedroom, LSUN-Church, and FFHQ. The show state-of-the-art FID scores on all six datasets for the proposed model. Further, Patch-DM also reduces memory complexity compared to the classic diffusion models."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1) The paper is reasonably well written and easy to follow \n2)  The quantitative results demonstrated in Table 1 and Table 2 shows that the model outperforms state of the art."
                },
                "weaknesses": {
                    "value": "1) The paper is not sufficiently novel. I'm not working in this domain, but the only novel part that the authors state is creating the collage of the patches in the feature / latent space based on their spatial embeddings. This does not sound like something that has not been done before in the field of image generation. It would  be helpful if the author come up with a more comprehensive literature survey that provides more related works to this particular  design choice and clearly shows the difference. For example, from a short search I found the following relevant paper: [1] https://arxiv.org/pdf/2207.04316.pdf --  Improving Diffusion Model Efficiency Through Patching \n[2] https://arxiv.org/abs/2304.12526 -- Patch Diffusion: Faster and More Data-Efficient Training of Diffusion Models (note the paper was first submitted on April 2023).\n\n2) Even if we consider the combination of Patch Diffusion in the latent space sufficiently novel, from the analysis in the supplementary material, I find that faces demonstrate usual artifacts around eyes and mouse (and I think that this is happening despite training on a dedicated dataset). Midjourney models generate much better faces. It would be great to understand why the proposed model fails on those."
                },
                "questions": {
                    "value": "1) Can you please add comparison to other techniques in the supplementary? It might be useful to reduce the example to great examples vs. poor examples and provide some discussion on failure modes\n2) In Table 2 - \"We bold the numbers to denote the best numbers in the same category.\" --> can you please explain what you mean by \"the same category\""
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1444/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698818782730,
            "cdate": 1698818782730,
            "tmdate": 1699636072843,
            "mdate": 1699636072843,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "pC8UZtWT6n",
                "forum": "TgSRPRz8cI",
                "replyto": "4YYTO2QybW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1444/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1444/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 7jCc"
                    },
                    "comment": {
                        "value": "We thank reviewer 7jCc for the valuable feedback.\n\n**The paper is not sufficiently novel... does not sound like something that has not been done before in the field of image generation. It would be helpful if the author come up with a more comprehensive literature survey...**\n\nThanks for the suggestion. There have been works using GANs for patch-based image generation, such as COCO-GAN, Infinity-GAN, AnyresGAN which we list them in the related works. To the best of our knowledge, we\u2019re not aware of such works that create the collage of the patches in the feature / latent space based on their spatial embeddings before, especially for diffusion models. We think the GAN-based methods (latent to image)  lack encoder architecture which makes it hard to create the collage of the patch features. Diffusion models have an image-to-image structure and also have an iterative mechanism which are suitable for our design. \n\nCurrently, there are few works working on patch-based diffusion models. [1] does a reshaping operation on the input image which only pushes the dimensions of the height and width to the channels. More specifically, the reshaping operation is from [B C H W] to [B (Cxhxw) H/h W/w]. It\u2019s not a true patch operation as the model still takes the whole image as input.  [2] does a patch operation during the training stage by concatenating another position embedding layer to the input. However, it still requires full-resolution operation during the inference stage. The paper doesn\u2019t conduct experiments on high-resolution datasets, possibly due to this reason. On the other hand, our method performs patch operation in both training and inference stages which enables the high-resolution generation present in the paper. Besides these two, there are also other works applying patch-based diffusion models to specific applications like image restoration under weather conditions[3] and anomaly detection in brain MRI[4]. Both of them utilize a condition diffusion mechanism in which [3] patchifies weather-degraded images to serve as conditions while [4] attempts to recover a patch in the MRI image with the rest of the images as conditions to perform anomaly detection. Both of them are not able to perform image generation.  We\u2019ve also updated the related works part of the manuscript. \n\n[1] Improving Diffusion Model Efficiency Through Patching\\\n[2] Patch Diffusion: Faster and More Data-Efficient Training of Diffusion Models\\\n[3] Restoring Vision in Adverse Weather Conditions With Patch-Based Denoising Diffusion Models\\\n[4] Patched Diffusion Models for Unsupervised Anomaly Detection in Brain MRI\n\n**sufficiently novel... faces demonstrate usual artifacts around eyes and mouse... Midjourney models generate much better faces. It would be great to understand why the proposed model fails on those.**\n\nThanks for pointing it out. We agree there might be some artifacts, though not significant, for the FFHQ1024 results. We think the main reason for this is we are using 64x64 patches to do 1024x1024 image generation, the patch number is 16x16 which might be a little larger. For comparison, COCO-GAN uses a 64x64-patch to train on 256x256 images, InfinityGAN uses 101x101-patch to train on 197x197 images. When training under the same settings, we can see from Table 1 that we outperformed all previous methods. When the setting comes to 64x64-patch on 256x256 training dataset where the patch number is 4x4 we can see the results usually don\u2019t contain such artifacts. Some possible fixes might be to increase the patch size of 64x64 to 128x128/256x256 or add more layers to improve the capability of our model. We leave it for future work to further alleviate those minor artifacts in high-resolution structural datasets. \n\nOn the other hand, midjourney is a large text-to-image diffusion model which trains on a very large dataset (no public technical report so we\u2019re not aware of other details) which might not be a direct comparison."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1444/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700681249765,
                "cdate": 1700681249765,
                "tmdate": 1700697372261,
                "mdate": 1700697372261,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kLad4GRDW1",
                "forum": "TgSRPRz8cI",
                "replyto": "4YYTO2QybW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1444/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1444/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 7jCc (Continued)"
                    },
                    "comment": {
                        "value": "**Can you please add comparison to other techniques in the supplementary? It might be useful to reduce the example to great examples vs. poor examples and provide some discussion on failure modes.**\n\nThanks for the suggestion. We\u2019ve added a section in the revised supplementary material. We select FFHQ1024 as it's a more structural dataset while other landscape datasets are not that structural, thus FFHQ1024 could better show how well the models learn the structural information which is more challenging to generate high-resolution images.  As there lacks results on FFHQ1024 using diffusion models, we compare our FFHQ1024 results with state-of-the-art ones from non-patch GAN-based methods StyleGAN3 and StyleGAN-XL. Our results might not have as good global consistency as those results since we utilize 16x16=256 patches in our case. For example, the wrinkles and eyes/glasses might be  asymmetrical. We think one can further improve this by utilizing a larger patch size or introducing better global-consistency-enforcing mechanisms which we leave for future work.\n\n\n**In Table 2 - \"We bold the numbers to denote the best numbers in the same category.\" --> can you please explain what you mean by \"the same category\"**\n\nIn Table 2, we divide the methods into non-patch-based (top) and patch-based (bottom). We\u2019ve updated the manuscript to make it more clear."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1444/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700681294118,
                "cdate": 1700681294118,
                "tmdate": 1700697383011,
                "mdate": 1700697383011,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]