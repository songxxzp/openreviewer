[
    {
        "title": "A Bayesian Framework for Clustered Federated Learning"
    },
    {
        "review": {
            "id": "HXo5dFuI66",
            "forum": "SqNi6Se1NT",
            "replyto": "SqNi6Se1NT",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5548/Reviewer_n2mn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5548/Reviewer_n2mn"
            ],
            "content": {
                "summary": {
                    "value": "This paper considers the problem of clustered federated learning. Inspired by a Bayesian modeling of clustered FL, the provides three clustering heuristics. The paper evaluates the proposed heuristics and shows that in practice they outperform  WeCFL, which is the state-of-the-art FL clustering method."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The proposed clustering heuristics exhibit superior performance compared to the current state-of-the-art WeCFL in federated learning.\n- The experimental results section is well-crafted, providing a thorough evaluation of the proposed methods.\n- BCFL distinguishes itself from other clustering federated learning methods by dynamically adjusting clusters, offering a unique approach that departs from hierarchical clustering strategies."
                },
                "weaknesses": {
                    "value": "- The paper's overall clarity is compromised due to unnecessarily complex and at times undefined notation, making it challenging to follow.\n- The update mechanism for the parameters of the Bayesian model lacks explicit explanation, leaving a crucial aspect of the methodology unclear.\n- The comparison and discussion of related work are superficial, lacking a fundamental exploration of how the proposed method differs from other clustering approaches. \n- The claim that the paper provides a \"unified framework with a rigorous formulation\" and offers \"new theoretical insights and algorithms\" is disputed, as the paper fails to present any novel theoretical contributions.\n- Section 3 contains derivations that are deemed obvious, contributing minimally to the understanding of the proposed approach.\n- The content in Sections 3 and 4.1 communicates a message that, in my interpretation, appears rather straightforward. The outlined two-step iteration involves computing the \"optimal\" association $\\theta^{*}_{t}$ given cluster weights $\\Omega$ and updating cluster weights based on the current association rule. Furthermore, the solution presented in equation (16) suggests associating a client $j$ with a cluster $i$ that maximizes $\\log(D^j|w^i)$. While these concepts are essential, the simplicity of the presented message does not seem commensurate with the extensive coverage given to these sections (four pages). The detailed explanation does not justify the space allocated, and as such, Sections 3 and 4.1 may be perceived as overextended for the relatively straightforward content they provide. \n- In light of the preceding remark, it might be beneficial for the paper to discuss soft clustering approaches based on the EM algorithm.\n- The proposed methods lack theoretical guarantees, relying solely on heuristics without a solid theoretical foundation.\n- The success of the proposed method heavily depends on the higher computational cost of BCFL-MH, raising concerns about the practicality and efficiency of the cheaper variants (BCFL-G and BCFL-C). In fact,  BCFL-G and BCFL-C do not show a significant improvement over FedAvg and WeCFL; the improvement never exceeds $1$ p.p., and is often lower then $0.1$ p.p.   \n- Several minor issues, including inconsistent citation style, grammatical errors, and unclear notation choices, need attention for a more polished presentation: \n     - The paper seems to use the wrong citation style. It refers to the authors when it should refer to the paper. For example, the sentence \"FL allows collaborative model training without data sharing across clients, thus preserving their privacy McMahan et al. (2017)\" should be \"FL allows collaborative model training without data sharing across clients, thus preserving their privacy (McMahan et al., 2017). \"\n    - In Section 1, \"the is a lack\" -> \"there is a lack\". \n    - What is the reason behind using $\\mathbb{P}$ instead of $P$ in page 3?\n    -In Page 4, \"to denote the a particular\" -> \"to denote a particular\".\n     - In Page 7, \"both feature- and label- skew\" -> \"both feature---and label---skew\""
                },
                "questions": {
                    "value": "- My understanding of the paper is that each iteration is split into two steps: compute the \"optimal\" association $\\theta^{*}_{t}$ given the weights of the clusters $\\Omega$, then, update the cluster weights given the current association rule. Could you please confirm or deny my interpretation?\n- Regarding (13), it seems for me that the optimal solution would pick $A^{i j} = 1$, for $i \\in \\text{arg}\\min_{i} L^{i, j}$. It translates in (16), to  associating the client $j$ to the cluster $i$ that justifies the best its data, i.e. the cluster $i$ such that $log(D^j|w^i)$ is maximal. Could you please confirm or deny my claim?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5548/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698859821850,
            "cdate": 1698859821850,
            "tmdate": 1699636569929,
            "mdate": 1699636569929,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "SrO3cBNv0h",
                "forum": "SqNi6Se1NT",
                "replyto": "HXo5dFuI66",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5548/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5548/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to weak points 1~7"
                    },
                    "comment": {
                        "value": "We thank the reviewer and we reply to the points raised by the reviewer.\n- The notation complexity is due to the indexing of the multiple association hypotheses, being random finite sets themselves, which is at the core of the conceptual solution of the association problem brought by clustering. It would be helpful if the reviewer could point out notation issues so we could address them. Notice that there is a summary table with notation in the appendix and that we are revising it to make sure relevant variables are defined.\n- At every communication round we use the Laplace approximation approach as discussed in \"Baseline models and system settings\" in Section 5.\n- We agree with the reviewer that the discussion presented in the related work section could be improved. We are working on improving the discussion for the revised manuscript.\n- We strongly disagree with the reviewer on this point. We formulated the problem of clustered federated learning as a Bayesian data association problem between clients and model parameter distribution. The theoretical approach presented in Section 3 is completely novel in the FL literature: section 3.1 presents the conceptual formulation and solution to compute the joint posterior of model parameters and data association; section 3.2 extends 3.1 to recursive Bayesian updates and associations, which are implemented as FL iterations and yield to an optimal yet intractable solution as pointed out in section 3.3. Furthermore, the derivations are far from trivial, they are rigorous in the sense that they rely on consolidated Random Finite Set and Bayesian theories with few assumptions, no approximations or heuristics. The proposed approach is a unified framework since the theory proposed in Section 3 is general and can lead to multiple solutions as those in Section 4. The proposed conceptual solution, however, leads to a high number of data associations and, consequently, to high computational complexity if applied naively. Nevertheless, the insights provided by the theory laid out in Section 3 were used to propose the approximations discussed in Section 4 which successfully aim at reducing the complexity through novel algorithmic solutions. We modified the title of Section 3 as \"Optimal Bayesian solution for clustered federated learning\" to make it clear that the material there is already part of the novel contribution of this paper, which we agree was not clear earlier when titled \"Problem formulation\". Overall, we believe that the proposed approach touches all the points we claimed. We hope that the reviewer can reconsider such a strong claim that the paper fails to present any novel theoretical contribution.\n- We disagree, see our previous answer.\n- The reviewer is mistaken. Section 3 and 4.1 present different pieces of the proposed BCFL: while section 3 presents the conceptual solution where all association hypotheses are kept, section 4.1 presents the basics to weight different hypotheses and design approximate solutions to the computational challenge. The solution of equation (16) leads to the set of data associations that maximizes the sum, over all $K$ clusters and all clients in each cluster, of the $\\log$ unnormalized marginal posterior of data association weights $\\tilde{\\pi}^{\\theta^{ij}_t|h}$, where $h\\triangleq \\theta\\_{1:t-1}$ indicates the history of past associations as discussed below equation (11). \nIn the case where Equation (9) is approximated using the expected value of the model parameters then $\\tilde{\\pi}^{\\theta^{ij}\\_t|h} = p(\\mathcal{D}^j|\\omega^i\\_{t-1})$. Nevertheless, only in the greedy approach discussed in Section 4.2 the selection is made solely on the $\\log p(D^j|\\omega^i\\_{t-1})$.\nWe hope that the reviewer can revisit this point and change the overall evaluation of the manuscript.\n- We will add these references to the related work section discussing it connects to BCFL. Note that a FedSoft is already cited in our work.  In our paper, we focused on the clustered FL problem as stated in (Ghosh et al., 2020; Mansour et al., 2020), where the objective is to learn multiple models under the assumption that data from each client comes from a specific distribution.  \nOur approach focuses in clustering the different clients using a data association Bayesian framework that provides a joint posterior distribution of client associations and model parameters. Opposed to existing methods, we consider multiple data association hypotheses that enable the uncertainty quantification of client-to-cluster associations, leading to noticeable performance improvement.\nIn EM-based FL approaches the assumption is that the data at each client belongs to a mixture of distributions. Then, FL approaches are used to independently train multiple models under this assumption, which does not explore different association hypotheses. Furthermore, FedEM still relies on frequentist estimation strategies which contrasts with our Bayesian framework."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5548/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700098153268,
                "cdate": 1700098153268,
                "tmdate": 1700098153268,
                "mdate": 1700098153268,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "dsoVaNZ8kf",
            "forum": "SqNi6Se1NT",
            "replyto": "SqNi6Se1NT",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5548/Reviewer_Yz5m"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5548/Reviewer_Yz5m"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a clustered FL method based on a bayesian framework which assigns clients to clusters, and provides three different variations of the proposed bayesian clustered FL framework to address practical considerations, namely approximate BCFL, greedy BCFL, and consensus BCFL. Specifically, for $K$ clusters and $C$ clients, BCFL assigns each client to its optimal cluster based on a target posterior characterization. The work performs preliminary experiments to validate the BCFL's performance on CIFAR10 and FMNIST."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The work proposes many different variants of BCFL that considers practicality.\n- The work investigates clustering in FL dependent on the different data distributions of the clients which is a relevant topic in FL where data heterogeneity can be particularly severe. \n- The work includes details of their experimental results including graphs that show the relationship across clients and their relatedness with the results."
                },
                "weaknesses": {
                    "value": "- A major concern I have is regarding the efficacy and practicality of the proposed framework. Although the authors have proposed the three variants of the proposed BCFL framework, they still require the downloading of the models and weights under past associations, and then again uploading the associations of the weights to get the association decisions again from the server. Then finally the clients upload the local models based on the conditional associations. This requires at least 2 times the communication rounds compared to the conventional FL framework as well as more computation imposed to the clients. I became more skeptical after looking at the experimental results which only include 10 clients in total or 8 clients in total for more cross-device like datasets such as digits-dive or amazon review. Another concern regarding this approach of BCFL is the sensitivity of the number of clusters $K$ to the performance. It will be difficult to know this value in advance in practice. How does the authors address this problem as well? Overall due to these issues I am skeptical of the efficacy and practicality of the proposed framework for realistic FL settings. \n\n- Another concern I had regarding the practical variant approximate BCFL, the authors assume that there is no overlap in the distribution across different client partitions. This is quite a strong assumption which does not hold in most of the cases in FL. Can the authors comment on this assumption and how realistic it is?\n\n- The writing of the paper can be improved. for instance in pg1 when the authors address problem 1 and problem 2 in bold, there seems to be a typo/error. Ex: Problems2: The is a lack of a united theory. Moreover, optimality is used throughout the paper from the beginning without a proper explanation on what the authors exactly mean by this. It can mean differently for different readers. In addition, the presentation of Figure 3 can be improved, It is quite hard to see the differences across the curves.\n\nDue to these concerns, I am leaning towards rejection for the work."
                },
                "questions": {
                    "value": "See weaknesses above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5548/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699214604991,
            "cdate": 1699214604991,
            "tmdate": 1699636569847,
            "mdate": 1699636569847,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "CJw6prRkEP",
                "forum": "SqNi6Se1NT",
                "replyto": "dsoVaNZ8kf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5548/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5548/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the appreciation of the strengths of our work and the feedback provided. Below we reply point by point to the weaknesses perceived by the reviewer.\n-  There are three comments within the first item. Namely: \n1) It is worth clarifying that downloading the model for all past hypotheses is only the case for the conceptual solution discussed in Section 3 of the paper. However, the approximate solutions proposed in Section 4 are aimed to reduce those communication needs and thus to improve the practicality of BCFL. Other things to consider are that i) the weight associations variable sent along the model weights are just a few scalar values, thus requiring minimal extra communication cost; and ii) this paper tackles the additional communication cost by way of the proposed approaches that reduce the need to send an otherwise growing number of hypotheses. \n2) We also conducted tests involving a larger number of clients and have presented additional examples with 40 clients in the appendix, we will emphasize this in the updated paper. In the main body, we included only a limited number of clients for the Digit5 and Amazon Review datasets to ensure the clarity of our cluster analysis results, as done in other related works. As depicted in Figure 4, the behavior of the clusters during training is distinctly observable. \n3) Regarding the knowledge of the number of clusters $K$, we preliminary followed common practice in CFL works where this is often predetermined. We agree that estimating $K$ is of interest and it is actually on our future research plan, which our Bayesian framework can account for although not straightforwardly enough to be included in this paper so we instead decided to keep it fixed (although not necessarily correctly specified). To address this comment we updated the appendix with an experiment where the sensitivity to under- and over-estimating $K$ is discussed.\n- We would like to clarify that the main assumptions in the paper are that given an hypothesis the model parameters per cluster are independent and that the local datasets are conditionally independent. The intuition behind the validity of those assumptions comes from the conditioning on the clustering hypothesis, whereby if we were to explore all of the hypotheses we would find that different clusters obey different models. We agree that this might not be always true, that is the reason behind explicitly stating it as an assumption. Doing so, we enable tractability of the problem, which otherwise would be extremely challenging. Finally, it is worth noting that other non-Bayesian CFL works implicitly make this assumption, although since those works are often not probabilistic this might not be as apparent as in our framework.\n- Thanks for pointing out the typos and figure suggestions, they are fixed. As for the use of `optimality', we refer to the fact that the conceptual solution in Section 3 provides the full posterior distribution of the model, where all association hypotheses are explored. That is, BCFL is optimal in the Bayesian sense, where the objective is the computation of the joint posterior distribution of the model parameters and the clustering hypotheses. Optimality of the algorithms is thus referring to a Bayesian inference notion of optimality, which is now mentioned in the first paragraph of Section 3. The approximate solutions developed in Section 4 are therefore suboptimal, although they ultimately aim at approximating that same joint posterior while maintaining tractability and practicality of BCFL."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5548/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700095789635,
                "cdate": 1700095789635,
                "tmdate": 1700095789635,
                "mdate": 1700095789635,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]