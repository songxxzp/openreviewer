[
    {
        "title": "Conditional Support Alignment for Domain Adaptation with Label Shift"
    },
    {
        "review": {
            "id": "klRstkihk5",
            "forum": "FJjHQS2DyE",
            "replyto": "FJjHQS2DyE",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4569/Reviewer_o6SV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4569/Reviewer_o6SV"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes Conditional Adversarial Support Alignment (CASA) whose aim is to minimize the Conditional Symmetric Support Divergence (CSSD) between the source\u2019s and target domain\u2019s feature representation distributions, aiming at a more discriminative representation for the classification task. Theoretical analyses are also provide in this work."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The proposed CASA addresses the drawback of Adversarial Support Alignment (ASA) by considering discriminative features to align the supports of two distributions, thus mitigating the risk of conditional distribution misalignment caused by indiscriminate reduction of marginal support divergence.\n2. Theoretical target error bound are provided in this work.\n3. Extensive experiments are conducted to demonstrate the effectiveness of the proposed method."
                },
                "weaknesses": {
                    "value": "1. The major difference between this work and ASA is the conditional alignment, specifically CSSD and SSD. However, the label in the target domain is unknown, and the authors utilize the entropy conditioning technique described in [1] to address this issue. As far as I know, the method in [1] is not specifically designed for generating pseudo-labels. Could the authors please explain how they adapt this method to mitigate the error accumulation problem associated with using pseudo-labels? A detailed explanation from the authors would be appreciated.\n2. More SOTA methods are suggested to discuss and compare, such as SHOT [2], BIWAA [3], CoVi [4], etc.\n\n\n[1] Long, M., Cao, Z., Wang, J., & Jordan, M. I. (2018). Conditional adversarial domain adaptation. Advances in neural information processing systems, 31.\n\n[2] Liang, J., Hu, D., & Feng, J. (2020, November). Do we really need to access the source data? source hypothesis transfer for unsupervised domain adaptation. In International conference on machine learning (pp. 6028-6039). PMLR.\n\n[3] Westfechtel, T., Yeh, H. W., Meng, Q., Mukuta, Y., & Harada, T. (2023). Backprop Induced Feature Weighting for Adversarial Domain Adaptation with Iterative Label Distribution Alignment. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (pp. 392-401).\n\n[4] Na, J., Han, D., Chang, H. J., & Hwang, W. (2022, October). Contrastive vicinal space for unsupervised domain adaptation. In European Conference on Computer Vision (pp. 92-110). Cham: Springer Nature Switzerland."
                },
                "questions": {
                    "value": "see Weakness"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4569/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4569/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4569/Reviewer_o6SV"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4569/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697859199253,
            "cdate": 1697859199253,
            "tmdate": 1699636434762,
            "mdate": 1699636434762,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zlSC9HoTRc",
                "forum": "FJjHQS2DyE",
                "replyto": "klRstkihk5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4569/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4569/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We greatly appreciate the reviewer's positive evaluation and constructive comments and suggestions. In the following, we provide the response to your main comments.\n\n### Explanation for mitigating the accumulative error by using pseudo-labels from CDAN\n\nTheorem 1 motivates us to minimize the conditional support divergence between the source and target domain, in order to minimize the target error risk.\nHowever, as mentioned in our main paper, due to the absence of labels in the target domain, the proposed CASA method resorts to using pseudo-labels on the target domain.\nBased on several works that have pointed out the error accumulation issue in naively adopting pseudo-labels under data distribution shift [], we utilize the entropy conditioning technique proposed in [1] to mitigate this issue.\nIntuitively, we reweight the importance of each sample in Eq. 9 and Eq. 10 by the quality of their pseudo-labels, which is measured by pseudo-labels' entropy $w_{i} = 1 + e^{-H(g(x_{i}))}$ [1].\nSince the prediction's entropy is closely related to model's error under domain shift [2], this reweighting scheme allows the model to prioritize easily transferable samples, and lessen the issue of error accumulation under domain shift.\nOn the other hand, more advanced techniques on generating high-quality pseudo-labels [3][4][5] can be utilized in our model, and potentially help achieve even higher results.\nInvestigating closely the effects of different pseudo-labels methods is outside the scope of our paper, and we leave this exploration to future works.\n\n$$\nL_{d}(\\phi) = - \\frac{1}{n_S}\\sum_{i=1}^{n_{S}} w_{i}^S\\ln \\left[G(x_i^S)\\right]-\\frac{1}{n_T}\\sum_{i=1}^{n_{T}} w_{i}^T \\ln \\left[1-G(x_i^T)\\right]\n$$\n\n$$\nL_{align}(f) = \\frac{1}{n_S}\\sum_{i=1}^{n_{S}} w_{i}^S d\\left(G(x_i^S),\\{G(x_j^T)\\}_{j=1}^{n_T}\\right)\n$$\n\n$$\n+\\frac{1}{n_T}\\sum_{i=1}^{n_{T}}  w_{i}^T d\\left(G(x_i^T),\\{G(x_j^S)\\}_{j=1}^{n_S}\\right)\n$$\n\n\n\n### Comparison with more SOTA methods: SHOT, BIWAA, and CoVi\nPlease refer to the additional experimental results mentioned in our global response. As expected, those new results show that our proposed CASA consistently outperforms others by a large margin.\n\n\n[1] Long et al. Conditional adversarial domain adaptation. Advances in neural information processing systems, 31.\n\n[2] Dequan Wang et al. \u201cTent: Fully test-time adaptation by entropy minimization\u201d. In: arXiv preprint arXiv:2006.10726\n\n[3] Jian Liang, Dapeng Hu, and Jiashi Feng. \u201cDo we really need to ac- cess the source data? source hypothesis transfer for unsupervised domain adaptation\u201d. In: International Conference on Machine Learning. PMLR. 2020, pp. 6028\u20136039.\n\n[4] Hong Liu, Jianmin Wang, and Mingsheng Long. \u201cCycle self-training for domain adaptation\u201d. In: Advances in Neural Information Processing Systems 34 (2021), pp. 22968\u201322981.\n\n[5] Qiming Zhang et al. \u201cCategory anchor-guided unsupervised domain adaptation for semantic segmentation\u201d. In: Advances in Neural Information Processing Systems 32 (2019)"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4569/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700303181134,
                "cdate": 1700303181134,
                "tmdate": 1700303181134,
                "mdate": 1700303181134,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "yyAgI46NjZ",
                "forum": "FJjHQS2DyE",
                "replyto": "klRstkihk5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4569/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4569/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer,\n\nThank you once again for taking the time to read and review our submission. We would be happy to address any remaining questions before the discussion period ends today.\n\nBest regards,\n\nAuthors."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4569/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700625492269,
                "cdate": 1700625492269,
                "tmdate": 1700625583451,
                "mdate": 1700625583451,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Ds43yeDZVv",
                "forum": "FJjHQS2DyE",
                "replyto": "yyAgI46NjZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4569/Reviewer_o6SV"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4569/Reviewer_o6SV"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for authors' responses."
                    },
                    "comment": {
                        "value": "Thanks for authors' responses. I keep my score."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4569/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700633107251,
                "cdate": 1700633107251,
                "tmdate": 1700633107251,
                "mdate": 1700633107251,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "UkYVOFI2KT",
            "forum": "FJjHQS2DyE",
            "replyto": "FJjHQS2DyE",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4569/Reviewer_UE75"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4569/Reviewer_UE75"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the distribution shift problem for the machine learning model. Specifically, the authors consider the label shift scenario and analyze the limitations in current label shift research, i.e., the strict identical assumption on the conditional distribution $P_{X|Y}$. To address this problem, a novel metric is developed based on the symmetric support divergence (SSD). Mathematically, the proposed metric can be taken as the sliced SSD on each conditional distribution $P_{X|Y=y}$. A new generalization upper bound and some theoretical properties of the proposed metric are provided, which ensure the metric-based model can reduce the generalization error and show the relation between marginal SSD and conditional SSD. Experiments are conducted to show the superiority of the proposed method."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ A conditional variant of SSD and corresponding theoretical analysis are provided.\n+ A discrepancy optimization model is proposed to address the domain adaptation with label shift.\n+ Superior experiment results are achieved."
                },
                "weaknesses": {
                    "value": "- The basic problem in this paper is indeed equivalent to the generalized target/label shift, where label distribution and conditional distribution change simultaneously. However, many important and closely related references are not introduced and discussed.\n- Consider the existing results for generalized target/label shift, the generalization error analysis provided in Thm. 1 seems to be less compact and not informative.\n- Important theoretical results for the main merits, i.e., conditional variant of SSD, are missing, which makes the proposed method less technically sound.\n- The organization and clarity should be improved. Some justification and intuition for the math definition or theoretical results are insufficient.\n- The experiment comparison is insufficient, where some related works are omitted."
                },
                "questions": {
                    "value": "1. The essential setting and problem that are considered in this submission is indeed similar to the well-known generalized target/label shift [a-f], which are not properly introduced and discussed. Besides, the label shift problem is also extensively studied and has shown promising theoretical results in many studies. From both the generalized target/label shift view and label shift view, this paper does not provide sufficient discussion with these existing methodological and theoretical results. Thus, it is hard to evaluate this paper's contributions, making this work less persuasive.\n\n2. In the generalized target/label shift literature [e, f], generalization bounds and theoretical analysis are also provided. Compared with these results that compactly decompose the shift on the joint distribution as the terms determined by label discrepancy and conditional discrepancy, this paper induces additional constants, i.e., joint error on both domains and the non-negative constants $\\delta, \\gamma$ induced by IMD.\n\n3. Considering the existing results, the main contribution in this paper is the new conditional discrepancy metric. However, it seems that it cannot be rigorously considered as the class-wise IMD. Specifically, note for the IMD in Def. 3, the weights of the two expected divergence terms are 1; however, in the conditional variant in Def. 4, the divergence terms are weighted by the label probability masses $P(Y=y)$. In such a definition, it naturally raises an crucial question, i.e., is the conditional SSD in Def.4 defines a metric on conditional distribution? This theoretical property is the foundation for the proposed method and should be treated rigorously.\n\n4. The justifications of the derived theoretical results should be improved. Though Thm. 1 ensures that the generalized label shift correction is sufficient to mitigate the label discrepancy and conditional discrepancy, the constants induced in upper-bound seem to be intractable.\n\n5. The discussion in Remark 3 is insufficient and seems to be improper. The advantages of existing results [e] are not properly stated, i.e., literature [e] does not induce additional constant that cannot be controlled by the learning model. Besides, the related works [d,f] are not discussed and compared.\n\n6. Since there are many related works in correcting label shift and conditional shift simultaneously [a-f], they should also be carefully compared in experiment validation. \n\n[a] Zhang, Kun, et al. \"Domain adaptation under target and conditional shift.\" International conference on machine learning. PMLR, 2013.\n\n[b] Gong, Mingming, et al. \"Domain adaptation with conditional transferable components.\" International conference on machine learning. PMLR, 2016.\n\n[c] Ren, Chuan-Xian, Xiao-Lin Xu, and Hong Yan. \"Generalized conditional domain adaptation: A causal perspective with low-rank translators.\" IEEE transactions on cybernetics 50.2 (2018): 821-834.\n\n[d] Rakotomamonjy, Alain, et al. \"Optimal transport for conditional domain matching and label shift.\" Machine Learning (2022): 1-20.\n\n[e] Tachet des Combes, Remi, et al. \"Domain adaptation with conditional distribution matching and generalized label shift.\" Advances in Neural Information Processing Systems 33 (2020): 19276-19289.\n\n[f] Kirchmeyer, Matthieu, et al. \"Mapping conditional distributions for domain adaptation under generalized target shift.\" International Conference on Learning Representations. 2022."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4569/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4569/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4569/Reviewer_UE75"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4569/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698254612378,
            "cdate": 1698254612378,
            "tmdate": 1700573999010,
            "mdate": 1700573999010,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fEbzoeiD74",
                "forum": "FJjHQS2DyE",
                "replyto": "UkYVOFI2KT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4569/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4569/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We greatly appreciate the reviewer\u2019s valuable comments and constructive suggestions. We address the main concerns as follows.\n\n### Comparison to other relevant theoretical works\nWe acknowledge the reviewer's concern over the lack of comparison with existing theoretical results on the generalized label shift problem. Our paper previously only focused on comparing the derived theoretical results with the existing literature on adversarial domain adaptation [3] [4], and adversarial support alignment [1] in section 2.3. To provide a more comprehensive treatment of existing approaches to  domain adaptation with generalized target shift, we discuss the result of Theorem 1 and other relevant theoretical results in Section B of the updated Appendix. Thus, we refer the reviewer to this section in the revised paper for more detailed discussion.\n\n\n### Tractability of the induced constants in Theorem 1\nWe agree with the reviewer that those constants might be computationally intractable. Nevertheless, in the theoretical development of our method, we assume the existence of those constants to obtain the exact theoretical upper bound of the target error. In fact, those induced constants remain unchanged in the training process of our CASA, whose main goal is to reduce the divergence term $D_{supp}^c(P^S_{Z|Y},P^T_{Z|Y})$. In particular, on the additional induced quantity $\\delta$, the appearance of additional term is due to the mild assumption ${f \\in F}_{\\epsilon}$ where $\\epsilon$ be greater than 0. In fact, if we impose a more stringent assumption by setting $\\epsilon$ to be precisely $0$ (indicating models making perfect predictions on labeled source data), these quantities will vanish. Please refer our updated Appendix for further details\n\n### Is conditional SSD in Def.4 a metric on conditional distribution?\n\nThe conditional symmetric support divergence (CSSD) $D_{supp}^c(P^S_{Z|Y},P^T_{Z|Y})$ is not a proper metric on conditional distribution; instead it serves a support divergence on conditional distribution. This is similar to [1] where their proposed symmetric support divergence is similarly categorized as a support divergence. The definition of support divergence is closely related to Chamfer divergence [1][2], which has been shown to not be a valid metric, since it does not satisfy the triangle inequality. In response to reviewer's suggestion, we have added Proposition 1 and the additional analysis to this on in the updated Appendix. Our approach distinguishes itself by targeting the alignment of supports in class-wise conditional distributions rather than those in margin distributions. Intuitively, two distributions $P_{Z|Y}, Q_{Z|Y}$ with the same label space can have $D^c_{supp}(P_{Z|Y}, Q_{Z|Y}) = 0$, if and only if their conditional supports are equal, i.e. $supp(P_{Z|Y=y}) = supp(Q_{Z|Y=y}) \\forall y$.\n\n*We sincerely hope that you can reconsider the review score. Please let us know if there are further things you would like us to address.*\n\n*Best regards,*\n\n*Authors*\n\n[1] Shangyuan Tong et al. \u201cAdversarial Support Alignment\u201d. In: arXiv preprint arXiv:2203.08908 (2022)\n\n[2] Haoqiang Fan et al. A point set generation network for 3d object reconstruction from a single image. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 605\u2013613, 2017\n\n[3] Shai Ben-David et al. \u201cAnalysis of representations for domain adaptation\u201d. In: Advances in neural information processing systems 19 (2006)\n\n[4] David Acuna et al. \u201cf-domain adversarial learning: Theory and algorithms\u201d. In: International Conference on Machine Learning. PMLR. 2021, pp. 66\u201375."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4569/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700302261779,
                "cdate": 1700302261779,
                "tmdate": 1700302261779,
                "mdate": 1700302261779,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "k4P15vzb6R",
                "forum": "FJjHQS2DyE",
                "replyto": "fEbzoeiD74",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4569/Reviewer_UE75"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4569/Reviewer_UE75"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the detailed responses."
                    },
                    "comment": {
                        "value": "I want to thank the authors for their comprehensive responses, where the questions on theoretical results are addressed and the existing results are properly compared. After checking the revision, I think the technical novelty of the paper is clear, which mainly focuses on the theoretical advances in *the support divergence* and shows merits compared with existing works. Therefore, I will improve the score to 6 for acceptance."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4569/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700573973881,
                "cdate": 1700573973881,
                "tmdate": 1700573973881,
                "mdate": 1700573973881,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "U6262DyO3l",
            "forum": "FJjHQS2DyE",
            "replyto": "FJjHQS2DyE",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4569/Reviewer_XqJP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4569/Reviewer_XqJP"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed conditional adversarial support alignment (CASA) to minimize the conditional symmetric support divergence between the source\u2019s and target domain\u2019s feature representation distributions. Generally, the paper is well-written and easy to follow. They evaluate the model on several benchmarks from various types of results. However, the model's novelty is incremental."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper proposed conditional adversarial support alignment (CASA) to minimize the conditional symmetric support divergence between the source\u2019s and target domain\u2019s feature representation distributions. Generally, the paper is well-written and easy to follow. They evaluate the model on several benchmarks from various types of results."
                },
                "weaknesses": {
                    "value": "The model's novelty is incremental over multiple loss functions. The alignment loss in Eq(10) is more like pair-wise alignment loss, which has been explored before for cross-domain graph alignment. It is hard to verify the novelty.\n\nFrom the experiments, they show the improvements when \\alpha decreases. However, there is no insight why this happens. It is better to discuss the intuition and data used behind. It needs more visualization to demonstrate the improvement."
                },
                "questions": {
                    "value": "The novelty clarification.\nThe performance analysis."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4569/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698809286649,
            "cdate": 1698809286649,
            "tmdate": 1699636434584,
            "mdate": 1699636434584,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "uAWKuPDK8Y",
                "forum": "FJjHQS2DyE",
                "replyto": "U6262DyO3l",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4569/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4569/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We greatly appreciate the reviewer\u2019s valuable comments and constructive suggestions. We address the main concerns as follows.\n\n### Novelty clarification\n\nWe would like to emphasize that our main contribution includes a novel conditional SSD-based domain adaptation bound in section 2.3. The main motivation for our proposed method is to utilize discriminative features during adversarial domain alignment to improve performance under different levels of label distribution shift. Based on the novel target error bound in section 2.3, we then propose the corresponding training scheme that empirically yields superior results on benchmark datasets under the setting of label distribution shift. We provide a detailed comparison of our proposed error bound to other relevant theoretical results on the problem of unsupervised domain adaptation, in Remark 3, and in our response to **Reviewer UE75**. The corresponding loss terms in Eq. 7, 8, and 10 are directly motivated by our proposed bound in Theorem 1. Furthermore, we provide an analysis of these three loss terms in section 3.3 and confirm the merits of optimizing each of these loss terms in our ablation study.\n\n### Similarity between CSSD and pair-wise graph alignment\n\nWe thank the reviewer for this suggestion and would appreciate it if the reviewer could point out which specific work on cross-domain graph alignment he/she is referring to. However, as pointed out in [1], the support alignment loss is closely related to the Chamfer divergence, which also has been used extensively in 3D point cloud modeling [4, 5] and learning document embedding [6]. Different from these works, ours is the first work that proposes using the conditional support divergence within the problem domain of domain adaptation with generalized target shift. Furthermore, we have provided a comprehensive discussion of our novel alignment loss in section 2.3, and the novel optimization scheme of this loss term in section 2.4.3 in our main paper.\n\n### More performance analysis\n\nWe thank the reviewer for the suggestion of clarifying the competitive performance of CASA over severe label shift. However, we would like to point out that, in fact, CASA's performance does not improve as alpha decreases, as **Reviewer XqJP** has claimed. In particular, Tables 1-3 show that more severe levels of label shift also degrade CASA's performance mildly on all of the benchmark datasets. More importantly, Tables 1-3 show that CASA performs competitively with other baselines under alpha=\\{None, 10.0\\}, and CASA attains the highest results under other smaller values of alphas. There are several reasons for this phenomenon. Since the objective of minimizing CSSD does not significantly degrade the model's performance under large label shifts, as we already discussed in Remark 3 and in our response to **Reviewer UE75**, CASA is more robust to large label distribution shifts. On the other hand, as support divergence is an extremely relaxed form of distribution divergence [1], aligning the supports of source and target features may not perform as well as aligning these distributions directly when there is no or mild label shift. We have visualized the feature distributions and studied the performance of CASA, in comparison to 2 other baselines CDAN and ASA in Figure 3 and Section 3.3. Compared to CDAN, the Wasserstein distance between source and target features of CASA is much larger, at a value of 0.65. This helps explain the superior performance of CASA over CDAN and other similar distribution-alignment methods under severe label shift of small alpha values [2,3].\n\nWe really hope that you can reconsider the review score. Please let us know if you would like us to do anything else.\n\nBest regards,\n\nAuthors\n\n\n\n[1] Shangyuan Tong et al. \u201cAdversarial Support Alignment\u201d. In: arXiv preprint arXiv:2203.08908 (2022)\n\n[2] Zhao et al. On learning invariant\nrepresentations for domain adaptation. In International Conference on Machine Learning, pp.7523\u20137532. PMLR, 2019\n\n[3] Johansson et al. Support and invertibility in domain-invariant representations. In The 22nd International Conference on Artificial Intelligence and Statistics, pp. 527\u2013536. PMLR, 2019.\n\n[4] Haoqiang Fan et al. A point set generation network for 3d object reconstruction from a single image. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 605\u2013613, 2017\n\n[5] Trung Nguyen et al. \u201cPoint-set distances for learning representations of 3d point clouds\u201d. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021, pp. 10478\u201310487\n\n[6] Matt Kusner et al. From word embeddings to document distances. In International conference on machine learning, pp. 957\u2013966. PMLR, 2015"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4569/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700298773681,
                "cdate": 1700298773681,
                "tmdate": 1700298773681,
                "mdate": 1700298773681,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YxT5vQCvF7",
                "forum": "FJjHQS2DyE",
                "replyto": "U6262DyO3l",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4569/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4569/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer,\n\nThank you once again for taking the time to read and review our submission. We would be happy to address any remaining questions before the discussion period ends today.\n\nBest regards,\n\nAuthors."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4569/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700625456574,
                "cdate": 1700625456574,
                "tmdate": 1700625567099,
                "mdate": 1700625567099,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]