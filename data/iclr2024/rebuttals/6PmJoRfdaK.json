[
    {
        "title": "LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models"
    },
    {
        "review": {
            "id": "nxauXgEcUM",
            "forum": "6PmJoRfdaK",
            "replyto": "6PmJoRfdaK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1602/Reviewer_WV76"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1602/Reviewer_WV76"
            ],
            "content": {
                "summary": {
                    "value": "this paper proposes some computationally efficient methods to continue finetuning a pretrained model to support longer context. The paper proposed a modification for localized attention to support longer context by shifting the subgroups during finetuning. The paper also experimented with LoRA for long-context adaptation."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. the paper is well written and easy to follow. the proposed approach is a simple method that can adapt LLM for longer context without too much compute.\n2. the paper has good ablation to show that LoRA on embedding and normalization is important for long-context adaptation."
                },
                "weaknesses": {
                    "value": "1. the paper only evaluated on retrieval and perplexity. It would be good to evaluate on other generative tasks that require longer context.\n2. the improvement on perplexity doesn't seem super consistent in Table. 4"
                },
                "questions": {
                    "value": "1. Have you tried evaluating on any generative tasks?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1602/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698421222287,
            "cdate": 1698421222287,
            "tmdate": 1699636088576,
            "mdate": 1699636088576,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KpNriD4skn",
                "forum": "6PmJoRfdaK",
                "replyto": "nxauXgEcUM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1602/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1602/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer WV76"
                    },
                    "comment": {
                        "value": "We are truly appreciated for your valuable comments. In the following, we provide responses to the concerns.\n\n**Q1: \u201cOther generative tasks that require longer context.\u201d**\n\nA: Thanks for your suggestion! We have included the evaluation on LongBench[1] and L-Eval [2], as shown in Table 9 and Table 10 in the revision. We fine-tuned Llama2 7B model with our long QA data. In these benchmarks, there are comprehensive generative tasks, including document QA, summarization, few-shot learning, code completion synthetic tasks, and other open-ended tasks in L-Eval. Our model presents comparable or even better performance than other counterparts, with about 4 hours for fine-tuning on 8 A100 GPUs. It takes 60 million tokens per epoch, 5 epochs, and 0.3 billion tokens in total for the supervised fine-tuning.\n\nTable 1 - Evaluation on LongBench English tasks.\n\n| Model | Avg | Single-Doc QA | Multi-Doc QA | Summarization | Few-shot Learning | Code | Synthetic |\n| --- | --- | --- | --- | --- | --- | --- | --- |\n| GPT-3.5-Turbo | 44.0 | 39.8 | 38.7 | 26.5 | 67.1 | 54.1 | 37.8 |\n| Llama2-7B-chat | 31.0 | 24.9 | 22.6 | 24.7 | 60.0 | 48.1 | 5.9 |\n| LongChat-v1.5-7B | 34.3 | 28.7 | 20.6 | 26.7 | 60.0 | 54.1 | 15.8 |\n| Vicuna-v1.5-7B | 31.9 | 28.0 | 18.6 | 26.0 | 66.2 | 47.3 | 5.5 |\n| Ours-7B | 36.8 | 28.7 | 28.1 | 27.8 | 63.7 | 56.0 | 16.7 |\n\nTable 2 - Evaluation on L-Eval open-ended tasks, i.e., comparing models to GPT-3.5-Turbo and judging win rates via GPT-4.\n\n| Model | Win-rate | Wins | Ties |\n| --- | --- | --- | --- |\n| LongChat-7B | 33.68 | 36 | 56 |\n| LongChat-v1.5-7B | 33.59 | 38 | 53 |\n| Vicuna-v1.5-7B | 25.52 | 22 | 54 |\n| Ours-7B | 39.06 | 45 | 60 |\n\n**Q2: \u201cThe improvement on perplexity in Table 4.\u201d**\n\nA: Sorry for the confusion caused. The original Table 4 (Table 3 in the revision) is not designed to show perplexity improvements, which we never claim. The goal of LongLoRA is to significantly improve the training efficiency (in terms of both memory consumption and training speed) without compromising the perplexity achieved through finetuning the full model. We have clarified this point in the caption of Table 3 In the revision. \n\n[1] Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, Juanzi Li: LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding. CoRR abs/2308.14508 (2023)\n\n[2] Chenxin An, Shansan Gong, Ming Zhong, Mukai Li, Jun Zhang, Lingpeng Kong,  Xipeng Qiu: L-Eval: Instituting Standardized Evaluation for Long Context Language Models. CoRR abs/2307.11088 (2023)"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1602/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700327850173,
                "cdate": 1700327850173,
                "tmdate": 1700377501828,
                "mdate": 1700377501828,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Ke4BKU68Fr",
            "forum": "6PmJoRfdaK",
            "replyto": "6PmJoRfdaK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1602/Reviewer_Sw6W"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1602/Reviewer_Sw6W"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a method to perform LLM context extension with less memory and wall-clock time than existing methods. Their main modifications to improve efficiency are (1) training on local rather than global attention using the shift-short attention pattern, (2) using LoRA, and (3) modifying the norm and embedding layers in addition to the self-attention and feed-forward layers. The resulting method performs similarly to full fine-tuning."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "(1) The method seems useful and impactful, and the evaluation is thorough with strong results.\n\n(2) The authors perform very thorough ablations and isolate key design decisions (attention shift, modifying the norm & embedding layers) that enable the method to match full fine-tuning.\n\n(3) The paper is well-written."
                },
                "weaknesses": {
                    "value": "No major weaknesses."
                },
                "questions": {
                    "value": "(1) While this is somewhat outside the scope of this paper, I would be curious about comparisons to methods that involve training a long-context LM from scratch.\n\n(2) I am a bit confused why regular LoRA and LoRA+ (Table 11) use the same amount of memory. Does S^2-Attn reduce memory usage as well, or only flops?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1602/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1602/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1602/Reviewer_Sw6W"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1602/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698806894993,
            "cdate": 1698806894993,
            "tmdate": 1699636088481,
            "mdate": 1699636088481,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "QZTSJ6bA0X",
                "forum": "6PmJoRfdaK",
                "replyto": "Ke4BKU68Fr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1602/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1602/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Sw6W"
                    },
                    "comment": {
                        "value": "We are truly appreciated for your valuable comments. In the following, we provide responses to the concerns.\n\n**Q1: \u201cComparisons to methods that train long-context from scratch.\u201d**\n\nA: Thanks for this question. We agree with you that training long-context from scratch could be an interesting direction. Nevertheless, training long-context LLMs from scratch is too computationally expensive and unaffordable for most researchers. LongLoRA provides an efficiency advantage of orders of magnitude compared to training long-context LLMs from scratch. For instance, Llama-2 models require training with 2 trillion tokens across hundreds of GPUs, whereas LongLoRA models are finetuned on about 2 billion tokens for 32k context length using just 8 A100 GPUs.\n\nRecently, Meta published a concurrent research paper, LLama2-Long [1]. In this paper, the authors empirically verified that long-context continual training on short-context models is more efficient and similarly effective compared to pretraining from scratch with long sequences. This conclusion could further magnify the importance of LongLoRA.\n\n**Q2: \u201cRegular LoRA and LongLoRA use the same amount of memory in Table 11. $S^2$-Attn for memory, or only \ufb02ops?\u201d**\n\nA: The reason why $S^2$-Attn does not reduce the memory usage on top of LoRA (LoRA+) in the original Table 11 is that we adopt FlashAttention-2 [2] for the implementation of self-attention layers. This library fuses all operators in multi-head self-attention (MHSA) into a single CUDA kernel and thereby avoids writing the attention score matrix to DRAM. So the memory space required for MHSA is around $2 * B * N * C$, where B is the batch size, N is the sequence length and C is the number of channels. The multiplier 2 corresponds to input and output activations. \n\nNevertheless, if FlashAttention-2 is not used, the vanilla dense attention requires $2 * B * N * C + B * N^2 * C$ memory space. Here, $B * N^2 * C$ corresponds to attention scores. In comparison, our $S^2$-Attn requires only $2 * B * N * C + 4 * B * (N/4)^2 * C = 2 * B * N * C + 1/4 * B * N^2 * C$ memory space. Empirically, we include an additional comparison that disables the FlashAttention-2 in the table below. Under a context length of 8192, $S^2$-Attn improves the training speed by 2.1x and memory usage by 1.8x. With a 16k context length, the original dense attention runs out of memory during training while $S^2$-Attn is still feasible. We have included this comparison in the Table 13 of the revision.\n\n| $S^2$-Attn | Train hours (8192) | Memory - GB (8192) | Train hours (16384) | Memory - GB (16384) |\n| --- | --- | --- | --- | --- |\n| x | 17.5 | 55.5 | OOM | OOM |\n| \u221a | 8.2 | 30.3 | 20.8 | 57.1 |\n\n[1] Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz, Madian Khabsa, Han Fang, Yashar Mehdad, Sharan Narang, Kshitiz Malik, Angela Fan, Shruti Bhosale, Sergey Edunov, Mike Lewis, Sinong Wang, Hao Ma: Effective Long-Context Scaling of Foundation Models. CoRR abs/2309.16039 (2023)\n\n[2] Tri Dao: FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning. CoRR abs/2307.08691 (2023)"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1602/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700327709934,
                "cdate": 1700327709934,
                "tmdate": 1700377371374,
                "mdate": 1700377371374,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "G9BK4W87t6",
                "forum": "6PmJoRfdaK",
                "replyto": "QZTSJ6bA0X",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1602/Reviewer_Sw6W"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1602/Reviewer_Sw6W"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the great answers to my questions."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1602/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700713427609,
                "cdate": 1700713427609,
                "tmdate": 1700713427609,
                "mdate": 1700713427609,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "LhmjbCk8CO",
            "forum": "6PmJoRfdaK",
            "replyto": "6PmJoRfdaK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1602/Reviewer_FsJg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1602/Reviewer_FsJg"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a novel approach to extend the context length of transformer-based language models. The approach consists of two main ideas: 1) split the context into smaller subgroups and conduct attention in each group individually; 2) adapt the model to make use of this new attention approach via parameter-efficient fine-tuning with LoRA.\n\nThe authors conduct experiments with the Llama2 model family using models with 7B, 13B, and 70B parameters and compare their newly proposed approach to several baselines. In terms of perplexity, their proposed approach is able to maintain performance even when extending the context size by a factor of 16. \n\nBeyond language modelling, the authors evaluate their method in a retrieval setup (finding a hidden key in a long sequence of text), demonstrating its improved performance over baselines."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The proposed method builds on previous work and shows strong empirical results on long lange language modelling and a retrieval task\n- The proposed approach is conceptually simple and can be implemented in a few lines of code (as demonstrated by the authors)\n- The proposed approach can be combined with existing approaches for context extension such as positional interpolation \n- The authors provide a detailed discussion of related work"
                },
                "weaknesses": {
                    "value": "- The efficiency aspect of the could could be more prominently discussed in the main body of the paper\n- The presentation of the work could be improved. See below for suggestions"
                },
                "questions": {
                    "value": "**Presentation**\n\n- I had difficulties understanding Figure 3. It would help if you add indices and annotations to the matrices in this plot. Additionally, it could be helpful to draw a (visual) connection between the blue matrices on the left and the attention patterns on the right. \n- Be more consistent about the usage of $S^2$, shift short attention, LoRA+, and LongLoRA. Make it more explicit that LongLoRA = $S^2$ attention + LoRA.\n- Table 7 is a great candidate for a line plot. \n- When pointing to results in the Appendix, make sure to reference a specific section in the Appendix. \n- The \"attention patterns\" ablation feels repetitive. How is it different from the \"consistency to full attention\" discussion in Section 3.2?\n- In the section on retrieval-based evaluation you mention that your model is \"somehow\" able to handle longer context. What does this mean?\n\n**Experiments**\n\n- You mention several times that the original standard self-attention can be retained at inference time. It would be helpful to provide more details on that. Also, Table 2 is mentioned as evidence for that. It would be helpful to elaborate more about the results in this table. \n- Table 3: What about an additional baseline that trains LoRA + embeddings?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1602/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1602/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1602/Reviewer_FsJg"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1602/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698841716709,
            "cdate": 1698841716709,
            "tmdate": 1700657825452,
            "mdate": 1700657825452,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8O206QCha7",
                "forum": "6PmJoRfdaK",
                "replyto": "LhmjbCk8CO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1602/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1602/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer FsJg"
                    },
                    "comment": {
                        "value": "We are truly appreciated for your valuable comments. In the following, we provide responses to the concerns.\n\n**Q1: About Figure 3**\n\nA: Thanks for your constructive suggestion. We have updated Figure 3 in the revision, with indices and annotations to the matrices, and the visual connections between matrices and attention patterns.\n\n**Q2: \u201cBe more consistent about $S^2$-Attn, shift short attention, LoRA+, and LongLoRA.\u201d**\n\nA: We have updated these terms to be more consistent in the revision. We make more explicit that LongLoRA is the combination of $S^2$-Attn and LoRA+ in the abstract. \n\n**Q3: \u201cTable 7 is a great candidate for a line plot.\u201d**\n\nA: We have transformed the original Table 7 into a line plot. Please refer to Figure 5 in the revision.\n\n**Q4: \u201cReference a speci\ufb01c section in the Appendix\u201d**\n\nA: In the revision, we have added specific section numbers in the appendix when we reference them in the paper.\n\n**Q5: \u201cThe attention patterns ablation feels repetitive to Section 3.2\u201d** \n\nA: Thanks for the reminder! To avoid repetition, we have relocated the original Table 2 to Table 6 (in Section 4.2) and shortened relevant descriptions in Section 3.2. In Section 3.2, the objective is to show that we can \u201ctrain sparse, test dense\u201d with $S^2$-Attn. In Section 4.2, we further demonstrate that $S^2$-Attn achieves superior performance to existing sparse attention patterns.\n\n**Q6: \u201cRetrieval-based evaluation - able to handle longer context.\u201d**\n\nA: Sorry for the confusion caused. We have removed the word \u201csomehow\u201d in the section you mentioned. Retrieval-based evaluation shows that LongLoRA extends the context window of a pre-trained LLM. Specifically, Llama-2-7b sharply fails to retrieve the passkey when the context window exceeds 4k, while our method maintains a high retrieval accuracy (60-90%) even at a much longer context length of 33k-45k. \n\n**Q7: \u201cThe original standard self-attention at inference time.\u201d**\n\nA: In Table 6 of the revision (the original Table 2), for each attention pattern, we evaluate its performance under two protocols. In the first row, we use sparse attention in both training and testing. In the second row, we use sparse attention only for training and the standard full attention for testing. For our $S^2$-Attn, it achieves the best perplexity with the original full self-attention at inference time. We have elaborated on these details in the table caption in the revision.\n\n**Q8: \u201cAn additional baseline that trains LoRA + embeddings.\u201d**\n\nA: Thanks for your reminder. We have conducted this ablation and included it in Table 2 in the revision. Finetuning the embedding layer brings larger benefits than normalization layers. \n\n| Embed | x | \u221a | \u221a |\n| --- | --- | --- | --- |\n| Norm | \u221a | x | \u221a |\n| PPL | 10.49 | 8.29 | 8.12 |"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1602/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700327027594,
                "cdate": 1700327027594,
                "tmdate": 1700377349369,
                "mdate": 1700377349369,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "z9ua7t5zCa",
                "forum": "6PmJoRfdaK",
                "replyto": "8O206QCha7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1602/Reviewer_FsJg"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1602/Reviewer_FsJg"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the update; raised score"
                    },
                    "comment": {
                        "value": "I thank the authors for their detailed answers to my questions/suggestions. \n\nIn the light of the author response to my questions, the other reviews, and the authors' response to these, I decided to increase my score from 6 to 8."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1602/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700657977544,
                "cdate": 1700657977544,
                "tmdate": 1700657977544,
                "mdate": 1700657977544,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "yHUD09zCLP",
            "forum": "6PmJoRfdaK",
            "replyto": "6PmJoRfdaK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1602/Reviewer_7K1u"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1602/Reviewer_7K1u"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a new method for adapting pretrained large language models (LLMs) to longer sequence lengths with a focus on efficiency. Prior works are either costly, due to requiring full fine-tuning of the language model or loose performance. The authors show that combining Low Rank Adaptation (LoRA) with sparse local attention provides improves efficiency while preserving performance. For LoRA, the authors note that a simple and cheap modification to LoRA, un-freezing embedding and normalization layer parameters, can prevent LoRA from loosing performance as sequence lengths increases. For sparse local attention, they employ a simple heuristic of splitting attention into independent groups of 2048 tokens. By overlapping groups within each layer at different attention heads, they ensure information flow between groups and are able to preserve performance at a level close to the much costlier full attention."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The authors propose an extremely simple method, that performs well and is applicable to existing pretrained models"
                },
                "weaknesses": {
                    "value": "- The authors only evaluate perplexity and retrieval setting"
                },
                "questions": {
                    "value": "- Have you done experiments / ablation on optimal group size for different target sequence lengths? It seems you have derived that setting the group size to 25% of target sequence length is reasonable for 8192 sequence length, but it is unclear whether this 25% heuristic or a constant group size translates to longer sequence lengths.\n- There are multiple ways to estimate model flops. Please provide the method / formula you used for Table 10."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1602/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1602/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1602/Reviewer_7K1u"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1602/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699714310194,
            "cdate": 1699714310194,
            "tmdate": 1699714310194,
            "mdate": 1699714310194,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "OC4quJ7JoB",
                "forum": "6PmJoRfdaK",
                "replyto": "yHUD09zCLP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1602/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1602/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 7K1u"
                    },
                    "comment": {
                        "value": "We are truly appreciated for your valuable comments. In the following, we provide responses to the concerns.\n\n**Q1: \u201cEvaluation only on perplexity and retrieval setting.\u201d**\n\nA: We include additional comparisons on LongBench [1] and L-Eval [2] benchmarks. We fine-tune Llama2 7B with our method on our long QA data. We compare our model with GPT-3.5-Turbo and other Llama2-based long-context models, Vicuna and LongChat models, in the tables below. It shows that our 7B model presents comparable or even better performance than these Llama2-based long-context models, while ours only takes about 4 hours, about 0.3 billion tokens, for supervised fine-tuning on a single 8x A100 machine. We have included these evaluations in Table 9 and Table 10 in the revision.\n\nTable 1 - Evaluation on LongBench English tasks\n\n| Model | Avg | Single-Doc QA | Multi-Doc QA | Summarization | Few-shot Learning | Code | Synthetic |\n| --- | --- | --- | --- | --- | --- | --- | --- |\n| GPT-3.5-Turbo | 44.0 | 39.8 | 38.7 | 26.5 | 67.1 | 54.1 | 37.8 |\n| Llama2-7B-chat | 31.0 | 24.9 | 22.6 | 24.7 | 60.0 | 48.1 | 5.9 |\n| LongChat-v1.5-7B | 34.3 | 28.7 | 20.6 | 26.7 | 60.0 | 54.1 | 15.8 |\n| Vicuna-v1.5-7B | 31.9 | 28.0 | 18.6 | 26.0 | 66.2 | 47.3 | 5.5 |\n| Ours-7B | 36.8 | 28.7 | 28.1 | 27.8 | 63.7 | 56.0 | 16.7 |\n\nTable 2 - Evaluation on L-Eval open-ended tasks, i.e., comparing models to GPT-3.5-Turbo and judging win rates via GPT-4\n\n| Model | Win-rate | Wins | Ties |\n| --- | --- | --- | --- |\n| LongChat-7B | 33.68 | 36 | 56 |\n| LongChat-v1.5-7B | 33.59 | 38 | 53 |\n| Vicuna-v1.5-7B | 25.52 | 22 | 54 |\n| Ours-7B | 39.06 | 45 | 60 |\n\n**Q2: \u201cAblation on group size for longer sequence lengths\u201d**\n\nA: Thanks for your suggestion! We perform additional ablation experiments on group size using a context length of 16384, as depicted in the table below. The results indicate that setting the group size to 1/4 of the context length is optimal in terms of efficiency-accuracy tradeoff. We have included these results in Table 7 in the revision.\n\n| Context Length | Full | 1/2 | 1/4 | 1/6 | 1/8 |\n| --- | --- | --- | --- | --- | --- |\n| 8192 | 8.02 | 8.04 | 8.04 | 8.10 | 8.16 |\n| 16384 | 7.82 | 7.84 | 7.86 | 7.94 | 7.98 |\n\n**Q3: \u201cThe method to estimate model FLOPs.\u201d**\n\nA: We profile the context stage FLOPs of Llama2-7B using a batch size of 1 and various context lengths using a third-party tool, torchprofile [3]. The tool traces the computation graph and sums up the FLOPs of each node in the graph (e.g. Q/K/V/O projections, multi-head self-attention, fully-connected layers, and normalization layers). \n\n[1] Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, Juanzi Li: LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding. CoRR abs/2308.14508 (2023)\n\n[2] Chenxin An, Shansan Gong, Ming Zhong, Mukai Li, Jun Zhang, Lingpeng Kong,  Xipeng Qiu: L-Eval: Instituting Standardized Evaluation for Long Context Language Models. CoRR abs/2307.11088 (2023)\n\n[3] torchprofile. https://github.com/zhijian-liu/torchprofile"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1602/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700326810672,
                "cdate": 1700326810672,
                "tmdate": 1700377324062,
                "mdate": 1700377324062,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]