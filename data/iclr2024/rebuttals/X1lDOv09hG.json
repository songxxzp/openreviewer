[
    {
        "title": "High variance score function estimates help diffusion models generalize"
    },
    {
        "review": {
            "id": "5iY9MpO3nF",
            "forum": "X1lDOv09hG",
            "replyto": "X1lDOv09hG",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8893/Reviewer_dwfL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8893/Reviewer_dwfL"
            ],
            "content": {
                "summary": {
                    "value": "The papers attempts the answer the question why diffusion models generalize well beyond the training dataset. To address this question, the paper studies linear score estimator and derives the optimal solution. Furthermore, the authors study the covariance of the optimal parameters. Some examples are given to illustrate this idea."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The authors study the linear score estimator class. The closed-form optimal solution to DSM is obtained.\n2. The paper further investigates the covariance of the parameters in the score estimator. The authors find that the phenomenon of high variance."
                },
                "weaknesses": {
                    "value": "1. The mathematical derivation in the paper looks like heuristics instead of rigorous proof. It is hard to tell the correctness of the arguments. \n2. No experiments are provided to justify the high variance arguments.\n3. Although the starting point of the paper is on the generalization of diffusion models, it is unclear how the high variance of the score estimators helps model generalization.\n4. The writing can be improved."
                },
                "questions": {
                    "value": "1. The paper only considers the linear score estimator and derives the optimal closed-form solution. How do you know the ground truth score function is linear? For a general score function, can we still have high variance parameters?\n2. The most important issue with this paper is that I cannot tell how the high variance is connected to the generalization of diffusion models. \n3. Can you provide some numerical experiments to support your claims in the paper?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8893/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8893/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8893/Reviewer_dwfL"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8893/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698561044658,
            "cdate": 1698561044658,
            "tmdate": 1699637118966,
            "mdate": 1699637118966,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "mSqlBVeVk8",
                "forum": "X1lDOv09hG",
                "replyto": "5iY9MpO3nF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8893/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8893/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for reading the paper and offering comments. Hopefully the new version of the paper (to be posted before AOE Nov 22) addresses your concerns. See below for detailed responses:\n\n**Weaknesses:**\n\n> 1. The mathematical derivation in the paper looks like heuristics instead of rigorous proof. It is hard to tell the correctness of the arguments.\n\nWe agree that this was a weakness of the original version, and the mathematical content has been substantially reorganized (and made more rigorous) to fit a more standard theorem/proof structure. The original result included in the paper has also been substantially generalized to remove two hypotheses; the result no longer requires a gaussian mixture training distribution or that score function parameters are learned separately for different times. \n\n> 2. No experiments are provided to justify the high variance arguments.\n\nNew experiments have been done to justify the high variance arguments (although we still refer heavily to previous work). Two important pieces of empirical evidence are that (i) the score function learned by models is usually not the same as the \"empirical\" score of the data set, and (ii) models trained using what we call the \"naive\" objective exhibit more memorization.\n\n> 3. Although the starting point of the paper is on the generalization of diffusion models, it is unclear how the high variance of the score estimators helps model generalization.\n\nWe agree that this could have been better explained in the original version. Two new sections have been added to the newest version to remedy this: one presenting some intuition about generalization, and another discussing concrete examples of how the additional variance discussed in the text implements an inductive bias that encourages interpolation (filling in gaps between training data), extrapolation (extending patterns in the training data), and feature blending (generating samples which include both feature $X$ and feature $Y$ even when training examples only involve one of the two features).\n\n> 4. The writing can be improved.\n\nThe aforementioned new sections have been added, and the discussion was clarified and reorganized in various places in order to try to remedy this issue.\n\n**Questions:**\n\n> 1. The paper only considers the linear score estimator and derives the optimal closed-form solution. How do you know the ground truth score function is linear? For a general score function, can we still have high variance parameters?\n\nIt is important to note that the score estimators considered in this paper are linear in *features*, not linear in *states*. This means that a variety of arbitrarily expressive function approximators (e.g., a Fourier basis, Gaussian basis functions, orthogonal polynomials) are permitted. This point of confusion has been clarified in the text. \n\nAnother case of interest is that of neural networks in the neural tangent kernel (NTK) regime. These are effectively linear, and the main result of the paper holds for them also. (A brief discussion of this has been added to the paper.) So additional effective variance due to noisy parameter estimates is a phenomenon that appears to happen for a fairly large class of models, although for reasons of mathematical tractability we are unable to discuss all possible models (e.g., neural networks in the \"rich\" learning regime) here.\n\n> 2. The most important issue with this paper is that I cannot tell how the high variance is connected to the generalization of diffusion models.\n\nSee above answer. Two new sections have been added to discuss this explicitly.\n\n> 3. Can you provide some numerical experiments to support your claims in the paper?\n\nVarious numerical experiments have been added in order to both (i) convince the reader that the high variance phenomenon is real, and (ii) exhibit the inductive biases it produces."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8893/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700687212392,
                "cdate": 1700687212392,
                "tmdate": 1700687212392,
                "mdate": 1700687212392,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0WOoQYGaV0",
                "forum": "X1lDOv09hG",
                "replyto": "mSqlBVeVk8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8893/Reviewer_dwfL"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8893/Reviewer_dwfL"
                ],
                "content": {
                    "title": {
                        "value": "Response to the rebuttal"
                    },
                    "comment": {
                        "value": "Thanks for your response. I appreciate your efforts on the revision. Most of my questions are well-addressed. However, I still have some minor comments:\n\n1. It is fine to argue that the feature map is nonlinear in the state but the NTK should not be covered. Although NTK is essentially a linear model, its mathematical form differs from the classical random feature models.\n2. It would be better to highlight your main changes in the revision. It's still hard to tell your revision in the updated manuscript.\n\nBased on this situation, I do believe my original evaluation is fair."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8893/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700716671157,
                "cdate": 1700716671157,
                "tmdate": 1700716671157,
                "mdate": 1700716671157,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "qp83joUlSP",
            "forum": "X1lDOv09hG",
            "replyto": "X1lDOv09hG",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8893/Reviewer_TEfw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8893/Reviewer_TEfw"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the effect of high variance of of score function estimates at early times. The key idea is to identify the score estimates by running the backward diffusion with the optimal score and convolving with some kernel function -- in other words diffusion models are similar to some kernel density estimation."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper tries to explain mathematically the effect of high variance of score estimation at early times in diffusion models -- which was observed empirically. The key takeaway is to identify the diffusion training by kernel density problems. This connection appears to be novel and insightful. The paper is generally well written, and I mostly enjoyed reading it. I have checked most computations, and they seem to be correct."
                },
                "weaknesses": {
                    "value": "The main concern is that the paper uses some simple diffusion models (1)-(2), as well as the Gaussian mixture training distribution. This limits the applicability of the results. For instance, there are more advanced diffusion models, e.g. VE, VP, sub-VP... What happens in these cases? Is it possible to identify all theses diffusion models by a suitable kernel density problem? The authors may want to comment or explain.\n\nAlso it is not clear whether the \"theoretical\" computations can carry over to other distributions than Gaussian mixture (or a mixture of delta mass). The authors may want to comment on this."
                },
                "questions": {
                    "value": "See the weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8893/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698712430246,
            "cdate": 1698712430246,
            "tmdate": 1699637118843,
            "mdate": 1699637118843,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "XO0yzRNG3x",
                "forum": "X1lDOv09hG",
                "replyto": "qp83joUlSP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8893/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8893/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for reading the paper and offering comments. Hopefully the new version of the paper (to be posted before AOE Nov 22) addresses your concerns. See below for detailed responses:\n\n**Weaknesses:**\n\n> The main concern is that the paper uses some simple diffusion models (1)-(2), as well as the Gaussian mixture training distribution. This limits the applicability of the results. For instance, there are more advanced diffusion models, e.g. VE, VP, sub-VP... What happens in these cases? Is it possible to identify all theses diffusion models by a suitable kernel density problem? The authors may want to comment or explain.\n\nA substantial amount of effort has been put into generalizing the results. In the newest version, two assumptions are no longer required: the training distribution need not be a Gaussian mixture, and score estimator parameters need not be learned independently for different times. \n\nThe score estimator is still assumed to be linear in a set of feature maps (which are now allowed to be both state- and time-dependent), but this is actually a fairly general class of estimators and includes many arbitrarily expressive function approximators (e.g., a Fourier basis, Gaussian basis functions, orthogonal polynomials). Importantly, the key assumption is that the score is linear in these features, not linear in *states*. This is a point of confusion that has been clarified in the text.\n\nOne case which is somewhat realistic that is covered now is the case of neural networks in the neural tangent kernel (NTK) regime. Because these are (to very good approximation, in the infinite width limit) linear in their parameters, they are now covered by the main result of the newest version. This takes the main result closer to SOTA models, although obviously there is a ways to go (e.g., what about neural networks in the so-called \"rich\" learning regime?). We still think this is a substantial theoretical contribution that could serve as a foundation for future work to generalize in various ways.\n\nVP, VE, and sub-VP models are all covered by the main result, and some discussion of their differences has been added to the main text. We agree that this is a practically relevant case worthy of explicit discussion.\n\nYes, all cases can be identified with a suitable kernel density problem. One nuance is that, in the new (more general) result, where feature maps are allowed to be time-dependent, the result generically includes an effective noise term in the probability flow ODE (as opposed to just a convolution at the end; see below). \n\n\n> Also it is not clear whether the \"theoretical\" computations can carry over to other distributions than Gaussian mixture (or a mixture of delta mass). The authors may want to comment on this.\n\nThis assumption has been removed in the newest version, and turns out to be completely unimportant.\n\n------------------\n\nFor completeness' sake, here is the main theorem in the newest version:\n\n**Theorem 1.** [**Linear score estimators trained via DSM asymptotically generalize**] Suppose that the parameters of a linear score estimator are optimized according to the DSM objective using $N$ independent samples from $\\lambda_0(t) p(\\mathbf{x}_0) p(\\mathbf{x}_t | \\mathbf{x}_0, t)$. Consider the result of reverse diffusion using this estimator by Euler-integrating the probability flow ODE with a small time step $\\Delta t$. If $N \\to \\infty$ and $\\Delta t \\to 0$ with $N \\Delta t = c$ held constant, then sampling from $\\mathbb{E}[ q(\\mathbf{x}_0 | \\mathbf{x}_T, \\boldsymbol{\\theta}) ]$ is equivalent to simulating the backwards-time (Ito-interpreted) SDE\n\n$$\\dot{\\mathbf{x}}_ t = -\\beta_ t \\mathbf{x}_ t - \\frac{1}{2} {g_ t}^2 \\mathbf{s}_ *(\\mathbf{x}_ t, t) + \\boldsymbol{\\xi}(\\mathbf{x}_ t, t) $$\n\nfrom $t = t_{max}$ to $t = 0$ with initial condition $\\mathbf{x}(t_{max}) = \\mathbf{x}_T$. The noise term $\\boldsymbol{\\xi}(\\mathbf{x}_t, t)$ is generically correlated across different times, and has\n\n$$\\text{Cov}_ {t, t', \\mathbf{x}_ t | t, \\mathbf{x}_ {t'} | t'}[ {\\xi}_ {i} (\\mathbf{x}_ t, t), {\\xi}_ {j} (\\mathbf{x}_ {t'}, t') ] = V_ {ij} (\\mathbf{x}_ {t}, \\mathbf{x}_ {t'}, t, t')$$\n\nwhere we define the $D \\times D$  \"V kernel\" $\\mathbf{V}$ via\n$$\nV_ {ij} := \\frac{\\delta_ {ij}}{{g_ 0}^2 Z_ {\\sigma} c} \\  \\left( \\frac{{g_ {t}}^2}{2 \\sigma_ t} \\cdot \\frac{{g_ {t'}}^2}{2 \\sigma_ {t'}} \\right)  \\ \\boldsymbol{\\phi}(\\mathbf{x}_ t, t)^T \\bar{\\mathbf{K}}^{-1} \\bar{\\mathbf{K}}(0) \\bar{\\mathbf{K}}^{-1} \\boldsymbol{\\phi}(\\mathbf{x}_ {t'}, t') .\n$$"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8893/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700689158136,
                "cdate": 1700689158136,
                "tmdate": 1700689158136,
                "mdate": 1700689158136,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "GdXwoE7y5y",
            "forum": "X1lDOv09hG",
            "replyto": "X1lDOv09hG",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8893/Reviewer_oZCe"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8893/Reviewer_oZCe"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes to study the generalization ability of diffusion models: why diffusion models do not simply remember the training set but can generate new samples. The paper argues that a key factor is the high variance score function estimates at small $t$. The paper studies the behavior of diffusion models with a specific setting where they are parametrized as a linear function of features, and shows that they learn a distribution that is mathematically equivalent to convolving the optimal distribution with a particular kernel function."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The issue the paper tries to study is an important one. While diffusion models are being deployed quite literally everywhere, it is paramount that we understand what makes them generalize."
                },
                "weaknesses": {
                    "value": "1. My biggest complaint about the paper is that I am not convinced by the paper's argument of the high variance score function estimates at small $t$, which also serves as the paper's motivation.\n    - I am not sure why in Eq.6, we are looking at the covariance over $x_t$, since the network is current-position-aware (the network has $x_t$ and $t$ as input). The network prediction targets are often regularized to have a unit norm [1], so the network predicted scale stays the same across $t$, which is not considered in the paper. Also, people have tried to directly predict $x$ through a different parameterization, which will make the prediction targets very stable at small $t$. This would invalidate the paper's analysis, and we still do not see perfect memorization happen.\n    - It makes more sense to consider the variance explored in [2], which is over $p_{x_0|x_t}$. It is also observed there that the variance of score function estimates is actually small with small $t$. In fact, in [2], it is observed that explicitly minimizing the variance of the score function estimation often leads to better results.\n    - The denoised score matching objective is always trying to fit the injected random noise. The ground truth score (marginalized over all samples) is very different only with moderate t. I agree that fitting small $t$ score functions is difficult as mentioned in prior works, but that is a different issue.\n2. The paper's analysis heavily relies on a particular parameterization (not used in practice), which in my opinion, is the cause of the observed phenomenon, instead of the denoising score matching loss as the authors suggest. What if the model can actually learn the distribution perfectly? Will the analysis in this paper still hold? Equivalently, if the network is trained with the \"naive\" approach, does the model actually learn the distribution perfectly? (with all the training details corrected, like sampling distribution of $\\sigma$ and network prediction target normalization) If not, then I am not sure how much of a contribution this paper is to the field, as the particular setting considered in this paper is impractical, and does not translate very well to the real usage of diffusion models.\n3. In the introduction, the paper rules out many candidates for generalization, purely based on prior works, intuitions, and observations. In order to make these claims, careful study is required. For example, the authors mention that modern architectures are flexible enough to in principle learn the optimal score. The authors need to back up the claims carefully, because that implies that you can perfectly reconstruct the dataset with these networks, which does not happen.\n4. Since the issue considered in this paper is really an empirical one (because the optimal solution to diffusion model training is memorizing all training samples), I highly suggest the authors do some experiments. Related issue: the presentation can be significantly improved with figures. The authors should have more than enough space in the current version.\n\n[1] Karras et al. \"Elucidating the design space of diffusion-based generative models.\" NeurIPS 2022.\n\n[2] Xu et al. \"Stable target field for reduced variance score estimation in diffusion models.\" ICLR 2023."
                },
                "questions": {
                    "value": "1. Where is Eq.8 used in practice? Or, equivalently, which diffusion models used in practice actually underweight small $t$ during training? I see the authors also cite [1], which used a log-normal distribution, and they basically train with more samples at small $t$ compared to large $t$. This fact directly counters one of the assumptions listed in the last paragraph in Sec.3: \u201cthe choice of a time sampling distribution that underweights small times\u2026\u201d\n\n[1] Karras et al. \"Elucidating the design space of diffusion-based generative models.\" NeurIPS 2022."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8893/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8893/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8893/Reviewer_oZCe"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8893/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698779451253,
            "cdate": 1698779451253,
            "tmdate": 1699637118716,
            "mdate": 1699637118716,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7TtCBo5t7j",
                "forum": "X1lDOv09hG",
                "replyto": "GdXwoE7y5y",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8893/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8893/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for reading the paper and offering comments. Hopefully the new version of the paper (to be posted before AOE Nov 22) addresses your concerns. See below for detailed responses:\n\n**Weaknesses**\n\n> My biggest complaint about the paper is that I am not convinced by the paper's argument of the high variance score function estimates at small $t$, which also serves as the paper's motivation. I am not sure why in Eq.6, we are looking at the covariance over \n$x_t$, since the network is current-position-aware (the network has and $x_t$ and $t$ as input). \n\nThis brief discussion is just meant to provide some intuition about a crucial difference between what we are calling the \"naive\" objective and the DSM objective. What actually matters is the covariance of the target as a function of all variables, i.e., with respect to $p(\\mathbf{x}_ t | \\mathbf{x}_ 0, t) p(\\mathbf{x}_ 0) \\lambda(t)$. Integrating wrt $x_ 0$ doesn't change anything, and integrating with respect to $t$ (assuming $\\lambda(t) \\sim \\sigma_ t^2$) produces the integral of a constant, and hence proportional to $t_{max}$. \n\nOther authors (like Song et al. 2021) have explicitly considered the covariance of the target with respect to $\\mathbf{x}_t$ alone, so I think it's a reasonable thing to look at. Moreover, in the (now more general) central result of the paper, this variance is indeed the key ingredient that creates what we are calling a type of generalization.\n\n> The network prediction targets are often regularized to have a unit norm [1], so the network predicted scale stays the same across $t$, which is not considered in the paper. Also, people have tried to directly predict $x$\n through a different parameterization, which will make the prediction targets very stable at small $t$. This would invalidate the paper's analysis, and we still do not see perfect memorization happen.\n\nI think this is a misunderstanding, although it is a subtle and important one. The situation is somewhat confusing because different authors use different notation, sometimes using the same symbol to mean very different things. A new appendix has been added that discusses this particular point, since it is potentially a major source of confusion.\n\nAdding to the confusion was the previous choice of learning parameters separately for different times. In the newest version, this restriction has been removed, so feature maps are allowed to depend on both state and time in a fairly arbitrary way (with some restriction, like square-integrability with respect to the relevant distributions, and smoothness).\n\nThe situation considered in the current version of the paper is (we will argue) very similar to current SOTA models. The time-sampling distribution used in the main result is\n$$ \\lambda_ *(t) = \\frac{\\sigma_ t^2}{\\int_0^{t_ {max}} \\sigma_ t^2 \\ dt} = \\frac{\\sigma_ t^2}{Z_ {\\sigma}} .$$\nIn the newest version of the paper, we are explicit about scaling the model as $\\mathbf{\\epsilon}_ {\\boldsymbol{\\theta}}(\\mathbf{x}_ t, t) = \\sigma_ t \\hat{\\mathbf{s}}_ {\\mathbf{\\theta}}(\\mathbf{x}_ t, t)$ (i.e., the function approximator $\\mathbf{\\epsilon}$ learns the true score multiplied by $\\sigma_ t$). With this change, the objective becomes\n$$\nJ_{1}(\\boldsymbol{\\theta}) = \\frac{1}{2 Z_ {\\sigma}} \\int \\ \\Vert \\boldsymbol{\\epsilon}_ {\\boldsymbol{\\theta}}(\\alpha_ t \\mathbf{x}_ 0 + \\sigma_ t \\boldsymbol{\\epsilon}, t) - \\boldsymbol{\\epsilon} \\Vert_2^2 \\ \\mathcal{N}(\\boldsymbol{\\epsilon}; \\mathbf{0}, \\mathbf{I}) p(\\mathbf{x}_ 0) \\ d\\boldsymbol{\\epsilon} d\\mathbf{x}_ 0 dt   $$\nThis is exactly the objective used by, e.g., Stable Diffusion (Rombach et al 2022; see Eq. 1) and the PNDM paper (Ho et al 2020; see Eq. 14). It turns out that the way we handled the independent-time case hid this scaling (although it was still implicitly there). Some discussion of the variance normalization is now present in the revised version."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8893/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700695283328,
                "cdate": 1700695283328,
                "tmdate": 1700695283328,
                "mdate": 1700695283328,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "GDjgunK1Q5",
            "forum": "X1lDOv09hG",
            "replyto": "X1lDOv09hG",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8893/Reviewer_V1yR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8893/Reviewer_V1yR"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates the following problem: How do diffusion-based generative modesl generalize beyond the training set? The authors claim that this phenomenon is at least partially explained by the fact that score function estimates have a large varaince. To support this claim, they consider a linear score estimator, where $\\hat{s}_{\\theta}(x_t, t) = W_0(t) + W(t) \\phi(x_t). $ Here, $\\phi$ is a fixed feature map, and $\\theta(t) = (W_0(t), W(t))$ are the parameters to be estimated. To train the estimator, they consider the limit of large number of samples $N \\to \\infty$ and small times bins $\\Delta t \\to 0$. They also assume a specific time sampling distribution $\\lambda(t)$. To learn $\\theta(t_i)$, they use $N \\lambda(t_i) \\Delta t$ samples. They propose to estimate the kernel functions using a sample mean estimator. They show that the learned distribution will not be $q_{\\ast}$, which is derived from the optimal linear estimator. Instead, they prove that the learned distribution will be $q_{\\ast}$ convolved with a specific Gaussian kernel. They apply their results to several machine learning tasks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper attempts to explain the generalization of diffusion models from a novel perspective: The high variance of the score function. Using a linear score function estimator, and assume an appropriate asymptotic regime for the training, they are able to explicitly characterize the distribution that is learned from data. This distribution is obtained with optimal score function convolved with a specific kernel. Their results find applications in many machine learning tasks and contribute to explain the generalization of diffusion models."
                },
                "weaknesses": {
                    "value": "I have some doubts on the asymptotic regime considered in this paper. I feel it is not very efficient to use only data in a small time window to train the score function at that time point. As far as I am concerned, $\\lambda(t)$ in the past papers was introduced to impose weights on the loss function instead of sample splitting. I think the authors should elaborate more on why the sample splitting scheme is a reasonable one. I feel in the main result they are getting variation because they do not have enough sample to train each single score function. \n\nIn addition, I feel using only linear score estimator is a bit restricted, as score function is defined as the gradient of the log density, I would expect that it is in general non-linear. Perhaps the authors can make their results more persuasive by giving several examples that have linear score functions?"
                },
                "questions": {
                    "value": "1. Is there a way to estimate the feature maps $\\phi$ when they are not known a priori? \n2. How accurately can a linear estimator learn the score function in typical situations? Maybe the authors can comment a little bit on that. \n3. If we use a better estimator than taking the sample average to estimate the score function, do we get a better result? Will it hurt or imporve the generalization ability? \n4. If a different sampling distribution is employed, how does the results change? \n5. In practice, the number of samples used for training will be very large, hence $c$ would also be large. In this case, the variance according to the theorem will be small. Do we even expect generalization to happen in such a large-sample situation?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8893/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8893/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8893/Reviewer_V1yR"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8893/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698868899474,
            "cdate": 1698868899474,
            "tmdate": 1699637118611,
            "mdate": 1699637118611,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "tUzYyx8P71",
                "forum": "X1lDOv09hG",
                "replyto": "GDjgunK1Q5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8893/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8893/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for reading the paper and offering comments. Hopefully the new version of the paper (to be posted before AOE Nov 22) addresses your concerns. See below for detailed responses:\n\n**Weaknesses:** \n\n> I have some doubts on the asymptotic regime considered in this paper. I feel it is not very efficient to use only data in a small time window to train the score function at that time point. As far as I am concerned,\u00a0$\\lambda(t)$\u00a0in the past papers was introduced to impose weights on the loss function instead of sample splitting. I think the authors should elaborate more on why the sample splitting scheme is a reasonable one. I feel in the main result they are getting variation because they do not have enough sample to train each single score function.\n\nThis is a good point, and it previously bothered us that we were only able to obtain a result for this (admittedly artificial) case. In the newest version, a substantial amount of work has been put in to generalize this result. Two restrictive hypotheses have been removed from the main result: sample splitting is no longer required (i.e., the feature maps are allowed to depend on time), and the training distribution need not be a Gaussian mixture. Even in the case of time-dependent feature maps, additional variance survives in the limit of many samples (as long as, as in the original result, the number of samples $N$ and the time step $\\Delta t$ used to simulate the probability flow ODE scale in a comparable fashion). We think, with this change, the result is much more interesting.\n\n> In addition, I feel using only linear score estimator is a bit restricted, as score function is defined as the gradient of the log density, I would expect that it is in general non-linear. Perhaps the authors can make their results more persuasive by giving several examples that have linear score functions?\n\nThis is a point of confusion other reviewers have had, so we have clarified it in the text. Our score function estimator need not be linear in *state* (i.e., $x_t$); it only need to be linear in some set of *feature maps*. Indeed, score functions are generally not linear in state (and the score function that *is* linear in state is that of a Gaussian). \n\nThe class of score estimators linear in some set of feature maps is actually quite broad, and includes many arbitrarily expressive function approximators (e.g., a Fourier basis, Gaussian basis functions, orthogonal polynomials). An important example of an estimator which is linear in this sense is a neural network in the neural tangent kernel (NTK) regime, since it is effectively linear in its (learned) weights; a brief discussion of this example has been added to the paper.\n\nSome more explicit examples of different possible feature maps have been added in order to make this point clearer.\n\n> **Questions:**\n\n> 1. Is there a way to estimate the feature maps\u00a0$\\phi$\u00a0when they are not known a priori?\n\nWe consider the problem of choosing 'good' feature maps out of the scope of this paper, since this is extremely similar to the question of which neural network architectures are best for diffusion models (and this is a difficult question for which the answer is not quite known, although it might involve transformers are convolutional neural nets). For our purposes, it suffices to use arbitrary expressive ones (e.g., an NTK neural net, or Fourier basis). Some examples of arbitrarily expressive feature maps have been added to the main text. \n\n> 2. How accurately can a linear estimator learn the score function in typical situations? Maybe the authors can comment a little bit on that. \n\nSee above answers. A variety of arbitrarily expressive feature maps can be used. Some clarification related to this has been added to the text.\n\n> 3. If we use a better estimator than taking the sample average to estimate the score function, do we get a better result? Will it hurt or improve the generalization ability?\n\nSince sample averages are the provably optimal result of (full batch) gradient descent, we have decided to restrict the scope to this case. It is an interesting question for future work what happens when something different is used, e.g., gradient descent using mini-batches. A note of this has been added to the discussion. \n\n\n> 4. If a different sampling distribution is employed, how does the results change?\n\nSome discussion of different time sampling distributions $\\lambda(t)$ (used during training) has been added to the main text. The particular choice of $\\lambda(t)$ considered in this paper (and used in various SOTA models, see e.g. Song et al 2021 and Karras et al 2022) is special because of its small-time behavior; sampling distributions with $\\lambda(dt) \\approx (dt)^n$ for some $n > 1$ do not generalize in the sense described in this paper. Some discussion of this point has been added to the text."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8893/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700690332863,
                "cdate": 1700690332863,
                "tmdate": 1700690332863,
                "mdate": 1700690332863,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]