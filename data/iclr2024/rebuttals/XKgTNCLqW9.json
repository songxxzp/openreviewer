[
    {
        "title": "Identifying Drivers of Predictive Uncertainty using Variance Feature Attribution"
    },
    {
        "review": {
            "id": "OIjmrd4NXw",
            "forum": "XKgTNCLqW9",
            "replyto": "XKgTNCLqW9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8144/Reviewer_3GEh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8144/Reviewer_3GEh"
            ],
            "content": {
                "summary": {
                    "value": "This paper is about explanations for aleatoric uncertainty of the variance output in a regression model trained with the Gaussian NLL. The authors propose to use gradient-based saliency maps to explain the variance head of a regression model under the Gaussian assumption. There are experiments on a synthetic dataset and on age regression.\n\nContributions are:\n- A scalable solution to explain aleatoric uncertainty in a regression model.\n- A method to extend pre-trained regression models to also consider aleatoric uncertainty, by training the variance head.\n- A synthetic toy regression problem with controllable factors for heteroscedastic aleatoric uncertainty prediction, allowing for efficient evaluation.\n- Results on the toy regression problem and age regression."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "- The paper's writing is good and mostly easy to understand.\n- The synthetic benchmark for evaluating aleatoric uncertainty explanations seems to be novel and significant (Sec 2.4). It basically uses Gaussian with a variable variance that has heteroscedastic and homoscedastic noise terms.\n\n(Unfortunately I do not find more strengths in this paper)"
                },
                "weaknesses": {
                    "value": "- I believe that explaining aleatoric uncertainty is not a very interesting problem, while the authors argue about explaining uncertainty, aleatoric uncertainty is just the uncertainty in the data, usually noise in the labels, etc, and this does not have the same impact as epistemic (model) uncertainty, which is usually the kind of uncertainty that is interesting as it provides feedback about the prediction being correct or not.\n\n- There are variations of the Gaussian NLL that have much less problems in optimization, like the beta-Gaussian NLL, etc. These variations are not used in this paper, which decreases the value to the community. Below I provide a reference to the beta-Gaussian NLL paper:\n\nSeitzer, Maximilian, et al. \"On the Pitfalls of Heteroscedastic Uncertainty Estimation with Probabilistic Neural Networks.\" International Conference on Learning Representations. 2021.\n\n- Section 2.2 presents a method to add variance outputs to pr-etrained regression models, while I believe that this is a kind of trivial extension and there are no new ideas here, additionally the authors do not evaluate this proposed method, for example, some basic questions might arise, is it better to pre-train on MSE and later train on Gaussian NLL, or directly train on Gaussian NLL and one of its variations? It is not clear what is the value of this method as there is no evaluation or comparison.\n\n- There is no proper comparison to the state of the art or ablation studies. The synthetic experiments are compared against CLUE, and it seems there is a small qualitative improvement (the paper does not use any quantitative metrics), but there is no comparison for the more interesting age regression experiment, which lowers the value of such comparison. Overall I understand that there is not much state of the art in this sub-field of explaining uncertainty, but then I suggest to perform ablation studies, compare multiple explanation methods, and multiple methods to estimate the variance of the data, to obtain useful insights for future research.\n\n- I am not sure how to interpret the results of the age regression experiment (Figure 4). It is difficult to evaluate and interpret explanations, and I believe it is more difficult to explain uncertainties, the authors make qualitative comparisons among the explanations, which is fine, but how do these explanations relate to the aleatoric uncertainty? In the age regression example, aleatoric uncertainty labels are not available, so it is very difficult to argue that the model is explaining its aleatoric uncertainty. I believe this experiment requires more thinking and a proper experimental design, opposite of the synthetic experiment.\n\nMinor Comments\n- The paper refers to \"Aleatory\" uncertainty in some parts, but the actual technical name is aleatoric uncertainty.\n- In Figure 4, only the saliency maps are presented, I believe there is more information to be presented, like the ground truth age, and predicted age mean and standard deviations, so the user can see how these three values relate to the saliency maps. Just by looking at the saliency maps without the predicted standard deviation is meaningless, as the whole aim of the paper is to explain the aleatoric uncertainty output head."
                },
                "questions": {
                    "value": "- What is the interest of explaining aleatoric uncertainty, as opposed to explain epistemic uncertainty? I believe explanations of epistemic uncertainty are much more useful to end users, so what is the value of a aleatoric uncertainty explanation?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8144/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698613075051,
            "cdate": 1698613075051,
            "tmdate": 1699637009242,
            "mdate": 1699637009242,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zJkE9cmjwE",
                "forum": "XKgTNCLqW9",
                "replyto": "OIjmrd4NXw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8144/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8144/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your thorough review of our submission, and we appreciate the time and effort you invested in providing detailed feedback.\nWe acknowledge your perspective on the interest in explaining epistemic uncertainty as a more robust measure in out-of-distribution scenarios.\nWe, however, think there is great value in explaining aleatoric uncertainty: We agree that aleatoric uncertainty is, as you mention, frequently noise in the labels. Mixture density networks model this noise conditioned on the input vector. We believe that it is crucial to know how a model came to the conclusion that a certain example has more noise in the label than another.\n\nConsider the example of an ML model predicting medical risk that has a higher uncertainty for instances from a certain demographic X even though demographic X is reasonably represented in the training data (as measured by: adding more data from demographic X only marginally decreases the uncertainty of the risk prediction).\nThis aleatoric uncertainty will provide feedback about whether the prediction is correct or not (for demographic X, the model will be more likely to err). The explanation for the high aleatoric uncertainty would be very interesting (socio-economic status? genetic differences? higher median age?) and might lead to ideas on how to refine the model or which additional features to collect. \n\nYour suggestion regarding alternative variations of the Gaussian NLL, such as the beta-Gaussian NLL, is well-received. We previously implemented the beta-Gaussian NLL and ran experiments for our age regression showcase. However, we did not observe any improvements, which is why we forwent it. We will investigate this in more detail for the next versions.\nWe also agree that it is difficult to interpret the results of the age regression experiment, and will provide additional information with the visualization as you proposed. We think it is generally difficult to evaluate explanation approaches without ground truth. That is why we devised the synthetic benchmark pipeline. Thank you for your suggestions in this area. We will devise a more quantitative evaluation strategy for this dataset that will strengthen our contribution."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8144/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700669534670,
                "cdate": 1700669534670,
                "tmdate": 1700669534670,
                "mdate": 1700669534670,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ZCNbvH5Yc8",
            "forum": "XKgTNCLqW9",
            "replyto": "XKgTNCLqW9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8144/Reviewer_H5dd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8144/Reviewer_H5dd"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes an XAI method based on the uncertainty of the prediction in the regression setting. The idea is to use a VAE-type of fitting with the black-box function $f(x)$ being the mean, assuming that all the training samples are available. Using the estimated predictive variance, the authors apply an existing attribution method of the Shapley value. In the computer vision domain, the authors report on interesting results."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Uncertainty quantification in the context of XAI is a relatively new topic.\n- The VAE-type variance estimation is a generic method and widely applicable as long as training data are available at hand."
                },
                "weaknesses": {
                    "value": "- A few important contexts of the related work are missing.  A few recent works in the XAI community clearly point out the importance of uncertainty quantification. The following papers should be cited and discussed at least. \n\t-  Xingyu Zhao, Wei Huang, Xiaowei Huang, Valentin Robu, and David Flynn. 2021. BayLIME: Bayesian local interpretable model-agnostic explanations. In Proceeding of the 37th Conference on Uncertainty in Artificial Intelligence (UAI 21). PMLR, 887\u2013896\n\t- Tsuyoshi Id\u00e9, Naoki Abe: Generative Perturbation Analysis for Probabilistic Black-Box Anomaly Attribution. KDD 2023: 845-856\n- The method is data-hungry. The assumption of the availability of training data may be unrealistic."
                },
                "questions": {
                    "value": "Please comment on the problem setting, where training data are assumed to be available, in light of prior works in the XAI research. Also, please clarify the novelty in light of the existing work, as pointed out above. \n\nI will update my rating depending on your reply."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8144/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698626036288,
            "cdate": 1698626036288,
            "tmdate": 1699637009116,
            "mdate": 1699637009116,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "UxXAEETwDd",
                "forum": "XKgTNCLqW9",
                "replyto": "ZCNbvH5Yc8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8144/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8144/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your detailed review and valuable feedback on our submission. We appreciate the time and effort you've dedicated to evaluating our work. Here are our responses to your points:\nWe appreciate your suggestion to include references to recent works in the XAI community and the field of uncertainty of explanations, specifically the papers by Zhao et al. and Id\u00e9 and Abe. Zhao et al. propose a Bayesian extension to the LIME method with the purpose of including prior knowledge and improving consistency and robustness, which is, therefore, focusing on improving general explainability. Id\u00e9 and Abe are concerned with the problem of anomaly attribution, where an observation strongly deviates from the prediction. Their proposed method allows for quantifying the uncertainty in the detected feature attributions while we explain the features affecting a model\u2019s uncertainty. In future versions, we will make sure to position our work also in the field of uncertainty of explanations that is related to our research focus.\nYour concern about the assumption of the availability of training data is valid since we inherit every limitation of mixture density networks. We recognize that the assumption of sufficient training data may not always hold in practical scenarios.\nThank you once again for your thoughtful review. We look forward to the opportunity to improve our work."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8144/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700669462377,
                "cdate": 1700669462377,
                "tmdate": 1700669462377,
                "mdate": 1700669462377,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "tOSQgXuaFh",
            "forum": "XKgTNCLqW9",
            "replyto": "XKgTNCLqW9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8144/Reviewer_fhD9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8144/Reviewer_fhD9"
            ],
            "content": {
                "summary": {
                    "value": "The authors address the task of identifying drivers of predictive uncertainty. To this end, they follow a 2-step approach. First, they adapt a neural network to a mixture density network with an additional neuron capturing variance. Next, they compute KernelSHAP values on the uncertainty."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The approach addresses an interesting problem explaining the drivers of uncertainty; their approach is very simple and combines 2 well-known paradigms in a straight-froward manner."
                },
                "weaknesses": {
                    "value": "- The main contribution of the paper is to demonstrate that the straight-forward combination of 2 well-known concepts (MDNs and KernelSHAP); for this to be a valuable resource I miss a  comparison to baselines (e.g. Watson et al as cited by the authors) and a systematic _quantitative_ evaluation on a representative number of real-world datasets. The qualitative evaluation on IMDB-clean is promising but does not warrant the strong conclusions of the authors\n- Important literature missing: While the authors mention some  recent work in their discussion of deep heteroscedastic regression, they miss the large body of literature following the introduction of this very model in 1994 as Mixture Density Networks in Chris Bishop's seminal paper (which is not even cited); the generalisation to introduce the variance neuron after training a vanilla network is trivial"
                },
                "questions": {
                    "value": "See above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8144/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698790022257,
            "cdate": 1698790022257,
            "tmdate": 1699637008982,
            "mdate": 1699637008982,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xGKcC6xZSs",
                "forum": "XKgTNCLqW9",
                "replyto": "tOSQgXuaFh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8144/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8144/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for taking the time to review our manuscript. We highly appreciate your feedback and constructive comments. Below are our responses to your points:\nWe understand your concerns about the perceived lack of a robust quantitative evaluation and a comparison to baselines. We plan to incorporate a systematic quantitative evaluation on a more representative set of real-world datasets and include comparisons with relevant baselines to strengthen our contribution for a resubmission.\nWe acknowledge the oversight in not citing Chris Bishop's seminal paper on Mixture \nDensity Networks from 1994 and will make sure to position our work more clearly concerning his and subsequent work in the field.\nWe value your insights, and your feedback will significantly contribute to the enhancement of our work. We are committed to addressing these concerns and providing a more robust and well-supported contribution in future versions. As many SOTA explainability methods such as CLUE rely, e.g., on generative models and require significant compute resources and long runtimes, a comprehensive and fair comparison of several methods is not possible within the revision's timeframe."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8144/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700669421936,
                "cdate": 1700669421936,
                "tmdate": 1700669421936,
                "mdate": 1700669421936,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "NavdJvgzpF",
            "forum": "XKgTNCLqW9",
            "replyto": "XKgTNCLqW9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8144/Reviewer_Dpa4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8144/Reviewer_Dpa4"
            ],
            "content": {
                "summary": {
                    "value": "The paper focuses on the rarely studied subject of explaining uncertainties (versus explaining predictions). This area hasn't gotten a lot of attention because Bayesian approaches to neural networks hasn't been widely embraced (given the difficulties with training them). The authors propose \"variance feature attribution\" an approach to explain aleatoric uncertainty. They adapt a traditional neural network to be suitable by adding a variance output, and fine-tuning pre-trained point estimate models (the point becoming the mean of the distribution) to provide a useful variance estimation. They demonstrate their approach on a synthetic dataset such that the uncertainty can be controlled and compare their approach to explainability against CLUE. The goal is to identify which factors/features contribute to elevated (or reduced) levels of uncertainty. They also demonstrate their approach on a non-synthetic dataset (related to age regression) and show which areas of an image cause increase of uncertainty (marks around the eyes/mouth)."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This is a well written paper that tackles an area that hasn't be given a lot of attention. I found the synthetic results very compelling, especially what was presented in Figure 3. I appreciate how they show that CLUE does explain the features causing uncertainty, but their approach makes the distinction between the features much more pronounced. The demonstration on a real-world dataset gives additional strength to their claims."
                },
                "weaknesses": {
                    "value": "I had issues with Section 2.2, which I will discuss more in the questions. I believe some details were left out (or the authors thought they could be assumed) which would have made the section more explicit and clear. In that section the authors state \"multi-layer regression head\", which seems wrong to me. Is it suppose to be \"multi-label regression head\"? Also, once the additional output is added (to capture variance) and the Gaussian negative log-likelihood is used as a loss function, how many iterations should be performed?"
                },
                "questions": {
                    "value": "1. In section 2.2 do you mean \"multi-label regression head\" instead of \"multi-layer regression head\". If not, could you please elaboriate.\n2. Can you explain why you used a batch size of 176 (it seems odd...well it is even, just uncommon)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8144/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8144/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8144/Reviewer_Dpa4"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8144/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699470099983,
            "cdate": 1699470099983,
            "tmdate": 1699637008786,
            "mdate": 1699637008786,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KuQGTg3xNK",
                "forum": "XKgTNCLqW9",
                "replyto": "NavdJvgzpF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8144/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8144/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your thorough review, insightful feedback, and for pointing out the strength of our approach to addressing the understudied problem of explaining uncertainty. We appreciate your feedback highlighting the need for additional detail in section 2.2. Regarding your comment on the \u201cmulti-layer regression head\u201d: What we meant to convey is that certain models incorporate an MLP serving as a read-out function for a scalar output. In some cases, rather than merely extending the final layer, a second MLP can be introduced to generate the variance output. Consequently, this setup results in the presence of two \u201cmulti-layer regression heads\u201d. We will ensure to clarify this aspect more explicitly in future iterations. \n\nRegarding the choice of a batch size of 176, this decision was made to optimize the GPU utilization within our local compute architecture. It was solely an engineering-based decision aimed at minimizing the training time. \n\nDue to the required time for performing extensions for other reviewer comments, we will not be able to submit a revised version to ICLR. Thanks again for your valuable input that will help improve the presentation of our work."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8144/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700669384259,
                "cdate": 1700669384259,
                "tmdate": 1700669384259,
                "mdate": 1700669384259,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]