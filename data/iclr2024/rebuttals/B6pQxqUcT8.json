[
    {
        "title": "ToolChain*: Efficient Action Space Navigation in Large Language Models with A* Search"
    },
    {
        "review": {
            "id": "w9ZRGgs4Pm",
            "forum": "B6pQxqUcT8",
            "replyto": "B6pQxqUcT8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6910/Reviewer_7Kjr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6910/Reviewer_7Kjr"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors propose a tree-search-based planning framework for Large Language Models (LLMs). Their framework is motivated by A* search in heuristic planning and uses a cost function, which is a sum of (past) accumulated cost and (future) heuristic cost to update their planning tree. The use of simple heuristics also makes it more resource-efficient than concurrent works based on Monte Carlo Tree Search. The authors demonstrate both empirically, and through various ablation studies, how their proposed method generates more feasible plans."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "* Clarity: The paper is well-written and easy to follow. \n* Novelty: The proposed tree-search-based method is novel (see footnote) and uses a simple but effective cost-function design that helps in better decision-making. \n* Significance: The experiments adequately justify the superior performance of their proposed method, both empirically and qualitatively. The use of heuristics for LLM planning is appealing to the LM planning & reasoning community and could be a potential research direction for the future.\n\nFootnote: Note the work \"SayCanPay: Heuristic Planning with Large Language Models using Learnable Domain Knowledge, Hazra, et al., 2023\" which also proposes heuristic planning with LLMs. However, given its recency, I have considered it as a concurrent work, and therefore have not compared them despite their overlapping contributions."
                },
                "weaknesses": {
                    "value": "I did not find any major weaknesses in the paper. However, it would be nice to have some clarifications regarding the proposed heuristic functions (see Questions)."
                },
                "questions": {
                    "value": "1. What do you mean by \"task-specific\" heuristic functions? Are the tasks here based on \"g\"? If so, do you use different memory buffers for each task, and how many trajectories do you need in the memory buffer before it produces a valid score? \n\n2. It seems to me that the future cost heuristic might mislead the planner since it is likely that the same action may appear at different positions (in the sequence) for different tasks. For instance: \"Clean a kitchen table\" and \"Set a kitchen table\" have their action sequences reversed."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6910/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6910/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6910/Reviewer_7Kjr"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6910/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698495627086,
            "cdate": 1698495627086,
            "tmdate": 1699636804037,
            "mdate": 1699636804037,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GNO5Fmwf08",
                "forum": "B6pQxqUcT8",
                "replyto": "w9ZRGgs4Pm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6910/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6910/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Initial Response to Reviewer 7Kjr"
                    },
                    "comment": {
                        "value": "We sincerely appreciate Reviewer 7Kjr for considering our work as novel and significant with empirical and qualitative improvements. Your suggestions and insights are very helpful for further enhancing the submission quality. We will include a comparison of concurrent work [1] and include the following changes in our updated draft. Please find the responses below:\n\n**[Question 1] What do you mean by \"task-specific\" heuristic functions? Are the tasks here based on \"g\"? If so, do you use different memory buffers for each task, and how many trajectories do you need in the memory buffer before it produces a valid score?**\n\nSimilar to the definition in [2], we use \u201ctask-specific\u201d to describe the heuristic functions for two main reasons: (1) different seed data are utilized for the long-term memory in each task (i.e., dataset); (2) the computed heuristic values are only meaningful within the context of their respective tasks, as similar questions often share underlying solution logic. All tasks are defined based on the natural language descriptions $g$, with further details in Section 2. The initial number of trajectories in the memory buffers for each dataset is as follows: \n|            | Home Search | Trip Booking | Google Sheets | Virtual Home |\n|------------|:-----------:|:------------:|:-------------:|:------------:|\n| # Initial Plans in Long-Term Memory | 11 | 11 | 10 | 50 |\n\nAll seed data for these memory buffers are derived from ToolBench [3], which is a reasonable number of trajectories and comparable to few-shot demonstrations.\n\n**[Question 2] It seems to me that the future cost heuristic might mislead the planner since it is likely that the same action may appear at different positions (in the sequence) for different tasks. For instance: \"Clean a kitchen table\" and \"Set a kitchen table\" have their action sequences reversed.**\n\nWe acknowledge that the future cost heuristic function may not always be accurate. With experimental results presented in Table 2, we have proposed two solutions to mitigate this issue: (1) we integrated the LLM imagination score with heuristic functions to mutually regularize their outputs; (2) the calculation of the heuristic future function involved averaging the relative positions of each action across different plans. Thus, if there exist two scenarios of \u201cClean a kitchen table \u2192 Set a kitchen table\u201d and \u201cSet a kitchen table \u2192 clean a kitchen table\u201d, our method effectively incorporates information from both sequences into the heuristic future score.\n\n**[Reference]**\n\n[1] Hazra et al. \u201cSayCanPay: Heuristic Planning with Large Language Models using Learnable Domain Knowledge.\u201d arXiv 2023.\n\n[2] Hao et al. \u201cReasoning with language model is planning with world model.\u201d EMNLP 2023.\n\n[3] Xu et al. \u201cOn the Tool Manipulation Capability of Open-source Large Language Models.\u201d arXiv 2023."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6910/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700388414485,
                "cdate": 1700388414485,
                "tmdate": 1700388414485,
                "mdate": 1700388414485,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kitDD8Gqnb",
                "forum": "B6pQxqUcT8",
                "replyto": "GNO5Fmwf08",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6910/Reviewer_7Kjr"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6910/Reviewer_7Kjr"
                ],
                "content": {
                    "title": {
                        "value": "Thank you authors"
                    },
                    "comment": {
                        "value": "Thank you for taking the time to address my doubts. I'm satisfied with the answers and have no further questions for now. Cheers!"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6910/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700518114282,
                "cdate": 1700518114282,
                "tmdate": 1700518114282,
                "mdate": 1700518114282,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "lHo2aXqeGP",
            "forum": "B6pQxqUcT8",
            "replyto": "B6pQxqUcT8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6910/Reviewer_tHXf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6910/Reviewer_tHXf"
            ],
            "content": {
                "summary": {
                    "value": "Leveraging powerful LLMs in complex decision-making tasks is a promising direction, and the authors discussed their categories with clear explanations. For efficient reasoning, the authors proposed a new method inspired by A* search and developed some designed evaluation functions for the search. Experimental results with various domains (e.g., Home Search, Trip Booking) with GPT-3.5 and GPT-4 show the effectiveness of the proposed method named ToolChain*."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- A firm contribution of the reasoning framework using LLMs inspired by the MCTS and search algorithm.\n- Extensive experimental evaluations show the effectiveness of the concept."
                },
                "weaknesses": {
                    "value": "- Posiblly handcrafted the two important components (g(n) and h(n) in the A* search) for the problem."
                },
                "questions": {
                    "value": "- Although the evaluated methods and experiments seem solid and interesting to the readers, I\u2019m curious about the designing part of the two important evaluators, $g(n)$ and $h(n)$.\n    - As we know, the naive A* algorithm has some theoretical discussions (e.g., admissible heuristic and optimality in the search problems). However, in the current status of the ToolChain*, the idea (evaluating the node with some function) is from the A*, but no theoretical discussions (e.g., the effectiveness of search) have arisen from the design of $f(n) = g(n) + h(n)$. Do you have any insights? Are there any reasons to adopt the current form of functions? Tried different ones? Although the ablation study in Table 2 is useful, particularly restricted to the current form of g and h, we could have further options here."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6910/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698715962895,
            "cdate": 1698715962895,
            "tmdate": 1699636803888,
            "mdate": 1699636803888,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "LXcwxinii9",
                "forum": "B6pQxqUcT8",
                "replyto": "lHo2aXqeGP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6910/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6910/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Initial Response to Reviewer tHXf"
                    },
                    "comment": {
                        "value": "We appreciate Reviewer tHXf for considering our work as a firm contribution with extensive experiments. Your suggestions and comments are very helpful for us in improving presentation quality and clarifications with wider impacts. Please find the corresponding responses below:\n\n**[Weakness 1 & Question 1] Questions about the design process of two important evaluators, g(n) and h(n) in the A$^\\*$ search. Further studies exploring different forms of cost functions could provide additional insights beyond existing ablation studies.**\n\nWe appreciate your insightful question regarding the design of our cost functions $g(n)$ and $h(n)$ in ToolChain*. Similar to most of the recent advances [1][2][3][4][5] in LLM agents and LLM evaluation, ToolChain* is also based on empirical findings. While lacking systematic theoretical proof, ToolChain* in designing $g(n)$ and $h(n)$ is rooted in empirical findings rather than relying solely on handcrafted functions. To validate the advantage of our proposed value function, we explored various cost function formulations, including self-evaluation prompting LLMs to generate verbalized scores [4][6][7], and vote scores in 'tree-of-thoughts' [3]. Our findings indicated that these scoring methods were not consistently accurate. We identified two key factors guiding our heuristic function design: (1) the black-box nature of state-of-the-art LLMs offering limited information for score formulation, and (2) the observation that similar tasks often share similar logical structures in their solution plans. Driven by the two motivations, we developed heuristic scores to maximize the use of additional information derived from past experiences. \n| Model | Home Search | Trip Booking | Google Sheets | Virtual Home | Average | \n|------------|:-----------:|:------------:|:-------------:|:------------:|:------------:|\n| ToolChain* (Self-Evaluation) | 84.0 | 83.3 | 49.9 | 21.5 | 59.7 |\n| ToolChain* (ToT-Vote) | 82.0 | 81.7 | 53.4 | 21.0 | 59.6 |\n| ToolChain* | 93.0 | 90.8 | 61.4 | 28.6 | 68.5 |\n\nThe results demonstrate that a combination of LLM-generated scores and heuristic scores yields superior performance. In the initial stages, due to limited data in the long-term memory, the model predominantly relies on the LLM-generated scores for generating plans. As the long-term memory expands, the heuristic functions increase in accuracy, effectively regularizing and refining the LLM-generated scores. \n\nFurthermore, we have conducted explorations to utilize additional information from open-sourced LLMs in order to formulate a more precise cost function. These explorations include: (1) action likelihood, calculated as the product of token log probabilities within an action; (2) answer probability, determined by the token probability of 'yes' in response to the question 'Is this action step correct?'; and (3) a reward-style probability, which applies weight decay based on the length of tokens in the actions and the total number of actions in a plan. Given the constraints of computational resources and limited time available during the rebuttal session, we conducted additional experimental results using LLaMA-2 (7B) for additional insights:\n| Model | Virtual Home |\n|------------|:-----------:|\n| ToolChain* (action likelihood) | 25.7 |\n| ToolChain* (answer probability) | 26.1 |\n| ToolChain* (reward function) | 27.4 |\n| ToolChain* | 28.7 |\n\nThe results indicate that our proposed cost functions outperform other value functions from open-source LLMs. We greatly appreciate your constructive feedback and will include further exploration of incorporating open-sourced LLM information into cost function computation in our future work.\n\n**[Reference]**\n\n[1] Yao et al. \u201cReAct: Synergizing Reasoning and Acting in Language Models.\u201d ICLR 2023.\n\n[2] Sun et al. \u201cAdaPlanner: Adaptive Planning from Feedback with Language Models.\u201d NeurIPS 2023. \n\n[3] Yao et al. \u201cTree of thoughts: Deliberate problem solving with large language models.\u201d NeurIPS 2023.\n\n[4] Hao et al. \u201cReasoning with language model is planning with world model.\u201d EMNLP 2023. \n\n[5] Lu et al. \u201cChameleon: Plug-and-play compositional reasoning with large language models.\u201d NeurIPS 2023.\n\n[6] Xie et al. \u201cSelf-Evaluation Guided Beam Search for Reasoning.\u201d NeurIPS 2023.\n\n[7] Xiong et al. \u201cCan LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs.\u201d arXiv 2023."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6910/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700388104912,
                "cdate": 1700388104912,
                "tmdate": 1700388104912,
                "mdate": 1700388104912,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SqDUpvp7sh",
                "forum": "B6pQxqUcT8",
                "replyto": "LXcwxinii9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6910/Reviewer_tHXf"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6910/Reviewer_tHXf"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your response"
                    },
                    "comment": {
                        "value": "I appreciate additional results and responses.\n\nUsing an open-source LLM (i.e., LLaMA-2) is interesting for the academic society. Due to the good writing of the paper, I can understand the findings and contributions of the authors, and to be honest, the results are interesting enough. However, I have some concerns about the balance between technical findings, theoretical justification, and empirical findings for the top-tier conferences, as learning value functions is an essential topic for this field (to evaluate states).\n\nJudging from the response and other review discussions, I'll increase my score."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6910/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700485076525,
                "cdate": 1700485076525,
                "tmdate": 1700485076525,
                "mdate": 1700485076525,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "I551GeQJ01",
                "forum": "B6pQxqUcT8",
                "replyto": "lHo2aXqeGP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6910/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6910/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you Reviewer tHXf"
                    },
                    "comment": {
                        "value": "Thank you so much for your thoughtful and constructive feedback, as well as your plan to raise the score. We greatly appreciate your confirmation on the findings and contributions of our work, and the understanding that our results are interesting enough."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6910/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700523776266,
                "cdate": 1700523776266,
                "tmdate": 1700524179740,
                "mdate": 1700524179740,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "z4Ipb97v0o",
            "forum": "B6pQxqUcT8",
            "replyto": "B6pQxqUcT8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6910/Reviewer_52p8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6910/Reviewer_52p8"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes ToolChain*, a best-first search algorithm for improving the performance of LLM-based agents on sequential decision-making problems. The main contributions are algorithmic and empirical. Specifically, the paper proposes a set of heuristic cost functions leveraging a dataset of solved plans to explore the search space more efficiently. Experiments are conducted on the ToolBench and GSM8K datasets. The results show that ToolChain* outperforms or matches strong recent baselines on these tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ The paper tackles an interesting and important research question with potentially large impact.\n\n+ The paper proposes a simple and intuitively clear best-first search approach to construct an improved LLM-based planning agent. The heuristic function seems novel with potential for broad use across tasks.\n\n+ The proposed method outperforms a number of strong baselines. The experiments include ablations, computational costs and other forms of analyses making it easier to evaluate. The experimental section and appendix has a good amount of detail, aiding reproducibility. \n\n+ The paper is reasonably well-written. The use of illustrative examples made it very easy to understand."
                },
                "weaknesses": {
                    "value": "- Some of the implementation details of the four components of the heuristic function are not clearly explained. These are central to evaluating the algorithmic contributions of the paper and reproducibility so a clearer description would be very useful. I found it particularly difficult to understand the implementation details of the \"imagination score\". See the questions for details.\n\n- Although the experiments have a good amount of detail including ablation experiments, the paper does not clearly identify the main sources of performance improvement. For example, how much does a better scoring function matter compared with a better LLM or search procedure? How does performance depend on the prompt and other hyper-parameters of the algorithm (heuristic score implementation, dataset size and quality, etc.)? What happens if we use non-LLM versions of the scoring function? The paper mentions the importance of the cost function (\"this efficiency gain may stem from the proposed superior cost function\") but does not investigate deeper. As a result, it becomes difficult to clearly identify the overall impact and broader utility of ToolChain*, especially compared to similar search-based ideas proposed in Tree-of-Thought."
                },
                "questions": {
                    "value": "- Please explain the details of the Imagination Score in more detail. For example, what does \"imagine more concrete future steps\" mean? Where is the \"imagined plan\" generated in Algorithm 1? (I think in the cost calculation $f(n)$ but not sure). How many extra calls to the LLM?\n\n- Is the LCS calculation in $g_{t,1}$ over the sequence of API actions ($a_i \\in \\mathcal{A}$) alone or does it include additional information about the task or problem in the text string? Do the API actions include the parameters or just the method names?\n\n- How much does overall performance vary with the prompt and the other hyper-parameters of the algorithm (dataset, heuristic score implementation, etc.)?\n\n- How much performance comes from the use of the LLM in different parts of the algorithm vs classical (non-LLM) best-first search? Specifically, how is performance impacted by non-LLM expansion with LLMs only used for node evaluation? Is it possible to construct a search-based baseline which **doesn't** use any LLMs but otherwise incorporates the same intuition (long-term memory, heuristic scoring, etc.)? If yes, how might it perform?\n  - In Appendix D.5, is it possible to characterize the computational costs of ToolChain* in more detail? For example, what are the the LLM costs in terms of the number of calls and generated tokens? Dollar cost? Heuristic computational costs? Environment (domain) \"simulation\" costs associated with the API calls, if any?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6910/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698849140875,
            "cdate": 1698849140875,
            "tmdate": 1699636803768,
            "mdate": 1699636803768,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "etaVsW7j8L",
                "forum": "B6pQxqUcT8",
                "replyto": "z4Ipb97v0o",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6910/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6910/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Initial Response to Reviewer 52p8 -- PART I"
                    },
                    "comment": {
                        "value": "We appreciate Reviewer 52p8 for considering our work as interesting and important with a potentially broad impact. Your suggestions for clarification and improvement are constructive. Please find the responses below:\n\n**[Weakness 1] Some of the implementation details of the four components of the heuristic function are not clearly explained. These are central to evaluating the algorithmic contributions of the paper and reproducibility so a clearer description would be very useful. I found it particularly difficult to understand the implementation details of the \"imagination score\". See the questions for details.**\n\nDetails of the imagination score are explained in Section 3.3 of our original paper. For further clarifications and an illustrative example, we kindly invite you to refer to our response to Question 1.\n\n**[Weaknesses 2.1] Although the experiments have a good amount of detail including ablation experiments, the paper does not clearly identify the main sources of performance improvement. For example, how much does a better scoring function matter compared with a better LLM or search procedure?**\n\nWe would like to emphasize that the improvement of our approach over the state-of-the-art approaches (i.e., baselines) [1][2][3][4] is not from the capability of a better LLM. This has been validated by the comparison between our approach and baselines using exactly the same LLM in Table 1. Instead, the improvement comes from (a) the scoring function design and (b) the search procedure. Although using a more advanced LLM may lead to improved performance, in our comparisons, we always keep using the same LLMs for our approach and baselines in Table 1.\n\nTo further understand the improvement by (a) the scoring function design and (b) the search procedure, as well as how significant the improvement is compared to the improvement by a better LLM, we have conducted a more thorough ablation study on ToolChain*. This extensive ablation study includes different base LLM models (GPT-3.5, GPT-4), scoring functions (ToT-Value, ToT-Vote, LLM scores, our proposed cost function $f(n)$), and search procedures (ToolChain*, ToT):\n| Row | Model | Home Search | Trip Booking | Google Sheets | Virtual Home | Average | \n|------------|------------|:-----------:|:------------:|:-------------:|:------------:|:------------:|\n| 1 | ToolChain* (GPT-4, $f(n)$ in Section 3.1) | +5.0 | +6.7 | +7.2 | +5.9 | +6.2 |\n| 2 | **ToolChain$^\\*$ (GPT-3.5, $f(n)$ in Section 3.1)** | **93.0** | **90.8** | **61.4** | **28.6** | **68.5** |\n| 3 | ToolChain* (GPT-3.5, ToT-Value [3]) | -9.0 | -7.5 | -11.5 | -7.1 | -8.8 |\n| 4 | ToolChain* (GPT-3.5, ToT-Vote [3]) | -11.0 | -9.1 | -8.0 | -7.6 | -8.9 |\n| 5 | ToolChain* (GPT-3.5, LLM scores) | -7.0 | -5.0 | -5.7 | -6.0 | -5.9 |\n| 6 | ToT [3] (GPT-3.5, $f(n)$ in Section 3.1) | -8.0 | - 4.1 | -9.5 | -8.9 | -7.6 |\n| 7 | ToT [3] (GPT-3.5, ToT-Vote) | -10.0 | -7.5 | -12.8 | -6.8 | -9.3 |\n\nIn this table above, $f(n)$ indicates our cost function design in Section 3.1, while ToT-Value and ToT-Vote are two value functions utilized in ToT [3].\n\nOur main contributions lie in the development of a more effective and efficient planning algorithm for LLM agents to address complicated real-world problems. We summarized the following observations from the table above: (1) It is important to note that the design of the scoring function is an integral component of a search procedure. While it is challenging to isolate and compare their individual contributions, our experiments indicate that the scoring function design has slightly more impact than the search algorithm itself. In addition, when compared with baselines under the same base LLMs (GPT-3.5 or GPT-4), both our proposed searching procedures and scoring functions have contributed to model performance (see details in Table 1). (2) ToolChain* shows better performance with GPT-4 than GPT-3.5, suggesting that employing a more advanced base LLM could lead to performance improvement, while this improvement is considered moderate compared to the improvement by our searching procedures and scoring functions."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6910/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700384981129,
                "cdate": 1700384981129,
                "tmdate": 1700384981129,
                "mdate": 1700384981129,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pLSQ3aZHw8",
                "forum": "B6pQxqUcT8",
                "replyto": "z4Ipb97v0o",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6910/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6910/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Initial Response to Reviewer 52p8 -- PART III"
                    },
                    "comment": {
                        "value": "**[Weakness 2.4] The paper mentions the importance of the cost function (\"this efficiency gain may stem from the proposed superior cost function\") but does not investigate deeper. As a result, it becomes difficult to clearly identify the overall impact and broader utility of ToolChain$^\\*$, especially compared to similar search-based ideas proposed in Tree-of-Thought.**\n\nTo elucidate its importance, we refer to our experimental results in Table 2 and additional experiments addressing Weaknesses 2.1. These experiments demonstrate that alternative cost function designs (by comparing row 2 with rows 3,4,5 in additional experiments in Weaknesses 2.1) or the removal of any component from the cost function (by comparing all results in Table 2) results in a notable average performance decrease of 7.9% and 5.0%, respectively. Moreover, these adjustments to the cost functions in ToolChain* lead to an approximate increase of 1.52x in computation time on average. These empirical findings highlight the significance of the cost function in ToolChain* for plan quality and efficiency.\n\nIn addition to the empirical results above, the overall impact and broader utility of our paper can be summarized as follows: (1) We propose ToolChain*, a novel A*-like tree search algorithm, to develop autonomous LLM-based agents for complex planning and reasoning tasks; (2) ToolChain* formulates the action space as a decision tree, effectively mitigating error propagation and expanding search space; (3) We leverage a combination of LLM-based functions and heuristic functions to efficiently navigate the agents in the action space. (4) Our extensive experiments demonstrate the effectiveness and efficiency of ToolChain* in tool usage across five distinct datasets including both planning and reasoning tasks (e.g., home searching, trip booking, mathematics, etc.).  \n\nUsing exactly the same score function, we compare ToolChain* with the tree-of-thoughts (ToT) approach [3], which relies on depth-first and breadth-first search methods. We summarize the advantages of ToolChain* as follows: (1) ToolChain* is based on A* search, utilizing future cost functions for goal-directed node prioritization. This estimation results in enhanced performance and efficiency compared to the ToT, as demonstrated in Table 1 and Figure 5; (2) ToolChain* significantly reduces the number of nodes that need exploration. While ToT-BFS explores all nodes at a given depth before moving to the next level and ToT-DFS potentially follows irrelevant paths, ToolChain* focuses only on the most promising paths through A* search; (3) ToolChain* is designed for more complex, real-world applications, demonstrating its broader utility in LLM agent tool use and reasoning scenarios.\n\n**[Question 1] Please explain the details of the Imagination Score in more detail. For example, what does \"imagine more concrete future steps\" mean? Where is the \"imagined plan\" generated in Algorithm 1? (I think in the cost calculation f(n) but not sure). How many extra calls to the LLM?**\n\nThe introduction of the imagination score is available in Section 3.3. We would like to provide further clarifications on the future steps and imagined plans with a detailed example. Consider a current plan $p=(a_0,a_1,\\cdots,a_t)$. The future score $h(\\cdot)$ aims to estimate the potential future cost required to complete the plan and reach the target. The imagination score is a component $h_{t,2}(\\cdot)$ of the future score $h(\\cdot)$. To calculate $h_{t,2}(\\cdot)$, we initially prompt LLMs to imagine subsequent steps to the current plan, resulting in an imagined plan $p_{im}=(a_{t+1},a_{t+2},\\cdots,a_T)$. We then calculate the proportion of current steps present in the imagined plan as the imagination score: $h_{t,2}(n)=t/T$. The imagined plan is generated in the cost function calculation $f(n)$ in Algorithm 1.\n\nFor example, assume that we have a current plan *\u201cAgent.WalkTo(dining_room) $\\to$ Agent.WalkTo(food) $\\to$ Agent.Find(food) $\\to$ Agent.Grab(food)\u201d*. We first prompt LLMs to use their imagination to complete the plan, resulting in  *\u201cAgent.WalkTo(dining_room) $\\to$ Agent.WalkTo(food) $\\to$ Agent.Find(food) $\\to$ Agent.Grab(food) $\\to$ Agent.Find(table) $\\to$ Agent.Put(food, table)\u201d*. We then use the proportion of current steps in the imagined plan as the imagination score, where $h_{t,2}(n)=4/6=0.67$, leading to an imagination-based future cost of $1-h_{t,2}(n)=0.33$.\n\nConcerning the additional API calls, each action requires two separate calls to calculate both the self-consistency sampling score and the LLM imagination score. Consequently, by leveraging the total API calls noted in our response to Question 5, we estimate the number of additional API calls as approximately 53.7 per question, which is similar to the number of extra API calls in other tree search-based methods [3][4]."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6910/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700386040622,
                "cdate": 1700386040622,
                "tmdate": 1700387194095,
                "mdate": 1700387194095,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dC3lOhYzBo",
                "forum": "B6pQxqUcT8",
                "replyto": "z4Ipb97v0o",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6910/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6910/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Initial Response to Reviewer 52p8 -- PART IV"
                    },
                    "comment": {
                        "value": "**[Question 2] Is the LCS calculation in $g_{t,1}$ over the sequence of API actions ($a_i\\in\\mathcal{A}$) alone or does it include additional information about the task or problem in the text string? Do the API actions include the parameters or just the method names?**\n\nWe apologize for the potential confusion. We only leverage the sequence of API actions in our calculations of the LCS calculation $g_{t,1}$. The API functions used in the LCS calculation include both the API function name and the parameters. We will add further clarifications in the updated manuscript.\n\n**[Question 3] How much does overall performance vary with the prompt and the other hyper-parameters of the algorithm (dataset, heuristic score implementation, etc.)?**\n\nFor additional experimental results and clarifications, we kindly invite the reviewer to refer to our response in Weakness 2.2 for more details.\n\n**[Question 4] How much performance comes from the use of the LLM in different parts of the algorithm vs classical (non-LLM) best-first search? Specifically, how is performance impacted by non-LLM expansion with LLMs only used for node evaluation? Is it possible to construct a search-based baseline which doesn't use any LLMs but otherwise incorporates the same intuition (long-term memory, heuristic scoring, etc.)? If yes, how might it perform?**\n\nLLM agents have demonstrated strong planning and reasoning capabilities in complicated problem-solving [1][2][3][4][6]. Following the most recent research in this line, our primary scope is to develop a planning method for LLM agents to enhance solution quality and search efficiency. Considering the very recent advances [1][2][3][4][6], approaches involving non-LLM best-first search may not lead to an optimized design, and thus not considered in our current research scope: To incorporate non-LLM best-first search (non-LLM agent) into ToolChain*, we need to consider all potential actions as candidates at each step. Given the extensive action space formed by API functions and their parameters, this approach would necessitate evaluating thousands of nodes at every step, leading to impracticality and inefficiency. In contrast, LLM agents utilize their inherent reasoning and planning capabilities to narrow down to the most promising potential next steps, allowing us to apply our proposed cost function to a more manageable set of options. This significantly reduces the need to exhaustively traverse the entire action space at each step. To assess the feasibility of a non-LLM approach, we experimented with randomly expanding 50 potential actions at each step (where >50 actions are less impractical due to the aforementioned efficiency issue), while keeping the cost function unchanged from the LLM agent settings:\n| Model | Home Search | Trip Booking | Google Sheets | Virtual Home | Average | \n|------------|:-----------:|:------------:|:-------------:|:------------:|:------------:|\n| ToolChain* | 93.0 | 90.8 | 61.4 | 28.6 | 68.5 |\n| Non-LLM best-first search ($f(n)$ in Section 3.1) | 0.0 | 0.0 | 12.0 | 5.2 | 4.3 |\n\nThese results offer insights into the comparative performance of LLM-based and non-LLM agents in the context of ToolChain*, highlighting the significant contributions of LLM agents to the efficiency and effectiveness of our method. We observe significantly lower performance (i.e., 0) by the non-LLM agent, because the non-LLM agent struggles to perform effectively when choosing the best one from the numerous available actions, with the evaluations being enabled by our LLM-based score functions. Interestingly, the non-LLM best-first search method demonstrates marginally superior performance on Google Sheets across other datasets, primarily due to the inclusion of single-step solutions in its test set. Other traditional non-LLM A* approaches, as referenced in [7], typically involve training for one-step expansion using additional annotated data, which is not always available in the evaluation sets considering the huge amount of actions."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6910/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700386369498,
                "cdate": 1700386369498,
                "tmdate": 1700387135066,
                "mdate": 1700387135066,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bIcqO7gOfc",
                "forum": "B6pQxqUcT8",
                "replyto": "z4Ipb97v0o",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6910/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6910/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Follow-up Response to Reviewer 52p8"
                    },
                    "comment": {
                        "value": "Dear Reviewer 52p8,\n\nWe sincerely appreciate your valuable and constructive feedback. We would like to kindly remind you that the author/reviewer discussion phase concludes on November 23rd. We hope our responses and additional experiments have addressed your concerns and improved the paper\u2019s quality. If you have any further suggestions or comments, please feel free to share them. We are looking forward to a constructive discussion during the rebuttal phase. \n\nBest Regards,   \nThe Authors"
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6910/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700600006268,
                "cdate": 1700600006268,
                "tmdate": 1700688552371,
                "mdate": 1700688552371,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "vYnNEAo9eS",
            "forum": "B6pQxqUcT8",
            "replyto": "B6pQxqUcT8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6910/Reviewer_gdT8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6910/Reviewer_gdT8"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes ToolChain*, an efficient tree search-based planning algorithm to augment large language models (LLMs) with external tools for complex real-world planning and reasoning tasks.  This paper provides very good insight that formulates the action space as a search tree, where each node is an API function call. This mitigates error propagation and expands the search space compared to linear reasoning in open-/closed-loop systems. It uses an A*-like algorithm to search the tree, prioritizing branches using a task-specific cost function with cumulative cost g(n) and future cost h(n). This balances exploration and exploitation for efficient search."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1) Novel formulation of LLM planning as tree search using A* search algorithm is intuitive and elegant. Allows structured exploration of expansive action space.\n2) Task-specific cost functions g(n) and h(n) provide a principled way to guide search and prioritize promising branches.\n3) Strong empirical results demonstrating improved success rate and efficiency over competitive baselines on diverse tasks."
                },
                "weaknesses": {
                    "value": "1) Cost functions rely on heuristic components like long-term memory, self-consistency sampling, and LLM imagination which may not always be accurate.\n2) Still trails open-loop systems in efficiency, albeit outperforming other tree search methods. There is a tradeoff between search depth and solution quality.\n3) Limited analysis on how the approach generalizes to even more expansive action spaces and longer planning horizons. \nHowever, I think that the above weaknesses are some hard open questions in NLP with LLM."
                },
                "questions": {
                    "value": "1) How robust are the cost functions g(n) and h(n) for entirely new tasks where long-term memory/heuristics may not be available?\n2) Could incremental search algorithms like IDA* further improve efficiency over A* search used here?\n3) For very large action spaces, is it possible to focus the search on high-level plans first before expanding API details? Hierarchical search?\n4) How well would ToolChain* generalize if the action space grows 10x or 100x in size? At what point would efficiency degrade?\n5) Beyond API functions, how can ToolChain* be extended to more general symbolic action spaces?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6910/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699554273050,
            "cdate": 1699554273050,
            "tmdate": 1699636803646,
            "mdate": 1699636803646,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "TwR4FMIZSQ",
                "forum": "B6pQxqUcT8",
                "replyto": "vYnNEAo9eS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6910/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6910/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Initial Response to Reviewer gdT8 -- PART I"
                    },
                    "comment": {
                        "value": "We sincerely appreciate Reviewer gdT8 for considering our work as novel and promising with solid empirical results. Your suggestions are decent insights for us to revise the submission draft. Please find the responses below:\n\n**[Weakness 1] Cost functions rely on heuristic components like long-term memory, self-consistency sampling, and LLM imagination which may not always be accurate.**\n\nWe recognize that individual heuristic components of our cost functions, such as (1) long-term memory, (2) self-consistency sampling, and (3) LLM imagination, might not always yield perfectly accurate results. Similar to recent works [1][2][3] in the field of LLM agents, our work leverages heuristic functions to evaluate actions during the search for solutions. In this paper, we innovatively identify three intrinsically complementary components and further discover that a combination of these components enhances model performance compared to using any single component in isolation (see Table 2 for empirical details). This improvement stems from each component providing a unique perspective in evaluating the nodes within the search tree. For example, in scenarios with limited plans in long-term memory, the other two components (self-consistency sampling and LLM imagination) become more influential in shaping the cumulative function $f(n)$ and the future score $g(n)$. Conversely, as more plans accumulate in the long-term memory, the heuristic scores gain reliability and effectively regularize the scores generated by LLMs. \n\n**[Weakness 2] Still trails open-loop systems in efficiency, albeit outperforming other tree search methods. There is a tradeoff between search depth and solution quality.**\n\nWe agree with the reviewer's point about the efficiency trade-offs of our method in comparison to open-loop systems. As acknowledged in Appendix A, our method, while effective, does not yet match the efficiency of open-loop systems. Our ToolChain* aims to strike an optimal balance between efficiency and solution quality. While open-loop systems are more efficient by directly generating the entire plan, they often compromise on plan correctness due to limited exploration in the potential action space. To fundamentally address the limited exploration issue of open-loop systems, similar to recent Tree-of-Thought (ToT) [1] and Monte Carlo Tree Search (MCTS) [2], our approach is based on a tree search system with inevitable additional costs, resulting in lower efficiency. When compared with recent state-of-the-art tree-search systems [1][2], Toolchain* stands out by achieving the best efficiency (as discussed in Section 4.2). Enhancing model efficiency without significantly compromising performance remains a key direction for our future work. \n\n**[Weakness 3] Limited analysis on how the approach generalizes to even more expansive action spaces and longer planning horizons. However, I think that the above weaknesses are some hard open questions in NLP with LLM.**\n\nWe acknowledge the importance of further analysis in more expansive action spaces and longer planning horizons. Besides Table 5 in Appendix C.2 (page 15), we provide more details of our tested environments:\n|            | Home Search | Trip Booking | Google Sheets | Virtual Home | Average |\n|------------|:-----------:|:------------:|:-------------:|:------------:|:-------:|\n| # Tools | 15 | 20 | 108 | 40 | 46 |\n| Avg. # APIs / Solution | 7.5 | 13.4 | 6.1 | 13.4 | 10.1 |\n\nThe # TooIs represents the number of different tools in the environment. The Avg. # APIs/Solution represents the average number of different API functions called in each solution. For our evaluated environments, while the number of tools may appear limited, the combination of these tools with flexible parameters significantly expands the action space, which is comparable to ToolLLM [4], the largest known dataset (concurrent to our work) in the tool-use domain. Considering the multi-tool scenarios only compose tools within the same category, the ToolLLM dataset is of a similar scale to ours, averaging around 70 different tools in each category. In terms of solution length, the ToolLLM dataset features solutions with 6 to 15 actions, a range comparable to our datasets. Notably, some solutions in our virtual home dataset extend to approximately 36 actions, indicating a more complex environment."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6910/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700383645482,
                "cdate": 1700383645482,
                "tmdate": 1700383645482,
                "mdate": 1700383645482,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kp5GnQpZFF",
                "forum": "B6pQxqUcT8",
                "replyto": "vYnNEAo9eS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6910/Reviewer_gdT8"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6910/Reviewer_gdT8"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your reply!"
                    },
                    "comment": {
                        "value": "Thank you for your reply! The authors have addressed my primary concerns effectively."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6910/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700549248984,
                "cdate": 1700549248984,
                "tmdate": 1700549449859,
                "mdate": 1700549449859,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]