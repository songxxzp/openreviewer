[
    {
        "title": "OmniControl: Control Any Joint at Any Time for Human Motion Generation"
    },
    {
        "review": {
            "id": "cyDowUPFZr",
            "forum": "gd0lAEtWso",
            "replyto": "gd0lAEtWso",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission195/Reviewer_4D6Y"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission195/Reviewer_4D6Y"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a generative model for text-conditional human motion synthesis offering detailed control over joint positions in each frame. The authors propose two guidance mechanisms, spatial and realism guidance, that aim to generate human motion which closely adheres to the guidance while maintaining realism. The effectiveness of these designs is established through experiments, supported by comprehensive data and high-quality visualizations."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper considers the novel task of guiding individual joints spatially in human motion generation. Experiments conducted on the HumanML3D and KIT-ML datasets show promising results.\n\n2. The experimental setup is robust and the accompanying visualizations are of high quality, reflecting the authors' meticulous efforts in this research."
                },
                "weaknesses": {
                    "value": "1. Previous works guiding the position of the pelvis could potentially be intuitively adapted to joint positions with proper coordinate transformations, which undermines the novelty of this work, making it seem more of an incremental step rather than a solution to fundamental problems in the field.\n\n2. Despite the novelty of controlling all joints at any frame, the paper lacks a discussion on efficient collection of the spatial guidance trajectory, particularly considering the fact that the joint positions are no more relative to the pelvis. This aspect is crucial for practical applications in industries such as 3D animation. Moreover, the paper does not discuss the model's tolerance for inherently unnatural guidance (*e.g.*, manually drawn trajectories or those from different datasets).\n\n3. There are some formatting errors (*e.g.*, the misuse of `\\citep` instead of `\\citet` in some citations)."
                },
                "questions": {
                    "value": "In most scenarios, the text input and the spatial guidance seem redundant. Can the model effectively comprehend and follow instructions that appear only in one modality? (*e.g.*, providing spatial guidance only on the foot or pelvis, while the hand activity is only described via the text input?)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission195/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission195/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission195/Reviewer_4D6Y"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission195/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698323924362,
            "cdate": 1698323924362,
            "tmdate": 1699635945312,
            "mdate": 1699635945312,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "N4rRu92zfx",
                "forum": "gd0lAEtWso",
                "replyto": "cyDowUPFZr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission195/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission195/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 4D6Y (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for the constructive and helpful feedback. We respond below to your questions and comments:\n\n```\nQ1: Previous works guiding the position of the pelvis could potentially be intuitively adapted to joint positions with proper coordinate transformations\u2026\n```\n**A1**: Do you mean that, if we know the coordinate transformation between the pelvis and other joints, we may be able to control them together using previous methods?\n\nIn general, we know in advance neither the position of the pelvis nor the coordinate transformations between the pelvis and other joints that we\u2019d to control, as discussed in the introduction (paragraph 2) and Section 3.1 (human pose representations). Thus adapting the previous pelvis-control-only methods to other joints is not feasible.\n\nIf it\u2019s not what you mean, we would greatly appreciate it if you can clarify the question.\n\n\n&nbsp;\n\n```\nQ2: The paper lacks a discussion on efficient collection of the spatial guidance trajector\u2026\n```\n**A2**: We envision there may be two ways to collect the spatial guidance trajectories. First, for practical applications in industries such as 3D animation, the user (designer) may provide such guidance trajectories as part of the application development. Second, the spatial guidance may be from the interactions and constraints of the scene/object. For example, the height of the ceiling or the position of the chair (and thus the position of the controlled joint), as we show in the last column of [Figure 1](https://github.com/OmniControl24/OmniControl24.github.io/blob/main/static/images/figure1.jpg) in the paper. In both cases, the spatial guidance trajectories can be efficiently collected.\nThe discussion has been added to **Appendix A.11** of the revised paper.\n\n&nbsp;\n\n```\nQ3: The paper does not discuss the model's tolerance for inherently unnatural guidance (e.g., manually drawn trajectories\u2026\n```\n**A3**: In all the figures and supplementary video, the input control signals are manually drawn (e.g. the circle, straight, and sine lines in [Figure 1](https://github.com/OmniControl24/OmniControl24.github.io/blob/main/static/images/figure1.jpg) and [supplementary video](https://youtu.be/29M-Shs1Orw?t=27)). To better resolve your concern, we added some results with extremely unnatural spatial control signals in the updated supplementary video [(from 4:41 to 5:23)](https://youtu.be/29M-Shs1Orw?t=284) and penultimate section of the [website](https://omnicontrol24.github.io/). These results show that our model has a good tolerance for unnatural trajectories (teleportation, spiral forward).\n\n&nbsp;\n\n```\nQ4: Misuse of \\citep instead of \\citet in some citations\u2026\n```\n**A4**: Thank you for pointing this out. We have revised these misuses in the citations. \n\n&nbsp;\n\n```\nQ5: The text input and the spatial guidance seem redundant\u2026\n```\n**A5**: In some cases especially when the human is interacting with the scene or object, the spatial control signals are usually provided as global locations of joints of interest in keyframes **as they are hard to convey in the textual prompt**. For instance, in the low-ceiling scenario shown in the last column of [Figure 1](https://github.com/OmniControl24/OmniControl24.github.io/blob/main/static/images/figure1.jpg) in the paper, it is infeasible to specify the spatial constraints in the textual prompt precisely. Even if we do, there is no guarantee that the model will faithfully incorporate them into the human motion generation, which will lead to unsatisfactory user experiences. Therefore, it is more natural and intuitive to provide spatial control signals as a separate input. At the same time, the textual description is helpful for conveying high-level semantic guidance, such as \u201cplaying the violin\u201d. Therefore, the text prompt and spatial guidance are complementary to each other instead of redundant."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission195/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700533304379,
                "cdate": 1700533304379,
                "tmdate": 1700535049073,
                "mdate": 1700535049073,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "puteQlDSpM",
                "forum": "gd0lAEtWso",
                "replyto": "cyDowUPFZr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission195/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission195/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 4D6Y (2/2)"
                    },
                    "comment": {
                        "value": "```\nQ6: Can the model effectively comprehend and follow instructions that appear only in one modality? (e.g., providing spatial guidance only on the foot or pelvis, while the hand activity is only described via the text input?)\n```\n**A6**: Yes it can. In the example in the top left of [Figure 1](https://github.com/OmniControl24/OmniControl24.github.io/blob/main/static/images/figure1.jpg), the spatial control signal is applied to the pelvis (walk along the circle), while the text is used to describe the hand activity (play the violin). \nAs can be seen, our model can effectively comprehend and follow instructions on separate joints (pelvis and hand). We provide the results with only a text prompt or only a trajectory as the condition in the supplementary video [(From 5:23 to 5:45)](https://youtu.be/29M-Shs1Orw?t=325) and the last section of the anonymous [website](https://omnicontrol24.github.io/). The results show that our model can follow the instructions that appear only in one modality.\n\nWe\u2019d like to note that since the text input and the control signals describe the motion of different joints, they are complementary instead of redundant."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission195/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700533367975,
                "cdate": 1700533367975,
                "tmdate": 1700533367975,
                "mdate": 1700533367975,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "cRAkNGGFEO",
            "forum": "gd0lAEtWso",
            "replyto": "gd0lAEtWso",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission195/Reviewer_gCs4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission195/Reviewer_gCs4"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a human motion generation method with the ability of manipulating any joint at any time. The method can take the language prompt as condition and both the spatial guidances and realism guidance as constraints. Compared to previous method, including MDM, based on which the method is developed, the proposed method showcases more flexibility for downstream applications. With a single model, the proposed OmniControl sets a new SOTA to control both the pelvis and other joints in motion generation."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The method allows using spatial guidance to constraint the generated motion sequence. The constraint can be put to any joint instead of just pelvis.\n- By combining the realism guidance, the conditionally generated motion sequences can be expected to be more natural under the spatial constraint. The realism guidance is a trainable copy of encoder to enforce the spatial constraints. It is essentially an enforced encoder fusing the information from the language prompt and the spatial constraints. Connected with the main transformer encoder during training, the realism guidance is trained to correct the signal passed in the corresponding layers."
                },
                "weaknesses": {
                    "value": "- The paper writing is not fluent enough and needs polishing to be easier to follow.\n- Given the carefully designed modules, the time efficiency for training is important to evaluate the significance of the proposed method. However, this part is missing in the paper.\n- Some important baselines are missing in the experiment sections, such as [1,2]. Adding the full set of published baselines on the benchmarks of HumanML3D and KIT-ML will change the position of the proposed methods highly. Can the authors elaborate more about the comparison with the baselines? Or maybe there is any reason that these baselines are not proper to compare with?\n- Some minor writing issues, such as duplicated typo: input -> \"input\"\n- Referring to Figure 5, which is placed in a very late position in the introduction section makes a bad reading flow. You may want to adjust the position of Figure 5 to make it closer to where it is referred to.\n\nReference:\n\n[1]: \"T2M-GPT: Generating Human Motion from Textual Descriptions with Discrete Representations\u201d CVPR 2023\n\n[2]: \u201cGenerating Diverse and Natural 3D Human Motions from Text\u201d, CVPR 2022"
                },
                "questions": {
                    "value": "See my concerns listed above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission195/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission195/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission195/Reviewer_gCs4"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission195/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698582477824,
            "cdate": 1698582477824,
            "tmdate": 1700684550674,
            "mdate": 1700684550674,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5DIvll4c6X",
                "forum": "gd0lAEtWso",
                "replyto": "cRAkNGGFEO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission195/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission195/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer gCs4"
                    },
                    "comment": {
                        "value": "Thank you for your time and helpful feedback. We respond below to your questions and comments:\n\n```\nQ1: The time efficiency for training\u2026\n```\n**A1**: Thank you for the suggestion to add the training time. It takes **29 hours** to train our model on a single NVIDIA RTX A5000 GPU with batch size of 64 and 250,000 iterations in total.\nAs a reference, MDM, PriorDM, and GMD take 72, 11.5, and 39 hours, respectively, according to what is reported in their papers. \nMDM and PriorMDM are trained on a single NVIDIA GeForce RTX 2080 Ti GPU (slower than the A5000 GPU).\nGMD is trained on a single NVIDIA RTX 3090 (roughly comparable with the A5000 GPU).\nOur training efficiency is better than GMD. PriorMDM performs better in training efficiency because it shares the same architecture as MDM and thus can directly finetune the pre-trained weight of MDM (the pre-training time was not included in the reported timing).\nWe have added this information in **Table 4 in Appendix A.2** of the revised version.\n\n&nbsp;\n\n```\nQ2: Some important baselines are missing in the experiment sections \u2026 maybe there is any reason that these baselines are not proper to compare with.\n```\n**A2**: Thank you for sharing these two papers. These baselines [1, 2] are indeed not feasible to compare with because they **cannot** integrate spatial control signals into text-based human motion generation, which is the main focus of our paper.\nThe baselines we compared in our paper are all the methods that can integrate the spatial control signals (to our best knowledge). We have compared with\n1. MDM and PriorMDM: they demonstrate that the inpainting-based methods can be integrated into their pipeline to achieve pelvis-controlling with dense signal in their paper. They can only control the pelvis with the global control signal.\n2. GMD: it focuses on exactly the same problem as ours but can only control the pelvis.\n\n*[1]: \"T2M-GPT: Generating Human Motion from Textual Descriptions with Discrete Representations\u201d CVPR 2023*  \n*[2]: \u201cGenerating Diverse and Natural 3D Human Motions from Text\u201d, CVPR 2022*\n\n&nbsp;\n\n```\nQ3: typo Imput -> input\n```\n**A3**: We believe you meant \u201cimput -> input\u201d. We have revised it to avoid any confusion.\n\n&nbsp;\n\n```\nQ4: Referring to Figure 5, which is placed in a very late position\u2026\n```\n**A4**: Thank you for the suggestion. We have removed the reference to Fig. 5 in the introduction section. It is now only referred to in Section 4.2, which is close to its position and should improve the reading flow. \n\n&nbsp;\n\n```\nQ5: The paper writing is not fluent enough and needs polishing to be easier to follow.\n```\n**A5**: We have revised the paper to improve the reading flow. We would greatly appreciate it if more insights could be shared on how to further improve it."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission195/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700533270746,
                "cdate": 1700533270746,
                "tmdate": 1700533270746,
                "mdate": 1700533270746,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "NgV60yvDS4",
                "forum": "gd0lAEtWso",
                "replyto": "5DIvll4c6X",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission195/Reviewer_gCs4"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission195/Reviewer_gCs4"
                ],
                "content": {
                    "title": {
                        "value": "Feedback to authors' reply"
                    },
                    "comment": {
                        "value": "Thanks for your reply.\n\nI read your revised version of the paper and the reply carefully. I think the current version is better organized and more self-contained. I appreciate the efforts of adding training time and more details as elaborated. \n\nYes, I intended to indicate the typo \"imput->input\" ... how could I write typo when indicating a typo :(\n\nThanks for the clarification about the reason of not introducing some related baseline methods in the evaluation.\n\nI will be re-considering my rating after the discussion with other reviewers."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission195/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700621155246,
                "cdate": 1700621155246,
                "tmdate": 1700621155246,
                "mdate": 1700621155246,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "snvxs0XtFT",
            "forum": "gd0lAEtWso",
            "replyto": "gd0lAEtWso",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission195/Reviewer_YHiJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission195/Reviewer_YHiJ"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces OmniControl, a novel method that enhances text-conditioned human motion generation by allowing flexible spatial control across multiple joints, ensuring realistic and coherent movements. The integration of spatial and realism guidance achieves a balance between accuracy and natural motion, demonstrating superior pelvis control and promising outcomes on various joints, marking an advancement in generating constrained, realistic human motions. The commitment to releasing code and model weights further enhances accessibility for future advancements in this field."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper offers a simple yet effective method to integrate spatial control signals into a text-conditioned human motion generation model based on the diffusion process.\nThe introduction of realism guidance to refine all joints for generating more coherent motion is commendable.\nThe evaluation is adequate and comprehensive."
                },
                "weaknesses": {
                    "value": "It would be better if the difference and advantage between the global coordinates and local coordinates could be visualized.\nInference time is higher than MDM and GMD.\nThe concept in Fig. 4, such as the input process, requires further clarification for better comprehension. In addition, The components of spatial encoder F and the size of output f_n are not explained.\nThe difference and advantage between the global coordinates and local coordinates were not visually explained."
                },
                "questions": {
                    "value": "In Fig. 7, understanding why higher density leads to higher FID and Foot skating ratio while other factors lead to lower FID and Foot skating ratio is required. Traditionally, higher density in certain contexts can lead to better performance due to increased information or more complex interactions. However, in your case, it seems to be causing a lower performance. Additionally, the paper mentions the Avg. error of MDM and PriorMDM being zero due to the inpainting property. Elaborating on the nature of this property would provide clarity. Moreover, why the proposed methods are with zero error when density is low should be addressed.\nIn the supplementary video, it would be better if the video demonstrate \u201cControl other joints\u201d can be visualized compared with GMD.\nQ: Where is the spatial control signal coming from? Is it given by dataset?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission195/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698730820300,
            "cdate": 1698730820300,
            "tmdate": 1699635945126,
            "mdate": 1699635945126,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4t56eCG315",
                "forum": "gd0lAEtWso",
                "replyto": "snvxs0XtFT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission195/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission195/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer YHiJ (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for your thoughtful comments. Please refer to \u201cThe General Response to Reviewers\u201d for the reply to the issue of the inference speed. We respond below to your other questions and concerns:\n\n```\nQ1: The difference and advantage between the global coordinates and local coordinates\u2026\n```\n**A1**: We show the visualization results with global pose representation in Appendix A.6. We found the model cannot converge and produce collapsed human poses as shown in [Figure 8](https://github.com/OmniControl24/OmniControl24.github.io/blob/main/static/images/figure8.jpg), which do not form the correct motion of a human.\nRecent work InterGen [1] proposes a global representation with bone length loss to enforce skeleton consistency and avoid collapsed human poses. We train MDM [2] with this global representation and bone length loss, and report the performance in [Table 6](https://github.com/OmniControl24/OmniControl24.github.io/blob/main/static/images/table6.jpg). The results show that there is still a significant performance drop in performance when we switch to the global representation for text-based human motion generation. Therefore, global coordinates are not an optimal choice for our task. We added these discussions to **Appendix A.7**.  \n\n*[1] Liang, Han, Wenqian Zhang, Wenxuan Li, Jingyi Yu, and Lan Xu. \"InterGen: Diffusion-based Multi-human Motion Generation under Complex Interactions.\" arXiv preprint arXiv:2304.05684 (2023).*  \n*[2] Guy Tevet, Sigal Raab, Brian Gordon, Yonatan Shafir, Daniel Cohen-or, and Amit Haim Bermano. \u201cHuman motion diffusion model.\u201d In ICLR, 2023.*  \n\n&nbsp;\n\n```\nQ2: The concept in Fig. 4, such as the input process, requires further clarification\u2026\n```\n**A2**: We have clarified these in **Appendix A.2** in the revised version. We post them here for convenience:\nThe input process mainly consists of a CLIP-based textual embedding to encode the text prompt and linear layers to encode the noisy motion. Then the encoded text and noisy motion will be concatenated as the input to the self-attention layers. \nThe spatial encoder consists of four linear layers to encode the spatial control signals. \nThe size of f_n is (L, B, C), where L = 196 is the sequence length, B is the batch size, and C=512 is the feature dimension. \n\n&nbsp;\n\n```\nQ3: In Fig. 7, understanding why higher density leads to higher FID and Foot skating ratio\u2026\n```\n**A3**: Ideally, higher density should lead to better performance if the generated motion can accurately follow the control signal, and make corresponding adjustments to the other joints to make the motion realistic and natural. \nThis is not the case for MDM and PriorMDM since they cannot effectively modify whole-body motion according to the input control signal. \nWhen the density is higher (the constraint is stricter) and other joints are NOT adjusted effectively to compensate for the more rigidity in the control signal, they will produce unnatural results, thus leading to higher FID and higher foot skating ratio.\nOn the contrary, GMD and ours, which are specially designed for both text and spatial control signal conditions, can efficiently adjust the whole-body motion and better leverage the context information in the input signal, yielding better performance when the density is higher.\nWe have added such discussions for Fig. 7 in **Appendix A.9** of the revised version.\n\n&nbsp;\n\n```\nQ4: The Avg. error of MDM and PriorMDM is zero due to the inpainting property\u2026\n```\n**A4**: Inpainting-based methods aim to reconstruct the rest of joint motions based on the given control signals over one or more control joints. The input control signals won't be changed during this process, i.e., the output motion over the control joints remains the same as the input control signal. As a result, the Avg. error is zero.\nWe have clarified this point in **Appendix A.9** of the revised version.\n\n&nbsp;\n\n```\nQ5: Why are the proposed methods with zero error when density is low\u2026\n```\n**A5**: [Table 10](https://github.com/OmniControl24/OmniControl24.github.io/blob/main/static/images/table10.jpg) in the revised paper shows the full results of the HumanML3D dataset. The Avg. error is not zero when the density is low. The Traj. err. and Loc. err. are sometimes zeros when the density is low since the definitions of these two metrics are not strict.\nFollowing GMD, Traj. err. (50 cm) is the ratio of unsuccessful trajectories. The unsuccessful trajectories are defined as the trajectories with any keyframe whose location error exceeds a threshold (50 cm). And Loc. err. (50 cm) is the ratio of unsuccessful keyframes whose location error exceeds a threshold (50 cm). When the density is low (e.g., only have spatial control signal in one keyframe), it is easier for all samples to meet this threshold and thus achieve zero errors. We have added this explanation to **Appendix A.12** of the revised version."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission195/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700533154357,
                "cdate": 1700533154357,
                "tmdate": 1700535460146,
                "mdate": 1700535460146,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GgrIymiZOX",
                "forum": "gd0lAEtWso",
                "replyto": "snvxs0XtFT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission195/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission195/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer YHiJ (2/2)"
                    },
                    "comment": {
                        "value": "```\nQ6: In the supplementary video \u201cControl other joints\u201d, it would be better if compared with GMD\u2026\n```\n**A6**: Unfortunately, GMD can only control the pelvis as discussed in the introduction (paragraph 2), so this comparison is not feasible.\n\n&nbsp;\n\n```\nQ7: Where is the spatial control signal coming from\u2026\n```\n**A7**: In both training and evaluation, all models are provided with ground-truth trajectories as the spatial control signals. In the visualizations or video demos, the spatial control signals are manually designed. More details can be found in **Appendix A.11**."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission195/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700533213515,
                "cdate": 1700533213515,
                "tmdate": 1700533213515,
                "mdate": 1700533213515,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "rCwfkG9n12",
            "forum": "gd0lAEtWso",
            "replyto": "gd0lAEtWso",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission195/Reviewer_EL5t"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission195/Reviewer_EL5t"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a unified approach that can support controlling any joint at any time for text-driven human motion synthesis. The core design is to integrate both spatial and realism guidances to keep the generated motion faithful to the control signals while improving its reality and naturalness. Experiments show that the proposed method outperforms baselines in terms of control accuracy, motion reality, and motion diversity."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "* (1) The method is the first to control any joint at any time for human motion synthesis, which can improve the flexibility of motion generation tasks and potentially benefit downstream applications such as generating human motion on different terrains.\n\n* (2) The method design is clear and reasonable.\n\n* (3) Experiments demonstrate the effectiveness of the proposed method.\n\n* (4) The analysis for method ablations is solid.\n\n* (5) The paper is well-organized and easy to follow."
                },
                "weaknesses": {
                    "value": "* (1) The inference speed for the proposed method is much lower than baselines, which could potentially impede the method to apply to a large amount of data.\n\n* (2) In the third column of Figure 1, the authors show that the method can support a combination of control signals from different joints. However, the paper lacks quantitative analysis to further examine its performance."
                },
                "questions": {
                    "value": "I wonder whether the proposed method can support motion editing where after a motion is generated, control signals can be edited and can further adjust the motion to be not only close to the previous one but also faithful to the new control signals."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission195/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission195/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission195/Reviewer_EL5t"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission195/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698739099639,
            "cdate": 1698739099639,
            "tmdate": 1699635945045,
            "mdate": 1699635945045,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "CJFr0t7Vaq",
                "forum": "gd0lAEtWso",
                "replyto": "rCwfkG9n12",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission195/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission195/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer EL5t"
                    },
                    "comment": {
                        "value": "Thank you for your time and helpful feedback. Please refer to \u201cThe General Response to Reviewers\u201d for the reply to the issue of the inference speed. We respond below to your other comments and questions.\n\n```\nQ1: Lack of quantitative analysis of the combination of control signals from different joints\u2026\n```\n**A1**: Thank you for the suggestion! We have added such quantitative results of the combination of control signals from different joints, which are inserted into **the last row of Table 1**. We have also added more discussions in **Appendix A.4**. \nWe also post the results here:\n|              | FID \u2193 | R Precision (top 3) \u2191 | Diversity-9.503\u2192 | Foot sliding ratio\u2193 | Traj. err. \u2193 (50 cm) | Loc. err. \u2193 (50 cm) | Avg. err. \u2193 |\n|--------------|----------------|----------------------------------|------------------|---------------------|----------------------|---------------------|-------------|\n| Single Joint | 0.310          | 0.693                            | 9.502            | 0.0608              | 0.0617               | 0.0107              | 0.0404      |\n| **Combination**  | 0.624          | 0.672                            | 9.016            | 0.0874              | 0.2147               | 0.0265              | 0.0766      |\n\nThere are 57 possible combinations for six types of joints. Since running an evaluation for each of them is costly, it's impractical to evaluate all the combinations. Instead, we randomly sample one possible combination for each motion sequence for evaluation. \nThe performance is lower compared to the single-joint control (not an apple-to-apple comparison though as the ground-truths are different). Nevertheless, the results show that controlling multiple joints is harder than a single one due to the increased degrees of freedom. \n\n&nbsp;\n\n```\nQ2: Whether the proposed method can support motion editing where after a motion is generated\u2026\n```\n**A2**: It depends on whether we can re-run our pipeline. \nIf yes, we can achieve this by following these steps: (1) extract the global trajectories of all the joints from the already generated motion; (2) Edit the control signals; (3) Combine the edited control signal and the global trajectories of rest joints from generated motion as the new spatial control signal; and (4) regenerate the motion using the new control signal.\nThe edited control signal ensures the motion can be faithful to the new edition and the global trajectories make the already generated motion close to the previous one. If not, the motion cannot be edited without re-running our pipeline as far as we know. That would be interesting future work.\n\nWe would greatly appreciate it if you could let us know if these can address your question."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission195/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700533101172,
                "cdate": 1700533101172,
                "tmdate": 1700615954660,
                "mdate": 1700615954660,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4vHK39dDWJ",
                "forum": "gd0lAEtWso",
                "replyto": "CJFr0t7Vaq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission195/Reviewer_EL5t"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission195/Reviewer_EL5t"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate the author's response. It has addressed my concerns."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission195/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700644186325,
                "cdate": 1700644186325,
                "tmdate": 1700644186325,
                "mdate": 1700644186325,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]