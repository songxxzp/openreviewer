[
    {
        "title": "TEDDY: Trimming Edges with Degree-based Graph Diffusion Strategy"
    },
    {
        "review": {
            "id": "vEWdsvnJX5",
            "forum": "5RUf9nEdyC",
            "replyto": "5RUf9nEdyC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2959/Reviewer_QTmY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2959/Reviewer_QTmY"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces an intuitively derived graph sparsification technique based on edge degrees, and integrates network distillation techniques for weight sparsification. The pruning process is independent of IMP and operates in a one-shot manner."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "S1. This work demonstrates that high degree edges are less important via empirical observations, which is  significant in LTH community.\n\nS2. Although many prior efforts have explored one-shot GLT; nevertheless, this paper seems to well strike the balance between efficiency and performance."
                },
                "weaknesses": {
                    "value": "W1. The authors have avoided discussions concerning complexity, yet it appears that $T_{edge}$ might necessitate an $O(N^2)$ space complexity. Any comments on this?\n\nW2. This work appears to be a fusion of two research lines:\n- From the perspective of graph sparsification, there have already been efforts to prune edges based on edge properties and graph connectivity. The assertions made in \\cite{wang2022pruning} closely align with this study, suggesting that the removal of \"non-bridge\" edges (corresponding to the \"low-degree edges\") has minimal impact on graph information flow. Furthermore, it provides theoretical support and error bound analysis.\n- From the perspective of weight sparsification, [1, 2] have extensively explored the feasibility of using PGD for parameter pruning. I cannot see the substantial differences or new contributions in this paper.\n\nW3. While the innovations are intuitively appealing, they still lack theoretical substantiation. The assertion that high-degree edges should be pruned is not a trivial conclusion. I find the similar claim in [3], which examines graph connectivity, to be more appealing. \n\n[1] Zhou X, Zhang W, Xu H, et al. Effective sparsification of neural networks with global sparsity constraint[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021: 3599-3608.\n[2] Cai X, Yi J, Zhang F, et al. Adversarial structured neural network pruning[C]//Proceedings of the 28th ACM International Conference on Information and Knowledge Management. 2019: 2433-2436.\n[3] Wang L, Huang W, Zhang M, et al. Pruning graph neural networks by evaluating edge properties[J]. Knowledge-Based Systems, 2022, 256: 109847.\n[4] Hui B, Yan D, Ma X, et al. Rethinking Graph Lottery Tickets: Graph Sparsity Matters[J]. arXiv preprint arXiv:2305.02190, 2023.\n[5] Chen T, Sui Y, Chen X, et al. A unified lottery ticket hypothesis for graph neural networks[C]//International conference on machine learning. PMLR, 2021: 1695-1706."
                },
                "questions": {
                    "value": "The baseline performance depicted in Fig 6 and 11 appears highly questionable, displaying a significant drop compared to the results reported in [4, 5], as well as my previous replications. Perhaps the authors should consider presenting a more rigorously established and fair baseline performance.\n\nThe references can be found in the weakness part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2959/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698392943291,
            "cdate": 1698392943291,
            "tmdate": 1699636240048,
            "mdate": 1699636240048,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "FhXHKG7en3",
                "forum": "5RUf9nEdyC",
                "replyto": "vEWdsvnJX5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2959/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2959/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer QTmY (1/4)"
                    },
                    "comment": {
                        "value": "We sincerely appreciate the reviewer\u2019s helpful and constructive feedbacks. We would like to address the reviewer\u2019s concerns as below.\n\n**[On Complexity]**\n\n**W1.** In practice, $T_{edge} \\in |\\mathcal{E}|$ is only computed for the existing edges, hence it requires only $O(|\\mathcal V|+|\\mathcal{E}|)=O(N+M)$ space complexity. The computation of $T_{edge}$ involves (1) degree computation for individual nodes by row-wise sparse summation of the adjacency matrix, (2) Mean-aggregating the degree vector with (sparse) adjacency matrix to obtain $\\widetilde g$ in a sparse matrix multiplication manner, and (3) Element-wise multiplication between $\\widetilde g(v)$ of nodes $v$ in rows (i.e., source nodes) and $\\widetilde g(u)$ of nodes $u$ (i.e., destination nodes) in columns of the edge index. For better understanding, we attached the code snippet for computing $T_{edge}$ in the supplementary material (`compute_edge_score` function in `pruning.py`)."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2959/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700555677576,
                "cdate": 1700555677576,
                "tmdate": 1700556429330,
                "mdate": 1700556429330,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xnAvTfBtSn",
                "forum": "5RUf9nEdyC",
                "replyto": "vEWdsvnJX5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2959/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2959/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer QTmY (2/4)"
                    },
                    "comment": {
                        "value": "**[On Graph Sparsification]**\n\n**W2-1.** We appreciate your observation in drawing parallels between our work and the approach presented in [1]. However, we respectfully disagree with the view that our work, TEDDY, is closely related to [1]. \n\nThere exist several **clear differences and superiority** of TEDDY from the methodology proposed in [1], which we believe are worthwhile to clarify:\n\n1. **No Direct Conceptual Alignment**: Although [1] demonstrates the edge importance via spectral analysis, the concept of the bridge is **NOT strictly aligned** with our high-degree/low-degree edges. According to the Definition 2 in the paper [1], the bridge refers to an edge whose deletion increases the number of connected components in the graph, and the reverse corresponds to the non-bridge. However, the bridge in the paper does not correspond to neither low-degree edge nor high-degree edge, since there could be still a chance of the existence of common neighbors between node pairs of both high- and low-degree edges, or the between the receptive fields (multi-hop neighbors).\n2. **Training Pipeline Distinction**: More importantly, the primary distinction between our method and [1] is in the training procedure. In practice, PGEP method proposed in [1] prunes two types of edges in a fully trained network: (i) edge $(i,j)$ with different **predicted labels** $\\widehat{y}_i\\neq \\widehat{y}_j$ (specified as negative edge) and (ii) non-bridge edges. Hence, TEDDY is the first approach to achieve effective pruning by utilizing **solely the structural information** of graph data.\n    \nFurthermore, the aforementioned approach in [1] faces several practical issues. Firstly, to identify all non-bridge edges in the entire graph, one needs to execute graph algorithms such as DFS for *each node*, which can be **highly time-consuming** when it comes to large-scale datasets. Secondly, even if all non-bridge edges are identified, if the total number of non-bridge edges and negative edges is less than the target sparsity ratio, PGEP fails to achieve the desired pruning level. In consequence, this method may **not** be **available** in **high sparsity regimes**. On the other hand, given sparsity ratio for both graph and parameter, TEDDY can exactly achieve the target sparsity level in a more efficient way. Unfortunately, due to this cumbersome training procedure, we were not able to conduct the additional evaluation on [1] during the rebuttal period. Nevertheless, we will include the experiment in the final version.\n\nThus, while there are superficial similarities in the definition, conceptual alignment and underlying training procedure of TEDDY are markedly different from [1]. We believe these differences are significant and contribute to the distinctiveness and novelty of our work in the realm of GNN pruning.\n\n---\n\n**[On Parameter Sparsification]**\n\n**W2-2.** The projected gradient descent (PGD) is a well-known optimization algorithm with rich properties that has been used in various machine learning literature for a long time. The characteristics of PGD depend on the appropriate setting of the constraint set. In this regard, the two mentioned works [2, 3] indeed utilize projected gradient descent, but each has a different constraint set from ours. In the case of [2], projection is performed onto the $\\ell_1$-ball, while [3] utilizes a constraint set satisfying the RIP-like condition. Note that both [2, 3] introduce additional hyperparameters, and tuning these hyperparameters is inevitable to reach the target sparsity level. In contrast, our approach directly projects onto the $\\ell_0$-ball ***without introducing additional hyperparameters***, making it a novel attempt in sparsification."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2959/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700556044819,
                "cdate": 1700556044819,
                "tmdate": 1700559354873,
                "mdate": 1700559354873,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VlqirgjNso",
                "forum": "5RUf9nEdyC",
                "replyto": "vEWdsvnJX5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2959/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2959/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer QTmY (3/4)"
                    },
                    "comment": {
                        "value": "**[On Theoretical Substantiation]**\n\n**W3.** As we discuss in **[Graph Sparsification]**, [1] introduce the concepts of bridge/non-bridge edges using graph spectral analysis, upon which the authors propose graph sparsification methods, but it is important to note that the notion of bridge/non-bridge edges in [1] does not entirely align with the low-degree/high-degree edges in our paper. As we note in **Theoretical evidence** of remarks in Section 4, the generalization error bounds of several GNN families heavily depends on the adjacency matrix rather than model parameter or node features. For the representative example, the error bound for GCN relies on the $\\Vert A_{\\text{sym}}\\Vert_{\\infty}$ for the symmetrically normalized adjacency matrix $A_{\\text{sym}}$ and this norm can be further upper-bounded by $\\sqrt{\\frac{deg_{max}+1}{deg_{min}+1}}$. Note also that PGEP method introduced in [1] would be **computationally intractable** in practice since it should execute multiple times of graph search algorithm (such as DFS) for each node to identify bridge/non-bridge edges through a whole graph, which would be more problematic in a large-scale scenario. On the other hand, our approach TEDDY can practically identify to-be-pruned edges in a more computationally efficient way (please refer to the performance evaluation and time consumption on large-scale datasets in Section 6.2 and Appendix A.8, respectively)."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2959/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700556996253,
                "cdate": 1700556996253,
                "tmdate": 1700556996253,
                "mdate": 1700556996253,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "NBX4IABoDY",
                "forum": "5RUf9nEdyC",
                "replyto": "vEWdsvnJX5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2959/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2959/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer QTmY (4/4)"
                    },
                    "comment": {
                        "value": "**[Baseline Performance]**\n\n**Q1.** We believe that our experiment is conducted upon the fair setting. To demonstrate this, we have attached the source code for both our TEDDY and the baselines in supplementary. \n\nIt's important to note the issue of the official implementation of UGS [4] on GAT (https://github.com/VITA-Group/Unified-LTH-GNN/blob/main/NodeClassification/gnns/gat_layer.py). According to the code, edge masking is applied *prior to* the softmax normalization of attention coefficients. However, this implementation leads to a notable issue where pruned edges inevitably receive **non-zero** normalized attention coefficients, incurring the revival of pruned connections during the neighborhood aggregation phase (hence, the corresponding edge is NOT actually pruned). \n\nOur experimental results using a revised implementation revealed a significant performance decrease compared to the results reported in [4]. However, we expect the performance enhancement of TEDDY as well as baselines if we adhere to the original implementation of UGS, since in this case, the amount of information received from neighbors also increases for our method. The same issue is pertinent on WD-GLT [5] as well, and we posit that this is due to the implementation on top of the UGS. Besides, we detailed the additional reproducing issue of WD-GLT in Appendix B.3. \n\n---\n**References**\n\n[1] Pruning graph neural networks by evaluating edge properties, Knowledge-Based Systems 2022.\n\n[2] Effective Sparsification of Neural Networks with Global Sparsity Constraint, CVPR 2021.\n\n[3] Adversarial structured neural network pruning, CIKM 2019.\n\n[4] A Unified Lottery Ticket Hypothesis for Graph Neural Networks, ICML 2021.\n\n[5] Rethinking Graph Lottery Tickets: Graph Sparsity Matters, ICLR 2023."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2959/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700557247133,
                "cdate": 1700557247133,
                "tmdate": 1700559408056,
                "mdate": 1700559408056,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "QpLk8kCKEy",
            "forum": "5RUf9nEdyC",
            "replyto": "5RUf9nEdyC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2959/Reviewer_SQ4D"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2959/Reviewer_SQ4D"
            ],
            "content": {
                "summary": {
                    "value": "This paper observes the importance of low-degree edges in the graph and proposes a one-shot edge sparsification framework that leverages edge-degree to find graph lottery tickets (GLT). They achieve superior performances on diverse benchmark datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The graph lottery tickets problem is an important direction, and the solution proposed in this paper is simple and elegant by utilizing the low-degree edges. The experimental results are also convincing."
                },
                "weaknesses": {
                    "value": "Could the authors compare the consumed time of the proposed method with other baselines to further emphasize the efficiency?"
                },
                "questions": {
                    "value": "Could the authors compare the consumed time of the proposed method with other baselines to further emphasize the efficiency?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2959/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2959/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2959/Reviewer_SQ4D"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2959/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698584684674,
            "cdate": 1698584684674,
            "tmdate": 1699636239971,
            "mdate": 1699636239971,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "LDcSWlig5Y",
                "forum": "5RUf9nEdyC",
                "replyto": "QpLk8kCKEy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2959/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2959/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer SQ4D"
                    },
                    "comment": {
                        "value": "We sincerely appreciate the reviewer\u2019s helpful and constructive feedbacks. We would like to address the reviewer\u2019s concern as below.\n\n**[On Wall-clock Time]**\n\n**Q1.** In response to the the reviewer\u2019s valuable recommendation, we compared the wall-clock time analysis of TEDDY against baselines, averaged over five runs in five distinct simulations. We conducted experiments in the same GPU and GPU models for baselines and our method for each setting, to ensure fair comparisons. \n\nOverall, the results clearly substantiates that TEDDY consistently achieves a significant reduction in pruning time relative to the baselines. This is especially pronounced in GIN on the Pubmed dataset, where TEDDY outperforms WD-GLT [1] by a remarkable margin, displaying a maximum time saving of **220.72** seconds during the 15-th simulation. Similar trend is observed in large-scale datasets, where TEDDY\u2019s time consumption is nearly half that of UGS [2]. The maximum duration gap is revealed in the Reddit dataset, with TEDDY concluding the last simulation **131.84** seconds quicker than UGS in GCN. For your convenience, we attached tables below in the revised manuscript, throughout Table 3-5.\n\n---\n**References**\n\n[1] Rethinking Graph Lottery Tickets: Graph Sparsity Matters, ICLR 2023.\n\n[2] A Unified Lottery Ticket Hypothesis for Graph Neural Networks, ICML 2021."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2959/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700556558045,
                "cdate": 1700556558045,
                "tmdate": 1700560134087,
                "mdate": 1700560134087,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "5m5ARul8jC",
            "forum": "5RUf9nEdyC",
            "replyto": "5RUf9nEdyC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2959/Reviewer_mYP6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2959/Reviewer_mYP6"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes TEDDY, a novel edge sparsification framework that considers the structural information of the graph. TEDDY sparsifies graph edges based on scores designed by utilizing edge degrees, and sparsifies parameters via projected gradient descent on $l_0$ ball. In particular, sparsification of both edges and parameters can be done in one-shot and the edge sparsification part does not to consider node features. Then the paper demonstrates the effectiveness of TEDDY over iterative GLT methods with comprehensive experiments."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.\tThe paper is overall easy to follow.\n2.\tThe experiments are diverse and thorough.\n3.\tIt is an interesting observation that low-degree edges are important, which is supported both empirically and theoretically. The proposed method is thus principally designed.\n4.\tThe proposed method is one-shot and efficient (but seems still need to train a dense network first; see weaknesses).\n5.\tThe proposed method does more than just preserving the performance of vanilla dense GNNs---it actually improves the original performance in many settings. This is an impressive and interesting result."
                },
                "weaknesses": {
                    "value": "1.\tThe main part of the proposed method and some discussion of the edge degrees are not clear and confusing. \n- It is not very clear to me how T in eq.(5) is actually used (just dropping the edges with the lowest scores)? Also it seems that T computes all scores for all node pairs. How to deal the case when there is no edge between two nodes?\n- Typo in eq.(5)? Should it not include specific node v instead?\n- The analysis of the effect of edge degree in Section 4 only applies for first-order degrees, yet the paper discusses why higher-order edge degrees need to be considered heuristically via a toy example and claims TEDDY is based on \u201cmulti-level consideration of degree information\u201c in section 5.1. Then in the experiments, it never touches upon higher-order edge degree information and only discusses about first-order edge degrees in Figure 8.\n2.\tThe loss objective involves distillation (6), meaning that one still needs to train a dense network on the entire graph first, weakening the efficiency of the proposed method."
                },
                "questions": {
                    "value": "1. Have you evaluated the zero-shot performance of the proposed edge sparsification method (with dense weights)? Intuitively it seems that it might able to work in the zero-shot setting, and in this way, one can single out the effect of sparse subgraph on the boost of performance.\n2.\tWhat happens if the distillation term in (6) is removed (so one does not to try a dense network at first)? How significantly the performance would be affected? Could you provide an ablation study on this?\n3.\tWould TEDDY work if one trains on small graphs and then applies to large graphs?\n4.\tAny intuition why TEDDY is still able to improve the performance of vanilla GNNs under extreme sparsity?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2959/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2959/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2959/Reviewer_mYP6"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2959/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698629975197,
            "cdate": 1698629975197,
            "tmdate": 1699636239897,
            "mdate": 1699636239897,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "yRDKS81fdu",
                "forum": "5RUf9nEdyC",
                "replyto": "5m5ARul8jC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2959/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2959/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We sincerely appreciate the reviewer\u2019s helpful and constructive feedbacks. We would like to address the reviewer\u2019s concerns as below.\n\n**[Computation of Edge Score]** \n\n**W1-1.** $T_{edge}\\in\\mathbb{R}^{|\\mathcal{E}|}$ is only computed ***for node pairs that exist as edges***, not for all possible node pairs. In practice, this is achieved by element-wise multiplication between $\\widetilde g(v)$ of nodes $v$ in rows (i.e., source nodes) and $\\widetilde g(u)$ of nodes $u$ (i.e., destination nodes) in columns of the edge index provided in Pytorch Geometric [1]. This ensures the obtainment of scores for only real edges. To clarify this, we have attached the code snippet of the edge score calculation in supplementary material (`compute_edge_score` function in `pruning.py`). \n\n---\n**[Typo in Eq. (5)]** \n\n**W1-2.** Thank you for the reviewer\u2019s correction. We modified the typo in Eq. (5) in the revised paper.\n\n---\n**[On Degree Analysis]** \n\n**W1-3.** We apologize to the reviewer for confusion. We provide the clarification as follows. We empirically validated the necessity to consider higher-order edge degrees in Figure 17 in the appendix. As evidenced by the figure, the multi-level consideration of edge degree clearly improves the performance (specified as Ours), compared to the outcomes of pruning based solely on first-order degree information (specified as 1-hop Degree). While Figure 8 focuses on first-order degrees, the motivation behind Figure 8 is on the particular emphasis of TEDDY performing degree-aware pruning, reflecting the prior observation highlighted in Section 4. Conversely, baselines lacks the edge degree consideration, correlating with their less optimal performance as compared to our method.\n\n---\n**[On Distillation]** \n\n**W2.** Our findings, as illustrated in Figure 17 and 18, do indicate performance improvement due to the model distillation. Note that, however, our analysis puts more emphasis on the significance of 2-hop degree based edge score, $T_{edge}$, in Figure 18 in revised paper. As demonstrated in the figure, the design of edge scores is more critical, since any configuration with the baseline including distillation fails to achieve better pruning performance than our method with a sole $T_{edge}$ (represented as Ours w/o $\\mathcal L_{dt}$). \n\n---\n**[On Zero-shot Performance]**\n\n**Q1.** In the development of TEDDY, we specifically focused on a one-shot edge sparsification approach, which inherently differs from zero-shot pruning strategies. Consequently, evaluating TEDDY in a zero-shot setting falls outside the scope of our current research. This distinction aligns with the methodology used in previous studies on GNN pruning [2, 3], which typically employ pretrained models as a foundation for iterative pruning. While zero-shot sparsification offers an interesting perspective, our method gains significant efficacy on one-shot mechanism, by eliminating the need for repeated retraining each pruning level adopted in [2, 3] (Please refer to the wall-clock time analysis of TEDDY against baselines in Appendix A.8 in the revised paper). \n\n---\n**[On Distillation Component Removal]**\n\n**Q2.** In response, we present the ablation study in Figure 17. As illustrated in the figure, removing distillation term in TEDDY (specified as Ours w/o $\\mathcal L_{dt}$) consistently achieves superior performance compared to baselines, even with those integrated with distillation component.\n\nFurther ablation on all benchmark datasets is detailed in Figure 18 in the revised paper. In the figure, the performance UGS [2] attached with distillation loss remains suboptimal compared to TEDDY without $\\mathcal L_{dt}$, regardless of benchmark datasets. This underscores the significant influence of our degree-based pruning component. Additional studies to further evaluate this effect on other architectures will be included in the later revision."
                    },
                    "title": {
                        "value": "Response to Reviewer mYP6 (1/2)"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2959/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700556606005,
                "cdate": 1700556606005,
                "tmdate": 1700559093893,
                "mdate": 1700559093893,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KlxDfqIbzD",
                "forum": "5RUf9nEdyC",
                "replyto": "5m5ARul8jC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2959/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2959/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer mYP6 (2/2)"
                    },
                    "comment": {
                        "value": "**[On Smaller Graphs to Larger Graphs]**\n\n**Q3.** The reviewer's question raises a consideration about the scalability of TEDDY, which may be interpreted as its adaptability to an inductive semi-supervised node classification setting, where the fraction of the validation and test set is unobserved and larger than that of the training set. In this perspective, GNNs are trained on the partial set of the entire dataset while tested on the remaining unseen portions. \n\nIn response to the reviewer\u2019s insightful question, we expanded our experiments to assess our model\u2019s performance in inductive scenarios. We adjusted the Cora, Citeseer, and Pubmed datasets to a 20/40/40% split for training, validation, and testing, guaranteeing no exposure to the validation and test nodes as well as edges connected to them during training.\n\nIn the corresponding results, illustrated in Figure 15 and 16 of the revised manuscript, TEDDY achieves prominent pruning performance on inductive setting as well. This is especially pronounced on the Cora dataset with GCN and on the Citeseer dataset across all benchmark architectures, surpassing the accuracy of vanilla GNNs. Hence, the results affirm TEDDY\u2019s capacity to generalize from smaller to larger graphs effectively.\n\n---\n**[On Extreme Sparsity]**\n\n**Q4.** Following the reviewer\u2019s insightful question, we believe that TEDDY strategically preserves low-degree nodes while pruning primarily focuses on high-degree nodes. This approach is rooted in the inherent design of GNNs, where neighborhood aggregation is a core component. Unlike vision domain, which often relies on complex, fully connected layers, GNNs typically employ lighter pre-aggregation embeddings. This characteristic is particularly crucial in such neural networks, where low-degree nodes have fewer neighbors to gather information from. Excessive pruning that drastically reduces the degree of these nodes, or in extreme cases, isolates them (equivalent to the forward pass of MLP), can severely hamper the model's ability to classify, since the amount of information they can obtain through the message-passing scheme becomes limited. By protecting these low-degree nodes, TEDDY ensures that even under extreme sparsity, there is sufficient local structure to facilitate effective classification, hence improving the performance of vanilla GNNs.\n\n---\n**References**\n\n[1] Fast graph representation learning with PyTorch Geometric, ICLR 2019 (RLGM Workshop).\n\n[2] A Unified Lottery Ticket Hypothesis for Graph Neural Networks, ICML 2021.\n\n[3] Rethinking Graph Lottery Tickets: Graph Sparsity Matters, ICLR 2023."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2959/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700557114522,
                "cdate": 1700557114522,
                "tmdate": 1700557114522,
                "mdate": 1700557114522,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "VaihFKWavb",
            "forum": "5RUf9nEdyC",
            "replyto": "5RUf9nEdyC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2959/Reviewer_vxbf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2959/Reviewer_vxbf"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a one shot graph pruning algorithm to find Graph Lotttery Tickets (GLTs) by (i) deleting graph edges from nodes with higher degrees and (ii) sparsifying node parameters using $l_0$ regularization with Projected Gradient Descent.\nThe sparse networks obtained by the authors are shown to improve performance over existing Graph Lottery Ticket methods."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The proposed method finds GLTs using a one shot training approach outperforming existing iterative graph pruning algorithms."
                },
                "weaknesses": {
                    "value": "1. While the proposed idea of sparsifying the graph edges based on the degree information is simple and makes intuitive sense (as also shown empirically), why do the authors consider only the degree information as opposed to other metrics like spectral information of the graph or centrality which might convey more information about the graph. Is the degree information sufficient in this regard to obtain a sufficiently sparse graph in comparison with these other metrics? Recent work has shown randomly dropping graph edges can also help training, in comparison to the proposed idea in Yu et al.  [1], how does randomly pruning graph edges compare?\n\n2. The parameters sparsification method used is PGD with $l_0$ regularization. But there is no justification provided for using this particular method. There are other continuous sparsification schemes that have shown to achieve highly sparse networks in a single training run for feedforward networks like Kusupati et al. [2] and Louizos et al. [3].\n\n3. The authors use GATs and GCNs in their experiments. However, the structure of GATs allows them to inherit some degree information in the form of attention while GCNs do not. Does this difference change the performance of the proposed graph pruning criterion for either architectures?\n\n4. The results in Table 1 might be better visualized via a heatmap showing how much Graph sparsity and Weight sparsity can be achieved and if there is a tradeoff between the two.\n\n\n[1] Rong, Yu, et al. \"Dropedge: Towards deep graph convolutional networks on node classification.\" *arXiv preprint arXiv:1907.10903* (2019).\n\n[2] Kusupati, Aditya, et al. \"Soft threshold weight reparameterization for learnable sparsity.\" *International Conference on Machine Learning* 2020.\n\n[3] Louizos, Christos, Max Welling, and Diederik P. Kingma. \"Learning Sparse Neural Networks through L_0 Regularization.\" *International Conference on Learning Representations*. 2018."
                },
                "questions": {
                    "value": "1. The proposed method uses a pretraining strategy on the full graph before the pruning step. Are the parameters reinitialized at the end of the pruning stage like lottery tickets or does training continue after graph sparsification?\n\n2. The authors mention the use of multilevel degree information for the graph pruning criterion in Eq. 4. However, it is not clear if the number of hops considered is determined by the number or layers in the network or is a hyperparameter that is tuned?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2959/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2959/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2959/Reviewer_vxbf"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2959/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698695966888,
            "cdate": 1698695966888,
            "tmdate": 1699636239817,
            "mdate": 1699636239817,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ydvRYFKWzP",
                "forum": "5RUf9nEdyC",
                "replyto": "VaihFKWavb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2959/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2959/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer vxbf (1/2)"
                    },
                    "comment": {
                        "value": "We sincerely appreciate the reviewer\u2019s helpful and constructive feedbacks. We would like to address the reviewer\u2019s concerns as below.\n\n**[On Motivation]**\n\n**W1-1.** While we agree with the reviewer that considering aspects such as spectral information or centrality is an appealing approach, it is worth noting that in case of large graphs, such as Reddit or ArXiv dataset, understanding the spectral properties of the graph Laplacian/adjacency matrix is often computationally infeasible. To consider spectral information, one would require information for spectrums (e.g. eigenvectors/eigenvalues), and exploiting such information necessitates algorithms like eigendecomposition, which requires a expensive time complexity of $O(|V|^3)$. Thus, our primary goal is to design a pruning algorithm that can quickly identify to-be-pruned edges even for large graphs. As an initial step toward our goal, we focus on considering degree information and believe that we successfully demonstrated that even with only degree information, our TEDDY could effectively maintain the performance of dense graphs/GNNs through extensive experiments (Section 6). However, in line with the reviewer's suggestion, we acknowledge the potential to incorporate a more diverse range of graph information and consider it as our future avenue.\n\n---\n**[On Comparison with Random Edge Pruning]**\n\n**W1-2.** In response to the reviewer\u2019s insightful suggestion, we expanded our experiments including random dropping strategy in DropEdge [1]. In this experiment, we solely conducted a graph sparsification to directly compare the impacts of diverse edge pruning approaches, including our TEDDY. \n\nAccording to the results illustrated in Figure 19, TEDDY consistently surpasses baseline approaches including DropEdge. Notably, our method not only preserves but outperforms the accuracy of vanilla GNNs in the Citeseer and Pubmed datasets across all benchmark GNNs. Figure 19 also demonstrates that utilizing DropEdge technique results in suboptimal performance, underscoring the necessity of sophisticated pruning strategy. Interestingly, in a distinct graph sparsification scenario, UGS [2] tends to exhibit the similar performance with that of the DropEdge.\n\n---\n**[On Parameter Sparsification]**\n\n**W2.** The most crucial aspect of our PGD on $\\ell_0$-ball is its capability **for one-shot pruning at any given sparsity level**. For example, the previous approaches like UGS and WD-GLT require sensitive tuning of the $\\ell_1$ regularization parameter due to an iterative process to reach the target sparsity level, while our method enables one-shot pruning without any regularization parameter. In a similar sense, references [3], [4], as the reviewer brings to us, also necessitate tuning of the regularization parameter to achieve the target sparsity level, which may be burdensome in practice. Additionally, by encouraging sparsity during training, we can gain computational advantages in terms of actual number of operations even during the training phase.\n\n---\n**[On Architectures]**\n\n**W3.** In addressing the differences between GATs and GCNs in our experiments, it's important to note the issue of the public code of UGS [2] on GAT (https://github.com/VITA-Group/Unified-LTH-GNN/blob/main/NodeClassification/gnns/gat_layer.py). According to the code, edge masking is applied *prior to* the softmax normalization of attention coefficients. However, this implementation leads to a notable issue where pruned edges inevitably receive **non-zero** normalized attention coefficients, incurring the revival of pruned connections during the neighborhood aggregation phase. Our experimental results using a revised implementation revealed a significant performance decrease compared to the results reported in [2]. This suggests that the removal of edges *before forwarding GAT layer*, may not significantly impact the performance if the attention mechanism inherently captures degree information beneficial for pruning. Given these observations, it remains unclear whether the attention mechanism in GATs offers a substantial advantage over GCN in encoding degree information."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2959/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700555993750,
                "cdate": 1700555993750,
                "tmdate": 1700558889232,
                "mdate": 1700558889232,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rOwyzgoq49",
                "forum": "5RUf9nEdyC",
                "replyto": "VaihFKWavb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2959/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2959/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer vxbf (2/2)"
                    },
                    "comment": {
                        "value": "**[On Table 1]**\n\n**W4.** We appreciate the reviewer\u2019s helpful feedback. In response to the reviewer\u2019s suggestion, we present the results of our method on GIN under extreme sparsity as a heatmap in Figure 20, in the revised paper. The performance in each heatmap element is an average performance over 5 random seeds. \n\nOverall, the performance of TEDDY is not uniformly affected by increased sparsity. In the Cora dataset, our method demonstrates resilience to higher graph sparsity levels, surpassing the vanilla GIN\u2019s accuracy of 76.34%. This suggests that a sparser graph generated from our method has a potential to mitigate overfitting and achieve generalization. For the Citeseer dataset, the performance remains relatively stable across a range of weight sparsity levels, underscoring a robustness to parameter reduction. Furthermore, all configurations exceed the vanilla GIN's accuracy of 68.1%, strongly indicating the effectiveness of TEDDY across diverse sparsities. The similar efficacy is observed in the Pubmed dataset, where all elements surpasses the vanilla GIN\u2019s performance of 77.9%. Interestingly, we observe a trend in the Pubmed where the performance generally improves with increased sparsity, with the highest accuracy achieved under the most extreme sparsity ($p_g=p_\\theta=85$%). \n\n---\n**[On parameter reinitialization at the end of the pruning stage]**\n\n**Q1.** Our method does not adhere to the standard lottery ticket hypothesis framework, where parameters are reinitialized to their original values during the post-pruning. Instead, our approach is **agnostic** to the initial parameter values. Following the graph sparsification, we do not revert to the original initialization but continue training from the current initialized state of the parameters, which may be randomly initialized or pre-trained. Hence, TEDDY can obtain robust performance as the original GNNs, irrespective of the initial starting points of the parameters.\n\n---\n**[The number of hops considered for multi-level degree consideration]**\n\n**Q2.** In our method, as outlined in Eq. 3-5, we considered two-hop degree information to calculate edge-wise scores $T_{edge}$ for all settings. There could be other options for the choice of the number of hops, but we fixed it as two, since two hop degree consideration is sufficient to yield prominent performance. Consequently, the number of hops in our pruning criterion is a predetermined choice.\n\n---\n**References**\n\n[1] Dropedge: Towards deep graph convolutional networks on node classification,\u00a0ICLR 2020.\n\n[2] A Unified Lottery Ticket Hypothesis for Graph Neural Networks, ICML 2021.\n\n[3] Soft threshold weight reparameterization for learnable sparsity, ICML 2020.\n\n[4] Learning Sparse Neural Networks through L_0 Regularization, ICLR 2018."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2959/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700556050329,
                "cdate": 1700556050329,
                "tmdate": 1700558497373,
                "mdate": 1700558497373,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xzJ9X3l4d9",
                "forum": "5RUf9nEdyC",
                "replyto": "rOwyzgoq49",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2959/Reviewer_vxbf"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2959/Reviewer_vxbf"
                ],
                "content": {
                    "title": {
                        "value": "Response to Rebuttal"
                    },
                    "comment": {
                        "value": "I thank the author's for additional experiments and revising the manuscript. I am satisfied with their response and I will keep my score for acceptance.\n\nI wanted to follow up on the W3 above, in the case where the input to the attention is zero the output of the softmax would be nonzero. However, if the edge has been masked out, it should not be included in the aggregation, which would imply that the attention value does not matter?"
                    }
                },
                "number": 27,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2959/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700648369576,
                "cdate": 1700648369576,
                "tmdate": 1700648369576,
                "mdate": 1700648369576,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "V19p29WZnQ",
            "forum": "5RUf9nEdyC",
            "replyto": "5RUf9nEdyC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2959/Reviewer_k6Ge"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2959/Reviewer_k6Ge"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces several techniques to sparsify graph data and networks in order to build a sparse graph learning model. The important aspects are as follows:\n1) Degree-based graph sparsification to remove low-degree edges and make graphs sparse;\n2) Distillation by matching the classification logits of the model pre-trained on the whole graphs;\n3) Parameter sparsification by using a subset of parameters (i.e., Projected Gradient Descent).\n\nWhen applied to classical GNNs such as GCN/GAT/GIN, the experimental results validate the proposed method by showing some performance improvements."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The proposed methods are generally straightforward and concise. They empirically improve performance compared to the baselines. The whole process is clearly stated in Algorithm 1."
                },
                "weaknesses": {
                    "value": "1. Baseline GNN models are somewhat outdated. Traditional GNNs such as GCN/GAT/GIN are known to suffer from limited expressivity, resulting in restricted empirical performances in many cases. Several studies have done expressiveness analysis, to name a few references [3,4]. Later works, such as graph transformers and expressive GNNs [1,2,3], have proven to be more powerful and meaningful for empirical studies.\n\n2. Detailed ablation experiments are lacking, which are necessary to clearly validate the effectiveness of each component.\n\n3. The theoretical analysis concerning the motivation behind preserving low-degree edges seems somewhat disconnected from the main idea.\n\n4. Some claims regarding training efficiency and generalization performance appear to be misleading.\n\nThese points are detailed in the questions section below.\n\n\n\n[1] Recipe for a general, powerful, scalable graph transformer. NeurIPS 2022.\\\n[2] Specformer: Spectral Graph Neural Networks Meet Transformers. ICLR 2023.\\\n[3] Provably Powerful Graph Networks. NeurIPS 2019.\\\n[4] Weisfeiler and Leman Go Neural: Higher-order Graph Neural Networks. AAAI 2019."
                },
                "questions": {
                    "value": "1. Conventional GNNs are empirically seen to suffer from limited expressivity. How do the latest transformer-based GNNs or more expressive models like GPS[1], Specformer[2], and PPGN[3] compare when applying the proposed techniques? These newer models have already surpassed the empirical performance of older baselines like GCN/GAT/GIN. Validating the proposed techniques on these latest methods rather than older models would make the arguments more convincing.\n\n2. About Figure 1: What is the precise step for removing high-degree or low-degree edges? Given a graph sparsity value, how exactly is the subgraph generated? Is random sampling involved in this process? If so, could you provide the results of multiple runs (e.g., mean and standard deviation) to separate the training-related noise?\n\n3. Ablation Studies: Could you provide detailed ablation studies on the effectiveness of the proposed three components? a) graph sparsification by removing edges; b) model distillation; c) parameter sparsification with projected gradient descent. It seems that these components could be applied separately to any of the baseline training methods (UGS/WD-GLT) in empirical studies.\nIn Figure 13, there are some comparisons made. Graph sparsification doesn\u2019t seem to improve empirical performance at all. There are no curves representing UGS/WD-GLT without distillation loss. Although considering multi-hop subgraphs appears to be effective in a way, this concept is not new in graph learning literature (e.g., k-WL GNNs [4]).\n\n4. The theoretical analysis involving the upper bound of the symmetrically normalized adjacency matrix is somewhat unclear.\na) Concerning the mentioned analysis, how realistic is the assumption of the Lipschitz constant being present?\nb) If the aforementioned assumptions hold true for the specified models, how plausible is it that removing low-degree edges actually increases the term $\\\\frac{\\\\text{deg}\\_\\text{max} + 1}{\\\\text{deg}\\_\\text{min} + 1}$? This aspect may be specific to datasets and could possibly be numerically simulated.\nc) Conversely, removing high-degree edges appears to reduce the generalization gap by lowering $\\\\text{deg}\\_\\text{max}$, according to the preceding analysis. What is the reason this aspect is not emphasized or motivated more within the discussion?\n\n4. In Eq. (5), the edge score is always symmetric. How does this approach apply to directed graphs?\n\n5. How crucial is the distillation training? It appears to undermine the purpose of utilizing sparse graphs, as there is a necessity to initially train the model on the full graphs. This approach also seems to contradict the claim of \"a single training,\" a statement that is reiterated numerous times throughout the paper.\n\n6. The paper asserts that \"TEDDY significantly surpasses conventional iterative approaches in generalization.\" Could you provide more details to substantiate this claim? Which sections or aspects of the experiments corroborate this assertion regarding generalization?\n\n7. The paper claims that the employed PGD training saves computation time compared to the iterative approach. Could you provide additional results to validate this claim, such as comparing training hours using the same GPU hardware?\n\n[1] Recipe for a general, powerful, scalable graph transformer. NeurIPS 2022.\\\n[2] Specformer: Spectral Graph Neural Networks Meet Transformers. ICLR 2023.\\\n[3] Provably Powerful Graph Networks. NeurIPS 2019.\\\n[4] Weisfeiler and Leman Go Neural: Higher-order Graph Neural Networks. AAAI 2019."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2959/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2959/Reviewer_k6Ge",
                        "ICLR.cc/2024/Conference/Submission2959/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2959/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698734117163,
            "cdate": 1698734117163,
            "tmdate": 1700729435698,
            "mdate": 1700729435698,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "UHaeL4liDX",
                "forum": "5RUf9nEdyC",
                "replyto": "V19p29WZnQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2959/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2959/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer k6Ge (1/8)"
                    },
                    "comment": {
                        "value": "We sincerely appreciate the reviewer\u2019s helpful and constructive feedbacks. We would like to address the reviewer\u2019s concerns as below.\n\n**[On Experiments for Recent GNNs]**\n\n**Q1.** In response to the reviewer\u2019s insightful suggestion, we expanded our experiments to encompass recent Transformer-based GNN architectures. While integrating TEDDY with these advanced models, we encountered several issues with some architectures suggested by the reviewer. \n\nTo begin with, PGNN [1] utilizes a **dense** adjacency matrix in the input tensor, resulting in an $N\\times N$ matrix for a graph with $N$ nodes. In general, adjacency matrix in GNNs are encoded as a **sparse** format, where the computation can be highly dependent on the number of edges. This is where the graph sparsification can offer a benefit. However, in the case of dense matrix representations as PGNN, both zero and non-zero elements are treated equally in terms of storage and computational overhead. Consequently, the inherent characteristics of PGNN's dense matrix approach do not align well with the advantages offered by graph sparsification. \n\nRegarding GraphGPS [2], its primary efficacy is demonstrated in graph-level tasks, diverging from our focus on node classification. Similarly, Specformer [3], while benchmarked against GraphGPS in graph-level tasks, was evaluated against other baselines in node-level tasks. In our experiments, we observed that GraphGPS did not yield satisfactory performance on Cora and Citeseer compared to classic GNNs.\n\nConsequently, we explore alternative graph transformers, specifically UniMP [4], NAGphormer [5], and Specformer [3], which have demonstrated strong performance in node classification without significant degradation on our benchmark datasets. However, both Specformer and NAGphormer replace the input adjacency matrix with eigenvectors and eigenvalues from eigendecomposition. The baselines\u2019 differentiable mask for the adjacency matrix requires backpropagation through eigenvectors and eigenvalues, introducing a practical challenge with high complexity of $O(N^3)$ for the number of node $N$ per iteration. Given these constraints, we conducted the evaluation of these two transformer architectures only on TEDDY, whereas including experiments on TEDDY and all baselines for UniMP [4], which has also been recognized as a graph transformer benchmark in [6].\n\nAs depicted in Figure 13 and 14, TEDDY accomplishes stable pruning performance, surpassing all baselines when equipped with UniMP as a backbone. In particular, UGS and WD-GLT show significant degradation as the sparsity increases, whereas the performance of TEDDY is stable across all benchmark datasets. Note that differentiable mask approach from baselines may prune edges more than pre-defined ratio, due to the possibility of multiple mask elements having the same value. Our method also yields decent performance in NAGphormer and Specformer, notably in NAGphormer on the Pubmed dataset, with a marginal accuracy loss of 0.56. Overall, these results demonstrate TEDDY's versatility across diverse foundational architectures."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2959/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700557098682,
                "cdate": 1700557098682,
                "tmdate": 1700557098682,
                "mdate": 1700557098682,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PLNUGwahif",
                "forum": "5RUf9nEdyC",
                "replyto": "V19p29WZnQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2959/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2959/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer k6Ge (2/8)"
                    },
                    "comment": {
                        "value": "**[On Figure 1]**\n\n**Q2.** In the analysis of Figure 1, we define the edge score of each edge $(i,j)$ as $T_{edge}(i, j) =(|\\mathcal N(i)|+|\\mathcal N(j)|)/{2}$, where $\\mathcal N(k)$ denotes the set of neighboring nodes connected to node $k$. For our experiments, we conduct 20 simulations where GNNs are trained after pruning edges based on their scores. Specifically, given a graph sparsity ratio $p_g$ for each simulation, we pruned $p_g\\%$ of edges having the highest edge score $T_{edge}$ in the high degree pruning scenario (represented as blue lines in the figure), and pruned edges with the lowest scores in the low-degree scenario (represented as orange lines in the figure). Hence, our TEDDY does not involve any stochastic procedures such as random sampling. \n\nHowever, to address the reviewer\u2019s suggestion, we conduct additional simulations with 5 different random seeds (0 to 4) and reported the average performance in Table 2. In the table, **deg_low** refers to pruning based on the lowest degree, while **deg_high** corresponds to the highest degree-based pruning. The results clearly demonstrate that pruning low-degree edges, i.e., low $T_{edge}$, can significantly deteriorate the performance, especially in GAT on the Pubmed dataset, where the performance between **deg_low** and **deg_high** reaches the maximum gap of 24.1% in the final simulation. For convenience, we attach the below tables in Table 2 in the revised manuscript.\n\n| Simulations |        | 1-st | 1-st  |  5-th | 5-th  | 10-th| 10-th | 15-th | 15-th | 20-th | 20-th |\n|-------------|--------|--------------|---------------|--------------|---------------|---------------|----------------|---------------|----------------|---------------|----------------|\n| GNNs        | Dataset|       deg_low       |      deg_high         |       deg_low       |          deg_high     |       deg_low        |          deg_high      |      deg_low         |         deg_high       |     deg_low          |       deg_high         |\n| GCN         | Cora   | 80.38 \u00b1 0.19 | **81.28 \u00b1 0.64** | 77.66 \u00b1 0.41 | **77.92 \u00b1 0.34** | 75.36 \u00b1 0.42 | **77.68 \u00b1 0.32** | 74.24 \u00b1 0.16 | **76.02 \u00b1 0.43** | 69.54 \u00b1 0.32 | **72.62 \u00b1 0.32** |\n|             | Citeseer | 70.44 \u00b1 0.21 | **70.88 \u00b1 0.28** | 67.0 \u00b1 0.27 | **70.60 \u00b1 0.18** | 64.78 \u00b1 0.31 | **67.58 \u00b1 0.72** | 63.46 \u00b1 0.42 | **67.72 \u00b1 0.37** | 63.22 \u00b1 0.51 | **65.70 \u00b1 0.56** |\n|             | Pubmed | 78.32 \u00b1 0.19 | **79.18 \u00b1 0.12** | 76.66 \u00b1 0.79 | **77.66 \u00b1 0.27** | **77.52 \u00b1 0.28** | 77.32 \u00b1 0.20 | **77.72 \u00b1 0.17** | 76.74 \u00b1 0.10 | 75.20 \u00b1 0.20 | **76.54 \u00b1 0.08** |\n| GAT         | Cora   | 77.64 \u00b1 1.33 | **79.72 \u00b1 1.06** | 68.34 \u00b1 1.26 | **78.50 \u00b1 1.16** | 56.72 \u00b1 0.48 | **72.44 \u00b1 1.10** | 50.12 \u00b1 0.93 | **67.74 \u00b1 0.92** | 44.78 \u00b1 0.80 | **51.64 \u00b1 1.67** |\n|             | Citeseer | 69.48 \u00b1 1.36 | **70.96 \u00b1 0.75** | 57.02 \u00b1 0.64 | **66.86 \u00b1 0.63** | 42.68 \u00b1 2.14 | **59.74 \u00b1 1.64** | 35.40 \u00b1 1.50 | **49.86 \u00b1 1.82** | 28.40 \u00b1 0.97 | **43.24 \u00b1 1.43** |\n|             | Pubmed | **78.40 \u00b1 0.61** | 78.14 \u00b1 1.01 | 71.66 \u00b1 0.59 | **76.32 \u00b1 0.55** | 58.48 \u00b1 0.85 | **74.52 \u00b1 0.56** | 49.52 \u00b1 0.64 | **72.12 \u00b1 0.64** | 44.10 \u00b1 0.57 | **68.20 \u00b1 0.61** |\n| GIN         | Cora   | 74.86 \u00b1 1.14 | **76.48 \u00b1 1.66** | 70.70 \u00b1 1.13 | **76.84 \u00b1 1.81** | 66.70 \u00b1 0.33 | **74.70 \u00b1 2.32** | 62.72 \u00b1 1.38 | **72.96 \u00b1 0.55** | 58.82 \u00b1 1.99 | **70.28 \u00b1 0.99** |\n|             | Citeseer | 67.52 \u00b1 0.99 | **68.78 \u00b1 0.98** | 63.80 \u00b1 1.48 | **69.16 \u00b1 0.96** | 62.32 \u00b1 1.14 | **68.46 \u00b1 0.48** | 60.60 \u00b1 1.27 | **66.92 \u00b1 0.27** | 59.48 \u00b1 2.25 | **65.64 \u00b1 1.09** |\n|             | Pubmed | 75.64 \u00b1 0.72 | **77.96 \u00b1 0.36** | 75.12 \u00b1 0.66 | **77.38 \u00b1 0.27** | 73.68 \u00b1 1.26 | **77.44 \u00b1 0.10** | 70.74 \u00b1 1.39 | **74.88 \u00b1 0.62** | 70.88 \u00b1 2.72 | **73.30 \u00b1 0.11** |"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2959/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700557255930,
                "cdate": 1700557255930,
                "tmdate": 1700557255930,
                "mdate": 1700557255930,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6QAECTiDAH",
                "forum": "5RUf9nEdyC",
                "replyto": "V19p29WZnQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2959/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2959/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer k6Ge (3/8)"
                    },
                    "comment": {
                        "value": "**[On Ablation Study]**\n\n**Q3-1-a. (On graph sparsification)** We sincerely appreciate the reviewer\u2019s constructive feedback. However, we would like to clarify that the purpose of graph sparsification is to **preserve the original GNN\u2019s accuracy**, rather than improving it as a form of regularization. Hence, we respectfully disagree in that our graph pruning method doesn\u2019t seem to improve empirical performance at all. And in fact, according to a sole graph sparsification performance comparison between our pruning approach and baselines, illustrated in Figure 19, our approach consistently achieves superior performance to baselines. Thus, this clearly substantiates that our graph sparsification indeed well-preserves, and in most cases even surpasses (**7 out of 9 settings** in Figure 19) the original GNNs\u2019 performance.\n\nFurthermore, following the reviewer\u2019s thoughtful opinion that components TEDDY can be applied separately, we provide comprehensive ablation studies for diverse combinations of our components. Utilizing GAT as a backbone architecture, we evaluated **7 distinct scenarios** detailed in Figure 18: \n\n(Scenario 1) TEDDY with all components (specified as Ours),\n\n(Scenario 2) TEDDY without distillation loss (specified as Ours w/o $\\mathcal L_{dt}$),\n\n(Scenario 3) TEDDY with magnitude-based weight mask pruning (specified as Ours w/ $m_\\theta$),\n\n(Scenario 4) TEDDY without weight sparsification, i.e., our graph sparsification method with dense weights (specified as Ours w/o weight spar.)\n\n(Scenario 5) UGS [7] with all components (specified as UGS),\n\n(Scenario 6) UGS with distillation loss (specified as UGS w/ $\\mathcal L_{dt}$),\n\n(Scenario 7) UGS with $\\ell_0$-based projected gradient descent (specified as UGS w/ $\\ell_0$ PGD).\n\n---\n\n**Q3-1-b. (On distillation loss)** The results, as depicted, do indicate performance improvement due to the model distillation. However, the results also clearly demonstrate that even when UGS is augmented with or without $\\mathcal L_{dt}$ (scenario 6 and 5), it fails to match the performance of our framework **without** $\\mathcal L_{dt}$ (scenario 2). Similar results regarding WD-GLT [8] are illustrated in Figure 17. This demonstrates that our edge sparsification method rooted in low edge degree possesses more significant impact than the distillation component in GNN pruning. \n\n---\n\n**Q3-1-c. (On weight sparsification)** Furthermore, our method including $\\ell_0$ PGD (scenario 1) shows comparable performance to that of TEDDY with dense weights (scenario 4), across varying pruning simulations, underscoring the robustness of our weight sparsification method. Additional studies to further evaluate this effect on other architectures will be included in the later revision.\n\n---\n\n**[On Novelty of Multi-hop Consideration]**\n\n**Q3-2.** We acknowledge that multi-hop subgraph analysis is a well-established concept in graph learning. Nevertheless, the application of leveraging **a sole degree information** in multi-level consideration in node classification task has been under-explored. The referenced work [9] provided mainly addresses the **performance enhancement** on graph-level tasks by incorporating a broader range of neighborhood **features** and structural complexities, rather than concentrating on the pruning aspect of GNNs to **preserve the original performance**. Our approach, which specifically considers multi-hop **edge degree** for GNN pruning, introduces a novel angle within this domain."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2959/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700557353676,
                "cdate": 1700557353676,
                "tmdate": 1700559253547,
                "mdate": 1700559253547,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cEjnRw8X1C",
                "forum": "5RUf9nEdyC",
                "replyto": "V19p29WZnQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2959/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2959/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer k6Ge (4/8)"
                    },
                    "comment": {
                        "value": "**[On Theoretical Claim]**\n\n**Q4.** Note that the generalization error bound for GNN [10] is not our contribution, but we can offer theoretical evidences for our TEDDY based on the results in [10]. In fact, the most challenging component in computing the Lipschitz constant for practical GNN families is the ***non-smooth ReLU function**.* Theoretically, [10] considered a slightly relaxed ReLU function that makes ReLU continuously differentiable. In such cases, since every part of the function becomes smooth, it is possible to compute the Lipschitz constant. It is anticipated that the Lipschitz constant of the original GNN can also be approximated to some extent through this relaxed activation function. Regarding the quantity $\\frac{deg_{max}+1}{deg_{min}+1}$, the action of removing low-degree ***edges (not nodes)*** tends to increase the generalization bound by consistently making $deg_{min}$ monotonically decreasing, thereby inducing worse generalization. Similarly, removing high-degree ***edges*** also consistently makes $deg_{max}$  monotonically decreasing. In contrast to the previous case, this tends to reduce the generalization gap, encouraging better generalization."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2959/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700557622026,
                "cdate": 1700557622026,
                "tmdate": 1700557622026,
                "mdate": 1700557622026,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PPVqWBpfDN",
                "forum": "5RUf9nEdyC",
                "replyto": "V19p29WZnQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2959/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2959/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer k6Ge (5/8)"
                    },
                    "comment": {
                        "value": "**[On Eq. (5)]**\n\n**Q5.** As we explicitly stated in Section 3.1, TEDDY is specifically tailored for *undirected graphs*, and thus does not directly address directed graphs. Although $T_{edge}$ is symmetric in directed graphs as the reviewer pointed out, directed graphs were out of scope for the design of $T_{edge}$."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2959/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700557699364,
                "cdate": 1700557699364,
                "tmdate": 1700557699364,
                "mdate": 1700557699364,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "AKzjydx4wz",
                "forum": "5RUf9nEdyC",
                "replyto": "V19p29WZnQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2959/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2959/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer k6Ge (6/8)"
                    },
                    "comment": {
                        "value": "**[On distillation]**\n\n**Q6.** We thank the reviewer for raising an important point. Our findings, as illustrated in Figure 17 and 18, do indicates an enhancement in performance due to the model distillation. However, our analysis emphasizes the significance of 2-hop degree based edge score, $T_{edge}$, in Figure 18 in revised paper. As demonstrated in the figure, the design of edge scores is more critical, since any configuration with the baseline including distillation fails to achieve better pruning performance than our method with a sole $T_{edge}$ (represented as Ours w/o $\\mathcal L_{dt}$). \n\nMoreover, the use of pretrained models is a common initial step in iterative pruning approaches as well, since they fully exploit the dense graph and GNNs at the initial pruning stage. Our method is tailored for practical scenarios in mind: for example, facilitating the deployment of GNNs on local devices with performance equivalent to fully trained models on central servers. Hence, the term **\u201csingle training\u201d** in our paper refers to the elimination of the iterative cycle of retraining GNNs for each pruning level. Instead, our TEDDY accomplishes the sparsification in a single training phase when given the sparsity levels and trained GNNs, promoting a significant efficiency, as displayed in Table 3-5 in the revised version."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2959/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700557745912,
                "cdate": 1700557745912,
                "tmdate": 1700557745912,
                "mdate": 1700557745912,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "S1GiM6Rxsp",
                "forum": "5RUf9nEdyC",
                "replyto": "V19p29WZnQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2959/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2959/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer k6Ge (7/8)"
                    },
                    "comment": {
                        "value": "**[On Superiority over Iterative Methods]**\n\n**Q7.** Our experimental results demonstrate superior performance compared to baselines across various graph sizes, underscoring our method's robust generalization ability. This is particularly evident in Section 6.1, where TEDDY, when utilizing GAT as the base architecture, shows a performance enhancement ranging from 12.8% to 20.4% over the optimal performances of baseline models. Additionally, TEDDY consistently exhibits stable performance across different datasets, as highlighted in our inductive semi-supervised node classification experiments. For instance, on the Pubmed dataset, TEDDY maintains decent accuracy despite the significant scarcity in the ratio of training nodes (20% for training vs. 80% for inference). These results confirm TEDDY's outstanding generalization abilities, marking it as a superior choice across diverse datasets compared to conventional iterative baselines."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2959/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700557787119,
                "cdate": 1700557787119,
                "tmdate": 1700557787119,
                "mdate": 1700557787119,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VAJNmARSfA",
                "forum": "5RUf9nEdyC",
                "replyto": "V19p29WZnQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2959/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2959/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer k6Ge (8/8)"
                    },
                    "comment": {
                        "value": "**[On Efficiency of PGD]**\n\n**Q8.** The existing iterative approaches to parameter sparsification involve the full training of an $\\ell_1$-regularized mask to identify crucial parameter coordinates. After training, $p_\\theta$% of coordinates with the smallest signals in the mask are zeroed out, and the obtained mask is used to train a subnetwork to obtain the graph lottery ticket. Therefore, iterative methods inevitably require two rounds of training to obtain the mask, and achieving the desired target sparsity ratio necessitates multiple times of training process. In contrast, TEDDY can obtain sparse parameters in a single training pass using Projected Gradient Descent on the $\\ell_0$-ball for any parameter initialization. Furthermore, since the parameters are already sparse during the training process, TEDDY can also reduce the training time. To validate this, we conduct wall-clock time comparisons for parameter sparsification across each method. Toward this, we use the original dense adjacency matrix while pruning only model parameters for fair comparisons. For your convenience, we attached the below tables in Table 6-8 in the revised paper.\n\n---\n\n**References**\n\n[1] Provably Powerful Graph Networks, NeurIPS 2019.\n\n[2] Recipe for a general, powerful, scalable graph transformer, NeurIPS 2022.\n\n[3] Specformer: Spectral Graph Neural Networks Meet Transformers, ICLR 2023.\n\n[4] Masked Label Prediction: Unified Message Passing Model for Semi-Supervised Classification, IJCAI 2021.\n\n[5] NAGphormer: A Tokenized Graph Transformer for Node Classification in Large Graphs, ICLR 2023.\n\n[6] Towards Deep Attention in Graph Neural Networks: Problems and Remedies, ICML 2023.\n\n[7] A Unified Lottery Ticket Hypothesis for Graph Neural Networks, ICML 2021.\n\n[8] Rethinking Graph Lottery Tickets: Graph Sparsity Matters, ICLR 2023.\n\n[9] Weisfeiler and Leman Go Neural: Higher-order Graph Neural Networks, AAAI 2019.\n\n[10] Towards Understanding Generalization of Graph Neural Networks, ICML 2023."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2959/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700557831909,
                "cdate": 1700557831909,
                "tmdate": 1700558510748,
                "mdate": 1700558510748,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7DYeo3HgaU",
                "forum": "5RUf9nEdyC",
                "replyto": "UHaeL4liDX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2959/Reviewer_k6Ge"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2959/Reviewer_k6Ge"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your feedback. Regarding Figures 13 and 14, there are just minor variations in the y-axis of the performance curves. In similar studies, such as UniMP [4], NAGphormer [5], and Specformer [3], the standard deviation is typically reported, providing a clearer understanding of performance variability. The slight differences observed at different graph sparsity in Figures 13 and 14 could potentially be explained by training noise. Could you offer additional insights on this matter?"
                    }
                },
                "number": 24,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2959/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700635567895,
                "cdate": 1700635567895,
                "tmdate": 1700635567895,
                "mdate": 1700635567895,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DVe1NXGRqC",
                "forum": "5RUf9nEdyC",
                "replyto": "6QAECTiDAH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2959/Reviewer_k6Ge"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2959/Reviewer_k6Ge"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response. Could you provide insights on the isolated impact of the proposed edge sparsification technique? How does the performance of the proposed edge pruning, as outlined in Equation (5), compare to basic high-degree pruning (as shown in Figure 1, Figure 10, and Table 2) when both the distillation loss and weight sparsification are disabled?\n\nBased on the observations from the top of Figure 18, it appears that weight sparsity has a minimal impact on performance. Additionally, the effectiveness of the distillation loss has been shown to be quite significant. Considering these factors, it would be interesting to explore the specific performance benefits attributable to the proposed edge sparsification technique. This would help in understanding the extent to which the edge pruning method, as a standalone component, contributes to the overall performance gains."
                    }
                },
                "number": 25,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2959/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700636573054,
                "cdate": 1700636573054,
                "tmdate": 1700636573054,
                "mdate": 1700636573054,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DaHSqUwKjb",
                "forum": "5RUf9nEdyC",
                "replyto": "V19p29WZnQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2959/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2959/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer k6Ge"
                    },
                    "comment": {
                        "value": "**[Additional Insights on Training Noise]**\n\nIn response to the reviewer\u2019s constructive feedback, we reported average pruning performance of our TEDDY and baselines across all sparsities, represented in Figure 13 and 14 in the revised manuscript. To ensure fair comparisons, we adopted uniform random seeds to all methods for each setting. Owing to the constraints of the rebuttal period, we present performance results of NAGphormer [1] and Specformer [2] on Cora and Citeseer datasets, averaged over 3 runs (Note that the eigendecomposition of Pubmed dataset required prohibitive time consumptions). Meanwhile, we adopted uniform five random seeds for UniMP [3]. \n\nAs depicted in the figures, our TEDDY still accomplishes prominent performance, surpassing all baselines when equipped with UniMP as a backbone. In particular, UGS [4] and WD-GLT [5] exhibit severe unstability as the sparsity increases, whereas the performance of TEDDY remains stable across all simulations with notably small standard deviations. Our method also achieves decent performance in NAGphormer and Specformer, notably in NAGphormer on the Cora dataset, with considerable performance enhancement over the original result. Hence, the results clearly demonstrate our method\u2019s versatility and robustness across diverse foundational architectures.\n\n---\n**References**\n\n[1] NAGphormer: A Tokenized Graph Transformer for Node Classification in Large Graphs, ICLR 2023.\n\n[2] Specformer: Spectral Graph Neural Networks Meet Transformers, ICLR 2023.\n\n[3] Masked Label Prediction: Unified Message Passing Model for Semi-Supervised Classification, IJCAI 2021.\n\n[4] A Unified Lottery Ticket Hypothesis for Graph Neural Networks, ICML 2021.\n\n[5] Rethinking Graph Lottery Tickets: Graph Sparsity Matters, ICLR 2023."
                    }
                },
                "number": 29,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2959/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700726008607,
                "cdate": 1700726008607,
                "tmdate": 1700727777390,
                "mdate": 1700727777390,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "sRsgK0c3ID",
                "forum": "5RUf9nEdyC",
                "replyto": "gLchkKWwGw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2959/Reviewer_k6Ge"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2959/Reviewer_k6Ge"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for addressing most of my technical concerns in your responses. I'd like to update my rating to \"5: marginally below the acceptance threshold\".\n\nHowever, I am still concerned about the presentation of the distillation effect in the paper. The experimental results clearly indicate that distillation is a crucial and significant component. Yet, this aspect is not mentioned until P6, Section 5.2. The earlier sections, up to Section 5.2, primarily focus on edge sparsification, without even a mention of 'distillation'. This approach raises serious doubts for me regarding the structure and narrative style of the paper. \n\nAs reviewer mYP6 also points out, \n> one still needs to train a dense network on the entire graph first\n\nThis is a somewhat significant shortcoming in terms of efficiency. However, it is somewhat hidden from the reader and is not made visible until Page 6 and Section 5.2. To the reviewer, for a paper that emphasizes empirical performance, it's more critical to clearly identify the most effective components in the main text, rather than adopting a storytelling approach. Being straightforward about the key elements contributing to performance should be prioritized over narrative-oriented writing."
                    }
                },
                "number": 32,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2959/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700729418712,
                "cdate": 1700729418712,
                "tmdate": 1700729418712,
                "mdate": 1700729418712,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]