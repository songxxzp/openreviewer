[
    {
        "title": "Mildly Constrained Evaluation Policy for Offline Reinforcement Learning"
    },
    {
        "review": {
            "id": "kqYgQ0FqhV",
            "forum": "iMRhuFS0Uz",
            "replyto": "iMRhuFS0Uz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4218/Reviewer_WVL4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4218/Reviewer_WVL4"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on the trade-off between value estimation stability and performance improvement caused by in offline RL. The authors propose a new algorithm by introducing a new mildly constrained policy to obtain both stable value estimation and good evaluation performance. The authors test their method on D4RL mujoco tasks to verify its effectiveness."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper is clearly written and easy to follow.\n- The trade-off between stability of value learning and policy improvement is important and not well-studied by previous work.\n- The experiment part adequately explains how the policy constraint influences the evaluation and policy evaluations, which validates the motivation of this paper."
                },
                "weaknesses": {
                    "value": "- The authors claim that a mild-constrained evaluation policy improves the final performances, but its effectiveness is questionable.\n    - The improvement may be attributed to the policy constraint strengths of original method are not well selected. E.g., in fig.4, the performance of TD3BC on hopper-m can achieve >80 with $\\alpha=10$. If we use this value as the baseline, then the improvement of the proposed method is actually limited. This also happens on other two settings plotted in fig.4. Meanwhile, if the original policy constraint strengths are suitable, a milder constraint in MCEP may actually degrade the performances (e.g., TD3+BC on medium-expert tasks).\n    - For DQL and DQL-MCEP, there are no remarkable differences on most tasks. So why MCEP is effective on some baselines but helps little on others?\n    - Based on the above analysis, we can find whether the additional evaluation policy improves the performances heavily depends on the strength of policy constraint in original baseline. MCEP can achieve better with better hyper-parameter, which is actually infeasible in offline RL setting, however.\n    - Although the authors give an ablation study in sec. 5.4, the improvement of MCEP is not very significant and the results seem to be inconsistent with previous figures and tables (See questions).\n- The authors only test their methods on mujoco tasks. To comprehensive verify the advantages of proposed method, more experiment results (e.g., on maze/kitchen/adroit) are needed. \n\nMinor issues:\n- There are two `3)` in the first paragraph of sec.5.\n- In the first paragraph in page 9, $\\\\tilde{\\\\alpha}, \\\\tilde{\\\\lambda}$ instead of $\\\\tilde{alpha}, \\\\tilde{lambda}$."
                },
                "questions": {
                    "value": "- Which level are you using in fig.7? If it is \"-medium\", why are the performances of TD3BC with $\\alpha=2.5$ and TD3BC-MCEP very different from the values reported in table 1? The hyper-parameters should be the same."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4218/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4218/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4218/Reviewer_WVL4"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4218/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697773189166,
            "cdate": 1697773189166,
            "tmdate": 1699636388833,
            "mdate": 1699636388833,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "nqptRT2MkB",
                "forum": "iMRhuFS0Uz",
                "replyto": "kqYgQ0FqhV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4218/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4218/Authors"
                ],
                "content": {
                    "title": {
                        "value": "A Response to Reviewer WVL4"
                    },
                    "comment": {
                        "value": "We thank the reviewer WVL4 for their review of our manuscript.\n\nWe appreciate reviewer WVL4 for investigating the experiment details but we find one overlooked setting of our experiment could guide to some misunderstanding.\n\n>The improvement may be attributed to the policy constraint strengths of the original method are not well selected\u2026\n\nWe use the setting of \u201csimilar hyparameters for all locomotion domains\u201d as mentioned in the paper (line 3, Section 5.3 *\u201cFor D4RL datasets, similar hyperparameters are used\u201d*). This setting is widely used in current offline RL literature (Fujimoto et al., 2020, Kostrikov et al., 2021, Hansen-Estruch et al., 2023, Wang et al., 2023, etc). Table 1 reports the best performance under this setting, both for the baseline and for our methods. With this evaluation protocol, **the hyperparameters for original methods are well-selected.** \n\nIn, fig.4, we investigate multiple strengths for the original methods. It is possible that one constraint strength suits a specific task best but does not derive the best general performance. We have an experiment that compares task-wise best hyper-parameters, which can be found in Section A.3 of the Appendix, where in 7 out of 9 tasks, the MCEP outperforms the original algorithm.\n\n>For DQL and DQL-MCEP, there are no remarkable differences on most tasks\u2026\n\nOne of the motivations for using MCEP is to reduce the Bellman estimate error brought by OOD actions. Conventional methods use a Gaussian policy that raises many OOD actions under multi-modal distribution (Cai et al., 2022, Wang et al., 2023, Hansen-Estruch et al., 2023). MCEP significantly eliminates these OOD actions by using a target policy of strong constraint, which leads to significant performance improvement. \n\nDQL utilizes diffusion policies that effectively learn the multi-modal distribution. Hence the Bellman estimate error is reduced. In this case, MCEP can still improve its general performance by optimizing the evaluation policy towards higher-value OOD actions further.\n\n>Based on the above analysis, we can find whether...\n\nThe performance of MCEP depends on the stable value estimate, which is related to the hyperparameter of the base algorithm. In our experiments, we use paper-recommended hyperparameters for the target policy.\n\nIn Section A.3 (fig. 9), under the recommended hyperparameter (same value for all tasks) for the target policy, only tuning the hyperparameter for the MCEP enables it to outperform the base algorithm that uses task-specific best hyperparameter in 7/9 tasks. This means that it is not necessary to have the best hyperparameter of the base algorithm for the MCEP to achieve better performance.\n\n>Although the authors give an ablation study in sec. 5.4, the improvement of MCEP is not very significant and the results seem to be inconsistent with previous figures and tables (See questions).\n\nUnder the setting of \"similar hyperparameters for all locomotion tasks\", all experiment results stay consistent. The improvement of MCEP is significant as under a widely used evaluation protocol, it empowers conventional algorithms to achieve near SOTA performance and provide a further improvement for the SOTA method.\n\n>The authors only test their methods on mujoco tasks\u2026\n\nAs this is asked by reviewer **r8JF, VtJK, WVL4 (3/4 of the reviewers),** we answer this question in the general review that can be found at the top of this page.\n\n>Which level are you using in fig. 7? \u2026\n\nWe apologize for the confusion. Fig. 7 presents the average performance of all versions of datasets for each domain. E.g. the bars above the \u201chalfcheetah\u201d presents the average scores of the \u201cmedium\u201d, \u201cmedium-replay\u201d and \u201cmedium-expert\u201d datasets. With this setting, it is reasonable for Fig.7 and Table 1 to have different scores. We add this explanation in Section 5.4 \u201cThe scores for different datasets are grouped for each domain.\u201d\n\nPlease let us know whether our answers address all your concerns. We are more than happy to answer more if you have further questions and take suggestions to further improve our work.\n\n## References\n\nFujimoto, S. and Gu, S.S., 2021. A minimalist approach to offline reinforcement learning.\u00a0*Advances in neural information processing systems*,\u00a0*34*, pp.20132-20145.\n\nKostrikov, I., Nair, A. and Levine, S., 2021. Offline reinforcement learning with implicit q-learning.\u00a0*arXiv preprint arXiv:2110.06169*.\n\nHansen-Estruch, P., Kostrikov, I., Janner, M., Kuba, J.G. and Levine, S., 2023. Idql: Implicit q-learning as an actor-critic method with diffusion policies.\u00a0*arXiv preprint arXiv:2304.10573*.\n\nWang, Z., Hunt, J.J. and Zhou, M., 2022. Diffusion policies as an expressive policy class for offline reinforcement learning.\u00a0*arXiv preprint arXiv:2208.06193*.\n\nCai, Y., et al, 2022, November. TD3 with Reverse KL Regularizer for Offline Reinforcement Learning from Mixed Datasets. In\u00a0*2022 IEEE International Conference on Data Mining (ICDM)*\u00a0(pp. 21-30). IEEE."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4218/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699974339883,
                "cdate": 1699974339883,
                "tmdate": 1699974339883,
                "mdate": 1699974339883,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Ez1N9WDISz",
                "forum": "iMRhuFS0Uz",
                "replyto": "nqptRT2MkB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4218/Reviewer_WVL4"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4218/Reviewer_WVL4"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your clarifications on the settings and results in experiment part and now I am convinced with the results on mujoco tasks in paper. However, based on your reply and your discussion with other reviews, I still have following concerns:\n\n- The performances of MCEP on Antmaze/Adroit/kitchen are not good and cannot outperform the baselines. Although the authors gave some reasons, it shows that the improvement by MCEP is not applicable to other tasks, which limits the contribution of this paper.\n\n- Even on the mujoco tasks, the idea of MCEP may not be applicable/helpful to other existing offline methods: 1) the improvement on DQL is marginal; 2) the authors claim that MCEP cannot be used on CQL, which is strange because CQL also encounters the tradeoff between stable Q estimate and evaluation performance."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4218/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700357342653,
                "cdate": 1700357342653,
                "tmdate": 1700357342653,
                "mdate": 1700357342653,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ixL6hcGCio",
            "forum": "iMRhuFS0Uz",
            "replyto": "iMRhuFS0Uz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4218/Reviewer_px9k"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4218/Reviewer_px9k"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the policy constraint methods in offline reinforcement learning. It takes an interesting idea: decoupling the constraint strength for stable value estimation and for policy learning. Specifically, they find that while we need a restrictive policy constraint to mitigate extrapolation error in value estimation, a milder constraint is allowed for policy learning. Thus, apart from the target policy used in actor-critic learning of standard offline RL, a mildly constrained evaluation policy (MCEP) is proposed to be separately learned with a more relaxed policy constraint. This paper instantiates MCEP with existing offline RL method TD3+BC, AWAC, and DQL and demonstrates improved performance."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. An ingenious idea of decoupling constraint strengths for stable value estimation and for policy learning. Two kinds of distributional shifts, namely OOD actions during Bellman bootstrapping and deployment, to my knowledge, are rarely distinguished in offline RL literature. \n2. The design of MCEP is simple and general for policy constraint methods.\n3. Challenging humanoid tasks are introduced in the experiments, and MCEP demonstrates good performance.\n4. The visualization of the toy example in Figure 2 illustrates the motivation well."
                },
                "weaknesses": {
                    "value": "1. The most important finding of this paper, i.e. the difference between policy constraint strengths for stable value learning and for a performant evaluation policy, is validated empirically but lacks a theoretical analysis.\n2. The improvement of MCEP upon DQL is limited, which doubts the benefits of MCEP for modern offline RL methods with better designs, such as ReBRAC.\n3. The proposed MCEP is only applicable to policy constraint methods and does not outperform other kinds of sota offline RL methods, such as MCQ and EDAC."
                },
                "questions": {
                    "value": "1. What do you mean by 'While the target policy may recover its performance by iterative policy improvement and policy evaluation, we observe that the evaluation policy may fail to do so.'\n2. How does MCEP 'overcome this drawback' (state-agnostic constraint) discussed in Singh et al.?\n3. As the authors claim in the introduction that the toy maze experiments 'validate the finding of (Czarnecki et al., 2019)', I recommend also mentioning Czarnecki et al. in Section 5.1.\n\nTypos: \n\n- Section 4.3 DQL WITH MCEP should be a paragraph in parallel with TD3BC with MCEP and AWAC with MCEP. \n- $C\\left(\\pi_{\\beta, \\pi^E}\\right)$ should be $C\\left(\\pi_{\\beta}, \\pi^E\\right)$ in the Equation (10)\n- There seem to be some explanation sentences missing after 'We next introduce the policy improvement step for the evaluation policy' and Equation (10).\n- The caption of Figure 6 is incorrectly the same as that of Figure 7."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4218/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4218/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4218/Reviewer_px9k"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4218/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697814195193,
            "cdate": 1697814195193,
            "tmdate": 1699636388758,
            "mdate": 1699636388758,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "59POD5nY0s",
                "forum": "iMRhuFS0Uz",
                "replyto": "ixL6hcGCio",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4218/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4218/Authors"
                ],
                "content": {
                    "title": {
                        "value": "A Response to Reviewer px9k"
                    },
                    "comment": {
                        "value": "We appreciate reviewer px9k for their review and kind comments on our manuscript.\n\n>1 The most important finding of this paper, is validated empirically but lacks a theoretical analysis \u2026\n\nAs you mentioned in your comment, *the two kinds of distributional shifts are rarely distinguished in offline RL papers*. This differentiation is fundamental but important for offline RL study. Our work focuses on empirical analysis to reveal this differentiation and proposes a general method to address them separately. We are looking forward to building up the theory by separating these distributional shifts in the near future.\n\n>2.The improvement of MCEP upon DQL is limited, which doubts the benefits of MCEP for modern offline RL methods with better designs, such as ReBRAC.\n\nOne of the motivations for using MCEP is to reduce the Bellman estimate error brought by OOD actions. Conventional methods use a Gaussian policy that raises many OOD actions under multi-modal distribution. MCEP significantly eliminates these OOD actions and leads to significant performance improvement. DQL utilizes diffusion policies that effectively learn the multi-modal distribution. Hence the Bellman estimate error is reduced. In this case, MCEP can still improve its general performance by optimizing the evaluation policy towards higher-value OOD actions further.\n**we respectfully argue that our contributions do not emphasize strong algorithmic performance, but are two-fold, as concluded below:**\n\n1) We propose to distinguish the distributional shifts for Bellman estimate error reduction and the inference-time performance. Existing policy constraints methods (Kumar et al 2019, Nair et al 2020, Wang et al., 2023 etc) focus on addressing the first type but wish to achieve the second type.\n\n2) We propose to address these two types of distributional shifts separately and propose a general algorithm that learns two policies with different policy constraints. Our experiments on both D4RL locomotion and the challenging humanoid illustrate the performance improvement brought by the MCEP.\n\n>3. The proposed MCEP is only applicable to policy constraint methods and does not outperform other kinds of sota offline RL methods, such as MCQ and EDAC.\n\nOne of the contributions of this work is to distinguish two types of distributional shifts. Our work investigates this phenomenon in policy constraint strengths, hence the resulting method is applied to policy constraints methods, which is a large group of offline RL methods. We are looking forward to investigating this phenomenon in other methods in the future.\n\nOur comparison includes EQL and DQL which are also highly-performed SOTA algorithms. As both the MCQ and EDAC reported performances with task-specific hyper-parameters while we use \u201csimilar hyperparameters to locomotion tasks\u201d, the reported performance under different evaluation protocols does not provide a straightforward comparison.\n\n>1. What do you mean by 'While the target policy may recover its performance\u2026\n\nUnstable Q learning means that the value of a bad-performed action could sometimes be arbitrarily high but can later back to an accurate estimate when 1) this action is queried during TD learning and 2) the dataset distribution covers this state-action. The target policy might learn this bad-performed action when it\u2019s of arbitrarily high value. But the critic will query this target policy during the TD learning, hence the inaccurate value estimate for this action is fixed. For the evaluation policy, it can learn a different bad-performed action from the target policy\u2019s. But the critic never queries the evaluation policy, hence the value estimate for this action is always arbitrarily high and the behavior of the evaluation policy does not recover to good behavior.\n\n>2. How does MCEP 'overcome this drawback' (state-agnostic constraint) discussed in Singh et al.?\n\nSignh et al studies the datasets where the behavior diversity varies among different states. The appropriate constraint strength for a state of low diversity may be too restrictive for the policy to learn behavoir of another state of high diversity. A milder constraint strength enables the policy to learn diverse behavior but increase the Bellman estimate error on the state of low diversity. The MCEP uses the target policy to obtain stable Bellman estimate and the evaluation policy to learn more diverse behavior.\n\n>3. I recommend also mentioning Czarnecki et al. in Section 5.1\n\nWe added these lines in Section 5.1 *\u201cThis approach of utilizing the value function of the imperfect teacher policy is originally suggested by Czarnecki et al. (2019)\u201d*. Thank you for your thoughtful suggestion.\n\n>Typos\n\nWe fixed all the typos in the updated manuscript.\n\nPlease let us know whether our answers address all your concerns. We are more than happy to answer more if you have further questions and take suggestions to further improve our work."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4218/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699972533271,
                "cdate": 1699972533271,
                "tmdate": 1699972533271,
                "mdate": 1699972533271,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "yF7vcMeUyE",
                "forum": "iMRhuFS0Uz",
                "replyto": "59POD5nY0s",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4218/Reviewer_px9k"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4218/Reviewer_px9k"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the detailed response!\n\n**On the limited improvement upon DQL**\n\nI am sorry that the response does not fully address my previous concern. I noticed that Reviewer WVL4 also mentioned the marginal performance difference between DQL and DQL-MCEP. The average score of DQL is 85+-1.6, and that of DQL-MCEP is 86.2+-1.2. Thus, the difference is not significant.  As the authors respond, it is because DQL utilizes diffusion policies that effectively learn the multi-modal distribution, which can effectively reduce Bellman estimate error. Given a powerful distribution estimator is sufficient to gain a performant policy, how much improvement room is there for a more mildly constrained evaluation policy, which still needs to be limited to avoid extreme OOD action? This is why I regret that there is no rigorous theoretical analysis.\n\nLooking forward to more insights from the authors.\n\n**(Minor) On the state-agnostic constraint**\n\nI understand that a complex behavior distribution demands different strengths for different states. But MCEP still adopts a state-agnostic constraint, right? It only utilizes two kinds of constraints, but each constraint the policy with an equal strength across states."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4218/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700033804247,
                "cdate": 1700033804247,
                "tmdate": 1700033804247,
                "mdate": 1700033804247,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ksxWbkhGOM",
                "forum": "iMRhuFS0Uz",
                "replyto": "ixL6hcGCio",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4218/Reviewer_px9k"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4218/Reviewer_px9k"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the prompt reply.\n\nLet me elaborate on my concern in the 'improvement room.' According to my understanding and the author's response, there is an \"edge\u201d in distribution avoiding severe Bellman approximate errors, and also an edge avoiding OOD states due to OOD actions during evaluation, which is wider than the former one and allows more performant learned policies. Given that the former edge can be pushed approaching the latter one with powerful policy distribution classes (e.g., beyond isotropic deviations) or better policy constraint formulations, my question is, how much room is still in between them? This determines the improvements that MCEP can give to base offline RL methods. Regretfully, the empirical results of MCEP upon DQL seem to suggest the improvement room is small for modern offline RL methods.\n\nPlease correct me if there is any misunderstanding."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4218/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700206510440,
                "cdate": 1700206510440,
                "tmdate": 1700206725687,
                "mdate": 1700206725687,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "rkSdiFMbNT",
            "forum": "iMRhuFS0Uz",
            "replyto": "iMRhuFS0Uz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4218/Reviewer_VtJK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4218/Reviewer_VtJK"
            ],
            "content": {
                "summary": {
                    "value": "In policy constraint offline reinforcement learning (RL) algorithms, it is a common practice that the constraints for both value learning and test time inference are the same. This paper argues that such a paradigm may hinder the performance of the agent during test time inference. To address this issue, they propose the Mildly Constrained Evaluation Policy (MCEP) for test time inference. The idea is quite simple and the implementation is also easy. MCEP has the same objective function as the policy trained during the offline phase, but it does not participate in the policy evaluation phase. The authors show that by doing so, the performance of the offline RL agents can be improved."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "# Strengths\n\nThis paper is generally well-written, and the logical flow is clear. I would say this paper is also well-motivated and proposes an interesting test-time inference algorithm. The resulting method is very simple, and the authors provide some figures and a toy example to illustrate to the readers the key idea behind their method, which I personally like very much. The authors combine their method with three off-the-shelf offline RL algorithms, and conduct some experiments on the D4RL locomotion datasets. The authors also conduct experiments on the Humanoid datasets, where the authors collect the corresponding static datasets by themselves. One can observe performance improvement by building MCEP upon numerous base algorithms. To summarize, the strengths and the advantages of this manuscript are\n\n- this paper is well-written with a clear logic flow\n\n- the core idea and the resulting method of this paper is quite simple and easy to implement\n\n- the improvements from the proposed method are significant on many base algorithms\n\n- the authors provide source codes, and I believe that the results presented in this paper are reproducible"
                },
                "weaknesses": {
                    "value": "# Weaknesses\n\nI think the submission has the following potential flaws\n\n- (major) limited evaluation. Though the authors combine their proposed MCEP method with three offline RL algorithms, they only evaluate them on locomotion tasks, which are actually simple and easy to get a high return. So, my question is, can the proposed method benefit other domains like antmaze, kitchen, and adroit? These domains are known to be more challenging than the MuJoCo tasks. I strongly believe that the empirical evaluations on these domains are critical to show the effectiveness and advantages of the proposed methods. If the proposed methods fail in these domains, I also expect possible explanations from the authors. This paper feels quite incomplete without the experiments on these domains.\n\n- (major) It turns out that the hyperparameter selection counts in MCEP. Based on the empirical results in Section 5.4, TD3BC-MCEP and AWAC-MCEP are slightly sensitive to the hyperparameters. This may cause issues when using the MCEP in practice. Can the authors further explain this phenomenon and are there any ways that we can get rid of it?\n\n- (minor) inconsistent abbreviation for some of the algorithms, e.g., the authors write TD3-BC in the first few paragraphs while using TD3BC later. This is not a big issue and can be easily fixed, please check your submission for potential similar issues.\n\nI will be happy to update my score if the concerns are addressed during the rebuttal or in the revised manuscript."
                },
                "questions": {
                    "value": "It seems your method is not restricted to the policy constraints offline RL methods, can your method be applied to value-based offline RL algorithms like CQL? I would expect explanations from the authors if CQL-MCEP fails and underperforms vanilla CQL."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4218/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4218/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4218/Reviewer_VtJK"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4218/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698385370926,
            "cdate": 1698385370926,
            "tmdate": 1700527930769,
            "mdate": 1700527930769,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KJaKDhLJpM",
                "forum": "iMRhuFS0Uz",
                "replyto": "rkSdiFMbNT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4218/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4218/Authors"
                ],
                "content": {
                    "title": {
                        "value": "A Response to Reviewer VtJK"
                    },
                    "comment": {
                        "value": "We thank the reviewer **VtJK** for their thorough feedback on our work.\n\n> (major) Limited Evaluation \u2026\n\nAbout the evaluation on antmaze, kitchen and adriot, as this is asked by reviewers **r8JF, VtJK, WVL4 (3/4 of the reviewers),** we answer this question in the general review that can be found at the top of this page.\n\n> (major) It turns out that the hyperparameter selection\u2026\n\n**An explanation for this phenomenon:** Consistent with the results of section 5.4 (Figure 8), a strict constraint hinders the policy from improving beyond the datasets so may show poor performance. With milder constraints, the policy is able to utilize the estimated q function and take some OOD actions of high value during the inference so it achieves better performance over the dataset.\n\n**Suggestions about the hyperparameter selection:** Based on the results of Section 5.4, we suggest using the recommended constraint strength from the base algorithm paper for the target policy. This ensures a stable q estimate. Then tune the evaluation policy constraint strength to wilder values. Our experiment also follows this tuning protocol for all MCEP methods.\n\n> (minor) inconsistent abbreviation\n\nwe unify them by using TD3BC (including the figures) in the newly submitted manuscript. Thanks for your thoughtful suggestion.\n>can your methods be applied to \u2026 like CQL?\n\nOne of the motivations for using MCEP is to avoid the Bellman estimate error brought by the actor with milder constraints. In the pessimism methods such as CQL, the Q estimate error is reduced by pushing the Q values down for OOD actions. The policy in CQL does not need the policy constraint therefore milder constraint (or non-constraint) policy does not exacerbate the Bellman estimate error and thus the MCEP does not help in these methods.\n\nPlease let us know whether our answers address all your concerns. We are more than happy to answer more if you have further questions and to take suggestions to improve our work."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4218/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699971938839,
                "cdate": 1699971938839,
                "tmdate": 1699971938839,
                "mdate": 1699971938839,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fHTgkrOxbp",
                "forum": "iMRhuFS0Uz",
                "replyto": "KJaKDhLJpM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4218/Reviewer_VtJK"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4218/Reviewer_VtJK"
                ],
                "content": {
                    "comment": {
                        "value": "My concerns seem not to be addressed.\n\nThe authors write that MCEP fails to outperform baselines on Adroit, AntMaze, and Kitchen. Meanwhile, MCEP cannot help value-pessimism methods like CQL. Can you provide detailed numerical results? It should not be difficult to run MCEP on these datasets.\n\nSo, MCEP can only aid policy constraint methods? Can it benefit IQL as well?"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4218/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700122022248,
                "cdate": 1700122022248,
                "tmdate": 1700122022248,
                "mdate": 1700122022248,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8nImP7w23F",
                "forum": "iMRhuFS0Uz",
                "replyto": "rkSdiFMbNT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4218/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4218/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your thoughtful discussion.\n\n> Can you provide detailed numerical results?\n\nWe provide the results of two tasks from Adroit below. The results were taken from the last step of the training, with the average return and standard error from 5 random seeds.\n\n| Dataset      | BC       | CQL    | IQL           | TD3BC | TD3BC-MCEP | AWAC | AWAC-MCEP |\n| ------------ | -------- | ---------- | --------- | --- | ---- | --- | -------- |\n| pen-human |$76.8\\pm4.8$ | $37.5$|$64.2\\pm10.4$|$61.6\\pm11$ | $58.6\\pm20.8$  | $34.7\\pm11.8$  | $23.3\\pm5.6$ |\n| pen-cloned |$28.5\\pm6.7$ |$39.2$|$32.1\\pm7.5$| **$49\\pm9.5$**  |  $43.4\\pm20.3$    |$20.8\\pm7.3$  | $19.0\\pm7.5$  |\n| Average      |$52.6$   | $38.3$    | $48.1$  |  **$55.3$**   | $51.0$  | $27.7$ |  $21.1$|\n\nTheir hypepameters are: IQL $\\tau=0.7, \\lambda=2.0$ (tuned among $[0.6, 0.7, 0.8, 0.9] \\times [1.0, 2.0, 3.0]$, The AWAC uses $\\lambda=1.0$ (tuned among $[0.1, 0.2, 0.3, 0.5, 0.8, 1.0, 1.2]$) and Td3BC uses $\\alpha=0.1$ (tuned among $[0.01, 0.1, 0.2, 0.5]$). AWAC-MCEP $\\tilde{\\lambda}=10.0, \\lambda^E = 1000$, TD3BC-MCEP $\\tilde{\\alpha}=0.1, \\alpha^E=0.1$.\n\nTD3BC under this high BC coefficiency is the only method that outperforms behavior cloning. However, the critic learning presents exponentially high values during the training of AWAC and TD3BC. In these cases, the MCEP fails to utilize the arbitrarily high values provided by the critics, resulting in worse performance.\n\n> So, MCEP can only aid policy constraint methods? Can it benefit IQL as well?\n> \nThe MCEP is desgined overcome an overlooked drawback of policy constraints methods, hence it aims to aid policy constraints methods. This weakness is that most policy constraints methods do not distinguish 1) avoiding OOD actions to address unstable value learning and 2) obtaining inference-time performance. Instead, Policy constraints methods treat 1) and 2) as one problem and expect one identical policy constraint to solve both. This is the source of the wellknown tradeoff (Fujimoto et al 2018, Kumar et al 2019). The general idea behihd MCEP is to address 1) and 2) separately, hence a simple two-constraint framework is devised.\n\nIQL is also an instance that follows this general idea and implicitly has two policy constraints. Its value learning can be transformed to actor-critic with implicit \\textit{target policy} (Hansen-Estruch et al., 2023). Its policy extraction derives an evaluation policy that potentially has a different constraint from the target policy.\n\nPessimistic value methods such as CQL also have the same drawback by solving 1) and 2) with the same degree of conservative. How to address this drawback for pessimism methods is also an open problem (Hong et al 2022, Hong et al 2023).\n\n**References**\n\nHansen-Estruch, P., Kostrikov, I., Janner, M., Kuba, J.G. and Levine, S., 2023. Idql: Implicit q-learning as an actor-critic method with diffusion policies. arXiv preprint arXiv:2304.10573.\n\nHong, J., Kumar, A. and Levine, S., 2022. Confidence-conditioned value functions for offline reinforcement learning. arXiv preprint arXiv:2212.04607.\n\nHong, Z.W., Kumar, A., Karnik, S., Bhandwaldar, A., Srivastava, A., Pajarinen, J., Laroche, R., Gupta, A. and Agrawal, P., 2023. Beyond Uniform Sampling: Offline Reinforcement Learning with Imbalanced Datasets. arXiv preprint arXiv:2310.04413."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4218/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700141921915,
                "cdate": 1700141921915,
                "tmdate": 1700274411174,
                "mdate": 1700274411174,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SKQVV8QPNb",
                "forum": "iMRhuFS0Uz",
                "replyto": "8nImP7w23F",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4218/Reviewer_VtJK"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4218/Reviewer_VtJK"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your rebuttal. It is good to see the numerical results. I have updated my score, as promised. I cannot support more since this method seems to be restricted in the category of offline policy constraint methods, and its performance on some challenging tasks is somewhat unsatisfying. I also hold my opinion that the results on wider datasets other than MuJoCo are necessary. It is okay that the proposed method does not have advantages over baselines. As I commented, if there is a failure on other datasets, explanations are expected to be included such that the readers can capture both the strengths and weaknesses of MCEP. I also strongly recommend the authors add the numerical results on Adroit/AntMaze datasets into the revision.\n\nNevertheless, I tend to like this work and what the authors have done so far. The weaknesses side does not downgrade the contribution of this work. Consequently, I believe this paper deserves a score of 6."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4218/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700528833376,
                "cdate": 1700528833376,
                "tmdate": 1700528833376,
                "mdate": 1700528833376,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "i5dSpH7TfK",
            "forum": "iMRhuFS0Uz",
            "replyto": "iMRhuFS0Uz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4218/Reviewer_r8JF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4218/Reviewer_r8JF"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a new approach to address the problems in offline policy learning (e.g. extrapolation). The idea is to train an extra policy (called evaluation policy) based on the $Q$ function learned from the critic of a standard constrained actor-critic offline method. The idea is that using different constraint weights for the critic and evaluation policy should addresses the trade-off between evaluation performance and stable value estimate."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is easy to follow and well written. As far as I know the idea is novel.\n\nExperiments seem to be well conducted and results are fairly explained."
                },
                "weaknesses": {
                    "value": "While the idea of the approach is interesting, I think the paper needs a bit more work. My main concern is that the contribution is limited and the results are not super clear and convincing to me.\n\n- For example, given that $\\pi_e$ is not involved in the optimization of $Q$, why are you training $\\pi_e$ at each step and not only at the end? Training $\\pi_e$ at the end will allow to do an analysis of the impact of the constraint. \n- Have you tried to train a greedy policy starting from the recovered Q, similarly to what done in the grid experiment?\n- Why haven't you tested other environments in D4RL, eg Antmaze-v0?\n- Figures are not readable when printed out. The font is too small.\n\n\nTypos:\n\nacheive -> achieve\n\npriority. i.e. \n\nwrong latex commands in page 9"
                },
                "questions": {
                    "value": "See Weaknesses part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4218/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698915701879,
            "cdate": 1698915701879,
            "tmdate": 1699636388599,
            "mdate": 1699636388599,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Mh5ZTJ0gZ9",
                "forum": "iMRhuFS0Uz",
                "replyto": "i5dSpH7TfK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4218/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4218/Authors"
                ],
                "content": {
                    "title": {
                        "value": "A Response to reviewer r8JF"
                    },
                    "comment": {
                        "value": "We appreciate reviewer **r8JF** for their review of our manuscript.\n\nFor your concern of contribution is limited, **we respectfully argue that our contributions do not emphasize strong algorithmic performance, but are two-fold, as concluded below:**\n\n1) We propose to distinguish the distributional shifts for Bellman estimate error reduction and the inference-time performance. Existing policy constraints methods (Kumar et al 2019, Nair et al 2020, Wang et al., 2023) focus on addressing the first type but wish to achieve the second type.\n\n2) We propose to address these two types of distributional shifts separately and propose a general algorithm that learns two policies with different policy constraints. Our experiments on both D4RL locomotion and the challenging humanoid illustrate that the MCEP empowers conventional methods (TD3BC and AWAC) to achieve near SOTA performance and shows a consistent performance improvement on SOTA algorithm DQL.\n\n>For example, given that\u00a0$\\pi_e$\u00a0is not involved \u2026\n\nWe provide a new experiment in Appendix Section A.4 to illustrate the difference between these two design options. Our algorithm optimizes two policies together to allow parallelizing their learning thus the training time is the same as the base algorithms. This is powerful for algorithms that require long training time for the policy (e.g. DQL which utilizes diffusion policies).\n\n>Have you tried to train a greedy policy \u2026\n\nWhile this idea works for the grid example, a greedy policy in larger MDPs will inquire the q function about OOD actions where the estimated Q is arbitrarily high. Based on this, the evaluation policy still needs a policy constraint (though milder). Pessimism Q methods (CQL etc) do not have this issue so their policy does not need the policy constraints.\n\n>Why have\u2019nt you tested other environments \u2026\n\nAs this is asked by reviewers **r8JF, VtJK, WVL4 (3/4 of the reviewers),** we answer this question in the general review that can be found at the top of this page.\n\n>Figures are not readable when printed out.\n\nWe apologize for the inconvenience during your review. We changed the font size in the new manuscript (as well as fixing the typos). Again, we appreciate your effort and suggestion.\n\nPlease let us know whether our answers address all your concerns. We are more than happy to answer more if you have further questions and to take more suggestions to improve our manuscript.\n\n**References:**\n\nKumar, A., Fu, J., Soh, M., Tucker, G. and Levine, S., 2019. Stabilizing off-policy q-learning via bootstrapping error reduction.\u00a0*Advances in Neural Information Processing Systems*,\u00a0*32*.\n\nNair, A., Gupta, A., Dalal, M. and Levine, S., 2020. Awac: Accelerating online reinforcement learning with offline datasets.\u00a0*arXiv preprint arXiv:2006.09359*.\n\nWang, Z., Hunt, J.J. and Zhou, M., 2022. Diffusion policies as an expressive policy class for offline reinforcement learning.\u00a0*arXiv preprint arXiv:2208.06193*."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4218/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699971785126,
                "cdate": 1699971785126,
                "tmdate": 1699971785126,
                "mdate": 1699971785126,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0egmouaPiw",
                "forum": "iMRhuFS0Uz",
                "replyto": "Mh5ZTJ0gZ9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4218/Reviewer_r8JF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4218/Reviewer_r8JF"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Thank you for the answers. I appreciate your clarifications, but I still have concerns about the novelty of the approach. For example, there are several methods that are not compared or tested in your work.\n\nApproximate policy improvement at test time is a well-studied topic. To my knowledge, Wang et al. introduced this idea in their paper \"Critic Regularized Regression\" (NeurIPS 2020). They also presented experiments on challenging environments, such as the humanoid.\n\nAn even simpler and more common approach for policy improvement at test time is to sample actions and take the empirical argmax (or constrained argmax) at evaluation. I would like to see these approaches integrated into your paper."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4218/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699988387612,
                "cdate": 1699988387612,
                "tmdate": 1699988387612,
                "mdate": 1699988387612,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]