[
    {
        "title": "Skill or Luck? Return Decomposition via Advantage Functions"
    },
    {
        "review": {
            "id": "DjmZMHIkMD",
            "forum": "ZFMiHfZwIf",
            "replyto": "ZFMiHfZwIf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3969/Reviewer_K7qX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3969/Reviewer_K7qX"
            ],
            "content": {
                "summary": {
                    "value": "The proposes to decompose the advantage function into two parts: 1) return due to the agent's action selection and 2) return due to transition dynamics of the environment. They then use this decomposition to extend an existing algorithm DAE to the off-policy setting."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The proposed decomposition is straightforward, and I think is a useful idea to spark newer ideas.\n2. The toy examples and Section 4 is useful."
                },
                "weaknesses": {
                    "value": "See questions"
                },
                "questions": {
                    "value": "1. It is not clear to me why off-policy corrections are not necessary given that the sequence of rewards was generated by a different policy; it feels like frequency at which the sequences appear must be corrected for.\n2. Related to above, it appears that the only \"off-policyness\" in Eqn 5 is the $\\pi$-centered constraint. Why is this sufficient for the off-policy correction?\n3. The decomposition seems related to exogenous and endogenous stochasticity. Is there a way to phrase the current work in that context? I'd also refer the authors to this paper that seems relevant: https://people.csail.mit.edu/hongzi/var-website/content/iclr-19.pdf\n4. I am curious if once the advantage function is decomposed into skill and luck, is there a benefit to weighing each component differently? I would suspect that this leads to some bias in the policy ordering, but I am wonder if say the skill related component is too small, it may get overshadowed by the luck component, and the agent may not learn efficiently.\n5. Related to above, I am curious how off-policy DAE performs as a function of environment stochasticity. \n6. In Figure 3 and 4, it is unclear to me why all methods are able to produce similar mean estimates? Of course each is different in terms of their variance, but all are centered around the same mean which is a bit surprising.\n7. What were the number of trials for the Figure 5 results? These should be mentioned."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "n/a"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3969/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698690902882,
            "cdate": 1698690902882,
            "tmdate": 1699636358035,
            "mdate": 1699636358035,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "QIRhEf66gI",
                "forum": "ZFMiHfZwIf",
                "replyto": "DjmZMHIkMD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3969/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3969/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the constructive feedback, please see our answers below.\n\n> It is not clear to me why off-policy corrections are not necessary given that the sequence of rewards was generated by a different policy; it feels like frequency at which the sequences appear must be corrected for.\n\n> Related to above, it appears that the only \"off-policyness\" in Eqn 5 is the \n-centered constraint. Why is this sufficient for the off-policy correction?\n\nThe key observation is that the following equation holds for *any* trajectory\n\n$G = \\sum_{t=0}^\\infty \\gamma^t (A^\\pi(s_t, a_t) + \\gamma B^\\pi(s_t, a_t, s_{t+1}) + V^\\pi(s_0)$.\n\nIn other words, this decomposition is invariant to the sampling policy.\nIn contrast, take MC methods as an example, they are based on *expectations* over trajectories, which is why additional corrections are required when the sampling distribution and the target distribution mismatch.\n\nOne problem of the decomposition is that it is not unique for arbitrary $\\hat{A}(s,a)$ and $\\hat{B}(s,a,s\u2019)$ (to see this, note that we can absorb $A^\\pi$ into $\\hat{B}$, such that $\\hat{A}=0$ and $\\hat{B}= B^\\pi + A^\\pi$ is also a solution). However, once we impose the centering constraints on $\\hat{A}$ and $\\hat{B}$, the decomposition becomes unique with solutions $A^\\pi$ and $B^\\pi$. For more details, please see appendix A for the proof.\n\n> The decomposition seems related to exogenous and endogenous stochasticity. Is there a way to phrase the current work in that context? I'd also refer the authors to this paper that seems relevant: https://people.csail.mit.edu/hongzi/var-website/content/iclr-19.pdf\n\nThe exogenous stochasticity introduced by [1] is mostly concerned with a special class of MDPs (input-driven MDPs) where external inputs may also interact with state variables, causing additional stochasticity. If the external inputs are observable, then the environment reduces to an ordinary MDP. In this case, the external inputs can be seen as actions from nature, and nature's policy can be factorized into two components: the external input and the state transitions. For unobserved external inputs, the problem becomes partially observable (POMDP), which is beyond the scope of the present work.\n\n> I am curious if once the advantage function is decomposed into skill and luck, is there a benefit to weighing each component differently? I would suspect that this leads to some bias in the policy ordering, but I am wonder if say the skill related component is too small, it may get overshadowed by the luck component, and the agent may not learn efficiently.\n\nWhile we have demonstrated that the decomposition can improve value estimation, it is not clear whether it is optimal to optimize the policy based on the estimated advantage function. It is possible that by weighing components differently can lead to biased but lower variance targets for policy optimization. \n\nIn the case where the luck component dominates, the problem is inherently difficult, as it means the agent\u2019s actions have little influence on the returns, and the returns are very noisy. This makes it challenging to differentiate between good and bad actions, and additional care may be required. \n\n> Related to above, I am curious how off-policy DAE performs as a function of environment stochasticity.\n\nThis can depend on several factors, such as whether the transition probabilities are known (see Figure 4 for comparison), or the backup lengths used. In the ideal case where the transition probabilities are known a priori along with infinite backup length, then we can expect similar convergence properties to a constrained linear regression as pointed out in Section 4.1.\n\n> In Figure 3 and 4, it is unclear to me why all methods are able to produce similar mean estimates? Of course each is different in terms of their variance, but all are centered around the same mean which is a bit surprising.\n\nWe note that these experiments were performed in tabular, and fixed-policy settings. Therefore, all the estimators were proven to converge to the true values of the states in the limit. \n\n> What were the number of trials for the Figure 5 results? These should be mentioned.\n\nThe results were aggregated over 20 random seeds. We have updated Figure 5 to include this information.\n\n[1] Mao, H., Venkatakrishnan, S. B., Schwarzkopf, M., & Alizadeh, M. (2018). Variance reduction for reinforcement learning in input-driven environments. arXiv preprint arXiv:1807.02264."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3969/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700096222061,
                "cdate": 1700096222061,
                "tmdate": 1700096222061,
                "mdate": 1700096222061,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YBno85PpUo",
                "forum": "ZFMiHfZwIf",
                "replyto": "QIRhEf66gI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3969/Reviewer_K7qX"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3969/Reviewer_K7qX"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the response. I am satisfied with the response. I will keep the score the same as it reflects an accurate evaluation. I do think this type of decomposition between impact from nature and agent is useful for future research."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3969/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700144085895,
                "cdate": 1700144085895,
                "tmdate": 1700144085895,
                "mdate": 1700144085895,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "sDiKZL90vc",
            "forum": "ZFMiHfZwIf",
            "replyto": "ZFMiHfZwIf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3969/Reviewer_NYwT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3969/Reviewer_NYwT"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a new return decomposition method via advantage functions, which contains the average return, the skill term, and the luck term."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The formulations of the paper are quite clear and easy to follow. Although I didn't get the main idea of the paper through the text, the formulations are quite clear for me to follow.\n\n\n\n\n\nGenerally, I think the paper contributes to giving some insight into the decomposition of the return."
                },
                "weaknesses": {
                    "value": "Although I can get the main ideas from the formulations of the paper, the writing of the paper is poor.\n\n- The introduction is not quite clear. Although it starts with an intuitive example, it doesn't contribute too much to get the idea of the paper.\n- Section 3 lacks motivation, making it kind of hard to follow.\n\n\n\nThe experimental results are not very convincing.\n\n- For Figure 9, Table 2, Figure 10, 11, 12, are the results in deterministic environments? Are there any results in stochastic environments?  I believe this is very important as the evaluation in stochastic environments is the main contribution of the paper.\n- The authors used very large $N$ (8) for Uncorrected. However, in existing literature, it's known 3 or 5 are the best. Therefore, the comparison is unfair.\n- What's the performance of 1-step DQN? \n- The paper doesn't compare to other state-of-the-art multi-step off-policy method, such as Retrace($\\lambda$) [1].\n\n\n\n[1] Munos R, Stepleton T, Harutyunyan A, et al. Safe and efficient off-policy reinforcement learning[J]. Advances in neural information processing systems, 2016, 29.\n\n\n\nThe below claim is exaggerated. I didn't find a theorem that clearly proves the properties of faster convergence.\n\n> We demonstrate that (Off-policy) DAE can be seen as generalizations of Monte-Carlo (MC) methods that utilize sample trajectories more efficiently to achieve faster convergence.\n\n\n\n\n\nMinor comments:\n\n- Near eq.7: The second term $E{[V^\\pi(s_{t+1})|s_t,a_t]}$. What are the random variables of expectation? If it's $s_{t+1}$, please use another notation to differentiate it from the existing one using $s_{t+1}$.\n- The complexity of the method seems very large (15 hours vs. 2 hours, as stated in D.7)"
                },
                "questions": {
                    "value": "- Why the decomposition of the return could benefit learning? Could the author give more insights into it?\n- What's the probability of the sticky action for stochastic environments?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3969/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698868542211,
            "cdate": 1698868542211,
            "tmdate": 1699636357954,
            "mdate": 1699636357954,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "UUdlTFlRra",
                "forum": "ZFMiHfZwIf",
                "replyto": "sDiKZL90vc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3969/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3969/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the constructive feedback, please see our answers below.\n\n> The introduction is not quite clear. Although it starts with an intuitive example, it doesn't contribute too much to get the idea of the paper.\n\n> Section 3 lacks motivation, making it kind of hard to follow.\n\nThe lottery example in the introduction serves as an example to demonstrate that the return of a trajectory can be attributed to two sources: (1) actions from the agent (choosing a set of numbers), and (2) stochastic transitions (lottery drawing). Section 3 further develops this idea in a quantitative way by first analyzing deterministic environments and then generalizing to stochastic environments. We have made this connection more explicit at the beginning of Section 3.\n\n> For Figure 9, Table 2, Figure 10, 11, 12, are the results in deterministic environments? Are there any results in stochastic environments? I believe this is very important as the evaluation in stochastic environments is the main contribution of the paper.\n\nWe note that the MinAtar suite includes both deterministic (Breakout, and Space Invaders) and stochastic (Asterix, Freeway, and Seaquest) environments. As such, the aforementioned figures and table already include results in both deterministic and stochastic environments. These were separately aggregated in Figure 5 for easier visualization.\n\n> The authors used very large N (8) for Uncorrected. However, in existing literature, it's known 3 or 5 are the best. Therefore, the comparison is unfair.\n\nWe have included new results below for Uncorrected (N={3, 5}): \n\n| | Asterix | Breakout | Freeway | Seaquest | Space Invaders |\n|-| - | - | - |- |- |\n| Uncorr. (N=3) | $25.5 \\pm 0.7$ |  $7157.7 \\pm 465.1$ | $2.4 \\pm 1.0$ |  $437.6 \\pm 21.5$ | $8826.9 \\pm 473.0$ |\n| Uncorr. (N=5) | $5.9 \\pm 0.2$ |  $8123.1 \\pm 610.1$ |  $0.0 \\pm 0.0$  |  $34.0 \\pm 1.4$ | $7353.5 \\pm 436.2$ |\n| DAE (worst N) | $155.6 \\pm 5.4$ | $8119.8 \\pm 617.5$ | $55.1 \\pm 0.1$ | $ 413.5 \\pm 25.1$ | $12560.4 \\pm 441.4$ |\n| OffDAE (worst N) | $161.5 \\pm 5.9$ | $7372.1 \\pm 582.0$ | $61.9 \\pm 0.6$ | $839.4 \\pm 48.3$ | $14970.3 \\pm 348.9$ |\n\n(the worst N results are from Table 2)\n\nFrom the table, we see that while decreasing backup lengths may improve the performance of Uncorrected in some of the environments, it still largely underperforms Off-policy DAE. In addition, we would like to point out that it is not conclusive whether N = 3 or 5 is the best for Uncorrected. For example, in [1], the authors found that N = 10 or 20 can significantly outperform N = 5. In [2], it was also found that Uncorrected can achieve competitive performance for N up to 20.\n\n> What's the performance of 1-step DQN?\n\nAs the aim of the experiments is to compare how different value estimators can impact policy optimization performance in a controlled setting, we have not included other deep RL algorithms as baselines, except for the on-policy PPO-DAE baseline, which serves as a validation that the proposed off-policy method is truly more sample efficient. Consequently, we believe the lack of 1-step DQN baseline does not invalidate our findings.\n\n> The paper doesn't compare to other state-of-the-art multi-step off-policy method, such as Retrace\n\nIn the present work, we have only compared to Tree Backup as it is similar to Off-policy DAE in that, it also does not impose any constraint (aside from coverage of state-actions), or even need any knowledge about the behavior policy. In contrast, importance-sampling based methods (e.g., Retrace) require the behavior policy to compute the likelihood ratio, and often assumes the behavior policy to be Markovian, which we do not assume.\n\n> The below claim is exaggerated. I didn't find a theorem that clearly proves the properties of faster convergence. \n\n> *We demonstrate that (Off-policy) DAE can be seen as generalizations of Monte-Carlo (MC) methods that utilize sample trajectories more efficiently to achieve faster convergence.*\n\nWe have removed \u201cto achieve faster convergence\u201d from the sentence. However, we would like to note that the toy experiments in section 4 were meant as empirical evidence that (Off-policy) DAE can achieve faster convergence.\n\n[1] Van Hasselt, H. P., Hessel, M., & Aslanides, J. (2019). When to use parametric models in reinforcement learning?. Advances in Neural Information Processing Systems, 32.\n\n[2] Hernandez-Garcia, J. F., & Sutton, R. S. (2019). Understanding multi-step deep reinforcement learning: A systematic study of the DQN target. arXiv preprint arXiv:1901.07510."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3969/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700098709596,
                "cdate": 1700098709596,
                "tmdate": 1700098709596,
                "mdate": 1700098709596,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hSDdIeQnD9",
                "forum": "ZFMiHfZwIf",
                "replyto": "sDiKZL90vc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3969/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3969/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Authors (part 2)"
                    },
                    "comment": {
                        "value": "> Near eq.7: The second term $E[V^\\pi(s_{t+1})|s_t, a_t]$. What are the random variables of expectation? If it's $s_{t+1}$, please use another notation to differentiate it from the existing one using $s_{t+1}$ \n\nWe have updated the equations to explicitly point out the variable being integrated.\n\n> The complexity of the method seems very large (15 hours vs. 2 hours, as stated in D.7)\n\nWe acknowledge that the computational cost of the proposed method can be significantly larger than other methods. In the present work, we have focused on the correctness of the method, and we will leave it for future work to explore more efficient ways to combine it with other deep RL algorithms.\n\n> Why the decomposition of the return could benefit learning? Could the author give more insights into it?\n\nEssentially, the decomposition enables better utilization of trajectories in comparison to MC methods, where only the starting state(-action)s are used. For example, in Figure 3 we can see that the rewards are caused by actions at state 3 for both starting states 1 and 2. However, MC methods ignore this relationship and estimate V(1) and V(2) independently. In contrast, DAE can account for the variance caused by the actions at state 3 through the advantage function, which is then shared between trajectories starting from either states 1 or 2. Off-policy DAE further extends this idea to include variance caused by stochastic transitions, as demonstrated in Figure 4.\n\n> What's the probability of the sticky action for stochastic environments?\n\nAs the MinAtar suite already includes both deterministic and stochastic environments, we did not use sticky actions in the experiments."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3969/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700099045642,
                "cdate": 1700099045642,
                "tmdate": 1700099045642,
                "mdate": 1700099045642,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0tWLF54kSD",
                "forum": "ZFMiHfZwIf",
                "replyto": "sDiKZL90vc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3969/Reviewer_NYwT"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3969/Reviewer_NYwT"
                ],
                "content": {
                    "title": {
                        "value": "Response to the authors' response"
                    },
                    "comment": {
                        "value": "Thanks for the response of the authors. And I'm happy to see the authors corrected some typos in the paper.\nBelow are some further concerns.\n\n>We note that the MinAtar suite includes both deterministic (Breakout, and Space Invaders) and stochastic (Asterix, Freeway, and Seaquest) environments. As such, the aforementioned figures and table already include results in both deterministic and stochastic environments. These were separately aggregated in Figure 5 for easier visualization.\n\nThen, Figure 5 is misleading. The paper never clearly stated that the deterministic environments are referred to as \"Breakout, and Space Invaders\", and stochastic environments refer to as \"Asterix, Freeway, and Seaquest\". \nI thought they referred to using sticky action prob=0 and stick action prob=0.1 at the beginning.\n\nMinor Comments: In Breakout, the starting direction of the ball is also random. It's hard to say it's deterministic.\n\n\n\n> As the MinAtar suite already includes both deterministic and stochastic environments, we did not use sticky actions in the experiments.\n\nI believe that evaluating the methods under larger stochasticity is necessary because this is the major contribution of the method. This implies that using a non-zero sticky action prob is necessary for this paper. First, the original stochasticity is not enough. Second, the zero sticky action prob is usually not convincing to test the behavior of the algorithm. See below for the statements in the MinAtar paper.\n\n\u201cThis deterministic behavior can be exploited by simply repeating specific sequences of actions, rather than learning policies that generalize. Machado et al. (2017) address this by adding sticky-actions, where the environment repeats the last action with a probability 0.25 instead of executing the agent\u2019s current action. We incorporate sticky-actions in MinAtar, but with\na smaller probability of 0.1. \u201d\n\nDue to the efficiency and simplicity of MinAtar compared to classic Atari environments, I would expect the author not to reduce the difficulty of the benchmark and evaluate the methods with sufficient stochasticity.\n\n> We have included new results below for Uncorrected (N={3, 5}):\n\nAs far as I know, Uncorrected DQN can perform quite well on Freeway and get a score of 60 even with N=3 (even under sticky action prob=0.1). I'm curious what happened to the 3-step Uncorred method here.  What's the result of 1-step method?"
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3969/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700575725326,
                "cdate": 1700575725326,
                "tmdate": 1700596606834,
                "mdate": 1700596606834,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "lvmEPQ4Jlv",
            "forum": "ZFMiHfZwIf",
            "replyto": "ZFMiHfZwIf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3969/Reviewer_y5tB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3969/Reviewer_y5tB"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents Off-policy DAE, a framework for off-policy learning that decomposes returns into two components: 1. those controllable by agents (skills) and 2. those beyond an agent's control (luck). Specifically, \"luck\" refers to stochastic transitions that the agent can't control. By explicitly modeling the advantage attributed to this luck factor, the agent can more effectively discern the impact of its own actions, leading to quicker generalization through enhanced credit assignment. Evaluations conducted on 5 MinAtar environments demonstrate improvements over baselines, particularly in scenarios with stochastic transitions."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "S1: This offers a methodical approach to incorporate the effects of non-controllable factors, leading to enhanced credit assignment in off-policy learning. It's a novel concept, one I haven't encountered in other papers.\n\nS2: The paper is very well-written and straightforward. I'm especially impressed by the intuitive examples provided to make the reader understand the main concept.\n\nS3: Experiments are conducted on a standard benchmark suite using 20 random seeds. The results are notable, demonstrating that in stochastic environments, the agent performs better compared to methods not leveraging the proposed advantage function decomposition."
                },
                "weaknesses": {
                    "value": "The paper is largely well-composed, with the proposed idea articulately presented. While I couldn't pinpoint any specific areas needing improvement,\n\nW1: Conducting experiments in more domains might bolster the paper's claims even further."
                },
                "questions": {
                    "value": "Q1: Can the method be directly applied to offline RL, and if so, what additional challenges might arise in that context?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3969/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698988248086,
            "cdate": 1698988248086,
            "tmdate": 1699636357888,
            "mdate": 1699636357888,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "eyKKXJd3Tk",
                "forum": "ZFMiHfZwIf",
                "replyto": "lvmEPQ4Jlv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3969/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3969/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the positive feedback, please see our answer to your question below.\n\n> Can the method be directly applied to offline RL, and if so, what additional challenges might arise in that context?\n\nOne major challenge of offline RL lies in distribution shifts between target policies and behavior policies, causing agents to drift into less visited states in the dataset, where value estimates may be unreliable[1]. Previously, this was mitigated by using conservative/uncertainty estimation of the value functions [2, 3]. While in principle, the proposed method can also be applied to offline RL problems, we expect similar difficulties to arise. As such, an important question is how we can extend previous approaches to the proposed method.\n\n[1] Levine, S., Kumar, A., Tucker, G., & Fu, J. (2020). Offline reinforcement learning: Tutorial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643.\n\n[2] Kumar, A., Zhou, A., Tucker, G., & Levine, S. (2020). Conservative q-learning for offline reinforcement learning. Advances in Neural Information Processing Systems, 33, 1179-1191.\n\n[3] Ghasemipour, K., Gu, S. S., & Nachum, O. (2022). Why so pessimistic? estimating uncertainties for offline rl through ensembles, and why their independence matters. Advances in Neural Information Processing Systems, 35, 18267-18281."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3969/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700093283943,
                "cdate": 1700093283943,
                "tmdate": 1700093283943,
                "mdate": 1700093283943,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "H9IKi7SDvB",
                "forum": "ZFMiHfZwIf",
                "replyto": "eyKKXJd3Tk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3969/Reviewer_y5tB"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3969/Reviewer_y5tB"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the response."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3969/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700724757380,
                "cdate": 1700724757380,
                "tmdate": 1700724757380,
                "mdate": 1700724757380,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "d1kYLVaxDk",
            "forum": "ZFMiHfZwIf",
            "replyto": "ZFMiHfZwIf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3969/Reviewer_rop6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3969/Reviewer_rop6"
            ],
            "content": {
                "summary": {
                    "value": "This paper extends Direct Advantage Estimation (DAE), an algorithm designed to improve credit assignment by directly learning the advantage function. The original DAE formulation was limited to the on-policy case and this paper derives an off-policy version. This new algorithm is shown to be beneficial in a toy example and with experiments on the MinAtar environments."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The decomposition of the return in terms of advantages of both the agent and the environment's \"actions\" is very intriguing and novel. It also leads to a practical algortihm, which could potentially outperform the standard approaches to learning advantage functions, a crucial part of actor-critic algorithms.\n\n- The paper is well-written and this makes the derivation much easier to follow.\n- The toy environments are well-designed to demonstrate the differences between the different approaches, DAE vs. off-policy DAE.\n- There's some nice insights into why the uncorrected off-policy n-step return may work well in practice due to certain environmental properties."
                },
                "weaknesses": {
                    "value": "- The larger-scale experiments are fairly limited with MinAtar being the most complex domain. Other environments could be considered.\n- Other baselines could be more approrpirate for the MinAtar experiment. See questions."
                },
                "questions": {
                    "value": "- Off-policy DAE requires an additional neural network to estimate the transition distribution. This seems slightly unelegant since actor-critic algorithms are usually model-free. Is it possible to avoid this by converting the constraint into a loss function instead? i.e. optimize the Lagrangian of the constraint with SGD? Perhaps we would have to use an approximate loss here to make it tractable. \n\n- If we do learn a model to estimate $B_\\pi$, is it possible to adapt the algorithm so the model is sampled-based only? Requiring a sum over all discrete latent states seems a bit restrictive in the choice of model and it seems to prevent easy scaling of the model size.\n\n- Could you clarify how equation (10) reduces to the policy improvement lemma? In particular, what happens to the $B^\\pi_t$ term? \n\n- Why was Tree Backup chosen as the baseline? It seems like a less popular or effective choice compared to, say, Retrace [1] or its successors. \n\n- It could be interesting to try to incorporate off-policy DAE with model-based RL methods since those algorithms would already have a learned model to use. E.g. Dreamer-v3\n\nMinor point:\n- To improve the clarity of the notation, I would suggest using capital letters for the random variables. In certain places, the lowercase letters are used to denote both fixed values and random ones. E.g. below equation (7), in the definition of $B^\\pi_t$, $s_{t+1}$ in the expectation should be uppercase.\n\n\n[1] \"Safe and efficient off-policy reinforcement learning\" Munos et al."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3969/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699237591249,
            "cdate": 1699237591249,
            "tmdate": 1699636357811,
            "mdate": 1699636357811,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "CvdWk9ruzx",
                "forum": "ZFMiHfZwIf",
                "replyto": "d1kYLVaxDk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3969/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3969/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the constructive feedback, please see our answers below.\n\n> Off-policy DAE requires an additional neural network to estimate the transition distribution. This seems slightly unelegant since actor-critic algorithms are usually model-free. Is it possible to avoid this by converting the constraint into a loss function instead? i.e. optimize the Lagrangian of the constraint with SGD? Perhaps we would have to use an approximate loss here to make it tractable.\n\nThis is indeed possible, and, in fact, conditional moment constraints is an active area of research in other domains such as econometrics and causal inference (e.g., [1], and [2] for its relationship to RL), where various methods have been developed to enforce such constraints. In the present work, however, we opt for the conceptually simpler approach by learning the transition probabilities, and leave it for future work to explore other more efficient possibilities.\n\n> If we do learn a model to estimate $B^\\pi$, is it possible to adapt the algorithm so the model is sampled-based only? Requiring a sum over all discrete latent states seems a bit restrictive in the choice of model and it seems to prevent easy scaling of the model size.\n\nWhile we have not tried a sampling-based approach to estimate $B^\\pi$, we do believe that approximating the sum (or integration) over the latent variables by Monte-Carlo methods is also a feasible approach. This may be especially useful in continuous control domains where the state transitions are better characterized by continuous distributions (e.g., robotics tasks).\n\n> Could you clarify how equation (10) reduces to the policy improvement lemma? In particular, what happens to the $B^\\pi_t$ term?\n\nFirstly, we note that $B^\\pi$ satisfies the centering property of the advantage function, i.e., $\\mathbb{E}\\_{s'}[B^\\pi(s,a,s')|s,a]=\\sum_{s\u2019}B^\\pi(s,a,s\u2019)p(s'|s,a)=0$. This means that in expectation, all the $B^\\pi_t$ terms vanish, leaving only the $G$, $A$, and $V$ terms, resulting in\n$V^\\mu(s) = \\mathbb{E}\\_\\mu[G|s_0=s] = \\mathbb{E}\\_\\mu[\\sum_{t=0}^\\infty \\gamma^t (A^\\pi_t + \\gamma B^\\pi_t) + V^\\pi(s_0)|s_0=s] \n= \\mathbb{E}\\_\\mu[\\sum_{t=0}^\\infty \\gamma^t A^\\pi_t|s_0=s] + V^\\pi(s)$.\n\n> Why was Tree Backup chosen as the baseline? It seems like a less popular or effective choice compared to, say, Retrace [1] or its successors.\n\nTree Backup was chosen as the baseline as it is similar to Off-policy DAE in that, it also does not impose any constraint (aside from coverage of state-actions), or even need any knowledge about the behavior policy. In contrast, importance-sampling based methods (e.g., Retrace) require the behavior policy to compute the likelihood ratio, and often assumes the behavior policy to be Markovian, which we do not assume.\n\n> It could be interesting to try to incorporate off-policy DAE with model-based RL methods since those algorithms would already have a learned model to use. E.g. Dreamer-v3\n\nThis is certainly an interesting direction, and we believe that existing model-based methods can benefit from the proposed approach to improve off-policy critic training. We will leave this for future work to explore the optimal way to combine Off-policy DAE and model-based methods.\n\n> To improve the clarity of the notation, I would suggest using capital letters for the random variables. In certain places, the lowercase letters are used to denote both fixed values and random ones. E.g. below equation (7), in the definition of in the expectation should be uppercase.\n\nWe avoided using capital letters for the random variables because we were worried that it might lead to confusion between the advantage function and actions. As a remedy, we have updated the equations by explicitly pointing out the variable that is being integrated.\n\n[1] Kremer, H., Zhu, J. J., Muandet, K., & Sch\u00f6lkopf, B. (2022, June). Functional generalized empirical likelihood estimation for conditional moment restrictions. In International Conference on Machine Learning (pp. 11665-11682). PMLR.\n\n[2] Chen, Y., Xu, L., Gulcehre, C., Paine, T. L., Gretton, A., De Freitas, N., & Doucet, A. (2022). On instrumental variable regression for deep offline policy evaluation. The Journal of Machine Learning Research, 23(1), 13635-13674."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3969/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700092929215,
                "cdate": 1700092929215,
                "tmdate": 1700092929215,
                "mdate": 1700092929215,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gUErEuXarT",
                "forum": "ZFMiHfZwIf",
                "replyto": "CvdWk9ruzx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3969/Reviewer_rop6"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3969/Reviewer_rop6"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the clarifications. \nAfter reading the other reviews and responses, I still lean favorably towards this paper and will keep my current score. \nIn the future, I think investigating a model-free formulation of off-policy DAE could be impactful by simplifying the method, reducing its runtime and making it more scalable."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3969/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700690876569,
                "cdate": 1700690876569,
                "tmdate": 1700690876569,
                "mdate": 1700690876569,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]