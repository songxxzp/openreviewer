[
    {
        "title": "RelationMatch: Matching In-batch Relationships for Semi-supervised Learning"
    },
    {
        "review": {
            "id": "ca8vgBo3AU",
            "forum": "Lu5gGqhFTw",
            "replyto": "Lu5gGqhFTw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission523/Reviewer_SHZg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission523/Reviewer_SHZg"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes the matrix cross-entropy (MCE) loss for semi-supervised learning (SSL). In addition to matching the output of the strong augmentation with the pseudo-label of the weak augmentation, they also match the pairwise product of the output of the strong augmentation with that of the weak augmentation. Extensive theoretical analysis on the MCE loss reveals the nice theoretical property of the proposed approach. Experiments on benchmark datasets validate the effectiveness of the proposal."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "- The proposed MCE loss is novel and interesting. As far as I know, this is the first time it has been applied to the SSL literature. \n- The theoretical analysis is very comprehensive and sound. The nice theoretical property can promote further investigation of MCE in the community.\n- The empirical performance is very strong since the compared methods are very recent and strong methods in SSL."
                },
                "weaknesses": {
                    "value": "My small comment concerns the details of the writing, especially the notations. There may be some typos or unclear statements. \n\n- In section 2.1, it should read $\\log q_1$ instead of $\\log q_i$. \n- In section 2.1, what's Eq.(2.1)?\n- In Eq. 4, is $\\tilde{Y}_s$ the model output of weak augmentations? In a line above it is written as $\\tilde{Y}$. If they mean the same thing, the notation should be the same. \n- In Definition 4.2, it should be $n=0$ instead of $i=0$. \n\nThe author should check the notation carefully."
                },
                "questions": {
                    "value": "Since MCE can be simplified with a $l_2$-normalized matrix, what is the loss function used in the experiments? Is it still equation (1) with a non-normalized matrix?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission523/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698211735764,
            "cdate": 1698211735764,
            "tmdate": 1699635979579,
            "mdate": 1699635979579,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "y5Q859RnCf",
                "forum": "Lu5gGqhFTw",
                "replyto": "ca8vgBo3AU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission523/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission523/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> W:  My small comment concerns the details of the writing, especially the notations. There may be some typos or unclear statements.\n> - In section 2.1, it should read $\\log q_1$ instead of $\\log q_i$.\n> - In section 2.1, what's Eq.(2.1)?\n> - In Eq. 4, is $\\tilde{Y}_s$ the model output of weak augmentations? In a line above it is written as $\\tilde{Y}$. If they mean the same thing, the notation should be the same.\n> - In Definition 4.2, it should be n=0 instead of i=0.\n> The author should check the notation carefully.\n\nThank you for your careful reading and suggestions; we have resolved these issues in the new submission.\n\n\n> Q: Since MCE can be simplified with a $l_2$-normalized matrix, what is the loss function used in the experiments? Is it still equation (1) with a non-normalized matrix?\n\nA: Thank you for your questions. In the experiment, in order to prevent errors caused by some unknown mistakes, we used equation(1) + row l2-normalized $X$ (recall $Q$ = $\\frac{1}{b} X X^T$), which makes equation(2) equivalent to equation(1). However, for the sake of caution, we still used equation(1) as our loss in the experiments."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission523/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700547232268,
                "cdate": 1700547232268,
                "tmdate": 1700547232268,
                "mdate": 1700547232268,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GJo3tBkfjb",
                "forum": "Lu5gGqhFTw",
                "replyto": "y5Q859RnCf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission523/Reviewer_SHZg"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission523/Reviewer_SHZg"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the reply. I would keep my score."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission523/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700548192786,
                "cdate": 1700548192786,
                "tmdate": 1700548192786,
                "mdate": 1700548192786,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "dig0Ls9oU1",
            "forum": "Lu5gGqhFTw",
            "replyto": "Lu5gGqhFTw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission523/Reviewer_Hi55"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission523/Reviewer_Hi55"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates the challenges in semi-supervised learning. The authors highlight that prior research has often overlooked the interconnections between data points in a batch. To address this gap, they introduce RelationMatch, an approach designed to harness the consistency of relationships within a batch of unlabeled data."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. The paper is well written and easy to understand. \n2. The authors present the derivation of the proposed MCE Loss through the lenses of matrix analysis and information geometry, showcasing its advantageous characteristics such as convexity, boundedness from below, and optimizable properties."
                },
                "weaknesses": {
                    "value": "1. My primary concern pertains to the paper's novelty. SimMatch[1] has previously addressed the relationship between data points by applying consistency regularization at both the semantic and instance levels, promoting identical class predictions and maintaining similarity relations with other instances for different augmentations of the same instance. A detailed discussion and comparison between SimMatch and RelationMatch are essential to elucidate the distinct contributions of the latter.\n\n2. The benchmark comparison appears outdated. The most recent method evaluated in the paper is from 2021, and although the authors mention some methods from 2022 and 2023, such as FreeMatch, MaxMatch, and NP-Match, these have not been included in the experimental comparisons. When compared with the latest methods, RelationMatch does not seem to meet the state-of-the-art standard.\n\n3. The experimental scope of the paper is limited to toy datasets. To bolster the findings, it is recommended to extend the experiments to more complex, real-world datasets, such as ImageNet.\n\n\n[1] Zheng, Mingkai, et al. \"Simmatch: Semi-supervised learning with similarity matching.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022."
                },
                "questions": {
                    "value": "See weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission523/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission523/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission523/Reviewer_Hi55"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission523/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698737119916,
            "cdate": 1698737119916,
            "tmdate": 1699635979512,
            "mdate": 1699635979512,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "oXTUfER7yH",
                "forum": "Lu5gGqhFTw",
                "replyto": "dig0Ls9oU1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission523/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission523/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your careful reading and questions.\n\n> W1: My primary concern pertains to the paper's novelty. SimMatch[1] has previously addressed the relationship between data points by applying consistency regularization at both the semantic and instance levels, promoting identical class predictions and maintaining similarity relations with other instances for different augmentations of the same instance. A detailed discussion and comparison between SimMatch and RelationMatch are essential to elucidate the distinct contributions of the latter.\n\n- Our approach is mainly a natural extension of CE (Cross-Entropy), and we find out that when using one-hot pseudo-labels in semi-supervised settings, our new method can be understood as capturing the relation. SimMatch, on the other hand, utilizes contrastive loss for consistency regularization of the relation, which differs from our starting point.\n\n- One point that we need to emphasize is that our method has very good theoretical properties (as shown in Section ...), and at the same time, our method can be easily applied to most existing methods. Therefore, for the sake of fairness, we will not consider using SimMatch as a baseline for performance comparison in the current version. We will include SimMatch as a baseline when we have sufficient computational resources to conduct experiments applying our method to SimMatch.\n\n  We have added these two discussions in the appendix (Section C).\n\n---\n\n> W2: The benchmark comparison appears outdated. The most recent method evaluated in the paper is from 2021, and although the authors mention some methods from 2022 and 2023, such as FreeMatch, MaxMatch, and NP-Match, these have not been included in the experimental comparisons. When compared with the latest methods, RelationMatch does not seem to meet the state-of-the-art standard.\n\n Thank you for your careful reading and suggestions. Due to the shortage of computational resources, we only had time to conduct complete experiments on FixMatch / FlexMatch + MCE.\n\n During the rebuttal period, we attempted to apply our method to FreeMatch. Due to the scarcity of computational resources, we are only able to finish experiments on CIFAR10 and STL-10. The results are shown in the following table, and it can be seen that our method has a significant improvement on the task of CIFAR10 with 40 labels. \n\n| Metric | CIFAR10 40 | CIFAR10 250 | CIFAR10 4000 | STL-10 40 | STL-10 250 | STL-10 1000 |\n|--------|------------|-------------|----------------|-----------|------------|-------------|\n| *FreeMatch* | 4.90 \u00b10.04 | 4.88 \u00b10.18 | 4.10 \u00b10.02 | 15.56 \u00b10.55 | - | 5.63 \u00b10.15 |\n| *RelationMatch (w/ self-adaptive threshold)* | 4.80 \u00b10.06 | 4.89 \u00b10.14 | 4.08 \u00b10.05 | 15.60 \u00b14.09 | 10.42 \u00b10.78 | 5.62 \u00b10.18 |\n\n\nWhat we need to point out is that our core idea is to propose a new loss, MCE, which is a universal enhancement that can be applied to most semi-supervised learning algorithms. We will supplement the experimental results with more advanced methods + MCE as soon as we have sufficient computing resources. Our current implementation of FixMatch / FreeMatch + MCE, when compared to the recent FreeMatch, MaxMatch, and NP-Match, is still not entirely fair. \n\n---\n\n> W3: The experimental scope of the paper is limited to toy datasets. To bolster the findings, it is recommended to extend the experiments to more complex, real-world datasets, such as ImageNet.\n\nThank you for your suggestion. We will carry out the ImageNet-related experiments as soon as we have sufficient computing resources available. However, we would like to emphasize that our method performs very well on our current benchmarks, especially when labels are scarce. For example, there is a 15.21% improvement on STL with 40 labels."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission523/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700547024211,
                "cdate": 1700547024211,
                "tmdate": 1700547024211,
                "mdate": 1700547024211,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3Ic1M4yKGt",
                "forum": "Lu5gGqhFTw",
                "replyto": "dig0Ls9oU1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission523/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission523/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Seeking Your Input on Revised Paper's Alignment with ICLR Standards"
                    },
                    "comment": {
                        "value": "Dear Reviewer Hi55,\n\nAs the discussion period approaches its conclusion, **we want to ensure that we have thoroughly addressed all your concerns and that our revised paper fully meets the standards of ICLR**. We would highly value any additional feedback you may provide.\n\nThank you sincerely for your time and consideration.\n\nBest regards,\n\nThe Authors"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission523/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700730098634,
                "cdate": 1700730098634,
                "tmdate": 1700730098634,
                "mdate": 1700730098634,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "U18u06jxGK",
                "forum": "Lu5gGqhFTw",
                "replyto": "3Ic1M4yKGt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission523/Reviewer_Hi55"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission523/Reviewer_Hi55"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your reply. I will keep my original score because (1)  experimental comparison with SimMatch is necessarily needed (2) when compared with freematch, the improvement is very marginal (in fact, on many datasets, freematch is better)."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission523/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700733101235,
                "cdate": 1700733101235,
                "tmdate": 1700733101235,
                "mdate": 1700733101235,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Erm35bTMGa",
            "forum": "Lu5gGqhFTw",
            "replyto": "Lu5gGqhFTw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission523/Reviewer_8nMg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission523/Reviewer_8nMg"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces the consistency between each pair of weak and strong augmentation within a batch in semi-supervised learning."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. the paper proposes a novel idea, which consider in-batch relationship in SSL.\n2. The paper proposes matrix cross-entropy, which has a theoretical foundation and interpretations.\n3. Good writing, easy to follow, I appreciate the warm-up example, which is helpful for understanding."
                },
                "weaknesses": {
                    "value": "1. Figure 1 can be improved. There are too many lines, which are confusing.\n2. Large dataset experiments are missing, e.g., ImageNet \n3. Ablation studies on $\\mu$ and $\\gamma$ are missing.\n4. Formulations and notations are not clear. What's the definition of $Y_s$ and $X_s$ in eq(4)? \n5. How MCE connect with Relation in the introduction?"
                },
                "questions": {
                    "value": "1. Would you consider a relation (strongeaug dog, strongaug cat) > relation(weakaug dog, weakaug cat)? Intuitively, this relation more close to nature's rule.\n2. Is there an intuitive explanation of matrix cross-extropy? \n3. MCE(P, Q) = tr(\u2212P log Q + Q). For matrix cross-entropy, why +Q?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission523/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698756767875,
            "cdate": 1698756767875,
            "tmdate": 1699635979450,
            "mdate": 1699635979450,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BOGHOG3BcO",
                "forum": "Lu5gGqhFTw",
                "replyto": "Erm35bTMGa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission523/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission523/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your careful reading and questions.\n\n> W1 : Figure 1 can be improved. There are too many lines, which are confusing.\n\nThank you for your suggestion, we will fix this issue in the next submission. We deeply apologize for any confusion caused to you.\n\n> W2: Large dataset experiments are missing, e.g., ImageNet\n\nThank you for your suggestion. We will carry out the ImageNet-related experiments as soon as we have enough computing resources. However, we would like to emphasize that our method has performed very well on our existing benchmarks, especially when labels are scarce. For instance, there is an improvement of 15.21 on STL with 40 labels, an increase of 0.6 (FixMatch) on CIFAR10 with 40 labels, and a rise of 0.63 (FixMatch) on CIFAR100 with 40 labels.\n\n> W3: Ablation studies on $\\mu$ and $\\gamma$ are missing.\n\nThank you for your suggestion. Please note that we have already emphasized in the original text that our hyperparameter selection is made to ensure that the coefficient of the cross-entropy loss in the loss function is consistent with that of the baseline, in order to make a fair comparison.\n\n> W4: Formulations and notations are not clear. What's the definition of $Y_s$ and $X_s$ in eq(4)?\n\nThank you for your question. We have made changes to the manuscript to make the equation clearer.\n\n> W5: How MCE connect with Relation in the introduction?\n\nThank you for your question. We have discussed in Section 2.1 that the relation matrix can be easily generated by the predicted probability. Note MCE has minimization property(Proposition 4.8), so it can match the relation matrix of weakly and strongly augmented images from RelationMatch loss given in equation (4).\n\n> Q1: Would you consider a relation (strongeaug dog, strongaug cat) > relation(weakaug dog, weakaug cat)? Intuitively, this relation more close to nature's rule.\n\nThank you for your question. In this paper, we consider improving SSL by incorporating matching the total relationship of **a batch** of weakly augmented images to their strongly augmented counterpart batches. This matching forces consistency regularization, not an unequal relationship like the one you show. We only consider consistency regularization mainly as this philosophy is shown to be beneficial in semi-supervised learning among literatures.\n\n> Q2: Is there an intuitive explanation of matrix cross-extropy?\n\nYes. When P and Q have all its trace 1 (density matrix), the MCE can be seen as using cross-entropy on the spectrum of P and Q.\n\n> Q3: MCE(P, Q) = tr(\u2212P log Q + Q). For matrix cross-entropy, why +Q?\n\nThis is for better theoretical properties for matrix Q when it is a positive semi-definite matrix and not a matrix whose trace is 1, we generalize the probability distribution defined in the density matrix (the standard result used in quantum information theory) into any positive semi-definite matrix, with multiple theoretical guarantees."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission523/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700546626961,
                "cdate": 1700546626961,
                "tmdate": 1700546749242,
                "mdate": 1700546749242,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "I9ogKylpqE",
            "forum": "Lu5gGqhFTw",
            "replyto": "Lu5gGqhFTw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission523/Reviewer_BTXg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission523/Reviewer_BTXg"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the problem of semi-supervised learning, which is a common and interesting area. The author proposes RelationMatch, an innovative semi-supervised learning framework that capitalizes on these relationships through a novel Matrix Cross-Entropy (MCE) loss function. Extensive empirical evaluations, including a 15.21% accuracy improvement over FlexMatch on the STL-10 dataset, have demonstrated that RelationMatch consistently outperforms existing state-of-the-art methods.corruptions."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper is well-written, well-organized, and easy to follow.\n2. The paper addresses a novel and important problem, i.e., the relationships among data points within a batch, which has not been well-studied in the literature. \n3. This method can be easily incorporated with other works"
                },
                "weaknesses": {
                    "value": "1. The experiment appears somewhat insufficient, as only two experiments were conducted in the main text, and they were tested on just two to three datasets. Additionally, I am curious as to why the STL-10 dataset was omitted from Table 1.\n2. Based on the results presented in Table 1, the displayed accuracy results show limited differentiation. The matrix cross-entropy outperformed by a margin of less than 0.3%. This could potentially be attributed to randomization and perturbations.\n3. Potential failure modes or limitations not discussed."
                },
                "questions": {
                    "value": "The primary questions for the rebuttal primarily arise from the \"weaknesses\" section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission523/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699014656988,
            "cdate": 1699014656988,
            "tmdate": 1699635979375,
            "mdate": 1699635979375,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "3i3n9RVfxA",
                "forum": "Lu5gGqhFTw",
                "replyto": "I9ogKylpqE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission523/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission523/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> W1: The experiment appears somewhat insufficient, as only two experiments were conducted in the main text, and they were tested on just two to three datasets. Additionally, I am curious as to why the STL-10 dataset was omitted from Table 1.\n\nThank you for your suggestions; we will add more experiments in the future version. As for the omission of the STL-10 dataset in Table 1, this is mainly because STL-10 essentially is not suited for supervised training, since the majority of images it contains are unlabeled, and within the labeled images, each class has 800 test images and 500 train images, which is clearly an unbalanced ratio for conducting supervised experiments.\n\n> W2: Based on the results presented in Table 1, the displayed accuracy results show limited differentiation. The matrix cross-entropy outperformed by a margin of less than 0.3%. This could potentially be attributed to randomization and perturbations.\n\nThank you for your careful reading and questions. In order to minimize the impact of randomness, we have repeated the experiment **3** times and reported the means and variances. Additionally, we must point out that, in supervised learning, when the baseline accuracy has already surpassed 95%, even an improvement of 0.3% is very significant.\n\n>W3: Potential failure modes or limitations not discussed.\n\nThank you for your suggestion. We will try to incorporate potential failure modes as well as limitations after a thorough investigation."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission523/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700546420655,
                "cdate": 1700546420655,
                "tmdate": 1700546420655,
                "mdate": 1700546420655,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]