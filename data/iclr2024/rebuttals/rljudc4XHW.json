[
    {
        "title": "DynamicBEV: Leveraging Dynamic Queries and Temporal Context for 3D Object Detection"
    },
    {
        "review": {
            "id": "L5cJcdS2To",
            "forum": "rljudc4XHW",
            "replyto": "rljudc4XHW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission384/Reviewer_Pvpq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission384/Reviewer_Pvpq"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a lightweight and effective method for aggregating BEV pillar features using K-means clustering and Top-K Attention. The authors also introduce a Diversity Loss to prevent the attention mechanism from focusing too heavily on the most relevant features. The proposed method is evaluated on the nuScenes dataset and outperforms previous methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The proposed clustering and top-K attention mechanism are simple and intuitive, yet achieve strong performance compared to previous state-of-the-art methods (Table 1). Extensive ablation studies in Section 4.4 demonstrate the benefits of the proposed modules."
                },
                "weaknesses": {
                    "value": "Latency: Section 3.3 states that \"the computational efficiency of DynamicBEV is one of its key advantages\u201d. However, not all floating-point operations (FLOPs) are created equal, especially for the clustering operation on GPUs, TPUs, and other edge devices. It would be helpful if the authors could measure the latency of the full model and provide a breakdown of the latency of each component (e.g., clustering, sorting top-k).\n\nGeneralization: Evaluating the proposed method on only one dataset is not sufficient. I suggest evaluating the proposed method on at least one additional dataset.\n\nVisualization: Could the authors provide detailed illustrations on K-mean clustering and Top-K attention in Fig1?\nFig 2 is not clear.  What does each color mean?"
                },
                "questions": {
                    "value": "Please see weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission384/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698634128872,
            "cdate": 1698634128872,
            "tmdate": 1699635965570,
            "mdate": 1699635965570,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "JFDUVnUcK0",
            "forum": "rljudc4XHW",
            "replyto": "rljudc4XHW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission384/Reviewer_hs8x"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission384/Reviewer_hs8x"
            ],
            "content": {
                "summary": {
                    "value": "The authors explore and analyze the existing query-based paradigm for 3D BEV-based object detection, and propose to adopt dynamic queries to do temporal feature extraction. The experimental results on nuScenes, show the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The task of query-based paradigm for 3D BEV-based object detection is popular and interesting in the 3D community. The authors propose to use dynamic queries to do feature learning and make it work on nuScenes."
                },
                "weaknesses": {
                    "value": "1. Performance difference on large-scale vs small-scale objects. It would be interesting if the authors could show the detailed detection performance of 10 classes on the nuScenes. From my understanding, the proposed method is kind of sensitive to different objects with different sizes.\n\n2. It is unclear to me how to define the size of associated feature cluster, and the number of the query.  \n\n3. More quantitative/qualitative results. The manuscript does report detection numbers on nuScenes validation set, however, the authors forgot to compare their methods with recent SOTAs on the test set. Also, it would be much convincing if the authors can present some qualitative results or report the results on more public datasets, i.e., KITTI or Waymo.\n\n4. I am curious about the inference time of the proposed method. The authors repeatedly claimed that the traditional temporal fusion is heavy computation, however, the attention computation is also heavy from my understanding."
                },
                "questions": {
                    "value": "Please refer to the weakness part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission384/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission384/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission384/Reviewer_hs8x"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission384/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698730836658,
            "cdate": 1698730836658,
            "tmdate": 1699635965496,
            "mdate": 1699635965496,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "L3hJ6S11A1",
            "forum": "rljudc4XHW",
            "replyto": "rljudc4XHW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission384/Reviewer_P6ky"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission384/Reviewer_P6ky"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents dynamic queries for 3D object detection in bird's-eye view, distinguishing it from the static queries employed in SparseBEV. To enhance the model's performance, the authors have introduced K-means clustering and Top-K Attention mechanisms, which facilitate the integration of global features into the queries. Additionally, the paper introduces a diversity loss to encourage queries to focus on all clustered features. Then, a Lightweight Temporal Fusion Module is illustrated to speed up multi-frame fusion by using pre-computed features."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. I would like to compliment you on the clear and concise language used throughout the manuscript, as well as the well-designed figures and tables. These elements greatly enhance the readability and understandability of the paper.\n2. It is clever to use clustering attention to reduce the computation cost of global attention."
                },
                "weaknesses": {
                    "value": "1. The proposed dynamic queries is not new. Prior research, such as CMT[1] and UVTR[2], has already demonstrated the adjustment of queries in each decoder layer. Moreover, CMT employs global attention, while UVTR utilizes local attention to update query features, raising questions about the novelty of the proposed dynamic queries.\n\n2. There are some experiment omissions that limit the comprehensiveness of the evaluation. Notably, there is a lack of crucial comparisons, such as latency comparisons with SparseBEV and an analysis of the performance-to-latency trade-off when employing clustering attention in contrast to the global attention mechanism used in CMT.\n\n[1] Cross Modal Transformer: Towards Fast and Robust 3D Object Detection, in ICCV 2023.\n[2] Unifying Voxel-based Representation with Transformer for 3D Object Detection, in NeurIPS 2022."
                },
                "questions": {
                    "value": "Please see the Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission384/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission384/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission384/Reviewer_P6ky"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission384/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698769471441,
            "cdate": 1698769471441,
            "tmdate": 1699635965409,
            "mdate": 1699635965409,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "oVgLitqOwT",
            "forum": "rljudc4XHW",
            "replyto": "rljudc4XHW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission384/Reviewer_NE19"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission384/Reviewer_NE19"
            ],
            "content": {
                "summary": {
                    "value": "The authors introduce a new paradigm in DynamicBEV, a novel approach that employs dynamic queries for BEV-based 3D object detection. The proposed dynamic queries exploit K-means clustering and Top-K Attention creatively to aggregate information more effectively from both local and distant features, which enables DynamicBEV to adapt iteratively to complex scenes. LTFM is designed for efficient temporal context integration with a significant computation reduction."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. k-means clustering determines how pillars fit into localized patterns and features in 3D space, facilitating a detailed understanding of the characteristics of the object.\n2. Diversity loss ensuring that the model is not overly focused on dominant features promotes a balanced focus on the clustering of various features\n3. LTFM embodies the essence of computational efficiency and relieves the need for resource-intensive operations by leveraging existing calculations to manage temporal context\n4. DynamicBEV outperforms sota methods on the nuScenes validation dataset"
                },
                "weaknesses": {
                    "value": "1. Missing nuScenes test results and the paper is difficult to understand and lacks the necessary visualizations\n2. What do the surrounding features mean, and can authors explain dividing the surrounding features F of each query into K clusters C1, . . . , CK? Why use k-means to cluster features? And Fig.3 (a) does not show a big gap between k=5,6 or 7. What is aggregate based on? Is it the distance between features?\n3. Why use tok-k attention? If the authors want local information, deformable attention is a choice. \n4. The authors use Iterative Update and repeat the K-means clustering and Top-K Attention steps. So, I think the authors should report the inference speed.\n5. For LTFM, authors should compare with StreamPETR for a fair comparison.\n\nMinor: Fig.2 shows the difference between static query and dynamic query-based methods but lacks detailed explanations. (Similar in Fig.1)"
                },
                "questions": {
                    "value": "see weakness"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission384/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699010430091,
            "cdate": 1699010430091,
            "tmdate": 1699635965339,
            "mdate": 1699635965339,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]