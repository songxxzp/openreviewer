[
    {
        "title": "3D Feature Prediction for Masked-AutoEncoder-Based Point Cloud Pretraining"
    },
    {
        "review": {
            "id": "TuUJMtTYsG",
            "forum": "LokR2TTFMs",
            "replyto": "LokR2TTFMs",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7920/Reviewer_UgQs"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7920/Reviewer_UgQs"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors apply the self-supervised pretraining paradigm of masked signal modeling to point cloud pretraining. They propose a novel approach called MaskFeat3D, which focuses on recovering high-order features of masked points rather than their locations. Additionally, they propose an encoder-agnostic attention-based decoder. The effectiveness of the proposed method is evaluated through experiments conducted on the ScanObjectNN dataset for shape classification and the ShapeNetPart dataset for shape part segmentation."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The authors present evidence that the recovery of high-order point features yields more effective results compared to the recovery of point positions for 3D masked signal modeling.\n- A novel encoder-agnostic attention-based decoder is proposed by the authors to accurately regress the high-order features of masked points.\n- The paper is well-written and provides clear explanations, making it easy to follow."
                },
                "weaknesses": {
                    "value": "- It appears that this is not the first work in 3D masked signal modeling that focuses on recovering high-order features of masked points. For example, MaskSurfel (Zhang et al.) specifically aims to recover surface normals of points. This similarity with previous work may diminish the novelty of the paper.\n- The results on the ScanObjectNN dataset indicate that Point-MA2E outperforms MaskFeat3D significantly; however, this comparison is not included in the paper.\n- To assess the effectiveness of the decoder, it is recommended to include results obtained by combining the block features and masked queries, and feeding them into self-attention blocks of the same depth, similar to the MAE approach, with points used as the positional embeddings.\n\n\n[1] Zhang et al., Point-MA2E: Masked and Affine Transformed AutoEncoder for Self-supervised Point Cloud Learning."
                },
                "questions": {
                    "value": "See Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7920/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7920/Reviewer_UgQs",
                        "ICLR.cc/2024/Conference/Submission7920/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7920/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698672502863,
            "cdate": 1698672502863,
            "tmdate": 1700661197631,
            "mdate": 1700661197631,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "nFNdl8MGXl",
                "forum": "LokR2TTFMs",
                "replyto": "TuUJMtTYsG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7920/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7920/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Authors (Part 1)"
                    },
                    "comment": {
                        "value": "We extend our heartfelt thanks to the reviewer for their insightful feedback. In addressing the concerns raised, we are pleased to provide the following clarifications and additional information.\n\n**Q: Not the first work on recovering high-order features of masked points.**\n\nWe thank the reviewer for pointing this out. We argue that there are key differences when comparing our method with MaskSurfel:\n\n1. From the perspective of insights, our approach, as the reviewer wYee notes, emphasizes the importance of ignoring the recovery of irregular and potentially noisy point positions, and focusing on intrinsic surface features for effective 3D MSM pre-training. In contrast, MaskSurfel predicts not only surface normals but also **point positions**. Furthermore, the surface normal prediction in MaskSurfel depends on the position prediction.\n2. From the perspective of model design, we propose a novel encoder-agnostic attention-based decoder specifically aimed at ignoring point position and concentrating solely on intrinsic features. Conversely, MaskSurfel made an extenion from Point-MAE. The decoder design of MaskSurfel is the same as Point-MAE.\n3. From the perspective of experiment result, our method shows better results than MaskSurfel. As detailed in Table 4 of the main paper (referenced below), MaskSurfel's result is categorized as \u201cposition + normal\" under the PointMAE section. As a fair comparison, our method taking only surface normal as the target feature (\"normal\" under MaskFeat3D section) also shows a significant improvement compared with MaskSurfel. This experiment further proves the validity of our insights and the effectiveness of our decoder design.\n\n----\n\n\n\n| Method     | Target Feature                 |      | ScanObjectNN PB-T50-RS |\n| ---------- | ------------------------------ | ---- | ---------------------- |\n| PointMAE   | position only                  |      | 85.2                   |\n|            | **position + normal**\u2217         |      | **85.7**               |\n|            | position + surface variation\u2217  |      | 85.9                   |\n|            | position + normal + variation\u2217 |      | 86.0                   |\n| MaskFeat3D | **normal**                     |      | **86.5**               |\n|            | surface variation              |      | 87.0                   |\n|            | normal + surface variation     |      | 87.7                   |\n\n**Table 4: Ablation study on different features.** \u2217 uses position-index matching Zhang et al.[1] for feature loss computation.\n\n----\n\n\n\n**Q: Point-MA2E shows better results.**\n\nWe are grateful to the reviewer for mentioning Point-MA2E, which introduces an intriguing concept of improving the masking strategy with affine transformation. However, it is important to clarify the idea of Point-MA2E is orthogonal to our method. Point-MA2E is dedicated to refining the masking strategy, while our work focuses on studying the pre-training pretext task. We speculate that Point-MA2E's mask strategy might improve our method, and we will provide a discussion on this in our revised manuscript.\n\nAdditionally, **the Point-MA2E paper has not yet been published, and its supplementary material remains unavailable**. This restricts our ability to fully grasp their evaluation procedures and conduct further systematical comparisons. \n\n\n\n[1] Zhang, Yabin, et al. \"Masked surfel prediction for self-supervised point cloud learning.\" *arXiv preprint arXiv:2207.03111* (2022)."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7920/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699907298276,
                "cdate": 1699907298276,
                "tmdate": 1699907298276,
                "mdate": 1699907298276,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dL1qybuNvv",
                "forum": "LokR2TTFMs",
                "replyto": "WY3kUJU3st",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7920/Reviewer_UgQs"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7920/Reviewer_UgQs"
                ],
                "content": {
                    "title": {
                        "value": "Post-rebuttal assessment"
                    },
                    "comment": {
                        "value": "I appreciate the authors' responses which sufficiently address my concerns. I also observed that they conducted more experiments on the semantic segmentation task in their response to Reviewer HyKn, which demonstrates the versatility of their methods. Consequently, I would revise my rating to a positive one."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7920/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700661181169,
                "cdate": 1700661181169,
                "tmdate": 1700661181169,
                "mdate": 1700661181169,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "cTJSF5NfY6",
            "forum": "LokR2TTFMs",
            "replyto": "LokR2TTFMs",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7920/Reviewer_cRKn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7920/Reviewer_cRKn"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a point cloud pre-training method to improve the downstream tasks\u2019 performances. More specifically, instead of predicting point positions by a masked autoencoder, the authors propose to recover high-order features at masked points including surface normals and surface variations through a novel attention-based decoder. To verify the effectiveness of the method, various point cloud analysis tasks have been tested, and promising results have been achieved."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The idea is interesting, and the results are promising.\n2. Extensive experiments are conducted with SOTA performances.\n3. The paper is clearly written and well-organized."
                },
                "weaknesses": {
                    "value": "It seems that the ablation study shown in Table 4 failed to support the idea that it is essential to disregard point position recovery, since at the same time to predict point positions using PointMAE, the decoder architecture is also changed when using MaskFeat3D architecture. To make a fairer comparison, the same decoder architecture from MaskFeat3D should be used to predict point position as well."
                },
                "questions": {
                    "value": "Since the authors claim that it is essential to disregard point position recovery. Hence, how to understand that predicting point positions actually enhances the performances when using PointMAE in Table 4?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7920/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7920/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7920/Reviewer_cRKn"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7920/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698678405291,
            "cdate": 1698678405291,
            "tmdate": 1699636972357,
            "mdate": 1699636972357,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Oj3attTqUu",
                "forum": "LokR2TTFMs",
                "replyto": "cTJSF5NfY6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7920/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7920/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We deeply appreciate your insightful and positive comments about our contributions to the community. \n\n**Q: Predict point position under the same archtecture of MaskFeat3D.**\n\nWe thank the reviewer for pointing this out. Since our method has already taken the point position as an additional input for the decoder, it is trivial to use our decoder architecture to predict the point position. The model would simply be learning to replicate the input it was already given. Following the reviewer's suggestion, we conducted this experiment, and the results are presented below. We found that only predicting point positions does not lead to any improvement in downstream tasks. Furthermore, we experimented additional experiments with  'position + normal', 'position + surface variation', and 'position + normal + surface variation'.  We observe that adding position prediction to our method does not yield any improvement. These results prove that incorporating position prediction to our method is unnecessary.\n\n----\n\n| Target Feature                        | ScanObjectNN **PB-T50-RS** |\n| ------------------------------------- | -------------------------- |\n| Train from scratch                    | 77.2                       |\n| position only                         | 77.5                       |\n| position + normal                     | 86.4                       |\n| normal                                | 86.5                       |\n| position + surface_variation          | 87.1                       |\n| surface variation                     | 87.0                       |\n| position + normal + surface variation | 87.6                       |\n| normal + surface variation            | 87.7                       |\n\n**Table: Ablation study on the MaskFeat3D architecture.** All experiments were conducted using the same MaskFeat3D decoder architecture. The term 'Train from scratch' denotes no pretraining procedure.\n\n----\n\n\n\n**Q: How to understand that predicting point positions actually enhances the performances?**\n\nWe are grateful for the reviewer's inquiry, although it does bring some confusion on our end. The primary objective of Table 4 is twofold: to demonstrate that integrating intrinsic 3D features can indeed enhance the efficacy of the Point-MAE framework, and to highlight that, even with these enhancements, their performance remains markedly inferior to that achieved by our decoder design. \n\nIt is crucial to clarify that the results in Table 4 do not suggest that predicting point positions enhances performance. Furthermore, as we mentioned in the previous question, our experimental findings indicate that focusing on the prediction of point positions under our decoder architecture does not lead to any improvements in performance."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7920/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699905669720,
                "cdate": 1699905669720,
                "tmdate": 1699905669720,
                "mdate": 1699905669720,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OP9zzCTwxX",
                "forum": "LokR2TTFMs",
                "replyto": "Oj3attTqUu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7920/Reviewer_cRKn"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7920/Reviewer_cRKn"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the rebuttal."
                    },
                    "comment": {
                        "value": "The rebuttal has resolved my concerns, and I keep my positive rating."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7920/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700573757591,
                "cdate": 1700573757591,
                "tmdate": 1700573757591,
                "mdate": 1700573757591,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ZkxefuMaWg",
            "forum": "LokR2TTFMs",
            "replyto": "LokR2TTFMs",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7920/Reviewer_wYee"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7920/Reviewer_wYee"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a self-supervised learning method from point cloud. Typically, this paper addresses the importance of using surface normal and surface variance instead of using point location as proposed by the previous studies. The idea is straightforward and easy-to-understand. The experiments demonstrate that the efficacy of the proposed method. Moreover, the ablation study consistently proves the addressed issue by the authors."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The authors address the importance of the geometric measurements for the usage of pre-training the network. Typically, using surface normal as surface variation are meaningful in point cloud based understanding. Typically, the authors provide the various experiments such as backbone architectures, loss designs, and downstream task evaluations. I really enjoyed reading this paper."
                },
                "weaknesses": {
                    "value": "There are some minor things that need to be discussed\n\nW-1. Analysis of 2D/3D masked autoencoders.\n\nIn the manuscript, the authors commented that __\"These designs make an intrinsic difference from 2D MSMs, where there is no need to recover masked pixel locations.\"__\n\nIt is true. I understand the analysis by the authors. When we think of the vanilla MAE, it also takes a masked image as an input and predicts the color information, not its pixel location. However, when we think of the nature of the point cloud, it is sparse, irregular, and unordered. Even, I would say _raw point cloud_ naturally does not involve color information. Accordingly, it is not feasible to extend the concept of the MAE for the 2D image into the MAE for the 3D points. In my opinion, the authors should have written such a clear understanding of MAE for 3D points. \n\nW-2. Details in computing surface normal and surface variance on scene-level experiments.\nWhile the various object-level datasets, such as shapenet, are synthetically created, the real-world points are captured by the sensors. Due to such difference, raw point cloud from the real world naturally involves lots of noise, which could be an issue when computing surface normal using PCA. So I wonder how the authors solve this issue when conducting experiments on Sec. 4-4 in the manuscripts.\n\nW-3. Insightful analysis\nI truly agree that the proposed experiments demonstrate that the surface normal and surface variance are important measurements for self-supervising learning using 3D points. Technically, I also agree with such an observation. However, I wonder why such an approach brings performance improvement. Is there any geometric analysis? Based on the manuscript, this approach can be viewed as a naive extension of the Point-MAE that additionally uses other geometric measurements. \n\nI want to know the author's own analysis of such problem setup and insights."
                },
                "questions": {
                    "value": "Alongside with the addressed weakness, I have one minor question.\n\nQ-1. __Is there any reason that authors did not conduct experiments on the S3DIS dataset using 3D semantic segmentation?\nIf there are some reasonable and meaningful results, I can convince the efficacy of this work. Otherwise, this work could be understood as naive extension.__"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "There is no ethic issues."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7920/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7920/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7920/Reviewer_wYee"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7920/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698752663106,
            "cdate": 1698752663106,
            "tmdate": 1700546897600,
            "mdate": 1700546897600,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Bk5qaUYTNS",
                "forum": "LokR2TTFMs",
                "replyto": "ZkxefuMaWg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7920/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7920/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Authors (Part 1)"
                    },
                    "comment": {
                        "value": "We extend our heartfelt thanks to the reviewer for the valuable feedback. In addressing the concerns raised, we are pleased to provide the following clarifications and additional information.\n\n**W-1: Analysis of 2D/3D masked autoencoders.**\n\nWe are grateful for the insightful suggestion and deeply appreciate the reviewer's accurate understanding of our insight. We agree that, unlike grid-organized 2D image data, point cloud data is irregular and possibly noisy, making direct recovery of point positions less effective in capturing the intrinsic features of 3D shapes. We will clarify our statement more explicitly in our revision.\n\n**W-2: Surface normal is noisy in real world scenes.**\n\nWe thank the reviewer for this valuable feedback. During our experiments, we also observed that surface normals in scene data are noisy and often lack consistent orientation due to incomplete point clouds. However, we found that surface variation remains robust in these conditions. Therefore, as mentioned in Section 4.4, we took the rgb color signal and surface variation as the target features, rather than the surface normal.  \n\n**W-3-1: Naive extension of Point-MAE**.\n\nWe appreciate the reviewer for raising this important concern. We would like to argue that our method is not merely a simple extension of Point-MAE.\n\nFirstly, as elaborated in Section 1 and illustrated in Figure 1, in addition to the insight mentioned by the reviewer in W-1 (which we believe is also an important contribution), we introduce a general pre-training framework with a novel attention-based decoder, thereby enhancing the pre-training model. Our model is not restricted to any specific encoder design and is easily adaptable to various task settings, including those in challenging indoor environments. Also, by disregarding point positions and focusing on intrinsic 3D shape features, our model learns robust features suitable for diverse downstream tasks.\n\nSecondly, as mentioned in Table 4 of the main paper (referenced below), a naive extension of Point-MAE with additional features does not yield the same level of significant improvement as our method. This finding further proves the validity of our insights and the effectiveness of our decoder design.  \n\n----\n\n\n\n| Method     | Target Feature                 |      | ScanObjectNN PB-T50-RS |\n| ---------- | ------------------------------ | ---- | ---------------------- |\n| PointMAE   | position only                  |      | 85.2                   |\n|            | position + normal\u2217             |      | 85.7                   |\n|            | position + surface variation\u2217  |      | 85.9                   |\n|            | position + normal + variation\u2217 |      | 86.0                   |\n| MaskFeat3D | normal                         |      | 86.5                   |\n|            | surface variation              |      | 87.0                   |\n|            | normal + surface variation     |      | 87.7                   |\n\n**Table 4: Ablation study on different features.** \u2217 uses position-index matching Zhang et al.[1] for feature loss computation.\n\n----\n\n**W-3-2: More analysis.**\n\nIn the supplementary material, we provide further experimental analysis. In Section 2.3, we visualized the point features of our model and other MAE-based approaches. Our approach shows more discriminative features than other methods. This observation proves that learning to reconstruct high-order geometric features robustly enables the encoder to extract more distinctive and representative features.\n\nIn Section 2.4 of the supplementary material, we investigated how the learned features of our model assist in zero-shot correspondence learning, showing that points nearest in the feature space exhibit similar semantic information, even across different objects within the same class. We hope these analyses further highlight the robustness and effectiveness of our pre-training strategy.\n\n[1] Zhang, Yabin, et al. \"Masked surfel prediction for self-supervised point cloud learning.\" *arXiv preprint arXiv:2207.03111* (2022)."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7920/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699897179936,
                "cdate": 1699897179936,
                "tmdate": 1699897179936,
                "mdate": 1699897179936,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dhBn1TUSSm",
                "forum": "LokR2TTFMs",
                "replyto": "ZkxefuMaWg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7920/Reviewer_wYee"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7920/Reviewer_wYee"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the rebuttal."
                    },
                    "comment": {
                        "value": "__Overall, I am quite positive about this paper.__ Though it could be understood as a naive extension using surface normal and normal variance. For me, it is good enough. \n\nThe experiments are conducted in object-scale datasets as well as room-scale datasets, which makes it reasonable for me to understand the benefit of using surface normal as self-supervision. I hope that the authors will put these results in the final manuscript as well as the supplementary material.\n\nAlso, __I recommend the authors to modify the title.__ This is not a pioneering work that applies MAE to point cloud understanding. However, the title can mislead the readers. I think that the main contribution comes from using surface normal as a self-supervision signal. __While the authors insist on the novelty of the decoder network design, it looks trivial.__ It is not a big deal. Accordingly, the titles should involve the terminologies, such as surface normal, intrinsics points, or point normal, etc.\n\n__Change rate: 5 --> 6: marginally above the acceptance threshold__"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7920/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700546876562,
                "cdate": 1700546876562,
                "tmdate": 1700546935866,
                "mdate": 1700546935866,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "xySBITLeOI",
            "forum": "LokR2TTFMs",
            "replyto": "LokR2TTFMs",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7920/Reviewer_HyKn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7920/Reviewer_HyKn"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a pre-training task for 3D encoders, so, later, can lead to improved performance when fine-tuned on a downstream task. The pre-training objective is the prediction of point-surface properties such as normal or surface variation from masked regions of the input point cloud."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper proposes an alternative to point coordinates prediction on a mask auto-encoder setup. Sampling point coordinates can be difficult for decoder architectures as the ones used by previous works. However, by fixing the point coordinate in the decoder these problems disappear and the task becomes to predict shape properties around the queried point."
                },
                "weaknesses": {
                    "value": "I like the main idea of the paper, it is well presented and presents a significant improvement over previous works for most of the task. However, my main concern is not only related to this work in particular but to this line of works where they focus on tasks related to single objects. I have been playing around with these datasets for many years already, and I can say that datasets such as classification on ModelNet40, and segmentation on ShapeNet are relatively \"easy\", there is a lot of noise in the annotations, and I believe the improvements presented by current methods is simply overfitting to this specific data set. In other subfields of computer vision, a pre-training paper that is only evaluated on MNIST or CIFAR10 would not be accepted, but for some reason, they do for point clouds. So, I don't find these works convincing since the reported results and architectures usually do not translate to more challenging tasks such as semantic segmentation or instance segmentation on real 3D scans. That being said, this work presents results on the task of object detection of ScanNet and SUN-RGBD, which I believe is the right direction. However, I think more results reported on other tasks such as semantic or instance segmentation should be necessary to determine the quality of the pre-training strategy. Therefore, I will rate this paper marginally below the acceptance but I will be happy to see additional results during the rebuttal phase."
                },
                "questions": {
                    "value": "I would encourage the authors to include more challenging tasks such as semantic and instance segmentation of 3D scans."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7920/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7920/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7920/Reviewer_HyKn"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7920/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698768813444,
            "cdate": 1698768813444,
            "tmdate": 1700643140546,
            "mdate": 1700643140546,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "d9pefN8eTG",
                "forum": "LokR2TTFMs",
                "replyto": "xySBITLeOI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7920/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7920/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We express our sincere gratitude to the reviewer for the insightful feedback. In response to the concerns raised, we offer the following clarifications and additional information.\n\n**Q: This field only focuses on tasks related to single objects.**\n\nFirstly, we deeply appreciate the reviewer highlighting this phenomenon. It is indeed true that many studies in the point cloud field have primarily concentrated on shape-level tasks, which may not always be the optimal choice to evaluate pre-training strategies. This is also one of the reasons why we extended our method to more challenging tasks such as 3D object detection. However, as the reviewer mentioned, most works in the point cloud field have not pursued this direction.\n\nActually, we have conducted the semantic segmentation task on S3DIS Area 5 using the PointNeXt [1] encoder backbone in our submission. These results are detailed in Table 4 of supplementary material and summarized below. We observed a significant improvement of +0.9/+1 in mean Intersection over Union (mIoU) and Overall Accuracy (OA) compared to the baseline training from scratch. This outcome underlines the potential efficacy of our pre-training approach.\n\n----\n\n| Method     | S3DIS Area 5 | Semantic Seg |      |      | S3DIS Area 5 | Detection   |\n| ---------- | ------------ | ------------ | ---- | ---- | ------------ | ----------- |\n|            | mIoU         | OA           |      |      | mAP$_{0.25}$ | mAP$_{0.5}$ |\n| PointNeXt\u2020 | 70.8         | 90.7         |      |      | -            | -           |\n| MaskFeat3D | **71.7**     | **91.7**     |      |      | -            | -           |\n| FCAF3D\u2020    | -            | -            |      |      | 66.7         | 45.9        |\n| MaskFeat3D | -            | -            |      |      | **71.6**     | **49.2**    |\n\n**Supp Table 4: Area 5 Semantic segmentation and detection results on S3DIS.** \u2020 represents the *from scratch* results and MaskFeat3D in the same section represents the *fine-tuning* results using pretrained weights under same backbone.\n\n----\n\n\n\nTo further validate the effectiveness of our method, we conducted additional experiments for the rebuttal. As we claimed that our method is encoder-agnostic, we experimented with another commonly used encoder backbone, Sparse-UNet [2]. During pre-training, we maintained identical settings, only replacing the encoder backbone with Sparse-UNet.  The results of this experiment are as follows:\n\n| Method             | Backbone        | S3DIS Area 5 mIoU | S3DIS 6-Fold mIoU | ScanNet mIoU |\n| ------------------ | --------------- | ----------------- | ----------------- | ------------ |\n| Train from scratch | Sparse-UNet     | 68.2              | 73.6              | 72.2         |\n| PointContrast      | Sparse-UNet     | 70.9              | -                 | 74.1         |\n| DepthContrast      | Sparse-UNet     | 70.6              | -                 | 71.2         |\n| **MaskFeat3D**     | **Sparse-UNet** | **72.3**          | **76.4**          | **74.7**     |\n\nThese results, spanning S3DIS Area 5, S3DIS 6-Fold, and ScanNet datasets, consistently show improvements over training from scratch and outperform other pre-training strategies under the same encoder backbone. This evidence supports the effectiveness of our method for more challenging downstream tasks in indoor environments.\n\n[1] Qian, Guocheng, et al. \"PointNext: Revisiting pointnet++ with improved training and scaling strategies.\" In NeurIPS 2022.\n\n[2] Choy, Christopher, JunYoung Gwak, and Silvio Savarese. \"4D spatio-temporal convnets: Minkowski convolutional neural networks.\" In CVPR 2019."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7920/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699894557285,
                "cdate": 1699894557285,
                "tmdate": 1699894557285,
                "mdate": 1699894557285,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "f0vM8qzeYY",
                "forum": "LokR2TTFMs",
                "replyto": "d9pefN8eTG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7920/Reviewer_HyKn"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7920/Reviewer_HyKn"
                ],
                "content": {
                    "title": {
                        "value": "Post-rebuttal assessment"
                    },
                    "comment": {
                        "value": "The authors have included additional experiments on more complex tasks according to my suggestions. Moreover, they pointed out additional results that were also in the supplementary material of the original submission. Therefore, all my concerns have been addressed. I increased my score from 5 to 6.\nOne small remark that I think could improve the paper is to include the supplementary material as an appendix in the main paper. This will allow readers to analyze these additional experiments directly in the paper."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7920/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700643201729,
                "cdate": 1700643201729,
                "tmdate": 1700643201729,
                "mdate": 1700643201729,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]