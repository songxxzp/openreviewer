[
    {
        "title": "Enhanced Visual Instruction Tuning for Text-Rich Image Understanding"
    },
    {
        "review": {
            "id": "j3RtMTUVAR",
            "forum": "tj4a1JY03u",
            "replyto": "tj4a1JY03u",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6737/Reviewer_bdXG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6737/Reviewer_bdXG"
            ],
            "content": {
                "summary": {
                    "value": "This work improves the collection pipeline of instruction-following data, allowing for the collection of a large-scale dataset of text-rich images. Leveraging GPT-4, this work further constructs an instruction tuning dataset consisting of 422K noisy pretraining data and 16K conversations and validates it on the recent work LLAVA. The results on multiple text-based VQA datasets show that this dataset improves the performance of LLAVA in text understanding. Case analysis also demonstrates that LLAVAR has stronger image-text understanding abilities than LLAVA."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is well organized and easy to follow.\n2. The improved data collection pipeline overcomes the limitations of the existing dataset, which lacks text-rich images and relevant instruction-tuning annotations.\n3. The explanation of the data collection process is detailed, and the relevant experimental analysis provides support for investigating how to enhance visual Large Language Models' understanding of text-rich visual content."
                },
                "weaknesses": {
                    "value": "1. The contribution of this paper appears to be limited. The proposed data collection pipeline in this paper is based on the one used in LLAVA, but with incremental improvements. Similarly, the model architecture in this paper also follows LLAVA without a specific design for text-rich scenarios.\n2. The dataset introduced in this paper only brings limited improvement. In Table 2, LLAVAR achieves comparable performance with mPLUG-Owl, a model that was not trained on a text-rich dataset. The paper also does not provide a detailed comparison with other state-of-the-art models in the field of text-rich image understanding, which would help to better understand the relative performance of the proposed method.\n3. The fragmented OCR results in a few words may also exist in real-world text-rich data such as poster, table, directly removing this kind of data may be also different from the real-world distribution."
                },
                "questions": {
                    "value": "1. How is the performance if your dataset is trained based on mPLUG-Owl? Since it is a high baseline of your method. Can you further fine-tune other open-source models using the dataset from this article and provide performance comparisons?\n2. Can you provide experimental results and analysis on more text-rich image understanding  benchmarks (e.g., Information Extraction, Document Classification, OCR)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6737/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698730806183,
            "cdate": 1698730806183,
            "tmdate": 1699636775432,
            "mdate": 1699636775432,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5YcJFnP19J",
                "forum": "tj4a1JY03u",
                "replyto": "j3RtMTUVAR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6737/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6737/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for acknowledging the organization and clarity of our paper, as well as appreciating the advancements we've made in data collection and our detailed analysis.\n\nFor __comparison with baselines/methodology novelty/evaluation scenarios__, please refer to the general responses.\n\n__LLAVAR achieves comparable performance with mPLUG-Owl, a model that was not trained on a text-rich dataset? :__\n\nIt is hard to conclude that mPLUG-Owl is a model not trained on a text-rich dataset, as many of the captions in LAION datasets are equivalent to incomplete OCR results (Texts in an online image will sometimes appear in the captions). In the scale of our experiment, we observe similar improvement that just training on captions of text-rich images can help with text recognition capability (In Table 3, variant (4) is better than variant (1)). However, training on captions only (variant (4)) is not as good as training on OCR-based data (variant (2)(6)), at least in the scale of our experiments. We assume training on captions can be powerful enough for the scale of mPLUG-Owl (1000M+ text-image pairs). However, we believe our data is lightweight and effective, and our pipeline is more customizable.\n\n__On fragmented OCR results:__\n\nThe reason why we tried to remove fragmented OCR results from prompting GPT-4 is to remove repetitive and meaningless questions generated by GPT-4. If there are only a few unrelated words presented to GPT-4, it is hard to generate meaningful question-answer pairs for the texts. Note that (1) our noisy instruction-following data based on raw OCR results still contain such fragmented OCR results. (2) Our experiments show that the learned text recognition capability transfers well to scenarios like ST-VQA and textVQA, where the texts in images are usually fragmented words."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6737/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700504947821,
                "cdate": 1700504947821,
                "tmdate": 1700504947821,
                "mdate": 1700504947821,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "UXSuGCPT2m",
            "forum": "tj4a1JY03u",
            "replyto": "tj4a1JY03u",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6737/Reviewer_miv5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6737/Reviewer_miv5"
            ],
            "content": {
                "summary": {
                    "value": "The paper collects noisy and high-quality instruction-following data to enhance visual instruction tuning for text understanding in images. Their model LLaVAR incorporates this new data and improves performance on text VQA and instruction following for text-rich images. The enhanced capability allows more natural interaction based on real-world online content combining text and images."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper focuses on an important problem of improving OCR ability for multimodal LLMs like LLaVA.\n2. It identifies that key factors to improve model performance are training data and image resolutions. To address this, the paper collects over 400k specialized training examples to enhance OCR capabilities.\n3. Extensive experiments verify the effectiveness of the proposed training data."
                },
                "weaknesses": {
                    "value": "1. The conclusions are a bit obvious - that higher resolution inputs and more specialized training data improve LLaVA's OCR performance.\n2. The most important contribution of the paper is the collected dataset. It succeeds in showing the data improves LLaVA's OCR capabilities, but does not demonstrate it is superior to other visual instruction datasets. For example, mPLUG-Owl has comparable OCR performance to LLaVAR under the same resolution in Table 2. This raises the question of whether OCR-specific data is needed, or if the scale of data in the paper is insufficient.\n3. The evaluation is limited, mostly relying on 4 OCR QA datasets. As the authors admit in Fig 4(5), this evaluation may be unreliable. More scenarios like the LLaVA benchmark would be expected, especially in ablation studies."
                },
                "questions": {
                    "value": "1. Why did the authors collect data based on LAION, rather than some well-annotated OCR dataset?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6737/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6737/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6737/Reviewer_miv5"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6737/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698731300758,
            "cdate": 1698731300758,
            "tmdate": 1700738693934,
            "mdate": 1700738693934,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "TkQQzx6BB3",
                "forum": "tj4a1JY03u",
                "replyto": "UXSuGCPT2m",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6737/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6737/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for recognizing the strengths of our approach and the efforts we put into enhancing OCR capabilities and data collection for multimodal language models.\n\nFor __comparison with baselines/methodology novelty/Evaluation metric/Evaluation Scenarios__, please refer to the general response.\n\n__Whether OCR-specific data is needed:__\n\nCompared to hundreds of millions of text-image pairs used in mPLUG-owl, we acknowledge that the scale of our data is relatively limited, which is also relatively affordable for most academic labs. We presume that training on large-scale non-OCR data improves OCR performance, as many of the captions in LAION datasets are equivalent to incomplete OCR results (Texts in an online image will sometimes appear in the captions). In the scale of our experiment, we observe similar improvement that just training on captions of text-rich images can help with text recognition capability (In Table 3, variant (4) is better than variant (1)). However, training on captions only (variant (4)) is not as good as training on OCR-based data (variant (2)(6)), at least in the scale of our experiments. We assume such a training signal can be strong enough for the scale of mPLUG-Owl. In general, we believe OCR-specific data is necessary for data efficiency.\n\n__Ablation study on the LLaVA benchmark__\n\n|                        | conversation | detail | complex |\n|------------------------|--------------|--------|---------|\n| LLaVA                  | 83.6         | 78.1   | 95.2    |\n| LLaVA + $R_{pretrain}$ | 86.7         | 79.3   | 95.1    |\n| LLaVA + $R_{finetune}$ | 79.0         | 79.4   | 98.0    |\n| LLaVAR                 | 84.4         | 78.9   | 96.6    |\n\nWe find that including pretraining data improves the conversation capability, probably because longer training data leads to generating longer responses (Table 1). On the other hand, including finetuning data only hurts the conversation capability but increases complex reasoning. Combining pretraining and finetuning data improves the trade-off between conversation and complex reasoning. Generally speaking, GPT-4-based evaluation is not very robust, as there are some clear clues that it favors long responses [1]. By providing results on the LLaVA benchmark, we prove that incorporating our data will at least not harm the performance of interacting with natural images.\n\n__Why based on LAION rather than well-annotated OCR datasets:__\n\nThe well-annotated OCR datasets are usually restricted to certain domains such as black-and-white documents, book covers, etc. As shown in Appendix Figure 10, the text-rich images in the LAION dataset, which contains all kinds of text-image pairs from the internet, are diverse and usually interleaved with natural images. We believe our instruction-following dataset based on LAION suffers from a relatively small domain shift compared to previously collected instruction-following data based on COCO, thus beneficial for potential knowledge transfer (Section 5.4). Also, we believe that collecting data based on real-world documents and well-annotated OCR datasets is an important next step to extend the scope of the data.\n\n[1] Canwen Xu, Daya Guo, Nan Duan, and Julian McAuley. Baize: An open-source chat model with parameter-efficient tuning on self-chat data, 2023."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6737/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700504830050,
                "cdate": 1700504830050,
                "tmdate": 1700504830050,
                "mdate": 1700504830050,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Wn3xcYBiJE",
                "forum": "tj4a1JY03u",
                "replyto": "UXSuGCPT2m",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6737/Reviewer_miv5"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6737/Reviewer_miv5"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the responses."
                    },
                    "comment": {
                        "value": "Thanks for the responses of the authors, which have solved most of my concerns. \n\nI would like to lift my score if the authors provide a revised paper with the new experiment results.\n\nBest."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6737/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700710538393,
                "cdate": 1700710538393,
                "tmdate": 1700710668732,
                "mdate": 1700710668732,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YQ8Ub8ibZ4",
                "forum": "tj4a1JY03u",
                "replyto": "czVqrf6sFn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6737/Reviewer_miv5"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6737/Reviewer_miv5"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the updated paper"
                    },
                    "comment": {
                        "value": "Thanks for the authors' responses.\n\nMost of my concerns about experiments have been solved. I have lifted the score."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6737/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700738776605,
                "cdate": 1700738776605,
                "tmdate": 1700738776605,
                "mdate": 1700738776605,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "FLtgA9eyH2",
            "forum": "tj4a1JY03u",
            "replyto": "tj4a1JY03u",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6737/Reviewer_2XNz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6737/Reviewer_2XNz"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a methodology to improve the text reading capability of large language and visual assistants. There are mainly two contributions: the data collection procedure and the improvement of LLaVA with the data. Text-rich images are collected by applying an image classifier with some filtering criteria. Off-the-shelf OCR tools are then used to obtain the texts in the images. A pretraining task is defined to output the transcribed texts as target. For finetuning, GPT-4 is used to generate instruction-following data. GPT-4 is asked to generate a sequence of question-answer pairs. The model is finetuned with the generated data. The experimental results confirm that LLaVAR improves LLaVA for tasks requiring reading texts. The code, data and model will be released to the public."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper shows a practical way to collect a large amount of text-rich data. The quality of the data is confirmed by the experiments where the training with the collected data improves the model. The data will be released to the public and the community will be able to benefit from the work.\n\nThe methodologies to generate the pretraining and finetuning data are reasonable. The use of GPT-4 to generate instruction-following data is very similar to the idea of LLaVA, but it seems also effective to generate such data for tasks requiring reading texts."
                },
                "weaknesses": {
                    "value": "What seems important to improve text reading capability of this type of models is to train the models with a task that requires to read texts. This work also does it by generating data with OCR-ed texts and defining tasks that require reading texts. As expected, it improves the model in terms of text reading capability. However, the problem is that this seems a shared problem in this field and there are other studies that tried to improve text reading capabilities of this type of models (e.g. PreSTU, Pix2Struct). There is no discussion in this aspect and it looks like this is yet another attempt with the same objective. It would be required to make the novelty and advantage clear against other studies.\n\nThis is essentially an extension of LLaVA with OCR tasks. It is certainly important to improve text reading capability of this type of models, but it looks a little bit incremental in terms of methodological novelty."
                },
                "questions": {
                    "value": "I wanted to understand the detail of \"GPT-4-based instruction-following evaluation\". My assumption was that GPT-4 was treated as Oracle (or GT) and some scores were computed against it. However, it was not very clear how text-based GPT-4 can be used to generate GT for tasks with image inputs. Also, how to compute the scores was not clear."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No concern."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6737/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698741944938,
            "cdate": 1698741944938,
            "tmdate": 1699636775180,
            "mdate": 1699636775180,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "dEjBGWBaZc",
                "forum": "tj4a1JY03u",
                "replyto": "FLtgA9eyH2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6737/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6737/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for recognizing the practicality and effectiveness of our data collection and training methodologies, as well as the potential community benefits from the release of our data and model.\n\nFor __methodology novelty__, please take a look at the general responses.\n\n__Comparing to previous studies like PreSTU, Pix2Struct:__\n\nOur work focuses on improving the text recognition capability of the multimodal instruction-following model, which can be built on related prior work like PreSTU and Pix2Struct. Assuming we have a good frozen image encoder (which can be CLIP, Pix2Struct, or Pix2Struct), we study how to align its text recognition capability with large language models (pretraining stage) and how to maintain and acquire such capability during instruction-following (finetuning stage). In current Table 4 and Appendix E, we provide the results for using Pix2Struct to augment CLIP and demonstrate its improvement over CLIP $224^2$.\n\nWe assume the amount of data needed for feature alignment should be much less than that needed for feature learning. As our data and pipeline focus on feature alignment, we believe it can naturally benefit from any advanced image encoders like Pix2Struct and PreSTU.\n\n__GPT-4-based instruction-following evaluation__\n\nGPT-4\u2019s responses are treated as oracles. We provide text-only GPT-4 with detailed descriptions of the image (human-written captions, OCR results) and collect feedback as oracles on related questions. To calculate the score, we provide text-only GPT-4 with the detailed description again, together with one question and two answers (one from text-only GPT-4, one from the model we want to test), and ask GPT-4 to give scores to the two answers (1 ~ 10). The final score is the ratio between the average score of the tested model and the average score of GPT-4. For example, \u201c83.1\u201d in Table 5 means its score is 83.1% of GPT-4\u2019s score."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6737/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700504727353,
                "cdate": 1700504727353,
                "tmdate": 1700504727353,
                "mdate": 1700504727353,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XUXgrnbqTk",
                "forum": "tj4a1JY03u",
                "replyto": "dEjBGWBaZc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6737/Reviewer_2XNz"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6737/Reviewer_2XNz"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the response"
                    },
                    "comment": {
                        "value": "Thank you for the response. I'm going to definitely reflect the rebuttal to make my final decision. Thanks again for your hard work."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6737/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700740176441,
                "cdate": 1700740176441,
                "tmdate": 1700740176441,
                "mdate": 1700740176441,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Xc3tmzbAFs",
            "forum": "tj4a1JY03u",
            "replyto": "tj4a1JY03u",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6737/Reviewer_sAP9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6737/Reviewer_sAP9"
            ],
            "content": {
                "summary": {
                    "value": "This paper enhances the visual text understanding ability of large multimodal models by instruction tuning methods. First, two sets of noisy and high-quality instruction-following data are constructed. Specifically, the high-quality instruction-following data are generated by prompting text-only GPT-4 with OCR results and captions. Then, a two-stage training strategy is developed, with the first stage learning OCR capability and the second stage learning high-level understanding capability. Extensive experiments verify that the proposed LLaVAR model can improve performance on both natural and text-rich images."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1) This work a pioneering exploration of visual instruction tuning for text images, which can provide some useful insights to the community. \n2) The proposed model has ability to deal with high-resolution images by integrating extra visual encoders and cross-attention modules.\n3) The experiments are basically sufficient to demonstrate the superiority of the LLaVAR method, especially relative to the LLaVA baseline."
                },
                "weaknesses": {
                    "value": "1) This work is less innovative in approach, as it mainly focuses on the construction of instruction-following data, while the proposed model and implementation pipeline basically follow LLaVA.\n2) From the results in Table 2, LLaVAR has no significant performance advantage compared with existing methods under the same resolution (2242), such as mPLGU-Owl (2023). Besides, it is better to evaluate the parameter sizes of these comparison models.\n3) In Section 5.3, only one case study is carried out, so the derived conclusion is hardly convincing.\n4) In Figure 7, the notations are not clearly explained, and the implementation details cannot be visually reflected in this figure."
                },
                "questions": {
                    "value": "1) As can be seen in Figure 4 and Figure 5, OCR errors are inevitable, e.g., \u201cBoynton\u201d vs. \u201cByington\u201d. Can you provide some results to analyze the impact of OCR errors?  \n2) As mentioned in the paper, the adopted metric only considers the recall, so it is not very reliable. Have you tried other quantitative metrics to prove the effectiveness of the method, such as the metrics designed for the image captioning task?  \n3) What does \u201ctemperature\u201d refer to in the first paragraph of Section 5?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6737/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6737/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6737/Reviewer_sAP9"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6737/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698810547614,
            "cdate": 1698810547614,
            "tmdate": 1699636775060,
            "mdate": 1699636775060,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "mb0npoFOom",
                "forum": "tj4a1JY03u",
                "replyto": "Xc3tmzbAFs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6737/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6737/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for recognizing our work's innovative aspects and potential impact!\nWe address your comments below:\n\nFor __comparison with baselines/methodology novelty/evaluation metric__, please refer to the general responses.\n\n__Table of language model parameter size and training data size.__\n\n|                  | Language Model Parameter | Training data size |\n|------------------|--------------------------|--------------------|\n| BLIP-2-FlanT5XXL | 11B                      | 129M               |\n| OpenFlamingo     | 7B                       | 15M                |\n| MiniGPT4         | 13B                      | 5.0M               |\n| LLaVA            | 13B                      | 0.6M               |\n| mPLUG-owl        | 7B                       | 1112M              |\n| LLaVAR           | 13B                      | 1.0M               |\n\nAs all baseline models use ViT CLIP 224 as their visual encoder, we list the parameter size of their language model above, together with their training data size.\n\nIn our experiments, we find that the language model size has a trivial effect (see the table below) on the performance related to text-based VQA. We believe data efficiency is a more important factor to consider while comparing performance.\n\n|            | ST-VQA | OCR-VQA | TextVQA | DocVQA |\n|------------|--------|---------|---------|--------|\n| LLaVAR 7B  | 28.9   | 25.6    | 37.8    | 6.2    |\n| LLaVAR 13B | 30.2   | 23.4    | 39.5    | 6.2    |\n\n__More comprehensive case study:__\n\nThe case study is now conducted on 825 examples from OCR-VQA. We have updated Figure 6 in the draft. In this large-scale case study, we still observe a threshold for recognizable texts for both $224^2$-based and $336^2$-based LLaVAR as the accuracy sharply decreases when the height is smaller than 7 pixels.\n\n__On Figure 7:__\n\nWe have provided an updated Figure 7 with detailed captions. A sketch of the implementation details is as follows: Given an image, it is simultaneously processed by visual encoders $V_1$ and $V_2$. $V_1$ features are transformed by transformation matrix $W$ and directly used as input embeddings to the language model. For $V_2$ features, they are transformed by transformation matrix $K$ and $V$ and used as keys and values to calculate the cross attention in every transformer layer (assume there are $N$ layers), which uses the transformed hidden states (through $Q$) from the self-attention module as queries.\n\n__The impact of OCR errors:__\n\nWe take 1673 examples from OCR-VQA, which have ground truth answers with more than 10 characters, to study such OCR errors. We (i) define \u201ccorrect\u201d as the ground truth answers that are exactly in the predictions, and (ii) define \u201cpartially correct\u201d as there exists a substring in the prediction that has high enough similarity with the ground truth but not the same. Specifically, we look at all substrings with the same length of the ground truth in the prediction to calculate ANLS (Average Normalized Levenshtein Similarity) and regard the prediction as \u201cpartially correct\u201d if the highest ANLS is greater or equal to 0.5 but smaller than 1.\n\n|           | Correct % | Partially Correct% |\n|-----------|-----------|--------------------|\n| LLaVA224  | 1.6%      | 8.7%               |\n| LLaVAR224 | 6.8%      | 22.8%              |\n| LLaVA336  | 2.2%      | 11.2%              |\n| LLaVAR336 | 9.0%      | 26.8%              |\n\nWe find that a considerable amount of predictions can be considered partially correct, which indicates the actual performance of tested models is better than the reported accuracy numbers. However, the percentage of partially correct predictions is highly correlated with the percentage of correct predictions. Therefore, we believe that the current metrics can effectively compare the performance of different models.\n\n__Clarification on the term \u201ctemperature\u201d:__\n\nThe temperature used to sample examples from language models."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6737/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700504656695,
                "cdate": 1700504656695,
                "tmdate": 1700504656695,
                "mdate": 1700504656695,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]