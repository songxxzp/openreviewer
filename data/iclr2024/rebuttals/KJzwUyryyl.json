[
    {
        "title": "ALMANACS: A Simulatability Benchmark for Language Model Explainability"
    },
    {
        "review": {
            "id": "f6oFjXKSCT",
            "forum": "KJzwUyryyl",
            "replyto": "KJzwUyryyl",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4167/Reviewer_vWZJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4167/Reviewer_vWZJ"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces ALMANACS, a new benchmark for evaluating explainability methods for language models. The key ideas are:\n\n-ALMANACS measures the simulatability of explanations, i.e. how well they help predict model behavior on new inputs. This relates to the desired properties of faithfulness and completeness.\n-The benchmark comprises 12 topics with safety-relevant scenarios. Questions are non-objective and designed to elicit complex, nonlinear behavior from models.\n-There is distributional shift between train and test sets to require generalization.\n-The authors test counterfactual, rationalization, and salience-based explanation methods. None consistently improve upon a no-explanation baseline, indicating simulatability on ALMANACS remains an open challenge."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "(1) Partially addresses the need for standardized benchmarks to evaluate and compare explanation methods.\n\n(2) Simulatability is a useful metric directly related to explanation quality. \n\n(3) Automated evaluation enables efficient benchmarking.\n\n(4) Non-objective questions and distribution shift require explanations to provide true insight rather than leveraging correlations."
                },
                "weaknesses": {
                    "value": "(1) Only safety-relevant scenarios are included and this is not general. \n\n(2) The choice of language models for evaluation versus being explained may affect results. More analysis of this factor could be useful.\n\n(3) Automated evaluation using a language model proxy for humans has limitations vs. human studies. Direct comparisons would be needed to validate the benchmark.\n\n(4) Testing on more model sizes, scaling effects, and model families instead of just focusing on flan-alpaca-gpt4-xl and vicuna-7b-v1.3."
                },
                "questions": {
                    "value": "Missing references for explanation distillation:\n\n(1) Li et al. Explanations from Large Language Models Make Small Reasoners Better. 2022."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4167/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698539426575,
            "cdate": 1698539426575,
            "tmdate": 1699636382527,
            "mdate": 1699636382527,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "YiqogMDtqI",
                "forum": "KJzwUyryyl",
                "replyto": "f6oFjXKSCT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4167/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4167/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer vWZJ"
                    },
                    "comment": {
                        "value": "Thank you for your review. If we understand correctly, your main concerns are that our experiments are not comprehensive enough. In particular, you would like to see tests with more model sizes and model families, more pairs of language models used for prediction with language models being explained, and human studies. While we leave human studies as an important direction for future work, **we have added experiments with 11 additional language models**. These include nine new language models being explained (in Appendix G) and two new language model predictors (in appendix J).\n\n**Do these changes address your concerns about the comprehensiveness of our experiments?** If not, we would be happy to improve the paper based on further input from you.\n\n> Only safety-relevant scenarios are included and this is not general.\n\nOur work contains 12 different scenario topics, and it has a total of 180,000 training examples and 18,000 test examples.\n\nWe are confused how this many different scenario topics and dataset examples is not general enough for ICLR. There are other natural language benchmarks published in ICLR that contain fewer topics and dataset examples. For instance:\n- \u201cWikiWhy: Answering and Explaining Cause-and-Effect Questions\u201d. ICLR 2023 (notable top 5%). The dataset has 9,406 \u201cwhy\u201d question-answer-rationale triplets spanning 11 topics.\n- \u201cReClor: A Reading Comprehension Dataset Requiring Logical Reasoning\u201d. ICLR 2020. The dataset has 6,138 questions (and no topic categories).\n\nWe hope that our work is judged consistently by the same standards as other papers.\n\n> The choice of language models for evaluation versus being explained may affect results.\n\nTo test if our choice of GPT-4 for evaluation affected the results, **we ran additional experiments with two additional language models for evaluation. The results are in Appendix J.** We find that no combination of predictor/explanation, for either model being explained, performs significantly better than using GPT-4 for evaluation with no explanations provided, or than the logistic regression baseline. Consistent with our main results, this indicates that none of these predictors are able to extract helpful information from the explanations beyond what is present in the model\u2019s answers.\n\n> Automated evaluation using a language model proxy for humans has limitations vs. human studies.\n\nThis is an important point; human studies are a valuable direction for future work. Nevertheless, even if an automated evaluation isn\u2019t totally consistent with a human study, we still think automated evaluation has a valuable role to play. Because automated evaluation is an order of magnitude faster than human evaluation, it can provide quick, coarse-grained feedback throughout the interpretability researcher\u2019s development cycle. A tool like ALMANACS is the difference between waiting weeks or months for the results of a human study, versus getting results of an automated benchmark in just a few hours.\n\n> Testing on more model sizes, scaling effects, and model families instead of just focusing on flan-alpaca-gpt4-xl and vicuna-7b-v1.3.\n\n**We have added scaling experiments to the paper, in Appendix G.** These experiments test models of a **variety of families with sizes ranging from 0.77B to 20B parameters on the advanced_ai_risk section of ALMANACS**. The benchmark difficulty, as measured by how well logistic regression is able to predict model behavior, is shown in Figure 10. We see a general trend that the behavior of larger models is harder to predict. The trend is more pronounced when we compare benchmark difficulty with model performance on a related task, with more capable models having behavior that is more difficult to predict.\n\n> Missing references for explanation distillation:\n>\n> (1) Li et al. Explanations from Large Language Models Make Small Reasoners Better. 2022.\n\nThank you. We have added this reference to the paper."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4167/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700523888775,
                "cdate": 1700523888775,
                "tmdate": 1700523888775,
                "mdate": 1700523888775,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "8HAGBpFSCb",
            "forum": "KJzwUyryyl",
            "replyto": "KJzwUyryyl",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4167/Reviewer_3Mpe"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4167/Reviewer_3Mpe"
            ],
            "content": {
                "summary": {
                    "value": "This submission introduces ALMANACS, a novel benchmark tailored for evaluating the explainability of language models via a concept termed \"simulatability.\" Using GPT-4 as a predictor, the benchmark assesses how well GPT-4 can simulate other language models that employ various explanation methods. Simulatability is defined by measuring the distribution distance between the outputs of GPT-4 and the target language model for previously unseen test tasks. A noteworthy finding is that the incorporation of explanations does not invariably enhance explanation performance for unseen inputs."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper innovatively offers a benchmark with a quantitative metric for assessing explainability in language models. Furthermore,it is interesting for the discovery that models with explanation input do not outperform non-explanatio in terms of simulatability."
                },
                "weaknesses": {
                    "value": "The primary focus of this paper appears to be on the introduction of the ALMANACS benchmark. Yet, the utilization of well-established distance measures like KLDiv and TVDist doesn't add a novel dimension to the study.\n\nThe observation that explanation techniques might not always heighten performance on unseen data is compelling, but the paper would benefit from a deeper analysis and discussion on the possible reasons behind this phenomenon.\n\nThe term \"simulatability\" appears to be inconsistently defined, leading to confusion. The initial definition of simulatability is \"how well the explanations improve behavior prediction on new inputs\". Subsequently, it seems to change to a definition centered on distribution distance, KLDiv or TVDist.\n\nThe paper doesn't provide a convincing argument for why simulatability is a good metric for language model explainability. Many factors can influence predictor outputs. Given that GPT-4 operates as a black box, it's hard to say GPT-4 predict solely to the presence or absence of explanations without providing additional constraints. This point is underscored by results from the NoExpl vs Expl comparison in Table 1, which indicates low KLDiv scores, hinting that GPT-4's predictions might be independent of input explanations."
                },
                "questions": {
                    "value": "1. Could the authors provide the precise definition of \"simulatability\"?\n1. The choice of GloVe embeddings for demonstration retrieval appears outdated. Have the authors considered more recent sentence embeddings, such as SimCSE?\n1. There seems to be a discrepancy between the comparison results for NoExpl in Figure 4 and Table 1 (PredictAverage vs NoExpl). PredictAverage outperforms NoExpl in Figure 4 but does not in Table 1. \nCould this be clarified?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4167/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698838022951,
            "cdate": 1698838022951,
            "tmdate": 1699636382452,
            "mdate": 1699636382452,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "hrHC5KYnmR",
                "forum": "KJzwUyryyl",
                "replyto": "8HAGBpFSCb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4167/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4167/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer 3Mpe"
                    },
                    "comment": {
                        "value": "> the paper would benefit from a deeper analysis... on the possible reasons [why explanations don't help on unseen data]\n\nThank you for this excellent suggestion. **In Appendix H, we have added additional analysis and discussion of possible reasons why the explanation methods are not improving performance.** Specifically, we provide examples of the explanation methods from our dataset, and we give a qualitative analysis of how much information is contained in the explanations. One insight is that to us, the authors of the study, the tokens with high salience scores do not intuitively suggest how the model will behave on new inputs. For example, the tokens are sometimes the question words, such as \u201cwould\u201d and \u201cchoose\u201d, rather than the details of the scenario. Another observation is that the language model\u2019s rationalizations often discuss the specifics of the input scenario without discussing possible variations of the scenario. This might be limiting how much the rationalizations help the predictor at test-time, when the predictor is presented with scenario variations.\n\n> The term \"simulatability\" appears to be inconsistently defined.... The initial definition of simulatability is \"how well the explanations improve behavior prediction on new inputs\". Subsequently, it seems to change to a definition centered on distribution distance, KLDiv or TVDist\u2026. Could the authors provide a precise definition of \u201csimulatability\u201d?\n\nIt seems our original submission did not clearly present the KLDiv and TVDist metrics. \u201cHow well the explanations improve behavior prediction on new inputs\u201d is indeed the definition of \u201csimulatability\u201d that we use throughout the paper. The key idea is that we are interested in distributions of behavior; we define \u201cbehavior\u201d as the yes-no probability distribution that the language model gives to the scenario. We also have the predictor give a yes-no probability distribution as its prediction. To measure how good the predictor is, we therefore used the distribution distances KLDiv and TVDist.\n\nPast work has considered binarized model output (taking just the most likely answer, either \u201cyes\u201d or \u201cno\u201d). This allows measuring simulatability with accuracy / F1 score. However, binarizing the model output is throwing away information. We choose to keep the entire distribution to create a more challenging benchmark, where the predictor must distinguish between models which answer \u201cyes\u201d 51% of the time versus models which answer \u201cyes\u201d 99% of the time.\n\n**We have added a discussion of why we use distribution distances to Section 2.3: Evaluation Metrics.**\n\n**In Appendix B, we also added a third quantitative metric, Spearman\u2019s rank correlation coefficient**. We calculate the Spearman correlation between the predictor\u2019s probability of \u201cyes\u201d and the language model\u2019s probability of \u201cyes\u201d across all questions in a topic. This provides another way, different from distribution distance, to measure simulatability. In Appendix B, we see that Spearman correlation results are consistent with the KLDiv and TVDist results: no explanation method does substantially better than the explanation-free control.\n\n> GPT-4's predictions might be independent of input explanations\n\nIt\u2019s important to know if GPT-4 has the ability to understand input explanations and change its predictions accordingly. To test this, in Section 4, we provide GPT-4 with explanations for a synthetic linear model. The explanations are hand-crafted to contain prediction-relevant information. **In Figure 4, we see that providing GPT-4 with the \u201cqualitative\u201d and \u201cweights\u201d explanations improves performance compared to the \u201cNoExpl\u201d no-explanation control.** Therefore, we have examples where we know that GPT-4\u2019s predictions depend on the input explanations.\n\n> Have [you] considered more recent sentence embeddings, such as SimCSE?\n\n**We have added experiments with more recent sentence embeddings to Appendix I, including SimCSE and SentenceBERT.** With these new embeddings, we find that the baseline methods have improved performance. Therefore, **we have updated the main results tables (Tables 1, 2, and 3) to use the more recent embeddings.** Our overall pattern of results remains the same: even with new embeddings, no explanation method provides a substantial benefit over the no-explanation control.\n\n> PredictAverage outperforms NoExpl in Figure 4 but does not in Table 1. Could this [discrepancy] be clarified?\n\nThanks for bringing this to our attention.\n\nPredictAverage depends only on the model being explained (predicting the average probability over the dataset). NoExpl, on the other hand, is GPT-4\u2019s prediction when prompted with few-shot examples of the model\u2019s behavior. We interpret this result as indicating that GPT-4 is better at in-context learning of other language models\u2019 behavior than in-context learning of a synthetic linear model. **We have added this observation to the paper in the \u201cNo-explanation predictions\u201d paragraph of Section 5: Results.**"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4167/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700514602444,
                "cdate": 1700514602444,
                "tmdate": 1700514765666,
                "mdate": 1700514765666,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "btMCkg3RqN",
                "forum": "KJzwUyryyl",
                "replyto": "8HAGBpFSCb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4167/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4167/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Summary of our Author Response to Reviewer 3Mpe"
                    },
                    "comment": {
                        "value": "Thank you for your review. If we understand correctly, a primary concern of yours is how KLDiv and TVDist relate to simulatability. You also would like to see a deeper analysis of why explanation methods don't improve performance and if GPT-4's behavior is independent of explanations. Finally, you would like more modern sentence embeddings, such as SimCSE.\n\n**We have added additional experiments and analyses to the paper, which we believe addresses all of your concerns.** Please see our other comment for a detailed discussion of these experiments and analyses. In summary:\n- We added to Appendix H an analysis and discussion of possible reasons why the explanations are not improving performance.\n- We have added a discussion of why we use distribution distances to Section 2.3: Evaluation Metrics.\n- We added a third quantitative metric, Spearman\u2019s rank correlation coefficient, to Appendix B.\n- In Section 4, Figure 4, we see an example where the GPT-4 predictor depends on the input explanations to improve its performance.\n- We have added experiments with more recent sentence embeddings to Appendix I, including SimCSE.\n- We have updated the main results tables (Tables 1, 2, and 3) to use more recent embeddings.\n- We have added an observation about the discrepancy between Figure 4 and Table 1 to the \u201cNo-explanation predictions\u201d paragraph of Section 5: Results.\n\n_Have we fully addressed your concerns?_ If not, please let us know. We would be glad to incorporate further suggestions into our paper."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4167/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700515570377,
                "cdate": 1700515570377,
                "tmdate": 1700515570377,
                "mdate": 1700515570377,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qMT3oiV8eV",
                "forum": "KJzwUyryyl",
                "replyto": "WvYWk8M9b6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4167/Reviewer_3Mpe"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4167/Reviewer_3Mpe"
                ],
                "content": {
                    "title": {
                        "value": "Response to Authors"
                    },
                    "comment": {
                        "value": "> It\u2019s important to know if GPT-4 has the ability to \n\nIt doesn't convince me because the results in Table 1 seems giving different conclusion. \"NoExpl\" has similar or even lower KLDiv scores compared to other explanation methods. I still challenge that GPT-4 itself can somehow generate good explanations internally and minimize the divergence between various explanation methods. \nWithout evidence that GPT-4 is able to disentangle the explanation for evaluation, GPT-4 with simulatability is not a good evaluation approach."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4167/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700718759553,
                "cdate": 1700718759553,
                "tmdate": 1700718759553,
                "mdate": 1700718759553,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "KqtwKuIVNj",
            "forum": "KJzwUyryyl",
            "replyto": "KJzwUyryyl",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4167/Reviewer_P4RQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4167/Reviewer_P4RQ"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents ALMANACS, a language model explainability benchmark that measures the efficacy of different explanation methods. The benchmark focuses on simulatability, which evaluates how well explanations improve behavior prediction on new inputs. ALMANACS consists of twelve safety-relevant topics with idiosyncratic premises and a train-test distributional shift. The authors evaluate counterfactual, rationalization, and salience-based explanations using another language model as a predictor. The results show that, on average, no explanation method outperforms the explanation-free control, highlighting the challenge of developing explanations that aid simulatability."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper addresses the need for a consistent evaluation standard for language model explainability methods.\n- ALMANACS provides a benchmark that measures simulatability, a necessary condition for faithful and complete explanations.\n- The benchmark includes safety-relevant topics and a train-test distributional shift to encourage faithful explanations.\n- The use of another language model as a predictor enables fully automated evaluation, speeding up the interpretability algorithm development cycle.\n- The paper presents results that highlight the limitations of current explanation methods and the open challenge of generating explanations that aid prediction."
                },
                "weaknesses": {
                    "value": "- The paper only evaluates the explanation methods based on Kullback-Leibler divergence (KLDIV) and total variation distance (TVDIST). While these metrics provide insights into the performance of the methods, they may not capture all aspects of explanation quality.\n- The paper acknowledges that the automated evaluation using language models may not be consistent with human evaluation. Human studies are still needed to validate the results and determine if humans can succeed where language models fail.\n- The paper evaluates only three explanation methods (counterfactual, rationalization, and salience-based). While these methods are commonly used in explainability research, there may be other methods that could be valuable to include in the benchmark."
                },
                "questions": {
                    "value": "None"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4167/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699014003960,
            "cdate": 1699014003960,
            "tmdate": 1699636382350,
            "mdate": 1699636382350,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "v3Pk9AE4YR",
                "forum": "KJzwUyryyl",
                "replyto": "KqtwKuIVNj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4167/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4167/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer P4RQ"
                    },
                    "comment": {
                        "value": "Thank you for your review. If we understand correctly, your concerns are that (i) we only use two evaluation metrics, (ii) we do not do a human study, and (iii) we only consider three explanation methods. While we acknowledge the lack of a human study, we have incorporated all your other suggested changes into the paper. In particular, we have **added a third evaluation metric, Spearman's rank correlation coefficient, to the paper**. We have also **added a fourth explanation method, Integrated Gradients**.\n\n_Does this fully address the reviewer's concerns?_ If not, please let us know, and we will be happy to make further changes.\n\n> The paper only evaluates the explanation methods based on Kullback-Leibler divergence (KLDIV) and total variation distance (TVDIST).\n\nIn Appendix B, we have added a third metric, Spearman\u2019s rank correlation coefficient. We calculate the Spearman correlation between the predictor\u2019s probability of \u201cyes\u201d and the language model\u2019s probability of \u201cyes.\u201d This provides another way, different from distribution distance, to measure simulatability. In Appendix B, we see that Spearman correlation results are consistent with the KLDiv and TVDist results: no explanation method does substantially better than the explanation-free control.\n\n> automated evaluation using language models may not be consistent with human evaluation\n\nThis is an important point; human studies are a valuable direction for future work. Nevertheless, even if an automated evaluation isn\u2019t totally consistent with a human study, we still think automated evaluation has a valuable role to play. Because automated evaluation is an order of magnitude faster than human evaluation, it can provide quick, coarse-grained feedback throughout the interpretability researcher\u2019s development cycle. A tool like ALMANACS is the difference between waiting weeks or months for the results of a human study, versus getting results of an automated benchmark in just a few hours.\n\n> The paper evaluates only three explanation methods\n\nWe have added an additional explanation method, Integrated Gradients. Integrated Gradients is a feature attribution method that is axiomatically motivated; it satisfies sensitivity and implementation invariance, and it is the unique path method that is symmetry preserving. (For details, see \u201cAxiomatic Attribution for Deep Networks\u201d by Sundararajan et al.) Integrated gradients was also one of the best-performing methods in Pruthi et al.\u2019s explanation evaluation paper, \u201cEvaluating Explanations: How much do explanations from the teacher aid students?\u201d\n\nIn Table 1, we see that the Integrated Gradients results are consistent with the results for other explanation methods: Integrated Gradients does not improve simulatability over the no-explanation control."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4167/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700513298198,
                "cdate": 1700513298198,
                "tmdate": 1700513298198,
                "mdate": 1700513298198,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zuph8WDpel",
                "forum": "KJzwUyryyl",
                "replyto": "KqtwKuIVNj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4167/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4167/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "With one day left in the discussion period, we are hoping to hear a reply from you. Please let us know if we have addressed your concerns."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4167/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700671312850,
                "cdate": 1700671312850,
                "tmdate": 1700671415297,
                "mdate": 1700671415297,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "n5HqHA0Rtn",
            "forum": "KJzwUyryyl",
            "replyto": "KJzwUyryyl",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4167/Reviewer_Lu72"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4167/Reviewer_Lu72"
            ],
            "content": {
                "summary": {
                    "value": "Simulatability refers to (a human\u2019s) capability to predict model behavior on unseen outputs. Improving simulatability has been considered an important goal for interpretability methods. This paper introduces a new benchmark to automatically evaluate simulatability for interpretability methods, using GPT-4 as a stand-in for humans. Notably, this new benchmark focuses on non-objective tasks with safety-relevant questions."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper is well-written and easy to follow. By focusing on safety-relevant, non-objective questions, the benchmark differentiates itself well from existing work on interpretability evaluations. The focus on distribution shift also makes the evaluation more realistic than some of the existing work. Overall the paper presents a well-executed idea with very clear motivation."
                },
                "weaknesses": {
                    "value": "As the authors acknowledged, the use of GPT-4 as a stand-in for human annotators limits how much we can take away from the evaluation. Although the paper frames ALMANACS as a benchmark, I find it more suitable to call it a dataset\u2014only when paired with a good-enough human approximator like GPT-4 would it become a benchmark. The lack of user study makes it difficult to judge the evaluation results conducted with the new dataset. But I think the dataset is an interesting starting point for future user studies."
                },
                "questions": {
                    "value": "Given the predictor is GPT-4, it seems like the benchmark can be applied to interpretability goals beyond simulatability. Any thoughts on that?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4167/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699504398516,
            "cdate": 1699504398516,
            "tmdate": 1699636382284,
            "mdate": 1699636382284,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gdJamPcXSV",
                "forum": "KJzwUyryyl",
                "replyto": "n5HqHA0Rtn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4167/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4167/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer Lu72"
                    },
                    "comment": {
                        "value": "Thank you for your comments; our reply is below. **If there is anything else we can do to improve the paper, please let us know. We will be happy to incorporate further suggestions into our work.**\n\n> Although the paper frames ALMANACS as a benchmark, I find it more suitable to call it a dataset\u2014only when paired with a good-enough human approximator like GPT-4 would it become a benchmark.\n\nWe agree that the dataset is a primary contribution of ALMANACS. The ALMANACS code we release also provides a complete implementation for using GPT-4 as a predictor; the user needs only to provide an OpenAI API key.\n\n> Given the predictor is GPT-4, it seems like the benchmark can be applied to interpretability goals beyond simulatability. Any thoughts on that?\n\nYes, we think it\u2019s exciting to consider how ALMANACS can be applied to interpretability goals beyond simulatability. Here are a few possibilities:\n- Studying _minimality_, the absence of extraneous information. To do this, one could randomly sample a subset of each explanation. Then, one could see if the GPT-4 predictor has the same level of performance when looking only at explanation subsets. If a subset of an explanation performs as well as the full explanation, then it suggests that the explanation contains extraneous information. (Although one has to be careful, because having _redundant_ information could be desirable; future work might need to distinguish between _redundant_ versus _irrelevant_ information.)\n- GPT-4 could automate _counterfactual analyses_, like Chen et al.\u2019s work \u201cDo Model\u2019s Explain Themselves? Counterfactual Simulatability of Natural Language Explanations.\u201d Future work could try performing similar sorts of counterfactual analyses in ALMANACS.\n- In order to scale to larger and larger neural networks, we will need _automated interpretability tools_. Because ALMANACS allows fully automated analysis, it can help test scalability of interpretability methods."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4167/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700511645429,
                "cdate": 1700511645429,
                "tmdate": 1700513337722,
                "mdate": 1700513337722,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]