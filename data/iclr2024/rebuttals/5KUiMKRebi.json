[
    {
        "title": "Implicit Neural Representation Inference for Low-Dimensional Bayesian Deep Learning"
    },
    {
        "review": {
            "id": "x4jofXBGSI",
            "forum": "5KUiMKRebi",
            "replyto": "5KUiMKRebi",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2362/Reviewer_wVmX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2362/Reviewer_wVmX"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose an approximate inference method for Bayesian neural networks. The idea is to introduce a second (auxiliary) network which computes an approximate posterior over the multiplicative noise applied to the deterministic weights of the main network. Such a multiplicative noise then induces an approximate predictive posterior distribution of the main network, they key object of interest in BNNs. The auxiliary network can be of much smaller size than the main one, and an approximate posterior over its weights can be approximated using one of the existing methods (e.g. Laplace, SWAG, Normalising Flows). The proposed method is shown to be competitive in comparison to a number of baselines."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+ An interesting approach to approximate BNN inference showing competitive performance\n+ Clear presentation, the paper is easy to follow"
                },
                "weaknesses": {
                    "value": "The baselines used for comparison (Dropout, BbH, Ensembles) are relatively old methods (by Deep Learning standards of course). I'd be very interested to see how the proposed method compares to more modern approaches, e.g. those applying Laplace approximation directly on to weights of the main network.\n\nA couple of minor points:\n- Typo in the second to last line on page 3 (wf).\n- Typo in Eq. (5) (w_{INR} should be in the subscript I guess?)"
                },
                "questions": {
                    "value": "- I wonder about the integer inputs (i.e. the tensor coordinates) to the INR network. Do you normalise these coordinates somehow or directly input the integers into the INR network? Did such an integer input space cause any problems during training?\n- Why do you think that sinusoidal activations are particularly suitable for the INR network? Do you expect the result to deteriorate if you used other activations (e.g. sigmoid)? \n- Why do you think the INR with 350 outperformed 4k and 10k versions on CIFAR in Fig. 1?\n- I was very interested to see that the predictive uncertainty in Fig. 2 has a stationary structure (i.e. dependent on the distance from the observations) similar to a GP with a stationary kernel. The Dropout and Deep Ensembles baselines clearly don't have such a property, but I wonder how such a figure would look like if we used a Laplace approximation directly on some layers of the main network (without INR), e.g. similar to Kristiadi et al. (2020)? In other words, I wonder how specific this stationary uncertainty structure is to the inference using an auxiliary network?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2362/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698433230662,
            "cdate": 1698433230662,
            "tmdate": 1699636168699,
            "mdate": 1699636168699,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BQ9XQsZRoU",
                "forum": "5KUiMKRebi",
                "replyto": "x4jofXBGSI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2362/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2362/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "*\u201dAn interesting approach to approximate BNN inference showing competitive performance [..] Clear presentation, the paper is easy to follow\u201d*\n\nThank you, we appreciate your positive comments. We are very content that you found the presentation clear."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2362/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700238033283,
                "cdate": 1700238033283,
                "tmdate": 1700238033283,
                "mdate": 1700238033283,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OaKQVReOZR",
                "forum": "5KUiMKRebi",
                "replyto": "x4jofXBGSI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2362/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2362/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "*\u201cThe baselines used for comparison (Dropout, BbH, Ensembles) are relatively old methods (by Deep Learning standards of course). I'd be very interested to see how the proposed method compares to more modern approaches, e.g. those applying Laplace approximation directly on to weights of the main network.\u201d*\n\nFollowing the reviewer\u2019s recommendations we added an additional ablation study. We tried to measure the quality of proposed subspaces in terms of predictive uncertainty. Specifically we compare our INR low dimensional space with: Rank-1 (Dusenberry et al. 2020); Wasserstein subnetwork  (Daxberger et al. 2021) and partially stochastic ResNets from (Sharma, Mrinank, et al. partially). \nWe trained (each method) combined with a Resnet18 for 100 epochs in both Cifar10 and Cifar100 datasets while keeping the approximate inference method fixed same across all low dimensional spaces.\n\n* Sharma, Mrinank, et al. \"Do Bayesian Neural Networks Need To Be Fully Stochastic?.\" International Conference on Artificial Intelligence and Statistics. PMLR, 2023.\n\n **RANK1**\n\n SWAG - CIFAR10 In-Dist Test data Accuracy: 91.78 LL: -0.4187 Error: 0.082 Brier: 0.1343 ECE: 0.0522\n\n SWAG - CIFAR10 Corrupted Test data Accuracy: 77.80 LL: -1.2537 Error: 0.222 Brier: 0.3596 ECE: 0.1469\n\n Laplace - CIFAR10 In-Dist Test data Accuracy: 91.01 LL: -1.5630 Error: 0.090 Brier: 0.6872 ECE: 0.6871\n\n Laplace - CIFAR10 Corrupted Test data Accuracy: 78.07 LL: -1.7068 Error: 0.220 Brier: 0.7396 ECE: 0.5737\n\n-----------------------------------------------------------------------------------------------------------------------------------------------------\n\nSWAG -   CIFAR100 In-Dist Test data         Accuracy: 65.84 LL: -2.29 Error: 0.34 Brier: 0.55 ECE: 0.228\n\nSWAG -   CIFAR100 Corrupted Test data   Accuracy: 42.80 LL: -4.77 Error: 0.57 Brier: 0.92 ECE: 0.398\n\n Laplace - CIFAR100 In-Dist Test data        Accuracy: 69.00 LL: -4.01 Error: 0.31 Brier:  0.9714 ECE: 0.668\n\n Laplace - CIFAR100 Corrupted Test data   Accuracy: 42.00 LL: -4.25 Error: 0.58 Brier 0.979 ECE: 0.401\n\n **INR**\n\n SWAG -   CIFAR10 In-Dist Test data Accuracy: 92.17 LL: -0.384 Error: 0.078 Brier: 0.1296 ECE: 0.0498\n\n SWAG -   CIFAR10 Corrupted Test data Accuracy: 78.60 LL: -1.164 Error: 0.214 Brier: 0.3566 ECE: 0.1447\n\n Laplace - CIFAR10 In-Dist Test data Accuracy: 89.0 LL: -1.563 Error: 0.110 Brier:  0.6869 ECE: 0.6647\n\n Laplace - CIFAR10 Corrupted Test data Accuracy: 81.0 LL: -1.660 Error: 0.190 Brier: 0.7156 ECE: 0.5890\n\n------------------------------------------------------------------------------------------------------------------------------------------------------\n\n SWAG -   CIFAR100 In-Dist Test data Accuracy: 69.12 LL: -2.094 Error: 0.308 Brier: 0.5006 ECE: 0.2046\n\n SWAG -   CIFAR100 Corrupted Test data Accuracy: 46.5 LL: -4.1878 Error: 0.535 Brier: 0.8418 ECE: 0.3640\n\n Laplace - CIFAR100 In-Dist Test data Accuracy: 70.0 LL: -3.9172 Error: 0.300 Brier:  0.967 ECE: 0.6747\n\nLaplace - CIFAR100 Corrupted Test data   Accuracy: 42.0 LL: -4.199 Error: 0.58 Brier:0.9770 ECE: 0.396\n\n **SUBNETWORK**\n\n SWAG -   CIFAR10 In-Dist Test data Accuracy: 92.54 LL: -0.4251 Error: 0.074 Brier: 0.1255 ECE: 0.0491\n\n SWAG -   CIFAR10 Corrupted Test data Accuracy: 76.90 LL: -1.4599 Error: 0.231 Brier: 0.3845 ECE: 0.1732\n\n Laplace - CIFAR10 In-Dist Test data Accuracy: 91.0 LL: -1.551723 Error: 0.090 Brier: 0.682 ECE: 0.6823\n\n Laplace - CIFAR10 Corrupted Test data  Accuracy:  81.0 LL: -1.650211 Error: 0.190 Brier: 0.7134 ECE: 0.5886\n\n------------------------------------------------------------------------------------------------------------------------------------------------------\n\n SWAG -   CIFAR100 In-Dist Test data Accuracy: 69.86 LL: -2.1430 Error: 0.30 Brier: 0.49 ECE: 0.207\n\n SWAG -   CIFAR100 Corrupted Test data Accuracy: 49.0 LL: -3.9721 Error: 0.51 Brier: 0.82 ECE: 0.3483\n\n Laplace - CIFAR100 In-Dist Test data Accuracy: 68.0 LL: -3.9505 Error: 0.32 Brier: 0.9682 ECE: 0.655\n\n Laplace - CIFAR100 Corrupted Test data Accuracy: 49.0 LL: -4.1309 Error: 0.51 Brier: 0.974 ECE: 0.466\n\n **PARTIALLY STOCHASTIC**\n\n SWAG -   CIFAR10 In-Dist Test data Accuracy: 92.54 LL: -0.4251 Error: 0.074 Brier: 0.1255 ECE: 0.0491\n\n SWAG -   CIFAR10 Corrupted Test data Accuracy: 76.92 LL: -1.4499 Error: 0.201 Brier: 0.3806 ECE: 0.1730\n\n Laplace - CIFAR10 In-Dist Test data Accuracy: 90.8 LL: -1.561 Error: 0.091 Brier: 0.68 ECE: 0.702\n\n Laplace - CIFAR10 Corrupted Test data Accuracy: 80.0 LL: -1.67 Error: 0.21 Brier: 0.72 ECE: 0.59\n\n\n***thread continued.........***"
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2362/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700238102535,
                "cdate": 1700238102535,
                "tmdate": 1700238102535,
                "mdate": 1700238102535,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "x7xDRkw6OZ",
                "forum": "5KUiMKRebi",
                "replyto": "x4jofXBGSI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2362/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2362/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "----------------------------------------------------------------------------------------------------------------------------------------------------\n\n SWAG - CIFAR100 In-Dist Test data Accuracy: 69.85 LL: -2.1430 Error: 0.3015 Brier: 0.4997 ECE: 0.2078\n\n SWAG - CIFAR100 Corrupted Test data Accuracy: 49.0 LL: -3.9721 Error: 0.51 Brier: 0.8218 ECE: 0.3483\n\n Laplace - CIFAR100 In-Dist Test data Accuracy: 66.0 LL: -3.9903 Error: 0.34 Brier:  0.978 ECE: 0.6377\n\n Laplace - CIFAR100 Corrupted Test data Accuracy: 49.0 LL: -4.1815 Error: 0.51 Brier 0.989 ECE: 0.4709\n\n\\*For SWAG and Linearized Laplace with GGN, in order to be able to run across low dimensional spaces we choose the covariance to have Diagonal structure.\n\nAs for the results, we believe that in both datasets there is at the very least a trend in favor of both proposed INR-x methods. The results validate to a considerable degree the premise of the proposed methods: Instead of choosing a subset or subnet following the rationale of the corresponding methods, the INR produces \"\u03be\" outputs that endow the full network with the desirable stochasticity, while keeping the dimensionality of the random process that we want to do inference upon at a low level."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2362/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700238157267,
                "cdate": 1700238157267,
                "tmdate": 1700238157267,
                "mdate": 1700238157267,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fJXzDQZmrn",
                "forum": "5KUiMKRebi",
                "replyto": "x4jofXBGSI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2362/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2362/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "*\u201cI wonder about the integer inputs (i.e. the tensor coordinates) to the INR network. Do you normalize these coordinates somehow or directly input the integers into the INR network? Did such an integer input space cause any problems during training?\u201d*\n\nFollowing the SIREN paper Sitzmann et al. (2020). The tensor coordinate inputs of the hypernetwork are real numbers normalized to [-1,1]. All the technical details for each experiment are in the Appendix but we could add a separate paragraph dedicated to the INR technical details."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2362/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700238183999,
                "cdate": 1700238183999,
                "tmdate": 1700238183999,
                "mdate": 1700238183999,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "heQ3oIAkAT",
                "forum": "5KUiMKRebi",
                "replyto": "x4jofXBGSI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2362/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2362/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "*\u201cWhy do you think that sinusoidal activations are particularly suitable for the INR network? Do you expect the result to deteriorate if you used other activations (e.g. sigmoid)?\u201d*\n\nWe trained Resnet18 in both CIFAR10 and CIFAR100 for 100 epochs to evaluate the predictive capabilities of the Sinusoidal hyper-network.\nHere we have a comparison of Sine vs ReLU. If you insist on including sigmoid or another activation function specifically, we'll be happy to include a comparison.\n \n **CIFAR10**\n\n RELU_MAP Accuracy: 91.11 LL: -0.4891 Error: 0.088 Brier: 0.1484 ECE: 0.05891\n\n SINE MAP   Accuracy: 91.70 LL: -0.4449 Error: 0.083 Brier: 0.138 ECE: 0.05401\n\n **CIFAR100**\n\n RELU_MAP Accuracy: 67.79 LL: -2.544 Error: 0.3221 Brier: 0.537 ECE: 0.23232\n\n SINE MAP   Accuracy: 68.49 LL: -2.3990 Error: 0.3151 Brier: 0.527 ECE: 0.2256\n\nWe find that Sine/Periodic activations \u2013 the \u201cdefault\u201d choice in SIREN \u2013 slightly outperforms a hypernet with ReLU activations. Still, results are very close, though there is a trend in favor of sine in all benchmarks.\nThe original motivation behind using the sine activation is related to modeling high-frequency content, which translates as details in structured signals such as images or video [Sitzmann 2020]. We can however see this \u201cin the top of its head\u201d, so to speak: in structured signals we care *more* for low-frequency content, and high-frequency is a \u201cgood-to-have\u201d content. We can interpret an input semantically if we see its low frequencies, but not necessarily vice versa. For example, image compression will invariably throw away high frequencies first, and the last frequencies to lose will be the lower ones.\nOur conjecture is as follows: When using an INR to model *perturbations*, we are faced with a different situation, that corresponds to a different \u201cfrequency landscape\u201d (perhaps even different than the one of model *weights*). In particular, we do not have any reason to differentiate lower or higher frequency content in any respect. We \u201ccare\u201d for all frequencies, so we need to have a good way to model high frequencies as well. Perhaps this is the reason the sine activation gives a small edge over ReLU."
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2362/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700238336326,
                "cdate": 1700238336326,
                "tmdate": 1700238336326,
                "mdate": 1700238336326,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UwjIr96Oib",
                "forum": "5KUiMKRebi",
                "replyto": "x4jofXBGSI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2362/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2362/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "*\u201cWhy do you think the INR with 350 outperformed 4k and 10k versions on CIFAR in Fig. 1?\u201d*\n\nWe believe that the reason is related to the complexity of each problem. In CIFAR, a smaller model seems to be \u201cenough\u201d in terms of capacity, while the bigger versions are overly complex. Note that in Corrupted CIFAR (columns on the right) the situation is reversed, because the problem is comparatively more difficult, and we need the extra model capacity."
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2362/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700238368704,
                "cdate": 1700238368704,
                "tmdate": 1700238368704,
                "mdate": 1700238368704,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "EpCZDwcSYL",
                "forum": "5KUiMKRebi",
                "replyto": "x4jofXBGSI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2362/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2362/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "*\u201cI was very interested to see that the predictive uncertainty in Fig. 2 has a stationary structure (i.e. dependent on the distance from the observations) similar to a GP with a stationary kernel. The Dropout and Deep Ensembles baselines clearly don't have such a property, but I wonder how such a figure would look like if we used a Laplace approximation directly on some layers of the main network (without INR), e.g. similar to Kristiadi et al. (2020)? In other words, I wonder how specific this stationary uncertainty structure is to the inference using an auxiliary network?\u201d*\n\nThe stationary structure (or in-between-uncertainty) is one of the benefits of the Linearized Laplace approximation as shown in multiple works (Kristiadi et al. (2020), Daxberger et al. (2021), Immer et al., 2021), ). In this Fig. 2 what we want to highlight is that the low dimensional INR space we propose is able to maintain the appealing characteristics of the approximate inference methods applied (in this particular case the stationary structure of the Lin. Laplace )"
                    }
                },
                "number": 24,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2362/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700238415045,
                "cdate": 1700238415045,
                "tmdate": 1700238415045,
                "mdate": 1700238415045,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kKOJfCMr5Y",
                "forum": "5KUiMKRebi",
                "replyto": "EpCZDwcSYL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2362/Reviewer_wVmX"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2362/Reviewer_wVmX"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the reply!"
                    },
                    "comment": {
                        "value": "Thank you for your detailed reply. I don't have further questions at this stage and I confirm my positive view on this submission."
                    }
                },
                "number": 28,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2362/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700490070159,
                "cdate": 1700490070159,
                "tmdate": 1700490070159,
                "mdate": 1700490070159,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "jsuF5bGbo1",
            "forum": "5KUiMKRebi",
            "replyto": "5KUiMKRebi",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2362/Reviewer_Ji8X"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2362/Reviewer_Ji8X"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes so-called implicit neural representation inference for Bayesian Neural Networks. In the past, several \"subspace\" inference frameworks have been devised, where the aim is to only model smaller part of the weight space in a Bayesian manner. This type of approaches promise to better scale the approximate Bayesian inference for deep learning, while making the inference procedure more accurate. Building upon, this paper proposes to obtain the \"subspace\" of weights using implicit neural representation. The paper shows how the proposed method can enhance popular frameworks such as Laplace Approximation, SWAG and normalizing flows. Experiments are conducted on UCI, cifar10 and cifar100 and the approach is compared against the baselines with performs inference over all the parameters of neural networks, as oppose to the subspace."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "In my view, the paper has the following strengths:\n\n- the idea of using implicit neural representation for improving Bayesian Neural Network is interesting and novel.\n\n- Implicit neural representations are currently a popular topic, and may be therefore relevant to many researchers."
                },
                "weaknesses": {
                    "value": "On the other-hand, I think the paper has several rooms to improve for a publication.\n\n- The paper could improve in terms of its clarity.\n\nSpecifically, I find it difficult to parse section 3.1. I get the problem statement, but the parts on implicit neural representation with the SIREN model was difficult to understand. It would help to have a figure on this since there are many notations introduced. The text contains many mathematical symbols, which would have been explained differently. \n\nSIREN should be explained more in depth since I think it is an important technical detail. Also, some technical details on how implicit neural representation is obtained, and the general working principles, like an algorithm behind, would be helpful for the reader.\n\n- Experiments may become more solid with other choices of baselines and datasets.\n\nFirst, there has been many subspace approaches and also many approaches that attempts to sparsely represent model uncertainty. Some examples are references in the paper: Kristiadi, Dusenberry, Daxberger, etc. I think the experiments should compare to these baselines as well, which can really show the advantages of implicit neural representation over existing methods within the same class of approaches. Comparisons to full weight space seem not natural.\n\nMoreover, the paper uses UCI and CIFAR as main datasets. I would have liked the paper more if \"uncertainty baselines\" from google was used, as such works represent the more upto date standard in experimental protocol. Speaking of the protocol, the included baselines seem not very consistent, e.g., Figure 1 misses laplace approximation, figure 2 again selectively reports INR Laplace and contains no SWAG, figure 3 misses swag, INR swag, etc.  \n\nOverall, I recommend a weak rejection. Improving the technical quality and clarity of the presentation would be meaningful here."
                },
                "questions": {
                    "value": "1.In line with section 3.1, is it possible to explain why INR is advantages to learn the subspace?  I could not get why it might be a good idea.\n\n2. Another question is on expressiveness Vs accuracy of the Bayesian inference. Basically, the weight space is very large. Having a simple distribution in such a complex high dimensional space can be already advantageous in terms of expressiveness of the probability distribution. Of course, a natural direction has been also improving the expressiveness through structured distribution, correlations amongst layers, etc. On the other hand, what these subspace approaches do is to model with smaller part of the network, which can actually reduce the overall expressiveness of the distribution, though overall inference might be easier and accurate. Then the question is, when should we look into the approaches for expressiveness, and when should we look into the subspace approaches?\n\n3. Why not only take last few layers of the network and make them probabilistic? What are advantageous of using INR against very simple baselines as taking last one or three layers? I think it might be interesting to include them as a baselines in the experiments."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2362/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2362/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2362/Reviewer_Ji8X"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2362/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698699204527,
            "cdate": 1698699204527,
            "tmdate": 1699636168616,
            "mdate": 1699636168616,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "m41XzEtEKc",
                "forum": "5KUiMKRebi",
                "replyto": "jsuF5bGbo1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2362/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2362/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "*\u201cThe paper could improve in terms of its clarity. [...] Specifically, I find it difficult to parse section 3.1. I get the problem statement, but the parts on implicit neural representation with the SIREN model was difficult to understand. It would help to have a figure on this since there are many notations introduced. The text contains many mathematical symbols, which would have been explained differently.\u201d*\n\nThank you for your comment. We will do our best to clarify this section in the final text. In the meanwhile, we\u2019d be happy to explain any step or detail of the method you would like to. We will add a figure representing the proposed model.\n\nWe added a graphical assessment which depicts our method in simple MLP Training Setting, where the main network consists of 4 linear layers and the INR hypernetwork has 2 layers producing accordingly 4 sets of \u03be factors (see https://freeimage.host/i/JnnQtyb)."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2362/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700236615750,
                "cdate": 1700236615750,
                "tmdate": 1700236615750,
                "mdate": 1700236615750,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "owVLfTqGOh",
                "forum": "5KUiMKRebi",
                "replyto": "jsuF5bGbo1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2362/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2362/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "*\u201cSIREN should be explained more in depth since I think it is an important technical detail. Also, some technical details on how implicit neural representation is obtained, and the general working principles, like an algorithm behind, would be helpful for the reader.\u201d*\n\nWe gladly add high level pseudocode to introduce our methods behavior in training and inference settings:\n\n**Algorithm 1** Training\n```\n Inputs: I (indices of main network weights), Net (main network), INR (INR hypernetwork), Dataset\n for number of epochs do\n \tfor x,y in Dataset do\n \t\t\u03be = INR(I)\n \t\ty* = Net(x,\u03be)\n \t\tloss = (y,y*)\n \t\tupdate INR w.r.t loss\n \t\tupdate Net w.r.t loss\n \tend\n end\n```\n\n**Algorithm 2** Inference\n```\n Inputs: I (indices of main network weights), Net (main network), INR (INR hypernetwork), Test_set\n Ap.-In (Approximate inference method*) MC_samples (Number of Monte Carlo samples)\n for x in Test_set do\n \tfor MC_samples do\n \t\t\u03bej = Ap.-In(INR, I)\n\t\ty* = Net(x,\u03bej)\n \tend\n \tcalculate y* statistics\n end\n```\n\n(In this setting a post training Monte Carlo-based approximate inference method is implied)."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2362/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700236842739,
                "cdate": 1700236842739,
                "tmdate": 1700236862511,
                "mdate": 1700236862511,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zTFqXurN5q",
                "forum": "5KUiMKRebi",
                "replyto": "jsuF5bGbo1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2362/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2362/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "*\"I would have liked the paper more if \"uncertainty baselines\" from google was used, as such works represent the more upto date standard in experimental protocol. Speaking of the protocol, the included baselines seem not very consistent, e.g., Figure 1 misses laplace approximation, figure 2 again selectively reports INR Laplace and contains no SWAG, figure 3 misses swag, INR swag, etc.\u201d*\n\nWe agree that \u201cUncertainty baselines\u201d is definitely useful, and we have tried to include as many benchmarks that appear in it.\n\nIn our work we tried to validate our INR-\u2019space\u2019 combined with a variety of approximate inference methods. Both in classification and regression experiments all the three posterior approximations combined our method yield good and calibrated results.\nThere are some reasons why some methods do not appear in some figures. For example in Fig1 we wanted to also measure the diversity so we wanted a Monte Carlo based method and to save space we chose SWAG. Also in Fig2 we wanted to highlight the benefits of Laplace approximation in particular. In general we followed a pattern that in Regression we use (Laplace/RealNVP) , and in Classification (SWAG/RealNVP). We will gladly include all three methods in all experiments in the appendix."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2362/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700237102704,
                "cdate": 1700237102704,
                "tmdate": 1700237102704,
                "mdate": 1700237102704,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "x3Ssd41SdQ",
                "forum": "5KUiMKRebi",
                "replyto": "jsuF5bGbo1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2362/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2362/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "*\"Experiments may become more solid with other choices of baselines and datasets. First, there has been many subspace approaches and also many approaches that attempts to sparsely represent model uncertainty.\"*\n\nFor our UCI regression experiments we added another strong baseline. For the small MLP network we use we were able to compute the full GGN matrix in the Laplace approximation of the main network. We copy the result in the form a figure (https://freeimage.host/i/Jnn6z6g)."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2362/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700237335342,
                "cdate": 1700237335342,
                "tmdate": 1700237335342,
                "mdate": 1700237335342,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "NFYVUyG8Y1",
                "forum": "5KUiMKRebi",
                "replyto": "jsuF5bGbo1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2362/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2362/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "*\u201c1.In line with section 3.1, is it possible to explain why INR is advantageous to learn the subspace? I could not get why it might be a good idea.\u201d*\n\nOur inspiration comes from works where Implicit Neural Representation are used to learn sets of network weights. For example in Romero et al. (2021a), convolutional kernels are represented in terms of INR-based Multiplicative Anisotropic Gabor Networks. Another recent example is Ashkenazi, Maor, et al. \"NeRN: Learning Neural Representations for Neural Networks.\" from ICLR 2022."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2362/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700237687918,
                "cdate": 1700237687918,
                "tmdate": 1700237687918,
                "mdate": 1700237687918,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aSAbHxAUxG",
                "forum": "5KUiMKRebi",
                "replyto": "jsuF5bGbo1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2362/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2362/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "*\u201cWhy not only take last few layers of the network and make them probabilistic? What are the advantages of using INR against very simple baselines as taking last one or three layers? I think it might be interesting to include them as a baselines in the experiments.\u201d*\n\nFollowing the reviewer\u2019s recommendations we added an additional ablation study. We tried to measure the quality of proposed subspaces in terms of predictive uncertainty. Specifically we compare our INR low dimensional space with: Rank-1 (Dusenberry et al. 2020); Wasserstein subnetwork  (Daxberger et al. 2021) and partially stochastic ResNets from (Sharma, Mrinank, et al. partially). \nWe trained (each method) combined with a Resnet18 for 100 epochs in both Cifar10 and Cifar100 datasets while keeping the approximate inference method fixed same across all low dimensional spaces.\n\n* Sharma, Mrinank, et al. \"Do Bayesian Neural Networks Need To Be Fully Stochastic?.\" International Conference on Artificial Intelligence and Statistics. PMLR, 2023.\n\n **RANK1**\n\n SWAG - CIFAR10 In-Dist Test data Accuracy: 91.78 LL: -0.4187 Error: 0.082 Brier: 0.1343 ECE: 0.0522\n\n SWAG - CIFAR10 Corrupted Test data Accuracy: 77.80 LL: -1.2537 Error: 0.222 Brier: 0.3596 ECE: 0.1469\n\n Laplace - CIFAR10 In-Dist Test data Accuracy: 91.01 LL: -1.5630 Error: 0.090 Brier: 0.6872 ECE: 0.6871\n\n Laplace - CIFAR10 Corrupted Test data Accuracy: 78.07 LL: -1.7068 Error: 0.220 Brier: 0.7396 ECE: 0.5737\n\n-----------------------------------------------------------------------------------------------------------------------------------------------------\n\nSWAG -   CIFAR100 In-Dist Test data         Accuracy: 65.84 LL: -2.29 Error: 0.34 Brier: 0.55 ECE: 0.228\n\nSWAG -   CIFAR100 Corrupted Test data   Accuracy: 42.80 LL: -4.77 Error: 0.57 Brier: 0.92 ECE: 0.398\n\n Laplace - CIFAR100 In-Dist Test data        Accuracy: 69.00 LL: -4.01 Error: 0.31 Brier:  0.9714 ECE: 0.668\n\n Laplace - CIFAR100 Corrupted Test data   Accuracy: 42.00 LL: -4.25 Error: 0.58 Brier 0.979 ECE: 0.401\n\n **INR**\n\n SWAG -   CIFAR10 In-Dist Test data Accuracy: 92.17 LL: -0.384 Error: 0.078 Brier: 0.1296 ECE: 0.0498\n\n SWAG -   CIFAR10 Corrupted Test data Accuracy: 78.60 LL: -1.164 Error: 0.214 Brier: 0.3566 ECE: 0.1447\n\n Laplace - CIFAR10 In-Dist Test data Accuracy: 89.0 LL: -1.563 Error: 0.110 Brier:  0.6869 ECE: 0.6647\n\n Laplace - CIFAR10 Corrupted Test data Accuracy: 81.0 LL: -1.660 Error: 0.190 Brier: 0.7156 ECE: 0.5890\n\n------------------------------------------------------------------------------------------------------------------------------------------------------\n\n SWAG -   CIFAR100 In-Dist Test data Accuracy: 69.12 LL: -2.094 Error: 0.308 Brier: 0.5006 ECE: 0.2046\n\n SWAG -   CIFAR100 Corrupted Test data Accuracy: 46.5 LL: -4.1878 Error: 0.535 Brier: 0.8418 ECE: 0.3640\n\n Laplace - CIFAR100 In-Dist Test data Accuracy: 70.0 LL: -3.9172 Error: 0.300 Brier:  0.967 ECE: 0.6747\n\nLaplace - CIFAR100 Corrupted Test data   Accuracy: 42.0 LL: -4.199 Error: 0.58 Brier:0.9770 ECE: 0.396\n\n **SUBNETWORK**\n\n SWAG -   CIFAR10 In-Dist Test data Accuracy: 92.54 LL: -0.4251 Error: 0.074 Brier: 0.1255 ECE: 0.0491\n\n SWAG -   CIFAR10 Corrupted Test data Accuracy: 76.90 LL: -1.4599 Error: 0.231 Brier: 0.3845 ECE: 0.1732\n\n Laplace - CIFAR10 In-Dist Test data Accuracy: 91.0 LL: -1.551723 Error: 0.090 Brier: 0.682 ECE: 0.6823\n\n Laplace - CIFAR10 Corrupted Test data  Accuracy:  81.0 LL: -1.650211 Error: 0.190 Brier: 0.7134 ECE: 0.5886\n\n------------------------------------------------------------------------------------------------------------------------------------------------------\n\n SWAG -   CIFAR100 In-Dist Test data Accuracy: 69.86 LL: -2.1430 Error: 0.30 Brier: 0.49 ECE: 0.207\n\n SWAG -   CIFAR100 Corrupted Test data Accuracy: 49.0 LL: -3.9721 Error: 0.51 Brier: 0.82 ECE: 0.3483\n\n Laplace - CIFAR100 In-Dist Test data Accuracy: 68.0 LL: -3.9505 Error: 0.32 Brier: 0.9682 ECE: 0.655\n\n Laplace - CIFAR100 Corrupted Test data Accuracy: 49.0 LL: -4.1309 Error: 0.51 Brier: 0.974 ECE: 0.466\n\n **PARTIALLY STOCHASTIC**\n\n SWAG -   CIFAR10 In-Dist Test data Accuracy: 92.54 LL: -0.4251 Error: 0.074 Brier: 0.1255 ECE: 0.0491\n\n SWAG -   CIFAR10 Corrupted Test data Accuracy: 76.92 LL: -1.4499 Error: 0.201 Brier: 0.3806 ECE: 0.1730\n\n Laplace - CIFAR10 In-Dist Test data Accuracy: 90.8 LL: -1.561 Error: 0.091 Brier: 0.68 ECE: 0.702\n\n Laplace - CIFAR10 Corrupted Test data Accuracy: 80.0 LL: -1.67 Error: 0.21 Brier: 0.72 ECE: 0.59\n\n\n***thread continued.........***"
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2362/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700237967350,
                "cdate": 1700237967350,
                "tmdate": 1700237967350,
                "mdate": 1700237967350,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "eua9J5urJa",
                "forum": "5KUiMKRebi",
                "replyto": "jsuF5bGbo1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2362/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2362/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "----------------------------------------------------------------------------------------------------------------------------------------------------\n\n SWAG - CIFAR100 In-Dist Test data Accuracy: 69.85 LL: -2.1430 Error: 0.3015 Brier: 0.4997 ECE: 0.2078\n\n SWAG - CIFAR100 Corrupted Test data Accuracy: 49.0 LL: -3.9721 Error: 0.51 Brier: 0.8218 ECE: 0.3483\n\n Laplace - CIFAR100 In-Dist Test data Accuracy: 66.0 LL: -3.9903 Error: 0.34 Brier:  0.978 ECE: 0.6377\n\n Laplace - CIFAR100 Corrupted Test data Accuracy: 49.0 LL: -4.1815 Error: 0.51 Brier 0.989 ECE: 0.4709\n\n\\*For SWAG and Linearized Laplace with GGN, in order to be able to run across low dimensional spaces we choose the covariance to have Diagonal structure.\n\nAs for the results, we believe that in both datasets there is at the very least a trend in favor of both proposed INR-x methods. The results validate to a considerable degree the premise of the proposed methods: Instead of choosing a subset or subnet following the rationale of the corresponding methods, the INR produces \"\u03be\" outputs that endow the full network with the desirable stochasticity, while keeping the dimensionality of the random process that we want to do inference upon at a low level."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2362/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700237994234,
                "cdate": 1700237994234,
                "tmdate": 1700237994234,
                "mdate": 1700237994234,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zmOKKNu6TW",
                "forum": "5KUiMKRebi",
                "replyto": "jsuF5bGbo1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2362/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2362/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "*\u201cAnother question is on expressiveness Vs accuracy of the Bayesian inference. Basically, the weight space is very large. Having a simple distribution in such a complex high dimensional space can be already advantageous in terms of expressiveness of the probability distribution. Of course, a natural direction has been also improving the expressiveness through structured distribution, correlations amongst layers, etc. On the other hand, what these subspace approaches do is to model with smaller part of the network, which can actually reduce the overall expressiveness of the distribution, though overall inference might be easier and accurate. Then the question is, when should we look into the approaches for expressiveness, and when should we look into the subspace approaches?\u201d*\n\nThis is an interesting question.\n\nOur take is that there are multiple trade-offs behind choices that we have to make in the context of a model solved with Bayesian inference. A more complex model should correspond to a more complex weight space, which translates to a more difficult optimization problem. Therefore \u2013 if we understand your position correctly, we have a decision to make regarding our \u201ccomplexity budget\u201d, so to speak. So there is one issue concerning the problem of whether we should prioritize learning an accurate estimator, or ensuring that we can have a learning system that will output a calibrated measure of uncertainty. In the context of the proposed INR-based approach, this is related to the complexity, expressiveness and capacity of the main network and the (SIREN) hypernetwork respectively. \n\nPerhaps closer to your point of view is the question regarding how to proceed more efficiently with respect to choices regarding inference. If we have to work on the entire space of weights as the domain for our posterior distribution, in *practice* we need to make concessions in inference, which translates as working with uncorrelated estimates per weight, or correlations only on the level of layer, or KFAC, and so on. The other approach, closer to what we do in the current work, is to choose some lower-dimensionality space to work with a minimum of inference constraints. This is what we do in the current work, and we think that a comparatively small INR is a very good way to proceed here. Concerning \u201cwhen\u201d is one way to proceed better than the other, our results indicate that the INR-based approach comes with clear advantages. We will note though, that this is related to the complexity of the problems we try to solve. We tried our best to include as many baselines and ablations in this respect.\n\nAs the future of the probabilistic ML subfield moves towards larger and more complex baselines, time will tell how the \u201cexpressiveness vs accuracy\u201d tradeoff will shape new models and approaches."
                    }
                },
                "number": 25,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2362/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700321666938,
                "cdate": 1700321666938,
                "tmdate": 1700321666938,
                "mdate": 1700321666938,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "FvgYJQkTWe",
            "forum": "5KUiMKRebi",
            "replyto": "5KUiMKRebi",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2362/Reviewer_VEKs"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2362/Reviewer_VEKs"
            ],
            "content": {
                "summary": {
                    "value": "The authors present a new framework for approximate Bayesian inference in neural networks based on low-dimensional hypernetwork representations of weight perturbations. In this framework, hypernetworks take in weight coordinates as input and output perturbation factors for each weight. An approximate posterior is fitted for the hypernetwork and perturbations are repeatedly sampled and multiplied with the main network weights to perform Bayesian model averaging. Three alternative Bayesian hypernetworks are considered: sinusoidal representation networks (SIRENs) with Laplace and SWA-Gaussian approximate posteriors, and normalizing flows with Real Non-Volume Preserving (RealNVP) transformations. The posteriors are benchmarked on UCI regression and gap datasets, and out-of-distribution (OOD) detection is tested on CIFAR-10 image recognition."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The proposed framework introduces a novel combination of approximate Bayesian inference and implicit neural representations as hypernetworks which, to the best of my knowledge, has not been explored in prior research.\n- The paper offers valuable insights, e.g. in considering the benefit of multiplicative over additive perturbances, and in finding that CNN models benefit from shared representations.\n- The evaluation of out-of-distribution (OOD) detection on the CIFAR-10 dataset is thorough and detailed. This plays a large role in showing the practical utility of the proposed approach."
                },
                "weaknesses": {
                    "value": "Regarding the method:\n- The derivation of the Laplace approximation is confusing: In Equations (4) and (5) a Laplace approximation with full Hessian is derived. While a full Hessian for the hypernetwork weights may be tractable for small enough networks, this approximation does not correspond to the closed form posterior of Equation (7). This closed form corresponds to linearized Laplace inference with generalized Gauss-Newton (GGN) approximation to the Hessian. It is unclear if the model evaluated in the experiments uses a full Hessian or the GGN approximation with closed form.\n- The use of SIREN activation suggests that the hypernetworks need to model high frequency representations of the weight perturbations. The SIREN models presented in [Sitzmann et al., 2020] benefit from consistency and repeating patterns in the signals they are fitting, making interpolation easier. There might be some form of structural consistency in neighboring weight perturbations for CNN kernels, but it seems unlikely for the weights of linear layers. I also expect that RealNVP hypernetworks have a harder time fitting high frequency representations when comparing to SIREN. This matter is only very briefly brought up in the discussion of Figures 9 and 10.\n\nRegarding experiments:\n- The methods are benchmarked against a last-layer Laplace approximation, however a block-diagonal Kronecker-factorized (KFAC) Laplace approximation should also be considered since this corresponds better to a full network variant in the linearized Laplace family of approximations.\n- Both theoretical and experimental runtimes and memory requirements are not discussed. It is unclear if the proposed models take significantly more time for training or inference, since in theory a forward pass of the hypernetwork is required for every \"main\" network weight."
                },
                "questions": {
                    "value": "- Does the INR-Laplace model in the experiments employ a full Hessian or a GGN approximation? Do you make any additional approximations (e.g. Kronecker-factorization) ?\n- Can you further evaluate the effect of INR network size on model performance and quality of the uncertainty estimates? Figure 1 already does this for CIFAR-10 but it would be interesting to see for MLPs on UCI and UCI-gap datasets, with INR-Laplace and INR-RealNVP, and perhaps on wider sets of network sizes. Would also be helpful to include error bars for these figures.\n- Have you tried using ReLU hypernetworks? Do these fail to recover high frequency representations of the perturbations compared to SIREN?\n- Do you have an intuition for why multiplicative perturbations are so successful compared to additive perturbations of the weights? Perhaps uncertainty is improved when perturbations are scaled up by the magnitude of the weight?\n- Is there a reason for INR-SWAG appearing in some experiments and INR-Laplace in others?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2362/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2362/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2362/Reviewer_VEKs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2362/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698778181236,
            "cdate": 1698778181236,
            "tmdate": 1699636168543,
            "mdate": 1699636168543,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "yboVS1v2M5",
                "forum": "5KUiMKRebi",
                "replyto": "FvgYJQkTWe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2362/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2362/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "*\u201cThe derivation of the Laplace approximation is confusing: In Equations (4) and (5) a Laplace approximation with full Hessian is derived. While a full Hessian for the hypernetwork weights may be tractable for small enough networks, this approximation does not correspond to the closed form posterior of Equation (7). This closed form corresponds to linearized Laplace inference with generalized Gauss-Newton (GGN) approximation to the Hessian. It is unclear if the model evaluated in the experiments uses a full Hessian or the GGN approximation with closed form. [... Question 1:] Does the INR-Laplace model in the experiments employ a full Hessian or a GGN approximation? Do you make any additional approximations (e.g. Kronecker-factorization) ?\u201d*\n\n Indeed, in practice we do not use the full Hessian here. We use linearized Laplace with GGN approximation to the Hessian. Eq.4 and 5 correspond to the more general case, which is nevertheless consistent with the rest of the model in theory. We do not do KFAC or other approximations concerning the Hessian."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2362/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700234937596,
                "cdate": 1700234937596,
                "tmdate": 1700234937596,
                "mdate": 1700234937596,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "NjJ4ZvL1kG",
                "forum": "5KUiMKRebi",
                "replyto": "FvgYJQkTWe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2362/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2362/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "*\u201cThe use of SIREN activation suggests that the hypernetworks need to model high frequency representations of the weight perturbations. The SIREN models presented in [Sitzmann et al., 2020] benefit from consistency and repeating patterns in the signals they are fitting, making interpolation easier. There might be some form of structural consistency in neighboring weight perturbations for CNN kernels, but it seems unlikely for the weights of linear layers. [..] This matter is only very briefly brought up in the discussion of Figures 9 and 10. [..] Have you tried using ReLU hypernetworks? Do these fail to recover high frequency representations of the perturbations compared to SIREN?\u201d*\n\nWe trained Resnet18 in both CIFAR10 and CIFAR100 for 100 epochs to evaluate the predictive capabilities of the Sinusoidal hyper-network:\n \n **CIFAR10**\n\n RELU_MAP Accuracy: 91.11 LL: -0.4891 Error: 0.088 Brier: 0.1484 ECE: 0.05891\n\n SINE MAP   Accuracy: 91.70 LL: -0.4449 Error: 0.083 Brier: 0.138 ECE: 0.05401\n\n **CIFAR100**\n\n RELU_MAP Accuracy: 67.79 LL: -2.544 Error: 0.3221 Brier: 0.537 ECE: 0.23232\n\n SINE MAP   Accuracy: 68.49 LL: -2.3990 Error: 0.3151 Brier: 0.527 ECE: 0.2256\n\nWe find that Sine/Periodic activations \u2013 the \u201cdefault\u201d choice in SIREN \u2013 slightly outperforms a hypernet with ReLU activations. Still, results are very close, though there is a trend in favor of sine in all benchmarks.\nThe original motivation behind using the sine activation is related to modeling high-frequency content, which translates as details in structured signals such as images or video [Sitzmann 2020]. We can however see this \u201cin the top of its head\u201d, so to speak: in structured signals we care *more* for low-frequency content, and high-frequency is a \u201cgood-to-have\u201d content. We can interpret an input semantically if we see its low frequencies, but not necessarily vice versa. For example, image compression will invariably throw away high frequencies first, and the last frequencies to lose will be the lower ones.\nOur conjecture is as follows: When using an INR to model *perturbations*, we are faced with a different situation, that corresponds to a different \u201cfrequency landscape\u201d (perhaps even different than the one of model *weights*). In particular, we do not have any reason to differentiate lower or higher frequency content in any respect. We \u201ccare\u201d for all frequencies, so we need to have a good way to model high frequencies as well. Perhaps this is the reason the sine activation gives a small edge over ReLU.\n\n*\u201cI also expect that RealNVP hypernetworks have a harder time fitting high frequency representations when compared to SIREN.\u201d*\n\nWe use RealNVP as a way to model the posterior approximation q(w_INR). SIREN is the architecture of the hypernetwork, so we can\u2019t really compare the two."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2362/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700235025812,
                "cdate": 1700235025812,
                "tmdate": 1700235056969,
                "mdate": 1700235056969,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bLgxmsAFHS",
                "forum": "5KUiMKRebi",
                "replyto": "FvgYJQkTWe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2362/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2362/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "*\u201cThe methods are benchmarked against a last-layer Laplace approximation, however a block-diagonal Kronecker-factorized (KFAC) Laplace approximation should also be considered since this corresponds better to a full network variant in the linearized Laplace family of approximations.\u201d*\n\nThank you for the suggestion. We agree that this would be a very useful experiment. However, we find it very difficult to complete any meaningful set of experiments on KFAC, given the time constraint of the discussion period. We can try featuring a result in the final version of the paper. \n\nFor our UCI regression experiments we added another strong baseline. For the small MLP network we use we were able to compute the full GGN matrix in the Laplace approximation of the main network. We copy the result in the form a figure (https://freeimage.host/i/Jnn6z6g)"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2362/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700235691229,
                "cdate": 1700235691229,
                "tmdate": 1700235691229,
                "mdate": 1700235691229,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "NTCo4DqcBW",
                "forum": "5KUiMKRebi",
                "replyto": "FvgYJQkTWe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2362/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2362/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "*\u201cBoth theoretical and experimental runtimes and memory requirements are not discussed. It is unclear if the proposed models take significantly more time for training or inference, since in theory a forward pass of the hypernetwork is required for every \"main\" network weight.\u201d*\n\nRegarding the computational time of our method, our methods computational time can be decomposed as follows:\n\n\tTime of our method = hypernetwork training/evaluation (1) + approximate inference (2)\n\nWhere \\(1\\) According to the table below is in practice \\~1.2 slower than the vanilla network training. As for \\(2\\) although approximate inference methods methods are expensive, because in our method they are applied  in the small dimensional INR space in general it takes less time to evaluate.\n\n\nTime experiments (Resnet-18 on cifar100, Batch size=64)\n\n**Our method (main network + INR hypernetwork)**\n\n Forward 0.0069 \u00b1 0. 0001 (sec)\n\n Backward 0.0145 \u00b1 0.0084 (sec)\n\n\n**Vanilla Network**\n\n Forward 0.00463 \u00b1 0.00013 (sec)\n\n Backward 0.01115 \u00b1 0.00015 (sec)\n\n**Our method (main network + fixed \u03be perturbations without evaluating INR)**\n\n Forward 0.004585 \u00b1 0.00028 (sec)\n\n Backward 0.00901 \u00b1 0.00127 (sec)\n\n\nAs for the overhead in terms of learnable parameters is #W_{inr} (total number of the hypernetwork parameters) #AI_{inr} (number of approximate inference parameters applied on the INR space) which is as we mention in the main paper is in fact much less than #AI_{W} (number of approximate inference parameters applied on the full set of main network weights). \nPerformance wise our method is still be competitive w.r.t. methods like ensembles of D networks which at best is D times slower than the vanilla network.\n\nThus, we believe that our method could be applied to ImageNet. \nFurthermore, because the main overhead of our method is the hypernetwork evaluation we investigated the following alternative training scheme, to further improve our method in terms of time. \nInstead of training the main network weights W and W_INR together we update the W_INR parameters every 10 epochs of the main network training, this significantly reduces the computational overhead of our method and we hypothesize it can scale to IMAGENET models and datasets.\n\n**Training of ResNet18 CIFAR100**\n\n Full Training Accuracy: 69.01 LL: -2.32 Error: 0.3099 Brier: 0.5181 ECE: 0.222\n\n Alternative    Accuracy: 68.59 LL: -2.38 Error: 0.3141 Brier: 0.5211 ECE: 0.224\n\n\nBy updating the W_INR parameters every 10 epochs of the main network training has some (minor) effect in performance."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2362/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700235758933,
                "cdate": 1700235758933,
                "tmdate": 1700235758933,
                "mdate": 1700235758933,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gtFwjfdgz8",
                "forum": "5KUiMKRebi",
                "replyto": "FvgYJQkTWe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2362/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2362/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "*\u201cCan you further evaluate the effect of INR network size on model performance and quality of the uncertainty estimates? Figure 1 already does this for CIFAR-10 but it would be interesting to see for MLPs on UCI and UCI-gap datasets, with INR-Laplace and INR-RealNVP, and perhaps on wider sets of network sizes. Would also be helpful to include error bars for these figures.\u201d*\n\nWe added an ablation w.r.t. INR size following the UCI regression setting in our method. We compare 4 different versions of INR hypernetworks with an increasing number of parameters each,namelly (BIG=2500 MED=625, SMALL=75,XSMALL=10).\n\n(Please see Figure in link: https://freeimage.host/i/JnnsF8G)\n\nFrom the experiments we can observe that there is a limit to where someone can easily scale the INR hypernetwork and simultaneously gain performance. Individual characteristics play significant role to the INR size (main network size, dataset size, dataset dimension)\n As for the RealNVP, it was difficult to find a setting where the performance of INR-size can be isolated (different RealNVP sizes,  different training and VI hyperparameters etc)"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2362/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700235940359,
                "cdate": 1700235940359,
                "tmdate": 1700235940359,
                "mdate": 1700235940359,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "r5Gk9ovO5k",
                "forum": "5KUiMKRebi",
                "replyto": "FvgYJQkTWe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2362/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2362/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "*\u201cDo you have an intuition for why multiplicative perturbations are so successful compared to additive perturbations of the weights? Perhaps uncertainty is improved when perturbations are scaled up by the magnitude of the weight?\u201d*\n\nWe believe that the benefits of multiplicative perturbations can be traced to the training procedure of the INR hypernetwork. Specifically consider the following example of a single linear layer with inputs x, outputs y, weights W and perturbations \u03be.\n\nFor the multiplication  we have y = x*w*\u03be and for  additive  y = x*w + x*\u03be \nThe back-propagated gradients w.r.t. perturbations \u03be  \u2207\u03be (which are responsible for the hypernetwork training) will be \u2202L/\u2202y *x*W and \u2202L/\u2202y*x respectively. Because in the multiplicative structure \u2207\u03be depends on W, we argue that because W is responsible for fitting the data can pass valuable information to the hypernetwork weights in the multiplicative case leading to significant increase in the over all performance."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2362/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700236131552,
                "cdate": 1700236131552,
                "tmdate": 1700236131552,
                "mdate": 1700236131552,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "v0JoVVsWzd",
                "forum": "5KUiMKRebi",
                "replyto": "FvgYJQkTWe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2362/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2362/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "*\u201cIs there a reason for INR-SWAG appearing in some experiments and INR-Laplace in others?\u201d*\n\nIn our work we tried to validate our INR-\u2019space\u2019 combined with a variety of approximate inference methods. Both in classification and regression experiments all the three posterior approximations combined our method yield good and calibrated results.\nThere are some reasons why some methods do not appear in some figures. For example in Fig1 we wanted to also measure the diversity so we wanted a Monte Carlo based method and to save space we chose SWAG. Also in Fig2 we wanted to highlight the benefits of Laplace approximation in particular. In general we followed a pattern that in Regression we use (Laplace/RealNVP) , and in Classification (SWAG/RealNVP). We will gladly include all three methods in all experiments in the appendix."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2362/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700236174265,
                "cdate": 1700236174265,
                "tmdate": 1700236174265,
                "mdate": 1700236174265,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HgNeii0d4S",
                "forum": "5KUiMKRebi",
                "replyto": "CumTyUGj2g",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2362/Reviewer_VEKs"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2362/Reviewer_VEKs"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your comprehensive response to my initial review.\n\n> We use linearized Laplace with a GGN approximation to the Hessian. Eq. (4) and (5) correspond to the more general case, which is nevertheless consistent with the rest of the model in theory.\n\nThank you for clarifying this. In that case, I think you could perhaps focus the main text on Laplace-GGN and, if you wish, include a derivation with full-Hessian Laplace in the appendix, so as to avoid potential confusion here.\n\n> We use RealNVP as a way to model [...] $q(\\\\mathbf{w}\\_{\\\\mathrm{INR}})$. SIREN is the architecture of the hypernetwork, so we can\u2019t really compare the two.\n\nYou are right, I apologize for the mixup here. What I was trying to say is that the \"*activation*\" in RealNVP corresponds to a series of invertible transformations which I believe would have difficulty recovering a perturbation map with high-frequency changes in magnitude when you compare to the SIREN-based INR-Laplace and INR-SWAG. (More on this later.)\n\n> We find it very difficult to complete any meaningful set of experiments on KFAC, given the time constraint of the discussion period. We can try featuring a result in the final version of the paper.\n\nThank you for taking the time to evaluate Laplace-GGN on the UCI datasets. I am a tiny bit surprised that full GGN is underperforming on some of the UCI-gap datasets given that this was the configuration suggested by Foong et al. In any case, I agree that you should consider including Laplace-KFAC as a baseline in the final version.\n\n> According to the table below [hyper-network training/evaluation] is in practice ~1.2 slower [...]. \n\nThank you for these numbers. You also say \"because [of the] the small dimensional INR space, in general, it takes less time to evaluate.\" Would be nice to also provide runtimes for inference. If I understood correctly, a forward pass of the hyper-network is required for every main network weight. Do you batch the weight coordinates when computing perturbations? Do you make a single pass with all coordinates or is this overly expensive?\n\n> We find that sine/periodic activations [...] slightly outperform hyper-networks with ReLU activations.\n\n> We compare 4 different versions of INR hypernetworks with an increasing number of parameters [...]\n\n> In the multiplicative case, $\\\\nabla\\_{\\\\varepsilon}$ depends on $\\\\mathrm{w}$, we argue that [...] $\\\\mathrm{w}$ [...] can pass valuable information to the hyper-network weights.\n\nI think I might have a clearer picture now with the additional experiments on hyper-network size and ReLU and, more importantly, thanks to your comment on gradient information and multiplicative perturbation.\n\nDo you think that maybe the main network weights in linear layers re-arrange themselves in each layer to benefit from structural consistency? For example, that weights with low pertubation variance tend to one side of the weight vector and those with high variance to the other? This could be facilitated, as you say, by the multiplicative nature of the perturbation and would also explain why ReLU hyper-networks are also successful.\n\nI realize that the hyper-network inputs are high dimensional (in particular for the CNN kernels), but an experiment that could demonstrate this would be to observe perturbation variance as a function of the input weight coordinate by taking 1D slices or 2D slices of the input space and seeing if this is highly jagged or is somewhat smooth in nature."
                    }
                },
                "number": 27,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2362/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700408989470,
                "cdate": 1700408989470,
                "tmdate": 1700408989470,
                "mdate": 1700408989470,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Sscx2WyPyV",
                "forum": "5KUiMKRebi",
                "replyto": "uyXKio1Qg8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2362/Reviewer_VEKs"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2362/Reviewer_VEKs"
                ],
                "content": {
                    "comment": {
                        "value": "> [...] INR is shared between all weights, we could run the computations per layer in parallel as they don't depend on each other.\n\nThis is interesting insight which I think the manuscript would benefit from if made explicit.\n\n> We plotted the \u201cperturbation variance\u201d (the values of \u03be) as a function of input weight coordinates. Specifically for ResNet-18 trained on CIFAR we plotted the flattened values for each specific kernel position across channels (channel slice) for 2 two different convolutional layers.\n\nThank you for these figures. I was looking for the variance, i.e. $\\\\mathrm{Var}[\\epsilon]$ as a function of input weight coordinates, not $\\epsilon$ itself. I was also more interested in a linear layer versus convolution. However, these figures are already quite valuable since they are slicing across the channel axis. If we assume that the variance has similar behavior, this would indicate that kernels with high variance are grouped together to benefit from the smooth nature of the INR function.\n\n> We tried to measure the quality of proposed subspaces in terms of predictive uncertainty. [...]\n\nI think these results would be of interest to reviewer tKmG. I tend to agree with their assessment that the manuscript could benefit from experiments which differentiate it from prior work on hypernetwork-based methods, and these results seem to be heading in this direction."
                    }
                },
                "number": 33,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2362/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700650449603,
                "cdate": 1700650449603,
                "tmdate": 1700650449603,
                "mdate": 1700650449603,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uSDe7lRKZA",
                "forum": "5KUiMKRebi",
                "replyto": "FvgYJQkTWe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2362/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2362/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "*\u201cThis is interesting insight which I think the manuscript would benefit from if made explicit.\u201d*\n\nWe agree. We will rework this part of the manuscript to make this point explicit.\n\n*\u201cThank you for these figures. I was looking for the variance, i.e. as a function of input weight coordinates, not itself. I was also more interested in a linear layer versus convolution. However, these figures are already quite valuable since they are slicing across the channel axis. If we assume that the variance has similar behavior, this would indicate that kernels with high variance are grouped together to benefit from the smooth nature of the INR function.\u201d*\n\nFollowing the same experimental procedure we plotted alongside the mean values of \u03be also their \u00b1 variance (as this was computed from the SWAG-diagonal approximate inference method), again as a function of channel coordinates. We can observe that the variance has the same structural properties as the mean values of \u03be. Thus we believe it is makes sense for the main network convolutional kernel to take advantage of each structure.\n\nhttps://freeimage.host/i/JoB5aaa\n\nhttps://freeimage.host/i/JoB5Ejp"
                    }
                },
                "number": 42,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2362/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700723434085,
                "cdate": 1700723434085,
                "tmdate": 1700723470859,
                "mdate": 1700723470859,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XvJaOARjUm",
                "forum": "5KUiMKRebi",
                "replyto": "FvgYJQkTWe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2362/Reviewer_VEKs"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2362/Reviewer_VEKs"
                ],
                "content": {
                    "comment": {
                        "value": "> Following the same experimental procedure we plotted alongside the mean values of \u03be also their \u00b1 variance [...]\n\nThese new figures are interesting but slightly unexpected. I was expecting to see larger changes in the variance as a function of weight coordinates but it seems to be more or less static. I suppose this might be linked to the multiplicative nature of the perturbations since the resulting main network weight variance then mostly depends on the magnitudes of the main network weights and perturbation means.\n\nI think the manuscript would benefit from an analysis in the dynamics of the weight perturbations maybe including these new figures and performing some more experiments which go in this direction."
                    }
                },
                "number": 43,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2362/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700726592262,
                "cdate": 1700726592262,
                "tmdate": 1700726869007,
                "mdate": 1700726869007,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "f4n0kumTmI",
            "forum": "5KUiMKRebi",
            "replyto": "5KUiMKRebi",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2362/Reviewer_tKmG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2362/Reviewer_tKmG"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a hypernetwork-based inference mechanism for BNNs, focusing on two key aspects: first, they want the hypernetwork to be compact and reusable across the main network; second, they perform approximate Bayesian inference over the compact hypernetwork. A key part of this work is the fact that the authors also keep around deterministic weights which are modulated by the stochastic nuisances sampled from the hypernetwork, allowing them to maintain good performance.\n\nThe method appears to be competitive in experiments."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The authors propose a relatively narrow idea: making a small hypernetwork probabilistic and sharing it across the network.\n\nWhat I like about this paper is the good exploration over ways to do this: the authors both try Laplace approximations, and stochastic weight averaging as pragmatic means to parametrize an approximate posterior over the hypernet. They also propose to use normalizing flows to shape the outputs of this hypernetwork to squeeze a bit more performance out of it.\n\nEmpirically, the work shows good performance.\n\nThe paper is overall well written and easy to follow and understand.\n\nI also want to call out that the authors are good scholars, the breadth of relevant work cited here is comprehensive and commendable."
                },
                "weaknesses": {
                    "value": "The elephant in the room with this paper is that it is very narrow in its contribution.\n\nTiny shared hypernernetworks parametrizing individual implicit weight outputs via coordinate systems go as far back as the cited paper Karaletsos et al 2018. Bayesian inference over such hypernetworks has likewise been performed before, in the shape of GPs and BNNs which the authors both cite in their related work.\nBlending deterministic and stochastic weights coming from hypernetworks also has been done, again cited absolutely correctly by the authors in numerous papers.\n\nThe contribution I see here is not the novelty of any of the ideas then, but the specific engineering combinations tweaked to obtain good performance. For example, executing on practical pieces like Laplace for the hypernetwork or SWA and pairing that object with normalizing flows is good execution that probably helps with performance compared to previously mentioned works."
                },
                "questions": {
                    "value": "Given the relative dearth of new ideas in this work, I would like to identify its strengths in execution and overall quality of the work.\n\nCould the authors argue that their specific combination of techniques could be applied to a larger neural network, i.e. an imagenet model?\n\nCould the authors share a bit more about scalability and performance/memory constraints/ number of forward passes needed to obtain good performance?\n\nI would enjoy seeing more evidence that highlights the merits of the execution here, given the relatively modest technical contributions.\n\nAs the paper stands, I would find the contributions somewhat thin, but I would enjoy seeing this line of work with hypernetworks for BNNs be paired with a very strong empirical result in the style I ask for above to enrich the community and hope the authors can find something more to show that differentiates this work more from e.g. Dusenberry et al. empirically, which can also be interpreted as a specific hierarchical weight model (with layer-specific structure).\n\nAgain, I commend the authors for their quality scholarship and would enjoy more sound arguments to place this work into the trajectory of papers they have mentioned as a primarily empirical contribution."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2362/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699597936846,
            "cdate": 1699597936846,
            "tmdate": 1699636168484,
            "mdate": 1699636168484,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "736dbtA3fI",
                "forum": "5KUiMKRebi",
                "replyto": "f4n0kumTmI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2362/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2362/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "*\u201cEmpirically, the work shows good performance. The paper is overall well written and easy to follow and understand. [..] I also want to call out that the authors are good scholars, the breadth of relevant work cited here is comprehensive and commendable. [...] Again, I commend the authors for their quality scholarship and would enjoy more sound arguments to place this work into the trajectory of papers they have mentioned as a primarily empirical contribution.\u201d*\n\nWe are happy that you appreciate our effort. Thank you for your comment!"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2362/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700234600067,
                "cdate": 1700234600067,
                "tmdate": 1700234600067,
                "mdate": 1700234600067,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qAVEtbX6Yj",
                "forum": "5KUiMKRebi",
                "replyto": "f4n0kumTmI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2362/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2362/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "*\u201cCould the authors argue that their specific combination of techniques could be applied to a larger neural network, i.e. an imagenet model? [..] Could the authors share a bit more about scalability and performance/memory constraints/ number of forward passes needed to obtain good performance?\u201d*\n\nRegarding the computational time of our method, our methods computational time can be decomposed as follows:\n\n\tTime of our method = hypernetwork training/evaluation (1) + approximate inference (2)\n\nWhere \\(1\\) According to the table below is in practice \\~1.2 slower than the vanilla network training. As for \\(2\\) although approximate inference methods methods are expensive, because in our method they are applied  in the small dimensional INR space in general it takes less time to evaluate.\n\n\nTime experiments (Resnet-18 on cifar100, Batch size=64)\n\n**Our method (main network + INR hypernetwork)**\n\n Forward 0.0069 \u00b1 0. 0001 (sec)\n\n Backward 0.0145 \u00b1 0.0084 (sec)\n\n\n**Vanilla Network**\n\n Forward 0.00463 \u00b1 0.00013 (sec)\n\n Backward 0.01115 \u00b1 0.00015 (sec)\n\n**Our method (main network + fixed \u03be perturbations without evaluating INR)**\n\n Forward 0.004585 \u00b1 0.00028 (sec)\n\n Backward 0.00901 \u00b1 0.00127 (sec)\n\n\nAs for the overhead in terms of learnable parameters is #W_{inr} (total number of the hypernetwork parameters) #AI_{inr} (number of approximate inference parameters applied on the INR space) which is as we mention in the main paper is in fact much less than #AI_{W} (number of approximate inference parameters applied on the full set of main network weights). \nPerformance wise our method is still be competitive w.r.t. methods like ensembles of D networks which at best is D times slower than the vanilla network.\n\nThus, we believe that our method could be applied to ImageNet. \nFurthermore, because the main overhead of our method is the hypernetwork evaluation we investigated the following alternative training scheme, to further improve our method in terms of time. \nInstead of training the main network weights W and W_INR together we update the W_INR parameters every 10 epochs of the main network training, this significantly reduces the computational overhead of our method and we hypothesize it can scale to IMAGENET models and datasets.\n\n**Training of ResNet18 CIFAR100**\n\n Full Training Accuracy: 69.01 LL: -2.32 Error: 0.3099 Brier: 0.5181 ECE: 0.222\n\n Alternative    Accuracy: 68.59 LL: -2.38 Error: 0.3141 Brier: 0.5211 ECE: 0.224\n\n\nBy updating the W_INR parameters every 10 epochs of the main network training has some (minor) effect in performance."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2362/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700234893429,
                "cdate": 1700234893429,
                "tmdate": 1700234893429,
                "mdate": 1700234893429,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JtnVvvvzKQ",
                "forum": "5KUiMKRebi",
                "replyto": "f4n0kumTmI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2362/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2362/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Additional experiments - ReLU vs Sine on hypernet"
                    },
                    "comment": {
                        "value": "We have completed a set of additional experiments, after discussion with the other reviewers. We copy them to you, as you might appreciate the extra empirical results.\n\nThe first set of results is about ablation on the hypernetwork:\n\n(Reviewer VEKs): *\u201cThe use of SIREN activation suggests that the hypernetworks need to model high frequency representations of the weight perturbations. The SIREN models presented in [Sitzmann et al., 2020] benefit from consistency and repeating patterns in the signals they are fitting, making interpolation easier. There might be some form of structural consistency in neighboring weight perturbations for CNN kernels, but it seems unlikely for the weights of linear layers. [..] This matter is only very briefly brought up in the discussion of Figures 9 and 10. [..] Have you tried using ReLU hypernetworks? Do these fail to recover high frequency representations of the perturbations compared to SIREN?\u201d*\n\nWe trained Resnet18 in both CIFAR10 and CIFAR100 for 100 epochs to evaluate the predictive capabilities of the Sinusoidal hyper-network:\n \n **CIFAR10**\n\n RELU_MAP Accuracy: 91.11 LL: -0.4891 Error: 0.088 Brier: 0.1484 ECE: 0.05891\n\n SINE MAP   Accuracy: 91.70 LL: -0.4449 Error: 0.083 Brier: 0.138 ECE: 0.05401\n\n **CIFAR100**\n\n RELU_MAP Accuracy: 67.79 LL: -2.544 Error: 0.3221 Brier: 0.537 ECE: 0.23232\n\n SINE MAP   Accuracy: 68.49 LL: -2.3990 Error: 0.3151 Brier: 0.527 ECE: 0.2256\n\nWe find that Sine/Periodic activations \u2013 the \u201cdefault\u201d choice in SIREN \u2013 slightly outperforms a hypernet with ReLU activations. Still, results are very close, though there is a trend in favor of sine in all benchmarks.\nThe original motivation behind using the sine activation is related to modeling high-frequency content, which translates as details in structured signals such as images or video [Sitzmann 2020]. We can however see this \u201cin the top of its head\u201d, so to speak: in structured signals we care *more* for low-frequency content, and high-frequency is a \u201cgood-to-have\u201d content. We can interpret an input semantically if we see its low frequencies, but not necessarily vice versa. For example, image compression will invariably throw away high frequencies first, and the last frequencies to lose will be the lower ones.\nOur conjecture is as follows: When using an INR to model *perturbations*, we are faced with a different situation, that corresponds to a different \u201cfrequency landscape\u201d (perhaps even different than the one of model *weights*). In particular, we do not have any reason to differentiate lower or higher frequency content in any respect. We \u201ccare\u201d for all frequencies, so we need to have a good way to model high frequencies as well. Perhaps this is the reason the sine activation gives a small edge over ReLU."
                    }
                },
                "number": 34,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2362/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700670554638,
                "cdate": 1700670554638,
                "tmdate": 1700670554638,
                "mdate": 1700670554638,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kDK1O902qF",
                "forum": "5KUiMKRebi",
                "replyto": "f4n0kumTmI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2362/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2362/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Full GGN in LA for UCI regression"
                    },
                    "comment": {
                        "value": "For our UCI regression experiments we added another strong baseline. For the small MLP network we use we were able to compute the full GGN matrix in the Laplace approximation of the main network. We copy the result in the form a figure (https://freeimage.host/i/Jnn6z6g)"
                    }
                },
                "number": 35,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2362/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700670728237,
                "cdate": 1700670728237,
                "tmdate": 1700670728237,
                "mdate": 1700670728237,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "49Vj57UMcp",
                "forum": "5KUiMKRebi",
                "replyto": "f4n0kumTmI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2362/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2362/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Ablation on INR size"
                    },
                    "comment": {
                        "value": "(Reviewer VEKs): *\u201cCan you further evaluate the effect of INR network size on model performance and quality of the uncertainty estimates? Figure 1 already does this for CIFAR-10 but it would be interesting to see for MLPs on UCI and UCI-gap datasets, with INR-Laplace and INR-RealNVP, and perhaps on wider sets of network sizes. Would also be helpful to include error bars for these figures.\u201d*\n\nWe added an ablation w.r.t. INR size following the UCI regression setting in our method. We compare 4 different versions of INR hypernetworks with an increasing number of parameters each,namelly (BIG=2500 MED=625, SMALL=75,XSMALL=10).\n\n(Please see Figure in link: https://freeimage.host/i/JnnsF8G)\n\nFrom the experiments we can observe that there is a limit to where someone can easily scale the INR hypernetwork and simultaneously gain performance. Individual characteristics play significant role to the INR size (main network size, dataset size, dataset dimension)\n As for the RealNVP, it was difficult to find a setting where the performance of INR-size can be isolated (different RealNVP sizes,  different training and VI hyperparameters etc)\n\nFollow up experiment ablation w.r.t. INR size following the UCI regression using SWAG approximate inference\n\n(Please see Figure in link: https://freeimage.host/i/uci-swag.JnYqx3v)"
                    }
                },
                "number": 36,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2362/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700670878448,
                "cdate": 1700670878448,
                "tmdate": 1700670957098,
                "mdate": 1700670957098,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "10U4c9lUD4",
                "forum": "5KUiMKRebi",
                "replyto": "f4n0kumTmI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2362/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2362/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Runtime comparison for inference"
                    },
                    "comment": {
                        "value": "*\u201cThank you for these numbers. You also say \"because [of the] the small dimensional INR space, in general, it takes less time to evaluate.\" Would be nice to also provide runtimes for inference. If I understood correctly, a forward pass of the hyper-network is required for every main network weight. Do you batch the weight coordinates when computing perturbations? Do you make a single pass with all coordinates or is this overly expensive?\u201d*\n\nYes, a forward pass of the hyper-network is required for every main network, in fact we created an algorithm for both inference and training of our method:\n\n**Algorithm 1** Training\n```\n Inputs: I (indices of main network weights), Net (main network), INR (INR hypernetwork), Dataset\n for number of epochs do\n\t for x,y in Dataset do\n\t\t \u03be = INR(I)\n\t\t y* = Net(x,\u03be)\n\t\t loss = (y,y*)\n\t\t update INR w.r.t loss\n\t\t update Net w.r.t loss\n\t end\n end\n```\n\n**Algorithm 2** Inference\n```\n Inputs: I (indices of main network weights), Net (main network), INR (INR hypernetwork), Test_set\n Ap.-In (Approximate inference method*) MC_samples (Number of Monte Carlo samples)\n for x in Test_set do\n\t for MC_samples do\n\t\t \u03bej = Ap.-In(INR, I)\n   \t y* = Net(x,\u03bej)\n\t end\n\t calculate y* statistics\n end\n```\n\n(In this setting a post training Monte Carlo-based approximate inference method is implied).\n\nAs for the weight coordinates, in practice these values are batched and computed separately for each layer (i.e. for the layer i-th layer indices/input-coordinates positions have the shape [#W, I_dims] where #W is the number of the total main network parameters of the i-th layer and I_dims is the dimensionality of the indices (ex. for convolutional main layer I_dims = 5, 4 positions for the kernel plus 1 dimension to act as a the layerwise position)).\n\nIn practice passing all the indices through the hypernetwork at once is more expensive than our approach. Furthermore because the INR is shared between all weights we could run the computations per layer in parallel as they don't depend on each other.\n\nWe included runtimes for inference: \n\nInference time for Resnet18 combined with different stochastic subspaces and different approximate inference methods (time is measured in seconds and for a batch of 10 CIFAR images). We included Inference time comparison for the same approximate inference method but for different \u2018subspaces\u2019. Specifically we compare our INR low dimensional space with: Rank-1 (Dusenberry et al. 2020); Wasserstein subnetwork (Daxberger et al. 2021) and partially stochastic ResNets from (Sharma, Mrinank, et al. 2023) we provide you with quantitative results at the the end of this thread.\n\n**For the Laplace approximate inference method**:\n\nSubnetwork: \t0.4211 \u00b1 0.024\n\nINR: \t\t0.5145 \u00b1 0.008\n\nRank1: \t0.5545 \u00b1 0.019\n\nPartially: \t0.4994 \u00b1 0.011\n\nAll: \t\t0.4989 \u00b1 0.003\n\n**For the SWAG approximate inference method**:\n\nSubnetwork: \t0.2917 \u00b1 0.0164\n\nINR: \t\t0.1149 \u00b1 0.0057\n\nRank1: \t0.2837 \u00b1 0.0027\n\nPartially: \t0.2813 \u00b1 0.0127\n\nAll: \t\t0.3235 \u00b1 0.0315\n\n**( Inference time for ResNet-50 on CIFAR datasets, cf. Figure 4 main paper)**:\n\nDeep Ensembles \t0.9014 \u00b1 0.0273\n\nDropout\t\t0.0372 \u00b1 0.0066\n\nLL Laplace \t\t2.0030 \u00b1 0.0073\n\nINR SWAG \t\t0.6393 \u00b1 0.0184\n\nINR RealNVP \t\t0.2045 \u00b1 0.0043\n\nFor the Deep Ensembles method the obtained values include additional overhead such as ensemble element loading e.t.c. as it is common practice. Furthermore the Linearized LL Laplace is much slower than the other methods as Computing the Jacobian for the ResNet50 reaches the limits of our computational budget at this time."
                    }
                },
                "number": 37,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2362/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700671253391,
                "cdate": 1700671253391,
                "tmdate": 1700671253391,
                "mdate": 1700671253391,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jz0iHO47YC",
                "forum": "5KUiMKRebi",
                "replyto": "f4n0kumTmI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2362/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2362/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "(Reviewer VEKs): *\u201cDo you think that maybe the main network weights in linear layers re-arrange themselves in each layer to benefit from structural consistency? For example, that weights with low pertubation variance tend to one side of the weight vector and those with high variance to the other? This could be facilitated, as you say, by the multiplicative nature of the perturbation and would also explain why ReLU hyper-networks are also successful. I realize that the hyper-network inputs are high dimensional (in particular for the CNN kernels), but an experiment that could demonstrate this would be to observe perturbation variance as a function of the input weight coordinate by taking 1D slices or 2D slices of the input space and seeing if this is highly jagged or is somewhat smooth in nature.\u201d*\n\nWe plotted the \u201cperturbation variance\u201d (the values of \u03be) as a function of input weight coordinates. Specifically for Resnet18 trained on CIFAR we plotted the flatten values for each specific kernel position across channels (channel slice) for 2 two different convolutional layers.\nBoth types of hypernetworks produce well structured perturbation functions.\nThe \u03be values produced from the sinusoidal hypernetwork are expressed as a somewhat oscillatory behavior w.r.t. channel position, which translates as higher frequency content. As for the ReLU perturbations, while having some high frequencies due to the discontinuity of the ReLU activation, the overall signal has a smooth structure less complicated that the sinusoidal ones in some cases. Unsurprisingly, the ReLU result consists of practically piecewise linear components. This is what we believe that highlights the *marginally* better performance of SIREN hypernetworks. \n\nThe plots can be examined here:\n\nhttps://freeimage.host/i/JnD7Pst\n\nand:\n\nhttps://freeimage.host/i/JnDYBmQ"
                    }
                },
                "number": 38,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2362/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700671320032,
                "cdate": 1700671320032,
                "tmdate": 1700671320032,
                "mdate": 1700671320032,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "whvi7qheFD",
                "forum": "5KUiMKRebi",
                "replyto": "f4n0kumTmI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2362/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2362/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We tried to measure the quality of proposed subspaces in terms of predictive uncertainty. Specifically we compare our INR low dimensional space with: Rank-1 (Dusenberry et al. 2020); Wasserstein subnetwork  (Daxberger et al. 2021) and partially stochastic ResNets from (Sharma, Mrinank, et al. partially). \nWe trained (each method) combined with a Resnet18 for 100 epochs in both Cifar10 and Cifar100 datasets while keeping the approximate inference method fixed same across all low dimensional spaces.\n\n* Sharma, Mrinank, et al. \"Do Bayesian Neural Networks Need To Be Fully Stochastic?.\" International Conference on Artificial Intelligence and Statistics. PMLR, 2023.\n\n **RANK1**\n\n SWAG - CIFAR10 In-Dist Test data Accuracy: 91.78 LL: -0.4187 Error: 0.082 Brier: 0.1343 ECE: 0.0522\n\n SWAG - CIFAR10 Corrupted Test data Accuracy: 77.80 LL: -1.2537 Error: 0.222 Brier: 0.3596 ECE: 0.1469\n\n Laplace - CIFAR10 In-Dist Test data Accuracy: 91.01 LL: -1.5630 Error: 0.090 Brier: 0.6872 ECE: 0.6871\n\n Laplace - CIFAR10 Corrupted Test data Accuracy: 78.07 LL: -1.7068 Error: 0.220 Brier: 0.7396 ECE: 0.5737\n\n-----------------------------------------------------------------------------------------------------------------------------------------------------\n\nSWAG -   CIFAR100 In-Dist Test data         Accuracy: 65.84 LL: -2.29 Error: 0.34 Brier: 0.55 ECE: 0.228\n\nSWAG -   CIFAR100 Corrupted Test data   Accuracy: 42.80 LL: -4.77 Error: 0.57 Brier: 0.92 ECE: 0.398\n\n Laplace - CIFAR100 In-Dist Test data        Accuracy: 69.00 LL: -4.01 Error: 0.31 Brier:  0.9714 ECE: 0.668\n\n Laplace - CIFAR100 Corrupted Test data   Accuracy: 42.00 LL: -4.25 Error: 0.58 Brier 0.979 ECE: 0.401\n\n **INR**\n\n SWAG -   CIFAR10 In-Dist Test data Accuracy: 92.17 LL: -0.384 Error: 0.078 Brier: 0.1296 ECE: 0.0498\n\n SWAG -   CIFAR10 Corrupted Test data Accuracy: 78.60 LL: -1.164 Error: 0.214 Brier: 0.3566 ECE: 0.1447\n\n Laplace - CIFAR10 In-Dist Test data Accuracy: 89.0 LL: -1.563 Error: 0.110 Brier:  0.6869 ECE: 0.6647\n\n Laplace - CIFAR10 Corrupted Test data Accuracy: 81.0 LL: -1.660 Error: 0.190 Brier: 0.7156 ECE: 0.5890\n\n------------------------------------------------------------------------------------------------------------------------------------------------------\n\n SWAG -   CIFAR100 In-Dist Test data Accuracy: 69.12 LL: -2.094 Error: 0.308 Brier: 0.5006 ECE: 0.2046\n\n SWAG -   CIFAR100 Corrupted Test data Accuracy: 46.5 LL: -4.1878 Error: 0.535 Brier: 0.8418 ECE: 0.3640\n\n Laplace - CIFAR100 In-Dist Test data Accuracy: 70.0 LL: -3.9172 Error: 0.300 Brier:  0.967 ECE: 0.6747\n\nLaplace - CIFAR100 Corrupted Test data   Accuracy: 42.0 LL: -4.199 Error: 0.58 Brier:0.9770 ECE: 0.396\n\n **SUBNETWORK**\n\n SWAG -   CIFAR10 In-Dist Test data Accuracy: 92.54 LL: -0.4251 Error: 0.074 Brier: 0.1255 ECE: 0.0491\n\n SWAG -   CIFAR10 Corrupted Test data Accuracy: 76.90 LL: -1.4599 Error: 0.231 Brier: 0.3845 ECE: 0.1732\n\n Laplace - CIFAR10 In-Dist Test data Accuracy: 91.0 LL: -1.551723 Error: 0.090 Brier: 0.682 ECE: 0.6823\n\n Laplace - CIFAR10 Corrupted Test data  Accuracy:  81.0 LL: -1.650211 Error: 0.190 Brier: 0.7134 ECE: 0.5886\n\n------------------------------------------------------------------------------------------------------------------------------------------------------\n\n SWAG -   CIFAR100 In-Dist Test data Accuracy: 69.86 LL: -2.1430 Error: 0.30 Brier: 0.49 ECE: 0.207\n\n SWAG -   CIFAR100 Corrupted Test data Accuracy: 49.0 LL: -3.9721 Error: 0.51 Brier: 0.82 ECE: 0.3483\n\n Laplace - CIFAR100 In-Dist Test data Accuracy: 68.0 LL: -3.9505 Error: 0.32 Brier: 0.9682 ECE: 0.655\n\n Laplace - CIFAR100 Corrupted Test data Accuracy: 49.0 LL: -4.1309 Error: 0.51 Brier: 0.974 ECE: 0.466\n\n **PARTIALLY STOCHASTIC**\n\n SWAG -   CIFAR10 In-Dist Test data Accuracy: 92.54 LL: -0.4251 Error: 0.074 Brier: 0.1255 ECE: 0.0491\n\n SWAG -   CIFAR10 Corrupted Test data Accuracy: 76.92 LL: -1.4499 Error: 0.201 Brier: 0.3806 ECE: 0.1730\n\n Laplace - CIFAR10 In-Dist Test data Accuracy: 90.8 LL: -1.561 Error: 0.091 Brier: 0.68 ECE: 0.702\n\n Laplace - CIFAR10 Corrupted Test data Accuracy: 80.0 LL: -1.67 Error: 0.21 Brier: 0.72 ECE: 0.59\n\n\n***thread continued.........***"
                    }
                },
                "number": 39,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2362/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700671365802,
                "cdate": 1700671365802,
                "tmdate": 1700671365802,
                "mdate": 1700671365802,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Kfh38BHwsO",
                "forum": "5KUiMKRebi",
                "replyto": "f4n0kumTmI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2362/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2362/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "----------------------------------------------------------------------------------------------------------------------------------------------------\n\n SWAG - CIFAR100 In-Dist Test data Accuracy: 69.85 LL: -2.1430 Error: 0.3015 Brier: 0.4997 ECE: 0.2078\n\n SWAG - CIFAR100 Corrupted Test data Accuracy: 49.0 LL: -3.9721 Error: 0.51 Brier: 0.8218 ECE: 0.3483\n\n Laplace - CIFAR100 In-Dist Test data Accuracy: 66.0 LL: -3.9903 Error: 0.34 Brier:  0.978 ECE: 0.6377\n\n Laplace - CIFAR100 Corrupted Test data Accuracy: 49.0 LL: -4.1815 Error: 0.51 Brier 0.989 ECE: 0.4709\n\n\\*For SWAG and Linearized Laplace with GGN, in order to be able to run across low dimensional spaces we choose the covariance to have Diagonal structure.\n\nAs for the results, we believe that in both datasets there is at the very least a trend in favor of both proposed INR-x methods. The results validate to a considerable degree the premise of the proposed methods: Instead of choosing a subset or subnet following the rationale of the corresponding methods, the INR produces \"\u03be\" outputs that endow the full network with the desirable stochasticity, while keeping the dimensionality of the random process that we want to do inference upon at a low level."
                    }
                },
                "number": 40,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2362/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700671411304,
                "cdate": 1700671411304,
                "tmdate": 1700671411304,
                "mdate": 1700671411304,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]