[
    {
        "title": "Exploring Memorization in Fine-tuned Language Models"
    },
    {
        "review": {
            "id": "wKfKfjBHrU",
            "forum": "sVs7lV691r",
            "replyto": "sVs7lV691r",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3829/Reviewer_wy1y"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3829/Reviewer_wy1y"
            ],
            "content": {
                "summary": {
                    "value": "The paper investigates to which extent language models (LMs) memorize their fine-tuning datasets, when fine-tuned for different tasks (summarization, medical dialog, QA, translation and sentiment classification). To detect memorization, the authors employ a plagiarism checker tool to detect whether models produce output that can be considered as one of three types of plagiarism (verbatim, paraphrase, idea) of text in the fine-tuning dataset, when prompted with prefixes from the same dataset.\nThe paper finds that the degree of memorization varies strongly based on the fine-tuning task, with summarization and dialog exhibiting much higher memorization than classification, translation and QA tasks. The paper further shows a correlation between attention distributions and the measured memorization scores, with high memorization corresponding to more uniform attention maps, and low memorization to more concentrated attention maps. Finally, the paper proposes to use multi-task fine-tuning to reduce memorization."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "- Understanding the memorization behavior of LMs is an important problem with ramifications for privacy and copyright considerations. Since fine-tuning plays a key role in creating usable LMs, the paper tackles an important open problem by investigating how this step impacts memorization.\n- Differences in the fine-tuning task can strongly influence model behavior, so investigating how different fine-tuning objectives influence memorization is important."
                },
                "weaknesses": {
                    "value": "- The paper has severe soundness issues, making the key findings rather unreliable.\n    1. The paper claims, that memorization varies based on the fine-tuning task, with summarization and dialog tasks exhibiting higher memorization than QA, translation and sentiment classification. It is very likely, however, that this result is confounded by the response lengths required for the different tasks. According to Figure 2, models generate in the order of > 120 output tokens for summarization and dialog tasks, but <= 6 output tokens for sentiment classification and QA tasks. Tasks that require long outputs are much more likely to produce sequences similar to those in the training corpus, just from a numerical perspective, and irrespective of memorization effects. The paper does not control for differences in output lengths of different tasks. Therefore, the results about differences in memorization behavior for different fine-tuning tasks are unreliable without controlling for this confounder.\n    2. The results on using attention maps as indicators of memorization appear similarly unreliable, because they also do not control for important potential confounders. Summarization and dialog (presumably high memorization tasks) are shown to have more uniform attention maps, compared to QA and sentiment classification, where attention is more concentrated. However, for the latter two tasks it may be sufficient to pay attention to a few keywords in the input, whereas summarization and dialog presumably require a more comprehensive understanding of the entire text. Therefore, the differences in attention patterns might simply be due to the nature of the different tasks, and not due to different degrees of memorization. For an apples to apples comparison, the authors should compare attention maps of instances with different degrees of memorization on the same task.\n    3. In Sections 4.2 and 5.2 the paper uses a model based on sparse coding to theoretically explain task-dependent differences in memorization and in the attention patterns, respectively. However, it is not clear that the sparse coding model is a meaningful approximation of the behavior of the investigated LMs. The sparse coding approach uses a simple linear model, and it is a priori questionable whether such a model is sufficient to approximate large and highly non-linear LMs. Sections 4.2 and 5.2 provide no empirical evidence showing that the sparse coding model is a meaningful approximation of the investigated LMs, so the theoretical arguments made in these sections do not appear to be meaningful.\n    4. The paper investigates memorization as a result of fine-tuning models. However, according to Figure 4, verbatim, idea and high-paraphrasation memorization appear to be in the same ballpark for T5-base and fine-tuned models. Therefore, it is not clear how much of the reported memorization is due to fine-tuning and how much is due to pre-training. To account for this, the authors should control for pre-training memorization in the memorization scores of fine-tuned models.\n- The paper has several clarity issues which make it difficult to understand some of the results.\n    1. The concept of idea memorization/plagiarism and how it is operationalized is not clearly defined. This makes it difficult to interpret the corresponding results.\n    2. How is fine-tuning for the different tasks performed? E.g. for sentiment classification, what is the target output ($y$) that the models are fine-tuned to predict? A single word/token, or a short sentence?\n    3. In Section 6, how do you use the base/non-fine-tuned models for summarization?\n    4. I have difficulty interpreting the memorization examples in Table 11 in Appendix D. Which prompts were used to generate the machine written text?\n- Contribution: The memorization detection approach and pipeline in the paper seems to be very similar to that of [A], upon which it builds. Given that the findings in the paper are unreliable (see above), the contribution is very marginal.\n\n[A] Lee et al., Do Language Models Plagiarize?, WWW'23"
                },
                "questions": {
                    "value": "1. Is there only one split of each input $x_i$ into prefix $p_i$ and suffix $s_i$? If yes, the memorization behavior of the model might differ, based on the length of prefixes $p_i$, so it might be worth testing memorization for different prefix lengths. If no, the memorization ratio definition seems difficult.\n2. What is the rationale for studying attention at the last decoder-encoder attention block in Section 5.1? Do the results look similar for other blocks in the network?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3829/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3829/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3829/Reviewer_wy1y"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3829/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698859277829,
            "cdate": 1698859277829,
            "tmdate": 1699636340770,
            "mdate": 1699636340770,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BWRJ4x3SgL",
                "forum": "sVs7lV691r",
                "replyto": "wKfKfjBHrU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3829/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3829/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer wy1y"
                    },
                    "comment": {
                        "value": "We are grateful for the insightful feedback and constructive critiques provided during the ICLR 2024 review process. We appreciate the opportunity to address the concerns raised and enhance the clarity and impact of our work. We have thoroughly addressed each point in our rebuttal and hope these improvements justify a reevaluation of our score.\n> **W1\uff1aThe paper does not control for differences in output lengths of different tasks. Therefore, the results about differences in memorization behavior for different fine-tuning tasks are unreliable without controlling for this confounder.**\n\n\n**Response:** Thanks for your comment. In our experiment, we did not standardize the output length; rather, **we provided the model with a sequence prefix and allowed it to autonomously determine the appropriate output length.** The generation process ceases upon encountering a designated stop symbol. It is accurate, as noted, that certain tasks\u2014such as summarization and dialogue\u2014tend to produce longer outputs in comparison to tasks like sentiment classification or question-answering.\n\n\nTo isolate the impact of the output lengh, we **augment the answer of sentiment** **classification task**. Specifically, we change the label \"Negative\" to \"According to the review, this audience gave negative feedback about the movie, indicating a general dissatisfaction with its overall quality and appeal. So the sentiment of this review is negative.\", the label \"possitive\" to \"According to the review, the audience shared positive feedback about this movie, highlighting its overall appeal and entertainment value. So the sentiment of this review is positive.\"(40 words) We call this task \"Sentiment-Aug\". In this case, the output lengh is comparable to tasks like summarization and dialog. **However, we can find that Sentiment-Aug still represents very low memorization ratio and is much lower than high-memorization tasks.** **This  underscores the nuanced relationship between output length and memorization, which is dependent on the intrinsic demands of the task at hand.**\n\n\n| Task| Dataset| Memorization Ratio | Verbatim | Idea | Paraphrase(p>0.5) | Paraphrase(p<0.5) |\n|-|-|-|-|-|-|-|\n| Summarization  | Multi_news| 22.33%| 4.23%| 0.65%| 6.23%| 11.22%|\n| Dialog| HealthCareMagic | 8.27%| 0.02%| 1.41%| 1.75%| 5.09%|\n| Sentiment| IMDB| 0.80%| 0.04%| 0.30%| 0.17%| 0.29%|\n| Sentiment-Aug | IMDB-Long| 1.17%| 0.03%| 0.34%| 0.23%| 0.66%|\n\n>  **W2: The results on using attention maps as indicators of memorization appear similarly unreliable because they also do not control for important potential confounders. Summarization and dialog (presumably high memorization tasks) are shown to have more uniform attention maps, compared to QA and sentiment classification, where attention is more concentrated. However, for the latter two tasks, it may be sufficient to pay attention to a few keywords in the input, whereas summarization and dialog presumably require a more comprehensive understanding of the entire text. Therefore, the differences in attention patterns might simply be due to the nature of the different tasks, and not due to different degrees of memorization. For an apples-to-apples comparison, the authors should compare attention maps of instances with different degrees of memorization on the same task.**\n\n**Response:**\n* Thank you for your insightful feedback. Indeed, as you have discerned, different tasks exhibit varying dependencies on specific informational elements, a characteristic intrinsic to the nature of the task itself. **Our findings corroborate your observation:**  \n    * Tasks that necessitate a comprehensive attention to detail, such as Summarization, demonstrate dense attention maps and a higher memorization ratio.\n    * Conversely, tasks like sentiment classification, which inherently require only a subset of the available information, show markedly lower memorization. \n    * **This suggests that the disparity in memorization across tasks is fundamentally linked to the volume of input information that the task demands for successful completion**.\n\n\n* Furthermore, in response to your suggestion, we conducted a comparative analysis of attention scores and memorization ratios **within the domain of summarization**. By examining T5 models fine-tuned on distinct datasets\u2014Multi-news and CNN_dail\u2014we discovered that the **Multi-news T5 model exhibits both a higher memorization ratio and a greater CAD. Conversely, the CNN_Daily T5 model displays a lower memorization ratio and reduced CAD.** These observations are in alignment with our conclusion, reinforcing the premise that the memorization disparity is attributable to the essential nature of the fine-tuned task.\n\n| Source mode | Task| Dataset| Memorization ratio | CAD|\n|-------------|---------------|------------|---------------------|------|\n| T5-base     | Summarization | CNN_Daily| 5.82%| 2.34 |\n| T5-base     | Summarization | Multi-news | 22.33%| 2.67 |"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3829/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700277553081,
                "cdate": 1700277553081,
                "tmdate": 1700277553081,
                "mdate": 1700277553081,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pWhQ0BiQNB",
                "forum": "sVs7lV691r",
                "replyto": "wKfKfjBHrU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3829/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3829/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer wy1y"
                    },
                    "comment": {
                        "value": "> **How is fine-tuning for the different tasks performed? E.g. for sentiment classification, what is the target output() that the models are fine-tuned to predict? A single word/token, or a short sentence?**\n\n**Response**: For sentiment classification, we use a single word, either positive or negative as output. During finetuning, we add a prefix for the input text: \u201cPlease classify the sentiment of the following paragraph: \u201d .For extractive QA, the output data which are the corresponding answer of the questions from the input data, is usually several words or a short sentence. We finetune this task in a sequence-to-sequence manner thus use the answer term as output. For other tasks like summarization, translation, and dialogue, we directly use the sentences presented in the original dataset as output. In our experiment, we consider all tasks as the format of generation tasks.(Like instruction tuning)\n\n> **In Section 6, how do you use the base/non-fine-tuned models for summarization?**\n\n**Response**: \nIn our experiment, the test procedure is the same for both base models and finetuned models. It means that we input the prefix $p$ of finetuning data  to models and compare the $f(p)$ with suffix $s$. For base model, we still input the finetuning data prefix for test, even if the model is not finetuned on the dataset.(So if the model remember the data means the data also presents in the pre-training set)\n> \n> **I have difficulty interpreting the memorization examples in Table 11 in Appendix D. Which prompts were used to generate the machine-written text?**\n\n**Response:** In our experiment to examine memorization, we don't have a task-specific prompt during the test time. The Machine-written text is the output of the model when we input the prefix of the original input. We follow the memorization inference setting from previous work[3].\n\n[3] Quantifying Memorization Across Neural Language Models\n\n> **Q1: Is there only one split of each input into prefixes and suffixes? If yes, the memorization behavior of the model might differ, based on the length of prefixes, so it might be worth testing memorization for different prefix lengths. If not, the memorization ratio definition seems difficult**.\n\n**Response:**\nThanks for your question, we conduct ablation studies on Dialog,Summarization, and Sentiment Classification with differnt prefixs k. and reported the result on Appendix B.3. And the results show that:\n* **The length of prefix tokens can affect memorization**. \n    * The length of prefix tokens does indeed impact memorization. Specifically, for summarization and Dialog tasks, the memorization ratio generally increases with the length of the prefix. This finding aligns with previous research on pretrained memorization[^2]. However, for sentiment classification, changing the prefix does not result in significant changes, and increasing the prefix length does not necessarily lead to an increase in the memorization ratio.\n* **The task disparity still exists when using different prefix lengths.**\n    * Furthermore, it is worth noting that despite the influence of different prefixes on memorization, there still exists a noticeable disparity in memorization across tasks. Therefore, our conclusion remains even using different prefixes.\n\n\n| Task| Dataset| Prefix length | Memorization Ratio | Verbatim | Idea  | Paraphrase (p>0.5) | Paraphrase (p<0.5) |\n|---------------|------------|---------------|---------------------|-----------|--------|----------------------|---------------------|\n| Summarization | Multi_news | 10| 12.25%| 1.74%| 2.85%  | 0.88%| 6.78%|\n| Summarization | Multi_news | 30| 20.68%| 7.07%| 1.41%  | 3.05%| 9.15%|\n| Summarization | Multi_news | 50| 22.33%| 4.23%| 0.65%| 6.23%| 11.22%|\n| Summarization | Multi_news | 100| 29.66%| 10.61%| 0.79%  | 4.27%| 13.99%|\n\n\n| Task   | Dataset| Prefix length | Memorization Ratio | Verbatim | Idea| Paraphrase (p>0.5) | Paraphrase (p<0.5) |\n|--------|----------------|---------------|---------------------|----------|-------|---------------------|--------------------|\n| Dialog | HealthCareMagic | 10| 6.28%| 0.03%| 1.94% | 0.85%| 3.46%|\n| Dialog | HealthCareMagic | 30| 7.76%| 0.04%| 1.28% | 1.72%| 4.72%|\n| Dialog | HealthCareMagic | 50| 8.27%| 0.02%| 1.41% | 1.75%| 5.09%|\n\n\n| Task| Dataset | Prefix length | Memorization Ratio | Verbatim | Idea  | Paraphrase (p>0.5) | Paraphrase (p<0.5) |\n|-----------|---------|---------------|---------------------|----------|-------|---------------------|--------------------|\n| Sentiment | IMDB    | 10| 1.37%| 0.00%| 1.12% | 0.06%| 0.19%|\n| Sentiment | IMDB    | 30| 1.18%| 0.01%| 0.51% | 0.15%| 0.51%|\n| Sentiment | IMDB    | 50| 0.80%| 0.04%| 0.30% | 0.17%| 0.29%|\n| Sentiment | IMDB    | 100| 1.39%| 0.05%| 0.23% | 0.33%| 0.78%|"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3829/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700280378973,
                "cdate": 1700280378973,
                "tmdate": 1700280463757,
                "mdate": 1700280463757,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OpXUjUWvKV",
                "forum": "sVs7lV691r",
                "replyto": "wKfKfjBHrU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3829/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3829/Authors"
                ],
                "content": {
                    "title": {
                        "value": "A friendly reminder"
                    },
                    "comment": {
                        "value": "We are grateful for the useful comments provided by you. We hope that our answers have addressed your concerns. If you have any further concerns, please let us know. We are looking forward to hearing from you."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3829/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700488954154,
                "cdate": 1700488954154,
                "tmdate": 1700488954154,
                "mdate": 1700488954154,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PUxJEhzKu4",
                "forum": "sVs7lV691r",
                "replyto": "wKfKfjBHrU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3829/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3829/Authors"
                ],
                "content": {
                    "title": {
                        "value": "A friendly reminder"
                    },
                    "comment": {
                        "value": "We appreciate your reviews. We hope that our responses have adequately addressed your concerns. As the deadline for open discussion nears, we kindly remind you to share any additional feedback you may have. We are keen to engage in further discussion."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3829/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700594271964,
                "cdate": 1700594271964,
                "tmdate": 1700594271964,
                "mdate": 1700594271964,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "no4MM8SAPc",
                "forum": "sVs7lV691r",
                "replyto": "wKfKfjBHrU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3829/Reviewer_wy1y"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3829/Reviewer_wy1y"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the extensive response and for responding to all of my questions and concerns!\nYour answers address a lot of the clarity issues I had with the work, and it's good to see the additional results you provide.\nMy main objections regarding soundness remain, however, so I will maintain my score.\n\nHere are my thoughts on the response:\n- W1: Sentiment-Aug task does not really address the problem of controlling for output length. The additional output tokens are just due to a fixed template that the model regurgitates, whereas in summarization and dialog the model produces potentially completely different token sequences depending on the input. Overall I'm not convinced that the notion of memorization based on whether sequences from the finetuning data are repeated is the right one for a task like sentiment classification. A more appropriate notion would seem to be based on detecting whether the assigned label strongly depends on certain substrings that also appeared in the finetuning data. It may be worth taking a look at work that proposed label memorization metrics for classification tasks, although it has been done in the vision domain and would have to be adapted to the NLP setting: https://arxiv.org/abs/1906.05271. I would consider developing separate approaches to memorization detection in tasks that require text generation vs classification or localization, since both the output formats as well as what constitutes memorization at a conceptual level differ significantly.\n- W2: Thank you for your clarification. The results shown are a good first step, but I think it is necessary to investigate whether the attention-memorization trend holds more broadly to infer a statistically significant correlation.\n- W4: It's good to see these additional numbers. I think the post-finetuning scores should be normalized by the pre-finetuning scores to make sure the measure is reporting the right results.\n- Q1: Thank you for running the additional analysis. It is interesting to see that different prefix lengths recall memorized text to different extents."
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3829/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700644563902,
                "cdate": 1700644563902,
                "tmdate": 1700644563902,
                "mdate": 1700644563902,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "sArK5aXiUO",
            "forum": "sVs7lV691r",
            "replyto": "sVs7lV691r",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3829/Reviewer_BVwv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3829/Reviewer_BVwv"
            ],
            "content": {
                "summary": {
                    "value": "1. The paper investigates the phenomenon of memorization within fine-tuned language models, specifically focusing on pre-trained T5 models applied to various downstream tasks, such as summarization, dialogue, QA, translation, and sentiment analysis.\n2. The primary goal of the study is to comprehend the variations in memorization across these diverse tasks and determine whether fine-tuned models exhibit task-specific memorization patterns.\n3. The paper introduces a categorization of memorization into three types: verbatim (exact memorization), paraphrase (memorization of alternative expressions), and idea (conceptual memorization).\n4. Additionally, the authors propose a novel method for estimating memorization using attention scores and demonstrate the possibility of reducing memorization through multitask fine-tuning."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. I like the idea of simplifying the concept of memorization by linking it to the extent of information required for a given task, which aligns with the sparse coding model.\n2. The paper offers a systematic analysis that spans various tasks, providing valuable insights into memorization patterns across these tasks and comparing memorization across different language models.\n3. The examination of attention scores and the presentation of encoded attention maps explains the initial claim well.\n4. The observation that multitask fine-tuning can lead to reduced memorization highlights the practical implications of the study for model training and application."
                },
                "weaknesses": {
                    "value": "1. Need for a more rigorous analysis of task specificity: The authors should consider potential confounding factors, especially the influence of the pre-training data used for the model, as this could impact memorization ratios. For instance, in the final section, the authors show that the T5 base model also shows a very high memorization ratio on the summarization task of multi-news which is 110 memorization ratio and the fine-tuned T5 model only goes to 222 which is twice more. However, the entire analysis of the paper before that was talking about how the summarization task requires more memorization because fine-tuning happened on that particular data set. I believe that a proper analysis of what was there in the pre-training data of a model and whether it confounds the memorization ratios is extremely important for such analysis and any conclusions about the task specificity or the nature of the amount of information required for the task. In fact, in the T5 base model that the authors chose, the entire C4 data set which it was trained on is totally public and the author should make such an analysis so that we can understand what is actually responsible.\n\n2. The rationale behind introducing attention discrimination as a metric is unclear. If it merely correlates with existing, computationally cheaper metrics, it may not be necessary to introduce an additional metric without a compelling justification.\n\n3. The experiment involving Flan T5, where the memorization ratio decreases after fine-tuning, lacks a clear explanation. The authors should provide a rationale for this unexpected result or revisit their analysis.\n\n4. To draw meaningful conclusions, it is essential for the authors to compare the memorization scores of the T5 base model before and after fine-tuning.\n\n5. The authors' choice of focusing on the last layers of encoder-decoder attention (as seen in Figure 2) appears somewhat arbitrary. An explanation for this choice and its potential impact on the correlation between attention scores should be provided.\n6. Expanding the analysis presented in Figure 4 to include tasks beyond summarization would enhance the comprehensiveness of the paper's findings.\n7. The concept of discriminating between different types of memorization: I believe idea memorization which encapsulates the fundamental sense of the initial information, should be a superset of all other forms of memorization. So, I do not understand this whole concept of categorizing memorization and what are we trying to achieve with this.\n8. Please use % and not %. the latter is very non standard for ML papers."
                },
                "questions": {
                    "value": "See weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3829/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699281095924,
            "cdate": 1699281095924,
            "tmdate": 1699636340662,
            "mdate": 1699636340662,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1CllkHTVOM",
                "forum": "sVs7lV691r",
                "replyto": "sArK5aXiUO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3829/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3829/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer BVwv"
                    },
                    "comment": {
                        "value": "Thank you for your valuable feedback and constructive comments in the review of our submission for ICLR 2024. We appreciate the opportunity to address the concerns raised and enhance the clarity and impact of our work. We believe that the revisions and additional experiments we propose, as detailed in this rebuttal, will significantly strengthen our paper. Therefore, we respectfully request the reviewers to reconsider the scoring in light of these improvements and our responses to the feedback provided.\n> Q1: Need for a more rigorous analysis of task specificity: The authors should consider potential confounding factors,**especially the influence of the pre-training data** used for the model, as this could impact memorization ratios. \n\n> Q4: To draw meaningful conclusions, the authors need to compare the memorization scores of the T5 base model before and after fine-tuning\n\n**Response:** Thanks for your valuable questions. we add experiment on Appendix B.1 to compare the memorization difference between T5-base and fine-tuned T5 to verify our conclusion. \n\n####  Memorization of T5-base\n\n| Task          | Dataset        | Memorization Ratio | Verbatim | Idea  | Paraphrase (p>0.5) | Paraphrase (p<0.5) |\n|---------------|----------------|---------------------|----------|-------|---------------------|--------------------|\n| Summarization | Multi_news     | 11.03%              | 3.58%    | 0.66% | 4.28%              | 5.46%              |\n| Dialog        | HealthCareMagic| 1.60%               | 0.03%    | 1.04% | 0.11%              | 0.42%              |\n| Sentiment     | IMDB           | 0.78%               | 0.05%    | 0.37% | 0.16%              | 0.20%              |\n| QA            | Squad          | 0.09%               | 0.02%    | 0.00% | 0.01%              | 0.05%              |\n| Translation   | WMT_16         | 0.00%               | 0.00%    | 0.00% | 0.00%              | 0.00%              |\n\n#### Memorization Difference after fine-tuning\n| Task          | Dataset          | Memorization Ratio difference |\n|---------------|------------------|--------------------------------|\n| Summarization | Multi_news       | 11.30%                         |\n| Dialog        | HealthCareMagic  | 6.67%                          |\n| Sentiment     | IMDB             | 0.2%                         |\n| QA            | Squad            | 0.06%                          |\n| Translation   | WMT_16           | 0.00%                          |\n\nAs you correctly mentioned, some of the fine-tuning data may also be present in the pre-training data, as T5 also shows some cases of memorization of the fine-tuning data. However, after fine-tuning, we can clearly see that the fine-tuned model's memorization for summarization and dialogues improves significantly. It is obvious that the model has memorized a large number of fine-tuning data samples during the fine-tuning stage. However, for other tasks such as sentiment classification, the memorization ratio does not change much, meaning that the model does not remember much information from the fine-tuned data.\n> Q2:  **The rationale behind introducing attention discrimination as a metric is unclear**\n\nThe reasons we introduce attention scores as an indicator are:\n* **Intrinsic Task Properties:** \n    * Attention scores inherently represent the unique characteristics of each task. Diverse tasks have varying requirements regarding the extent to which they necessitate focus on the entire set of presented information. These requirements are aptly mirrored in the attention heatmap patterns. \n    * We visualize the attention heatmap of the T5 base (non-finetuned) when we use the non-finetuned model to perform different tasks. We were surprised to find, in Appendix F.1 and Figure 5, that the patterns still differed significantly across tasks (high memory-intensive attention). It further verifies that attention distribution is an intrinsic property of the task itself. In other words, taking summary as an example, no matter what model we use (even the human brain) to complete the task, we need to pay attention to the details of the input to complete the task. If we train the model on this \"high memory\" task, the model tends to remember more.\n\n*  **Robustness of Attention Scores**: Compared to memorization ratios, attention scores are less susceptible to external variables such as model type, training duration, and hyperparameters. This robustness makes them a more reliable indicator of a task's intrinsic characteristics. \n*  **Practical Implications**: One use case might be: before fine-tuning a model for a specific task, the model builder could first test attention patterns for that task by inferring other models (e.g., pre-trained models). Based on the attention pattern, the model builder can know whether the training data will be easily remembered if we fine-tune it for this task."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3829/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700275145074,
                "cdate": 1700275145074,
                "tmdate": 1700275145074,
                "mdate": 1700275145074,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "s7qTjRrKbK",
                "forum": "sVs7lV691r",
                "replyto": "sArK5aXiUO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3829/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3829/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer BVwv"
                    },
                    "comment": {
                        "value": "> Q3:**The experiment involving Flan T5, where the memorization ratio decreases after fine-tuning, lacks a clear explanation. The authors should provide a rationale for this unexpected result or revisit their analysis.**\n> \nIn our study, we conducted memorization tests on the instruction-tuning set of Flan-T5, which encompasses a variety of tasks. Our findings reveal a notable decrease in the memorization rate when compared to models fine-tuned on single tasks. A plausible explanation for this observation is the enhanced generalization capability of the model in a multi-task setting. Exposure to a broader array of tasks and data appears to facilitate this improved generalization[1]. Consequently, the model's ability to execute tasks is not solely dependent on memorization but also on its refined generalization skills. This explanation has been incorporated into our manuscript to elucidate the observed decrease in memorization rates. Nevertheless, we acknowledge that the underlying causes of this intriguing phenomenon are likely complex and multifaceted and requires further investigation.\n\n> Q5. **The authors' choice of focusing on the last layers of encoder-decoder attention (as seen in Figure 2) appears somewhat arbitrary. An explanation for this choice and its potential impact on the correlation between attention scores should be provided.**\n> \n\nIn our T5 architecture study, we examined three types of attention layers: encoder, decoder, and encoder-decoder attention layers. Our focus on encoder-decoder attention layers stems from their ability to capture the attention distribution across input features for each output, unlike encoder and decoder attention layers, which track internal attention within inputs and outputs, respectively.\n\nThis choice is based on the hypothesis that memorization behavior differences are linked to the specific information requirements for task completion. Encoder-decoder attention scores are thus more aligned with our research objectives.\n\n**We observed consistent patterns across various layers of the encoder-decoder attention mechanism**, with high memorization tasks showing dense attention and low memorization tasks focusing attention on fewer positions.(And we add the attention maps of other encoder-decoder layers in Appendix-F.2) Due to this consistency, we report primarily on the final layer, closest to the output, for clarity and relevance. Detailed attention maps across different layers are now available in the Appendix-F.2 for further reference.\n\n> Q7. **The concept of discriminating between different types of memorization: I believe idea memorization which encapsulates the fundamental sense of the initial information, should be a superset of all other forms of memorization. So, I do not understand this whole concept of categorizing memorization and what are we trying to achieve with this**.\n> \nThank you for the question. We  want to clarify that these three types of memorization cases are non-overlapping and idea memorization is not a superset of others. Here we will distinguish 3 types of memorization. First, verbatim memorization means exact copies of words or phrases without transformation. In the cases of paraphrase and idea memorization, the output are not identical to the original text but share similar meanings.  **While paraphrase plagiarism targets sentenceto-sentence transformations, idea plagiarism reads a chunk of the content and condenses its main information into fewer sentences(or vice versa)**.  It is crucial to report and distinguish these types of memorization, as each poses unique implications for privacy concerns. By categorizing and examining these memorization behaviors, we aim to deepen the understanding of language models' capacity for retaining and reproducing information.\n```\nExamples:\nVerbatim:\nText A: My name is Jack\nText B: My name is Jack\n\nParaphrase: \nText A: My name is Jack\nText B: Jack is my name \n\nIdea plagirism:\nText A\uff1a A boy tell me in the class that his name is Jack\nText B: A boy is Jack\n```\n\nIn practice of the PAN2014-detection, It starts by identifying closely matched short document fragments (referred to as 'seeds') and then expands these seeds into longer text segments. This is achieved by clustering these fragments based on their separation and employing a 'maxgap' threshold parameter to form coherent clusters.They experimentally find out the most suitable threshold for different plagiarism datasets so that those parameters could be used for detection of a specific type of memorization. In another word, each memorization cases will be count only once and there will not be overlapping across different catagories. \n\n\n\n\n> Q8. **Please use % and not %. the latter is very non-standard for ML papers.**\n\nThanks for your comment. We have changed the symbol in the revision.\n\n[1]: Scaling Instruction-Finetuned Language Models"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3829/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700275587593,
                "cdate": 1700275587593,
                "tmdate": 1700278452099,
                "mdate": 1700278452099,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tO0KNeFRdR",
                "forum": "sVs7lV691r",
                "replyto": "sArK5aXiUO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3829/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3829/Authors"
                ],
                "content": {
                    "title": {
                        "value": "A friendly reminder"
                    },
                    "comment": {
                        "value": "We are grateful for the useful comments provided by you. We hope that our answers have addressed your concerns. If you have any further concerns, please let us know. We are looking forward to hearing from you."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3829/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700488920443,
                "cdate": 1700488920443,
                "tmdate": 1700488920443,
                "mdate": 1700488920443,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Kq5McRm7ci",
                "forum": "sVs7lV691r",
                "replyto": "sArK5aXiUO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3829/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3829/Authors"
                ],
                "content": {
                    "title": {
                        "value": "A friendly reminder"
                    },
                    "comment": {
                        "value": "We appreciate your reviews. We hope that our responses have adequately addressed your concerns. As the deadline for open discussion nears, we kindly remind you to share any additional feedback you may have. We are keen to engage in further discussion."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3829/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700594223659,
                "cdate": 1700594223659,
                "tmdate": 1700594223659,
                "mdate": 1700594223659,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "dGTDXzZJYT",
            "forum": "sVs7lV691r",
            "replyto": "sVs7lV691r",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3829/Reviewer_gHsj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3829/Reviewer_gHsj"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies memorization in fine-tuned language models. The authors observe a higher degree of memorization for models trained on summarization tasks, compared to simpler tasks like sentiment classification. Furthermore, they relate entropy in attention patterns to the degree of memorization in fine-tuned language models. Finally, they show that multi-task training reduces some memorization. They conclude their observations with a short theory on sparse coding models."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The main strength of the paper lies in its simplistic approach to studying memorization in fine-tuned language models. The authors take insights from existing memorization works on pre-trained models to conduct their analysis. The differences between different tasks showcase different mechanisms that models employ for each of them. Furthermore, the short theory on the sparse coding model helps formalize a reader's intuition for the observations. Overall, I believe the paper is a positive contribution to the community."
                },
                "weaknesses": {
                    "value": "I have a couple of questions about the experimental setup and the observations that the authors draw from their results.\n\n(a) What does x% memorization mean in the experiments? It would be great to demonstrate perfect and no memorization baselines to get a sense of the numbers.\n\n(b) How do the authors measure Idea memorization?  Furthermore, how do they differentiate Idea memorization from summarization (which is the task of a summarization-tuned model)? \n\n(c) How much do hyperparameters during fine-tuning affect the memorization results? Does a lower learning rate and longer training time result in more memorization? \n\n(d) \"k\", the length of the prefix tokens, was fixed for all the experiments. How much do the observations vary with varying \"k\"?\n\n(e) What is the decoding algorithm used for generation? Is it greedy decoding? If so, will the observations change with a more nuanced decoding algorithm, like nucleus sampling? This might allow the model to change its generation output, which will reduce the frequency of the generation of perfectly memorized solutions.\n\nOverall, I believe the paper is a positive contribution to the community. I am happy to interact further with the authors during the rebuttal period."
                },
                "questions": {
                    "value": "Please check my questions in the previous section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3829/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699577803652,
            "cdate": 1699577803652,
            "tmdate": 1699636340584,
            "mdate": 1699636340584,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "20C7djF5Fl",
                "forum": "sVs7lV691r",
                "replyto": "dGTDXzZJYT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3829/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3829/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer gHsj"
                    },
                    "comment": {
                        "value": "Thank you for your valuable feedback and constructive comments in the review of our submission for ICLR 2024. We appreciate your positive feedback and the opportunity to address the concerns raised and enhance the clarity and impact of our work. We believe that the revisions and additional experiments we propose(as detailed in this rebuttal) will significantly strengthen our paper . We hope this rebuttal demonstrates the advances and merits a higher score.\n\n> **(a) What does x% memorization mean in the experiments? It would be great to demonstrate perfect and no memorization baselines to get a sense of the numbers.**\n> \n**Response:** In our experiment, we input prefix $p_{i}$ to the model and compare $f(p_{i})$ with the all suffixes $\\{s\\}$ to identify whether it constitutes memorization. A memorization rate of x\\% indicates that an equivalent proportion of the prefixes induces the model to produce memorized content. A rate of 0\\% implies no memorized content is produced, whereas a rate of 100\\% indicates that all prefixes lead to memorized output. This metric is essential for assessing the model's propensity to replicate known information.\n\n> **(b) How do the authors measure Idea memorization? How do they differentiate Idea memorization from summarization?**\n\n**Response:** In our research, although summarization and idea memorization share similarities, they actually refer to different concepts. Summarization is a task that the model conducts. The user inputs x(e.g. news), and the model should output the summarization of the input(e.g. The title). However, the idea memorization in our paper means a different thing. We input the part of information of x(x_prefix) to the model, if the model outputs some key information of the remaining part of x(x_suffix), we regard it as Idea memorization. E.g. We input the beginning of the news and the model tells us information of the remaining part of the news.\n\n> **(d) How much do the observations vary with varying \"k\"?**\n\n**Response:** Thanks for your question, we conducted ablation studies on dialog, summarization, and sentiment classification with different prefixes k and reported the result on Appendix B.3. And the results show that:\n* **The length of prefix tokens can affect memorization**. \n    * The length of prefix tokens does indeed impact memorization. Specifically, for summarization and Dialog tasks, the memorization ratio generally increases with the length of the prefix. This finding aligns with previous research on pre-trained memorization[1]. However, for sentiment classification, changing the prefix does not result in significant changes, and increasing the prefix length does not necessarily lead to an increase in the memorization ratio.\n\n[1] Quantifying Memorization Across Neural Language Models\n* **The task disparity still exists when using different prefixes.**\n    * Furthermore, it is worth noting that despite the influence of different prefixes on memorization, there still exists a noticeable disparity in memorization across tasks. Therefore, our conclusion remains even using different prefixes.\n\n\n| Task          | Dataset    | Prefix length | Memorization Ratio | Verbatim | Idea  | Paraphrase (p>0.5) | Paraphrase (p<0.5) |\n|---------------|------------|---------------|---------------------|-----------|--------|----------------------|---------------------|\n| Summarization | Multi_news | 10| 12.25%| 1.74%| 2.85%| 0.88%| 6.78%|\n| Summarization | Multi_news | 30| 20.68%| 7.07%| 1.41%| 3.05%| 9.15%|\n| Summarization | Multi_news | 50| 22.33%| 4.23%| 0.65%  | 6.23%| 11.22%|\n| Summarization | Multi_news | 100| 29.66%| 10.61%| 0.79%| 4.27%| 13.99%|\n\n\n| Task   | Dataset        | Prefix length | Memorization Ratio | Verbatim | Idea  | Paraphrase (p>0.5) | Paraphrase (p<0.5) |\n|--------|----------------|---------------|---------------------|----------|-------|---------------------|--------------------|\n| Dialog | HealthCareMagic | 10| 6.28%| 0.03%    | 1.94% | 0.85%              | 3.46%              |\n| Dialog | HealthCareMagic | 30| 7.76%| 0.04%    | 1.28% | 1.72%| 4.72%|\n| Dialog | HealthCareMagic | 50| 8.27%              | 0.02%    | 1.41% | 1.75%              | 5.09%              |\n\n\n| Task      | Dataset | Prefix length | Memorization Ratio | Verbatim | Idea  | Paraphrase (p>0.5) | Paraphrase (p<0.5) |\n|-----------|---------|---------------|---------------------|----------|-------|---------------------|--------------------|\n| Sentiment | IMDB    | 10| 1.37%| 0.00%    | 1.12% | 0.06%| 0.19%|\n| Sentiment | IMDB    | 30| 1.18%| 0.01%    | 0.51% | 0.15%              | 0.51%              |\n| Sentiment | IMDB    | 50            | 0.80%              | 0.04%    | 0.30% | 0.17%              | 0.29%              |\n| Sentiment | IMDB    | 100           | 1.39%              | 0.05%    | 0.23% | 0.33%              | 0.78%              |"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3829/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700287972220,
                "cdate": 1700287972220,
                "tmdate": 1700287972220,
                "mdate": 1700287972220,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Jn79fdqJWJ",
                "forum": "sVs7lV691r",
                "replyto": "dGTDXzZJYT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3829/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3829/Authors"
                ],
                "content": {
                    "title": {
                        "value": "A friendly reminder"
                    },
                    "comment": {
                        "value": "We are grateful for the useful comments provided by you. We hope that our answers have addressed your concerns. If you have any further concerns, please let us know. We are looking forward to hearing from you."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3829/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700488878665,
                "cdate": 1700488878665,
                "tmdate": 1700488878665,
                "mdate": 1700488878665,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JKydTPtVzp",
                "forum": "sVs7lV691r",
                "replyto": "Jn79fdqJWJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3829/Reviewer_gHsj"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3829/Reviewer_gHsj"
                ],
                "content": {
                    "title": {
                        "value": "Response to the authors"
                    },
                    "comment": {
                        "value": "Thank you for such a detailed response.\n\nA fundamental question that can arise is \"Why is it a bad thing for the model to memorize for summarization and dialog tasks? How can you quantify the difference between summarization and memorization? E.g. in your example, how can you quantify if the model memorizes or summarizes a given input?\".\n\nI had asked about memorization definition before because there were numbers (still present e.g. in section 3.2) like 207% and 196% in the paper. I think they are typographical issues. \n\nI believe the paper is interesting. Anyway, I maintain my score (for now) and will discuss it with my fellow reviewers during the discussion period."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3829/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700577862680,
                "cdate": 1700577862680,
                "tmdate": 1700577862680,
                "mdate": 1700577862680,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]