[
    {
        "title": "Query-Dependent Prompt Evaluation and Optimization with Offline Inverse RL"
    },
    {
        "review": {
            "id": "hOq4oHQBrF",
            "forum": "N6o0ZtPzTg",
            "replyto": "N6o0ZtPzTg",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission993/Reviewer_xV9W"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission993/Reviewer_xV9W"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes an \u201coffline\u201d (not using LLM as evaluators or prompt searchers) method to search effective \u201cquery-level\u201d prompts. This method can potentially reduce the cost of prompt searching for enhancing the arithmetic reasoning ability of LLMs."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "In the \u201coffline\u201d setting, this paper conducts relatively thorough experiments and ablations to find the optimal method."
                },
                "weaknesses": {
                    "value": "However, my main concern is that it lacks a very important comparison with \u201con-line\u201d methods. If the proposed method can achieve similar (or just a little bit lower) performance compared with \u201con-line\u201d methods and it is significantly cheaper, it would be a good evidence that the proposed method can potentially be useful practically."
                },
                "questions": {
                    "value": "See weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission993/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission993/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission993/Reviewer_xV9W"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission993/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698684627875,
            "cdate": 1698684627875,
            "tmdate": 1700672552455,
            "mdate": 1700672552455,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "bjgX6qYrEV",
                "forum": "N6o0ZtPzTg",
                "replyto": "hOq4oHQBrF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission993/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission993/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer xV9W"
                    },
                    "comment": {
                        "value": "We deeply appreciate your time and effort devoted to reviewing our paper. And we are happy to have your acknowledgment of the significance and soundness of our work. Your insightful comments on the relationship between Prompt-OIRL and the online methods remind us to better present and contrast those approaches in our revision, and we have added a corresponding discussion section to our revised manuscript in Appendix D (New).**\n\n---\n\n In the following, we would start by noting that the online methods are the foundations for our query-dependent offline approach, and **additional prompts discovered by online algorithms can improve the performance of Prompt-OIRL.**\n\n1. Additional prompt generated by the online method improves Prompt-OIRL\u2019s performance upper bound.\n\nAs the number of available prompts increases, the best-of-n performance among those prompts will be monotonically better. In addition to the evidence we have shown in the paper (Figure 6), the same conclusion can be drawn by comparing the performance upper bound of Prompt-OIRL on different tasks. \n\nIn the table below, we use UB-Train-6 to denote the best performance (performance Upper-Bound) that can be achieved using the training prompts on the test queries, and use UB-Test-10 to denote the best performance that can be achieved using the 10 test prompts. Finally, we use UB-16 to denote the best performance that can be achieved using all those 16 prompts.\n\n| Dataset | LLM        | UB-Train-6 | UB-Test-10 | UB-16 |\n|---------|------------|--------------------------|-------------------------|--------------------------|\n| MAWPS   | GPT3.5-turbo | 0.942                | 0.941                  | 0.957                   |\n| SVAMP   | GPT3.5-turbo | 0.882                | 0.869                  | 0.903                   |\n| GSM8K   | GPT3.5-turbo | 0.872                 | 0.873                   | 0.914                    |\n| MAWPS   | LLaMA2-7B   | 0.891                 | 0.855                  | 0.926                   |\n| SVAMP   | LLaMA2-7B   | 0.876                 | 0.837                  | 0.907                   |\n| GSM8K   | LLaMA2-7B   | 0.582                  | 0.501                   | 0.648                    |\n| MAWPS   | TigerBot-13B | 0.929                | 0.642                  | 0.935                   |\n| SVAMP   | TigerBot-13B | 0.928                 | 0.913                 | 0.954                   |\n| GSM8K   | TigerBot-13B | 0.754                 | 0.7                     | 0.822                    |\n\nFrom the results, we can observe that the best performance can be achieved by the test prompts are usually worse than the best of the training prompts \u2014 this is not surprising given the fact that those training prompts are generated and optimized by experts or online algorithms. That said, we can also observe that selecting from the combination of all those 16 prompts can always lead to a significant improvement. Such an observation verifies the idea that adding additional prompts that are generated by the online prompts, either to the training set or to the test set, can be beneficial to Prompt-OIRL.\n\n2. Monotonic Performance Improvement with the Number of Prompts.\n\nTo add additional empirical evidence to our discussion, we can observe from Figure 6 in the main text that all methods, including all baseline methods and our proposed method, can have improved performance when more available prompts are available.\n\nThe table below contextualizes the averaged performance improvement over different LLMs and tasks. \nWe use \u201cImprov. w/ +train\u201d to highlight the improvement achieved using 6 training prompts as compared to only using 1 prompt. We use \u201cImprov. w/ +test\u201d to highlight the improvement achieved using additional test prompts as compared to only selecting from the training prompts (i.e., the improvement of Prompt-OIRL over BoTr Eqn(2)). \n\n| Method      | K = 1    | K = 2    | K = 3    | K = 4    | K = 5    | K = 6    | Improv. w/ +train |\n|-------------|----------|----------|----------|----------|----------|----------|--------------------|\n| LLM-Conf.   | 0.4751   | 0.5184   | 0.5427   | 0.5543   | 0.5594   | 0.5601   | + 17.89%           |\n| BoTr Eqn(1) | 0.4751   | 0.5379   | 0.5654   | 0.5800   | 0.5893   | 0.5944   | + 25.11%           |\n| BoTr Eqn(2) | 0.4751   | 0.5553   | 0.5910   | 0.6130   | 0.6295   | 0.6393   | + 34.56%           |\n| Prompt-OIRL | 0.5904   | 0.6095   | 0.6225   | 0.6331   | 0.6410   | 0.6459   | + 9.4%             |\n| Improv. w/ +test | +24.27% | +9.76% | +5.33% | +3.23% | +1.83% | +1.03% |          -          |\n\n**The monotonic improvement in all those comparisons demonstrates the benefit of integrating additional prompts, which can either be discovered through an existing online algorithm or crafted by human experts.**\n\n---\n\nOnce again, we thank the reviewer for their effort in improving our work. If there should be any remaining concerns or questions, we are keen to do our utmost to address them."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission993/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700057672291,
                "cdate": 1700057672291,
                "tmdate": 1700057672291,
                "mdate": 1700057672291,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5gLfgj9Yxl",
                "forum": "N6o0ZtPzTg",
                "replyto": "hOq4oHQBrF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission993/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission993/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Further Discussions and Feedback Welcome"
                    },
                    "comment": {
                        "value": "We deeply appreciate the insights you've shared during the review process. Following our revisions and previous responses, we are genuinely curious if we have adequately addressed the concerns you raised.\n\nWe would appreciate it if you could kindly let us know if there were any further questions. In the extremely limited time remaining, we are still eager to do our utmost to address them!"
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission993/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700594493631,
                "cdate": 1700594493631,
                "tmdate": 1700594493631,
                "mdate": 1700594493631,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wyuRTmb2p1",
                "forum": "N6o0ZtPzTg",
                "replyto": "hOq4oHQBrF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission993/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission993/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Summarization of Feedback and Author Response"
                    },
                    "comment": {
                        "value": "We extend our gratitude to the reviewer for their insightful feedback. As we approach the conclusion of the discussion period and have yet to receive further comments, we would like to summarize the initial feedback from the reviewer and our subsequent responses.\n\n---\n\nIn their initial review, the reviewer acknowledged the soundness of our contribution by conducting a thorough empirical study and detailed ablations in an offline setting. They recognized the effectiveness of our approach in utilizing query-level prompts to reduce costs and improve the arithmetic reasoning capabilities of LLMs. They raised a concern regarding the **connection between Prompt-OIRL and the online prompt-generation methods.**\n\nTo address the reviewer\u2019s concern about the link with the online methods, \n\n1. Analytically, \nWe emphasized that **Prompt-OIRL is not in direct competition with online methods but rather builds upon their generated knowledge**. This approach is akin to standing on the shoulders of giants; it improves upon the existing prompts by considering query dependency in test time, whether they are generated by online algorithms or domain experts. **This is evident in Figure 6, where an increase in the number of prompts correlates with consistent improvements in performance.** This demonstrates **Prompt-OIRL's capacity to draw valuable insights from online methods and exhibit monotonically better results.**\n\n2. Synthetically, in our previous response, we posited that assuming an oracle reward model, the **incorporation of more prompts at test time consistently enhances the performance upper bound**. This holds true even if the additional prompts sourced from online methods are suboptimal. Thus, our study highlights the **importance of diverse prompting strategies**. This concept echoes our initial motivating example where different queries benefit from varying prompts, a finding supported in the concurrent literature [OPRO].\n\n3. Empirically, we underpinned our reasoning with experimental evidence, demonstrating that:\n  - (1). Increasing the number of prompts sourced from online algorithms **during training consistently enhances the performance of Prompt-OIRL** and other baseline methods. This improvement **underscores the value of enriched demonstration data derived from online algorithms**.\n  - (2).  Increasing the number of prompts sourced from online algorithms **during testing leads to consistent improvements in query-dependent prompt optimization**. This is clearly observed when comparing the outcomes of Prompt-OIRL \u2014 which selects the most effective prompt from both training and testing sets using its learned reward model \u2014 against BoTr Eqn(2), which limits its selection to training prompts. The **performance gains observed in Prompt-OIRL affirm the benefits of integrating additional prompts from online sources.**\n\n---\n\nWe trust that the aforementioned clarifications sufficiently address the reviewer's concern. We are grateful for the opportunity to elaborate on these aspects and to underscore the importance of our findings in contributing to the field.\n\n\n**We would greatly appreciate any additional feedback from the reviewer on ways to further enhance the quality of our manuscript. Despite the extremely limited time remaining, we remain committed to doing our utmost to address any additional concerns or suggestions the reviewer may have.**\n\n---\n**_Reference_**\n\n[OPRO]  Large Language Models as Optimizers (OPRO) Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V. Le, Denny Zhou, Xinyun Chen arXiv 2023."
                    }
                },
                "number": 24,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission993/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700664971493,
                "cdate": 1700664971493,
                "tmdate": 1700664971493,
                "mdate": 1700664971493,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ScibuVp0rH",
            "forum": "N6o0ZtPzTg",
            "replyto": "N6o0ZtPzTg",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission993/Reviewer_QVHX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission993/Reviewer_QVHX"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a new method called Prompt-OIRL to improve the arithmetic reasoning abilities of large language models (LLMs) through zero-shot prompt optimization. They identify query dependency as an important objective in prompt optimization, but current techniques face two key challenges: 1) the lack of effective methods to evaluate prompts without access to the golden answers, and 2) the high computational cost of exploring the vast prompt space through interactions with the LLM. To address these issues, Prompt-OIRL utilizes offline inverse reinforcement learning on existing prompting benchmark data to learn a prompt evaluation model without needing the LLM. This model can then efficiently recommend optimal prompts in a best-of-N fashion for new queries. Experiments across various LLM scales and math datasets demonstrate Prompt-OIRL's efficacy and cost-effectiveness for zero-shot prompt optimization with query dependency."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper is well-structured and easy to follow \n2. The idea is very interesting and the topic is important for automatic prompt engineering \n3. Query-dependent evaluation is an essential challenge that is typically ignored. The authors identify this unique issue and solve it by proposing an RL framework, which looks very promising. \n4. The experiments are strong enough to support the claims by comparing them with multiple SOTA baselines."
                },
                "weaknesses": {
                    "value": "1.  While the method looks promising, I still expect to see potential discussion about the limitations, e.g., stability of inverse RL\uff1f \n2. The scope is limited by arithmetic reasoning but the title seems a more generic framework that can be used to solve more broader tasks across different NLP tasks. \n3. What's the current bottleneck if the proposed framework is applied to other instruction prompt optimization tasks, listed in ORPO[1], APE[2], and APO[3] baseline methods? \n4. Without query dependence, what's the performance drop? Can you prove the necessity of that\uff1f\n5. What's the current computational cost of the proposed framework\uff1f  \n6. Is that possible to compare with the GPT-4 or PaLM 2 model as well\uff1f \n\n\n[1] Large Language Models as Optimizers (OPRO)\nChengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V. Le, Denny Zhou, Xinyun Chen\narXiv 2023. \n\n[2] Automatic Prompt Optimization with \"Gradient Descent\" and Beam Search (APO)\nReid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chenguang Zhu, Michael Zeng\nEMNLP 2023. \n\n[3] Large Language Models Are Human-Level Prompt Engineers (APE)\nYongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, Jimmy Ba\nICLR 2023."
                },
                "questions": {
                    "value": "see weakness section"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission993/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698992217235,
            "cdate": 1698992217235,
            "tmdate": 1699636025501,
            "mdate": 1699636025501,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "UCE0RyAyNf",
                "forum": "N6o0ZtPzTg",
                "replyto": "ScibuVp0rH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission993/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission993/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer QVHX (1/3)"
                    },
                    "comment": {
                        "value": "**We thank the reviewer for their insightful comments, and their acknowledgement of the soundness, presentation, and contribution of our paper. We aim to address all the individual points in your review here, but please also see the revised manuscript for changes (highlighted in red).**\n\n---\n\n### Q1. While the method looks promising, I still expect to see potential discussion about the limitations, e.g., stability of inverse RL\uff1f\n\n### A1.\nThank you for raising the concern about stability and robustness. In general, our offline-IRL approach enjoys high stability since the reward modeling step is based on gradient boosting methods and optimized through supervised learning. Similar to other offline-RL literature, learning with a fixed dataset also avoids the dilemma of exploration-exploitation trade-off which is usually challenging in the online approach. (e.g., in OPRO, APE, and APO [1-3]). In our work, we applied xgboost with a universal hyper-parameter setting to demonstrate the robustness of our approach.\n\nA temporary challenge for Prompt-OIRL\u2019s wide application lies in the availability of open-accessible data. We will elaborate on this point with more details in A3 below.\n\n---\n\n### Q2. The scope is limited by arithmetic reasoning but the title seems a more generic framework that can be used to solve more broader tasks across different NLP tasks.\n\n### A2. \nThank you for the suggestion. To avoid potential misunderstanding, we have updated our title with a subtitle in our revision\nto better highlight the use case studied in our work. The updated title is \n> *Query-Dependent Prompt Evaluation and Optimization with Offline Inverse RL\n\u2014 A Case Study on Arithmetic Reasoning*"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission993/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700056848069,
                "cdate": 1700056848069,
                "tmdate": 1700056848069,
                "mdate": 1700056848069,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jY8dPMgTy4",
                "forum": "N6o0ZtPzTg",
                "replyto": "ScibuVp0rH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission993/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission993/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Further Discussions and Feedback Welcome"
                    },
                    "comment": {
                        "value": "We deeply appreciate the insights you've shared during the review process. Following our revisions and previous responses, we are genuinely curious if we have adequately addressed the concerns you raised.\n\nWe would appreciate it if you could kindly let us know if there are any further questions. In the extremely limited time remaining, we are still eager to do our utmost to address them!"
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission993/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700594594422,
                "cdate": 1700594594422,
                "tmdate": 1700594594422,
                "mdate": 1700594594422,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Or1Hjy9KFh",
                "forum": "N6o0ZtPzTg",
                "replyto": "jY8dPMgTy4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission993/Reviewer_QVHX"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission993/Reviewer_QVHX"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your detailed response"
                    },
                    "comment": {
                        "value": "Dear authors,\n\nThanks very much for your detailed responses. I think they have addressed my concern so I maintain my score to support the acceptance of this work."
                    }
                },
                "number": 27,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission993/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700706041801,
                "cdate": 1700706041801,
                "tmdate": 1700706041801,
                "mdate": 1700706041801,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "W0VFgooG3p",
            "forum": "N6o0ZtPzTg",
            "replyto": "N6o0ZtPzTg",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission993/Reviewer_mPPy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission993/Reviewer_mPPy"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes to optimize/choose prompts on a per-query basis to improve LLM performance on arithmetic reasoning tasks. They first identify two challenges towards this objective: a) ground truth labels are missing at inference time, making prompt evaluation challenging and b) repeated LLM interactions are costly. To overcome both of these challenges they propose Prompt-OIRL, an offline inverse reinforcement learning approach to learn a reward function per model and dataset, predicting the success of a  prompt query pair. They demonstrate empirically that this approach outperforms a range of baselines and can improve LLM performance."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The query-dependent prompt optimization setting is a novel and promising direction.\n* Prompt-OIRL is both more accurate and precise at assessing prompt success than LMSC for held-out and seen prompts.\n* The evaluation covers a range of 3 different tasks and models of different sizes."
                },
                "weaknesses": {
                    "value": "* Dataset generation requires a large number of model interactions for every new task and model, as no cross-task or -model generalization has been demonstrated.\n* Many experimental details remain unclear or are only discussed in the appendix. E.g. the modelling of the proxy reward is not discussed at all in the main text, with the appendix suggesting a combination of an LLM embedding of prompt and query followed by an XGBT.\n* It is unclear which single prompt was used for the scarce demonstration setting in Figure 5. Crucially, there, all baseline methods were limited to this single prompt, while Prompt-OIRL could choose any of the 6 considered prompts. Similarly, a comparison to always choosing each of the considered prompts and always choosing the best one (using an oracle) is missing but would be crucial to assessing performance\n* The failures of Prompt-OIRL (e.g. GSM8K with LLaMA or TigerBot (Figures 20 and 21)) where it performs worse amongst all considered methods by a large margin are only shown in the appendix. Ironically this section is titled \"REWARD MODELING: IMPLEMENTATION MATTERS\", suggesting it should be discussed in the main text.\n* While the use of LLMs for embedding computation of prompts is unproblematic when choosing the best-of-n (with n=6) fixed prompts, it might be prohibitively expensive for different policy optimization approaches requiring a substantially larger number of prompts to be embedded. This should be highlighted more prominently and not presented as a major advantage of this method."
                },
                "questions": {
                    "value": "### Questions\n1) Can you describe in detail the modeling of the proxy reward and conduct an ablation over the approaches mentioned but rejected in Appendix C.2\n2) Can you report the performance of the (on average) best (on the test set) prompt and what this prompt is for the different models and datasets? Can you similarly report the performance of always choosing the best prompt for every query (corresponding to the setting where the proxy reward models the success flawlessly)?\n3) How does Prompt-OIRL differ from  BoTr Eqn.2 when trained on all considered prompts? Its description suggests that also selects the best of n prompts but it achieves worse performance in Figure 6. \n4) How does performance depend on the training data-set size given that much smaller datasets with gold-standard annotations will be available for many real-world applications?\n\n### Comments\n* The paper could benefit from a careful copywriting pass that addresses typos and grammatical errors while homogenizing the writing style.\n* Figure 7 would benefit from a relative scale, especially for the \"Cost with Commercial APIs\". It generally seems slightly misleading to report improvements when 100 Prompts are used here while the rest of the paper considers 6 prompts. Further, it remains unclear if or how the cost of embedding the prompts and evaluating the proxy reward is considered here.\n\n### Conclusion\nThe paper presents query-dependent prompt optimization as an interesting and novel approach to improving LLM performance. While multiple models and benchmarks are considered in the empirical evaluation, details remain unclear and some baselines are missing, eroding the confidence in the presented improvements. Combined with the missing details on the exact modeling of the proxy reward as well as any ablation of this key component, I am leaning toward rejecting this paper.\n\n### Post Rebuttal Update\nI have raised my score in response to the detailed rebuttal addressing my questions satisfactorily. I believe the paper could further benefit from incorporating some of these results, presented during the rebuttal."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission993/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission993/Reviewer_mPPy",
                        "ICLR.cc/2024/Conference/Submission993/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission993/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699011186086,
            "cdate": 1699011186086,
            "tmdate": 1700571133972,
            "mdate": 1700571133972,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ML47o5KqfX",
                "forum": "N6o0ZtPzTg",
                "replyto": "W0VFgooG3p",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission993/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission993/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer mPPy (1/6)"
                    },
                    "comment": {
                        "value": "Thank you for the insightful comments and your acknowledgment of the novelty and contribution of our work. \n\nIn our initial manuscript, we focused more on illustrating the key insight of leveraging offline IRL in prompt evaluation and optimization, and deferred the implementation details into the appendix due to the space limit of the main text. As pointed out by the reviewer, we agree expanding those details can further enhance the understanding of our work. As a consequence, we have updated those sections with expanded details. We have also updated the paragraph on training-test prompt generation in our updated version to enhance clarity. \n\n**In the following, we aim to address all the individual points in your review. Please also see the revised manuscript for changes (highlighted in red). As because of your insightful feedback, the clarity of our work has been significantly improved.**"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission993/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700056188768,
                "cdate": 1700056188768,
                "tmdate": 1700056188768,
                "mdate": 1700056188768,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7b2M0VAKgT",
                "forum": "N6o0ZtPzTg",
                "replyto": "W0VFgooG3p",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission993/Reviewer_mPPy"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission993/Reviewer_mPPy"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the Detailed Rebuttal"
                    },
                    "comment": {
                        "value": "I want to thank the authors for going to great lengths to provide a detailed rebuttal, addressing all my questions and concerns. I believe especially the results on generalization across datasets and strong performance even on small datasets to be promising signs for the practical applicability of this approach. \n\nI have thus raised my score.\n\nI would still encourage the authors to address the following points:\n* Highlight the fact that they use an LLM generated embedding as input to their XGBT in Section 3 Step 2.\n* Include a direct comparison of oracle performance with their approach and BoTr Eqn. 1 to better contextualize the performance improvement realized by their method.\n* Update Figure 7 with relative scales."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission993/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700570980710,
                "cdate": 1700570980710,
                "tmdate": 1700570980710,
                "mdate": 1700570980710,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SngOClBJqu",
                "forum": "N6o0ZtPzTg",
                "replyto": "Hw4tI2PuvL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission993/Reviewer_mPPy"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission993/Reviewer_mPPy"
                ],
                "content": {
                    "title": {
                        "value": "Wrong PDF version?"
                    },
                    "comment": {
                        "value": "I want to thank the authors for being so responsive and actively engaging in the discussion\n\nIt seems like the changes to Section 3 Step 2 present in the previous revision got lost in the current version. Similarly, I did not spot the additional information on using embeddings. It also seems that Figure 7 is unchanged.\n\nI am thus wondering if they perhaps uploaded an incorrect pdf."
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission993/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700595795490,
                "cdate": 1700595795490,
                "tmdate": 1700595795490,
                "mdate": 1700595795490,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "g7ijlxXf9j",
            "forum": "N6o0ZtPzTg",
            "replyto": "N6o0ZtPzTg",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission993/Reviewer_wFdE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission993/Reviewer_wFdE"
            ],
            "content": {
                "summary": {
                    "value": "This paper aims to enhance the arithmetic reasoning ability of LLM via prompt optimization. Inspired by the fact that no prompt is perfect for all queries and existing online evaluations of different prompt choices are expensive, this work proposes an offline RL based prompt optimization solution. Given the existing human crafted prompts for different arithmetic reasoning datasets, a reward model without depending on LLM is trained to approximate the prediction by using LLM. Experimental evaluation on multiple arithmetic reasoning dataset with 3 different LLMs shows a strong performance."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This manuscript addresses a compelling issue, namely the optimization of query-dependent prompts, which is becoming increasingly relevant as Large Language Models (LLMs) see wider application across various contexts. Developing an efficient prompt strategy tailored to individual queries, without relying on costly LLM inference, is a pertinent and significant challenge. \n\nThe method put forward is both technically robust and effectively articulated. The authors have offered a comprehensive account of their approach, with a few exceptions (see below) The empirical outcomes presented are robust and offer a positive indication of the method's potential. \n\nMoreover, the analysis and ensuing comparison with pertinent literature succinctly underscore the advantages and novel contributions of this research."
                },
                "weaknesses": {
                    "value": "How does the proposed method perform when the offline prompt-alignment dataset is small? It is encouraging to observe that the method shows promise with the 10 held-out prompts and an expanded set of 100 prompts. Nevertheless, in real-world scenarios, we may encounter new tasks with a limited number of available prompts for offline data. I am curious about the method's performance across various quantities of training prompts. \n\n \n\nSeveral critical technical details are absent from the main text. For example, there is little to no information about the curated offline dataset or the design principles behind the parameterized proxy reward model, among others. \n\n \n\nFollowing these points, it's also vital to explore and articulate the different design choices for reward models. It is mentioned that an MLP model is less effective, yet a detailed analysis would be invaluable, assisting the reader to understand and tailor the method to their specific use cases. \n\n \n\nRegarding the proxy reward model, a simpler, more straightforward query-dependent model, such as a nearest-neighbor based solution, could be considered. This would involve, for each query, locating the closest match whose prompt yields a correct answer and utilizing that prompt for the new query. Please consider incorporating this simpler solution as a baseline for comparison."
                },
                "questions": {
                    "value": "What are the values of K, M and P in appendix section C.1?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission993/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699060394494,
            "cdate": 1699060394494,
            "tmdate": 1699636025370,
            "mdate": 1699636025370,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8imyJBekeC",
                "forum": "N6o0ZtPzTg",
                "replyto": "g7ijlxXf9j",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission993/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission993/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer wFdE (1/3)"
                    },
                    "comment": {
                        "value": "**We thank the reviewer for the insightful comments and for their acknowledgment of the contribution, novelty, and significance of our work. We aim to address all the individual points in your review here, but please also see the revised manuscript for changes (highlighted in red).**\n\n---\n\n### Q.1 How does the proposed method perform when the offline prompt-alignment dataset is small?\n### A.1 \nThank you for highlighting this concern. We agree that the applicability of our idea in settings when demonstration data is scarce (i.e., when the knowledge of effective prompting strategy is limited) is important. \n\n\nIn our experiments, we consider the empirical setting when having access to a different number of expert prompting demonstrations. Specifically, the **scarce demonstration setting** presented in Figure 5 (left panel) experiments with only 1 (out of the 6) training prompt. \n\n\nIn such a setting, as there is only a single expert prompt available during training time, the best-of-training time prompt (BoTr) strategy must choose to use this training prompt, and so does the LLM-Confidence. Differently, with a reward model training using the demonstration data collected from a single expert-crafted prompt, Prompt-OIRL can generalize its evaluation ability on held-out prompts and achieve significantly improved performance.\n\n\nIn Figure 6, we change the availability of training prompts by the x-axis. We can observe that the performance improvement over baseline methods is most significant when only a single training prompt is used. Not surprisingly, when the number of human-crafted prompts increases, the best-of-training performance is greatly improved. \n\n\nTherefore, in practice, Prompt-OIRL shines even with a limited number of demonstration prompts, proving its great potential to be applied to settings when expert prompting knowledge is relatively scarce.\n\n\n> **Actions taken: We\u2019ve updated our manuscript and improved the clarity in conveying such a point. We appreciate the reviewer for commenting on this point, as because of it, the clarity of the significance and general applicability of our work has been enhanced. Revisions have been highlighted in red in our updated manuscript.**"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission993/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700055110296,
                "cdate": 1700055110296,
                "tmdate": 1700594715302,
                "mdate": 1700594715302,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ngNBOiPmpG",
                "forum": "N6o0ZtPzTg",
                "replyto": "g7ijlxXf9j",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission993/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission993/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer wFdE (2/3)"
                    },
                    "comment": {
                        "value": "### Q.2 Details of the offline dataset and the reward model.\n### A.2 \nWe thank the reviewer for pointing out the importance of reward modeling details. Due to space constraints in the main text, we deferred the detailed information about the offline dataset and the discussion on the reward model to the appendix. To make our main text better self-contained, we have revised our manuscript and provided the essential empirical implementation of the reward model in the main text.\n\n\nIn addition, we agree with the reviewer that providing a more detailed empirical comparison and analysis will be invaluable not only for enhancing the understanding of our method but may also contribute to the general community in understanding the learning with embeddings. Therefore, we have also updated our appendix and provided more details on the reward modeling to including the following empirical evidence:\n\n\n1. MLPs easily converge to the trivial classifiers\n\nFor the MLP model, we have tried different choices of its hyper-parameters, including unit numbers, layers, various drop-out choices, and dual-channel architecture with each channel process query and prompt embeddings individually. However, we find all of those choices tend to converge to the trivial solution that predicts either all 0 or all 1 for the binary classification task. Such a reward model does not have the prediction ability in inference time. In practice, with such a reward mode, we can do nothing better than select the best-performing prompt on the training dataset. Therefore, the performance of BoTr Eqn.(1) represents the best-achievable performance of using a dummy MLP reward model.\n\n\n2. XGBoost is robust on hyper-parameter for reward modeling\n\nFor the xgboost method, we set a universal hyper-parameter for all tasks and LLMs. When deploying Prompt-OIRL in practice, a case-by-case engineering on the hyper-parameters on reward modeling should be able to further boost the performance of the algorithm, yet this is out of our research scope. In our paper, we experiment with a single universal hyper-parameter setting for all LLMs and tasks, demonstrating the robustness of the proposed method.\n\n\n3. Instance-wise prediction of outcome is better than pair-wise prediction (preference-based learning)\n\nAnother alternative for reward modeling is based on preference-based learning, which is effective in some conventional inverse RL tasks [1. T-REX]. To be specific, for every training query $x^{(i)}$, there may exist multiple prompts (denoted as $p_+^{(i)}$) that lead to a correct answer, and some other prompts ($p_-^{(i)}$) that lead to a wrong answer. \n\n\nWe can also organize the offline prompt demonstration in the form of preference-based dataset $ (x^{(i)} $ , $p_-^{(i)}$, $p_+^{(i)} )$, and learn from those pair-wise preference data. In this pair-wise preference approach, the learned reward model takes both prompts and the query as input, and outputs the preferred prompt. Given a new query in the inference time, such a reward model can be applied to all K candidate prompts with K-1 comparisons to find the best prompt. (so it is more computationally expensive than the direct reward modeling method used in our work). We empirically studied whether such an approach can lead to better performance on the MAWPS dataset with the GPT-3.5-turbo model. The results are shown in the table below:\n\n\n| # Training Prompts | 1          | 2          | 3          | 4          | 5          | 6          |\n|--------------------|------------|------------|------------|------------|------------|------------|\n| BoTr Eqn(1)        | 0.6598     | 0.6988     | 0.7118     | 0.7155     | 0.7173     | 0.7175     |\n| BoTr Eqn(2)        | 0.6598     | 0.7742     | 0.8150     | 0.8350     | 0.8473     | 0.8546     |\n| BoTr Eqn(2) Paired | 0.6598     | 0.7195     | 0.7203     | 0.7194     | 0.7236     | 0.7247     |\n| Prompt-OIRL        | 0.7637     | 0.8032     | 0.8379     | 0.8750     | 0.8916     | 0.8944     |\n\n\nIt can be concluded that the pair-wise reward model can not achieve better performance than the direct reward modeling used in Prompt-OIRL. \n\n\nAll our offline datasets and code for processing those datasets, as long as the MLP implementation and pair-wise reward modeling implementation will be made publicly available and contributed as an asset for future research.\n\n\n> **Actions taken: We have updated our manuscript accordingly to include extended details for the reward model training. Revisions have been highlighted in red in our updated manuscript.**\n\n---\n\n**_Reference_**\n\n[1. T-REX] Brown, Daniel, et al. \"Extrapolating beyond suboptimal demonstrations via inverse reinforcement learning from observations.\" International conference on machine learning. PMLR, 2019."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission993/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700055399873,
                "cdate": 1700055399873,
                "tmdate": 1700594747255,
                "mdate": 1700594747255,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xXBPD6YXRg",
                "forum": "N6o0ZtPzTg",
                "replyto": "g7ijlxXf9j",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission993/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission993/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer wFdE (3/3)"
                    },
                    "comment": {
                        "value": "### Q.3 A Nearest-Neighbor Baseline\n### A.3\nWe thank the reviewer for pointing out the nearest-neighbor method as an additional practical baseline. We would begin with a discussion on the differences between such a potential baseline and Prompt-OIRL, followed by experiment results.\n\n\n1. The Nearest-Neighbor method can not be generalized to new prompts. The idea of using a nearest-neighbor approach can be an alternative to using a parameterized model in selecting from the training prompts. However, it can not be generalized to unseen prompts due to a lack of support.\n\n\n2. As a non-parametric method, the Nearest-Neighbor approach requires memorization of the training embeddings, which can be expensive when the number of demonstrations increases. \n\n\nEmpirically, we implemented this idea with GPT-3.5-turbo on the MAWPS dataset. To be specific, for every test query, we look up the training queries\u2019 embedding to find the closest neighbor of this test query\u2019s embedding, and select from the training prompt(s) that has successfully prompted correct answers on such a training query. \n\n\nThe results are shown in the table below:\n\n\n| # Training Prompts | 1          | 2          | 3          | 4          | 5          | 6          |\n|--------------------|------------|------------|------------|------------|------------|------------|\n| BoTr Eqn(1)        | 0.6598     | 0.6988     | 0.7118     | 0.7155     | 0.7173     | 0.7175     |\n| BoTr Eqn(2)        | 0.6598     | 0.7742     | 0.8150     | 0.8350     | 0.8473     | 0.8546     |\n| Nearest-Neighbor   | 0.6598     | 0.7856     | 0.8242     | 0.8339     | 0.8492     | 0.8530     |\n| Prompt-OIRL        | 0.7637     | 0.8032     | 0.8379     | 0.8750     | 0.8916     | 0.8944     |\n\n\nThe nearest-neighbor approach, as a query-denpendent baseline, outperforms the BoTr Eqn(1) baseline, and achieves on-par performance as BoTr Eqn(2). However, it underperforms Prompt-OIRL as the generalization ability of the learned reward model in Prompt-OIRL further enhanced its prompting performance.\n\n\n> **Actions taken: We analyzed the potential and challenge of applying the nearest neighbor method as an alternative approach to parametric models used in Prompt-OIRL, and provided additional experiment results to verify the idea.**\n\n---\n\n### Q.4 What are the values of K, M and P in appendix section C.1?\n### A.4\nIn Appendix C.1, N is the number of samples (query-answer pairs) in the training dataset, M is the number of samples in the held-out test dataset, K is the number of training prompts, and P is the number of test prompts. \n\n\nFor all datasets, we experiment with different choices of K = [1, 2, 3, 4, 5, 6]. (i.e., the x-axis of Figure 6: number of training prompts, ranging from 1 - 6). And use a total number of P=110 held-out prompts. \n\n\nFor GSM8K, there are N=7473 samples used for training, and M=1319 samples for testing.\nFor SVAMP, there are N=15000 samples used for training, and M=4690 samples for testing.\nFor MAWPS, there are N=6000 samples used for training, and M=1685 samples for testing.\n\n\n> **Actions taken: We have updated our description in Appendix C.1 to enhance the clarity. Our revision is highlighted in red.**\n\n\n---\nOnce again, we thank the reviewer for their effort in improving our work. If there should be any remaining concerns or questions, we are keen to do our utmost to address them."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission993/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700055509356,
                "cdate": 1700055509356,
                "tmdate": 1700594812100,
                "mdate": 1700594812100,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "F7hlDZX6ux",
                "forum": "N6o0ZtPzTg",
                "replyto": "g7ijlxXf9j",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission993/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission993/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Further Discussions and Feedback Welcome"
                    },
                    "comment": {
                        "value": "We deeply appreciate the insights you've shared during the review process. Following our revisions and previous responses, we are genuinely curious if we have adequately addressed the concerns you raised.\n\nWe would appreciate it if you could kindly let us know if there are any further questions. In the extremely limited time remaining, we are still eager to do our utmost to address them!"
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission993/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700594648588,
                "cdate": 1700594648588,
                "tmdate": 1700594648588,
                "mdate": 1700594648588,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DKTH09b8AS",
                "forum": "N6o0ZtPzTg",
                "replyto": "xXBPD6YXRg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission993/Reviewer_wFdE"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission993/Reviewer_wFdE"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the response."
                    },
                    "comment": {
                        "value": "Thank you for your thorough response. I will maintain my positive rating. Although the nearest neighbor method is simple and possesses inherent limitations, as discussed, it remains an essential baseline for analysis. I recommend that the authors incorporate it into the final version of the paper."
                    }
                },
                "number": 25,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission993/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700679053384,
                "cdate": 1700679053384,
                "tmdate": 1700679053384,
                "mdate": 1700679053384,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]