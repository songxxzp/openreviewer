[
    {
        "title": "On the Role of General Function Approximation in Offline Reinforcement Learning"
    },
    {
        "review": {
            "id": "YsAxtasZq1",
            "forum": "JSS9rKHySk",
            "replyto": "JSS9rKHySk",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7145/Reviewer_MhTC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7145/Reviewer_MhTC"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the offline RL with general function approximation.\n\nThe central problem of this topic is to identify the structural assumptions that empower sample-efficient learning. Many previous works impose assumptions on the data coverage and/or assumptions on the function class/MDP structures. Recently, foster et. al. established the fundamental limit when the completeness is missing but there are also works like [1] and [2], bypassing the lower bound in foster et. al. with different function approximation target.\n\nThis work aims  to present a rather unified framework to connect the function approximation to the realizability, which is interesting and very relevant to the community.\n\n\n[1] Offline reinforcement learning with realizability and single-policy concentrability\n[2] When is realizability sufficient for off-policy reinforcement learning?"
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The topic is interesting. I have been curious about what we can do without strong completeness-type assumption and super-strong data coverage conditions since [1]. While there are works bypassing the lower bounds in [1] with different type of realizability assumptions and refined characterization of completeness [2, 3], the relationship between various assumptions in the literature is still not very clear. So it is exciting to see that the authors are trying to fill the gap.\n\nHowever, the current version is still not ready for publication due to the writing quality and since the relationships to previous works are not clear. \n\n[1] Offline reinforcement learning: Fundamental barriers for value function approximation\n[2] Offline reinforcement learning with realizability and single-policy concentrability\n[3] When is realizability sufficient for off-policy reinforcement learning?"
                },
                "weaknesses": {
                    "value": "The main drawback of this paper is the writing. The authors state many things without concrete examples and arguments but only abstract summarizations (by using the word certain). For instance, the authors mention the previous works of ``Xie and Jiang''by their refined data assumptions, which assuming that the audience is familiar with these previous works. Meanwhile, many notations are used without a formal definitions although I can understand some of them as they have been used by the previous papers but this can still be a problem for general readers. \n\n1 It would be great if the authors could instantiate the definitions present in this paper or discuss the relationships with the previous assumptions like the Bellman completeness in [1] and the realizability in [2]. For instance, in the example 2, it would be better to prove that the standard Q-pi-realizability is a special case of definition 2 by explicitly specifying the $\\mathcal{F}$ and $\\mathcal{G}$.\n\n2 In example 3, may I take \\phi as the mapping from a Q-function to its greedy policy? So completeness w.r.t. $\\mathcal{T}^\\pi$ for all the functions whose greedy policy is $\\pi$ is indeed completeness w.r.t. $\\mathcal{T}$ for all the functions in $\\mathcal{F}$?\n\n3 While proposition 1 is intuitive and reasonable, it seems that many previous works are indeed constructing the lower bounds through a worst-case consideration on the MDP by constructing a family of MDP classes satisfying the assumptions (e.g. [3]). \n\n4 How did the conditions of theorem 1 compare to the assumptions in [1,2]? This should be highlighted so that the readers can evaluate the roles of the corollaries presented in section 5.1.\n\n5 Most of the coverage assumptions are made with respect to the $\\xi(M) \\in \\Pi$, which is more related to the coverage over $\\Pi$, in contrast to the single-policy coverage assumption ($\\pi^*$) commonly used in the previous works. I am trying to understand the case of $\\pi^* \\in \\Pi$. But due to the presence of $\\xi$, the obtained lower bound may not compete with $\\pi^*$. Can you give an example of the unknown mapping $\\xi(\\cdot)$ so that we can understand the meaning of the target $J_M(\\xi(M))-J_M(\\hat{\\pi})$?\n\n\n\n[1] Bellman-consistent Pessimism for Offline Reinforcement Learning\n\n[2] Offline reinforcement learning with realizability and single-policy concentrability\n\n[3] is pessimism provably efficient for offline rl"
                },
                "questions": {
                    "value": "See weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7145/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7145/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7145/Reviewer_MhTC"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7145/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698633283468,
            "cdate": 1698633283468,
            "tmdate": 1700538241489,
            "mdate": 1700538241489,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "hPQq5wMfQK",
                "forum": "JSS9rKHySk",
                "replyto": "YsAxtasZq1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7145/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7145/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate the reviewer for the valuable feedback, particularly the critical comments on writing have helped us improve the presentation of our paper. We have addressed the comments, and incorporated your suggestions in the revised paper. We hope the reviewer will be content with our response and revision. Additional comments are also welcome during the discussion phase.\n\n## Response to the first paragraph in the weakness part: \n\nYour comments have been well-taken. We acknowledge that the paper and certain terminologies are somewhat technical. One reason for this perception is that we intended to convey as much information as possible to readers, which may result in a lack of clarity for general readers. In fact, to address this, we included a notation table in Appendix A to ensure the clarity of the notations. \n\nIn response to your comment about the data assumption by Xie and Jiang, we have added a detailed description at its first occurrence in the revision. Moreover, we have also incorporated your other suggestions into the revision (please refer to our subsequent responses). We hope that the presentation of our revised paper meets your expectations.\n\n## Response to Weakness 1\n\nThanks for your suggestion. The definition of Bellman-completeness was provided in Example 3. In the revision, we have added its definition at its initial occurrence to ensure better clarity.\nFor the comparison with assumptions in previous works, please see our response to weakness 4.\nRegarding the Q-pi-realizability in example 2, corresponding to Definition 2, we have $\\mathcal{F}^\\star=${$Q_{\\pi}|\\pi\\in\\Pi$}, $\\mathcal{F}=\\mathcal{Q}$ and $\\mathcal{G}=\\Pi$.  Explicit specifications in the examples have also been added in the revision.\n\n## Response to Weakness 2\n\nYes, you are correct. As stated in Example 3, the function $\\phi$ is a mapping from $q$ to $\\pi_q$, where $\\pi_q$ is the optimal/greedy policy for $q$. This conversion is to emphasize the role of policy in RL analysis.\n\n## Response to Weakness 3\n\nMost previous works construct MDP families satisfying assumptions of model-free functions (as mentioned in section 2.4.), whereas we propose building lower bounds for model-based functions.\nThis type of construction enables us to extend the lower bound to a wide class of functions, making it more generic and versatile.\n\n## Response to Weakness 4\n\nAs stated in the paragraph before theorem 1, our data assumptions are stronger than the partial coverage (the assumption in works like [1,2,4,8]), exploratory coverage (the assumptions in works like [7]), and some refined data assumptions [5,6].\n\nOur function assumptions are designed to verify if learning with weak function assumptions is possible.\nTo build some generic lower bounds, we focus on the model class in the theorem. \nOur function class assumptions are weaker than the ones made in model-based analysis ([8], as mentioned in remark 4).\nA comparison with model-free algorithms is hard to compose.\nNevertheless, when specifying to concrete functions like the value function and the density ratio, \none can show that our function class assumptions are weaker than the ones with completeness-type assumptions (like [1,4,7]), \nand are at the same level compared with literature making only realizability-type assumptions (like [2,5,6]).\nWe aim to demonstrate that completeness-type assumptions are particularly challenging to be mitigated.\nWe have added this part of the comparison to our paper (Appendix F, due to the page limitation). Thanks again for your advice.\n\n## Response to Weakness 5\n\nThe lower bound construction (as shown in section D.1) can actually ensure that we can have $J_M(\\xi(M))=J_M(\\pi^\\star)$, i.e., $\\xi(M)$ is optimal w.r.t. $M$ from the initial distribution. \nTherefore, we have $J_M(\\xi(M))-J_M(\\hat{\\pi})=J_M(\\pi^\\star_M)-J_M(\\hat{\\pi})$.\nThis makes the comparison interesting.\nAlso, a relationship with the coverage assumptions in previous works is that: coverage assumptions w.r.t. $\\xi(M)$ are the same as $\\pi^\\star$ (which is optimal from every state or state-action pair), since we only consider coverage assumptions from the initial distribution.\n\nMoreover, as the dataset is uncontrollable in most cases, assuming it covers the optimal policy is a bit strong.\nOne may instead want to learn a policy that performs better than a specific one. This corresponds to the use of $\\xi$.\n\n[1] Bellman-consistent Pessimism for Offline Reinforcement Learning\n\n[2] Offline reinforcement learning with realizability and single-policy concentrability\n\n[3] is pessimism provably efficient for offline rl\n\n[4] Provably good batch reinforcement learning without great exploration\n\n[5] Batch value-function approximation with only realizability\n\n[6] Refined value-based offline rl under realizability and partial coverage\n\n[7] information-theoretic considerations in batch reinforcement learning\n\n[8] Pessimistic Model-based Offline Reinforcement Learning under Partial Coverage"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7145/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699982665907,
                "cdate": 1699982665907,
                "tmdate": 1699982665907,
                "mdate": 1699982665907,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JBQcymMB3E",
                "forum": "JSS9rKHySk",
                "replyto": "hPQq5wMfQK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7145/Reviewer_MhTC"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7145/Reviewer_MhTC"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the detailed responses"
                    },
                    "comment": {
                        "value": "Thanks for the detailed responses. \n\nI believe that this is an interesting and solid works, and is also novel in providing new insights for the readers. The only concern from my side is the clarity and the way of conveying the ideas and information. The responses are clear and address most of my concerns so I raise my score to 6 to support the acceptance of this work. I still have some minor problems and suggestions. \n\n1. It is a good idea to use one section (in the appendix) to throughly review the existing function approximation assumption and data coverage assumption, where you may clearly state their formal definition and the intuitions, followed by some comparisons to reveal their connections and differences. I believe that this can not only make the paper more friendly to the readers, but also promote the impacts of the work. You may achieve this goal by elaborating on the last section in the revised version.\n\n2 In the last section, you state that the realizability-type assumption in theorem 2 is comparable with Zhan et al. Does this mean the two assumptions can imply each other? Or does it mean they are only similar in principle (realizability)? Could you position the result of Zhan at al. 2022 in the unified framework of this paper? In my mind, Zhan et al. 2022 considers a different function approximation method (compared to the value-based one in the Foster et al. 2021). Since we have a unified lower bound here, an interpretation of this less-explored function approximation method would be interesting and showcase the power of the presented framework.\n\n3 I agree that in most of times, we may not expect that the dataset covers the pi^* well but only can expect to compete with the best policy covered by the offline dataset. I would like to thank for the classification on $J_M(\\xi(M))=J_M(\\pi^\\star)$ and this makes a lot of sense. But I noticed that in [1] (and also some other works like [2] where the results can be easily extended to compete with the covered policies with minor modification), the results are indeed with respect to any policies (in a pre-determined policy class though). Therefore, it is still helpful to give a concrete example of \\xi to help the readers to evaluate the theorems provided in this paper.\n\nThanks again for your efforts in exploring this core problems in rl theory.\n\n[1] Bellman-consistent Pessimism for Offline Reinforcement Learning\n[2] is pessimism provably efficient for offline rl"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7145/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700344932974,
                "cdate": 1700344932974,
                "tmdate": 1700344932974,
                "mdate": 1700344932974,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "49xiSVeOX0",
                "forum": "JSS9rKHySk",
                "replyto": "ZEI0wLnEbi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7145/Reviewer_MhTC"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7145/Reviewer_MhTC"
                ],
                "content": {
                    "title": {
                        "value": "thanks for the follow-up responses"
                    },
                    "comment": {
                        "value": "I have further raised my score to 8 to support the acceptance."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7145/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700538209521,
                "cdate": 1700538209521,
                "tmdate": 1700538209521,
                "mdate": 1700538209521,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "nVIj5wADJe",
            "forum": "JSS9rKHySk",
            "replyto": "JSS9rKHySk",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7145/Reviewer_REan"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7145/Reviewer_REan"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies offline RL with general function approximation, focusing on an information-theoretic understanding of different types of assumptions (such as completeness and realizability) and how they translate to assumptions on the underlying MDP. This results in a principled way of constructing lower bounds in offline RL with general function approximation via model-realizability, and with this method the authors prove the necessity of completeness assumptions."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "- This a well-motivated and interesting work and I enjoyed reading the paper. The work puts common assumptions for PAC learning of offline RL with general function approximation under scrutiny and provides a clear overview and concrete definitions of completeness and realizability assumptions. \n- Connecting the model-free completeness assumptions to assumptions that impose restrictions on the underlying MDP is very useful as it unifies lower-bound constructions for different function approximation methods. Particularly given that worst-case information-theoretic lower bounds are specific to function-class assumptions\u2014e.g., Foster et al. 2021 prove a lower bound for value-based and exploratory data settings while recent papers such as Zhan et al. 2022 overcame the difficulty by changing the function approximation method. \n- With the model-realizability technique introduced in this paper, the authors show the necessity of the completeness-type assumptions."
                },
                "weaknesses": {
                    "value": "There are no major weaknesses."
                },
                "questions": {
                    "value": "--"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7145/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698904337738,
            "cdate": 1698904337738,
            "tmdate": 1699636846110,
            "mdate": 1699636846110,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "swHtW29o4I",
                "forum": "JSS9rKHySk",
                "replyto": "nVIj5wADJe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7145/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7145/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We would like to thank the reviewer for the positive assessment of this work. We\nalso appreciate your efforts and valuable time for carefully reviewing this paper."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7145/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700052360527,
                "cdate": 1700052360527,
                "tmdate": 1700052360527,
                "mdate": 1700052360527,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "4PRYxCwMdl",
            "forum": "JSS9rKHySk",
            "replyto": "JSS9rKHySk",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7145/Reviewer_85dZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7145/Reviewer_85dZ"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies general function approximations in offline reinforcement learning (RL) settings. The authors firstly formally defined two types of usual assumptions on function approximations, i.e., realizability-type and completeness-type. They then discussed the relationship between assumptions on function approximations and the Markov decision processes (MDPs) those can be realized. Section 5 presented the main Theorem 1, which says that only realizability-type of assumptions are not enough to learn better policies. Corollaries 1-3 then assure similar results for different variants of realizability-type assumptions. The authors later discussed the limitations of the lower bound results, i.e., using over-coverage and not containing $Q^*$-realizability. Theorem 2 then shows a lower bound for $Q^*$-realizability but with partial coverage which may not cover optimal policy."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The presentation is clear, with enough discussion about the background and related work.\n2. The problem is convincingly motivated and important.\n3. The results are novel to my knowledge."
                },
                "weaknesses": {
                    "value": "1. As the authors also noted, the main point of arguing that only realizability-type assumptions are not enough to learn better policies is kind of weakened by the fact that upper bounds for $Q^*$-realizability already exist, and Theorem 2 also feels short of compensating since as mentioned the covered policy is not optimal."
                },
                "questions": {
                    "value": "Could you explain how it happens for $\\xi(M)$ can be optimal while the value function class does not contain optimal $Q^*$. Example or intuition could be helpful."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7145/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699084858071,
            "cdate": 1699084858071,
            "tmdate": 1699636846005,
            "mdate": 1699636846005,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kSKzJ3JBAI",
                "forum": "JSS9rKHySk",
                "replyto": "4PRYxCwMdl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7145/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7145/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the valuable comment. Here are our responses:\n\n> Could you explain how it happens for $\\xi(M)$ can be optimal while the value function class does not contain optimal $Q^\\star$. Example or intuition could be helpful.\n\nThis is a good question. Note that $\\xi(M)$ is only required to be optimal with respect to the initial state distribution, \nwhereas $\\pi^\\star$ should be optimal from every state or state-action pair. The degree of strictness in optimality is the major difference."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7145/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700052313678,
                "cdate": 1700052313678,
                "tmdate": 1700052336136,
                "mdate": 1700052336136,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "avqTi2lPhq",
            "forum": "JSS9rKHySk",
            "replyto": "JSS9rKHySk",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7145/Reviewer_PRdx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7145/Reviewer_PRdx"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on offline learning setting with general function approximation.\nThe authors first formalize different types of assumptions in offline RL (realizability, completeness).\nIn Section 3, the authors reveal that the lower bounds derived for model-realizability can also be applied to other types of functions.\nAfter that, in Section 4 and 5, they establish lower bounds for learning in offline setting either without $Q^*$-realizability or weaker data coverage assumptions."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper writing is relatively clear to me. The authors contribute interesting lower bounds, which reveal new understanding for learning in offline setting with general function approximation."
                },
                "weaknesses": {
                    "value": "1. Although lower bound results are also meaningful, it would be better if there are some upper bound results to understand what we can do.\n\n2. As for the results in Section 6, can you explain more about the difference between the data coverage assumptions in (Xie & Jiang 2020) and the lower bound instance in Theorem 2?\n\n    Besides, as for the data coverage assumption used in Theorem 2, how does it compare with the practical scenarios? Would it be too restrictive in practice (maybe in practice we may rarely encounter such bad case)?"
                },
                "questions": {
                    "value": "I'm still trying to understand more about Def. 2. Consider the completeness assumption that $\\forall f\\in\\mathcal{F}$ we have $\\mathcal{T}^* \\in \\mathcal{F}$. I wonder what the corresponding $\\mathcal{F}$ and $\\mathcal{G}$ in Def. 2 should be in this case?\n\nIt also seems unclear to me how Def. 2 \"captures\" what common completeness assumptions ensure: \"the existence of a\nfunction class that can minimize a set of loss functions indexed by another function class\"."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7145/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699093319482,
            "cdate": 1699093319482,
            "tmdate": 1699636845900,
            "mdate": 1699636845900,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "hppG9WQOin",
                "forum": "JSS9rKHySk",
                "replyto": "avqTi2lPhq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7145/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7145/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the valuable comment. Here are our responses:\n\n> Although lower bound results are also meaningful, it would be better if there are some upper bound results to understand what we can do.\n\nThe focus and novelty of this work is to develop new lower bounds to understand the fundamental limitation of learnability in offline RL. Several prior works have already established upper bounds on the same problem. In our paper, we have provided a review of these works in Sections 2.2 and 2.3.\n\n> As for the results in Section 6, can you explain more about the difference between the data coverage assumptions in (Xie & Jiang 2020) and the lower bound instance in Theorem 2?\n\nThanks for your question. In short, the data assumption in section 6 is weaker than the one in (Xie & Jiang 2020)---it is the partial coverage, while the one in (Xie & Jiang 2020) is even more stringent than exploratory coverage.\n\n> Besides, as for the data coverage assumption used in Theorem 2, how does it compare with the practical scenarios? Would it be too restrictive in practice (maybe in practice we may rarely encounter such bad case)?\n\nThe data assumption used in Theorem 2 is the partial coverage, which assumes that the dataset covers only one policy. This is practical and is fulfilled in benchmarks like D4RL.\nFurthermore, as we construct information-theoretic lower bounds, we prefer a \"restrictive\" condition, as it reveals a more crucial fundamental limitation.\n\n> I'm still trying to understand more about Def. 2. Consider the completeness assumption that $\\forall f\\in\\mathcal{F}$, we have $\\mathcal{T}^\\star\\in\\mathcal{F}$. I wonder what the corresponding $\\mathcal{F}$ and $\\mathcal{G}$ in Def. 2 should be in this case?\n\nBellman-completeness is paired with realizability-type assumptions (e.g., $f^\\star\\in\\mathcal{F}$ where $f^\\star$ is the optimal value function) in most cases, thus we can conclude that $\\mathcal{F}$ is realizable. Therefore, $\\mathcal{F}$ in your question can be considered as $\\mathcal{F}$ and $\\mathcal{Q}$ in Definition 2 simultaneously. Also, we have that $\\mathcal{F}^\\star=${$\\mathcal{T}^\\star f| f\\in\\mathcal{F}$}.\n\n> It also seems unclear to me how Def. 2 \"captures\" what common completeness assumptions ensure: \"the existence of a function class that can minimize a set of loss functions indexed by another function class\".\n\nThis is a good question. Note that approximating one target $g\\in\\mathcal{G}$ with a function class $\\mathcal{F}$ is identical to finding $f\\in\\mathcal{F}$ to minimize the loss $\\lVert f-g\\rVert_{\\infty}$. Thus, the completeness-type assumption essentially imposes a set of loss functions as {$\\lVert f-g\\rVert_{\\infty}|g\\in\\mathcal{G}$}."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7145/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699982135305,
                "cdate": 1699982135305,
                "tmdate": 1699982135305,
                "mdate": 1699982135305,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "7hT756YiLt",
            "forum": "JSS9rKHySk",
            "replyto": "JSS9rKHySk",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7145/Reviewer_XLGY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7145/Reviewer_XLGY"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the role of function approximation in offline reinforcement learning (RL). Specifically, the paper first propose various types of function classes used in offline RL, and then show the learning lower bound under these types of function classes. Generally speaking, given functions with a policy class domain, it is impossible to learn a good policy."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper proposes a unified way to study general function approximation in offline RL.\n2. The theoretical results are solid and interesting."
                },
                "weaknesses": {
                    "value": "1. The presentation is not clear. Some new words, such as exploratory-accurate(accuracy) and data assumption are used without pre-defined.\n2. While there are examples about those assumptions, I think a big table summarizing and connecting existing concrete examples and assumptions and the proposed general assumptions can make the paper easier to understand."
                },
                "questions": {
                    "value": "It seems like function approximation on policy class or value function class results in impossible learning tasks, does it mean that learning with a model class might have positive results?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7145/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699162171558,
            "cdate": 1699162171558,
            "tmdate": 1699636845789,
            "mdate": 1699636845789,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "suopW3ru0w",
                "forum": "JSS9rKHySk",
                "replyto": "7hT756YiLt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7145/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7145/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the valuable comment. Here are our responses:\n\n> 1. The presentation is not clear. Some new words, such as exploratory-accurate(accuracy) and data assumption are used without pre-defined.\n\nYour comments have been well-taken. In fact, the exploratory-accurate is defined right after the definition of the Realizability-type assumption (Definition 1). Different coverage assumptions are defined in the second paragraph in section 2.2. \n\nIn the revised version, we have made every effort to provide formal or informal definitions (or at least references to formal definitions) for technical terms when they are first used. For instance, in footnote 2, we provide a reference to the definition places for the terms you point out.\n\n> 2. While there are examples about those assumptions, I think a big table summarizing and connecting existing concrete examples and assumptions and the proposed general assumptions can make the paper easier to understand.\n\nThanks for your suggestion. In the revision, we have added a table (on page 32 of the Appendices) that compares the assumptions made in this work with those in other related works. We believe this addition further enhances the quality of this paper.\n\n> It seems like function approximation on policy class or value function class results in impossible learning tasks, does it mean that learning with a model class might have positive results?\n\nThis is a good question. In fact, both model-free learning and model-based learning have positive results. Whether we have negative or positive results depends on the assumptions of function classes and the data. This paper aims to understand that if we can learn good policies under weak function assumptions in offline RL."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7145/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699981938469,
                "cdate": 1699981938469,
                "tmdate": 1699981938469,
                "mdate": 1699981938469,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lbXkzE3mHo",
                "forum": "JSS9rKHySk",
                "replyto": "suopW3ru0w",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7145/Reviewer_XLGY"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7145/Reviewer_XLGY"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the responses, and I decide to keep my score."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7145/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700698614187,
                "cdate": 1700698614187,
                "tmdate": 1700698614187,
                "mdate": 1700698614187,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]