[
    {
        "title": "Multiple Positive Views in Self-Supervised Learning"
    },
    {
        "review": {
            "id": "p99FC4UnpF",
            "forum": "WGP2pHtLtn",
            "replyto": "WGP2pHtLtn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission126/Reviewer_pKGj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission126/Reviewer_pKGj"
            ],
            "content": {
                "summary": {
                    "value": "This work analyzes the limitations of contrastive learning with two-views and extend it to multiple-views through the lens of information theory. They provide theory to show their objective is a lower bound of the information bottleneck. Finally, their experiments show improved performance and speed-up."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "The problem is underexplored and the novelty of the paper is strong.\n\nThe theoretical analysis is rigorous with clear definitions."
                },
                "weaknesses": {
                    "value": "The presentation can be improved.\n\nIn method section,\n1) The assumptions of the proposed method are not clearly stated. \n2) What are the failure cases for the method?\n3) What is an intuitive example of view-invisible bias? Could you please clarify \"a mutually exclusive state of Z owing to the invariant nature of label and view information in optimization\"? What are the \"certain approaches\" that \" assume sharable task information ...\"?  Are there experiments to support eq 10? Adding concrete examples could help to better digest the theorems.\n\nIn experiment section,\n\n4) Could you clarify why \"we adopted unselected data augmentations that slightly deviate from a Sweet Spot\"? Does it affect the improvements over baselines?\n5) Is the linear accuracy of the main experiment, table 2&3, omitted?\n6) Is the method scalable to medium size datasets such as ImageNet?\n \nAblation study is not a key contributions to the SSL.\n\nFor three variables MI is defined as I(x; y, z) = I(y; z) - I(y; z | x). There are typos in eq 12."
                },
                "questions": {
                    "value": "This is an interesting paper in multiple aspects, however the presentation can be significantly improved."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission126/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission126/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission126/Reviewer_pKGj"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission126/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698733003718,
            "cdate": 1698733003718,
            "tmdate": 1699635938095,
            "mdate": 1699635938095,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fLc3eEowjZ",
                "forum": "WGP2pHtLtn",
                "replyto": "p99FC4UnpF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission126/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission126/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer,\nThank you for your insightful comments. Your suggestions have helped us greatly improve the manuscript, and the corresponding modified parts are underlined in green.\n\n### Response to Q1\n\nIt is indeed essential to clarify the assumptions of our method. As mentioned in section 3.2.1 of our paper, SSL inherently implies a series of assumptions that can be established only under an exceedingly strict set of conditions. These conditions act as limiting constraints on stability and consistency. Thus, our work primarily focuses on discarding rather than defining more assumptions. We want to draw the reader's attention to this aspect, and our discussion of assumptions unfolds in three stages. Additionally, our assumptions are minimal, with the Information Bottleneck being the primary one. We believe the appropriately revised article now offers a reasonable exposition of these assumptions.\n\n**1. Clarifying Theoretical and Hypothetical Context**: \nAs detailed in the first paragraph of the method on Page 3, we state the theory and assumption before introducing the method.\n\n**2. Concretizing Inferential Assumptions**: \nIn Section 3.1, third paragraph, we explicitly add the necessary representative assumptions for formula derivation.\n\n**3. Discussing Limitations of Assumptions and Comparing with Previous Assumptions**: \nIn Section 3.22, the second paragraph, we compare and illustrate the assumptions we have discarded. Lastly, as this content is not the focal point of our work, we elaborate on the assumptions in Appendix 8.3, specifically emphasizing the application scope and limitations within this assumption framework.\n\nWe believe this will provide a valuable reference for readers interested in understanding the underpinnings of our approach.\n\n### Response to Q2\n\nIn our paper, we mentioned \"DCL perform poorly on a smaller dataset are highly probable to fail on larger datasets as well. Theoretically, methods that strive to maximize MI may be more successful\". Since failure cases are rare and space constraints, and Max-->MI is almost consensus in most methods, we only briefly mention that in the main text. \n\nHowever, your insightful comment has prompted us to reconsider your 1st question. So far, we have observed positive effects across nearly all datasets and methods, yet DCL[1] and Barlow Twins[2] failed. The key to DCL's approach is replacing the original objective function with a ratio of similarities between positive and negative samples, eliminating the negative-positive-coupling (NPC) effect. On the other hand, Barlow Twins focuses on preventing collapse through the cross-correlation matrix. These methods share one commonality: they diverge significantly from information entropy.\n\nThe discussion of these failure cases, as examples outside our assumption scope, is intrinsically linked to our assumptions about our method (See updated Appendix 8.3). We are very grateful for your insight, prompting this connection.\n\n### Response to Q3\n\n>What is an intuitive example of view-invisible bias? \n\n\"View-invisible bias\" refers to the phenomenon where crucial information for a downstream task, such as prior knowledge in domain A classification, is missing in the enhanced views (e.g., cropped images). The essential information needed for classification is represented by the solid-line circle in the figure 2.\nWhen we apply enhancements like cropping to a view, there's a high probability that necessary information for classification will be lost. Different views may crop different areas, resulting in non-overlapping missing information.\nTherefore, in scenarios with a limited number of views (e.g., 2), the amount of lost information increases. Having more views can mitigate this issue. However, the information contained in view 1 alone can only cover a part of the necessary information for the task. This bias demonstrates the importance of considering multiple views \n\n\n\n>Could you please clarify \"a mutually exclusive state of Z owing to the invariant nature of the label and view information in optimization\"? \n\nGood question.\n\nIn eq 2-3, \n\n$\\min _{p(z \\mid v)} I(V ; Z \\mid Y)-\\beta I(Z ; Y)+\\sigma$\n\n$\\stackrel{21}{=} \\min _{p(z | v)} I(V ; Z)-(\\beta+1) I(Z ; Y)+\\sigma$\n\n$\\stackrel{22}{\\propto} \\min \\_{p(z \\mid v)} \\underbrace{I(V ; Z \\mid Y)}\\_{\\text {Minimality bias }}+\\underbrace{\\beta I(V ; Y \\mid Z)+\\sigma}\\_{\\text {Sufficiency bias }}$\n\nYou might be curious about why I(Z;Y) can be proportionally equivalent to I(V;Y|Z). This is due to the fact that I(V;Y) = I(Z;Y) + I(V;'Y|Z). During a certain step in the gradient optimization process, the information from both V and Y remains constant. As a result, these two terms are mutually exclusive and both fall within the part that can be optimized by Embedding Z. We have made corresponding modifications in the text to ensure a clear exposition of this concept."
                    },
                    "title": {
                        "value": "Reply to reviewer pKGj (part1)"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission126/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700388726495,
                "cdate": 1700388726495,
                "tmdate": 1700389062453,
                "mdate": 1700389062453,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1V6uuQ7r0w",
                "forum": "WGP2pHtLtn",
                "replyto": "p99FC4UnpF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission126/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission126/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to reviewer pkGj (part2)"
                    },
                    "comment": {
                        "value": ">What are the \"certain approaches\" that \" assume sharable task information ...\"? \n\nWe rewrite it as \"...prior approaches focused on minimizing $I(z_1, z_2)$ assume that the two views share solely task-relevant information and are independent of redundancy information,\"\n\n>Are there experiments to support eq 10? Adding concrete examples could help to better digest the theorems.\n\nTo address your question about experiments supporting equation 10, we draw on the intuition that the information shared among variables cannot exceed the total amount of information contained in those variables combined. This is akin to the concept of meaning embeddings distilling redundant information. A visual representation from MacKay's \"Information Theory, Inference, and Learning Algorithms\" illustrates this point:\n```\n=====================H(X,Y)======================\n===============H(X)============|\n                    |============H(Y)============\n=======H(X|Y)=======|====I(X;Y)===|====H(Y|X)====\n```\n\nHowever, it's important to note that this visualization is an analogy and not an exact representation. To empirically validate equation 10, we conducted statistical tests on each model. Specifically, we employed binomial statistics tests. The detailed methodology and results can be found on our anonymous GitHub page: https://anonymous.4open.science/r/Multiple-Positive-View-F043/Analysis/equation10.ipynb\n\nAs for the results:\n- Number of samples satisfying the condition: 391 out of 400\n- Proportion of satisfactory samples: 0.98\n- Binomial test p-value for threshold 0.96: 0.0403\n\nThese findings imply that the proportion of samples satisfying $I(z_1;\\ldots;z_n)\\leq H(z\\_{\\text{mean}})\\leq{H(z_1,\\ldots,z_n)}$ is significantly higher than a set threshold (e.g., 85%). This provides strong evidence that most of your data follows the relationship proposed in equation 10.\n\n\n### Response to Q4\n\nWe adopt unselected data augmentations that slightly deviate from a Sweet Spot[3] made to demonstrate the robustness of our method, and it does not significantly impact the improvements over baselines.\n\nAs discussed in Tian et al., 2020b[3], there is a consensus that the relationship between data augmentation and training effectiveness follows a U-shaped curve. Carefully selected augmentations can minimize view-invisible bias and view-specific bias, achieving an optimal solution for \\( I(Z1, Z2) \\) as the objective function in ideal scenarios.\n\nHowever, we ensured the use of a broadly accepted augmentation strategy, recognizing that perfect augmentation does not exist and is inherently random. Secondly, if we designed specific augmentations for each model, it would be challenging to compare improvements across different baselines due to our strategy or the augmentation. Thirdly, according to our theory, nearly any point on the U-shaped curve can be leveraged to eliminate these biases through multi-view strategies. Our experimental results validate that our approach is capable of challenging well-selected baselines and augmentations.\n\n### Response to Q5\nBoth linear probing and K-Nearest Neighbors (KNN) are common evaluation metrics in the SSL framework, and their results are often highly correlated. However, in scenarios where class decision boundaries may not be linear, we find KNN to be more persuasive due to its robustness to intra-class variations and a more interpretable feature space.\n\nAdditionally, our intention is to align our metrics with established benchmarks like those outlined in the [Benchmarks \u2014 lightly 1.4.20 documentation](https://docs.lightly.ai/self-supervised-learning/getting_started/benchmarks.html#cifar-10). This alignment facilitates easier comparison and consistency with prevailing standards in the field."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission126/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700388907828,
                "cdate": 1700388907828,
                "tmdate": 1700389020049,
                "mdate": 1700389020049,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "x4LvnW46Jc",
            "forum": "WGP2pHtLtn",
            "replyto": "WGP2pHtLtn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission126/Reviewer_K5DW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission126/Reviewer_K5DW"
            ],
            "content": {
                "summary": {
                    "value": "The paper considers using multiple positive views in training the SSL model. The paper derives certain theoretical aspects and advantages of introducing the multi view self-supervised learning method. Empirical evidence demonstrates the superiority of the proposed method."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "S1. The paper explores the integration of multiple positive views during the training of the SSL model. \n\nS2. It establishes specific theoretical implications and benefits associated with the implementation of the multi-view self-supervised learning approach. \n\nS3. Empirical findings affirm the effectiveness of the proposed methodology."
                },
                "weaknesses": {
                    "value": "W1. The paper lacks novelty. There are many works introducing multiple views in the SSL pre-training, including DINO, SwAV  and so on, The paper does not compare with these well known SOTA methods. \n\nW2: It is unclear what is the explicit loss function of the proposed method (although it seems Eq. (11) is the loss), and how it hears advantages over SOTA or how it distinguishes in motivation between the existing multiview method such as SwAV (clustering based method with multi-views).\n\nW3: It is unclear if there are fairness issues during the training (empirical evidence), i.e., does the proposed multi-view contrastive learning simply benefits from more \"effective epochs\" because of its multi-view training (more data in each batch) in comparison to other SOTA methods? \n\nPlease help clarify the above concerns. \n\n[A] Mathilde Caron et al., SwAV: Unsupervised Learning of Visual Features by Contrasting Cluster Assignments. \n\n[B] Mathilde Caron et al., . Emerging Properties in Self-Supervised Vision Transformers"
                },
                "questions": {
                    "value": "Please see the above weakness for the questions to be addressed. Please correct me during rebuttal, if there is any misunderstanding."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission126/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698780979203,
            "cdate": 1698780979203,
            "tmdate": 1699635938011,
            "mdate": 1699635938011,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Ut85KOmRYA",
                "forum": "WGP2pHtLtn",
                "replyto": "x4LvnW46Jc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission126/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission126/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer K5DW  (Part1)"
                    },
                    "comment": {
                        "value": "Dear Reviewer,\n\nWe appreciate your concerns regarding SOTA models and their multiview strategy. It is a critical perspective to understand our contribution. The updated part has been underlined green.\n\nSo in addressing the Reviewer's comment, we aim to elucidate the unique aspects of our approach in the context of SSL (Self-Supervised Learning) architectures. Our work diverges from methodologies like DINO[1] and SwAV[2], which adopt a multi-cropping strategy, as it does not propose a new architectural baseline. Instead, it focuses on integrating a multiview strategy, an effective and integral component, within the framework of contrastive learning. Our research is more inclined towards functionality, addressing an unexplored area in current research.\n\nParticularly, upon reviewing existing methods, we observe a notable lack of discourse, both theoretical and empirical, regarding such multi-view strategies. Our research aims to strike a balance between achieving state-of-the-art (SOTA) performance and enhancing the robustness, generalizability, and interpretability of the multiview strategy. The balance is essential in advancing the field, and our work contributes significantly to this objective.\n\n\n### Response to W1, Q1\n\nIt's indispensable to compare with state-of-the-art (SOTA) architectures. Accordingly, we have updated, reiterated, and supplemented our data to more clearly demonstrate the efficacy of our component:\n1. **Directly Compare:** We compared our method with multi-cropping and other strategies, observing a clear advantage in our approach (refer to Table 1 for details).\n2. **Change Strategy on Dino and SwAV**: Applying our strategy to these methods demonstrated accuracy comparable to multi-cropping, though this is likely coincidental (for the underlying reasons, please refer to Appendix 8.3).\n3. **Enhanced Baseline vs.  (DINO, SwAV)**: Comparing our improved baseline with DINO and SwAV,  we noted that methods enhanced by our strategy generally match even higher than these architectures (please see Table 2 for details)\n\n### Response to W2\n\nIn our paper, to clarify and compare loss function, we follow the notations of existing methods. Since our multi-view strategy is a component for assembling multiple views, it does not involve specific metrics (e.g., cosine similarity). For instance, \n\nIn SwAV[2]:\n$$L\\left(\\mathbf{z}\\_{t_1}, \\mathbf{z}\\_{t_2}, \\ldots, \\mathbf{z}\\_{t_{V+2}}\\right)=\\sum_\\{i \\in\\{1,2\\}} \\sum_\\{v=1}^{V+2} \\mathbf{1}_\\{v \\neq i} \\ell\\left(\\mathbf{z}\\_{t_v}, \\mathbf{q}\\_{t_i}\\right)$$\n\nIn CMC[3]:\n$$\\mathcal{L}\\_F=\\sum\\_{1 \\leq i<j \\leq M} \\mathcal{L}\\left(V_i, V_j\\right)$$\n\nIn our Paper,\n$$L\\left(\\mathbf{z}\\_{1}, \\mathbf{z}\\_{2}, \\ldots, \\mathbf{z}\\_{n}\\right)=\\sum\\_{i=1}^{n} \\ell\\left(\\mathbf{z}\\_{mean}, \\mathbf{z}\\_{i}\\right) \\; \\;\\; \\text{where} \\; \\; z_{mean}=\\sum\\_{i=1}^{n}z_{i}/n $$\nFurthermore, as illustrated in Table 1, we provide a comprehensive summary of nearly all existing multiview methods, including SwAV. This table facilitates a comparison of their accuracy, loss functions, and underlying design, supplemented by simple illustrations.\nWe wish to emphasize that our method primarily derives its motivation from critically reviewing the inherent biases in existing contrastive learning approaches(e.g. InfoMin[4]), leading us to design a multiview strategy aimed at mitigating these biases. In contrast, multi-cropping largely draws inspiration from blending local and global perspectives, which is a more intuitive approach. While this strategy is employed in DINO and SwAV, as expressed in your W3 concerns, our paper addresses numerous multiview-specific questions like \u201cDoes the proposed multiview contrastive learning simply benefit from more 'effective epochs'?\u201d. These discussions are crucial for the SSL community, contributing to the advancement of model interpretability and robustness."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission126/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700387317224,
                "cdate": 1700387317224,
                "tmdate": 1700389147023,
                "mdate": 1700389147023,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "L2fYXSJ8V0",
                "forum": "WGP2pHtLtn",
                "replyto": "8niIkuAOfD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission126/Reviewer_K5DW"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission126/Reviewer_K5DW"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your response"
                    },
                    "comment": {
                        "value": "Thanks for the response, which somehow addressed some of my concerns. However, I am still confused of explicit loss function used in the paper, as $\\ell$ is not properly defined. The novelty is also limited without comparison with other multi-view methods such as SwAV (e.g., Table 2, 3). I am afraid after taking into account these issues, I may not be able to increase my score."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission126/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700626039089,
                "cdate": 1700626039089,
                "tmdate": 1700626039089,
                "mdate": 1700626039089,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "XWMPlzIr0p",
            "forum": "WGP2pHtLtn",
            "replyto": "WGP2pHtLtn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission126/Reviewer_z9y7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission126/Reviewer_z9y7"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a \"plug-and-play\" approach to multi-positive-views learning, seamlessly integrating with existing two-view self-supervised learning (SSL) architectures. The authors challenge traditional assumptions about multiview learning and explore its complexities. The proposed method incorporates multiple positive views to enhance traditional SSL models, improving accuracy and speed across various benchmarks and SSL architectures."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- This paper explores the complexities of multi-positive-views learning and provides an alternative way to understand multiview learning.\n- Extensive experiments support the effectiveness of multiview learning.\n- The paper is well-organized and easy to follow."
                },
                "weaknesses": {
                    "value": "- Although the proposed strategy (Eq. 11) is an alternative way for multiple positive view contrastive learning, its novelty is limited.\n- Extensive experiments are conducted. However, I can hardly find insights different from previous multiple positive view contrastive learning methods.\n- In Table 2, the training epochs for each setting are not clear. If all methods share the same training epoch, the comparison is not fair since 4-view models observe more data than 2-view models."
                },
                "questions": {
                    "value": "- Could you please highlight unique insights different from existing multi-positive view methods?\n- In Table 2, do all the methods share the same training epochs? If yes, could you please conduct additional 2-view experiments with double training epochs for fairness?\n- In Figure 7, could you please explain why GPU usage decreases as the number of views increases?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission126/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission126/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission126/Reviewer_z9y7"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission126/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698819005301,
            "cdate": 1698819005301,
            "tmdate": 1699635937944,
            "mdate": 1699635937944,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "XDRIiLuLJj",
                "forum": "WGP2pHtLtn",
                "replyto": "XWMPlzIr0p",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission126/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission126/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to reviewer z9y7 (Part 1)"
                    },
                    "comment": {
                        "value": "Dear Reviewer,\n\nThank you very much for your insightful questions. In response, we have conducted additional experiments and updated our draft accordingly. We hope that these enhancements, along with our answers to your queries, will effectively address and resolve confusion. The updated part has been underlined green.\n\n\nBefore delving into the main topic, we summarize major multiview strategies in recent years. Our aim is to clarify misconceptions in this area, which, despite seeming under-researched, indeed has several important gaps. The key multiview papers have been summarized as follows:\n\n\n| Name    | Year | MultiView Strategy Analysis                                                                      | Underlying Motivation                         |\n|---------|------|--------------------------------------------------------------------------------------------------|----------------------------------------------|\n| VICRegL[1] | 2022 | Multiple sub-views from 2 'main views' contrasted via distance weighting                         | Multi-view: Distance-based feature vector learning |\n| DINO[2]    | 2021 | Multi Cropping: Sum of loss functions for 2 'main views' and multiple sub-views via Multi Cropping | Averaging teacher and student models, Multi-view: Global and local feature matching |\n| SwAV[3]    | 2020 | Also Multi Cropping                                                                              | Cluster-based, Multi-view: Global and local feature matching |\n| CMC[4]     | 2019 | Any number of views, aggregate of pairwise loss functions                                       | Based on CPC, Multi-view: Providing data sufficiency value |\n\nAdditionally, MIB[5] introduced a multi-view loss function based on the Information Bottleneck principle, While MIB did not extend beyond two views successfully, it was the first theoretical work to discuss multiview strategies, laying the groundwork for our research.\n ###  Response to W1, W2, Q1\n \n The multiview methods from these articles outlined in the table, in a strict sense, are more akin to intuitive designs for enhancing existing features, rather than being comprehensive strategies. In section 3.2, we discussed why two views (and other multiview strategies)will not work. A true multiview strategy, which goes beyond the theoretical limitation of two views, necessitates:\n\n\n1. **Firstly, not all multiview strategies benefit from advantages of \"multiview\" we introduced in the paper**: We have discussed the intrinsic invisible and view-specific bias that occur in two views case. However, this also exists in most of the previous multi-view designs.\n\n2. **Independent Discussion on Multiview Strategy**: This includes a focus on robustness and generalization capabilities. We conducted extensive experiments across various datasets and models, engaging in a thorough comparative analysis of different strategies.\n3. **Interpretability**: As a crucial aspect of deep learning, our approach delineates a clear and logical progression from the 'why' to the 'how' of multiview strategies. Our loss function capitalizes on the multiview framework to stabilize the minimum sufficient bias, representing a significant advancement beyond a simple summation.\n4. **Scalability**: Our method's adaptability to multiple baselines not only enhances accuracy but also helps mitigate the risk of model collapse.\n\nIn conclusion, for researchers seeking to enhance or develop architectures utilizing multiview strategies, our study provides a substantial theoretical framework and compelling empirical evidence.\n\n### Response to W3, Q2: Experiment Setup Enhancements\nWe acknowledge the points raised and have accordingly augmented our experimental setup in three key respects:\n\n1. **Equal Training Epochs**: To ensure a fair comparison of model performance, we maintained uniform training durations across all models. The outcomes are detailed in Table 2.\n2. **Equal Data Exposure with Consistent Training Views**: To remove potential biases due to varying view exposures, we adopted consistent training views across all models. Detailed results can be found in Table 3.\n3. **Efficiency in Achieving Specific Convergence Accuracy**: This metric provides valuable insights into the relative time efficiency of our method in comparison to others. Detailed information is available in Table 3.\n\nFor more detailed explanations, please refer to the third point of our response to Reviewer K5DW."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission126/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700386262908,
                "cdate": 1700386262908,
                "tmdate": 1700389130600,
                "mdate": 1700389130600,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zTykXfUnw6",
                "forum": "WGP2pHtLtn",
                "replyto": "XWMPlzIr0p",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission126/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission126/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to reviewer z9y7 (Part 2)"
                    },
                    "comment": {
                        "value": "### Response to Q3: GPU Usage\n\nIn an ideal scenario, a rudimentary formula for GPU usage decomposition is:\n$$\\text{GPU Usage} = N_{\\text{views}} \\times (C_{\\text{augmentation}} + C_{\\text{forward}}) + C_{\\text{loss}} + C_{\\text{backward}} + C_{\\text{update}}$$\nThis formula indicates that the model's weight and batch size have a linear influence on GPU Usage. Under consistent configurations, GPU usage for each computational task should remain nearly constant. However, our review of debug data from previous experiments revealed some anomalies, possibly due to environmental factors, system load, or randomness.\n\nTo ensure a more uniform and reliable testing environment, we repeated each test five times for every view count, discarding outliers and calculating the mean. This approach is based on the assumption that GPU usage for the same process should be consistent. We have updated our results in Figure 7.\n\nReference:\n\n[1] Bardes et al., 2022b \"Vicregl: Self-supervised learning of local visual features.\"\n\n[2] Caron et al., 2021 \"Emerging properties in self-supervised vision transformers.\"\n\n[3] Caron et al., 2020 \"Unsupervised learning of visual features by contrasting cluster assignments.\"\n\n[4] Tian et al., 2020a \"Contrastive multiview coding.\"\n\n[5] Federici et al., 2020 \"Learning robust representations via multi-view information bottleneck.\""
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission126/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700386277287,
                "cdate": 1700386277287,
                "tmdate": 1700389077115,
                "mdate": 1700389077115,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5sl8v3tjXb",
                "forum": "WGP2pHtLtn",
                "replyto": "zTykXfUnw6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission126/Reviewer_z9y7"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission126/Reviewer_z9y7"
                ],
                "content": {
                    "title": {
                        "value": "Response by Reviewer z9y7"
                    },
                    "comment": {
                        "value": "Thank you for addressing some of my concerns in your response. However, my primary issue remains regarding the novelty of the approach, particularly when compared to existing multi-view methods. The experiments in Table 1 tend to highlight the effectiveness of your approach compared with other multi-view methods. However, the experimental setup lacks clarity, which is a significant issue given the importance of this comparison. Additionally, the choice of the CIFAR-10 dataset for evaluating SSL methods is unconventional. Considering these points, I decided to maintain the original score."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission126/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700637559677,
                "cdate": 1700637559677,
                "tmdate": 1700637559677,
                "mdate": 1700637559677,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]