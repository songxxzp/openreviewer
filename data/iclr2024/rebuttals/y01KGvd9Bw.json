[
    {
        "title": "DreamLLM: Synergistic Multimodal Comprehension and Creation"
    },
    {
        "review": {
            "id": "o0PXDyZuPH",
            "forum": "y01KGvd9Bw",
            "replyto": "y01KGvd9Bw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1397/Reviewer_jWvU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1397/Reviewer_jWvU"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a framework that unifies generation of text and images. Specifically, it utilizes a newly introduced <dream> token to encode the representations that will later be forwarded to the diffusion model to decode to images. The authors have conducted extensive experiments across multiple tasks and benchmarks to showcase the ability of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper proposes a unified and promising framework for multimodal generation, strong performances are reported. The proposed approach to integrate diffusion models with LLM seems reasonable and could inspire following works in this area."
                },
                "weaknesses": {
                    "value": "The major concern I have is the necessity to utilize the token from LLM for image decoding. What is is going to be if you let the LLM to first output the image description, then extract it and feed it directly to the diffusion model? In this case, the original text encoder of diffusion models are leveraged. The results in Table 2 show that the specialists still outperform dreamLLM, which means the above naive alternative could potentially perform better? In addition, is it possible that directly use output text can also alleviate the loss of LLM's original power?"
                },
                "questions": {
                    "value": "See weakness ."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1397/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1397/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1397/Reviewer_jWvU"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1397/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698564781079,
            "cdate": 1698564781079,
            "tmdate": 1700623905423,
            "mdate": 1700623905423,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Nv8tKjONFs",
                "forum": "y01KGvd9Bw",
                "replyto": "o0PXDyZuPH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1397/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1397/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer jWvU (Part #1)"
                    },
                    "comment": {
                        "value": "Thanks for your positive, insightful reviews and your acknowledgment of our contribution to the community! We respond to your major concerns as follows.\n\n> **Q1:** What is it going to be if you let the LLM first output the image description, then extract it and feed it directly to the diffusion model? ... In addition, is it possible that directly using output text can also alleviate the loss of LLM's original power?\n\n**Analysis of rewrite-then-generate strategy.** Thank you for pointing out this strategy, which has been used by the recently released DALLE-3 [1] based on GPT-4 (released less than a week before the Sep 28'23, ICLR'24 submission deadline). We agree that this `rewrite-then-generate` strategy has the advantage of using textual prompts that may be better suited for specific image synthesis decoders (including DreamLLM). However, there are some challenges and limitations that are non-trivial to overcome:\n\n- **Lacking in data.** The first and the major challenge is the serious lack of training data. In order to train a Large Language Model (LLM) to generate preferred prompts for an image decoder, a dataset specifically tailored to this need is required. For instance, DALLE-3 necessitates the initial training of a bespoke image captioning specialist capable of producing high-quality descriptive captions, followed by model training in a data-rich environment featuring these written captions. This process is far from trivial, hinging heavily on the availability of substantial volumes of high-quality data. However, the interleaved MMC4 dataset lacks alternative versions of rewritten prompts, offering only original internet content. Additionally, different image decoders may perform optimally with different prompts. The collection or labeling of such datasets is a non-trivial, costly endeavor, particularly for the research community.\n- **Limitation in Image-Conditional Image Generation Scalability.** A further limitation lies in the model's inability to scale efficiently in the context of image/document conditional image generation. Given that the image decoder only receives text inputs, no information or details about conditional images can be parsed to serve as conditions for image synthesis. This limitation becomes evident when attempting subject-driven image generation. Please refer to **Appendix A.4, Fig.8, page 25** for our newly conducted **subject-driven image generation** experiments. Moreover, when applied to document-conditional image generation, the text may be excessively lengthy for effective image synthesis. For instance, the maximum token length limitation for the original Stable Diffusion is a mere 77. Furthermore, while prompts generated in context may be suitable for image synthesis, they might appear incongruous when featured in specific documents, thereby disrupting the document's coherency. \n- **LLMs as agents?** We agree that specialist image decoders can be used if we just view the LLMs as agents to call different APIs, but it has its limitations. For example, the data lacking issues and model limitations, as mentioned before, remain. Furthermore, the key limitation may be the **non-end-to-end** pipeline. While tasks can be executed effectively, this disjointed training strategy offers no assurances for potential learning synergies between multimodal comprehension and generation. In principle, this method echoes the Flamingo-like models, which are limited to language-only output. This differs from the **research motivation and purpose** of our work and exploration. Essentially, our exploration of DreamLLM has unveiled the significant potential of LLMs to attain a comprehensive understanding of multimodality that **genuinely comprehends modalities beyond mere language**. It is **novel and presents a promising solution for future-generation MLLMs**, particularly when scaling up LLMs or enhancing the training data (making it more abundant, cleaner, and larger) becomes feasible."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1397/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700601179917,
                "cdate": 1700601179917,
                "tmdate": 1700601968025,
                "mdate": 1700601968025,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zHB4QnlADG",
                "forum": "y01KGvd9Bw",
                "replyto": "o0PXDyZuPH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1397/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1397/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer jWvU (Part #2)"
                    },
                    "comment": {
                        "value": "**Additional experiments.** Following your instructions, we endeavored to train a model using the proposed `rewrite-then-generate` strategy. Given the absence of an optimal dataset, we modify the original MMC4 by using <dream> start \\& end tokens before and after the specific text prompt that has the highest CLIP similarity to a specific image, which can be used as text prompts for image generation. In this setting, we only train the LLMs to output texts, and no image decoders are involved during training. During inference, when the model outputs texts encompassed by the <dream> tokens, the texts are used for an off-the-shelf SD image decoder for generating images. After training, we test the model's language processing and multimodal capabilities.\n\n- **Language processing.** The results in the following table show that the `rewrite-then-generate` achieves similar performance to DreamLLM. This demonstrates that both methods won't impact the language capability, which is as expected.\n\n| Method                  | PIQA     | SIQA     | HellaSwag | WinoGrande | BoolQ    | MMLU     |\n| :---------------------- | -------- | -------- | --------- | ---------- | -------- | -------- |\n| Vicuna Baseline         | 77.7     | 47.5     | 75.7      | 67.5       | 73.9     | 45.0     |\n| DreamLLM                | **78.6** | **48.8** | **77.4**  | **68.5**   | 75.2     | 41.8     |\n| `rewrite-then-generate` | 78.2     | 48.5     | 75.8      | 68.3       | **77.4** | **43.1** |\n\n- **Multimodal comprehension and creation.** The results presented in the subsequent table clearly indicate that the performance of the `rewrite-then-generate` baseline falls short when compared to DreamLLM, particularly in the context of text-to-image generation on the COCO dataset. This underlines the efficacy of the synergistic learning approach inherent in DreamLLM, suggesting its potential superiority over the baseline methodology.\n\n| Method                  | VQAv2 | MM-Vet | COCO  |\n| :---------------------- | ----- | ------ | ----- |\n| DreamLLM-7B             | 56.6  | 35.9   | 8.46  |\n| `rewrite-then-generate` | 54.2  | 34.1   | 11.91 |\n\n\n> **Q2:** The results in Table 2 show that the specialists still outperform DreamLLM, which means the above naive alternative could potentially perform better?\n\n- **Comparison.** The other state-of-the-art text-to-image specialist models such as Imagen-3.4B and Parti-20B have **different image decoder architectures** and are trained on **broad in-house data**. Besides, these models are **not open-sourced**, and thus, we cannot leverage them as our DreamLLM image decoder. However, compared to these SOTA models, our DreamLLM still demonstrates strong performance, successfully improving the SD2.1 baseline to be comparable to these powerful specific models. \n- **Clarification.** We want to clarify that in this work, we mainly build DreamLLM based on **open-sourced SD2.1 baseline**. Compared to the SD2.1 baseline, Table 2 demonstrates that DreamLLM brings **a significant improvement of 3.97/13.73 FID on MS-COCO/LN-COCO, respectively**. \n- **Scalability of DreamLLM.** Our DreamLLM is a general learning framework that does not rely on specific image decoders or LLMs architectures. Therefore, it reveals the great potential and scalability of DreamLLM when incorporating more powerful image decoder baselines or stronger LLMs, if available.\n\n[1] Betker et al. Improving Image Generation with Better Captions. OpenAI."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1397/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700601267525,
                "cdate": 1700601267525,
                "tmdate": 1700602106513,
                "mdate": 1700602106513,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HIGAcSkxaI",
                "forum": "y01KGvd9Bw",
                "replyto": "zHB4QnlADG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1397/Reviewer_jWvU"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1397/Reviewer_jWvU"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors"
                    },
                    "comment": {
                        "value": "I appreciate the authors' effort in conducting the comprehensive experiments during rebuttal period. I strongly recommend the authors to add those experiments to the main paper in their final version. Given that the authors have addressed most of my concerns, and the paper indeed proposes a novel approach in multimodal learning, I raise my score to leaning acceptance for this paper."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1397/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700623872655,
                "cdate": 1700623872655,
                "tmdate": 1700623872655,
                "mdate": 1700623872655,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Lq5NctUOaD",
            "forum": "y01KGvd9Bw",
            "replyto": "y01KGvd9Bw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1397/Reviewer_t2eT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1397/Reviewer_t2eT"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a framework to allow multimodal large language models combining multimodal comprehension and generation. To enable image generation, instead of fitting CLIP feature, this work directly optimizes the diffusion model's objective to achieve modeling multimodal posteriors. The training pipeline is comprised of three stages: alignment pretraining, interleaved generative pretraining, and supervised finetuning. Extensive experiments on multimodal comprehension and creation have shown the superiority of the proposed model."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This model proposed a unified framework for joint multimodal comprehension and generation which shows impressive performance on various tasks, demonstrating the benefits of synergizing these two tasks.\n2. The usage of score distillation avoids the possible information loss to greatly improve the image generation ability.\n3. The proposed training pipeline enables the free-form interleaved generative ability of multimodal models.\n4. The experiments are comprehensive and convincing."
                },
                "weaknesses": {
                    "value": "1. For free-form interleaved generation, it is important to ensure the consistency between related images. However, as we can view the model as replacing the CLIP text encoder of stable diffusion with a much more powerful LLM for the image generation aspect, the control of the generated image is still limited. As we can see from the Figure 3, the phones in generated samples have large discrepancy.\n2. The paper does not demonstrate the in-context comprehension ability of the model.\n3. Ablation studies on the choices, combination ways, and the importance of filtering process of the training datasets are not shown, which might provide insights for future study."
                },
                "questions": {
                    "value": "Will including samples from training datasets as in-context examples improve the performance during evaluation?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1397/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1397/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1397/Reviewer_t2eT"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1397/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698815033306,
            "cdate": 1698815033306,
            "tmdate": 1699636067450,
            "mdate": 1699636067450,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zJGbUpcZvv",
                "forum": "y01KGvd9Bw",
                "replyto": "Lq5NctUOaD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1397/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1397/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer t2eT"
                    },
                    "comment": {
                        "value": "Thanks for your solid and positive reviews, which make significant contributions to our manuscript revision, and we are glad that you're pleased with our work! We respond to the Weakness points as follows, which have been incorporated in the revision.\n\n> **W1:** The control of the generated image is still limited considering consistency between images in interleaved generation.\n\n**Clarification.** We highly appreciate your insights into this perceived limitation. In fact, the proposed DreamLLM model architecture is able to generate highly consistent images. However, the images in the interleaved MMC4 dataset used to train DreamLLM inherently have low consistency as it is collected from the noisy Internet. This factor could potentially compromise the model's ability to fully manifest an emergent image consistency property.\n\n**Evidence from subject-driven generation.** To substantiate our assertion that data with superior image consistency can fully leverage the capabilities of DreamLLM in generating consistent images, we have conducted additional experiments focusing on subject-driven image generation. This type of image generation necessitates the model to comprehend intricate instructions while concurrently preserving the subject identity features with utmost precision. Our DreamLLM has been fine-tuned for image-to-image generation in the third stage of SFT using the same data employed by BLIP-Diffusion [1], a recently proposed model in the domain of subject-driven generation. As this data set is not publicly available, we have constructed it independently based on the procedures outlined in the original paper. The results are shown in **Appendix A.4, Fig.8, page 25**. The results compellingly validate the effectiveness and great potential of applying DreamLLM for image-consistent generation.\n\n[1] Li et al. BLIP-Diffusion: Pre-trained Subject Representation for Controllable Text-to-Image Generation and Editing. In NeurIPS 2023.\n\n> **W2:** Will including samples from training datasets as in-context examples improve the performance during evaluation?\n\nIndeed. We extend our gratitude for your insightful suggestion! Please refer to our [General Response](https://openreview.net/forum?id=y01KGvd9Bw&noteId=xe4TvtyFfS) for the in-context learning and few-shot experimental results, the results demonstrate the powerful in-context learning and few-shot learning capabilities of DreamLLM. \n\n> **W3:** Ablation studies on training datasets are not shown.\n\nThanks for your suggestions! Due to time limitations, it is impractical to perform a comprehensive ablation study encompassing all training datasets and filtering strategies, as such an undertaking may indeed exceed the scope of the current research. However, during rebuttal, we have conducted some ablation studies on instruction-following data and training data. We kindly refer you to our response to **Q1.a of Reviewer h8i6** for details. In addition, please note that the filtering strategies on datasets such as MMC4 are basically intuitive. During our manual review of the data, we identified several instances of misalignment between images and their corresponding text, which served as an impetus for our filtering approach. It is our future work to identify and resolve all data-related discrepancies."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1397/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700600954746,
                "cdate": 1700600954746,
                "tmdate": 1700601933756,
                "mdate": 1700601933756,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "i8P3oEz0Mj",
                "forum": "y01KGvd9Bw",
                "replyto": "zJGbUpcZvv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1397/Reviewer_t2eT"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1397/Reviewer_t2eT"
                ],
                "content": {
                    "title": {
                        "value": "Responce to Authors"
                    },
                    "comment": {
                        "value": "Thanks for your feedback and additional results. I would like to keep my original rating."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1397/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700678184064,
                "cdate": 1700678184064,
                "tmdate": 1700678184064,
                "mdate": 1700678184064,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gBY05xIDdm",
                "forum": "y01KGvd9Bw",
                "replyto": "Lq5NctUOaD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1397/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1397/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you!"
                    },
                    "comment": {
                        "value": "Dear Reviewer t2eT,\n\nWe sincerely appreciate your positive review and acknowledgment of our response!\n\nBest Regards,\\\nAuthors"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1397/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700716703896,
                "cdate": 1700716703896,
                "tmdate": 1700716859914,
                "mdate": 1700716859914,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "YqLz0OxUjV",
            "forum": "y01KGvd9Bw",
            "replyto": "y01KGvd9Bw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1397/Reviewer_h8i6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1397/Reviewer_h8i6"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces \"DREAM LLM\", a novel learning framework designed to enhance Multimodal Large Language Models (MLLMs). The model is designed to 1) The model late-fused three pretrained models, a text-LLM, an image generation model (SD), a CLIP model. Unlike conventional methods that utilize intermediate image representations, DREAM LLM leverages score distillation techniques with a diffusion image generation model inputs raw data modalities and outputs them in the same format. 2) The model is also trained on interleaved multimodal corpora sourced from the internet. \nThis leads to superior performance in several benchmarks and the ability to generate free-form interleaved content."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The model late-fused three pretrained models, a text-LLM, an image generation model (SD), a CLIP model. With the designed dream query and score distillation with SD model, the model is superior than earlier models that utilize intermediate image representation. \n2. What's more, the model is trained on carefully collected various datasets, including, various text-image datasets, interleaved multimodal corpora sourced from the internet, instruction datasets. With joint-training, it also shows synergy of text and image, understanding and creation. \n3. The model shows very comprehensive experiment results in various benchmarks like MS-COCO, MMBench, and MM-Vet, in different setup zero-shot understanding, image generation, interleaved generation, etc."
                },
                "weaknesses": {
                    "value": "1. The model is trained on very rich data, and it is not clear how does those data contribute to the zero-shot evaluation. \n\na. several dataset used by the model are derived from COCO datasets, e.g Laion-COCO, LLaVaInstruct, etc. How do we know if there is data leakage in the training datasets. This applies to the results in table 1, as well as in table2. \n\nb. Is the model in table 1 after instruction tuning? \n\nc. if you continue training SDv2.1 with the collected dataset, what are the MS-COCO, and LN-COCO FID number?  \n\n2. Some model details are not clear. \n\na. how is the multi token dream query implemented. For decoder-only transformer training, every token needs a loss (or a score). What us the loss of each query token during training, and are they generated sequentially or altogether during inference?\n\nb. in the interleaved multimodal joint training, for the second/third image generation, do they condition on both the image1 dream query and image1 visual encoder? Or just image1 visual encoder. \n\nc. for the stage1, stage2, stage3, what's the final loss? Do they both have L_DM (formula 5) and L_MLLM (formula 6)? Is there a weight? \n\nd. for the I-GPT training, does the visual projector, condition projector, dream embedding get updated? or only the MLLM transformer got updated?\n\ne. in section 5.1, the L_CLIP is not clear. Which two embeddings are used to calculate the loss?"
                },
                "questions": {
                    "value": "1. Can the model do k-shot learning for image understanding task?\n2. table4 shows amazing results that the model is better at text task than the pretrained text LLM, does the author have any hypothesis why?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1397/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698819928284,
            "cdate": 1698819928284,
            "tmdate": 1699636067380,
            "mdate": 1699636067380,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "idBwNItcpJ",
                "forum": "y01KGvd9Bw",
                "replyto": "YqLz0OxUjV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1397/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1397/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer h8i6 (Part #1)"
                    },
                    "comment": {
                        "value": "Thank you for your constructive and supportive reviews that greatly improve our manuscript! We respond below to your questions and concerns, which we have incorporated into the revised version.\n\n> **Q1.a:** Is there data leakage?\n\n| Stage | Objective     | Trainable                              |        Dataset        |   Image Source    | Data Size |                          Reference                           |\n| ----- | :------------ | -------------------------------------- | :-------------------: | :---------------: | :-------: | :----------------------------------------------------------: |\n| I     | Comprehension | visual projector                       |    `LLaVAPretrain`    |      `CC-3M`      |   558K    | [URL](https://huggingface.co/datasets/liuhaotian/LLaVA-CC3M-Pretrain-595K) |\n|       | Comprehension | visual projector                       |      `LAION400M`      |    `LAION400M`    |    11M    |     [URL](https://laion.ai/blog/laion-400-open-dataset/)     |\n|       | Creation      | condition projector + dream embeddings |      `LAION400M`      |    `LAION400M`    |    11M    |     [URL](https://laion.ai/blog/laion-400-open-dataset/)     |\n|       | Creation      | condition projector + dream embeddings |     `BLIP-LAION`      |    `LAION400M`    |    8M     | [URL](https://github.com/salesforce/BLIP#pre-training-datasets-download) |\n|       | Creation      | condition projector + dream embeddings |     `LIAON-COCO`      |   `LAION2B-EN`    |    11M    |           [URL](https://laion.ai/blog/laion-coco/)           |\n| II    | Both          | ALL (including LLM)                    |        `MMC4`         |      `MMC4`       |    2M     |            [URL](https://github.com/allenai/mmc4)            |\n|       | Creation      | ALL (including LLM)                    |     `BLIP-LAION`      |    `LAION400M`    |    8M     | [URL](https://github.com/salesforce/BLIP#pre-training-datasets-download) |\n| III   | Both          | ALL (including LLM)                    |    `InstructMMC4`     |      `MMC4`       |    20K    |                             N/A                              |\n|       | Creation      | ALL (including LLM)                    | `Instruct-BLIP-LAION` |    `LAION400M`    |    20K    |                             N/A                              |\n|       | Comprehension | ALL (including LLM)                    |    `LLaVAInstruct`    | `COCO train 2017` |    80K    | [URL](https://github.com/haotian-liu/LLaVA/blob/main/docs/Data.md) |\n\nTo answer this question, we revisit the datasets used in each training stage, as detailed in the previous table, to enhance comprehension of training data utilization.\n\n**Comprehension. (Tab. 1)** The provided table clearly demonstrates that only `LLaVAInstruct`, utilized during Stage III SFT training, incorporates images from `COCO train 2017`. However, several key points should be noted:\n\n- Only images from the `train` split are used, which have no overlap with the images used in evaluations. This means **no images in the evaluation benchmarks are leaked**.\n\n- `LLaVAInstruct` is constructed by LLaVA using GPT-4. It's **different from any of the training/validation data used in our evaluations like VQAv2**. It's worth noting that LLaVA and Emu, which we're comparing, also use this `LLaVAInstruct` dataset during SFT.\n\n- Following the reviewers' suggestions on a further ablation study of the effect of `LLaVAInstruct` during Stage III SFT training, we conduct Stage III SFT by replacing the instruction-tuning comprehension dataset `LLaVAInstruct` with an instruction-following data that truly covers some VQA training datasets in our evaluations (other SFT datasets are the same). To this end, we use `LLaVAInstruct 1.5`, which is a new visual instruction-following dataset constructed by LLaVA 1.5 [1] very recently (post Sep 28'23, ICLR'24 submission deadline). This is collected by mixing a variety of QA, VQA, conversation, and other type datasets into instruction-following conversations, including VQAv2 and OKVQA, that are used in our evaluations. Under this setting, the results will not be zero-shot for VQA on VQAv2 and OKVQA. \n\n  The ensuing table presents the results of our study. These results compellingly illustrate that for both VQAv2 and OKVQA, the application of the training dataset leads to a substantial enhancement in accuracy. Besides, it is noteworthy that a remarkable improvement is achieved on datasets like VizWiz that consist of substantially different images from COCO (i.e., images taken by blind people with the VizWiz software).\n\n  | Instruct Dataset           | VQAv2 | VizWiz | OKVQA | TextVQA | MM-Vet |\n  | -------------------------- | :---: | ------ | :---: | :-----: | :----: |\n  | `LLaVAInstruct` (80K)      | 56.6  | 45.8   | 44.3  |  34.9   |  35.9  |\n  | `LLaVAInstruct 1.5` (665K) | 72.9  | 49.3   | 53.3  |  41.8   |  36.6  |\n  | $\\Delta$ Acc.              | +16.3 | +3.5  | +9.0  |  +6.9   |  +1.0  |"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1397/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700599517888,
                "cdate": 1700599517888,
                "tmdate": 1700599517888,
                "mdate": 1700599517888,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]