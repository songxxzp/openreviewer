[
    {
        "title": "Deepfake Caricatures: Amplifying attention to artifacts increases deepfake detection by humans and machines"
    },
    {
        "review": {
            "id": "5CmRvnGMJz",
            "forum": "fg772k6x6U",
            "replyto": "fg772k6x6U",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2227/Reviewer_iHin"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2227/Reviewer_iHin"
            ],
            "content": {
                "summary": {
                    "value": "This paper trains a model on human responses to generate attention maps that amplify artifacts in deepfake videos. They build two datasets of human labels highlighting areas perceived as fake."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Overall, the paper is generally well-written, clearly structured, and quite easy to follow. The main strength of the paper is detecting human-relevant artifacts and amplifying them to increase human detection. This task and the technical route are rarely explored."
                },
                "weaknesses": {
                    "value": "1. The words in Fig.1 are too small and hard to read, such as L_{human}.\n2. Currently, many deepfake methods are developed to identify fake faces and artifacts region. So, why do we still need to make artifacts more detectable to human observers? It is not clear in this paper.\n3. Lots of deepfake methods [1,2,3]  based on attention have been proposed. The author should compare the existing attention mechanisms and the attention used in this paper.\n\n[1] Multi-Attentional Deepfake Detection. CVPR2021\n[2] Detection of Deepfake Videos Using Long-Distance Attention. TNNLS2022\n[3] An Information Theoretic Approach for Attention-Driven Face Forgery Detection. ECCV2022"
                },
                "questions": {
                    "value": "See Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2227/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2227/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2227/Reviewer_iHin"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2227/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698119830663,
            "cdate": 1698119830663,
            "tmdate": 1699636156171,
            "mdate": 1699636156171,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "LdBnEOi4ha",
                "forum": "fg772k6x6U",
                "replyto": "5CmRvnGMJz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2227/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2227/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer iHin"
                    },
                    "comment": {
                        "value": "Thank you for your comments. We respond to your concerns below.\n\n## **Caption size in Figure 1.** \nWe will modify Figure 1 and increase the size of the font corresponding to the loss for the camera ready version.\n\n## **Need to make artifacts more detectable to human observers.**\nThe main objective of our paper is not only to detect fake faces and regions, but to make them _more obvious to human observers_. This helps humans detect fakes early, avoid being fooled and avoid consuming fake information. The core motivation for this, as we describe in the second paragraph of our introduction and in Section 4, is that current methodologies for making a user aware that a video is fake rely on text labels in UIs (e.g. \u201cThis Video is Fake\u201d, \"Video modified by AI\"), and this is suboptimal - easily missed, hardly believed, as many studies show (Introduction, paragraph 2, line 3). Making artifacts more detectable to human observers is key to increasing the chances that humans detect fakes: in fact, our proposed deepfake caricature visual indicator almost _doubles_ human detection performance when compared to the unaided condition, and increases it by 20.5% when compared to a text-based indicator (Section 4, Paragraph 2). This further confirms the necessity for evidencing artifacts to humans instead of just showing text labels. The potential societal impact of this is large: our experiments essentially show that humans choose to not trust a text-based indicator (e.g. \"Video modified by AI\") **22% of the time**, but only distrust our deepfake caricatures **6% of the time**.\n\nOur paper showcases, through several experiments, that humans are _less easily fooled_ when a technique that amplifies artifacts is used to show them that a video is fake. We hope this clarifies the reason (and the importance) around why we choose to exacerbate artifacts for humans in this work.\n\n## **Comparison to attention-based deepfake detection methods**\nWe already showcase a performance comparison to the attention-based method most aligned with our framework in Table 4 (Shiohara 2022). We additionally provide comparisons to [1] in Table 1, row 7. Further, we bring comparisons with [2,3] in the Table below (cross dataset generalization, FF++ to CelebDF, showing AUC results). We will add these results in the supplemental. \n\n|                   | FF++  | CelebDF |\n|-------------------|-------|---------|\n| Long-Distance [2] | 99.97 | 70.33   |\n| Att-Driven [3]    | 96.94 | 77.35   |\n| CariNet (Ours)    | 99.8  | 88.8    |\n\nIn terms of functionality, the proposed attention mechanisms are substantially different from our own. Most rely on an internal measure of self consistency, while ours are sourced from large scale human annotations. Automatically generated heatmaps such as in [1, 2, 3] do not make compelling caricatures, as they highlight face boundaries or the whole swapped area, rather than individual artifacts (unlike our CariNet). Our ablation study shows the effect of simpler synthetic heatmaps (centered gaussian, Table 4) and heatmaps from a relevant work (Shiohara 2022, Table 4). Performance degrades in both cases when our artifact attention module is replaced. We will add short additional insights about attention mechanism differences to our related work section.\n\nWe hope these responses helped alleviate your concerns, and we are open to further discussion."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2227/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700638280486,
                "cdate": 1700638280486,
                "tmdate": 1700638317610,
                "mdate": 1700638317610,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "iqpKxSKd9V",
            "forum": "fg772k6x6U",
            "replyto": "fg772k6x6U",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2227/Reviewer_7ZWE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2227/Reviewer_7ZWE"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors propose to make deepfakes seems more 'fake' via the 'Deepfake Caricatures.' Specifically, the authors, relying on manual annotations and heatmaps, guide the model to learn the forged regions within the fake videos. Subsequently, a reconstruction module is applied to introduce additional noise and fluctuations into these regions, aiming to make it clear to viewers that the video is a forgery.\n\nIn general, this is an interesting research topic. However, I still have some concerns."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The proposed 'Deepfake Caricatures' is interesting in Deepfake Detection task.\n2. The authors construct an extensive dataset that contributes to both deepfake detection and localization tasks.\n3. The experiments validate the effectiveness of the proposed method."
                },
                "weaknesses": {
                    "value": "1. Application and Distinctiveness. The concept of 'Caricatures' in this paper aims to achieve an exaggerated appearance in forged videos. How can this 'Caricatures' concept be distinguished from genuine satirical videos? For instance, if an attacker uses another model trained on 'Caricatures' to introduce disturbances similar to the style in this paper into pristine videos, could it lead viewers to question the authenticity of the pristine videos? In other words, the approach presented in this paper may enhance the distinctiveness of certain fake videos compared to their corresponding real videos in the training dataset. However, does this also result in these fake videos being more challenging to distinguish from some other videos?\n\n2. More qualitative results should be presented in the main manuscript, even though the authors have provided numerous videos in the supplementary materials."
                },
                "questions": {
                    "value": "The primary concern to address is the effectiveness of the method, as discussed in the weaknesses. Does this method enable viewers to effectively distinguish forged videos?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2227/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698214730053,
            "cdate": 1698214730053,
            "tmdate": 1699636156072,
            "mdate": 1699636156072,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "V6tjnrZQBm",
                "forum": "fg772k6x6U",
                "replyto": "iqpKxSKd9V",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2227/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2227/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 7ZWE"
                    },
                    "comment": {
                        "value": "Thank you for your review and comments. We appreciate the recognition towards our dataset.\n\nWe respond to your comments below.\n### **Application and Distinctiveness.** \nCaricatures are substantially different from satirical videos: they aim at evidencing artifacts instead of generating a satirical depiction of a character. They work by amplifying defects, highlighted by our artifact attention module, across frame embeddings, to generate a magnified version of those defects. They have a particular \"uncannyness\" in their motion (as can be seen in our gallery) that makes them distinct from a human-crafted satirical video.\nAttackers could potentially generate disturbances similar to the ones exhibited by caricatures to confuse a viewer, but the video itself would be detected as fake by our classifier. Additionally, our caricature framework ensures that only detected fake artifacts are magnified, which reduces the probability that a real video passed through our pipeline results in distortions.\n\n### **Does our approach make fake videos more challenging to distinguish from other videos?**\nThis is an interesting question - our caricatures could be making fake videos more similar to in-the-wild videos through their distortion. We propose that this effect is a) very unlikely to occur, and b) preferable to having fake videos indistinguishable from their real counterparts. First, the distortion applied by our caricatures are very particular: because they stem from a magnification of embedding differences gated by an attentional heatmap, they have a very specific signature that makes them substantially different to other videos in the wild that have been distorted in one way or another. This means that generally, caricatures will not become indistinguishable from another random subset of videos. Second, in the unlikely scenario where they do become hard to distinguish from another set of real distorted videos, we argue that it is still preferable to have fake videos in that state than to disseminate the perfectly crafted fake, where the alteration is made precisely to fool people into thinking they represent real faces (more likely to spread misinformation).\n\n\n### **Qualitative Results**\nWe will ensure that the main manuscript showcases more qualitative examples - thank you for your suggestion. We will add a figure to the appendix presenting more examples. We do highlight, however, that the caricatures are much better appreciated in video form. This is why we decided to use the limited space in the main manuscript to properly showcase our experimental results and figures, leaving most of the visual qualitative exploration to a better suited web gallery.\n\nWe hope these comments help address your concerns."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2227/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700636392068,
                "cdate": 1700636392068,
                "tmdate": 1700636392068,
                "mdate": 1700636392068,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "4ztVWoTruw",
            "forum": "fg772k6x6U",
            "replyto": "fg772k6x6U",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2227/Reviewer_Rnom"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2227/Reviewer_Rnom"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a face forgery detection model incorporating artificial responses. The aim is to combine machine-learned features with human intuition. The authors use artificial intuition to feel distorted images, similar to an intuition-based data enhancement approach. The whole framework is mainly based on the self-attention mechanism."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The authors are to be commended for building attention maps by introducing human intuition. And the distorted faces are then generated using the generated attention map.\n- The authors introduce the generated attention map to the self-attention mechanism for better performance."
                },
                "weaknesses": {
                    "value": "- As the authors state, the primary contribution of the framework is the CARICATURE GENERATION MODULE. However, human intuition can be relatively biased, especially for visually hidden tampering. After all, one of the problems DeepFake detection research is trying to solve is to detect videos that humans can't distinguish.\n- The general framework of the method is not NOVEL enough and still uses the usual self-attention mechanism. The focus is on the generation and role of caricatures. And caricatures are more akin to a way of utilizing human abilities to provide auxiliary samples. I'm not sure this comes at a greater cost.\n- The experimental results are not solid enough, there are many new methods published in 2023 and the authors should refer to more recent work."
                },
                "questions": {
                    "value": "I noticed a similar paper [1] on arxiv. The two papers adopted the same method and framework, but there were some differences in experimental results. CariNet18 and CariNet34 in [1] should be similar to Carinet-S and CariNet in this paper. The results of CariNet(CariNet34[1]) in the two papers are somewhat different (the performance of this paper is better). However, the results of the ablation study of CariNet-S(CariNet18[1]) (Table 3) were completely consistent.\n\nMy question is what changes in the method have brought about the improvement of performance, and why CariNet has been improved while Carinet-S remains unchanged.\n\n[1] https://arxiv.org/abs/2206.00535"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No ethics review needed."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2227/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2227/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2227/Reviewer_Rnom"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2227/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698657307193,
            "cdate": 1698657307193,
            "tmdate": 1699636156001,
            "mdate": 1699636156001,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "q6X1Oq0uPS",
                "forum": "fg772k6x6U",
                "replyto": "4ztVWoTruw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2227/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2227/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Rnom"
                    },
                    "comment": {
                        "value": "Thank you for your review. We respond to your main concerns below.\n\n**Bias of human intuition.** We agree that human intuition is biased. We claim, however, that artifact annotations collected by many different humans over many videos _do_ provide strong signal to improve detection and generate effective caricatures. Most experiments in our paper aim at showcasing the usefulness of our artifact attention module and caricature module, both built on human annotations at scale. We repeatedly show the advantage of these human annotations: \n1. They can improve classification through supervising our artifact attention module, as CariNet beats 13 previous works over 4 datasets and 3 tasks. Our ablation studies in Table 4 further show that using the artifact attention module is better than not using it or replacing it with a different type of attention - crucially, using attention alternatives that are not derived from our human annotations degrades performance. \n2. They can help humans recognize deepfakes better: Section 4 and Figure 5 show that caricatures, which are a direct product of artifact attention heatmaps, greatly improve human accuracy at the task of recognizing fakes. This improvement is important: humans become 20.5% better at picking up deepfakes with our caricatures (and thus, with the help of our human annotations) than when exposed to text based indicators (e.g. \"THIS VIDEO IS FAKE\" or \"Video Modified by AI\")\n\nWe recognize that human intuition can be biased, but as we show in this work, using large quantities of human annotations of fake regions improves performance unequivocally, across tasks and across domains. We agree that this result may seem surprising, but we believe that that makes it even more important for the community.\n\n**Novelty.**\nOur framework introduces a larger variety of novelty points than the ones constrained to our attention mechanism. We present the following innovations:\n- a) The integration of artifact attention maps into self-attention structures by directly gating the keys, which to the best of our knowledge has not been previously introduced in any current attention mechanisms (Figure 4B).\n- b) The targeted embedding distortion framework using heatmaps (equation 3) which allows us to amplify specific defects in a video with precision, hijacking the residuals of contiguous time-sliced coded representations built by our encoder (Figure 2).\n- c) Our specific combination of human supervision with machine supervision, leveraging 4 different losses at two key points of the network (to the best of our knowledge, a novel combination of losses), before feeding back the output of one task (artifact attention heatmap reconstruction) as input to the other (classification).\n\nAdditionally, our paper introduces **two novel human annotation datasets** (artifacts labeled on DFDCp and FF++) totaling more than 20k annotations on well-known benchmarks, **novel visual indicators** (Caricatures) as well as the framework to make them, and **multiple human experiments** of our own design that showcase the utility of our caricatures. \n\nWe hope this helps shed light on the strength of our contributions and the novelty they exhibit, as they go beyond the architectural choice of one part of our backbone.\n\n**Focus on generation and role of caricatures.** We respectfully disagree with the statement that caricatures are closer to \"a way of utilizing human abilities to provide auxiliary samples\". First, our caricatures are not used as auxiliary samples: the classifier never sees caricatures, and instead leverages the artifact attention heatmaps produced by our artifact attention module (Figure 1A, Figure 4B). Second, our caricatures are much more than auxiliary samples: they are a novel visual indicator that is **20.5% more effective** at evidencing deepfakes to humans than text indicators (Section 4, paragraph 2). That is, when humans are confronted with a fake video, distorting that video with our caricature framework increases hit rates to **0.94** (vs. 0.78 in text based indicator conditions, p<0.0001 for both), making it much more likely that humans will correctly recognize it as fake than showing text that says \"THIS VIDEO IS FAKE\". We believe this result alone makes caricatures particularly impactful."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2227/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700630865675,
                "cdate": 1700630865675,
                "tmdate": 1700637443467,
                "mdate": 1700637443467,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "H3M2fImRVo",
            "forum": "fg772k6x6U",
            "replyto": "fg772k6x6U",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2227/Reviewer_qLFh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2227/Reviewer_qLFh"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a method to not only classify deepfake videos but also create caricatures to amplify the artifact area, so that fake video detection can be more easily interpreted by humans.  More specifically, the proposed method consists of three modules: 1) artifact attention module, which is guided by human artifact annotations and to predict artifact region in the video 2) A classifier to classify whether the video is fake or real 3) A caricature module by amplifying the representation difference of consecutive frames around the artifact area, and then applying motion magnification to generate caricatures videos. Experiments are conducted under different scenarios including generalization on unseen datasets, generalization on unseen forgery methods, robustness on unseen perturbations, all showing promising results compared to existing baselines. Moreover, some ablation studies are also provided."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "--The idea of predicting artifact regions and further amplify artifact regions to create caricatures for deepfake detection are quite interesting.\n\n--The proposed method with the three modules look quite solid. Predicting attention map for artifacts supervised by  human annotated data is somewhat novel to the best of my knowledge.\n\n--The experiments are quite convincing. Promising results on generalization of unseen datasets, forgery methods, and perturbations make me want to try the proposed method.\n\n--The paper is organized quite well and presented clearly. I enjoy reading this paper."
                },
                "weaknesses": {
                    "value": "--Will it help if the classifier takes caricature videos as input, or amplified representations as input?\n\n--Are there any evaluation for the caricatures? E.g., will there be real videos that people will think it is fake after seeing the caricatures? What are percentages when human agree with machines? etc"
                },
                "questions": {
                    "value": "See weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Responsible research practice (e.g., human subjects, data release)"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "The paper uses a data set of human artifact annotation."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2227/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698802324424,
            "cdate": 1698802324424,
            "tmdate": 1699636155930,
            "mdate": 1699636155930,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "oMjPXi1tL3",
                "forum": "fg772k6x6U",
                "replyto": "H3M2fImRVo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2227/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2227/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer qLFh"
                    },
                    "comment": {
                        "value": "Thank you for your comments. We really appreciate your thoughts on the experiments!\n\nHere are our responses to the weaknesses raised:\n\n**Amplified Representations.**\nWe tried feeding the caricatures themselves to the classifier in early experiments, but that structure does not improve performance - the caricatures are distorted when they reach the classifier and not helpful enough for that module to be able to effectively utilize it. We did some initial experiments with CariNet-S in earlier stages of the project, and obtained an AUC of **64.5** on DFDCp - compare that to 72.9 when using artifact attention maps. We hypothesize that the signal of artifact location gets mixed with the movement of the pixels themselves. The artifact attention heatmaps, however, are consequential, as shown in our experiments - they are better than attention maps from contemporary papers (Siohara 2022) and synergize well with the modified attention layers of our classification module.\n\n**Caricature Evaluation.** The caricatures are evaluated through our extensive human experiments: we show how humans exposed to caricatures become better at detecting fake videos in Section 4 and Figure 5. We compare to scenarios with no caricatures and with text labels, and observe a significant difference in hit rate when using Caricatures as the visual indicator of fakeness.\n\nOur caricature system will typically not affect a real video, as showcased in our gallery. However, an attacker could hijack our caricature generation module and force a caricature on a real video. In that case, humans could indeed consider that the video is fake. This impression of fakeness could happen in many scenarios however: people will think a video is fake when the lighting is weird, when the people are funny-looking, when the video quality is low, when the subject matter clashes with their beliefs, etc. Given this, to truly answer the question of how often caricatured videos make humans think real videos are fake, we would first need to measure how often this happens without caricatures in the mix, which is out of the scope of this study."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2227/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700625620186,
                "cdate": 1700625620186,
                "tmdate": 1700625620186,
                "mdate": 1700625620186,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]