[
    {
        "title": "Eye Fairness: A Large-Scale 3D Imaging Dataset for Equitable Eye Diseases Screening and Fair Identity Scaling"
    },
    {
        "review": {
            "id": "byO4PLyvj4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7116/Reviewer_8dk2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7116/Reviewer_8dk2"
            ],
            "forum": "Lv9KZ5qCSG",
            "replyto": "Lv9KZ5qCSG",
            "content": {
                "summary": {
                    "value": "The paper introduces the EyeFairness dataset, aimed to promote the fairness study for medical imaging. The dataset comprises 30,000 subjects with both 2D and 3D imaging data, capturing various demographic attributes. Additionally, the authors propose a fair identity scaling (FIS) approach to enhance model fairness for this dataset."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- This paper studies an important topic of fairness for medical imaging. The authors introduce a relatively large-scale dataset for 2D fundus photos and 3D OCT scans. It covers major eye diseases and captures a few different demographic attributes, which can be a useful resource for the community.\n- The authors propose Fair Identity Scaling (FIS) to improve the fairness of the model."
                },
                "weaknesses": {
                    "value": "- The authors didn't tune the hyper-parameters of the baseline methods but only used the default HPs, which leads to unfair comparisons. The baseline methods are not designed for medical imaging, so if applied to a different setting, the hyperparameters should be carefully tuned to get the best performance. Especially there're adversarial training method and self-supervised pretraining method that are very sensitive to the HPs.\n- The definition of performance-scaled disparity (PSD) is not clear. It says in the paper: \"PSD metrics are calculated as the standard deviation of group performance or absolute maximum group performance difference divided by overall performance.\" Which one did the authors use? And what does Mean PSD and Max PSD mean?\n- Also, regarding the metrics, authors can provide the worst-case AUC and the AUC gap between best-performing and worst-performing groups besides the current overall AUC and group-wise AUC. It would be clearer to directly look at the AUC gap to validate the effectiveness of the proposed methods. Additionally, what are the advantages of using PSD instead of the AUC gap? Consider an extreme case: for model 1, two groups have an AUC of 25% and 51% while for model 2, two groups have an AUC of 50% and 100%. According to the authors' definition, $PSD_1 = (51-25)/51=0.51 < PSD_2 = (100-50)/100=0.5$, but can you say model 2 is fairer than model 1 as it's the smaller the better? I know the AUC usually is higher than 50% but this is just an example and I think there are many similar cases in regular scenarios.\n- I think Table 1 in this paper is taken from Table 1 in [1] without reference as (1) the number of images of each dataset is not the original number but the number after preprocessing by [1] and (2) the so-called ADNI 1.5T is a subset of the large ADNI dataset [2] extracted by [1].\n- At the time I wrote this review, the GitHub repo authors provided was empty.\n- Minor: the current citation style makes reading difficult. The author should use (Deng et al. (2009)) instead of Deng et al. (2009), i.e. \\citep instead of \\cite.\n\n[1] Zong et al. MEDFAIR: Benchmarking Fairness for Medical Imaging. ICLR'23.\\\n[2] Wyman et al. Standardization of analysis sets for reporting results from adni mri data. Alzheimer\u2019s & Dementia 2013."
                },
                "questions": {
                    "value": "- Can the authors also provide further breakdown statistics of the intersectional groups, e.g. black females?\n- The dataset also contains some other attributes such as preferred language. I'm not very sure how this is related to eye diseases and fairness, e.g. do non-English speaking patients get lower AUC? But how does the eye imaging model perceive the speaking? Also, the authors did not evaluate the performance of different subgroups of preferred language and marital status."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Responsible research practice (e.g., human subjects, data release)"
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7116/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697144170322,
            "cdate": 1697144170322,
            "tmdate": 1699636841394,
            "mdate": 1699636841394,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "CJjuTfQO9P",
                "forum": "Lv9KZ5qCSG",
                "replyto": "byO4PLyvj4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7116/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7116/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response (Part 1 out of 2)"
                    },
                    "comment": {
                        "value": "**Question**: The authors didn't tune the hyper-parameters of the baseline methods but only used the default HPs, which leads to unfair comparisons. The baseline methods are not designed for medical imaging, so if applied to a different setting, the hyperparameters should be carefully tuned to get the best performance. Especially there're adversarial training method and self-supervised pretraining method that are very sensitive to the HPs.\n\n**Response**: We have meticulously tuned the hyperparameters for all baseline models presented in this paper. It's important to note that without such tuning, the baseline approaches yield inferior results. To enhance clarity and provide a comprehensive understanding, we have included a detailed discussion on this aspect in the revised manuscript.\n\n\n**Question**: The definition of performance-scaled disparity (PSD) is not clear. It says in the paper: \"PSD metrics are calculated as the standard deviation of group performance or absolute maximum group performance difference divided by overall performance.\" Which one did the authors use? And what does Mean PSD and Max PSD mean?\n\n**Response**: PSD, performance-scaled disparity, aims to use a single scaler to quantify group disparities based on the AUC metric. The PSD formula is defined as follows: \n$$\n\\text{Mean PSD} = \\frac{ \\text{STD}(\\text{Group-wise AUCs}) } {\\text{Overall AUC}}\n$$\n$$\n\\text{Max PSD} = \\frac{ \\text{Max}(\\text{Group-wise AUCs}) -  \\text{Min}(\\text{Group-wise AUCs})  } {\\text{Overall AUC}}\n$$\n\nWe have revised the manuscript accordingly to improve the clarity.\n\n**Question**: Also, regarding the metrics, authors can provide the worst-case AUC and the AUC gap between best-performing and worst-performing groups besides the current overall AUC and group-wise AUC. It would be clearer to directly look at the AUC gap to validate the effectiveness of the proposed methods. Additionally, what are the advantages of using PSD instead of the AUC gap? Consider an extreme case: for model 1, two groups have an AUC of 25% and 51% while for model 2, two groups have an AUC of 50% and 100%. According to the authors' definition,, but can you say model 2 is fairer than model 1 as it's the smaller the better? I know the AUC usually is higher than 50% but this is just an example and I think there are many similar cases in regular scenarios.\n  \n\n**Response**: We thank the reviewer for the comment. The table of the performance on Race for AMD detection is as follows. FIS continues to demonstrate superior performance in fairness metrics. Furthermore, as outlined in the definition of Max PSD,  Max PSD is closely correlated with AUC gap that is part of Max PSD. We fully agree that the PSD metric has its own limitations in representing the model fairness in scenarios outlined by the reviewer. The advantage of PSD is that: when the AUC gap is the same for two models, PSD considers the model with a higher AUC performance as the fairer model, while the AUC gap itself in this case considers the two models are equally fair. Realizing the fact that each fairness metric may have its limitations, we also calculated other fairness metrics in our manuscript such as DPD and DEOdds.\n\n\n| Method | Worst-case AUC | AUC Gap |\n|----------|----------|----------|\n| EffNet | 68.55 |  10.02 |\n| EffNet+Adv | 71.33 | 7.62 |\n| EffNet+FSCL | 71.37 | 7.40 |\n| EffNet+FIS | 73.22 |  5.96 |\n\n\n**Question**: I think Table 1 in this paper is taken from Table 1 in [1] without reference as (1) the number of images of each dataset is not the original number but the number after preprocessing by [1] and (2) the so-called ADNI 1.5T is a subset of the large ADNI dataset [2] extracted by [1].\n\n**Response**: We thank the reviewer for pointing this out. We have cited the paper of MedFair [1] and clarified that ADNI 1.5T is a subset of the large ADNI dataset extracted by [1] in the revision.\n\n\n**Question**: At the time I wrote this review, the GitHub repo authors provided was empty.\n\n**Response**: We apologize for the delay in making our dataset and code available. They were released for peer review on October 16th at https://github.com/anonymous4science/EyeFairness. To comply with IRB requirements, we meticulously reviewed our dataset including 30,000 patients to ensure the removal of any identifiable information, such as names, locations, and dates, before the data release.\n\n**Question**: Minor: the current citation style makes reading difficult. The author should use (Deng et al. (2009)) instead of Deng et al. (2009), i.e. \\citep instead of \\cite.\n\t\n**Response**: We have fixed our citation style in the revised manuscript."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7116/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700695840986,
                "cdate": 1700695840986,
                "tmdate": 1700695840986,
                "mdate": 1700695840986,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dXxsj1RQDB",
                "forum": "Lv9KZ5qCSG",
                "replyto": "byO4PLyvj4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7116/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7116/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response (Part 2 out of 2)"
                    },
                    "comment": {
                        "value": "**Question**: Can the authors also provide further breakdown statistics of the intersectional groups, e.g. black females?\n\n**Response**: The statistics of the intersectional groups (race + gender) are presented in the following table. We have added more statistics of intersectional groups to the revised manuscript.\n\n| Combination | AMD | DR | Glaucoma |\n|----------|----------|----------|----------|\n| Asian Female | 6.78 | 8.07 | 8.41 |\n| Asian Male | 4.93   | 6.50 | 6.46 |\n| Black Female | 3.89 | 4.11 | 4.49 |\n| Black Male | 3.17   | 3.53 | 3.98 |\n| White Female | 48.00 | 43.35 | 44.13 |\n| White Male  | 33.23 | 34.44 | 32.53 |\n\n\n**Question**: The dataset also contains some other attributes such as preferred language. I'm not very sure how this is related to eye diseases and fairness, e.g. do non-English speaking patients get lower AUC? But how does the eye imaging model perceive the speaking? Also, the authors did not evaluate the performance of different subgroups of preferred language and marital status.\n\n**Response**: Our dataset is from the United States, where a patient's primary language can be potentially indicative of their socioeconomic status. This factor could impact the fairness of AI models. For instance, patients with lower incomes, who might seek hospital care later and often present with multiple confounding eye/systemic diseases, could influence the predictions of deep learning models. The research [2] has shown that non-English speakers often have diseases at more advanced stages when they seek medical help.  \n\nFollowing the suggestion, the experimental results of preferred language and marital status are reported in the first and second tables below, respectively. FIS demonstrates consistent superiority over the baseline model, showcasing enhanced generalizability across metrics such as AUC, Mean PSD, and Worst-case AUC. Notably, the AUC gap when compared to EfficientNet is narrower than that with EfficientNet+FIS. This indicates that FIS not only enhances the worst-case AUC but also improves the best-case AUC. The results, along with a relevant discussion, have been included in the revised manuscript.\n\n| Method | AUC | Mean PSD | Worst-case AUC | AUC Gap |\n|----------|----------|----------|----------|----------|\n| EffNet | 78.99 |  6.76 | 73.70 |  11.72 |\n| EffNet+FIS | 80.17 |  6.24 | 74.87 |  12.24 |\n\n| Method | AUC | Mean PSD | Worst-case AUC | AUC Gap |\n|----------|----------|----------|----------|----------|\n| EffNet | 78.99 |  1.19 | 76.97 |  2.13 |\n| EffNet+FIS | 79.65 |  0.83 | 78.51 |  1.74 |\n\n\n**References**: \n\n[1] Zong, Yongshuo, Yongxin Yang, and Timothy Hospedales. \"MEDFAIR: Benchmarking Fairness for Medical Imaging.\" The Eleventh International Conference on Learning Representations. 2022.\n\n[2] Halawa, Omar A., et al. \"Race and ethnicity differences in disease severity and visual field progression among glaucoma patients.\" American journal of ophthalmology 242 (2022): 69-76."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7116/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700695872583,
                "cdate": 1700695872583,
                "tmdate": 1700695934219,
                "mdate": 1700695934219,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "DYeY5RMc7r",
            "forum": "Lv9KZ5qCSG",
            "replyto": "Lv9KZ5qCSG",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7116/Reviewer_7iiL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7116/Reviewer_7iiL"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a new large-scale dataset for eye disease diagnostic comprising of 30\u2019000 2D fundus as well as 3D OCT images for AMD retinopathy and glaucoma diagnostics. In addition to some baseline comparison on fairness metics it also proposes a new fair identity scaling."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "The proposed dataset is very valuable as it is very large in size (30\u2019000 patients with 2D and 3D imaging) covering three relevant eye diseases. The analysis is strong and the explored fair identity scaling is a reasonable approach to address inequality in datasets in general. Providing (to my understanding) paired 2D fundus and 3D OCT imaging could also pave the way for new hybrid diagnostic tools."
                },
                "weaknesses": {
                    "value": "Overall, despite its value, the proposed dataset is somewhat limited in that it seems to be acquired from a single centre in the US. A pooling with previous public datasets would likely increase its value and reduce the \u201cunfairness\u201d by design (rather than re-weighting). \nThe statement \u201ceffective image augmentation strategies for 3D imaging data are largely unclear\u201d is wrong in my opinion and there is no citation that backs it up. Many 3D medical image analysis methods make good use of image augmentation strategies. \nThe chosen baselines are rather simple and no true SOTA results are presented. The presentation of the results is mainly focussed on numerical comparison and I would be missing a more in-depth analysis or discussion why certain races perform better or worse. E.g. since Hispanics are under-represented in the dataset it is not intuitive that this group achieves the highest AUC without and not with FIS. \nIt remains unclear whether the data is always \u201cpaired\u201d, ie. the same patient is measured with 2D and 3D imaging."
                },
                "questions": {
                    "value": "Clarify the 2D/3D data split wrt. patients. Discuss a pooling of other public datasets from centres with different scanners / from different countries. Expand the baselines and correct the statement of non-existing 3D augmentation strategies."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7116/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698611924306,
            "cdate": 1698611924306,
            "tmdate": 1699636841275,
            "mdate": 1699636841275,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GKWzgQyoqY",
                "forum": "Lv9KZ5qCSG",
                "replyto": "DYeY5RMc7r",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7116/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7116/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "**Question**: Discuss a pooling of other public datasets from centres with different scanners / from different countries\n\n**Response**: As shown in Table 1 in the manuscript, for eye diseases, there are no public fairness datasets available that have comprehensive demographic identity attributes as in our dataset including age, gender, race, ethnicity, preferred language, and marital status. Our dataset is also the only public dataset with 3D eye imaging data measured by optical coherence tomography that enables 3D fairness learning modeling. For instance, the ODIR-2019 dataset only contains demographic identity attributes of age and gender, which is insufficient for comprehensive fairness learning. Another example is that the AMD-OCT dataset only contains the age attributes. Therefore, given the uniqueness of our dataset, which covers three major eye diseases, including age-related macular degeneration, diabetic retinopathy, and glaucoma, affecting 380 million patients worldwide, contains a large number of patients of 30,000 in total and six demographic attributes (age, gender, race, ethnicity, preferred language, and marital status) for fairness learning, there are no existing public datasets that can be used to pool with our dataset. In addition, we would like to stress that our dataset is from a prominent eye hospital located in a region with a large population with high diversities.\n\n\n| Dataset       | Imaging Modality | Number of Patients  | Disease Focus | Identity Attribute | \n|----------|----------|----------|----------|----------|                          \n| ODIR-2019      | Fundus          | 5,000  | AMD, Glaucoma, DR | Age; Gender  |         \n| AMD-OCT        | OCT                |  384   | AMD |   Age   |             \n\n\n\n**Question**: Clarify the 2D/3D data split wrt. patients.\n\n**Response**: As detailed in Section 5.1, our dataset utilizes a patient-level split. It comprises one 2D fundus image and a set of 3D OCT B-Scans for each patient. This information has been further clarified in the revised version of our manuscript.\n\n**Question**: Expand the baselines.\n\n**Response**: We are grateful for the reviewer's insightful comment. We have included the state-of-the-art fairness method, FairAdaBN [1], in our experiments. The results for AMD detection based on race are presented below. We tune the hyperparameters of FairAdaBN to ensure optimal performance and utilize its best-reported backbone network, ResNet. However, it's important to note that FairAdaBN's dependency on the backbone network architecture restricts its generalizability and ability to leverage more powerful pre-trained models, such as EfficientNet. We have updated the experiment in the revised manuscript accordingly.\n\n| Method | AUC | Mean PSD |\n|----------|----------|----------|\n| FairAdaBN [1] | 78.88 |  3.63 |\n| EffNet | 79.10 |  5.45 |\n| EffNet+Adv | 78.95 | 3.80 |\n| EffNet+FSCL | 79.74 | 4.20 |\n| EffNet+FIS | 79.95 |  3.40 |\n\n**Question**: Correct the statement of non-existing 3D augmentation strategies.\n\n**Response**: We appreciate the reviewer\u2019s contribution to this discussion. Rather than delving into general 3D augmentation strategies, our discussion focus is on augmentation methods specifically pertinent to fairness. To our knowledge, there is a lack of established fairness methods demonstrating that algorithmic fairness can be improved through 3D augmentation. The application of current 3D augmentation techniques, such as cutmix, random crop, and mixup, raises concerns. These methods could inadvertently lead to inaccurate disease detection by obscuring or eliminating abnormal lesions in the 3D space. This discussion has been included in the revised manuscript. \n\n**References**: \n\n[1] Zikang Xu, Shang Zhao, Quan Quan, Qingsong Yao, and S. Kevin Zhou. 2023. FairAdaBN: Mitigating Unfairness with Adaptive Batch Normalization and Its Application to Dermatological Disease Classification. In Medical Image Computing and Computer Assisted Intervention \u2013 MICCAI 2023: 26th International Conference, Vancouver, BC, Canada, October 8\u201312, 2023, Proceedings, Part II. Springer-Verlag, Berlin, Heidelberg, 307\u2013317."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7116/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700695719535,
                "cdate": 1700695719535,
                "tmdate": 1700695950171,
                "mdate": 1700695950171,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "WugK6yDqsI",
            "forum": "Lv9KZ5qCSG",
            "replyto": "Lv9KZ5qCSG",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7116/Reviewer_fenP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7116/Reviewer_fenP"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces the EyeFairness dataset that includes both 2D fundus photos and 3D optical coherence tomography (OCT) scans, together with six demographic features (age, gender, race, ethnicity, preferred language, marital status), and proposes a fair identity scaling (FIS) approach to combine group and individual scaling to improve model fairness. FIS was demonstrated to improve performance in eye disease screening according to fairness metrics, when implemented together with EfficientNet-B1, against other fairness methods. Fairness methods are especially appropriate in the eye screening domain due to known differing burdens of eye diseases amongst ethnicities."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "-\tFIS exploits both group and individual scaling to manage within-group sample variation\n-\tDetailed comparison of proposed FIS against other fairness methods (Adv, FSCL) on various demographic features"
                },
                "weaknesses": {
                    "value": "-\tMinimal ablation analysis temperature scaling parameter, FIS group/individual scaling trade-offs actually not very consistent (Figure 2)\n-\tSide-effects of fairness on adjacent demographic features not considered"
                },
                "questions": {
                    "value": "1. The main contribution of a Fair Identity Scaling (FIS) model with learnable group weights and past individual loss data, might have been analyzed in greater details as to the temperature scaling parameter (alongside fusion weight).\n2. Conceptually, the distinction between improvements in \u201cgeneral (AUC) performance\u201d and \u201cfairness\u201d might have been further considered. In particular, from the results in Tables 2 to 7, FIS appears often capable of not only improving \u201cfairness\u201d (i.e. minimizing performance-scaled disparity), but instead often improves performance in all groups (and overall). As such, a natural question might be whether FIS might be used with arbitrary groupings of data, to improve classifier performance.\n3. Returning to fairness as a focus, the presented analysis does not appear to be concerned with the impact on fairness amongst other demographic features, when FIS is applied to a particular feature. For example, when FIS is applied on race (as in Tables 2 & 3), what is the effect on results stratified with other features such as gender, ethnicity, age etc.? This appears particularly relevant since the other demographics may be no less significant for the consideration of fairness/equity purposes.\n4. The costs of considering fairness might be discussed in greater detail, in particular the possibility that optimizing the proposed PSD metric possibly reduces overall classification performance (and thus medical care). This is because PSD (and other fairness metrics) emphasize between-group equality, which may come at the cost of reduced aggregate performance (although this is largely not the case in this study)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7116/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698742672849,
            "cdate": 1698742672849,
            "tmdate": 1699636841167,
            "mdate": 1699636841167,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2jME8JFcTX",
                "forum": "Lv9KZ5qCSG",
                "replyto": "WugK6yDqsI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7116/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7116/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response (Part 1 out of 2)"
                    },
                    "comment": {
                        "value": "**Question**: Minimal ablation analysis temperature scaling parameter, FIS group/individual scaling trade-offs actually not very consistent (Figure 2) / The main contribution of a Fair Identity Scaling (FIS) model with learnable group weights and past individual loss data, might have been analyzed in greater details as to the temperature scaling parameter (alongside fusion weight).\n\n**Response**: As suggested by the reviewer,  we have included a table below showcasing the experimental results for AMD detection with different temperature values, specifically focusing on the attribute of race in fundus images, with the hyperparameter c set to 0.5. This analysis has been incorporated into the revised version of our paper. \n\n| $\\tau$ | AUC | Mean PSD |\n|----------|----------|----------|\n|0.2 | 77.25 |  0.0565 |\n|0.4 | 77.86 | 3.68 |\n|0.8 | 79.39 | 5.59 |\n|1 | 79.95 | 3.40 |\n|2 | 79.54 | 5.18 |\n|3 | 79.49 | 5.01 |\n|4 | 79.60 | 7.02 |\n|5 | 79.49 | 5.07 |\n\n**Question**: Side-effects of fairness on adjacent demographic features not considered/Returning to fairness as a focus, the presented analysis does not appear to be concerned with the impact on fairness amongst other demographic features, when FIS is applied to a particular feature. For example, when FIS is applied on race (as in Tables 2 & 3), what is the effect on results stratified with other features such as gender, ethnicity, age etc.? This appears particularly relevant since the other demographics may be no less significant for the consideration of fairness/equity purposes.\n\n**Response**: We thank the reviewer for the insightful comment. Focusing on a single attribute can make the cause-and-effect relationship clear. However, as the reviewer highlighted, examining the impact of applying FIS to one attribute on the performance metrics of other attributes is also valuable. To this end, we have conducted an analysis where FIS is specifically applied to race in the context of AMD detection, and we report its effects on gender and ethnicity performance. For our FIS model specifically addressing the fairness for race, the model fairness quantified by PSD values for gender and ethnicity attributes is also improved. It's important to note that the individual scaling in Equation (1) is attribute-independent and is designed to adaptively influence the significance of each sample during back-propagation. This approach could be a contributing factor to the observed improvements in fairness.\n\n| Method | AUC (Race) | Mean PSD (Race) | Mean PSD (Gender) | Mean PSD (Ethnicity) |\n|----------|----------|----------|----------|----------|\n| EffNet | 79.10 |  5.45 |  7.24 |  4.78 |\n| EffNet+FIS (Race) | 79.95 |  3.40 |  1.58 |  0.58 |\n\n\n\n**Question**: Conceptually, the distinction between improvements in \u201cgeneral (AUC) performance\u201d and \u201cfairness\u201d might have been further considered. In particular, from the results in Tables 2 to 7, FIS appears often capable of not only improving \u201cfairness\u201d (i.e. minimizing performance-scaled disparity), but instead often improves performance in all groups (and overall). As such, a natural question might be whether FIS might be used with arbitrary groupings of data, to improve classifier performance.\n\n**Response**: The observed improvement in both fairness and performance can be attributed to the individual scaling mechanism in FIS, which considers the losses of samples within a mini-batch to determine their relative importance for back-propagation. This approach enables the training process to give priority to samples with larger losses, effectively balancing the learning focus across the dataset.\n\nFollowing the suggestion, we present the experimental results for arbitrary groupings in AMD detection, as detailed in the table below. We observe that combining race, gender, and ethnicity enhances performance, although the resulting mean PSDs are not consistently optimal compared to other methods. This highlights the intricate and intriguing interplay between attributes. We plan to delve deeper into this aspect in our future research.\n\n\n| Method | AUC (Race) | Mean PSD (Race) | Mean PSD (Gender) | Mean PSD (Ethnicity) |\n|----------|----------|----------|----------|----------|\n| EffNet | 79.10 |  5.45 |  7.24 |  4.78 |\n| EffNet+FIS (Race) | 79.95 |  3.40 |  1.58 |  0.58 |\n| EffNet+FIS (Race+Gender) | 79.84 | 3.90 | 1.40 | 0.38 |\n| EffNet+FIS (Race+Gender+Ethnicity) | 80.28 | 3.95 | 1.30 | 0.32 |"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7116/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700695521015,
                "cdate": 1700695521015,
                "tmdate": 1700695521015,
                "mdate": 1700695521015,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "R95u4vd5x4",
            "forum": "Lv9KZ5qCSG",
            "replyto": "Lv9KZ5qCSG",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7116/Reviewer_eXPH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7116/Reviewer_eXPH"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a publicly available large-scale (30,000 subjects) 3D eye imaging dataset (OCT/Fundus) for disease screening and fair identity scaling. The authors also propose a fair identity scaling metric to evaluate model performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1. Addressing the fairness issue is an important topic and organizing such a large-scale dataset including three types of measurements: 1) retinal imaging 2) demographic group information 3) disease diagnosis requires a large amount of effort.\n\n2. The authors ran several baselines (EfficientNet, 3D CNN) and evaluated the classification with some fairness metrics (e.g. PSD, DPD)."
                },
                "weaknesses": {
                    "value": "1. The abstract and introduction section is quite lengthy. The core contributions and the highlights can be combined.\n\n2. Some of the writing needs to be improved (e.g. \"model performance across different models\")."
                },
                "questions": {
                    "value": "1. The trend of hyperparameter $c$ in Figure 2 is not quite clear since it kind of alternates for both AUC and mean PSD. The authors might need to further discuss the choice of $c$, which still seems rather empirical given the current visualization.\n\n2. The experimental results section is currently flooded with numbers and quite hard to follow."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7116/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7116/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7116/Reviewer_eXPH"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7116/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699239605086,
            "cdate": 1699239605086,
            "tmdate": 1699636841013,
            "mdate": 1699636841013,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "bHma1ALc6o",
                "forum": "Lv9KZ5qCSG",
                "replyto": "R95u4vd5x4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7116/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7116/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "**Question**: The trend of hyperparameter  in Figure 2 is not quite clear since it kind of alternates for both AUC and mean PSD. The authors might need to further discuss the choice of, which still seems rather empirical given the current visualization.\n\n**Response**: We adhere to the Keep It Simple and Stupid (KISS) principle by setting the default hyperparameter c to 0.5, striking a balance between individual and group scaling to achieve a trade-off between effectiveness/accuracy and fairness. $c = 1$ represents exclusive group scaling, while $c = 0$ denotes solely individual scaling. As corroborated by the analysis in Figure 2, our choice of c at 0.5 aligns with empirical results, demonstrating that higher AUC coincides with lower mean PSD, compared to other c values.\n\n\n**Question**: The experimental results section is currently flooded with numbers and quite hard to follow.\n\n**Response**: We appreciate the reviewers' feedback on the comprehensiveness of our study, which encompasses a wide range of factors such as identity attributes, metrics, and methods, and includes extensive experiments across three eye diseases. Recognizing the importance of thoroughness in our experimental results, we have enhanced the readability of our experimental section by incorporating visualizations in the revised manuscript."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7116/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700694665746,
                "cdate": 1700694665746,
                "tmdate": 1700694665746,
                "mdate": 1700694665746,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]