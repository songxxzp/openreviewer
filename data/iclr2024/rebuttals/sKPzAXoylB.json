[
    {
        "title": "Addressing Catastrophic Forgetting and Loss of Plasticity in Neural Networks"
    },
    {
        "review": {
            "id": "2KZ92F5dGM",
            "forum": "sKPzAXoylB",
            "replyto": "sKPzAXoylB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8965/Reviewer_KmBd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8965/Reviewer_KmBd"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a novel approach called Utility-based Perturbed Gradient Descent (UPGD) to address catastrophic forgetting and loss of plasticity in neural networks. UPGD combines gradient updates with a mask to protect useful weights from being forgotten and reuse less useful weights. The paper also proposes metrics to evaluate loss of plasticity and catastrophic forgetting. Empirically, the method outperforms existing methods in streaming learning problems in terms of retaining plasticity and avoiding catastrophic forgetting."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "**Originality**\nConceptually, the problem addressed by the authors of avoiding forgetting while retaining plasticity in streaming learning settings remains underexplored. The specific method proposed by the authors is relatively straightforward and conceptually similar to prior approaches; however, it empirically outperforms prior methods as the experimental results demonstrate.\n\n**Quality**\nThe convergence guarantee results are valuable. The experiments are generally comprehensive and well-conducted. Assessing the quality of the approximated utilities in section 4.1 is \nof critical importance, and the results are convincing. Conducting miniImagenet scale experiments is a solid addition to the experimental section. The ablation study in Figure 8 is also insightful.\n\n**Clarity**\nThe writing is generally clear and the figures are well-illustrated.\n\n**Significance**\nOverall, the paper addresses a major issue in the field of streaming learning. Given that the paper doesn't investigate the theoretical properties of UPGD, the significance of the paper hinges on the strength of the empirical results."
                },
                "weaknesses": {
                    "value": "Since the proposed method lacks theoretical performance guarantees, its empirical performance is critical. The authors have generally done a good job demonstrating that UPGD avoids forgetting and maintains plasticity; however, a few concerns remain:\n\n- It appears that S-EWC does not have too much of a gap with UPGD judging from figure 7: it entirely avoids catastrophic forgetting, and the only setting where it loses plasticity where UPGD does not is on MNIST\n- S-MAS outperforms UPGD on miniImagenet at the end of training, and does not have a large gap overall\n- The ablation of figure 8 checks the contribution of each component of UPGD sequentially as they are added to regular SGD. Ideally, the ablation would study how each component affects UPGD when they are *individually* removed (e.g. UPGD without WP).\n\n**Minor comments**\nFigure 7 is referred to before Figure 6; ideally, their order would be swapped.\nI see in Section 4 that the results are averaged over 20 trials, but the meaning of the error margins in some of the figures is not made clear (e.g. figure 2). I would also suggest increasing the number of trials to smooth out the curves if possible."
                },
                "questions": {
                    "value": "Is it possible to show theoretical performance guarantees for UPGD? For instance, can the approximation error of equation 2 be bounded? Alternatively, if the true utilities are used in equation 3, is it possible to derive some guarantees against forgetting or loss of plasticity?\n\nHow much more significantly does UPGD improve upon baselines S-EWC and S-MAS?\n\nHow does UPGD-W perform with WP and WD removed individually?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8965/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698096053419,
            "cdate": 1698096053419,
            "tmdate": 1699637128872,
            "mdate": 1699637128872,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "UPbJOG1cxg",
                "forum": "sKPzAXoylB",
                "replyto": "2KZ92F5dGM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8965/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8965/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We would like to extend our sincere gratitude to the reviewer for the time and effort invested in reviewing our paper. We are also grateful to read that the reviewer thinks our work is significant and novel in an underexplored area with comprehensive and well-conducted experiments and clearly written paper. Here, we give our response to the points the reviewer mentioned.\n\n&nbsp;\n\n> It appears that S-EWC does not have too much of a gap with UPGD judging from Figure 7: it entirely avoids catastrophic forgetting, and the only setting where it loses plasticity where UPGD does not is on MNIST\n\nWhile S-EWC is effective in catastrophic forgetting evaluation, it suffers from loss of plasticity, which was the aspect we evaluated in the input-permuted MNIST task. Only UPGD addressed both loss of plasticity and catastrophic forgetting, which is shown by its performance on both evaluations. \n\n&nbsp;\n\n> S-MAS outperforms UPGD on miniImagenet at the end of training, and does not have a large gap overall\n\nWhile we do not expect a new method to outperform in every single task, UPGD still achieves higher average accuracy in the first half of the experiment. It is desirable in streaming learning where the online accuracy at each time step is more important than the accuracy only at the end of training.\n\n&nbsp;\n\n> The ablation of figure 8 checks the contribution of each component of UPGD sequentially as they are added to regular SGD. Ideally, the ablation would study how each component affects UPGD when they are individually removed (e.g. UPGD without WP).\n\nWe conducted the requested ablation. Please see this figure [[link]](https://drive.google.com/file/d/15aeYYq3QjeEgAbeFVeqzSNT5Ms2QvSYr/view?usp=sharing). The requested ablation bolsters the contribution of utility gating even more. Using utility gating on top of sgd makes sgd maintain its performance instead of dropping on input-permuted MNIST and makes sgd improve its performance continually on label-permuted problems. The role of weights decay and weight perturbation is not significant in label-permuted problems but including both with utilty gating improves performance and plasticity on input-permuted MNIST.\n\n&nbsp;\n\n> Minor comments Figure 7 is referred to before Figure 6; ideally, their order would be swapped. I see in Section 4 that the results are averaged over 20 trials, but the meaning of the error margins in some of the figures is not made clear (e.g. figure 2). I would also suggest increasing the number of trials to smooth out the curves if possible.\n\nThank you for pointing out the order of mention for Figures 6 and 7; we will make the necessary amendments. The error margins represent standard error. Our results are averaged over 20 trials. We use non-overlapping intervals to average over. Therefore, our plots can have variabilities even though we have a large number of trials. We don\u2019t average by using an exponential moving average, which will smoothen out the curves since we are interested in what each point in the plot means. For example, each point in Figure 3 represents the average performance over a single task. Smoothing over time wouldn\u2019t give the same meaning. \n\n&nbsp;\n\n**Answers the Questions:**\n\n> Is it possible to show theoretical performance guarantees for UPGD?\n\nWe think this would be interesting direction that is left for future work.\n\n> How much more significantly does UPGD improve upon baselines S-EWC and S-MAS?\n\nBased on our results, we can see that UPGD outperforms S-EWC in all experiments and outperforms S-MAS in 3 out of 4 experiments.\n\n> How does UPGD-W perform with WP and WD removed individually?\n\nPlease see this figure [[link]](https://drive.google.com/file/d/15aeYYq3QjeEgAbeFVeqzSNT5Ms2QvSYr/view?usp=sharing), which includes the requested ablation."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8965/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699939533728,
                "cdate": 1699939533728,
                "tmdate": 1699940279218,
                "mdate": 1699940279218,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "dc9aLBYCdY",
            "forum": "sKPzAXoylB",
            "replyto": "sKPzAXoylB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8965/Reviewer_3zCE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8965/Reviewer_3zCE"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes to modify stochastic gradient descent (SGD) to overcome forgetting and promote plasticity in continual learning. These goals are achieved by masking out the parameters with high utility and perturbing gradient direction by Gaussian noise. For utility computation, the authors propose an approximate but efficient scheme based on second-order Taylor expansion of the loss. Experiments demonstrate that (i) the proposed utility approximation is more accurate than simple baselines such as weight magnitude, (ii) it maintains plasticity, (iii) plasticity and accuracy are in general correlated, (iv) the method tends to forget less than baselines, and (v) it simultaneously promote plasticity and prevents forgetting."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "This paper is written well. The notation is okay and the mathematical derivations seem correct. The baseline methods are clearly outperformed and the experiments verify the central claim of the paper."
                },
                "weaknesses": {
                    "value": "Although continual learning is an important machine learning challenge, I feel the paper suffers from significant weaknesses:\n\n  - First and foremost, I do not think this paper makes a significant contribution. The methodology is incremental in that it combines two well-known ideas (perturbed gradient descent + keeping active neurons unchanged). \n  - Second, it is tested on very toy setups. The experiments are not convincing enough to show the applicability of the method to interesting real-world setups. For instance, I am not sure the networks achieve similar plasticity if tested on, e.g., webcam data instead of MNIST, where the feature space is a lot richer and hence plasticity is much more difficult.\n  - Third, theoretical properties/implications of the method should be carefully examined. \n    - For instance, the Taylor expansion would only hold if $W_{l,i,j}$ are infinitesimally small. We do not know in general if this holds or not. I suggest the paper should include a (preferably rigorous) discussion on this.\n    - Likewise, gradient descent is no longer steepest descent but some approximation to it. Investigating why it works is important. As shown by the results, no collapse occurs but again, I wonder how this translates into more challenging settings where utilities of most parameters are high."
                },
                "questions": {
                    "value": "Here I list my questions as well as suggestions:\n\n- It would be better if Label-Permuted EMNIST was described before the results are discussed in paragraph 5.\n- _Although a few methods address both issues simultaneously, such methods expect known task boundaries, maintain a replay buffer, or require pretraining, which does not fit streaming learning._ <--- reference needed for this claim.\n- What does \"a Hessian diagonal approximation in linear complexity\" mean? Linear in the number of parameters?\n- It would be better if the main text included details on the \"utility propagation theorem\".\n- It would be better if the descriptions of the tasks/datasets (e.g. Input-Permuted MNIST in section 4.2) were given before the details.\n- Does \"each learner is trained for 1M samples, one sample each time step\" mean gradient descent using one sample only? Is this realistic?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8965/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698692987251,
            "cdate": 1698692987251,
            "tmdate": 1699637128739,
            "mdate": 1699637128739,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "O4lVs7PeYH",
                "forum": "sKPzAXoylB",
                "replyto": "dc9aLBYCdY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8965/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8965/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We would like to extend our sincere gratitude to the reviewer for the time and effort invested in reviewing our paper. We are also grateful to read that the reviewer thinks our paper is well-written and that our empirical evaluation verifies the central claim of the paper. Here, we give our response to the points the reviewer mentioned.\n\n&nbsp;\n\n> I do not think this paper makes a significant contribution. The methodology is incremental in that it combines two well-known ideas (perturbed gradient descent + keeping active neurons unchanged). \n\nWe respectfully disagree with the reviewer. First, our method does not work by keeping active neurons unchanged. Our method uses a utility-gating mechanism to protect useful weights from change and perturb less useful ones. This idea is compatible with weight perturbation and weight decay, which makes the method stronger. Second, our novel utility-gating mechanism presents an instance of a new class of methods that has not been introduced before. We kindly ask the reviewer to refer us to works to which UPGD is considered incremental. \n\n&nbsp;\n\n> It is tested on very toy setups.\n\nOur tasks are widely used in continual learning for evaluating forgetting and plasticity (see [1,2,3,4]), where input-permuted tasks were used for evaluating plasticity and output-permuted tasks were used for evaluating forgetting. We certainly acknowledge the reviewer\u2019s valuable suggestion for real-world setups, such as webcam data. However, such benchmarks are unavailable, especially for streaming learning, due to its complexity, and requires a separate dedicated paper. As you recognize that our experiments verify the paper\u2019s central claim and reviewer KmBd highlights their scale, solidity, and comprehensiveness, we believe they sufficiently demonstrate the core contribution of our work.\n\n&nbsp;\n\n> Third, theoretical properties/implications of the method should be carefully examined.\n\nWe provided theoretical work in the Appendix. Please refer to Appendix A for convergence analysis, Appendix B for how utilities can be propagated in the network, and Appendix D for the connection between weight and feature utilities.\n\nRegarding the comment on Taylor expansion, our first and second-order Taylor approximations require ignoring higher-order terms, which is common and valid for our case when weights are proper fractions, making them sufficiently small. Our empirical evaluation shows the effectiveness of this approximation in continual learning. For future work, one can consider a different utility definition based on the change in loss from sample to sample, which depends on the difference between the values of the weight from two consecutive time steps, making the approximation more accurate (see [5] and Appendix M).\n\n&nbsp;\n\n> gradient descent is no longer steepest descent but some approximation to it. Investigating why it works is important.\n\nIn Appendix A, we provided convergence analysis for our method, where we showed that our method converges to a stationary point, similar to other methods (e.g., SGD). Our empirical evaluation in Figure 9 shows that UPGD also outperforms SGD in stationary problems.\n\n&nbsp;\n\n**Answers to the Questions:**\n\nFirst, we thank the reviewer for the suggestions. We will incorporate them in our revision.\n\n> reference needed for this claim.\n\nThank you for letting us know. We will add these references in the paper revision (see [6,7]).\n\n> What does \"a Hessian diagonal approximation in linear complexity\" mean? Linear in the number of parameters?\n\nThis is correct: linear in the number of parameters.\n\n> Does \"each learner is trained for 1M samples, one sample each time step\" mean gradient descent using one sample only? Is this realistic?\n\nThat is correct. The update is made based on one sample at a time. This is often used as the more challenging constraint of streaming learning [e.g., 8]\n\n&nbsp;\n\n*References:*\n\\\n[1]. Lyle, C., Zheng, Z., Nikishin, E., Pires, B. A., Pascanu, R., & Dabney, W. (2023). Understanding plasticity in neural networks.\n\\\n[2]. Dohare, S., Hernandez-Garcia, J. F., Rahman, P., Sutton, R. S., & Mahmood, A. R. (2023). Maintaining Plasticity in Deep Continual Learning.\n\\\n[3]. He, X., Sygnowski, J., Galashov, A., Rusu, A. A., Teh, Y. W., & Pascanu, R. (2019). Task agnostic continual learning via meta learning.\n\\\n[4]. Kumar, S., Marklund, H., & Van Roy, B. (2023). Maintaining plasticity via regenerative regularization.\n\\\n[5]. Zenke, F., Poole, B., & Ganguli, S. (2017, July). Continual learning through synaptic intelligence.\n\\\n[6]. Hayes, T. L., Kanan, C. (2022). Online continual learning for embedded devices.\n\\\n[7]. Van de Ven, G. M., Siegelmann, H. T., & Tolias, A. S. (2020). Brain-inspired replay for continual learning with artificial neural networks.\n\\\n[8] Sahoo, D., Pham, Q., Lu, J., & Hoi, S. C. (2017). Online deep learning: Learning deep neural networks on the fly."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8965/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699939426038,
                "cdate": 1699939426038,
                "tmdate": 1699939426038,
                "mdate": 1699939426038,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "iEYpSpjw20",
                "forum": "sKPzAXoylB",
                "replyto": "O4lVs7PeYH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8965/Reviewer_3zCE"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8965/Reviewer_3zCE"
                ],
                "content": {
                    "title": {
                        "value": "my response to author response"
                    },
                    "comment": {
                        "value": "Thanks for the author response. My main concern is novelty, which I feel is still not addressed in the author response. As such, the methodology has two pillars: perturbed masked gradient descent and masking based on utility. Concerning the first pillar, please see \"Masked Training of Neural Networks with Partial Gradients\" paper and all the references therein. Further, see \"Learning where to learn: Gradient sparsity in meta and continual learning\" paper for yet another presentation of gradient masking for increasing plasticity. About the second pillar, my main concern is that the Taylor approximation does not hold unless $W_{l,i,j}$ is infinitesimally small. I really think this is a strong assumption that does not necessarily generalize, although I acknowledge that the method works fine in practice."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8965/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700669153798,
                "cdate": 1700669153798,
                "tmdate": 1700669153798,
                "mdate": 1700669153798,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "xHSKCcJ5Or",
            "forum": "sKPzAXoylB",
            "replyto": "sKPzAXoylB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8965/Reviewer_y5kB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8965/Reviewer_y5kB"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a measure of weight utility of weights in neural networks for given loss and using it to modify a gradient-based weight update in networks to alleviate the problem of catastrophic forgetting.  The authors identify two fundamental aspect of catastrophic forgetting - the forgetting aspect (not losing what the network already know) and plasticity aspect (ability to learn new concepts).  The proposed method is meant to address two problems at the same time, preseving high utility weights with no modifications (to prevent forgetting) while randomly perturbing low utility weights to \"encourage\" them to participate in the computations related to new tasks (plasticity).  Empirical evaluations show solid performance of the proposed method according to the forgetting and plasticity metrics newly defined by the authors."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The proposed rule is straight forward.  \n\nComputational complexity of the evaluation of true utility is well addressed making the method practical.\n\nEmpirical evidence provided shows the proposed rule is effective for alleviation of catastrophic forgetting.\n\nDecomposing the catastrophic forgetting problem into two aspects: forgetting and plasticity, seems very sensible.\n\nProposed measures of plasticity and forgetting seem sensible.\n\nThe paper is well written."
                },
                "weaknesses": {
                    "value": "Though empirical evidence provided in the paper suggest it does (in that it works), I am not sure that the proposed definition of weight utility make sense.  The power of neural networks (and the problem of the interpretation of its computation) is its distributed computation.  Utility of an individual weight is almost always nothing - in fact, quite often any particular weight, sometimes even large number of weights, can be taken out of the network, with little impact on performance.  So, it's more about combinations of weights working together...and the proposed utility doesn't measure that.  I understand that evaluating utility of combinations of weights is intractable, but I worry that this simplification, of judging utility of each weight in isolation, is encouraging less distributed representation, which might come with a penalty in performance.\n\nFundamentally, on the forgetting front, the proposed method is just another weight consolidation method, and it's a bit hard to believe it beats Elastic Weight Consolidation.  It am not 100% sure that the proposed method doesn't favour plasticity over forgetting nor that the forgetting evaluation isn't biased towards methods that favour plasticity (see questions below)."
                },
                "questions": {
                    "value": "Though I understand (and like) in principle what the utility-based update is supposed to do, I can't quite understand why it actually works.  The proposed measure of the utility of parameters is a measure with respect to the loss on the new input/output pair.  If this pair comes from a new task, how does measuring utility of the model parameters with respect to the loss of this new task have bearing on the utility of the parameters for the old tasks?  Just because utility of a given weight is, say, low for the current sample, it doesn't mean it's low for previous samples.  It seems to me that the proposed method would score high on plasticity (it finds available weights for new task)...but I don't see how it protects against forgetting, in principle, though if we are to talk about empirical evidence...  I don't understand how 4.3 measures catastrophic forgetting.  Permuting labels of CIFAR10 with the new tasks suggests to me that it's all about plasticity again.  Shouldn't it be an experiment, where labels are kept intact, but new tasks are added...and previous tasks examples are not used?  Am I missing something about how experiments reported in 4.3 are done?\n\nWhy are the accuracy results of training on CIFAR-10 and EMNIST so poor in Figure 6?  State of the art CIFAR-10 is close (or above) 90%.  Something close to 80% would be probably still acceptable...but 60% is quite poor.  I am not exactly sure what EMNIST variant entails, but is 70% accuracy a good accuracy for this dataset?  It is often easy to shown improvements of something at the low end of the models' performance, but that doesn't always translate to same effect at the high (or close to) end of the models' performance...and in the end, the latter is what we really care about.  So, does the proposed method prevent forgetting at the high end, when model is performing at or reasonably close to state of the art?\n\nThis is not a massive issue, but does the per batch normalisation of utility make the performance of the method variable with different  mini-batch size settings?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8965/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8965/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8965/Reviewer_y5kB"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8965/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698706759624,
            "cdate": 1698706759624,
            "tmdate": 1699642867494,
            "mdate": 1699642867494,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ZTB8L8Rya5",
                "forum": "sKPzAXoylB",
                "replyto": "xHSKCcJ5Or",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8965/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8965/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We would like to extend our sincere gratitude to the reviewer for the time and effort invested in reviewing our paper. We are also grateful to read that the reviewer thinks our method is straightforward and practical, our metrics are reasonable, our paper is well-written, and our method has solid performance. Here, we give our response to the points the reviewer mentioned.\n\n&nbsp;\n\n> Utility of an individual weight is almost always nothing...it's more about combinations of weights working together...and the proposed utility doesn't measure that\n\nWe agree with the reviewer that a better utility definition would include the interaction between weights (as we pointed out in Appendix M). However, including such interactions makes the method intractable, as the reviewer mentioned. Most methods addressing forgetting (e.g., EWC) use individual weight measures, and yet they are effective. While we are interested in learning about the kind of representations learned by each method, we believe this merits a dedicated paper to address fully. From our experiment, however, we found that UPGD improves performance over SGD even in the stationary case (see Figure 9 in Appendix F), likely due to finding better representations.\n\n&nbsp;\n\n> the proposed method is just another weight consolidation method, and it's a bit hard to believe it beats Elastic Weight Consolidation.\n\nWhile there are strong connections and similarities between utility-gating and regularization-based methods, our approach differs from Elastic Weight Consolidation (EWC) in two major ways: 1) In EWC, a regularization term is essential, while our method has no such term., and 2) our proposed utility measure is different than EWC's squared gradient metric. The results from Figure 2 show the empirical superiority of our utility measure over the squared gradients, which partially explains the performance difference. More future theoretical work is needed to link between these two classes of methods. \n\n&nbsp;\n\n> It am not 100% sure that the proposed method doesn't favour plasticity over forgetting nor that the forgetting evaluation isn't biased towards methods that favour plasticity.\n\nIn our catastrophic forgetting (CF) evaluation, methods that only address CF can achieve high performance (e.g., S-MAS) since the problem is designed to favor methods that keep useful representations and improve on them continually. We do not think our CF evaluation is biased towards plasticity. Plasticity-inducing methods (e.g., Shrink and Perturb) achieve low performance in CF evaluation. \n\n&nbsp;\n\n**Answers:**\n\n> Just because utility of a given weight is, say, low for the current sample, it doesn't mean it's low for previous samples.\n\nThe utility measure we use is based on an exponential moving average of the sample utilities, as we explain in the last sentence of the second paragraph of section 4.2. Thus, it does not suffer from the issue the reviewer mentioned about sample instantaneous utilities.\n\n> how 4.3 measures catastrophic forgetting. Permuting labels of CIFAR10 with the new tasks suggests to me that it's all about plasticity.\n\nSection 4.3 indeed evaluates catastrophic forgetting, as previously learned representations transfer and remain useful from one task to another. Hence, methods that forget previously learned representations should suffer in performance. Our results also confirm that methods that protect useful representations and improve them with more data achieve higher performance (e.g., S-MAS). A method that does not protect useful representations (e.g., SGD) forgets them by overwriting useful information and, therefore, does not improve its performance continually.\n\n> Why are the accuracy results of training so poor?\n\nIn our streaming non-stationary problems, achieving state-of-the-art results as in stationary settings is unexpected due to their incomparability. Please see Figure 2b (rightmost of the three) in [1], where accuracy decreases with increased permutation frequency: from 99% (permuting every 1M samples) to about 88% (every 10K). Our experiment, with permutations every 5K samples, achieves a reasonable best accuracy of around 78%, aligning with this trend. \n\n> does the proposed method prevent forgetting at the high end?\n\nIn streaming learning, the evaluation is done on-the-fly at every time step, so maximizing the online performance (accuracy) at every step by reducing forgetting is desired. It\u2019s a different problem setting from the one the reviewer described, where the final performance is what is important. \n\n> does the per batch normalisation of utility make the performance of the method variable with different mini-batch size settings?\n\nIn our paper, we used a batch size of 1 (streaming case). Our method can also work with mini-batches where batch normalization might help.\n\n&nbsp;\n\n*References:*\n\n[1]. Dohare, S., Hernandez-Garcia, J. F., Rahman, P., Sutton, R. S., & Mahmood, A. R. (2023). Loss of Plasticity in Deep Continual Learning."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8965/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699939269350,
                "cdate": 1699939269350,
                "tmdate": 1699939269350,
                "mdate": 1699939269350,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "N8ipgSQcgB",
                "forum": "sKPzAXoylB",
                "replyto": "ZTB8L8Rya5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8965/Reviewer_y5kB"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8965/Reviewer_y5kB"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for response"
                    },
                    "comment": {
                        "value": "I would like to thank the authors for answering my questions.  They cleared up a number of my misunderstandings, which reassures my rating of marginally above acceptance threshold."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8965/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700642288202,
                "cdate": 1700642288202,
                "tmdate": 1700642288202,
                "mdate": 1700642288202,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "DvFEu4D6aC",
            "forum": "sKPzAXoylB",
            "replyto": "sKPzAXoylB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8965/Reviewer_XNx6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8965/Reviewer_XNx6"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes Utility-based Perturbed Gradient Descent(UPGD). A modification to the vanilla gradient descent update rule that helps the model to operate in a more challenging scenario of streaming learning. The authors introduced their utility function as an importance weight for each parameter of a neural network. The authors show the effectiveness of their contribution compared to common importance assignment methods in the continual learning literature."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "**Clear Structure and Writing:** The paper benefits from a clear structure and concise writing style.\n\n**Addressing a Complex Issue:** The authors tackle an underexplored yet challenging problem, and I appreciate their efforts to address online continual learning.\n\n**Mathematical Foundation:** The definition of utility introduced in the paper is based on simple and sound mathematical derivations. \n\n**New metric:** The introduction of a new plasticity metric is a nice contribution to the relatively uncharted territory of streaming learning."
                },
                "weaknesses": {
                    "value": "**Unscaled perturbations:** My main point of issue is the reasoning behind the perturbations in the update rule. The authors claim that by adding the perturbation we are making the unimportant weights more plastic however I am not really convinced by this explanation I believe it requires elaboration both in the rebuttal and in the paper. \n\nAnother related issue with the proposed perturbation is the fact that all of them are getting drawn from the same standard normal distribution. This design choice is strange to me since the parameters of a neural network usually differ in magnitude from layer to layer. By adding an unscaled random perturbation to all of the weights we are ignoring this scale difference which I believe is sub-optimal. I know that in the unprotected version, they are getting weighted by different values but this particular scaling is more correlating with changes in the loss value rather than the parameter magnitudes.\n\nHighly relevant to the above issue, I believe it is also necessary to have an additional ablation study, investigating the role of having and not having the perturbations in the update rule. I also want to disentangle the effect of weight decay. The only time that UG is added in the ablation is in the presence of WD. More specifically I am curious about the following scenarios in Figure 8: \n\n* Added ablations:\n    + SGD + UG + WP + WD (present in the paper)\n    + SGD + UG + WP \n    + SGD + UG + WD\n    + SGD + UG\n\n    \n**Including more diverse experiments:** Moreover, in the experiments section I believe the authors need to include more diverse experiments. All of the streaming tasks are permutations of the same task. Whether in the label or in the input space. It is not as obvious as the authors' claim that after the permutation of the input space the previously learned representations are not relevant anymore (end of page 6). In the input-permuted scenario, only the first layer needs to have significant change. This is especially true for the label-permuted tasks as the network does a good job of clustering the data up to the final FC layer. I encourage the authors to use the Cifar100 superclass dataset (or any similar sequence of tasks that does not simply rely on the permutation).\n\n**Visualization:** Finally, I believe the visualization needs several improvements: the legends on the plot are very hard to read (Fig 2, 3, 4, 5). Some colors are similar to each other and the width of the lines in the legends is too thin. (Especially in figure 4). In Figure 7, some numbers in dark blue cells are almost impossible to read."
                },
                "questions": {
                    "value": "**Q1:** Have the authors tried to use an scaled version of the perturbation that takes the magnitude of the parameters into account? (Other than the unprotected version). Also I would appreciate the if you could elaborate on the effect of perturbations.\n\n**Q2:** Could you also explain about the average online accuracy? it is stated that \"The average online accuracy is the percentage of correct predictions within each task.\" I cannot see the average part here. Is it calculating the accuracy on each task separately then averaging over the number of tasks?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8965/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8965/Reviewer_XNx6",
                        "ICLR.cc/2024/Conference/Submission8965/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8965/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699165868969,
            "cdate": 1699165868969,
            "tmdate": 1700672717423,
            "mdate": 1700672717423,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "T2wXYg3wzl",
                "forum": "sKPzAXoylB",
                "replyto": "DvFEu4D6aC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8965/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8965/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We would like to extend our sincere gratitude to the reviewer for the time and effort invested in reviewing our paper. We are also grateful to read that the reviewer thinks our paper tackles an underexplored but challenging problem and our paper is well-written with sound mathematical derivations. We are also happy that the reviewer appreciated our new plasticity metric contribution. Here, we give our response to the points the reviewer mentioned.\n\n&nbsp;\n\n> The authors claim that by adding the perturbation we are making the unimportant weights more plastic however, I am not really convinced\n\nNoise injection has been shown in the literature of plasticity as a way of introducing plasticity (see [1]). In Figure 3 of our work, S&P maintains its plasticity compared to SGD with weight decay, mainly because of noise perturbation (which is unscaled). While noise perturbation alone might not be effective (see PGD in the same figure), its role is pronounced when used with weight decay (S&P) or when used on a per-feature level [1]. We will add a sentence to make this clearer in our discussion of the method.\n\n&nbsp;\n\n> all of proposed perturbation are getting drawn from the same standard normal distribution...adding an unscaled random perturbation to all of the weights we are ignoring this scale difference\n\nFirst, we note that the standard deviation of the noise perturbation is a hyperparameter that can be tuned, which is effectively a scaling of the perturbation (see Table 2 and Table 3 for the values used). Second, we agree that noise perturbation can also use explicit layer-wise scaling. However, our utility scaling takes into account the importance of weight, meaning that weights in the earlier layers generally have higher utility and, therefore, perturbed less. Finally, we acknowledge that there are always ways to improve any method (we discussed some of them in Appendix M). Yet, our simple design was found to be very effective compared to other methods.\n\n&nbsp;\n\n> It is also necessary to have an additional ablation study, investigating the role of having and not having the perturbations in the update rule.\n\nWe conducted the requested ablation. Please see this figure [[link]](https://drive.google.com/file/d/15aeYYq3QjeEgAbeFVeqzSNT5Ms2QvSYr/view?usp=sharing). The requested ablation bolsters the contribution of utility gating even more. Using utility gating on top of sgd makes sgd maintain its performance instead of dropping on input-permuted MNIST and makes sgd improve its performance continually on label-permuted problems.\n\n&nbsp;\n\n> It is not as obvious as the authors' claim that after the permutation of the input space the previously learned representations are not relevant anymore\n\nWe agree with the reviewer on the comment about the phrase \u201cpreviously learned representations are not relevant anymore\u201d. We meant to use the word \"features\" instead of \"representations\". We will correct it in our revision. \n\n&nbsp;\n\n>Including more diverse experiments\n\nThe problems we use are extensively adopted by the continual learning community to evaluate forgetting or plasticity in a controlled manner. We tried split-based tasks, as the reviewer kindly suggested, with CIFAR 100, but we found that these problems become very easy to the extent that all methods achieve an online accuracy of above 98% because, in streaming learning with long sequences, tasks have to re-occur which makes the problem easy (see [2]). Therefore, we chose label-permuted tasks since task re-occurrence probability becomes very small. Finally, due to the complexity and significance of designing realistic benchmarks suitable for streaming learning, we believe it merits a dedicated paper to address fully.\n\n&nbsp;\n\n> I believe the visualization needs several improvements.\n\nWe thank the reviewer of this note. We will improve visualization in our revision. \n\n&nbsp;\n\n**Answers:**\n\n*A1:* The weight perturbations in UPGD are still scaled by utility. They are just scaled by the same utility the gradients are scaled by. Future work can study or improve the relative scaling between weight perturbation and gradient information.\n\n*A2:* In streaming learning, the learner receives a data stream and outputs a stream of predictions that are either correct (sample accuracy of 1) or incorrect (sample accuracy of 0). The average accuracy on a certain task is the average of these sample accuracies, which is also the percentage of correct predictions since the sample accuracies are either zero or one. We show the online average accuracy on each of the sequential tasks we have, so it is an average over each task separately.\n\\\n\\\n*References:*\n\\\n[1]. Dohare, S., Hernandez-Garcia, J. F., Rahman, P., Sutton, R. S., & Mahmood, A. R. (2023). Loss of Plasticity in Deep Continual Learning.\n\\\n[2] Lesort, T., Ostapenko, O., Rodriguez, P., Arefin, M. R., Misra, D., Charlin, L., & Rish, I. (2023). Challenging Common Assumptions about Catastrophic Forgetting and Knowledge Accumulation."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8965/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699939110769,
                "cdate": 1699939110769,
                "tmdate": 1699939110769,
                "mdate": 1699939110769,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZRx6ibpSG8",
                "forum": "sKPzAXoylB",
                "replyto": "T2wXYg3wzl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8965/Reviewer_XNx6"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8965/Reviewer_XNx6"
                ],
                "content": {
                    "title": {
                        "value": "Response to Authors"
                    },
                    "comment": {
                        "value": "I thank the authors for carefully addressing my concerns. **I have read the author's comments** and was convinced by their answers. The new ablation studies shed light on the role of individual components and it seems to me that each extension is contributing to improving the performance. I still think that there should be a better way of addressing the scale difference of the weights at different layers but at the same time, the weight decay, in a sense tries to mitigate that problem. Thereby, I raised my initial score."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8965/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700673219174,
                "cdate": 1700673219174,
                "tmdate": 1700673219174,
                "mdate": 1700673219174,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]