[
    {
        "title": "On Trajectory Augmentations for Off-Policy Evaluation"
    },
    {
        "review": {
            "id": "RJwJ0RbbfF",
            "forum": "eMNN0wIyVw",
            "replyto": "eMNN0wIyVw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5773/Reviewer_pod6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5773/Reviewer_pod6"
            ],
            "content": {
                "summary": {
                    "value": "The paper is proposed for the Off-policy evaluation tasks in the RL domain. The proposed method OAT intends to solve the scarce and underrepresented offline trajectory challenges by data augmentation. Specifically, a sub-trajectory mining method and fuse process is designed to find potential exploration space and integrate into existing state-action trajectories seamlessly."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper first proposes to augment the HIS trajectory data for offline RL Off-policy evaluation tasks, which is important for the real-world policy evaluation challenges.\n\n2. The process of detecting the potential sub-sequence is explicitly explained and demonstrated, easy to follow and understand, such as from the discrete representations to determine the support value, and eventually identify the PSTs.\n\n3. Authors applied the VAE-MDP process to generate the potential sub-trajectory, and empirical study shows the overall framework OAT is achieving promising results."
                },
                "weaknesses": {
                    "value": "1. The main paper omitted some significant parts and placed them in the appendix, however should be explained in the main context, such as how the latent latent prior is constructed. And how the C different clusters are initially divided when identifying the PST is not clear.\n\n2. Even though the experiment could show that the OPE method performance is improved, the paper is suggested to make a fair analysis of how reliable the reward is from the algorithm augmentation, which, however, is significant for the result value of off-policy methods."
                },
                "questions": {
                    "value": "1. Could the authors discuss if it's only applicable for the behavior data to be augmented? Or can we also augment the target policies? \n2. When identifying the PST, at the first step, How are these C different clusters divided? And how is the number of c determined?\n3. As shown in Fig3, It is suggested to introduce how the latent prior is constructed, since it is the key step in constructing the new augmented PST.\n4. What is the complexity of the training VAE-MDP? Is there any analysis conducted to show the relations between training time and TDSS length/Potential trajectory scale?\n5. In Fig 4, it is obvious that the trajectories coverages are different on the left hand and right hand corner on the top of the maze, however if these two corners are not the potential area selected to augment, should they maintain the original distribution to the maximum extent? Apart from this illustration, It is suggested that the paper provides quantitative evaluation of the difference between augmented trajectory and ground truth trajectory."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Privacy, security and safety"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "Healthcare and e-learning data is used in the paper, it is suggested to make sure it follows the privacy policies."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5773/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5773/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5773/Reviewer_pod6"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5773/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698552040696,
            "cdate": 1698552040696,
            "tmdate": 1700502300266,
            "mdate": 1700502300266,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "AZWJGbLrwa",
                "forum": "eMNN0wIyVw",
                "replyto": "RJwJ0RbbfF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5773/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5773/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors Responses (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for your time and efforts on evaluating our work. Please find our point-by-point response below.\n\nQ1. The main paper omitted some significant parts and placed them in the appendix, however should be explained in the main context, such as how the latent latent prior is constructed. And how the C different clusters are initially divided when identifying the PST is not clear.\n\nA1. Due to the space limitations, we leave the technical details straightforwardly adapting VAE into MDP in Appendix. But **we moved the details of constructing latent prior to Section 2.2 (highlighted in blue)**. The number of clusters is determined by a fully data-driven procedure following the original TICC paper (i.e., C is determined with the highest silhouette score in clustering historical trajectories) [1].  \n\nQ2. Even though the experiment could show that the OPE method performance is improved, the paper is suggested to make a fair analysis of how reliable the reward is from the algorithm augmentation, which, however, is significant for the result value of off-policy methods.\n\nA2. We provided Tables 4-16 in Appendix C.1, presenting the original results, in terms of four different metrics (i.e., mean absolute errors (MAE), rank correlation, regret@1, and regret@5), on each OPE method without and with augmentation in Adroit. Given MAE directly measures the distance between estimated and true returns, we can observe that OAT leads to more accurate return estimates for different OPE methods on target policies. Especially for IS that purely relies on the importance ratio over behavior and target actions and rewards from historical trajectories, OAT effectively facilitates it on different tasks, indicating OAT providing more reliable rewards. Moreover, **we added scatter plots plotting the true returns of each target policy against the estimated returns, in Appendix A.2.1.** We can observe that the estimated rewards of each OPE method were generally improved by OAT, with less distance to true rewards. \n\nQ3. Could the authors discuss if it's only applicable for the behavior data to be augmented? Or can we also augment the target policies?\n\nA3. Our works follow the general OPE problem setup [2], where **only a fixed set of offline data collected from behavioral policy is available**, and the goal is to use such data to estimate the return of target policies (that are different from behavioral policies), without any online deployment of the target policies. Consequently, we would not be able to augment the trajectories collected under target policies, as they would not be available. \n\nQ4. When identifying the PST, at the first step, How are these C different clusters divided? And how is the number of c determined?\n\nA4. The C different clusters are divided by solving the TICC problem [1], by capturing graphical connectivity structure of both temporal and cross-attributes information. **We added those details to Appendix A.3.1 (highlighted in blue)**. Following [1], C is determined by the one with the highest silhouette score clustering historical trajectories within range [10,20].\n\nQ5. As shown in Fig3, It is suggested to introduce how the latent prior is constructed, since it is the key step in constructing the new augmented PST.\n\nA5. Thanks for the suggestion. **We moved the details of constructing latent prior to Section 2.2 (highlighted in blue).**\n\nQ6. What is the complexity of the training VAE-MDP? Is there any analysis conducted to show the relations between training time and TDSS length/Potential trajectory scale?\n\nA6. Given that training VAE-MDP requires stochastic gradient descent algorithms with parameter tuning (e.g., for step size and network architecture), we found that analyzing the complexity theoretically would be challenging, as there lacks a standard framework to facilitate that. Instead we provide additional empirical results here on training time -- specifically, **we plotted relations between training time and length of PSTs/corresponding TDSSs, in Appendix A.1.1.** The results show that the training time of VAE-MDP is increased almost linearly with the length of PSTs."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5773/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700202771954,
                "cdate": 1700202771954,
                "tmdate": 1700202771954,
                "mdate": 1700202771954,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fUVjekUHSr",
                "forum": "eMNN0wIyVw",
                "replyto": "abxUsABBBR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5773/Reviewer_pod6"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5773/Reviewer_pod6"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the rebuttal"
                    },
                    "comment": {
                        "value": "I want to thank the authors for addressing my concerns and I have decided to update my review and raise my score."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5773/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700502272185,
                "cdate": 1700502272185,
                "tmdate": 1700502272185,
                "mdate": 1700502272185,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ykBVo14DUf",
            "forum": "eMNN0wIyVw",
            "replyto": "eMNN0wIyVw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5773/Reviewer_3xn9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5773/Reviewer_3xn9"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes to use a specialized form of data-augmentation, specifically to improve the accuracy of off-policy evaluation algorithms. It bases its algorithm on looking at promising sub-trajectories and augmenting the dataset with new samples to improve coverage of the state-action space."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper tackles an important problem of OPE, which receives relatively far less attention than control-based papers.\n2. Adapting data augmentation techniques for the RL setting, and particularly the OPE setting, is a very interesting direction.\n3. The paper applies the proposed idea to more real-life datasets, which is great since lot of OPE work just evaluates ideas on mujoco."
                },
                "weaknesses": {
                    "value": "1. It feels like the paper was somewhat rushed. There are some confusing parts/writing errors such as: what is \"human-involved RLs\" (third line in intro), \"while human may behave\" (5th line last para of intro), Figure 2 I believe should be referencing Sections 2.1/2.2/2.3 (not 3.X), \"intrinsic nature that human may follow\" (2nd line Section 2.1).\n2. The notation in Section 2.1 is very difficult to parse. I suspect there is an easier way to explain this. Also Figure 3 is very confusing, and is not explained in the text (there are some references in the appendix, but I think there should be much more explanation of it given the complexity of the diagram).\n3. I think drawing connections to how humans behave (such as in Figure 2 caption) is too strong. It is appropriate for intuition purposes but making general statements on how humans behave seems a bit strong.\n4. Biggest concern. I dont think the paper gives a good intuition for why this works. While coverage is important, it seems like that is insufficient. Another challenge is distribution shift. Prior work [1] has discussed that under extreme distribution shift, OPE methods can diverge. However, this work does not discuss this. Moreover, it seems possible to me that this data augmentation technique could introduce samples that worsen the distribution shift, which could worsen accuracy of OPE methods.\n\n[1] Instabilities of Offline RL with Pre-Trained Neural Representation. Wang et al. 2021."
                },
                "questions": {
                    "value": "1. Could one generate better samples by somehow tying in the evaluation policy behavior into the data generation process? It seems like if the goal is to evaluate some policy $\\pi_e$, we should account for what $\\pi_e$ does/samples actions?\n2. How is step 1 in the algorithm actually done? That is, under what basis are states grouped into clusters. Some  representation learning work does this based on behavior similarity [1] etc. How the states are grouped is important for all the remaining steps.\n3. The \"grounding\" process of Eqn 2 is unclear to me. How do you avoid the model from hallucinating and generating samples that cannot occur in the MDP since interaction with the MDP does not happen in the OPE setting?\n\n[1] MICo: Improved representations via sampling-based state similarity for Markov decision processes. Castro et al. 2021."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "n/a"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5773/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5773/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5773/Reviewer_3xn9"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5773/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698684070268,
            "cdate": 1698684070268,
            "tmdate": 1700283257690,
            "mdate": 1700283257690,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "E68XjaImQv",
                "forum": "eMNN0wIyVw",
                "replyto": "ykBVo14DUf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5773/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5773/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors Response"
                    },
                    "comment": {
                        "value": "Thank you for your time and efforts on evaluating our work. Please find our point-by-point response below.\n\nQ1. There are some confusing parts/writing errors such as: what is \"human-involved RLs\" (third line in intro), \"while human may behave\" (5th line last para of intro), Figure 2 I believe should be referencing Sections 2.1/2.2/2.3 (not 3.X), \"intrinsic nature that human may follow\" (2nd line Section 2.1).\n\nA1. Thanks for pointing out the typos. We have fixed the references on Figure 2 in the manuscript.\nFor the mentioned \u201cconfusing parts\u2019\u2019, please see our comments point-by-point below:\n* Human-involved RLs generally refer to the context of RL with human involvements, including human-involved decision-making processes, human-guided RL algorithms, etc., which are commonly discussed in related works [1-5].\n* The phrase, \u201cwhile human may behave\u201d, is discussed under the context that human-involved systems have limited state-action coverage, which can be caused by homogeneous behavior policies (as discussed in 1st paragraph of Introduction); on the other hand, human can behave diversely under different policies [6-7]. Following different policies can result in varied state-action coverage [8-10], which is a fundamental problem in RL.  \n* The phrase, \u201cintrinsic nature that human may follow\u201d, is under the context discussing an intrinsic nature/characteristic in human-involved systems, that human could follow homogeneous behavioral policies or specific guidelines when they perform their professions.\n\nWe have added more references in our manuscript, for readers' potential interests.\n\nQ2. The notation in Section 2.1 is very difficult to parse. I suspect there is an easier way to explain this. Also Figure 3 is very confusing, and is not explained in the text (there are some references in the appendix, but I think there should be much more explanation of it given the complexity of the diagram).\n\nA2. We understood that the notations/equations may be a bit intense in section 2.1, as we are the first work that introduces to OPE with sub-trajectory augmentation, which is a relatively new framework. As a result, we chose to present our methodology as detailed as possible so readers with different backgrounds can get the idea thoroughly (in case they would like to build on top in the future). We have double checked that there do not exist any sub-/super-scripts or variables that are redundant. **We further noticed that the other reviewer pod6 specifically pointed out that this part is easy to follow, as well as WbKh who pointed out our figures are informative -- we would greatly appreciate it if the reviewer can point out the specific part of methodology that is redundant/hard to follow.**\n\n\nQ3. I think drawing connections to how humans behave (such as in Figure 2 caption) is too strong. It is appropriate for intuition purposes but making general statements on how humans behave seems a bit strong.\n\nA3. When the trajectories are collected from humans, the states and/or actions are highly  related to human behaviors. Similar sub-trajectories may exhibit similar human behaviors. Such findings are reported in related works [21-23].\n\nQ4. I don't think the paper gives a good intuition for why this works. \n(Q4-1) While coverage is important, it seems like that is insufficient. \n\nA4-1. We respectfully disagree with the comment. Many existing works have justified that improving state-action coverage is an important and still open-ended problem [4, 8-10]. **Our work is the first to carry it onto the OPE domain, and attempts to solve it through offline trajectory augmentation.**\n\n(Q4-2) Another challenge is distribution shift. Prior work [1] has discussed that under extreme distribution shift, OPE methods can diverge. However, this work does not discuss this. \n\nA4-2. We agree distribution shift is a challenge for OPE. Although the mentioned work by Wang et al. only considers a very specific type of OPE method under a specific setting (i.e., FQE with linear approximation using pre-trained features from neural networks), DOPE benchmark [10] also found that OPE methods in general have such issues. *This is beyond the scope of this work, as we use existing OPE methods as backbones to process the augmented trajectories.* Hope in the future, the communities will come up with new OPE methods that can resolve the distribution shift in OPE."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5773/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700203356390,
                "cdate": 1700203356390,
                "tmdate": 1700203356390,
                "mdate": 1700203356390,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mOVTXu4kcX",
                "forum": "eMNN0wIyVw",
                "replyto": "Pbweb91NLy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5773/Reviewer_3xn9"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5773/Reviewer_3xn9"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks to authors for responding. I think offline RL + data augmentation is an important direction that the community should be looking at. I am a bit concerned that the method: 1) does not account for the possibility of worsening distribution shift and 2) may generate samples/transitions that do not comply with the true dynamics of the environment. While their algorithm worked in their empirical setting, I think it may fail in other settings (I am not sure where).\n\nBut as the authors point out, they believe these directions are for future work. I am sympathetic to this because I think initial work in data augmentation + offline RL is important, and perhaps others can build on this work to tackle the above problems.\n\nThat said, I will raise my score. However, I would insist that the authors: 1) clear all the typos (review the paper again for anything I may have missed) and 2) point the above two points as future work explicitly and explain their thoughts on how their algorithm relates to the two challenges. It will provide a basis to other researchers to know which parts of the algorithm can be improved."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5773/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700283210242,
                "cdate": 1700283210242,
                "tmdate": 1700283210242,
                "mdate": 1700283210242,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "VxxVBzFGoS",
            "forum": "eMNN0wIyVw",
            "replyto": "eMNN0wIyVw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5773/Reviewer_WbKh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5773/Reviewer_WbKh"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a novel approach to augment offline datasets for off-policy evaluation. This is achieved by introducing a three-step process (i) select relevant sub-trajectories in the dataset, (ii) use a VAE-based architecture to generate new trajectories, and (iii) add these trajectories back to the dataset.\n\nEmpirically, the authors show that the proposed method outperforms other data-augmentation methods on a diverse set of problems."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "I am not an expert in the field of data-augmentation for RL, but I enjoyed the thought process behind the development of the framework: (i) select a criterion for what makes a specific region of the state-action space interesting for data augmentation, and (ii) use temporal generative models to sample new sub-trajectories. \n\nFigures are generally informative and overall the paper is well written."
                },
                "weaknesses": {
                    "value": "In my opinion, the main weaknesses of this work lie in (i) the unsupported justifications of the results, (ii) the lack of ablations to validate the proposed innovations, and (iii) the relatively narrow-scoped experiments."
                },
                "questions": {
                    "value": "(i) The unsupported justifications of the results\nIn more than one occasion, the authors (rightly) discuss very specific reasons that could confirm/justify the observed results. However, I feel like in most cases in this work, the justifications are not supported by data. I feel this is better explained through an example. The authors say: \"In contrast, the historical trajectories induced from simulations tend to result in better coverage over the state-action space in general, and the augmentation methods that do not consider the Markovian setting may generate trajectories that could be less meaningful to the OPE methods, making them less effective.\" What do the authors mean by less meaningful? And do they believe that this (i.e., non-Markovian augmentation are worse in scenarios with better state coverage) can be confirmed more generally?\n\nIn my opinion, the authors use phrases like \"we conjecture\", \"a possible reason\" without strictly backing up the claims with evidence (which would in turn greatly improve the quality of the paper)\n\n(ii) The lack of ablations to validate the proposed innovations:\nThe authors compare against a wide set of benchmarks, although, as I'm not an expert in OPE, it is unclear to me whether these are explicitly tailored for the OPE problem or not. Moreover, since the proposed framework is a composed of multiple smaller contributions (discretization, VAE architecture, etc.), the authors should make sure to isolate each of these contributions individually and support the claims with evidence and experiments.\n\n(iii) The relatively narrow-scoped experiments:\nDid the authors consider using this approach within an RL context? How would this perform?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5773/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5773/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5773/Reviewer_WbKh"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5773/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698818142687,
            "cdate": 1698818142687,
            "tmdate": 1700501931798,
            "mdate": 1700501931798,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zy2VkNP9BC",
                "forum": "eMNN0wIyVw",
                "replyto": "VxxVBzFGoS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5773/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5773/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors Responses"
                    },
                    "comment": {
                        "value": "We sincerely appreciate your time and efforts on evaluating our work. Please find our point-by-point response below.\n\nQ1. The unsupported justifications of the results.\n\nA1. **We added related citations and detailed evidence to justify our findings and statements in the revised manuscript (highlighted in blue).**\n\nQ2. The lack of ablations to validate the proposed innovations: \n(Q2-1)The authors compare against a wide set of benchmarks, although, as I'm not an expert in OPE, it is unclear to me whether these are explicitly tailored for the OPE problem or not.\n\nA2-1. To the best of our knowledge, no prior work systematically investigates trajectory augmentation for facilitating OPE, our work is the first to investigate it and provide a possible solution (i.e., OAT). Thus, we examine a broad range of possible data augmentation methods, which were original proposed towards *data that may sharing some characteristics to offline trajectories*, or *sharing some intuitions with OAT*, including: 4 RL-oriented (TDA, Permutation, jittering, scaling), 2 generative models (TimeGAN, VAE), 2 time series-oriented methods by capturing similar behaviors (SPAWNER, DGW). Moreover, we investigated 5 popular OPE methods that are broadly used in prior works [1-11], spanning 3 major categories of OPE as defined by Yue\u2019s group [11].  \n\n(Q2-2) Moreover, since the proposed framework is a composed of multiple smaller contributions (discretization, VAE architecture, etc.), the authors should make sure to isolate each of these contributions individually and support the claims with evidence and experiments.\n\nA2-2. Given the key idea is to mine potential sub-trajectories and considering Markovian nature on trajectories, we compared OAT to several ablations:\n* OAT w.o. Discretization & support determination (TDA)\n* OAT w.o. PSTs mining (VAE-MDP)\n* OAT w.o. MDP (VAE)\n\nNote that both discretization and support determination are two common successive techniques to extract shared high-level representations (e.g., PSTs) from high-dimensional complex data [14-16]. Thus, we ablated the proposed PSTs mining from two perspectives: ablating the support determination to identify PSTs from TDSSs by randomly selecting sub-trajectories to augment (which aligns with the original idea of TDA [17]); ablating the concept of PSTs by applying VAE-MDP on entire trajectories (VAE-MDP). The third ablation is applying VAE on entire trajectories, without adaptation to Markovian setting.\n\nQ3. The relatively narrow-scoped experiments. Did the authors consider using this approach within an RL context? How would this perform?\n\nA3. In this work, we specifically focus on the OPE problem [1-10] as the improvements resulting from the model we designed can be isolated, as opposed to policy optimization where the policy improvement step will also impact the performance. We followed the guidelines and standardized procedures introduced in a recent benchmark, DOPE [1], from Levine's group, which provided for each D4RL environment the target policies to be evaluated as well as an off-policy training dataset. The effectiveness and robustness of OAT was extensively validated over **34 datasets with varied characteristics** including applications, state-action coverage, human involvements, horizons, density of rewards, dimensions of states and actions, etc. The experiments contained 2 real-world applications, education and healthcare. Moreover, OAT can be **stand-alone** to generate trajectories without any assumptions over target policies. And it can be easily utilized by built-on-top works such as policy optimization and representation learning. **We added such discussions in the conclusion section -- highlighted in blue.**\n\nWe hope these answers provide some explanations to address your concerns and showcase that our work is solving a significant challenge in a satisfying manner. We are happy to answer any followup questions or hear any comments from you."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5773/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700203023636,
                "cdate": 1700203023636,
                "tmdate": 1700203023636,
                "mdate": 1700203023636,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lULFfP1PDc",
                "forum": "eMNN0wIyVw",
                "replyto": "v4eLLEz54D",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5773/Reviewer_WbKh"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5773/Reviewer_WbKh"
                ],
                "content": {
                    "title": {
                        "value": "Feedback on author response"
                    },
                    "comment": {
                        "value": "I want to thank the authors for their efforts in addressing my concerns. Based on their answers, I have decided to update my review and raise my score."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5773/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700501916731,
                "cdate": 1700501916731,
                "tmdate": 1700501916731,
                "mdate": 1700501916731,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]