[
    {
        "title": "Distributed Linear Dimensionality Reduction Assisted by Centralized NN for Classification"
    },
    {
        "review": {
            "id": "NWSJFXNzER",
            "forum": "ID8IQUDM4v",
            "replyto": "ID8IQUDM4v",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5957/Reviewer_Tdne"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5957/Reviewer_Tdne"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on a classification scenario that the central server will take as input compressed data from distributed devices and then execute classification on those data. The paper claimed that the traditional linear dimension reduction methods like PCA and LDA cannot achieve good results especially when the distributed devices have different compression rates. Therefore they proposed to use a trainable linear transformation to accomplish the compression."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "- The paper is easy to understand and the proposed method is quite simple."
                },
                "weaknesses": {
                    "value": "- The method is too simple to be published in a top-tier machine learning conference. \n- I personally have some concerns about the experimental results.   \nPlease refer to the questions part. Thanks."
                },
                "questions": {
                    "value": "Major Problems:\n- Theoretical Section (3.1): The theoretical content in Section 3.1 appears to be somewhat redundant and perhaps unnecessary. In the current landscape of neural network research, discussions on the global optimality of neural networks have gained prominence [1,2]. Theorems 1 and Proposition 1 do not seem to introduce any particularly unique or insightful content compared to these existing discussions.\n- Experimental Settings: The choice of experimental settings raises some concerns. Since the paper deals with classification tasks, it is imperative to conduct experiments using well-established neural network models such as ResNet and Transformer, rather than Random Forest and K-Nearest Neighbors. Additionally, using more standard and widely recognized classification datasets like CIFAR10, CIFAR100, and ImageNet would be also necessary for evaluation. The choice of MNIST and a simple face classification dataset might be considered less suitable (it's weird to choose a face classification dataset as well). \n- Experimental Results: The experimental results presented in the paper are not entirely convincing. The lack of details on how PCA or LDA was employed in the experiments is a significant concern. To ensure fairness and clarity in the experiments, it is essential to compare the proposed method with PCA by only switching the $W_1$ to $W_N$ in Figure 3 to PCA, keeping both the DNN part and the transformations in the server side unchanged, and then training the neural network with the PCA-based data. Moreover, PCA, while not trainable like the proposed method, is still a linear transformation, and the paper should justify the observed test error gap between the two methods.\n\nMinor Problems:\n- Reference Format: The format of the references appears to be problematic, with the reference text mixed with the main text. I suggest reviewing and correcting the reference format to ensure it aligns with standard citation conventions.\n- Paper Title: The paper title may require reconsideration. The current title suggests that the proposed reduction method is assisted by the centralized neural network. However, it might be more appropriate to frame it as the centralized neural network being assisted by the proposed reduction method in adapting to data with varying compression rates.\n\n[1] Haeffele B D, Vidal R. Global optimality in neural network training, CVPR2017.    \n[2] Sun R. Optimization for deep learning: theory and algorithms[J]. arXiv preprint arXiv:1912.08957, 2019."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5957/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5957/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5957/Reviewer_Tdne"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5957/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698008727767,
            "cdate": 1698008727767,
            "tmdate": 1699636635806,
            "mdate": 1699636635806,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "AZBUiDM6Ut",
                "forum": "ID8IQUDM4v",
                "replyto": "NWSJFXNzER",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5957/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5957/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Tdne"
                    },
                    "comment": {
                        "value": "We appreciate your insightful feedback and constructive criticism. Here is our response to the points raised:\n\n\n1. Theoretical Section 3.1:\nWe understand your perspective on the theoretical content of Section 3.1. Our intention was to provide a foundational understanding, but we recognize the need to align more closely with existing discussions on neural network optimality. To address this, we will condense Section 3.1 to ensure our discussion is not redundant.\n\n2. Experimental Settings:\nYour suggestion to use more established neural network models and widely recognized datasets is well-taken. We plan to extend our experiments to include standard classification datasets like CIFAR10, CIFAR100, and ImageNet, addressing the concern regarding the initial dataset choices and enhancing the relevance and applicability of our findings in future work.\nIt's important to note that our design is inspired by the MLP-Mixer architecture, which has shown state-of-the-art performance on various real-world datasets, rivaling both ResNets and Transformers. \nThis aspect underscores the potential of our method in achieving high-performance results and will be highlighted in our revised experimental setup. \n\n3. Experimental Results:\nWe appreciate your concern regarding the clarity of our experimental approach, particularly in relation to the use of PCA and LDA. In our experiments, when comparing with PCA and LDA, we initially employed these methods to derive W1 to WN, and subsequently retrained the entire neural network with data processed by PCA/LDA. This approach aligns with your suggestion.\nOur findings indicate that, within this framework, our imbalanced-NN method significantly outperforms PCA/LDA.\nWe acknowledge that this was not sufficiently clear in our initial manuscript, and we will ensure to clarify this methodology and highlight these results in our revised submission."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5957/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700587301979,
                "cdate": 1700587301979,
                "tmdate": 1700587301979,
                "mdate": 1700587301979,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "yPEkFX5s5C",
            "forum": "ID8IQUDM4v",
            "replyto": "ID8IQUDM4v",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5957/Reviewer_X1NB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5957/Reviewer_X1NB"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a linear dimensionality reduction method for distributed edge devices, balancing resource constraints like data-rate and computing power at the device side, while ensuring high classification accuracy at the server side. The proposed method conducts the simultaneous training of a unique single-layer for each distributed device, determined by its compression needs, coupled with a centralized deep neural network on the server for all-device classification. When integrating a new device aiming to compress data in an untrained dimension, only minimal training for the device\u2019s initial two layers is needed, leaving the server\u2019s centralized deep neural network and the\ncompression layers for all existing devices untouched."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is well-written and easy to follow.\n\n2. The proposed method has correct derivations."
                },
                "weaknesses": {
                    "value": "1. It is unclear this method is useful in distributed learning. Actually there is no practical application for this proposed method. There is no need for linear dimensionality reduction. The deep neural networks conduct the nonlinear way and can achieve better performance.\n\n2. The experiments were conducted on extremely small dataset with a small number of devices."
                },
                "questions": {
                    "value": "The experiment section is very weak. In edge computing, we expect the system has large data and many devices."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5957/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698641158101,
            "cdate": 1698641158101,
            "tmdate": 1699636635676,
            "mdate": 1699636635676,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ovmaFmcEdP",
                "forum": "ID8IQUDM4v",
                "replyto": "yPEkFX5s5C",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5957/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5957/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer X1NB"
                    },
                    "comment": {
                        "value": "Thank you for your feedback. We recognize your concerns regarding the applicability of our method in distributed learning, its practical utility, and the scale of our experiments. We would like to address these points as follows:\n\n1. Practical Applications:\nThe practicality of our approach lies in scenarios where bandwidth and computational resources are at a premium. For instance, in IoT networks or remote sensor setups, where transmitting large amounts of data is impractical or costly, our method can significantly reduce transmission needs while still providing effective classification capabilities. We will clarify and expand on these application scenarios in our manuscript to better highlight the practical utility of our approach.\n\n2. Need for Linear Dimensionality Reduction:\nWhile it's true that DNNs perform nonlinear transformations and can yield high performance, they are not always the optimal choice in resource-constrained environments. Linear dimensionality reduction offers a valuable trade-off between complexity and performance, particularly in edge computing scenarios where device capabilities and network bandwidth are limited. Our method aims to optimize this trade-off.\n\n3. Scale of Experiments:\nWe acknowledge your concern regarding the scale of our experiments. Our initial experiments were designed to demonstrate the proof of concept and were hence conducted on a smaller scale for clarity and control. However, we recognize the importance of evaluating our method in more realistic settings involving larger datasets and a higher number of devices. We will  extend our experiments to include larger datasets and a more extensive network of devices to better replicate real-world edge computing environments in future work."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5957/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700587277711,
                "cdate": 1700587277711,
                "tmdate": 1700587277711,
                "mdate": 1700587277711,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "KHzbr4qfDh",
            "forum": "ID8IQUDM4v",
            "replyto": "ID8IQUDM4v",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5957/Reviewer_32g2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5957/Reviewer_32g2"
            ],
            "content": {
                "summary": {
                    "value": "This submission proposed to conduct data compression in client device and the compressed data are transfered to server device for leraning. Compression is performed by linear projection into various dimension in different clients. The server unifies the dimension by using a fully-connected layer for each client, then performan training for all data."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "N.A."
                },
                "weaknesses": {
                    "value": "It seems the proposed method contains few novelty: data compression by linear projection for transmission and re-projection for training is a very straight forward idea."
                },
                "questions": {
                    "value": "I don't have question currently. Please clarify my concern on novelty."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N.A."
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5957/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5957/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5957/Reviewer_32g2"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5957/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698676640463,
            "cdate": 1698676640463,
            "tmdate": 1699636635584,
            "mdate": 1699636635584,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "LBlLN4rPfV",
                "forum": "ID8IQUDM4v",
                "replyto": "KHzbr4qfDh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5957/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5957/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 32g2"
                    },
                    "comment": {
                        "value": "We appreciate your feedback and the opportunity to clarify the novelty aspects of our proposed method. While at first glance, the approach of data compression via linear projection for transmission and re-projection for training might appear straightforward, the novelty of our work lies in the specific implementation and the unique challenges it addresses in the context of edge computing:\n\n1. Imbalanced Neural Network Approach: Our method goes beyond simple linear projection. It introduces an imbalanced Neural Network (NN)-based approach that is specifically tailored for resource-constrained edge devices. This aspect of our work is not just about compression but also about optimizing the balance between data reduction and classification accuracy, which is a non-trivial challenge in edge computing scenarios.\n\n2. Superior Performance Over Existing Linear Methods: An additional point to highlight is the performance superiority of our approach compared to existing linear methods. Despite its seemingly straightforward nature, our method outperforms all current linear dimensionality reduction techniques in the literature, particularly in the context of similar computational complexity at the device side. This performance edge is a significant contribution of our work, as it demonstrates that even 'simple' methods, when innovatively applied and fine-tuned, can lead to substantial advancements in the field. \n\n3. Adaptability to Device Constraints: Another innovative aspect of our approach is its adaptability to different device capabilities, particularly in scenarios with devices of varying computational powers and data dimensions. Our technique dynamically adjusts to these variations, which is a significant advancement over traditional methods that often assume uniform device capabilities.\n\n4. Integration of New Devices with Minimal Retraining: Another novel contribution is the method's ability to integrate new devices with different data dimensions into an existing network with minimal retraining. This feature significantly reduces the computational overhead and is particularly beneficial in real-world applications where edge networks are dynamic and continually evolving."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5957/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700587251947,
                "cdate": 1700587251947,
                "tmdate": 1700587251947,
                "mdate": 1700587251947,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "9WhGoY01Nl",
            "forum": "ID8IQUDM4v",
            "replyto": "ID8IQUDM4v",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5957/Reviewer_YX5B"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5957/Reviewer_YX5B"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a linear dimensionality reduction technique specifically designed for distributed edge devices. The primary goal is to balance the constraints of data-rate and computing power on the device side while ensuring high classification accuracy on the server side. The approach involves training a unique single-layer for each distributed device based on its compression needs. The paper claims that the accuracy achieved through this method is close to the optimal accuracy of the Maximum Likelihood classifier, outperforming traditional techniques like PCA and LDA. Additionally, the method offers reduced training complexity for large datasets compared to distance-metric-based strategies."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. The method allows for the easy integration of new devices without the need to retrain the entire system."
                },
                "weaknesses": {
                    "value": "1. The evaluation is only performed on very small scale dataset, which is a toy dataset for modern NN system. It's not persuasive for the effectiveness of proposed methodology, especially under such a practical application scenario. I would suggest use larger dataset like images for autonomous driving, multi-dimensional time series data, etc.\n2. For the problem setting in section 2, why is this topic important? why is this problem challenging?\n3.  I did not see much technical merits of the proposal methodology. I would suggest the author highlight the technical contribution, conclude it with an illustrative figure and explain with plain words.\n4. There is no testing performed on real devices. We cannot see the improvement of efficiency."
                },
                "questions": {
                    "value": "1. What are the popular datasets for this domain and the popular testbeds/devices for the problem?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5957/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698687440125,
            "cdate": 1698687440125,
            "tmdate": 1699636635450,
            "mdate": 1699636635450,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "67RlW9ZXHk",
                "forum": "ID8IQUDM4v",
                "replyto": "9WhGoY01Nl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5957/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5957/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer YX5B"
                    },
                    "comment": {
                        "value": "We appreciate your detailed review and valuable suggestions. Below, we address each of your points:\n\n1. Use of Larger Datasets:\nWe acknowledge your concern regarding the dataset scale. The initial choice of a smaller dataset was driven by its common use as a benchmark in this domain, allowing for direct comparison with existing methods. However, we understand the need for testing on larger, more complex datasets to better demonstrate the effectiveness of our methodology. We will  conducting additional experiments using larger datasets in future work.\n\n2. Importance and Challenges of the Problem Setting:\nThis research is crucial as it addresses the growing need for efficient data processing in low-cost edge devices, which are often constrained by limited computational resources and bandwidth. The challenge lies in performing effective dimensionality reduction without compromising classification accuracy, even when new devices with untrained dimensions are introduced. We will revise Section 2 to better highlight the importance and the challenges of the problem setting.\n\n3. Technical Merits and Contributions:\nWe understand that the technical merits of our proposed method might not have been clearly articulated. Our primary contribution lies in developing an imbalanced NN-based linear dimensionality reduction technique that is adaptable to varying device capabilities and can integrate new devices with minimal retraining. This approach significantly reduces the computational load on edge devices and conserves bandwidth.\n\n4. Datasets and Testbeds/Devices in the Domain:\nPopular datasets in this domain, besides the ones we used, include CIFAR and ImageNet for image classification. Common testbeds for edge computing scenarios include Raspberry Pi devices.\nWe might investigate them in future research."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5957/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700587226018,
                "cdate": 1700587226018,
                "tmdate": 1700587226018,
                "mdate": 1700587226018,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]