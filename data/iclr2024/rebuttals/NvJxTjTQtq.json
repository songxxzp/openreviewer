[
    {
        "title": "EGraFFBench: Evaluation of Equivariant Graph Neural Network Force Fields for Atomistic Simulations"
    },
    {
        "review": {
            "id": "AJA3blBkQy",
            "forum": "NvJxTjTQtq",
            "replyto": "NvJxTjTQtq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7127/Reviewer_Kn6d"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7127/Reviewer_Kn6d"
            ],
            "content": {
                "summary": {
                    "value": "This work performs a systematic benchmarking of six equivariant graph neural networks designed for force field prediction. It conducts an analysis of how these models behave in realistic atomistic simulations. This work also investigated the generalization ability of these models on out-of-distribution data."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. This work proposes two new datasets, GeTe and LiPS20. Based on LiPS20, it proposes a new OOD task that aims to evaluate the model\u2019s generalizability to unseen crystalline structures or unseen composition. \n\n2. This work proposes four metrics to evaluate how these models perform in molecular simulations."
                },
                "weaknesses": {
                    "value": "1.\tI have serious doubts about the experimental results in Table 1. The MD17 results of NequIP and Equiformer are much worse than reported in their original papers. Although BOTNet and MACE report rMD17 results instead of MD17 in their papers, it doesn\u2019t make any sense that their performance on MD17 will be such bad. I don\u2019t think authors run their code correctly. \n\n2.\tSince the molecular dynamic simulations are based on frozen models that are trained to predict energy and force, the quality of Table 1 makes the MD simulation not convincing. \n\n3.\tAuthors claim that they propose three new challenging tasks. However, the evaluation of model generalizability to higher temperatures (Sec 4.4.2) is not new. Actually, it\u2019s proposed by LinearACE [1] and this task has also been studied by Allegro, BOTNet, and MACE. Although these works are focused on energy and force errors, the conclusion that \u201cOOD is challenging\u201d is not surprising to me. \n\n4.\tConcluding insights in Sec 5 are not surprising to researchers studying graph neural networks for force fields. Technical novelty is very limited. \n\n[1]. Kov\u00e1cs, D\u00e1vid P\u00e9ter, et al. \"Linear atomic cluster expansion force fields for organic molecules: beyond rmse.\" Journal of chemical theory and computation 17.12 (2021): 7696-7711."
                },
                "questions": {
                    "value": "1.\tIn Figure 3, does PDF refer to Pair Distribution Functions?\n2.\tErrors in energy and force are commonly used as evaluation metrics for force field prediction. The EV and FV proposed by this work are similar to these two metrics. Considering RDF is a good idea, but there\u2019re more metrics can be considered to evaluate molecular dynamics. For example, RMSD and temperature are used in [2]. Could you discuss why other metrics like RMSD and temperature are not selected as metrics in this benchmark?\n\n[2]. Musaelian, Albert, et al. \"Scaling the leading accuracy of deep equivariant models to biomolecular simulations of realistic size.\" arXiv preprint arXiv:2304.10061 (2023)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7127/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7127/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7127/Reviewer_Kn6d"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7127/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697575408862,
            "cdate": 1697575408862,
            "tmdate": 1700669246590,
            "mdate": 1700669246590,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xz2WxVK68D",
                "forum": "NvJxTjTQtq",
                "replyto": "AJA3blBkQy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7127/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7127/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Kn6d: part 1"
                    },
                    "comment": {
                        "value": "*Q1. I have serious doubts about the experimental results in Table 1. The MD17 results of NequIP and Equiformer are much worse than reported in their original papers. Although BOTNet and MACE report rMD17 results instead of MD17 in their papers, it doesn\u2019t make any sense that their performance on MD17 will be such bad. I don\u2019t think authors run their code correctly.*\n\n**Response**: We thank the reviewer for the careful reading and critical comments. We appreciate the opportunity to address your concerns, which has indeed enhanced the quality of our work. Please find below our detailed response to the issues raised:\n\n1. Rectification of Methodological Issues: We are grateful for your observation regarding the high MAE values. We carefully looked at the codes, log files, and the datasets, and identified an issue related to the MACE model for the MD17 dataset. Specifically, while training the models on MD17, the periodic boundary condition was kept as 'True' while keeping the box large enough to avoid any self interaction. However, the internal architectural setup of MACE (and BOTNet) requires the lattice parameter to be specifically mentioned when the PBC is set to \"True\", even for small molecules. To address this concern, we have now re-trained all the models on the MD17 dataset without PBC and updated all the Tables and results associated with it. We believe these revised results align more closely with the expected outcomes based on the original papers of the compared models.\n\n2. Comparison with literature data: Further to compare our reported results with existing literature studies, we have included a new Table 19 in our revised manuscript. This table provides a direct comparison of our results with those reported in the existing literature. It can be observed that the values obtained in the present work are comparable with those available in the literature. We believe this addition will offer a clearer context and a more comprehensive understanding of our experimental outcomes.\n\nHowever, it should be noted that the results cannot be directly compared with the literature in many cases as the literature does not clearly provide the exact training-dev-test dataset. To address this concern, we have included the exact training-dev-test datasets, codes, hyperparameters, config files, and log files in the repository associated with the present work. For example, DimeNET and PaiNN model in Fu et al. was trained on 9500 configurations. In our study, we have used a training set of 950 configurations across all the models for a fair comparison. The dataset size was limited due to the high computational cost in transformer models. \n\nOverall, with the release of all hyper-parameters, train-dev-test splits, etc., we believe we present the most transparent and reproducible evaluation of GNN force fields. Nonetheless, if there exist any outstanding concerns regarding codes or results, we request the reviewer to raise any of those. \n\n*Q2. Since the molecular dynamic simulations are based on frozen models that are trained to predict energy and force, the quality of Table 1 makes the MD simulation not convincing.*\n\n**Response:** We agree with the reviewer that higher energy and force error in training will make the MD simulation unrealistic. As mentioned in the previous comment, we have now retrained the models for the MD17 datasets, and performed MD simulation with the new trained models and corrected the results accordingly in Tables 1, 2 and 3. \n\nMoreover, as mentioned earlier, all the codes and datasets to reproduce to the present work are made available in the GitHub (https://anonymous.4open.science/r/MDBENCHGNN-BF68). The logfiles are also included for easy analysis. With these additional data, we hope the reviewer can evaluate the results and authenticity of the present work in a transparent fashion. \n\nNevertheless, if there are any outstanding concerns, we request the reviewer to raise those."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7127/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700367401668,
                "cdate": 1700367401668,
                "tmdate": 1700367401668,
                "mdate": 1700367401668,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Byv3P1eTWV",
                "forum": "NvJxTjTQtq",
                "replyto": "AJA3blBkQy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7127/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7127/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Keenly awaiting post-rebuttal feedback"
                    },
                    "comment": {
                        "value": "Dear Reviewer Kn6d,\n\nSince we are into the last few hours of author-reviewer discussion, we are keenly awaiting your feedback on the responses to your comments. Specifically, we have \n* **retrained all the models on MD17**,\n* **compared the results with the literature**, \n* **included two additional models, namely DimeNet++ and PaiNN**, and performed all the tasks on these models,\n* **released all the log files, codes, and datasets** and \n* **addressed all other concerns raised**.\n\nWith these changes, we hope the reviewer finds the manuscript suitable for ICLR as a benchmarking paper. If there are any outstanding, we request you to raise those. Otherwise, we request you support the manuscript by increasing the score. Looking forward to your response."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7127/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700656428306,
                "cdate": 1700656428306,
                "tmdate": 1700668374191,
                "mdate": 1700668374191,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "MSVl6j7kVW",
                "forum": "NvJxTjTQtq",
                "replyto": "Byv3P1eTWV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7127/Reviewer_Kn6d"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7127/Reviewer_Kn6d"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for correcting the MD17 experiments. I've raised the score and will encourage further improvement in the manuscript."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7127/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700669269448,
                "cdate": 1700669269448,
                "tmdate": 1700669269448,
                "mdate": 1700669269448,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "9uh9h5Qhip",
            "forum": "NvJxTjTQtq",
            "replyto": "NvJxTjTQtq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7127/Reviewer_dGG5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7127/Reviewer_dGG5"
            ],
            "content": {
                "summary": {
                    "value": "The benchmarks 6 different equivariant force field models using a combination of existing and new benchmarks. The authors use several evaluation metrics (e.g. predicted structures, dynamics, evaluation time) in addition to traditional force / energy MAE. The aim of the paper is to evaluate the methods in a variety of realistic settings and to determine the strengths / weakness of the methods tested."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is thorough and benchmarking 6 models on such a breadth of tasks is a technical challenge in itself and helpful to the community. The paper provides concrete observations such as the lack of transferability of IPs trained on single molecules even to molecules of similar composition and structure. It also provides concrete conclusions that suggest paths for improvement of equivariant graph force fields."
                },
                "weaknesses": {
                    "value": "I believe this would be difficult to do given the amount of work it would take, but it would be really valuable to have more of an ablation style study of what particular architectural choices help / hurt in different metrics. Even with the present benchmarks, there are many architectural difference between these models that the takeaways are a bit binary -- e.g. this model is or is not enough for this task -- rather than -- e.g. this architectural choice seems to help with X. If you think that your results can support such guidance I think that would be extremely useful to the community."
                },
                "questions": {
                    "value": "For each of the models, you can predict forces either as a direct prediction or via backprop to atomic coordinates. From 2.1 it seems that backprop was always used for forces for this benchmark (which makes sense given the desire for conservative forces). However, it would still be interesting to contrast the evaluation efficiency gain vs. stability loss in this setting and whether this changed substantially between methods. Do you have any runs that would give insight into this?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7127/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7127/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7127/Reviewer_dGG5"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7127/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698768105764,
            "cdate": 1698768105764,
            "tmdate": 1699636843240,
            "mdate": 1699636843240,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "s5E7tp3acw",
                "forum": "NvJxTjTQtq",
                "replyto": "9uh9h5Qhip",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7127/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7127/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer dGG5"
                    },
                    "comment": {
                        "value": "*Q1. I believe this would be difficult to do given the amount of work it would take, but it would be really valuable to have more of an ablation style study of what particular architectural choices help / hurt in different metrics. Even with the present benchmarks, there are many architectural difference between these models that the takeaways are a bit binary -- e.g. this model is or is not enough for this task -- rather than -- e.g. this architectural choice seems to help with X. If you think that your results can support such guidance I think that would be extremely useful to the community.*\n\n**Response:** Indeed, we observe that models perform different across different atomic systems. This could be attributed to the differences in the nature of bonding, size of the molecules or atomic systems, and the nature of the training set in terms of how much scenario it covers. A detailed analysis of which features associated with each architecture benefits a given dataset is proposed as a future work. \n\nTo address this comment, we have now included additional text in the Limitations and future work section of the main manuscript as follows.\n> Another interesting aspect is the empirical evaluation of which particular architectural feature of a model helps in giving a superior performance for a given dataset or system (defined by the type of bonding, number of atoms, crystalline vs disordered, etc.). Such a detailed analysis can be a guide to designing improved architecture while also providing thumb rules toward the use of an appropriate architecture for a given system.\n\n*Q2. For each of the models, you can predict forces either as a direct prediction or via backprop to atomic coordinates. From 2.1 it seems that backprop was always used for forces for this benchmark (which makes sense given the desire for conservative forces). However, it would still be interesting to contrast the evaluation efficiency gain vs. stability loss in this setting and whether this changed substantially between methods. Do you have any runs that would give insight into this?*\n\n**Response**: We agree with the reviewer that a study evaluating the direct force architecture vs backprop force architecture would be insightful. However, we did not employ this approach due to two reasons as follows. \n1. Being a benchmarking work, we adhered to the architectures as reported in the original works. \n2. These models specifically considered backpropagation for force calculations to make them conservative as the reviewer rightly pointed out. This property is crucial to run stable molecular dynamics simulations.\n\nIndeed, to address this comment, we attempted to directly predict forces from the model. However, several major architectural modifications would be required to achieve this in some of the models. For instance, allegro predicts the energy directly from the edge embedding, which is then summed up to obtain the total energy. Thus, obtaining the per atom force from this architecture requires a non-trivial modification of the architecture. Nevertheless, we thank the reviewer for this suggestion. We will consider this approach as part of a future study."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7127/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700366556790,
                "cdate": 1700366556790,
                "tmdate": 1700366556790,
                "mdate": 1700366556790,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "fEG37ZI6Mp",
            "forum": "NvJxTjTQtq",
            "replyto": "NvJxTjTQtq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7127/Reviewer_ewM6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7127/Reviewer_ewM6"
            ],
            "content": {
                "summary": {
                    "value": "In this work, the authors conduct benchmarking of equivariant GNN force field for molecular simulations. The work includes some latest equivariant models and introduces 2 more datasets. Besides, the work introduces structure based metrics and dynamic metrics. The former evaluates how ML simulated molecular structures compare with ground truth and the latter evaluates how ML simulated forces/energies compare with ground truth. It further evaluate the performance on out-of-distribution data and shows that none of the models perform reliably in the proposed setting."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Thorough evaluation of ML force field rather than accuracy of energy/force is important in applying to molecular simualtions. 2\n2. This work extends previous benchmark efforts with latest equivariant GNN/Transformer models. \n3. This work introduce new datasets and evaluation metrics for ML force fields."
                },
                "weaknesses": {
                    "value": "1. The work neglects some useful metrics from the previous works, like stability, RDF, diffusivity, etc in [1]. It would be better to include the performance of latest models on these metrics. \n2. Though new datasets and metrics are proposed. The major conclusion that low energy/force doesn't guarantee performing well in molecular simulations is not fresh, as pointed out in [1]. \n3. Some benchmarking settings may not be convincing and not reflect the scenario in real applications. More discussions are included in the following Questions section. \n\n[1] Fu, X., Wu, Z., Wang, W., Xie, T., Keten, S., Gomez-Bombarelli, R. and Jaakkola, T., 2022. Forces are not enough: Benchmark and critical evaluation for machine learning force fields with molecular simulations. (https://openreview.net/forum?id=A8pqQipwkt)"
                },
                "questions": {
                    "value": "1. The work include latest equivariant models. However, most models reported in previous works [1] are ignored. The authors may consider adding some more models to further validate the limitations of previous datasets and metrics. \n2. What are the hyperparameter settings for the equivariant models in the experiments?\n3. In Table 1, there are models that perform well on one dataset but fail on the other. For example, allegro performs well on MD17 but is really bad on GeTe. What are the differences in the molecular systems that lead to the divergence? Are there possible under-fit?\n4. In section 4.4.1, ML models are trained on a subset of 3 moelcules in MD17 and evaluated on another molecule. Not surprisingly, none of the models perform well in this setting as the types and number of molecular data are limited. However, this may not reflect the real application. [2] unveils that allegro pre-trained on SPICE, a large dataset with molecules < 100 atoms, can conduct molecular simulations of large molecular systems with 1M atoms. So a more realistic setting for OOD generalization may be training on large datasets with a wide variety of small molecules and test how it performs on other molecular systems. \n5. For the dynamics metrics, if two molecular systems start from the same initial structure but different initial velocities, they can diverge later. When that happens, EV & FV may fail to provide meaningful evaluations. Are there controls over the initial configuration when comparing ML-based simulations with ground truth?\n6. How does EGraFFBench handle periodic boundary condition (PBC)?\n7. In table 2, some models are not properly highlighted though superior performance is achieved. \n\n\n[1] Fu, X., Wu, Z., Wang, W., Xie, T., Keten, S., Gomez-Bombarelli, R. and Jaakkola, T., 2022. Forces are not enough: Benchmark and critical evaluation for machine learning force fields with molecular simulations. (https://openreview.net/forum?id=A8pqQipwkt)\n\n[2] Musaelian, A., Johansson, A., Batzner, S. and Kozinsky, B., 2023. Scaling the leading accuracy of deep equivariant models to biomolecular simulations of realistic size. arXiv preprint arXiv:2304.10061. (https://arxiv.org/abs/2304.10061)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7127/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7127/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7127/Reviewer_ewM6"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7127/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699405715511,
            "cdate": 1699405715511,
            "tmdate": 1700668873788,
            "mdate": 1700668873788,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "LuUGbwCeab",
                "forum": "NvJxTjTQtq",
                "replyto": "fEG37ZI6Mp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7127/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7127/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ewM6: part 1"
                    },
                    "comment": {
                        "value": "*Q1. The work neglects some useful metrics from the previous works, like stability, RDF, diffusivity, etc in [1]. It would be better to include the performance of latest models on these metrics.*\n\n**Response**: Thank you for raising this concern. Indeed, previous work referred to by the reviewer uses stability, RDF, and diffusivity. Among these metrics, while stability is an evaluation of dynamics, RDF is an evaluation of the structure. In the present work, we have proposed metrics that capture both structure and dynamics in a more systematic fashion. Specifically, to evaluate the structure, we have already added two RDF-based metrics, namely JS Divergence and Wright Factor. While the first metric captures how close the distributions of predicted and actual RDF is, the second metric represents the relative error between the predicted and actual RDF. Similarly, stability of the dynamics is captured through energy and force violation. Specifically, in these two metrics, we compare the energy and force on a molecular dynamics (MD) simulation performed with the machine learned potential with respect to the ground truth trajectory. Thus, the force and energy violation error clearly evaluate the divergence of force and energy of the simulated trajectory with respect to the ground truth. These metrics, in turn, give a clear indication of the stability of the simulation. \n\nThe third metric proposed, namely, diffusion is not necessarily metric; it is rather a property of the system which is derived from the dynamics. Note that there are several such properties which can be obtained from the MD simulations. Nevertheless, in order to address the concern, we have now **included RMSD** (note that diffusion constant is the slope of MSD with respect to time) of the trajectories for all the datasets and models in Appendix A.9 (see Figs. 8 and 9).\n\n*Q2. Though new datasets and metrics are proposed. The major conclusion that low energy/force doesn't guarantee performing well in molecular simulations is not fresh, as pointed out in [1].*\n\n**Response**: We agree with the reviewer that the low energy or force doesn't inherently ensure superior performance in molecular simulations, which has also been highlighted previously [1]. However, we do not consider this to be the major contribution of the work. \n\nThe main contributions of our work are in the novel tasks proposed to evaluate equivariant graph neural network forcefields as detailed below. \n1. **Complex tasks:** We introduce complex tasks that evaluate ML potentials for realistic applications. This involves training on diverse liquid systems with varying chemical compositions (LiPS20, see App. A.2, Table 11) and assessing performance on the dynamic simulations of unseen crystalline and disordered structures. To the best of our knowledge, this is the first time such a benchmarking task has been performed on equivariant graph neural network force fields (see Sec 4.4). Such tasks can be a critical one for evaluating any new forcefield for materials discovery.\n2. **Equivariant graph neural network forcefield:** This is the first work that evaluates state-of-the-art equivariant graph neural network forcefields in a consistent fashion on the same dataset and on challenging tasks with rigorous structure and dynamics metrics. To further emphasize this, we show table 21 and table 10, the datasets on which previous works have evaluated the equivariant GNN forcefields. It can be observed that there is a clear lack of systematic evaluation of these forcefields.\n\nWe also thank the reviewer for noting the contribution of new datasets and metrics in our work. In addition, the key insights obtained from the work are included in the conclusion section in a point-by-point fashion.\n\n*Q3. Some benchmarking settings may not be convincing and not reflect the scenario in real applications. More discussions are included in the following Questions section.*\n\n**Response:** In our assessment, we've thoroughly considered the questions raised regarding the benchmarking settings. Our approach aims to encompass a diverse range of tasks and metrics to gauge performance comprehensively. We have addressed the concerns raised by the reviewer in the Questions section. We're open to further discussions to refine our benchmarking methodology and ensure a more accurate representation aligned with real applications."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7127/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700366367730,
                "cdate": 1700366367730,
                "tmdate": 1700366367730,
                "mdate": 1700366367730,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Q3Vu7E5Q6v",
                "forum": "NvJxTjTQtq",
                "replyto": "fEG37ZI6Mp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7127/Reviewer_ewM6"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7127/Reviewer_ewM6"
                ],
                "content": {
                    "title": {
                        "value": "Official Comments by Reviewer ewM6"
                    },
                    "comment": {
                        "value": "I appreciate the authors' efforts in answering my questions. However, there are still some questions:\n1. Indeed diffusivity is a property from MD simulations, but it could still be an important metric to evaluate ML force field. Since in most cases for MD simulations, we care about such statistical properties rather than the accuracy of each timestep. \n2. I thank the authors for including the hyperparameter in A.10. However, as reviewer Kn6d pointed out, the reproduced results (as shown in Table 19) are usually worse than the number reported in original and other works, especially for the new added PaiNN and DimeNet++. So I still have the concern of underfitting."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7127/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700594115262,
                "cdate": 1700594115262,
                "tmdate": 1700594115262,
                "mdate": 1700594115262,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "y7uGNeh7Cd",
                "forum": "NvJxTjTQtq",
                "replyto": "FGQmv0pDJb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7127/Reviewer_ewM6"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7127/Reviewer_ewM6"
                ],
                "content": {
                    "title": {
                        "value": "Official Comments by Reviewer ewM6"
                    },
                    "comment": {
                        "value": "I thank the authors for the clarification. I have increased my score."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7127/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700668850405,
                "cdate": 1700668850405,
                "tmdate": 1700668850405,
                "mdate": 1700668850405,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]