[
    {
        "title": "How I Warped Your Noise: a Temporally-Correlated Noise Prior for Diffusion Models"
    },
    {
        "review": {
            "id": "bxgUDtCuXN",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3760/Reviewer_tVGv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3760/Reviewer_tVGv"
            ],
            "forum": "pzElnMrgSD",
            "replyto": "pzElnMrgSD",
            "content": {
                "summary": {
                    "value": "This paper studied the influence of initialized noise when adopting diffusion based image model to video model.\n\nA new initialization method called integral noise is propose, which is composed of a conditional upsampling, rasterization, and aggregation stage. It can maximizing the correlation between the warped and the original sample and maintain the independence of pixels in each sample."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "The method is well illustrated in Figure 2 and is easy to understand.\nA deep and comprehensive theoretical analysis is provided in the method and supplementary material.\nThe method is evaluated on several tasks, including rerendering (SDEdit), pose-to-person, and video restoration.\nGood video illustration in supplementary material."
                },
                "weaknesses": {
                    "value": "I guess most experiments in this paper are conducted in zero-shot way which applies the image processing model and method to video processing problems. Looking forward to seeing whether this helps when training a video model, like the PYoCo model\nConsidering video restoration (**Zeroscope Text-to-Video**) and pose-to-human (dreampose) already have good performance, I wonder the gap between image model+ integrate noise and these video models.\nAs stated in Appendix E, integrat noise does not work well in latent diffusion model. I wonder if this is applicable to other pixel-level diffusion models like the open-sourced deepfloyd from stabilityAI, whose capacity is comparable or better than stable diffusion."
                },
                "questions": {
                    "value": "I am happy to see the comparison with the video processing model and the integration with DeepFloyd"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3760/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3760/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3760/Reviewer_tVGv"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3760/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697434409292,
            "cdate": 1697434409292,
            "tmdate": 1699636332376,
            "mdate": 1699636332376,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Us6DZFikzq",
                "forum": "pzElnMrgSD",
                "replyto": "bxgUDtCuXN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3760/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3760/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer tVGv"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the constructive feedback. We will address the remaining questions below.\n\n---\n\n**Zero-shot and training**\n\n>I guess most experiments in this paper are conducted in zero-shot way which applies the image processing model and method to video processing problems. Looking forward to seeing whether this helps when training a video model, like the PYoCo model.\n\nThis is correct, we propose our noise warping method primarily as a novel tool to improve temporal coherency of video editing tasks in a zero-shot setting. This is part of an effort to leverage pretrained image models for video tasks without requiring additional resource-intensive training of models on video datasets. However, as the reviewer mentioned, employing this temporally-correlated noise prior for training or fine-tuning video diffusion models is definitely an exciting direction for future work, and we will be happy to see adoption of our $\\int$-noise in these contexts.\n\n---\n\n**Comparison with video models**\n\n>Considering video restoration (Zeroscope Text-to-Video) and pose-to-human (dreampose) already have good performance, I wonder the gap between image model+ integrate noise and these video models.\n\nIn our work, we raised awareness on the importance of noise priors for temporal consistency and focused on solving the mathematical problem of distribution-preserving Gaussian noise warping. We then validated our proposed solution on several zero-shot video editing tasks with diffusion models, making it an effective tool for improving temporal coherency in these applications.\n\nWhile our proposed method is compatible with other zero-shot techniques like cross-frame attention (which we implemented in some of our results), it is challenging to directly compare our method alone with video models for several reasons. \n\nFirst, as many previous work on zero-shot video editing with image diffusion models have shown, it is often necessary to combine different techniques together to achieve good temporal coherency. For instance, Pix2Video [1] uses a combination of DDIM inversion, loss guidance and depth conditioning in their pipeline. A fair comparison against existing video models would have to be task-specific (super-resolution, stylization, video generation etc.), as different works adopt different pipelines. Our approach is a general method to enforce noise priors for diffusion models, and could be easily incorporated into different pipelines. \n\nSecondly, a comparison with these video models requires at least similar inputs to be fair. In the example of pose-to-person, Dreampose [2] is trained to take a very informative DensePose [3] parameterization as input, while we based our method on PIDM which only takes an Openpose poses [4]. Note that we did additionally use DensePose in our application results, but only for motion estimation (see Appendix D.3).  \n\nAll in all, we believe combining our $\\int$-noise with other techniques like depth conditioning can further improve the quality of the results.\n\nOn another note, we have added more visual comparisons of our methods with other noise priors such as Control-A-Video and DDIM inversion, which we believe can be helpful. Please refer to the new project webpage for additional results: https://warpyournoise.github.io/ \n\n---\n\n**Integration with DeepFloyd IF**\n\n>I wonder if this is applicable to other pixel-level diffusion models like the open-sourced deepfloyd from stabilityAI, whose capacity is comparable or better than stable diffusion.\n\nOur noise warping method is also applicable to other non-latent diffusion models like DeepFloyd from StabilityAI. We added some examples of using our noise warping with the open-source model for the task of video super-resolution and stylization to support our claim. The results can be visualized on our project webpage: https://warpyournoise.github.io/#deepfloyd We thank the reviewer for pointing this out, as this broadens the generalization abilities of our method beyond the category specific models we showcased in our initial manuscript.\n\n---\n\n**Conclusion / References**\n\nWe hope this clarified some of the interrogations. Please feel free to reach out for any further questions/comments regarding the submission and our answers above.\n\nReferences:\n\n[1] Ceylan, Duygu, Chun-Hao P. Huang, and Niloy J. Mitra. \"Pix2video: Video editing using image diffusion.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023.\n\n[2] Karras, Johanna, et al. \"Dreampose: Fashion image-to-video synthesis via stable diffusion.\" arXiv preprint arXiv:2304.06025 (2023).\n\n[3] G\u00fcler, R\u0131za Alp, Natalia Neverova, and Iasonas Kokkinos. \"Densepose: Dense human pose estimation in the wild.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2018.\n\n[4] Cao, Zhe, et al. \"Realtime multi-person 2d pose estimation using part affinity fields.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2017."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3760/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700589248353,
                "cdate": 1700589248353,
                "tmdate": 1700589248353,
                "mdate": 1700589248353,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kmPXLW6HfZ",
                "forum": "pzElnMrgSD",
                "replyto": "Us6DZFikzq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3760/Reviewer_tVGv"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3760/Reviewer_tVGv"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Author"
                    },
                    "comment": {
                        "value": "Thanks so much for your great work! The shared results are intereseting."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3760/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700600776083,
                "cdate": 1700600776083,
                "tmdate": 1700600776083,
                "mdate": 1700600776083,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "XOCBTdMMgw",
            "forum": "pzElnMrgSD",
            "replyto": "pzElnMrgSD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3760/Reviewer_H5Nc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3760/Reviewer_H5Nc"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a method for warping Gaussian noise while preserving its Gaussian properties. \nTo achieve the goal, the authors consider a mathematical model of the Brownian sheet (i.e. the distributional derivative of the two-dimensional Brownian motion) and develop a noise transport equation for that model. \nIn applications, the following approximation is applied. First, the input noise map is conditionally up-scaled so that it remains Gaussian. Second, the up-scaled map is warped according to the equation derived for the Brownian sheet. \nFinally, the warped noise is down-scaled to the original size.\n \nThe method is tested on a number of vision problems. According to the provided evaluations, typically it produces better temporal coherence than the baseline methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "From my point of view, this paper tackles a pretty important problem for the community, since the application of image translation models to videos is a popular direction which still suffers from flickering and other artifacts. It seems to me that the Brownian sheet model and conditional Gaussian upscaling both are a very good fit for that problem as they take into account the intrinsically hierarchical nature of image resolution. The video results in the supplementary demonstrate that the amount of flickering and, vice versa, texture sticking in videos is reduced."
                },
                "weaknesses": {
                    "value": "1. From the theoretical point of view, the presented approach relies on the warping field being a diffeomorphism, while in practice to the best of my knowledge optical flows estimated with off-the-shelf methods are rarely invertible mappings.\n1. As the authors themselves admit (Appendix E.3), \"the impact of the noise scheme is negatively correlated with the amount of constraints given to the model\", and, in particular, the method has a limited impact on latent diffusion models. However, it is obviously still useful for cascaded models.\n1. As mentioned in the paper (Sec. 6) the method is computationally less efficient than other techniques.\n1. According to Tab. 1, while the proposed method typically increases temporal coherence, at the same time it worsens frame-wise image plausibility metrics such as LPIPS or FID.\n1. None of the video samples in the supplementary except for fluid dynamics provides the results of noise handling with the method proposed by Control-A-Video model. Please note that this model more close to the presented method in spirit since it also takes input video into account while PYoCo does not."
                },
                "questions": {
                    "value": "1. As indicated in Tab. 1, bilinear interpolation often results in quite decent quality (excl. Video SR task). Haven't the authors considered a small modification of bilinear interpolation which is able to preserve the variance of pixels, namely, replacing the commonly used weighting coefficients with their square roots? Probably, this simple modification can even further improve its performance. Also note that Eq. 5 is also essentially \"Gaussian averaging\" of subpixels, i.e. averaging using variance-preserving weighting coefficients of $1/\\sqrt{k}$ instead of $1/k$.\n1. As mentioned above, the presented method is less computationally efficient than the baselines. It would be nice to provide a quantitative evaluation of that (in)efficiency.\n1. From the manuscript, it is a bit unclear how the 9 triangulation points are warped in Fig. 1a. How exactly is the estimated optical flow $\\mathcal{T}^{-1}$ which typically has the same resolution as the image itself, applied to non-integer pixel coordinates? In other words, how is the operation $\\textrm{WARP}_{\\infty}$ from Algorithm 2 implemented?\n1. I suggest adding an explicit proof of why $\\textbf{Z}$ is a Gaussian vector (Appendix B.2). Probably, the shortest proof is the one considering the linear combinations $\\kappa^T \\textbf{Z}$ for all possible $\\kappa$, which are obviously Gaussian random variables.\n1. I recommend adding more comments on how Eq. 24 for the continuous case turned into Eq. 5 for the discrete case. Why is it valid to still rely on the first-order approximation?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3760/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697908057566,
            "cdate": 1697908057566,
            "tmdate": 1699636332261,
            "mdate": 1699636332261,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "obvojUbEVc",
                "forum": "pzElnMrgSD",
                "replyto": "XOCBTdMMgw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3760/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3760/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer H5Nc (1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the constructive feedback. We will address the weaknesses mentioned below. \n\n---\n\n**Treatment of non-diffeomorphic warping fields**\n\n>From the theoretical point of view, the presented approach relies on the warping field being a diffeomorphism, while in practice to the best of my knowledge optical flows estimated with off-the-shelf methods are rarely invertible mappings.\n\nIt is true that the theoretical derivation relies on the warping field being a diffeomorphism, which is generally not the case for optical flow maps. In particular, disocclusions can reveal empty spaces that aren\u2019t mapped to any part of the initial frame. \n\nWe solve this in practice by replacing missing values with newly sampled noise. On very long sequences, we also resample the high-resolution noise periodically, which effectively updates the anchor frame. We will add a more detailed explanation regarding how we handle warping in practical settings in the Appendix. \n\n--- \n\n**Image quality measure in quantitative evaluation**\n\n> According to Tab. 1, while the proposed method typically increases temporal coherence, at the same time it worsens frame-wise image plausibility metrics such as LPIPS or FID.\n\nFor the appearance transfer example, we used FID as a measure of image quality, since there is no ground truth in this case. FID is computed between the LSUN Bedroom dataset and a dataset we create from frames of the video results we have. Unfortunately, FID simultaneously measures sample quality and diversity, while we are only interested in quality. Since we are computing it over a sequence of frames from the same video, the diversity is naturally very low, which explains the overall high values we have in Tab. 1 (typical FID values are <50). Additionally, less temporally coherent methods like \u2018Random Noise\u2019 will likely have more variations between frames, which improves diversity. This explains why random noise performs the best wrt. FID, and our method typically performs a bit worse. All in all, the main take-away from the FID comparison is that our method performs in the same range as other Gaussian noise priors (random, fixed, PYoCo, Control-A-Video) and much better than the other interpolation methods. This, in complement with the warp error metric, shows that we are able to take the \u201cbest of both worlds\u201d.\n\nRegarding the LPIPS metric for video super-resolution and restoration, we do observe a slight decrease in image quality according to the metrics, which we cannot fully explain. Similarly, the main take-away is that we are performing in the same range as the other noise priors methods, while having much better temporal coherency.\n\n--- \n\n**More comparisons with Control-A-Video**\n\n> None of the video samples in the supplementary except for fluid dynamics provides the results of noise handling with the method proposed by Control-A-Video model. Please note that this model more close to the presented method in spirit since it also takes input video into account while PYoCo does not.\n\nThe residual noise from Control-A-Video behaves like fixed noise in regions of uniform color, and like random noise around moving edges between objects. Our experiments have shown that this generally leads to the \u201cworst of both worlds'', as both flickering and texture sticking happens. We have updated the results with more comparisons with the residual noise prior. They can be found on our project webpage: https://warpyournoise.github.io/\n\n---"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3760/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700152494924,
                "cdate": 1700152494924,
                "tmdate": 1700152494924,
                "mdate": 1700152494924,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KPl9e35J7t",
                "forum": "pzElnMrgSD",
                "replyto": "obvojUbEVc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3760/Reviewer_H5Nc"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3760/Reviewer_H5Nc"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks a lot for the prompt response!\n\n> Unfortunately, FID simultaneously measures sample quality and diversity, while we are only interested in quality.\n\nThis is true, so I would like to bring to your attention the well-adopted metrics which measure the \"quality\" of the images. Probably, the best known is the Improved Precision [1]. Density [2] is similar although less adopted by the community.\n\n[1] Kynk\u00e4\u00e4nniemi et al. Improved Precision and Recall Metric for Assessing Generative Models. In NeurIPS, 2019.\n\n[2] Naeem et al. Reliable Fidelity and Diversity Metrics for Generative Models. In ICML, 2020."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3760/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700213916688,
                "cdate": 1700213916688,
                "tmdate": 1700213916688,
                "mdate": 1700213916688,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "2aJQPL7qKT",
            "forum": "pzElnMrgSD",
            "replyto": "pzElnMrgSD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3760/Reviewer_3JNo"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3760/Reviewer_3JNo"
            ],
            "content": {
                "summary": {
                    "value": "This work attempts to mitigate the lack of temporal correlation seen in diffusion based video generation models. The proposed method attempts to generate new noise samples that preserve the correlations induced by motion vectors. To this end, first, this work reinterprets individual noise samples used in diffusion models as a continuously integrated noise field called integral noise. With the help of the derived noise transport equation, a transport algorithm is developed to generate noise with temporal correlation between samples while preserving the desired properties of noise samples. Results are demonstrated to indicate the potential of the proposed method for tasks such as video restoration and editing, surrogate rendering, and conditional video generation."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The problem tackled in this work has significant practical impact.\n2. The paper is written very well.\n3. Motivation is clear, ideas are well formulated with theory, and the results are impressive."
                },
                "weaknesses": {
                    "value": "1. The proposed method is computationally inefficient as compared to prior arts. Quantitative comparisons on this aspect would have been more helpful for future research works."
                },
                "questions": {
                    "value": "1. Figure 5 illustration is hard to follow, it\u2019s not clear how to judge the performance based on the images shown. It\u2019s mentioned that, the Random Noise creates incoherent details while Fixed Noise suffers from sticking artefacts. Our R -noise moves the fluid in a smoother way. However, I find this explanation hard to map to the results shown. Maybe the authors can highlight based on the image contents the reasons for each of these conclusions."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3760/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698561407019,
            "cdate": 1698561407019,
            "tmdate": 1699636332183,
            "mdate": 1699636332183,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "nVUOv3C1dX",
                "forum": "pzElnMrgSD",
                "replyto": "2aJQPL7qKT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3760/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3760/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer 3JNo"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the constructive feedback. We will address the remaining questions below.\n\n---\n\n**Computational Efficiency**\n\n> The proposed method is computationally inefficient as compared to prior arts. Quantitative comparisons on this aspect would have been more helpful for future research works.\n\nWe estimate both wall time and CPU time for all methods on a video sequence from the surrogate rendering application (resolution at 256x256). By evaluating the run time for different sub-sequence lengths, we can linearly regress the average per frame computation time of each method. This is evaluated on a GeForce RTX 3090 GPU and an Intel i9-12900K CPU. The table below summarizes the run time of our method against previous approaches. We will add this table to the updated manuscript appendix.\n\n| Method | Wall Time (in ms/frame) | CPU Time (in ms/frame) |\n| :-- | :--: | :--: |\n| Random | 0.01| 0.01|\n| Fixed | 0.01  | 0.01|\n| PYoCo (mixed) [1] | 0.01| 0.01 |\n| PYoCo (progressive) [1]  | 0.01| 0.01|\n| Control-A-Video [2]| 6.08   | 95.46|\n| Bilinear  | 5.26| 76.76|\n| Bicubic | 6.00| 87.73|\n| Nearest| 5.17| 75.73 |\n| Root-bilinear| 7.66 |103.78|\n| DDIM inversion (20 steps) [3] | 853.42 | 2226.6|\n| DDIM inversion (50 steps) [3] | 2125.5  | 3608.3 |\n| **$\\int$\ufeff-noise (ours, k=3, s=4)**| **981.76**| **2953.6**|\n\nOur method is indeed less efficient. This is a trade-off that we decided to make, as our intent was to solve Gaussian noise warping as accurately as possible, to understand how far that could bring us. It is worth noting that we have explored several simpler, less computationally heavy ways to warp the noise by making different simplifying assumptions (such as the root-bilinear interpolation methods shown in the table, a modification also suggested by Reviewer 3 (H5Nc) in Question 1). However, none of those led to satisfying solutions.\n\nAn analysis of the table shows that simple noise sampling methods like PYoCo, fixed or random noise can be executed efficiently on GPU. Methods that rely on information from the input sequence in a simple way such as Control-A-Video or interpolation methods are almost 3 orders of magnitude slower than the simple noise sampling methods. Finally, our warping method requires ~1s per frame to warp the noise. While being considerably less efficient than aforementioned methods, the proposed approach is comparable to DDIM inversion. With a standard setting of 50 DDIM steps, DDIM inversion takes longer than our method to produce a noise map, while being similar to ours in run time if we reduce the number of steps to 20. Please refer to our answer to Reviewer 1 (13WQ) for more qualitative comparisons with DDIM inversion.\n\n| k \\ s | 1| 2| 3| 4|\n| -- | -- | -- | -- | -- |\n| **0**| 10.0 | 10.3| 10.7| 12.1 |\n| **1**| 10.1 | 10.5| 10.9| 12.2 |\n| **2**| 10.3 | 10.5| 10.9| 12.3 |\n| **3**| 12.2 | 12.5| 13.0| 14.2 |\n| **4**| 17.9 | 17.9 | 18.4  | 19.5 |\n\nAdditionally, our method has two parameters $k$ and $s$ which control the resolution of the upsampled noise and the discretization level of the pixel polygons respectively. These parameters provide users with the possibility to trade off accuracy in the temporal correlations against efficiency. We refer to the table above to demonstrate how the parameters impact the performance. The measurements represent the wall clock time in seconds that our method requires to compute noise for a video sequence of 24 frames at resolution 256x256. Additionally, we refer to the Section 4 of the paper on how these parameters affect the quality of the results. \n\nLastly, our implementation was not optimized for efficiency. We believe it can still be drastically improved, which would make the computational cost of using our noise scheduler negligible. \n\n---\n\n**Better visual comparison for fluid super-resolution**\n\n>Figure 5 illustration is hard to follow, it\u2019s not clear how to judge the performance based on the images shown. \n\nUnfortunately it is hard to show video results properly on paper. We will update the visualization in the submission to reflect the feedback. Additionally, the video results of this figure can be found on our project webpage at https://warpyournoise.github.io/#fluid-sr, where the visual comparison is much clearer.\n\n---\n\n**Conclusion / References**\n\nWe hope this clarified some of the interrogations. We will update the paper to integrate the feedback. Please feel free to reach out for any further questions/comments regarding the submission and our answers above.\n\nReferences:\n\n[1] Ge, Songwei, et al. \"Preserve your own correlation: A noise prior for video diffusion models.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023.\n\n[2] Chen, Weifeng, et al. \"Control-A-Video: Controllable Text-to-Video Generation with Diffusion Models.\" arXiv preprint arXiv:2305.13840 (2023).\n\n[3] Song, Jiaming, Chenlin Meng, and Stefano Ermon. \"Denoising diffusion implicit models.\" arXiv preprint arXiv:2010.02502 (2020)."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3760/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700149152242,
                "cdate": 1700149152242,
                "tmdate": 1700149152242,
                "mdate": 1700149152242,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "X1N3Gkl63S",
                "forum": "pzElnMrgSD",
                "replyto": "nVUOv3C1dX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3760/Reviewer_3JNo"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3760/Reviewer_3JNo"
                ],
                "content": {
                    "title": {
                        "value": "Comment on Author Response"
                    },
                    "comment": {
                        "value": "Many thanks for the detailed response and clarifications. Authors have addressed all my concerns satisfactorily."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3760/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700297241272,
                "cdate": 1700297241272,
                "tmdate": 1700297241272,
                "mdate": 1700297241272,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "JgyloWVjJY",
            "forum": "pzElnMrgSD",
            "replyto": "pzElnMrgSD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3760/Reviewer_13WQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3760/Reviewer_13WQ"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces the novel problem of Gaussian noise warping. Therein, the authors raised the interesting question of \"how to properly warp the Gaussian noise such that the warped noise map still has the Gaussian distribution preserved?\". To this end, the authors discussed why applying conventional warping operation on the noise map is not suitable, and they proposed a mathematically grounded solution to the problem with their \u222b-noise formulation. The authors motivated the practical value of this problem with the applications of lifting image diffusion models to perform temporally consistent video editing. Experiment results with different video editing tasks demonstrate the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The research question \u201chow to properly warp a Gaussian noise map\u201d introduced by this paper is interesting scientifically.\n\nThe problem is well motivated with clear explanations on why the problem is not trivial, i.e. why the conventional warping operations are not suitable. The proposed solution is technically sound and well-grounded mathematically.\n\nThe practical aspects of the problem are also well-motivated, with interesting video editing applications."
                },
                "weaknesses": {
                    "value": "The major weakness from the practical point of view is the implicit assumption that temporally correlated noise maps can induce temporally consistent video editing results, which is often not true. This limitation, however, has been acknowledged and explained by the authors in the paper."
                },
                "questions": {
                    "value": "Other than the comments above, there are a couple of questions I\u2019m curious about:\n+ I\u2019m wondering how sensitive the method is with respect to the underlying estimated flow map? In other words, how do the errors in the estimated optical flows affect the results?\n+ One principled way to obtain a sequence of correlated noise maps is to perform inverse DDIM on the reference video frames (assuming such video is available, which is true in most of the use cases demonstrated in this paper). The authors did mention that technique in the paper, but did not elaborate further and did not show visual results or comparison. I\u2019m wondering how will the noise maps obtained that way compare to the ones obtained from the \u222b-noise in terms of the final video quality?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3760/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698814275453,
            "cdate": 1698814275453,
            "tmdate": 1699636332101,
            "mdate": 1699636332101,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "sKbeRa6bwW",
                "forum": "pzElnMrgSD",
                "replyto": "JgyloWVjJY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3760/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3760/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer 13WQ"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the constructive feedback. We will address the remaining questions below. \n\n---\n\n**Implicit assumption on temporally-coherent noises**\n\n> The major weakness from the practical point of view is the implicit assumption that temporally correlated noise maps can induce temporally consistent video editing results, which is often not true. \n\nWhile there is no theoretical guarantee that diffusion models produce temporally-consistent video editing results with a given temporally-correlated noise map, we observe that this is often the case in practice. Take the translating pizza example on Appendix E: if we add a noise that undergoes the same translation as the input video, the generated images are more temporally coherent. For better visualization and understanding of these results, we refer to the videos at https://warpyournoise.github.io/#warp-ldm. \n\nOther examples in the paper also highlight that transformations that are applied to the noise will produce similar image transformations, even though that\u2019s not explicitly enforced during training of diffusion models. With this in mind, we believe that our proposed temporally coherent noise would indeed improve the training of video diffusion models, similar to PYoCo [1]. \n\n--- \n\n**Sensitivity with respect to flow map**\n\n> I\u2019m wondering how sensitive the method is with respect to the underlying estimated flow map? In other words, how do the errors in the estimated optical flows affect the results?\n\nOverall, as one would expect, accurate optical flow is important for achieving good results. From our experiments, we observed that poorly estimated optical flow will still be able to warp the noise, but it tends to create visual artifacts in the form of temporal misalignment, i.e. fine details in the video will not seem to be perfectly synchronized with the motion of larger structures. \n\nThat being said, it is worth noting that the method is still relatively robust to small errors. For the video super-resolution and JPEG restoration task we show in the submission (Figure 4), the optical flow is computed on the degraded, low-resolution video, and our method is still able to produce decent, coherent results.\n\nLastly, as our noise warping method is agnostic to the way the deformation field is computed, it can also work with more robust methods such as Neural Layered Atlases [2], when flow-based methods struggle to provide accurate motion estimation. These methods map each pixel in each frame to a canonical space common to all frames. In this case, the canonical space is considered the \u201cfirst frame\u201d and the mapping replaces the accumulated flow map in our algorithm. Please refer to our answer to Reviewer 3 for a more complete discussion about the properties of the warping velocity field. \n\n---\n\n**Comparison with DDIM inversion**\n\n> I\u2019m wondering how will the noise maps obtained [using DDIM inversion] compare to the ones obtained from the $\\int$-noise in terms of the final video quality?\n\nThe main issue that prevents a fair and straightforward comparison with DDIM inversion is that the latter only produces one single noise map per image, whereas the applications we show generally use DDPM and require as many noise maps as there are steps in the denoising process.\n\nWe included a new visual comparison for the bedroom SDEdit surrogate rendering results with DDIM noise inversion that can be visualized here: https://warpyournoise.github.io/#ddim. There are two ways we can think of to use DDIM inversion with SDEdit. \n* The first consists in partially inverting the image to an intermediate noisy version using DDIM inversion and denoising it with forward DDIM. This expectedly leads to a reconstruction of the input synthetic scene with no additional realistic details.\n* The second way consists in fully inverting each frame to obtain a set of noise maps, which we treat similarly to the other noise priors we compare to in the original submission. Since the synthetic renders we use as input video are far from the distribution of the LSUN dataset, the inverted noises are not Gaussian. Our results demonstrate this creates noticeable artifacts. \n\nLastly, DDIM inversion deterministically maps each frame to a noise, which removes any diversity in the results. In comparison, our method can apply the same warping to different noise maps to generate different equally temporally-consistent appearances.\n\n---\n\n**Conclusion / References**\n\nWe hope this clarified some of the interrogations. We will update the paper to integrate the feedback. Please feel free to reach out for any further questions/comments regarding the submission and our answers above.\n\nReferences:\n\n[1] Ge, Songwei, et al. \"Preserve your own correlation: A noise prior for video diffusion models.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023.\n\n[2] Kasten, Yoni, et al. \"Layered neural atlases for consistent video editing.\" ACM Transactions on Graphics (TOG) 40.6 (2021): 1-12."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3760/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700062459650,
                "cdate": 1700062459650,
                "tmdate": 1700062459650,
                "mdate": 1700062459650,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QyK1nA66w2",
                "forum": "pzElnMrgSD",
                "replyto": "sKbeRa6bwW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3760/Reviewer_13WQ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3760/Reviewer_13WQ"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for addressing my comments. I have no further questions at this point."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3760/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700326448135,
                "cdate": 1700326448135,
                "tmdate": 1700326448135,
                "mdate": 1700326448135,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]