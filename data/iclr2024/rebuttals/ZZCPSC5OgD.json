[
    {
        "title": "LipVoicer: Generating Speech from Silent Videos Guided by Lip Reading"
    },
    {
        "review": {
            "id": "xQgGylJjiv",
            "forum": "ZZCPSC5OgD",
            "replyto": "ZZCPSC5OgD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1192/Reviewer_nskp"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1192/Reviewer_nskp"
            ],
            "content": {
                "summary": {
                    "value": "This paper describes a new lip-to-speech method that leverages diffusion models via classifier and classifier-free guidance to reproduce accurate speech from silent videos. The base diffusion model is built upon the DiffWave architecture but generates mel-spectrograms instead. It receives a video of the speaker's mouth (encoded into feats via a lipreading backbone) and a still frame representing the speaker's identity (encoded via a simple ResNet) as the condition for generation. This condition is randomly removed during training to perform classifier-free guidance during inference, as proposed in many other diffusion models. After this is trained, the model is further guided via classifier guidance during inference so that the text extracted by a lip reading model from the video matches the text extracted via a speech recognition model from the generated audio. This model achieves SOTA performance on LRS2 and LRS3, and the design decisions are justified by a set of thorough ablations. Demos are also provided, which help contextualize these results empirically."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "In general, I believe this paper is strong. It clearly sets a new state-of-the-art for lip-to-speech, which is a highly competitive field. \n\nI think the paper is well-written and the motivation for the task and each specific decision in the methodology is concise and meaningful. The methodology is clear and the discussion around the results is welcome. The model figure is adequate in my opinion, and the demos are also very welcome.\n\nThe method here is clearly novel - I don't think I've seen a lip-to-speech paper before with a similar architecture and that leverages classifier guidance from text in such an effective way. The choice for each model component makes sense and the training hyperparameters are described in detail to aid reproducibility. \n\nThe results are clearly strong and are compared with other works via subjective and objective metrics, which are very convincing. The ablations in tables 5, 6, and 7 are insightful and provide some further information about the importance of the weight of the classifier-free (w1) and classifier (w2) guidance, as well as the lip reading model that is used for the classifier guidance. The avoidance of intrusive measures such as PESQ or STOI is well-justified.\n\nThe limitations and social impacts are well addressed, and the conclusions are succinct and valid."
                },
                "weaknesses": {
                    "value": "First and foremost, it is unfortunate that the authors do not compare directly with ReVISE in their tables, although this is fully justified by the lack of code, difficulty in reproducing their results from scratch, and the lack of samples for comparison. Therefore, I don't think it's fair for me or other reviewers to let this affect our judgment of the paper, as this is not the authors' fault. The presented model seems to compare favorably against ReVISE in the demos, which is encouraging.\n\nThe use of the DiffWave vocoder is reasonable, but it seems to be outperformed by HiFi-GAN and especially the recent BigVGAN. Would be interesting to see a comparison with these, or at least to justify why DiffWave was chosen, as HiFI-GAN is the typical choice in the majority of papers.\n\nIt would also be interesting to scale the model to larger datasets such as LRS3+VoxCeleb2 as was done in SVTS. This would help demonstrate the model's scalability to larger datasets, which is an important aspect of models in this field since audio-visual data is so plentifully available.\n\nI could not find any typos or substantial errors in the writing."
                },
                "questions": {
                    "value": "The authors mention \" To encourage future research and reproducibility, our source code will be made publicly available\". Will this include training code, inference code, and pre-trained models? These would all be hugely helpful to the community in reproducing the authors' state-of-the-art results."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1192/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1192/Reviewer_nskp",
                        "ICLR.cc/2024/Conference/Submission1192/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1192/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698306681732,
            "cdate": 1698306681732,
            "tmdate": 1700569237420,
            "mdate": 1700569237420,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "CM74O3kqf5",
                "forum": "ZZCPSC5OgD",
                "replyto": "xQgGylJjiv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1192/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1192/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "1. *In general, I believe this paper is strong. It clearly sets a new state-of-the-art for lip-to-speech, which is a highly competitive field.*\n\n**Response:**\n\nWe thank the reviewer for pointing out that our method currently achieves state-of-the-art results.\n\n2. *First and foremost, it is unfortunate that the authors do not compare directly with ReVISE in their tables, although this is fully justified by the lack of code, difficulty in reproducing their results from scratch, and the lack of samples for comparison. Therefore, I don't think it's fair for me or other reviewers to let this affect our judgment of the paper, as this is not the authors' fault. The presented model seems to compare favorably against ReVISE in the demos, which is encouraging.*\n\n**Response:**\n\nWe thank the reviewer for making this observation. We did our best to make the comparisons to ReVISE that were possible given the circumstances.\n\n3. *The use of the DiffWave vocoder is reasonable, but it seems to be outperformed by HiFi-GAN and especially the recent BigVGAN. Would be interesting to see a comparison with these, or at least to justify why DiffWave was chosen, as HiFI-GAN is the typical choice in the majority of papers.*\n\n**Response:**\n\nWe chose to use DiffWave as our vocoder since we had already used its architecture for MelGen, so applying a DiffWave vocoder was immediate.\nSince Reviewer gU1H has raised the argument that LipVoicer outperforms the baseline thanks to the vocoder it uses, we are now training also a HiFi-GAN vocoder and once training is completed we will use it to synthesise test audio samples and upload the calculated metrics to this page. HiFi-GAN was used in ReVISE, for instance.\n\n4. *It would also be interesting to scale the model to larger datasets such as LRS3+VoxCeleb2 as was done in SVTS. This would help demonstrate the model's scalability to larger datasets, which is an important aspect of models in this field since audio-visual data is so plentifully available.*\n\n**Response:**\n\nWe have been downloading the VoxCeleb2 dataset over the last few days, and have been experiencing a slow download speed of tens to hundreds of KB/s. From our diagnosis, the source of the slow speed is on the server side. Since VoxCeleb2 is a big dataset and the downloaded videos have to undergo pre-processing stages (audio extraction etc), we cannot be assured that we will manage to carry out the experiments with VoxCeleb2 by the end of the discussion period. Instead, in the meantime, we are conducting this experiment with the concatenation of LRS3 and LRS2, including their pretrain splits. We will upload the results when they are ready, and will add the experiment with VoxCeleb2 to the camera ready version of this paper.\n\n5. *The authors mention \" To encourage future research and reproducibility, our source code will be made publicly available\". Will this include training code, inference code, and pre-trained models? These would all be hugely helpful to the community in reproducing the authors' state-of-the-art results.*\n\n**Response:**\n\nYes, we will upload the training and inference code as well as the pre-trained models of MelGen and the ASR."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1192/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700099331549,
                "cdate": 1700099331549,
                "tmdate": 1700099331549,
                "mdate": 1700099331549,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KniIEY0Yz8",
                "forum": "ZZCPSC5OgD",
                "replyto": "CM74O3kqf5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1192/Reviewer_nskp"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1192/Reviewer_nskp"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the response. Looking forward to the HiFi-GAN experiments and the code release. \n\nRegarding VoxCeleb2, other colleagues and I have experienced this. This almost certainly means that Google has blocked your IP and throttled your download speed. I would suggest downloading using a different IP via a different router or a VPN, this usually works. KB/s speeds are not normal from YouTube. In any case, I understand that the timeline is tight and will not lower the score if you can't provide VoxCeleb2 results."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1192/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700145557568,
                "cdate": 1700145557568,
                "tmdate": 1700145557568,
                "mdate": 1700145557568,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xvIaiUH3dP",
                "forum": "ZZCPSC5OgD",
                "replyto": "ehFFlVyZoU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1192/Reviewer_nskp"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1192/Reviewer_nskp"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the HiFi-GAN ablation. Looking forward to the LRS2+LRS3 results."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1192/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700485086174,
                "cdate": 1700485086174,
                "tmdate": 1700485086174,
                "mdate": 1700485086174,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GbNADJl4uY",
                "forum": "ZZCPSC5OgD",
                "replyto": "xQgGylJjiv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1192/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1192/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Here are the results on the test set of LRS3, when the training set is LRS2+LRS3\n\n| Training data | WER$\\downarrow$   | STOI-Net$\\uparrow$ | DNS-MOS$\\uparrow$ | LSE-C$\\uparrow$ | LSE-D$\\downarrow$ |\n|---------------|-------|----------|---------|-------|-------|\n| LRS3          | 21.4% | 0.92     | 3.11    | 6.239 | 8.266 |\n| LRS2+LRS3     | 21.2% | 0.93     | 3.15    | 6.308 | 8.214 |\n\nThis tables shows that the model's performance scales with the amount of training data."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1192/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700567848846,
                "cdate": 1700567848846,
                "tmdate": 1700568674945,
                "mdate": 1700568674945,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rcVzsmGgxF",
                "forum": "ZZCPSC5OgD",
                "replyto": "GbNADJl4uY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1192/Reviewer_nskp"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1192/Reviewer_nskp"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for these new results. Would be great to include these in the paper of course. While the performance improvement is not terribly impressive I understand this experiment was run in a rush due to the tight rebuttal time so it's understandable. Maybe with some hyperparameter tuning it can get better. Also, would still be great to see VoxCeleb2 results in the camera-ready version. In any case, given that you have addressed all of my concerns, your response to other reviewers seems adequate, and the paper was already strong in my opinion, I'm happy to raise my score."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1192/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700569222051,
                "cdate": 1700569222051,
                "tmdate": 1700569222051,
                "mdate": 1700569222051,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "BsPSf71ZUb",
            "forum": "ZZCPSC5OgD",
            "replyto": "ZZCPSC5OgD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1192/Reviewer_gU1H"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1192/Reviewer_gU1H"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a method to generate a natural-sounding speech from silent video called LipVoicer. The method is different from previous work in a two key ways: (1) the proposed method uses a lip reading model during inference to generate guidance for the generation model, (2) the generative model is based on a diffusion model. The model is trained on LRS2 and LRS3 datasets, which contains challenging examples from near in-the-wild conditions. The proposed system significantly outperforms the baselines.\n\nThe two key ideas actually appeared in accepted recent/concurrent papers, and authors acknowledge these works. Lip reading-based text guidance is proposed in (Kim et al., ICASSP 2023), although it is not exactly the same in that this paper uses the text guidance during inference, whereas Kim et al. uses the guidance during training. The use of diffusion-based model for the lip-to-speech task has been proposed in (Choi et al, ICCV 2023a). Authors are not required to compare their own work to that paper under ICLR rules."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The key ideas are reasonable, and well-engineered combination of proven methods.\n- The use of pre-trained state-of-the-art lip reading model significantly lowers the WER significantly compared to existing methods. \n- The diffusion model generates natural-sounding output, according to the qualitative results reported."
                },
                "weaknesses": {
                    "value": "- It is not clear if the performance improvement comes from the key improvements, or the replacement of the vocoder, which can be seen as a post-processing step rather than a key part of the algorithm. It is well known that DiffWave produces much more natural-sounding output compared to the Griffin-Lim algorithm used by the previous works.\n- The authors request subjective assessors to rate Intelligibility, Naturalness, Quality and Synchronisation, but it is not clear what the difference between Naturalness and Quality are. There is a screenshot of the evaluation page in the appendix, but it does not make it clear what 'quality' means. \n- The baseline models appear to be using pre-trained model weights. However, the models are not trained on the same data, so the results cannot be compared directly.\n- The method appears to apply Guided-TTS techniques to the problem of lip-to-speech. Although this is well engineered, in my opinion this work is better suited to speech or CV conference compared to ICLR."
                },
                "questions": {
                    "value": "- It is not clear why the addition of text guidance helps sync performance.\n- If lip reading networks are used, what is the advantage of the proposed system over a cascaded lip reading + TTS system apart from obtaining duration prediction from sync."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1192/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698589612081,
            "cdate": 1698589612081,
            "tmdate": 1699636045648,
            "mdate": 1699636045648,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "a1bekscp4N",
                "forum": "ZZCPSC5OgD",
                "replyto": "BsPSf71ZUb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1192/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1192/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "1. *It is not clear if the performance improvement comes from the key improvements, or the replacement of the vocoder, which can be seen as a post-processing step rather than a key part of the algorithm. It is well known that DiffWave produces much more natural-sounding output compared to the Griffin-Lim algorithm used by the previous works.*\n\n**Response:**\n\nAs Figures 2 & 3 in the Appendix show, LipVoicer recovers mel-spectrograms that look like the mel-spectrogram of natural speech. The competitors, however, fail to do so and are missing crucial pitch information and hence sound unintelligible and metallic. Even an excellent vocoder cannot compensate for degraded input mel-spectrograms. Moreover, we trained the vocoder on mel-spectrograms which were extracted from natural speech signals, and not on the mel-spectrograms that we generated with LipVoicer. This gives a strong qualitative indication that the source of the improvement comes mainly from the quality of the mel-spectrogram reconstructed by LipVoicer.\nWe note that we compare favourably to SVTS that uses a modern neural vocoder, specifically Parallel WaveGan, and not Griffin-Lim. In general, Griffin-Lim is rarely used nowadays in recent papers in the text- and lip- to speech techniques, since more advanced vocoders currently exist.\n\n2. *The authors request subjective assessors to rate Intelligibility, Naturalness, Quality and Synchronisation, but it is not clear what the difference between Naturalness and Quality are. There is a screenshot of the evaluation page in the appendix, but it does not make it clear what 'quality' means.*\n\n**Response:**\n\nQuality is the overall quality of the audio. It is equivalent to the single value of MOS that most papers report. The other metrics try to break down the quality into different components to understand the specific pros and cons of each model. Naturalness refers to how natural the audio is, for example in terms of the voice and prosody. A speech signal can achieve high Naturalness, but at the same time low Quality score since, for instance, some words are not clearly pronounced or they are not well synchronised with the video. \n\n3. *The baseline models appear to be using pre-trained model weights. However, the models are not trained on the same data, so the results cannot be compared directly.*\n\n**Response:**\n\nWe trained the baselines on LRS3 and LRS2, and we thank the reviewer for drawing our attention that we forgot to mention it in the text. Our comparisons are not based on the pre-trained models available on the project pages of the baselines. We will clarify this in the text.\n\n4. *The method appears to apply Guided-TTS techniques to the problem of lip-to-speech. Although this is well engineered, in my opinion this work is better suited to speech or CV conference compared to ICLR.*\n\n**Response:**\n\nWe would like to draw the reviewer\u2019s attention that papers of similar or related flavour have been published at ML conferences. For example, [1-3] were published at ICLR, [4] was published at NeurIPS and [5] was published at ICML.\n\n[1] Felix Kreuk, Gabriel Synnaeve, Adam Polyak, Uriel Singer, Alexandre D\u00e9fossez, Jade Copet, Devi Parikh, Yaniv Taigman, and Yossi Adi. AudioGen: textually guided audio generation. The International Conference on Learning Representations (ICLR). 2023.\n\n[2] Haoyue Cheng, Zhaoyang Liu, Wayne Wu and Limin Wang. Filter-recovery network for multi-speaker audio-visual speech separation. The International Conference on Learning Representations (ICLR). 2023.\n\n[3] Kai Li, Runxuan Yang and Xiaolin Hu. An efficient encoder-decoder architecture with top-down attention for speech separation. The International Conference on Learning Representations (ICLR). 2023.\n\n[4] Minsu Kim, Joanna Hong, and Yong Man Ro. Lip to speech synthesis with visual context attentional gan. Advances in Neural Information Processing Systems, 34:2758\u20132770, 2021.\n\n[5] Eliya Nachmani, Yossi Adi, and Lior Wolf. Voice separation with an unknown number of multiple speakers. Proceedings of the 37th International Conference on Machine Learning, 2020."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1192/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700097600181,
                "cdate": 1700097600181,
                "tmdate": 1700100478450,
                "mdate": 1700100478450,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Dz1eHtRJKY",
                "forum": "ZZCPSC5OgD",
                "replyto": "BsPSf71ZUb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1192/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1192/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We compare here the metrics for LRS3 as computed on speech signals generated using HiFi-GAN and DiffWave as the vocoders. Both vocoders used the exact same mel-spectrograms generated by LipVoicer. HiFi-GAN is widely used in the literature, including in (Choi et al, ICCV 2023a). \n\n| vocoder  | WER   | STOI-Net | DNS-MOS | LSE-C | LSE-D |\n|----------|-------|----------|---------|-------|-------|\n| DiffWave | 21.4% | 0.92     | 3.11    | 6.239 | 8.266 |\n| HiFi-GAN | 21.9% | 0.93     | 3.19    | 6.308 | 8.166 |\n\nWe see in this table that both options that use two different vocoders yield state-of-the-are results.\nThis clearly indicates that the main and critical source of improvement is the mel-spectrograms generated by our method."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1192/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700434508520,
                "cdate": 1700434508520,
                "tmdate": 1700434508520,
                "mdate": 1700434508520,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "oaeHGe3PqA",
            "forum": "ZZCPSC5OgD",
            "replyto": "ZZCPSC5OgD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1192/Reviewer_x1vF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1192/Reviewer_x1vF"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes LipVoicer, which incorporates the text modality by predicting the spoken text using a pre-trained lip-reading network and conditioning a diffusion model on both the video and the extracted text. To utilize the text modality into the diffusion model, the authors apply classifier-guidance mechanism, where a pre-trained automatic speech recognition (ASR ) serves as the classifier. The results demonstrate the effectiveness of LipVoicer in producing natural, synchronized, and intelligible speech."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. LipVoicer greatly improves the intelligibility of the generated speech and outperforms existing lip-to-speech baselines on challenging datasets, demonstrating its superior performance. \n\n2. The paper provides detailed implementation details, making it easier for others to reproduce and further improve upon the LipVoicer method. \n\n3. By introducing a pre-trained ASR model, this paper realizes a good application of classifier-guidance diffusion model in lip2speech task."
                },
                "weaknesses": {
                    "value": "1. After listening to Demo page, it is found that the gap between different models is mainly in sound quality. The baselines are too weak in sound quality. However, the problem of sound quality can be solved by many existing generative models based on VAE/GAN/FLOW model. If the sound quality problem of baselines is solved, the advantage of the model proposed in this paper may not be so great.\n\n2. In previous studies, a very important motivation for lip2speech tasks was to dispense with text modality (otherwise, this task can be transformed into lipreading+TTS), because 80% of the world's languages have no written text. However, this paper still depends on the text modality, so it is difficult to give a high score to this article."
                },
                "questions": {
                    "value": "None"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1192/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698760154384,
            "cdate": 1698760154384,
            "tmdate": 1699636045576,
            "mdate": 1699636045576,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4PupVCzkSD",
                "forum": "ZZCPSC5OgD",
                "replyto": "oaeHGe3PqA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1192/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1192/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "1. *After listening to Demo page, it is found that the gap between different models is mainly in sound quality. The baselines are too weak in sound quality. However, the problem of sound quality can be solved by many existing generative models based on VAE/GAN/FLOW model. If the sound quality problem of baselines is solved, the advantage of the model proposed in this paper may not be so great.*\n\n**Response:**\n\nOne of the main contributions of our paper **exactly is** the improvement in speech quality. The lip-to-speech task is more than just generating high quality audio. We need to produce a speech signal that follows the words spoken in a silent video with unrestricted vocabulary, while achieving naturalness, appealing timing, intonation and vocal features that reflect the appearance of the speaker. This very information must be extracted from the silent video. As Figures 2 & 3 in the Appendix show, LipVoicer recovers mel-spectrograms that look like the mel-spectrogram of natural speech and follow the words likely to be spoken in the video. The competitors, however, fail to do so and are missing crucial pitch information and hence sound unintelligible and metallic.\n\n2. *In previous studies, a very important motivation for lip2speech tasks was to dispense with text modality (otherwise, this task can be transformed into lipreading+TTS), because 80% of the world's languages have no written text. However, this paper still depends on the text modality, so it is difficult to give a high score to this article.*\n\n**Response:**\n\nOur problem is fundamentally different from a lip-reading+TTS setup. The generated speech must be synchronised with the video, match the speaker appearance and convey prosody. Whereas lip-reading+TTS cannot achieve these objectives, we show how to successfully incorporate the text in a way which does manage to satisfy all of these tasks. Our claim is that the text is important, and should not be dispensed so fast. Importantly, we utilise it without resorting to transcribed videos by humans. The way that we acquire (and exploit) the textual modality is exactly one of the virtues of  LipVoicer. We suggest using a lip-reader, and therefore the text comes for free. We show in our experiments and the provided speech samples that text plays a major part in the success of our method.\n\nRegarding the fact that 80% of the world's languages have no written text, while this might be true, the vast majority of speakers do speak languages with written text. While our solution does not solve all languages, it is relevant to the vast majority of the population and as such we believe this is significant enough to be of value to the community."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1192/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700097152602,
                "cdate": 1700097152602,
                "tmdate": 1700100519075,
                "mdate": 1700100519075,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "NOdpAbVWa1",
            "forum": "ZZCPSC5OgD",
            "replyto": "ZZCPSC5OgD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1192/Reviewer_eBPV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1192/Reviewer_eBPV"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes to perform the lip-to-speech task by incorporating the predicted text to guide the diffusion model based learning process. Experiments on the large scale LRS2 and LRS3 show its superiority over others. The results are indeed appealing."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The general structure is clear. The method is simple in general. It\u2019s easy to follow. The performance is good, with a large margin over other methods. It\u2019s also a nice try to include the predicted text into the learning process."
                },
                "weaknesses": {
                    "value": "(1) I am a little confused with fig1.a. The output of the lipreading module is the predicted text. The output of the ASR modules is also the predicted text. There should be no connections from the output of the predicted text to the ASR module? The ASR module should take the output of MelGen as input? without the text predicted from LR module?\n(2) Lip2speech (Kim et al.(2023)) takes the ground-truth text as input to constrain the learning process and has shown the success of the role of text modality in this task. In this paper, the work uses the predicted text instead of the ground truth as Lip2Speech. But the manner is similar to Kim et al.(2022). So, besides using the predicted text with an existing method, is there some new contributions in the view of methodology?"
                },
                "questions": {
                    "value": "(1) I am a little confused about the fig.1(a) as described above.\n(2) The modules and manners in the framework seems to be not new in the view of methodology, with the lipreading module, MelGen, text alignment manners already proposed by other works. Could the authors give a clarification of the contributions? Maybe I miss something?\n(3) The performance using the predicted text is already very appealing, but the involved lip reading model are almost the best two ones at present, with WER=19% and 26%. if the lip reading performance has been a much low value, e.g. WER=50%, what would be the performance here like?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Privacy, security and safety"
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1192/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699096339309,
            "cdate": 1699096339309,
            "tmdate": 1699636045492,
            "mdate": 1699636045492,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0ISXRHinpN",
                "forum": "ZZCPSC5OgD",
                "replyto": "NOdpAbVWa1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1192/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1192/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "1. *I am a little confused with fig1.a.*\n\n**Response:**\n\nThe inference procedure of LipVoicer involves the estimation of two noise terms. The first one, $\\epsilon_{mg}$ is computed via classifier-free guidance using the MelGen module, whose inputs are the noisy mel-spectrogram $\\mathbf{x}\\_{t+1}$, the lip motion video and a face image. The second noise term is computed via classifier guidance. The ASR takes $\\mathbf{x}\\_{t+1}$ as an input, and computes the loss at the output with respect to the text predicted by the lip-reading network.\nWe have updated the figure so it better reflects the methodology of our method. We thank the reviewer for this comment which has helped us improve the clarity of the Fig. 1.a. We have also added a detailed Algorithm in the Appendix for further clarification.\n\n2. *In this paper, the work uses the predicted text instead of the ground truth as Lip2Speech. But the manner is similar to Kim et al.(2022). So, besides using the predicted text with an existing method, is there some new contributions in the view of methodology?*\n\n**Response:**\n\nWe would like to draw the reviewer\u2019s attention that LipVoicer utilises the **unsupervised text** at **inference time**, whereas Lip2Speech uses the **ground-truth text** for **training** in the form of an additional loss term. Integrating the textual modality with the audio at inference time is not straightforward, and it considerably changes the algorithmic approach that needs to be taken. The words in the textual modality bear no information about their timing, duration and intonation. As Tables 1-4 demonstrate,  by using the classifier guidance, LipVoicer manages to deploy the text fairly better than Lip2Speech despite using noisy text from the lip-reader. Moreover, the ablation study in Table 6 shows that when the text is not used (w2=0), the WER score drops drastically. However when w2 is adequately chosen, we manage to achieve both excellent WER **and** audio quality. Another important difference between LipVoicer and Lip2Speech is that in LipVoicer we manage to extract vocal features of the speaker and make the generated audio reflect the identity of the speaker, whereas Lip2Speech lacks this ability. \n\n3. *The modules and manners in the framework seems to be not new in the view of methodology, with the lipreading module, MelGen, text alignment manners already proposed by other works*\n\n**Response:**\nLipvoicer does indeed use existing algorithms as key components, but the key contribution and innovation is in the use of predicted text from lipreading in the speech generation process. We show that we can use a lip-reading network to acquire the textual modality without any additional supervision, and use the extracted text in the speech generation process in a way which significantly increases the intelligibility while maintaining speech quality.\nA benefit of the design of LipVoicer is that it is highly modular - every single block in the architecture, e.g. MelGen/lip-reader/ASR can be substituted by a more advanced module. As our ablation studies show, this leads to improvement of the generated speech. \nAdditionally, unlike previous methods that align text, we use an ASR and not a phoneme classifier, which is cumbersome and harder to train. For example, LRS3 comprises several dialects of the English language. Training a phoneme classifier on such a dataset requires a comprehensive dictionary that translates all the words to phoneme sequences according to each dialect, and the probability of occurrence for each word-phonemes pair. Subsequently, a model that aligns the phonemes to the spectrogram frames must be trained. Compared to this, an ASR is much easier to use and does not require in-depth knowledge of phonemes or the dialects present in the dataset.\n\n4. *The performance using the predicted text is already very appealing, but the involved lip reading model are almost the best two ones at present, with WER=19% and 26%. if the lip reading performance has been a much low value, e.g. WER=50%, what would be the performance here like?*\n\n**Response:**\n\nOur main claim is that the text modality, even if it is not perfect, can be of great help to audio generation from video. Therefore we do not argue that our method will perform well with a bad lip-reader, we rather say that the existing lip-readers can greatly boost performance. This is further corroborated in the ablation study we show in Table 7. A lip-reader with WER=32.3% is already not so good, and we show that LipVoicer is adversely affected by it. The better the lip-reader gets, the better our model performs."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1192/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700096522365,
                "cdate": 1700096522365,
                "tmdate": 1700097769190,
                "mdate": 1700097769190,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]