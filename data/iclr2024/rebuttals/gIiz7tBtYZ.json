[
    {
        "title": "Neural Optimal Transport with General Cost Functionals"
    },
    {
        "review": {
            "id": "I3hhIkd7xD",
            "forum": "gIiz7tBtYZ",
            "replyto": "gIiz7tBtYZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5898/Reviewer_BH6f"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5898/Reviewer_BH6f"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a general OT formulation, which uses a functional \\mathcal{F} to encompass common objectives and regularizers as special cases. The authors introduced a method for addressing the continuous general OT problem and illustrated how a general functional \\mathcal{F} can incorporate information, such as the presence of class labels in the data. To validate the method, the authors use synthetic datasets and various MNIST datasets for testing."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The closest prior work appears to be the study by Korotin et al. (2023b), as cited in the paper. Building upon this previous research, the authors demonstrate how one can preserve the class-label structure in OT. This contribution is novel to my knowledge."
                },
                "weaknesses": {
                    "value": "As I understand it, one benefit of employing a general functional F is to account for the class-label structure. Are there any other intended applications of a general F? If the sole purpose is to consider the class-label structure, perhaps some proofs (e.g., the proof to Theorem 3) could be simplified.\n\nI do not fully understand the image data experiments depicted in Fig 3(a) and (b) in section 5.2, and I would appreciate it if the authors could provide further explanation. From my understanding, the goal here is to identify an optimal transport (OT) map between two data distributions (e.g., the distribution of MNIST and KMNIST images) while preserving the class correspondence. To achieve this, one could visualize several source images from the same class in the source dataset and check if the corresponding target images are from the same class. However, Fig 3 appears to display only a single source image per class (top rows) along with a target image per class (2nd rows), making it unclear whether the class correspondence is preserved overall. \n\nAnother reason why Figures 3(a) and 3(b) are challenging to understand is the absence of a natural correspondence between the classes of MNIST -> KMNIST and FMNIST -> MNIST images. It is thus difficult to check the correspondence visually. To enhance visualization, would it be reasonable to use only a single dataset, such as MNIST, and establish the correspondence of images from class 0, 1, 2, ..., 9 to a permuted class order, such as 1, 2, 3, ..., 9, 0? This approach would help to visualize the class correspondence."
                },
                "questions": {
                    "value": "See the \"Weaknesses\" section above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5898/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698598187171,
            "cdate": 1698598187171,
            "tmdate": 1699636626047,
            "mdate": 1699636626047,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GZ8Br6p0Mm",
                "forum": "gIiz7tBtYZ",
                "replyto": "I3hhIkd7xD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5898/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5898/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your review"
                    },
                    "comment": {
                        "value": "Thank you for your comments and questions. Please find the answers to your questions below.\n\n**Q1: (...) Are there any other intended applications of a general F? (...)**\n\nThe primary advantage of the general cost is its ability to incorporate additional information beyond just class labels. We added an additional example of a general cost functional, please refer to the **general answer for all the reviewers** and **Appendix E**.\n\n**Q2: I do not fully understand the image data experiments depicted in Fig 3(a) and (b) in section 5.2, and I would appreciate it if the authors could provide further explanation.  (...) one could visualize several source images from the same class in the source dataset and check if the corresponding target images are from the same class. (...)**\n\nYes, your understanding is correct. Please note that the key goal of this visualization is to show that the proposed method generates images without noise and artefacts. The fact that the class if preserved is assessed via the **quantitative  metric** (*accuracy*), see Table 1.2. Following your comment, we have provided **an additional illustration** for 3 input images per class on the MNIST $\\to$ FMNIST dataset. These results are included in **Appendix C.4**. This additional visualization aims to address the concern and provide a clearer qualitative examples of how our method preserves class correspondence across multiple input images per class. \n\n**Q3: (...) To enhance visualization, would it be reasonable to use only a single dataset, such as MNIST, and establish the correspondence of images from class 0, 1, 2, ..., 9 to a permuted class order, such as 1, 2, 3, ..., 9, 0? This approach would help to visualize the class correspondence.**\n\nWe appreciate your suggestion on presentation improvements. We conducted experiments using a single dataset, specifically MNIST. We established the shifted correspondence of images from classes $0\\to9, 1\\to0, 2\\to1, 3\\to2, 4\\to3, 5\\to4, 6\\to5, 7\\to6, 8\\to7, 9\\to8$. The results of these experiments have been included in **Appendix C.11** to enhance visualization. It's worth mentioning that, in addition to mapping unrelated domains, we also explored related domain experiments, as detailed in **Appendix C.3**.  \n\n**Concluding remarks**: Please respond to our post to let us know if the clarifications above suitably address your concerns about our work. We are happy to address any remaining points during the discussion phase; if the responses above are sufficient, we kindly ask that you consider raising your score."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5898/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700514376801,
                "cdate": 1700514376801,
                "tmdate": 1700514376801,
                "mdate": 1700514376801,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "X5GIVGEh4V",
            "forum": "gIiz7tBtYZ",
            "replyto": "gIiz7tBtYZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5898/Reviewer_iaG1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5898/Reviewer_iaG1"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a novel neural network-based algorithm for computing optimal transport plans (OT) for cost functionals that go beyond typical Euclidean costs like $\\ell^1$ or $\\ell^2$. These functionals offer greater flexibility and allow the incorporation of auxiliary information, such as class labels, in constructing the transport map. \n\nExisting methods for general costs are discrete and lack out-of-sample estimation capabilities. The paper addresses the challenge of designing a continuous OT approach for general costs that can generalize to new data points in high-dimensional spaces like images. Additionally, it provides theoretical error analysis for the recovered transport plans. \n\nAs an application, the paper demonstrates how to construct a cost functional that maps data distributions while preserving class-wise structures."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper is well-structured with a clear motivation, thorough literature review, rigorous theoretical analysis, comprehensive numerical experiments, detailed implementation, and insightful discussions.\n- The paper expands upon existing neural OT techniques to accommodate general cost functionals, offering potential applications in mapping data distributions while maintaining class-wise structures."
                },
                "weaknesses": {
                    "value": "- The paper heavily draws upon the prior work of (Korotin et al. 2023a) for its theoretical foundations. Approximately 5 out of 9 pages are dedicated to presenting these theoretical results, which may not constitute the primary novelty of the paper. Based on my personal reading, it seems that the authors might be overselling their theoretical contributions, potentially leading to a less reader-friendly introduction.\n- The paper exceeds the strict upper limit of 9 pages for the main text of the submission by including a section on reproducibility (Section 7) on Page 10. As a reviewer, this doesn't pose an issue for me, but I would advise adhering to the prescribed page limits as a matter of following the rules.\n\n\n[Korotin et al. 2023a] Alexander Korotin, Daniil Selikhanovych, and Evgeny Burnaev. Kernel neural optimal transport. In\nInternational Conference on Learning Representations, 2023a."
                },
                "questions": {
                    "value": "- A direct comparison of the theoretical findings with those presented in (Korotin et al. 2023a) is essential. Readers are likely seeking a consolidated presentation rather than having to review two separate papers with overlapping theoretical content.\n- From my perspective, the paper's novelty appears to lie more in its practical applications, particularly in utilizing a cost functional to map data distributions while maintaining class-wise structures. In light of this, it would be beneficial for the paper to allocate more of its main text to discussing practical implementation aspects.\n- From my own interests, I would find it valuable if the paper could delve further into potential applications involving general cost functionals."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5898/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698635351250,
            "cdate": 1698635351250,
            "tmdate": 1699636625929,
            "mdate": 1699636625929,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "o1wqw3A3Yo",
                "forum": "gIiz7tBtYZ",
                "replyto": "X5GIVGEh4V",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5898/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5898/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your review"
                    },
                    "comment": {
                        "value": "Thank you for spending time reviewing our paper and providing valuable feedback that will help us improve the manuscript. Please find below the answers to your questions.\n\n**Q1: The paper heavily draws upon the prior work of (Korotin et al. 2023a) for its theoretical foundations. (...). Based on my personal reading, it seems that the authors might be overselling their theoretical contributions...**\n\nWe appreciate the reviewer's observation and concerns, but we believe that the theorems, corollaries, and results presented in Sections 3 are novel and important. These results are form the theoretical backbone of our work and crucial in establishing a theoretically justified algorithm that surpasses prior OT methods in practice (Section 5) as it allows using additional label information. Moreover, the unique error analysis via duality gaps for the general cost in Section 4 represents a novel aspect of our approach, distinguishing it from existing methods. \n\n**Q2:The paper exceeds the strict upper limit of 9 pages for the main text of the submission by including a section on reproducibility (Section 7) on Page 10. (...).**\n\nPlease note that according to ICLR 2024 rules, it is allowed to include the reproducibility statement as an additional 10th page, see the reproducibility section in https://iclr.cc/Conferences/2024/AuthorGuide. \n\n**Q3:A direct comparison of the theoretical findings with those presented in (Korotin et al. 2023a) is essential. Readers are likely seeking a consolidated presentation rather than having to review two separate papers with overlapping theoretical content.**\n\nTo be honest, we do not clearly understand your concern. Korotin et. al. (2023a) proposes kernel regularizers which we use as an example of a strongly convex regularizer in **Appendix D** of our paper. Korotin et. al. (2023a) uses only the *strict* convexity of this functional, while we derive its **strong** convexity for our purposes. Please see the detailed discussion before Proposition 3 (Page 28).\n\n\n**Q4: (...) the paper's novelty appears to lie more in its practical applications (...) it would be beneficial for the paper to allocate more of its main text to discussing practical implementation aspects.**\n\nThank you for acknowledging our practical contribution. However, it is important to note, as mentioned in our response to Q1, that our theoretical contribution is equally significant. General costs constitute a fruitful field in discrete OT, and our work extends them to the continuous case, effectively bridging a gap between OT and deep learning. We have established theoretical properties that specifically arise in the continuous case. We believe that our analytical insights can prove valuable for future studies in neural optimal transport.\n\nThe technical details are presented in **Appendix C.1** and the source code is in the supplementary material. *Could you please recommend, which practical implementation aspects to move to the main text from Appendix?* \n\n\n**Q5:From my own interests, I would find it valuable if the paper could delve further into potential applications involving general cost functionals.**\n\nThank you for you suggestion. First, to demonstrate the flexibility of our proposed approach, we conducted additional experiments with the newly-introduced pair-guided cost functional, please refer to the **general answer for all the reviewers** and **Appendix E**. \n\nSecond, as an example of a potential practical application, in the initial submission, we provided experiments on the batch effect problem. The batch effect is a well-known problem in biology, especially in high-throughput genomic studies such as gene expression microarrays, RNA-seq, and proteomics. See **Appendix I** for details.\n\nMore generally, our training method holds promise in addressing a broad spectrum of problems, including transfer between different modalities such as audio-to-image [1,2] and fMRI-to-image [3]. Acquiring a fully labeled dataset for these types of problems is often costly, making it crucial to develop a method that utilizes labels to construct the required transformation. Our method can be directly applied to such challenges, and we believe that delving into these types of problems is an interesting direction for future research.\n\n**References:**\n\n(Korotin et al. 2023a). Kernel neural optimal transport. (In ICLR, 2023a)\n\n[1] CH Wan et al. Towards audio to scene image synthesis using generative adversarial network.\n\n[2] Chunjin Song et al. AudioViewer: Learning To Visualize Sounds.\n\n[3] Zijiao Chen et al. Seeing Beyond the Brain: Conditional Diffusion Model with Sparse Masked Modeling for Vision Decoding\n\n**Concluding remarks:** Please respond to our post to let us know if the clarifications above suitably address your concerns about our work. We are happy to address any remaining points during the discussion phase; if the responses above are sufficient, we kindly ask that you consider raising your score."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5898/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700513762302,
                "cdate": 1700513762302,
                "tmdate": 1700513762302,
                "mdate": 1700513762302,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "mGZ9tp2Myw",
            "forum": "gIiz7tBtYZ",
            "replyto": "gIiz7tBtYZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5898/Reviewer_LT3k"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5898/Reviewer_LT3k"
            ],
            "content": {
                "summary": {
                    "value": "The paper discusses the problem of computing optimal transportation plans for a general cost functional. In particular, the paper proposes a max-min formulation of the problem, provides theoretical consistency results, and a stochastic optimization algorithm to numerically solve it. The problem is motivated by a \"dataset transfer problem\" where class labels are required to be preserved with the transportation plan. The proposed algorithm is illustrated on an a toy example with moon dataset and an example involving image dataset."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The topic of the paper is interesting and valuable to the researchers\n- The paper is written with mathematical rigor\n- The proposed algorithm is supported by theoretical arguments and numerical experiments \n- The theoretical results are important and useful"
                },
                "weaknesses": {
                    "value": "1- The novelty of the paper, in comparison to Ref[1], is weak. \n- The theoretical novelty, in comparison to the existing theoretical result in Ref[1] is not explained well (what is the new approach or new tool that is being used here). What are the challenges of considering a general cost functional that the previous approach could not handle.   \n- The computational algorithm is also very similar. The NOT algorithm block in Ref[1] can be simply extended to general cost functional. The proposed algorithm block is very similar, with the only difference that it is written for a class-guided functional.       \n\n2 - As I was reading the paper, I found the motivation, the theoretical discussion, and the numerical examples a bit disconnected. If the main contribution of the paper is the OT with general cost functional, it is necessary to provide several examples of a general cost functional, rather than just focusing on class guided cases. If the main goal of the paper is to do the class guided transportation, it should be reflected in the title, there should be more motivation why this is useful in practice, and what are the existing approaches for this particular problem. \n\n3 - The comparison with the existing OT approaches does not seem fair as they do not optimize the same cost function as your apporach. A possible comparison is to use the existing OT algorithms to do to transportation for each class separately, resulting in 10 different maps (for the MNIST case) and discuss how your proposed method, which only trains one map, is computationally more efficient, while the loss in accuracy is not significant.\n\n[1] Alexander Korotin, Daniil Selikhanovych, and Evgeny Burnaev. Neural optimal transport. In International Conference on Learning Representations, 2023"
                },
                "questions": {
                    "value": "- It is interesting to see and discuss how the algorithm performs when the data from classes overlap. \n- Regarding the discussion at the beginning of Sec 3.2., why is there a \"measurable\" map for each coupling? I understand existence for each x, but not sure how to argue existence of a measurable map as a function of x and z."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5898/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5898/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5898/Reviewer_LT3k"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5898/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698692385472,
            "cdate": 1698692385472,
            "tmdate": 1700676274634,
            "mdate": 1700676274634,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BABTO02pkC",
                "forum": "gIiz7tBtYZ",
                "replyto": "mGZ9tp2Myw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5898/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5898/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your review (Part 1)"
                    },
                    "comment": {
                        "value": "Thank you for spending time reviewing our paper and providing useful feedback that will help us improve the manuscript. Please find the answers to your questions below.\n\n**Q1.1:  The novelty of the paper, in comparison to Ref[1] (...) what is the new approach or new theoretical tool that is being used here.**\n\n- When deriving our max-min duality formula for general OT (section 3.1/3.2), we do not introduce new principal tools which have not been considered before. We just derive a max-min reformulation of general OT; it generalizes previously known classical and weak costs Ref[1], see the discussion at the end of Section 3.2. This form enables us to establish an algorithm for general OT which **can be applied to problems where the predecessors are not directly applicable** (*see the answers to Q1.2, Q2 below*).\n\n\n- The **newly introduced theoretical tool** is the duality gap analysis of Section 3.3. Please see the discussion around the Theorem 3. Previously known error analysis works exclusively with the classical OT and operate only under certain restrictive assumptions such as the convexity of the dual potential. In contrast, our error analysis is free from assumptions on the dual variable and *is applicable* not only to general OT but also *to weak OT Ref[1], for which there is currently no existing error analysis*. Please reconsider the end of **Section 3.3** (relation to prior works) and **Appendix D**.\n\n\n**Q1.2. What are the challenges of considering a general cost functional that the previous approach could not handle?**\n\nOur general cost functional-based algorithm can use **both labeled** and **unlabeled** target samples for training, which can be useful for the data transfer tasks. Existing continuous OT approaches do not handle a such type of training. Indeed, suppose we have additional information (labels) in the dataset and try to solve the class-guided mapping using the Ref[1] (as you mentioned in your Q4 below). In this scenario, we can train Ref[1] using only the labeled samples (10 separate maps in case of MNIST). In this case, the **unlabeled data immediately becomes useless**. Indeed, using unlabeled data for a class during training for that class implies that we know the labels for that class, which is a contradiction. Thus, training Ref[1] using both labeled and unlabeled target samples, as in our approach, is not possible.\n\n**Q2. The computational algorithm is also very similar. The NOT algorithm block in Ref[1] can be simply extended to general cost functional. (...) the only difference that it is written for a class-guided functional.**\n\nWe agree that our algorithms may look similar, but this is because Ref[1] is a special case of our general approach. Our algorithm provides a better flexibility for using continuous OT. As we write in **Section 4**, the **differences** lie is the estimation of the cost functional $\\mathcal{F}$ to perform the gradient updates. This difference is crucial, because such estimation, for example, allows to exploit both labeled and unlabeled data (see the answer to your previous question). \n\n**Q3: (...) I found the motivation, the theoretical discussion, and the numerical examples a bit disconnected. (...) it is necessary to provide several examples of a general cost functional (...).**\n\nOur primary conceptual contribution involves extending continuous optimal transport to general cost functionals, not exclusively focusing on the class-guided costs.\nWe appreciate your suggestion to include more examples of general cost functionals in the paper, and we have addressed this concern, please see **general response to all the reviewers**.\n\n\n**Q4: (...) A possible comparison is to use the existing OT algorithms to do to transportation for each class separately, resulting in 10 different maps (...).**\n\n\nIn response to your suggestion, we conducted this experiments using the Ref[1] algorithm for single class separately. The results of these experiments are presented in **Appendix C.9**. We can see that the qualitative results are not competitive with our algorithm. This is because Ref[1] is forced to train using only 10 target samples (the only labeled target samples in the problem setup), see the **discussion in Q1.2, Q2 above**."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5898/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700515704718,
                "cdate": 1700515704718,
                "tmdate": 1700515704718,
                "mdate": 1700515704718,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gOvbe7OtDI",
                "forum": "gIiz7tBtYZ",
                "replyto": "dx4b8MewzK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5898/Reviewer_LT3k"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5898/Reviewer_LT3k"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate the author's response and answering my questions. I see the contribution of the paper as extending [1] to a general class of cost functions. The author's response did not show me any algorithmic or theoretical novelty.  I find the applications with the labeled data, and their additional experiments, very interesting. I wish the paper was written in a way that was more focused on the application, rather than claiming to propose a novel algorithm. I increase my score to above acceptance but I do not strongly support acceptance."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5898/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700676245153,
                "cdate": 1700676245153,
                "tmdate": 1700676245153,
                "mdate": 1700676245153,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Z4FmwKaBLV",
            "forum": "gIiz7tBtYZ",
            "replyto": "gIiz7tBtYZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5898/Reviewer_nKa4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5898/Reviewer_nKa4"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes to study the so-called general cost OT problem, that is $\\min_{\\pi\\in\\Pi(P, Q)}F(\\pi)$ where $F$ is a general functional and $|\\Pi(P, Q)$ is the set of couplings between $P$ and $Q$. This is called general cost OT, because classical OT is a special case, taking $F(\\pi) = \\int c(x, y) d\\pi$.\nThe authors derive a max-min reformulation for the approach as $\\sup_{v}\\inf_{\\pi\\in \\Pi(P)} F(\\Pi) - \\int v d\\pi(y) + \\int v dQ(y)$, and then propose to parametrize the coupling $\\pi$ as samples from $[x, T(x, z)]$ where $x\\sim P$ , $z$ are samples from a Latent distribution like a Gaussian, and $T$ is a map that can itself be parameterized by a neural net. The potential $v$ is also parameterized by a NN.\nIn cases where the functional $F$ has a nice structure, the above max-min formulation can be estimated from random samples of $P, Q$ and $z$. \n\nThe authors then provide an error analysis for the method where $F$ is strongly convex.\n\nThe main application of the method discussed in the paper is to do optimal transport that is faithful to labels. Given two mixtures $P = \\sum \\alpha_n P_n$ and $Q = \\sum \\beta_n Q_n$, the authors want to estimate a transport plan between P and Q that should also map, as well as possible, each $P_n$ to $Q_n$. The corresponding cost function, a sum of energy distances between $T\\\\#P_n$ and $Q_n$, fits nicely into the proposed general cost framework, as it can be estimated from samples using a (costly) U-statistics, and is different from classical OT costs.\n\nThe experiments are on toy MNIST, KMNIST and fashion MNIST datasets, where the authors try to match each class. They compute the corresponding FID between mapped source and target dataset, and accuracy on the mapped set of a resnet trained on the target set."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is very well written and easy to follow.\nThe algorithm developed in the paper is sound, and is an interesting method to estimate transport plans for general costs.\nThe theoretical results of the paper are interesting. \nThe problem of optimal transport with label faithfulness is also interesting for ML applications, and the proposed method is an elegant solution."
                },
                "weaknesses": {
                    "value": "The main weakness of this paper is the experimental validation.\n- the setup is very toyish: the datasets are toy datasets, and the labels are entirely unrelated: why match the digit '1' to 'trouser'? This is fine as a first toy experiment but there must be some more interesting ML applications where the labels from P and from Q have a relationship and are not paired randomly: having only this artificial experiment is underwhelming, and does not convince the reader that it is actually an interesting problem for machine learning.\n- what does fig.3 show ? the description is far too short. Some methods are stochastic (i.e. T(x, z) with z random), how is the sample chosen? \n- Same question for the metrics: how it is computed for stochastic outputs could be clearer. is it averaged over z?\n- is FID computed per class or on the whole dataset? this should be clarified.\n- all fonts are too small in the figures\n\nAnother point is that the cost function proposed in Prop.1 contains a quadruple sum over samples: a discussion about its variance would be welcome. Also, the proposition mentions that it is an estimator: in which sense?"
                },
                "questions": {
                    "value": "See above.\n\n\nMisc. minor remarks:\n- in the abstract, the use of transport \"map\" vs \"plan\" can be confusing\n- Why write $\\mathcal{X} = \\mathcal{Y}$ in the notation? why bother with two spaces if they are the same.\n- $\\gamma$ is used for two different things on top of page 3\n- \"The performance of such methods in high dimensions is questionable\": a reference to elaborate on this would be welcome.\n- In eq.6, the second $\\sup \\inf$ can be removed to make it clearer what $\\mathcal{L}$ is.\n- middle of page 5: \"it follows by solving (2)\"  I think this refers to the unlabeled equation in corollary (2), not to eq.(2). Same with \"Overall, Problem (2)\" shortly after. It would probably be best to number the equation in corollary 7.\n- \"it does not always have a solution with each P_n exactly mapped to Q_n\" a few more words about why this is the case would be nice.\n- \"we employ the Sinkhorn\"\n- what is the baseline accuracy of each resnet used in the experiments?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5898/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698768605417,
            "cdate": 1698768605417,
            "tmdate": 1699636625691,
            "mdate": 1699636625691,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "eafDbA3qeY",
                "forum": "gIiz7tBtYZ",
                "replyto": "Z4FmwKaBLV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5898/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5898/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your review"
                    },
                    "comment": {
                        "value": "Thank you very much for your detailed analysis of our paper and thoughtful suggestions on paper improvements. We provide a response to your comments below.\n\n**Q1: the datasets are toy datasets, and the labels are entirely unrelated: why match the digit '1' to 'trouser'?  (...) there must be some more interesting ML applications where the labels from P and from Q have a relationship and are not paired randomly.**\n\nThese experiments are for illustrative purposes only. Our goal here is to show that even when classes are *unrelated* (\"1\" to \"trouser\", etc.), our method still can learn an class-preserving map. For experiments on the *related* domains, when the labels from $\\mathbb{P}$ and from $\\mathbb{Q}$ have a relationship, please see **Appendix C.3**. \n\nIn our paper, we also considered the application of our method to the well-known problem in biology called the *batch effect*, see **Appendix C.12**. This problem often occurs in genomic studies such as gene expression microarrays, RNA-seq, and proteomics analyses.\n\nFor **new additional experiments** involving high-resolution images, please see the **general answer** provided to all reviewers and **Appendix E**.\n\n**Q2: what does fig.3 show ? the description is far too short. Some methods are stochastic (i.e. T(x, z) with z random), how is the sample chosen?.**\n\nFigure 3 provides the results of solving the dataset transfer problem with various methods. Each column shows the transfer result of a random (test) input $x\\sim\\mathbb{P}_{n}$ (first row) from a particular class ($n=0,1,\\dots,9$). Each row show the results of transfer via a particular method in view. For methods which learn a stochastic map $T(x,z)$, we show their output $T(x,z)$ for a random noise $z$. *We added this description to Figure 3 in the revised paper.*\n\n**Q3: Same question for the metrics: how it is computed for stochastic outputs could be clearer. is it averaged over z?**\n\nInitially, we tried using multiple $z$ per $x$, generating $T(x,z)$, and averaging the metric over them. Later, we found that the result is the same if we sample only one $z$ per $x$. So we finally decided to sample a single $z$ per $x$ for simplicity. We have clarified this in the revised version. (Appendix C.1).\n\n**Q4: Is FID computed per class or on the whole dataset? this should be clarified? All fonts are too small in the figures**\n\nThe FID was calculated on the entire dataset, not separately for each class. Following you suggestion, we clarified this in the **Metrics** paragraph of the paper (Section 5).\n\n\n**Q5: Another point is that the cost function proposed in Prop.1 contains a quadruple sum over samples: a discussion about its variance would be welcome. Also, the proposition mentions that it is an estimator: in which sense?**\n\nThe reported quantity in Eq. (15) is the estimator of energy distance $\\mathcal{E}$ in the sense that its expectation w.r.t. batch samples $X_n, Z_n, Y_n$ yields $\\Delta \\mathcal{E}^2(T_{\\pi}\\sharp (\\mathbb{P}_n \\times \\mathbb{S}), \\mathbb{Q}_n)$. \n\nIt equals to $\\mathcal{E}^2$ up to $T_\\pi$-independent constant $C = - \\frac{1}{2} \\mathbb{E} \\Vert Y_2 - Y_2' \\Vert_2$, where $Y_2, Y_2'$ are independent copies from the target distribution $\\mathbb{Q}_n$. \n\nRegarding the quadruple sum, we refer the reviewer to [1] for the analysis of similar quantities. Note that in practice we did not experience any problems with estimation of Eq. (15). \n\n[1] Sutherland et. al. Unbiased estimators for the variance of MMD estimators."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5898/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700512439997,
                "cdate": 1700512439997,
                "tmdate": 1700514551049,
                "mdate": 1700514551049,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xtqOMugyAD",
                "forum": "gIiz7tBtYZ",
                "replyto": "Z4FmwKaBLV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5898/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5898/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Part.2 (Minors)"
                    },
                    "comment": {
                        "value": "**Q6.1. Why write $\\mathcal{X}=\\mathcal{Y}$ in the notation? why bother with two spaces if they are the same.**\n\nThis is done for readability purposes and helps to better distinguish integrals over the input and target data spaces.\n\n**Q6.2 \"The performance of such methods in high dimensions is questionable\": a reference to elaborate on this would be welcome.**\n\nThe general problem with these methods (wavelet/kernel-based, barycentric)  is actually the curse of dimensionality. Application of these methods bears similarity to application of kernel density estimators for approximating a probability distribution. It is not applicable to the distribution of images. This is a general problem with non-parametric setup. Regarding more theoretical justification, see [1]. They showed that as the dimensionality $d$ of data grows, the quality of the recovered map in the worst case behaves like $\\approx N^{-c/d}$ for a constant $c$, where $N$ is the number of empirical samples. \n\n[1] Hutter et. al., Minimax rates of estimation for smooth optimal transport maps. \n\n\n**Q6.3: \"it does not always have a solution with each $\\mathbb{P}_n$ exactly mapped to $\\mathbb{Q}_n$\" a few more words about why this is the case would be nice.**\n\n\nThe problem may occur with the existence of the solution since we try to map all class-specified distributions $\\mathbb{P}_n$ to their counterparts $\\mathbb{Q}_n$ with a **singe** stochastic map $T$. If, say, the source distributions are the same while the target ones are different, there is obviously no suitable map $T$.\n\n**Q6.4 what is the baseline accuracy of each resnet used in the experiments?**\n\nThe accuracy of the ResNet18 classifiers is 99.17 for the MNIST and 95.56 for the USPS datasets. For the KMNIST and MNITST, the accuracy's are 99.39 and 97.19, respectively. We have included this information in Appendix C.1.\n\n**Concluding remarks**: Please respond to our post to let us know if the clarifications above suitably address your concerns about our work. We are happy to address any remaining points during the discussion phase; if the responses above are sufficient, we kindly ask that you consider raising your score."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5898/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700512622688,
                "cdate": 1700512622688,
                "tmdate": 1700512744757,
                "mdate": 1700512744757,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]