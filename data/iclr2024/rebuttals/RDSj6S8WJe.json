[
    {
        "title": "Demystifying Linear MDPs and Novel Dynamics Aggregation Framework"
    },
    {
        "review": {
            "id": "uLaD7PlmBM",
            "forum": "RDSj6S8WJe",
            "replyto": "RDSj6S8WJe",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6687/Reviewer_Taww"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6687/Reviewer_Taww"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors prove a lower bound of $d$ for the linear MDP to aptly represent the transition probability. Therefore, they claim that linear MDPs may have regret guarantees dependent of the state space. To address the issue, they propose a novel structural aggregation framework based on dynamics, named as the dynamics aggregation. For this framework, they design a provably efficient hierarchical reinforcement learning algorithm and provide a regret upper bound for this algorithm."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The problem of proving lower bounds for $d$ and considering hierarchical structure is very interesting and important.\n\n2. The paper is solid, the proof looks correct to me. \n\n3. The lower bound is meaningful, demonstrating the limitations of the linear MDP. \n\n4. The presentation is clear in general. The simulation is interesting."
                },
                "weaknesses": {
                    "value": "My main concern is about technical novelty. The dynamics aggregation is very similar to the misspecified linear MDP considered in [1]. Also, the algorithm is adapted from LSVI-UCB. The result can be expected given [1]. \n\n[1] Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Provably efficient reinforcement learning with linear function approximation."
                },
                "questions": {
                    "value": "Please refer to the weakness section. Generally speaking, I lean towards acceptance of this paper because of the interesting lower bound and its meaningful insights."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6687/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6687/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6687/Reviewer_Taww"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6687/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697779864434,
            "cdate": 1697779864434,
            "tmdate": 1699636767303,
            "mdate": 1699636767303,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Kl1ogUAuGh",
                "forum": "RDSj6S8WJe",
                "replyto": "uLaD7PlmBM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6687/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6687/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We sincerely thank you for your time to review our paper and for your positive and valuable feedback. As you pointed out, the findings of Theorem 1 are of great importance. We anticipate that these results will significantly influence the RL community.\n\nRegarding the second part of our paper, we emphasize the **broad impact** of our dynamics aggregation framework, particularly its seamless integration into existing algorithms. This indicates that our framework can be readily applied to a wide range of Linear MDP algorithms, thereby improving their efficiency. Additionally, we believe that investigating methods with higher versatility \u2013 those that can easily adapt to existing algorithms \u2013 is a crucial research direction.\n\n\nHere are our response to your question:\n\n### 1. Dynamics aggregation vs misspecified Linear MDP\nIf we have understood your question correctly, it appears that you are inquiring about the distinction between employing dynamics aggregation and directly learning from an aggregated MDP that contains a misspecification error. However, we *cannot* directly consider the aggregated MDP since the Q-value may differ for two states that are mapped to the same aggregated (or latent) state. Let's provide an example:\n\n*Consider an MDP with $\\mathcal{S} = \\{s_1, s_2\\}$, $\\mathcal{A} = \\{a\\}$, and $r(s_1, a) =1$ and $r(s_2, a) =0$. Let the horizon length $H$ be 1 for simplicity. And assume that $s_1$ and $s_2$ are mapped into one aggregated state $\\bar{s}_1$. Then, it is clear that $Q(s_1, a) = 1$ and $Q(s_2, a) = 0$ since $H=1$.*\n\nTherefore, if we consider only the aggregated MDPs, we cannot differentiate between $Q(s_1, a)$ and $Q(s_2, a)$. Note that in our definition of Q-values (Definition 5), we distinctly define the aggregated Q-values depending on the subMDP to which the state $s$ belongs. Hence, they have the subMDP index $i$. \n\n\n### 2. Comparison to LSVI-UCB\nThe key distinction of our algorithm compared to LSVI-UCB lies in its **model-based** nature. This primarily brings the advantage of **reusability**. In model-free approaches, Q-values are defined by the aggregated feature and specific parameters ($w_h^{\\pi}$ in Jin et al., 2020). Thus, it's impossible to distinguish the Q-values for two states mapped to the same aggregated state $\\bar{s}$. As mentioned above, the Q-value may differ for two states mapped to the same aggregated state. In essence, model-free approaches fail to effectively leverage the hierarchical structure, even with perfectly learned sub-structures and accurate mapping. On the other hand, our model-based approach enables the reuse of learned dynamics of subMDPs ($\\hat{\\mu}^{(n)}$) in similar subMDPs. Therefore, it\u2019s important to note the fundamental intuition and significance of using a model-based approach to exploit the hierarchical structure."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6687/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699889196858,
                "cdate": 1699889196858,
                "tmdate": 1699889196858,
                "mdate": 1699889196858,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jC66feIOR6",
                "forum": "RDSj6S8WJe",
                "replyto": "Kl1ogUAuGh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6687/Reviewer_Taww"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6687/Reviewer_Taww"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the detailed explanation. I will keep my score to vote for acceptance."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6687/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700172575206,
                "cdate": 1700172575206,
                "tmdate": 1700172575206,
                "mdate": 1700172575206,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ZnKbLOSaAc",
            "forum": "RDSj6S8WJe",
            "replyto": "RDSj6S8WJe",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6687/Reviewer_JZay"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6687/Reviewer_JZay"
            ],
            "content": {
                "summary": {
                    "value": "This paper provides a new perspective into the low-dimensional representation structures of MDPs. On the one hand, it casts reasonable doubt on the popular linear representation structure via a simple lower bound on the feature dimension $d$, showing that $d$ may actually scale up with $S$ when the direct reachability $U$ of the environment is limited. On the other hand, it proposes a novel dynamics-based hierarchical aggregation framework that leverages *known* mappings to aggregated sub-MDPs (each equipped with linear representation structure), and shows that it achieves a competitive regret *under certain assumptions*."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper provides a new angle for researchers to understand the fundamental limit of linear MDPs. The result by itself is technically simple and straightforward, but the valuable part of it is the motivation it provides to reflect upon a popular modelling option that is potentially subject to implementation issues.\n2. The flow and writing of this paper is good. It provides the reader with adequate background knowledge, illustrates the key points clearly with concrete examples and figures, and accompanies the main results with intuitions and discussions.\n3. The mathematical proofs seem correct to the reviewer in the form they are presented in the paper, though results cited from literature are taken for granted."
                },
                "weaknesses": {
                    "value": "1. The authors claim that the dynamics-based aggregation framework proposed in the second part of the paper *addresses the limitation of linear MDPs*. The reviewer is skeptical about the contribution, in that:\n    * The aggregation framework seems very artificial. There is not enough motivation why people would have the aggregation mapping $\\psi^{i \\to (n)}$ in hand *a priori*. Specifically, why don't people directly consider the aggregated MDP when they model real-world scenarios, but rather introduce a large-scale MDP and identifies the similarity between (unnecessarily differentiated) states in the meantime?\n    * Apart from the novel idea of substructures, the contribution of the second part seems minimal to the reviewer since it looks like a simple application of LSVI-UCB in sub-MDPs. The proof structure is also similar to that of LSVI-UCB with minor changes.\n    * The results will be very interesting if the aggregation structure can be learned (either online or offline) rather than given, but reviewer fails to come up with a quick fix that enables such learning. The reviewer would be more positive about this paper if the authors can, at least, illustrate a potential algorithm design idea to learn the structure.\n2. The comparison against LSVI-UCB seems sketchy. The authors claim in the abstract that $d_{\\psi}^3 N \\ll d^3$ is \"readily met in real-world environments\", but the only discussion about this is a few conjectured inequalities on page 9 without any real-world data. This seems like too much overclaiming to the reviewer, and thus the authors are urged to provide real-world evidence for their claim.\n3. The experiment design can be improved in the following ways:\n    * The source code to reproduce the results is not publicly available.\n    * The environment is designed to be tabular, which is reducible to linear MDPs, but only in a very inefficient way. The comparison is therefore unfair. It would be more convincing if the algorithms can be evaluated and compared in MDPs with intrinsic low-dimensional structures.\n    * The MDPs used in the experiment are very small in size. Experiments are expected to, at least, show adequate scalability of the algorithm."
                },
                "questions": {
                    "value": "1. Why would people have the aggregation mapping $\\psi^{i \\to (n)}$ in hand *a priori* in modelling?\n2. In what kind of real-world environments would the condition $d_{\\psi}^3 N \\ll d^3$ be met? Please provide concrete examples.\n3. In Algorithm 1, $n$ and $i$ seems to be variables that automatically get their values upon observation of states. Should it be written in a clearer way to show that $n$ and $i$ are actually calculated from the state?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6687/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6687/Reviewer_JZay",
                        "ICLR.cc/2024/Conference/Submission6687/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6687/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698330723759,
            "cdate": 1698330723759,
            "tmdate": 1700091438516,
            "mdate": 1700091438516,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "SHNV4MjNCV",
                "forum": "RDSj6S8WJe",
                "replyto": "ZnKbLOSaAc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6687/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6687/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We sincerely appreciate the time and effort you have invested in reviewing our paper. Thank you for your feedback, and we are more than happy to address your comments and questions. Below, we address each point raised in your review with the aim of clarifying and further enriching our research.\n\n### 1. Motivation for dynamics aggregation\nWe *cannot* directly consider the aggregated MDP since the Q-value may differ for two states that are mapped to the same aggregated (or latent) state. Let's provide an example:\n\n*Consider an MDP with $\\mathcal{S} = \\{s_1, s_2\\}$, $\\mathcal{A} = \\{a\\}$, and $r(s_1, a) =1$ and $r(s_2, a) =0$. Let the horizon length $H$ be 1 for simplicity. And assume that $s_1$ and $s_2$ are mapped into one aggregated state $\\bar{s}_1$. Then, it is clear that $Q(s_1, a) = 1$ and $Q(s_2, a) = 0$ since $H=1$.*\n\nTherefore, if we consider only the aggregated MDPs, we cannot differentiate between $Q(s_1, a)$ and $Q(s_2, a)$. Note that in our definition of Q-values (Definition 5), we distinctly define the aggregated Q-values depending on the subMDP to which the state $s$ belongs. Hence, they have the subMDP index $i$. \n\nWe acknowledge that our initial explanation of the motivation lacked sufficient detail. Thank you for bringing this to our attention. We will ensure to provide a more detailed explanation in the revised version of our paper.\n\n### 2. Contribution of the second part\n\nThe core contribution of our paper extends far beyond the introduction of a new algorithm. Our work provides critical and previously overlooked insights into linear function approximation (the fundamental limitation of Linear MDP that no previous works have addressed), insights that have the potential to fundamentally redirect the trajectory of future research in this area. This novel perspective, presented in the first part of our paper, represents a significant implication on the current theoretical RL research. \nTherefore, we respectfully suggest that the evaluative focus should not solely be on the latter sections of our work. The first part of our paper, which lays the theoretical groundwork for the second part, is deserving of particular attention. It is here that we challenge and expand upon the established norms, offering a fresh lens through which to view and understand the field. We firmly believe in the substantial impact our paper can have on the academic community. \n\n#### 2-1. Versatility of dynamics aggregation\nWe believe that the ability to seamlessly integrate our dynamics aggreagtion framework into existing algorithms indeed indicates its extensive impact, rather than suggesting a minimal contribution. This implies that dynamics aggregation can be easily applied to various Linear MDP algorithms, enhancing their efficiency. We also think that exploring methods with greater versatility, ones that can easily adapt to existing algorithms, is an essential avenue for research.\n\n\n#### 2-2. Comparison to LSVI-UCB\nCompared to LSVI-UCB, the most salient feature of our algorithm is that it is **model-based**, which notably offers the advantage of **reusability**. As previously mentioned in 1, re-using aggregated Q-values is not feasible in model-free approaches because the Q-values for two states mapped to the same aggregated state can differ. This limitation means that model-free approaches are unable to effectively utilize the hierarchical structure, even with perfectly learned sub-structures and accurate mapping. In contrast, our model-based approach allows for the reuse of learned dynamics of subMDPs ($\\hat{\\mu}^{(n)}$) in similar subMDPs. Although the proof structure appears similar, the underlying intuition and significance of the model-based approach in leveraging the hierarchical structure are crucial.\n\n### 3. Learning dynamics aggregation\nWhile it is not our focus to deal with learning mapping, we believe that the known dynamics aggregation assumption can be **relaxed** through the use of model selection techniques, as proposed in [4] and [5]. At each episode, the agent selects one of the base mappings to play and receives the rewards associated with the (low level) policy deployed by that base mapping. Then, it updates the (high level) policy  for selecting the base mapping. However, this is currently unclear and beyond the scope of our work. We will leave such research directions for future work.\n\nHowever, more importantly, we highlight that the assumption of a known hierarchical structure is actually common in hierarchical RL (for example, Wen et al., 2020 [3], also assumed a known equivalence mapping).Furthermore, in real-life scenarios, *humans often utilizes the (approximately) known mapping* between two similar environments, even if they don't know the transitions. Take, for instance, playing different versions of video games such as Super Mario. Despite variations in each game, the core gameplay mechanics stay the same -- a fact that players often grasp intuitively or through prior knowledge."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6687/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699889054162,
                "cdate": 1699889054162,
                "tmdate": 1699892669507,
                "mdate": 1699892669507,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Xrx4pbBIhu",
                "forum": "RDSj6S8WJe",
                "replyto": "ZnKbLOSaAc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6687/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6687/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Therefore, especially in scenarios typical of human learning, knowledge of dynamics aggregation is quite common. We suggest that the RL community should focus more on HRL frameworks. Doing so could lead to the development of algorithms that approach the efficiency seen in human learning.\n\nFurthermore, it is important to recognize that the presence of an additional structure, or a seemingly stronger assumption, in a research framework should **NOT** lead to its immediate dismissal. On the contrary, when such structures are well-justified and relevant, they can be pivotal in advancing efficient learning methodologies. For example, in the study of LinearMDPs, initial efforts concentrated on cases with *known* features. Only in more recent times have researchers started to develop techniques for learning these features. Similarly, in HRL,  the long-standing assumption has been the presence of *known* options. Very recently, the focus has shifting towards the emerging field of option learning. This principle is at the core of our approach.  Our assumptions regarding dynamics aggregation are not just theoretical ideas; they are based on observable phenomena in actual learning environments (refer Appendix C.2). We are convinced that our discoveries about the fundamental limits of Linear MDPs are vital. Our proposed framework lays the groundwork for future research. It creates opportunities for a balanced approach that combines theoretical thoroughness with practical applicability.\n\n\n[1] Cutkosky, Ashok, Abhimanyu Das, and Manish Purohit. \"Upper confidence bounds for combining stochastic bandits.\" arXiv preprint arXiv:2012.13115 (2020).\n\n[2] Cutkosky, Ashok, et al. \"Dynamic balancing for model selection in bandits and rl.\" International Conference on Machine Learning. PMLR, 2021.\n\n[3] Wen, Zheng, et al. \"On efficiency in hierarchical reinforcement learning.\" Advances in Neural Information Processing Systems 33 (2020): 6708-6718.\n\n### 4. Additional explanations about $d_\\psi^3 N \\ll d^3$\nIn the abstract, we claimed that the inequality $d_\\psi^3 N \\ll d^3$ is valid in *most real-world environments* with hierarchical structures.  By *most real-world environments*, we refer to scenarios where the size of directly reachable states **$U$ is much smaller than $S$** (for continuous state space, consider $U$ as $Vol(\\mathcal{U})$ and $S$ as $Vol(\\mathcal{S})$). The real-world examples where this condition holds are extensively discussed in Section 4. Examples 1, 2, 3, and 4 all meet the condition that $U$ does not scale with $S$. On the other hand, since $M$ represents the maximum cardinality of subMDPs, $M \\ll S$ is true if  a hierarchical structure exists (with small sub-structures repeating). Consequently, the inequality  $M^2 \\leq S^2/U^3$ is satisfied (as $U$ is negligible), leading to the desired result.\n\n\n### 5. Experiment design\n\n#### 5.1 Source code\nWe have attached the source code as supplementary material.\n\n#### 5.2 Desigining low-dimensional features\nWe appreciate your question as it allows us to clarify our main claims and what we aim to emphasize in Theorem 1: it is **impossible** to design low-dimensional Linear MDP structures. The transition kernel of the RiverSwim environment is fully ranked, i.e., $rank(\\mathbb{P}) = S$ (refer Appendix B.5). In this scenario, can any method decompose $\\mathbb{P}$ into two matrice $\\mathbb{P} = \\Phi \\mu$ without an approximation error, where the column dimension of $\\Phi$ and the row dimension of $\\mu$ are significantly smaller than $S$? The answer is **NO**. In linear algebra, particularly in methods like SVD, the column dimension of $\\Phi$ and the row dimension of $\\mu$  are required to be $S$. **This is exactly what we claim in Theorem 1!** Unless directly reachable states (by single-step transition) scale with the entire state space by constant factor, (i.e., $U = \\Theta(S)$) the feature dimension $d$ should scale with $S$ in Linear MDP. \n\n#### 5.3 Size of experimental environment\nSince designing low-dimensional Linear MDP structures is impossible, implementation of algorithms in Linear MDPs confines us to a (nearly) tabular setting ($d \\approx S$). A major challenge with all existing Linear MDP algorithms is the necessity to compute the inverse of the Gram matrix, incurring a computational cost of $O(d^2) \\approx O(S^2)$. This is **intractable** when $S$ is very large. This limitation hinders the expansion of the state space size. It is important to note that all existing Linear MDP algorithm experiments have been conducted in a tabular setting ([4] and [5]). And in this work, we have provided a theoretical proof explaining why they couldn't (Theorem 1). \n\nPlease not that, this issue is a common challenge encountered in all Linear MDP algorithms. However, in the case of an MDP with a hierarchical structure, where small sub-structures repeat ($MN \\ll S$), our algorithm can be effectively implemented since $d_{\\psi}\\leq M \\ll S$, requiring only a"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6687/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699889138686,
                "cdate": 1699889138686,
                "tmdate": 1700661578786,
                "mdate": 1700661578786,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "oYgTyx4Vkw",
                "forum": "RDSj6S8WJe",
                "replyto": "ruYPEmctT8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6687/Reviewer_JZay"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6687/Reviewer_JZay"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the responses!"
                    },
                    "comment": {
                        "value": "I appreciate the authors' efforts to settle my questions and doubts via concrete examples and detailed explanations.\n\nThe motivation for, and the necessity of, introducing dynamics aggregation is clearer now. The proposed solution to learn to aggregate seems more or less like a bandit over a few candidate mappings, which is reasonable and does sound promising.\n\nThe justification for the contribution of the second part is acceptable. In the original review I mentioned that I also found the first part more interesting, and the argument of versatility is reasonable. It would be even better if you could come up with a general pipeline to convert popular RL algorithms into aggregation-based algorithms, though that is potentially another independent work.\n\nI'm still not fully convinced about the claim $d_{\\psi}^3 N \\ll d^3$. Since you have mentioned the examples listed in Section 4, maybe the most straight-forward is to show a few working aggregation mappings for them (which I think should be doable). It's even better if you can come up with systematic methods to do so.\n\nI appreciate the points that the authors make about experimental design. It again justifies a common myth that low-dimensional features can be designed in common environments.\n\nBased on the current responses, I'm happy to raise my rating to 6."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6687/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700091427705,
                "cdate": 1700091427705,
                "tmdate": 1700091427705,
                "mdate": 1700091427705,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "n4OC8cN4pZ",
            "forum": "RDSj6S8WJe",
            "replyto": "RDSj6S8WJe",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6687/Reviewer_M46L"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6687/Reviewer_M46L"
            ],
            "content": {
                "summary": {
                    "value": "This paper provides two interesting contributions:\n\n1) It shows a lower bound on the feature dimension in linear MDPs which depends on the inverse of the maximum reachability of the environment.\n\n2) It provides a novel algorithm with sublinear regret for hierarchical RL where each of the subMDP is a linear MDP."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "I think that (albeit simple and potentially expected) the lower bound on the feature dimension of Linear MDP is an important result for the RL theory community.\n\nThe algorithm (UC-HRL) seems to be an interesting and novel contribution to hierarchical RL."
                },
                "weaknesses": {
                    "value": "The regret bound in Theorem 2 has a linear term which can be made sublinear only if $\\epsilon_P$ in Definition 4 is of order $\\mathcal{O}(1 / poly(T))$. However it is not very clear from the paper how big $\\epsilon_P$ can be for common choices of the approximate feature aggregation mappings $\\psi$.\n\nThe fact that the aggregating functions $\\psi$ are required to be known in advance seems rather strong but somehow common in hierarchical RL.\n\nThe discussion after Theorem 2 that justifies that $d^3_{\\psi} N \\leq d^3$ is unclear in my opinion in particular I do not understand why the regime $MN \\leq S$ and $M^2 \\leq S^2/U^3$ is a reasonable one. Maybe the author should consider expanding this discussion in their revision."
                },
                "questions": {
                    "value": "1) Can you provide an example of aggregating functions $\\psi$ such that the error $\\epsilon_P \\leq 1/\\sqrt{T}$ ?\n\n2) Another related question is: do you expect $\\epsilon_P$ to decrease as the number of subMDP $N$ increase ? Is it possible in this way to find the value of $N$ which minimizes the regret bound? \n\n3) Could add an example of Linear MDP where the conditions $MN \\leq S$ and $M^2 \\leq S^2/U^3$ hold and therefore the hierarchical algorithm has a clear advantage ?\n\n4) Can the assumption of known aggregating functions $\\psi$ be relaxed ?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6687/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6687/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6687/Reviewer_M46L"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6687/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698337003776,
            "cdate": 1698337003776,
            "tmdate": 1699636767026,
            "mdate": 1699636767026,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8YNDqmnRgw",
                "forum": "RDSj6S8WJe",
                "replyto": "n4OC8cN4pZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6687/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6687/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We sincerely appreciate the time and effort you have invested in reviewing our paper. Thank you for overall positive evaluation of our paper. Below, we address each point raised in your review with the aim of clarifying and further enriching our research.\n\n### 1. Dynamics aggregation misspecification error\n\nThe term $\\epsilon_p$ represents the model misspecification error allowing that our modeling assumption may be imperfect. Model misspecification is very **common** in real life and has been often considered in many previous works in Linear MDP. For example, in [1] (Theorem 3.2), [2] (Theorem 1) and [3] (Theorem 1), the authors proposed regret bounds that also involve the term $T \\epsilon$, where $\\epsilon$ is the model misspecification error. Therefore, the term $\\epsilon_p$ represents the effect of model misspecification on regret, rather than implying that regret is directly proportional to $T$. Now, there are many works that **assume** there is no misspecification error, namely assuming that their modeling assumption **perfectly** reflects the environment. We could also do so and not deal with the misspecification error if we desired. Yet, we wanted our framework to be more inclusive when presenting the problem formulation, allowing for possible model misspecification. In essense, this is just a matter of problem setting by our choice. That is, if you wish, we can certainly consider problem setting **without model misspecification**. We strongly believe that this should not be any disadvantage on this part. We will make this much clearer in our revision.\n\nFurthermore, the relationship between $N$ and $\\epsilon_p$  is not clear to us. The model misspecification error $\\epsilon_p$ is generally not within our control, as it is primarily determined by the specific model (or mapping) we select. \n\n[1] Jin, Chi, et al. \"Provably efficient reinforcement learning with linear function approximation.\" Conference on Learning Theory. PMLR, 2020.\n\n[2] Zanette, Andrea, et al. \"Frequentist regret bounds for randomized least-squares value iteration.\" International Conference on Artificial Intelligence and Statistics. PMLR, 2020.\n\n[3]  Zanette, Andrea, et al. \"Learning near optimal policies with low inherent bellman error.\" ICML, 2020.\n\n### 2. Additional explanations about $d_\\psi^3N \\ll d^3$\n#### 2-1. $MN \\leq S$\nNote that $M$ denotes the maximum number of states within a subMDP, and $N$ indicates  the number of aggregated subMDPs. Therefore, $MN \\leq S$ suggests the presence of a hierarchical structure characterized by repeating small sub-structures. Our experimental setup, Block-RiverSwim, exhibits this hierarchical pattern (see Appendix G and Figure G.1 for details). Figure G.1 illustrates the Block-RiverSwim structure with $S=14$ and $L = 6$ (representing the number of subMDPs). Each substructure, or 'Block', is comprised of 3 states with identical dynamics, resulting in 3 distinct  aggregated subMDPs, thus, $N = 3$:  $\\\\{s_1\\\\}, \\\\{s_{14}\\\\}$ and Blocks.\n\n#### 2-2. $M^2 \\leq S^2/U^3$\nThis condition is satisfied in *most real-world environments* with hierarchical structures.  By *most real-world environments*, we refer to scenarios where the size of directly reachable states **$U$ is much smaller than $S$** (for continuous state space, consider $U$ as $Vol(\\mathcal{U})$ and $S$ as $Vol(\\mathcal{S})$). The real-world examples where this condition holds are extensively discussed in Section 4. Examples 1, 2, 3, and 4 all meet the condition that $U$ does not scale with $S$. Since $M$ represents the maximum cardinality of subMDPs, $M \\ll S$ is true if  a hierarchical structure exists (with small sub-structures repeating). Consequently, the inequality  $M^2 \\leq S^2/U^3$ is satisfied (as $U$ is negligible), leading to the desired result.\n\n### 3. Relaxing known mapping assumption\nWhile it is not our focus to deal with learning mapping, we believe that the known dynamics aggregation assumption can be **relaxed** through the use of model selection techniques, as proposed in [4] and [5]. At each episode, the agent selects one of the base mappings to play and receives the rewards associated with the (low level) policy deployed by that base mapping. Then, it updates the (high level) policy  for selecting the base mapping. However, this is currently unclear and beyond the scope of our work. We will leave such research directions for future work.\n\nHowever, more importantly, we highlight that the assumption of a known hierarchical structure is actually common in hierarchical RL (for example, Wen et al., 2020 [6], also assumed a known equivalence mapping). Furthermore, in real-life scenarios, *humans often utilizes the (approximately) known mapping* between two similar environments, even if they don't know the transitions. Consider the example of playing various versions of video games like Super Mario. Despite the differences in each game, the underlying gameplay mechanics remain consistent, and this is something we often know."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6687/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699888880067,
                "cdate": 1699888880067,
                "tmdate": 1700723688722,
                "mdate": 1700723688722,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "x76nsbYzD0",
                "forum": "RDSj6S8WJe",
                "replyto": "GpF5FUSD03",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6687/Reviewer_M46L"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6687/Reviewer_M46L"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks to the authors for their response. \n\nI now understand better the role of $\\epsilon_P$ I think you could add a comment saying that $\\epsilon_P=0$ in case the real MDP is perfectly approximated by the hierarchical linear model.\n\nSecondly, I think it would be fair to add also an example of MDP where there is no hierarchical  structure to exploit. In this case I think that there would be no hope to improve upon the bound that LSVI-UCB provides for linear MDPs.\n\nI will keep my positive evaluation of the paper.\n\nBest,\n\nReviewer M46L"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6687/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700040156579,
                "cdate": 1700040156579,
                "tmdate": 1700040156579,
                "mdate": 1700040156579,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "VvhGRYIc61",
            "forum": "RDSj6S8WJe",
            "replyto": "RDSj6S8WJe",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6687/Reviewer_hMDT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6687/Reviewer_hMDT"
            ],
            "content": {
                "summary": {
                    "value": "The paper focuses on the problem in the linear Markov decision process and its linear representation to the transition probability kernel. It first shows that the current regret results form the literature has the dependency on the state cardinality, which comes from the fact that the dimension of the linear representation for the transition kernel is lower bounded by the rank of the matrix. Then it leverages the technique from the previous works on state aggregation and group mapping to propose a hierarchical version of the linear MDP algorithm, reducing the final regret dependency on the state cardinality to the grouped MDP dimension."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. The paper shows that the dimension of the linear representation for probability transition kernel is lower bounded by |S|/|U|, where |S| is the cardinality of the states and |U| is the maximum size of directly reachable states. If |U| is not the order of |S|, the regret would depend on the state cardinality.\n2. The paper develops a hierarchical linear MDP algorithm to reduce the state cardinality dependency in the final regret. It leverages the internal structure of the problem with the state aggregation and mapping from previous works. The final regret and examples show the effect of it."
                },
                "weaknesses": {
                    "value": "1. The paper makes stronger assumption than previous linear MDP algorithms. For the sub-structure that is explored by the paper, it assumes that the dynamic aggregation is known and has the desired boundedness in Definition 4. \n2. For the final regret proven by the paper, although the regret seems to be improved in terms of the state cardinality theoretically, it also introduces another T-dependent term characterizing the aggregation gap w.r.t the original probability transition kernel. It's not clear to me whether the newly introduced term would cancel out the improvement from the first term in Theorem 2.\n3. The algorithm seems to be a direct extension of the previous works in the tabular case by adding in linear representation and similar analysis."
                },
                "questions": {
                    "value": "The assumption of known dynamic aggregation is strong to me. In reality, how could we extract such information without knowing the transition kernel?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6687/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698692404400,
            "cdate": 1698692404400,
            "tmdate": 1699636766872,
            "mdate": 1699636766872,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "m6X845Bknf",
                "forum": "RDSj6S8WJe",
                "replyto": "VvhGRYIc61",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6687/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6687/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We sincerely appreciate the time and effort you have invested in reviewing our paper. Thank you for your feedback, and we are more than happy to address your comments and questions. Below, we address each point raised in your review with the aim of clarifying and further enriching our research.\n\n### 1. The assumption of known dynamics aggregation stronger?\n\nThe reviewer raises a concern about our assumption of known dynamics aggregation being \"stronger\" than those used in Linear MDPs. We appreciate this perspective, but our analysis suggests a more nuanced understanding is required. In Linear MDPs, for algorithms to achieve an $S$-independent regret bound, a critical yet implicit assumption is that the size of directly reachable states scales with the entire state space. This assumption, as we rigorously show in the paper, is quite stringent and often unrealistic in practical scenarios.\n\nIn contrast, our approach, while indeed assuming known dynamics aggregation, does not necessitate such extensive reachability. This difference is pivotal. Our assumption of dynamics aggregation being known, or at least approximable, is not only less restrictive but also more aligned with real-world learning scenarios, as detailed in our paper (refer Appendix C.2). Such dynamics aggregation can often be observed or inferred in various practical contexts, unlike the far-fetched assumption of reachability scaling with the entire state-space, although not explicitly stated (since we are the first to show), required by Linear MDPs.\n\nTherefore, while it might initially appear that our assumptions are stronger, a closer examination reveals that they are, in fact, more grounded in real-world applicability. This distinction is crucial in evaluating the relative \"strength\" of these assumptions. Hence, it is not straightforward to deem one set of assumptions categorically stronger than the other without considering their practical implications and feasibility.\n\nMoreover, it is important to recognize that the presence of an additional structure, or a seemingly stronger assumption, in a research framework should **NOT** lead to its immediate dismissal. On the contrary, when such structures are well-justified and relevant, they can be pivotal in advancing efficient learning methodologies. This principle is at the core of our approach. The assumptions we make about dynamics aggregation, far from being mere theoretical constructs, are grounded in observable phenomena in real-world learning environments. We believe that our new findings of the fundamental limits of the Linear MDPs is crucial and our proposed framework sets the stage for future research, opening new avenues that blend theoretical rigor with practical relevance. \n\nLet us emphasize that the primary contribution of our paper lies not just in introducing a new algorithm, but in providing a **critical and new insight that all exisitng works in linear function approximation overlooked**  and offering **newer direction for future research**. In this regard, we sincerely ask you to reconsider the **broader impact** that the first part of the paper (on the limitation of Linear MDPs) provides and how it connects to the second part.\n\n#### 1-1. Reemphasize Theorem 1\nTo begin with, we want to reiterate the importance of Theorem 1. Let us rephrase the key implications of Theorem 1 and Corollary 1. \n\n*Unless directly reachable states (by single-step transition) scale with the entire state space by constant factor, (i.e., $U = \\Theta(S)$) the feature dimension $d$ should scale with $S$ in Linear MDP.*\n\nIn the vast majority of practical environments, by a single transition, it is impossible that the entire state space or the constant factor of the entire state space (especially for large state spaces) is reachable, as we discussed in Section 4. Then, our Theorem 1 establishes that the feature dimension $d$ should scale with $S$ to properly express the transition probability in Linear MDP. This result is absolutely crucial, and even serves as a counter-example to the assertion that Linear MDP automatically can induce efficient learning for large state space or even infinite state space (without further assumptions on reachability). \n\nIn other words, all existing works in Linear MDPs that claim $S$-independent regret are **only** efficient under a very strong and impractical assumption that $U = \\Theta(S)$ (once again, this was an aspect overlooked in all existing papers.). Compared to this, we believe our assumption of known dynamics aggregation is not as strong. As another reviewer mentioned, the assumption of a known hierarchical structure is actually **common in hierarchical RL**: [1] Wen et al., 2020, for instance, also assumed a known equivalence mapping."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6687/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699888345832,
                "cdate": 1699888345832,
                "tmdate": 1699888345832,
                "mdate": 1699888345832,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wQwrE00Tud",
                "forum": "RDSj6S8WJe",
                "replyto": "VvhGRYIc61",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6687/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6687/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "#### 1-2. Intuition behind the dynamics aggregation\nIn the second part of our paper (Section 5, 6), we aims to **offer a possible solution to overcome the limitation of Linear MDPs**. While the structural assumption might appear stylistic, it provides meaningful insights. Merely employing function approximation is insufficient; instead, effectively utilizing hierarchical structures can prove beneficial.\n \nMoreover, the dynamics aggregation carries the **practical implications for human learning** (refer Appendix C.2). For example, a driver who can drive in New York will be able to drive immediately in San Francisco and would highly likely be able to drive in Paris as well. This is possible mainly due to the existence of repeating structures and mappings between different structures. Often such mappings are readily available, and we humans are good at exploiting them. Mappings do not have to be perfect, but they could be approximate, as we consider in our work. The decompositions we consider in our work incorporate such intuition into Linear MDPs.\n\n#### 1-3. Known dynamics aggregation\n\nIn response to your question, it's important to emphasize that *learning transitions* and *knowing dynamics aggregation* are **entirely separate concepts**. We can learn transitions regardless of our knowledge of dynamics aggregation. Knowledge of dynamics aggregation enables us to leverage the hierarchical structure, which in turn makes the learning process more efficient. However, it's important to note that this knowledge, while beneficial, is not essential for learning transitions.\n\nFurthermore, in real-life scenarios, *humans often utilizes the (approximately) known mapping* between two similar environments, even if they don't know the transitions. Take playing different versions of video games such as Super Mario as an example. Although each game has its unique features, the fundamental gameplay mechanics stay largely the same, and this is something players tend to be aware of. Therefore, in common scenarios, particularly in human learning, knowledge of dynamics aggregation is prevalent. Based on this observation, we think that the RL community should give more attention to HRL frameworks. Doing so could lead to the creation of algorithms that emulate the efficiency of human learning.\n\nOur assumption of known dynamics aggregation in the proposed framework is a **strategic starting point** rather than a limitation. We believe it's sensible to first establish efficiency in situations where dynamics aggregation is *known* before moving on to scenarios with *unknown* dynamics aggregation. This gradual progression lays a solid groundwork for future research, as seen in many prior studies. In Linear MDPs, for instance, the initial focus was on scenarios with known features. The shift towards developing methods to learn these features is a more recent development. Similarly, in HRL, particularly within the 'option' framework [2, 3], relying on known options has been a long-standing practice. Only in recent times has the community begun exploring option learning.  Yet, this area, especially in the context of learning options for regret guarantees, still remains underexplored and presents a fertile ground particularly for functiona approximation.\n\n\nThe models studied in theoretical RL research are mostly still distant from the real world, to start with. Take the widely used Linear MDPs and linear mixture MDPs, for example. Is it reasonable to assume a transition probability is linear in real life? Probably not. However, the RL research community is making concerted efforts to tackle settings that are more aligned with practical applications. We are confident that our work contributes to this endeavor, even though it may not be entirely perfect yet. We hope that this work serves as an important step toward solving RL with function approximation more efficiently, as evidenced by the numerical experiments as well as the theoretical guarantees.\n\n\n[1] Wen, Zheng, et al. \"On efficiency in hierarchical reinforcement learning.\" Advances in Neural Information Processing Systems 33 (2020): 6708-6718.\n\n[2] Richard S Sutton, Doina Precup, and Satinder Singh. Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning. Artificial intelligence, 112(1-2):181\u2013211, 1999.\n\n[3] Ronan Fruit, Matteo Pirotta, Alessandro Lazaric, and Emma Brunskill. Regret minimization in mdps with options without prior knowledge. Advances in Neural Information Processing Systems, 30,\n2017.\n\n### 2. Dynamics aggregation misspecification error\n\nThe term $\\epsilon_p$ represents the model misspecification error allowing that our modeling assumption may be imperfect. Model misspecification is very **common** in real life and has been often considered in many previous works in Linear MDP. For example, in [4] (Theorem 3.2) and [5] (Theorem 1), the authors proposed regret bounds that also involve the term $T \\epsilon$,"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6687/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699888516588,
                "cdate": 1699888516588,
                "tmdate": 1699892541233,
                "mdate": 1699892541233,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1uOVMq3l5s",
                "forum": "RDSj6S8WJe",
                "replyto": "VvhGRYIc61",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6687/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6687/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "where $\\epsilon$ is the model misspecification error. Therefore, the term $\\epsilon_p$ represents the effect of model misspecification on regret, rather than implying that regret is directly proportional to $T$. Now, there are many works that **assume** there is no misspecification error, namely assuming that their modeling assumption **perfectly** reflects the environment. We could also do so and not deal with the misspecification error if we desired. Yet, we wanted our framework to be more inclusive when presenting the problem formulation, allowing for possible model misspecification. In essense, this is just a matter of problem setting by our choice. That is, if you wish, we can certainly consider problem setting **without model misspecification**. We strongly believe that this should not be any disadvantage on this part.\n\n[4] Jin, Chi, et al. \"Provably efficient reinforcement learning with linear function approximation.\" Conference on Learning Theory. PMLR, 2020.\n\n[5]  Zanette, Andrea, et al. \"Learning near optimal policies with low inherent bellman error.\" ICML, 2020.\n\n### 3. Comparison to the tabular case\n\nDynamics aggregation is a more **general** framework than the equivalence mapping in the tabular case proposed by Wen et al. (2020). The equivalence mapping does not project states into a latent space. Instead, it partitions the original MDPs into disjoint subMDPs and groups them based on identical or similar structures. Therefore, their mapping is defined as a bijection. However, our dynamics aggregation is a surjection: given the aggregated state, we cannot identify the exact states mapped to it. Since all bijective functions are surjective, our framework is more general.\n\nFurthermore, equivalence mapping necessitates that every aspect of the states be identical, even when certain detailed information of the states may not be crucial for discovering the optimal policy. This requirement can potentially constrain the range of scenarios where this framework can be effectively applied.\n\nOur framework, dynamics aggregation, **combines the benefits of both state aggregation and equivalence mapping**. Thus, it results in a considerably simplified representation compared to the other two frameworks. For more details, we provieded a comprehensive description of dynamics aggregation with an example in Appendix C."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6687/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699888663828,
                "cdate": 1699888663828,
                "tmdate": 1699892552219,
                "mdate": 1699892552219,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DxrIE5cUL0",
                "forum": "RDSj6S8WJe",
                "replyto": "VvhGRYIc61",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6687/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6687/Authors"
                ],
                "content": {
                    "title": {
                        "value": "To Reviewer hMDT"
                    },
                    "comment": {
                        "value": "Dear Reviewer hMDT,\n\nAs we approach the conclusion of the discussion period, we want to ensure that all your questions and comments have been comprehensively addressed. Should there be any outstanding concerns or points of clarification, please let us know. Otherwise, we sincerely and respectfully ask you to consider re-evaluating our work.\n\nWe would like to highlight the potential far-reaching impact of our findings on the fundamental limitations of linear MDPs. The first part of our paper, in particular, presents critical insights that we believe are imperative to share with the broader RL research community. This contribution alone, we feel, has significant implications for the field.\n\nMoreover, the second part of our work lays a crucial foundation for addressing RL with a hierarchical structure under function approximation. This represents not only the first formal attempt to provide a rigorous theoretical framework for hierarchical structure in conjunction with function approximation but also includes empirical validation of its effectiveness. Such empirical demonstration and theoretical guarantee, especially within RL theory literature, is relatively uncommon.\n\nImportantly, our approach achieves these results without relying on the impractical assumption of direct reachability to a large portion of the MDP, a necessity in conventional linear MDPs for state-space independent performances as shown in Section 4. We earnestly hope that the value and novelty of our work, as reflected in both theoretical, intuitive, and empirical facets, are recognized. Your support in disseminating these findings is crucial for fostering further research and dialogue within the community."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6687/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700565606645,
                "cdate": 1700565606645,
                "tmdate": 1700565606645,
                "mdate": 1700565606645,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "XKs0BWXKb9",
            "forum": "RDSj6S8WJe",
            "replyto": "RDSj6S8WJe",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6687/Reviewer_kJCM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6687/Reviewer_kJCM"
            ],
            "content": {
                "summary": {
                    "value": "Recent advancements in reinforcement learning (RL) have spotlighted function approximation to address the generalization challenges in tabular Markov Decision Processes (MDPs). Linear MDP, a cornerstone model, has demonstrated that regret bounds are influenced more by feature dimension rather than state space size. However, the authenticity of this claim is examined in this paper. Researchers found that for appropriate representation of the probability space, the feature dimension is inevitably influenced by the size of the state space. A discrepancy was observed in the relationship between the feature dimension and state space size, especially as the latter expands. It's concluded that linear MDPs might not inherently allow learning detached from state space size. To counter this, the paper presents a new hierarchical framework called dynamics aggregation. It encompasses both state aggregation and equivalence mapping, promoting efficiency and adaptability. A hierarchical reinforcement learning (HRL) algorithm is proposed within this structure, which is statistically efficient and offers a comprehensive regret bound. The algorithm is validated against existing methods, showcasing its superior performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper questions the widely accepted belief about linear MDPs, delivering a comprehensive critique of its fundamentals.\n  \n2. The new framework, which fuses state aggregation and equivalence mapping, holds promise for addressing the limitations of linear MDPs, making it a significant contribution to the field.\n\n3. The proposed HRL algorithm not only introduces an innovative approach to RL but is the first of its kind to provide proven guarantees in function approximation.\n\n4. The inclusion of numerical experiments fortifies the theoretical claims, showcasing the algorithm's efficacy against existing counterparts."
                },
                "weaknesses": {
                    "value": "1. While the new algorithm excels in controlled experiments, its scalability and performance in more complex, real-world scenarios are yet to be determined.\n\n2. While numerical experiments are conducted, this paper mentions several examples in section 4 but does not include experiments and analysis in these examples."
                },
                "questions": {
                    "value": "1.For the proof provided for Theorem 1: suppose that there exists a state-action pair$(s, a)$ for which the transition probabilities are non-zero for more than $U$ states. How would this affect the recursive logic applied in the derivation of $\\operatorname{rank}(\\mathbb{P}_h) \\geqslant\\lfloor S / U\\rfloor$? Would the derived relationship between $d$, the rank of $\\mathbb{P}_h$, and the relationship $\\lfloor S / U\\rfloor$ still hold? \n\n2. Feature Dimension vs. State Space Size: Given that the research reveals a deeper connection between feature dimension and state space size than previously thought. What is potential future work especially in contexts where the state space is vast?\n\n3. Hierarchical Structures in Real-world Scenarios: With the proposed dynamics aggregation framework depending heavily on the hierarchical structure of problems, how feasible is it to identify or establish such hierarchies in complex, real-world scenarios, where the state dynamics might be more intricate and less structured?\n\n4. This paper mentions several examples in section 4. How does this method work and does it perform well in these examples?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6687/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6687/Reviewer_kJCM",
                        "ICLR.cc/2024/Conference/Submission6687/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 6,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6687/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698738449945,
            "cdate": 1698738449945,
            "tmdate": 1700578982444,
            "mdate": 1700578982444,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "PixoG2ARBr",
                "forum": "RDSj6S8WJe",
                "replyto": "XKs0BWXKb9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6687/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6687/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We sincerely appreciate the time and effort you have invested in reviewing our paper. Thank you for overall positive evaluation of our paper. Below, we address each point raised in your review with the aim of clarifying and further enriching our research.\n\n### 1. Large-scale experiment\nWe appreciate your question as it allows us to clarify our main claims and what we aim to emphasize in Theorem 1: designing large-scale environments in Linear MDPs is **extremely challenging** due to expensive computational costs. As stated in Section 4, in most real-world scale problems, $U$ is minimal compared to $S$, leading to an enormously large $d \\approx S$ in Linear MDPs (by Collorary 1). A major challenge with all existing Linear MDP algorithms is the necessity to compute the inverse of the Gram matrix, incurring a computational cost of $O(d^2) \\approx O(S^2)$. This becomes  **intractable** when $S$ is very large. Similarly, in response to Q4, this is the reason we could not include experiments for examples $(b)$, $(c)$, and $(d)$ (Riverswim is an example of $(a)$ Gridworld). Please note that, this issue is a common challenge encountered in Linear MDP algorithms. Whereas, in the case of an MDP with a hierarchical structure, where small sub-structures repeat ($MN \\ll S$), our algorithm can be effectively implemented since $d_{\\psi}\\leq M \\ll S$. Note that our approach requires only a computational cost of  $O(d_{\\psi}^2)$.\n\n\n### 2. What happens if a state-action pair that can trasit to more than $U$ states exists?\n\nBefore addressing your question, we would like to clarify that according to the definition of $U$, a state-action pair $(s,a)$ cannot have non-zero transition probabilities across more than $U$ states. Nonetheless, if your query relates to a situation in which there exists a state-action pair $(s,a)$ has non-zero transition probabilities for almost all states (which means $U = \\Theta(S)$), we can provide an explanation. However, please note, as discussed in Section 4, that such a scenario is generally  unrealistic.\n\nThe existence of a state-action pair $(s,a)$ with non-zero transition probabilities for almost all states has **little impact** on the result of Theorem 1. Let's reconstruct the transition kernel by excluding the state $s$. And let $U'$ be the maximum number of directly reachable states of this new transition kernel. Now, we can then apply Theorem 1 to the new transition kernel. Consequently, we obtain a similar bound $d \\geq \\lfloor(S-1)/U' \\rfloor$. Hence, our main claim remains valid.\n\n### 3. Potential future work\nWe believe there are two possible directions for future work. The first involves utilizing hierarchical structures, as proposed in this paper. A future direction could be to develop more practical methods for leveraging these hierarchical structures.\n\nThe second direction is focused on minimizing or completely removing the dependence on $d$ in the regret bound. Although this may entail some concessions in terms of the $H$ or $T$ terms, achieving a reduction in $d$ could prove to be a significant advancement in our research.\n\n### 4. Hierarchical structures in real-world scenarios\nFirstly, we would like to highlight that the assumption of a known hierarchical structure is actually **common** in hierarchical RL (for example, Wen et al., 2020 [1], also assumed a known equivalence mapping). Furthermore, in real-life scenarios, humans often know the (approximate) mapping between two similar environments, even if they don't know the transitions. Consider the example of playing various versions of video games like Super Mario. Despite the differences in each game, the underlying gameplay mechanics remain consistent, and this is something we often know. Therefore, in common scenarios, particularly in human learning, knowledge of dynamics aggregation is prevalent. We advocate that the RL community should place greater emphasis on these hierarchical RL frameworks to develop algorithms that mimic human-level efficiency.\n\n\nOur assumption of known dynamics aggregation in the proposed framework is a **strategic starting point** rather than a limitation. It is logical to first demonstrate efficiency in scenarios with a *known* dynamics aggregation before feasibly extending our approach to cases involving *unknown* dynamics aggregation. This step-by-step progression ensures a solid foundation for future explorations as did numerous previous researches. In the field of Linear MDPs, the research initially focused on scenarios with known features. Only recently has there been a shift towards developing methods to learn these features. Similarly, in hierarchical RL, specifically within the *option* framework [2, 3], the assumption of *known* options has been a standard for an extended period. It is only relatively recently that the theory RL community has ventured into the problem of option learning. Yet, this area, especially in the context of learning options for regret guarantees, still remains"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6687/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699888181026,
                "cdate": 1699888181026,
                "tmdate": 1699888181026,
                "mdate": 1699888181026,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hPxcFGVab8",
                "forum": "RDSj6S8WJe",
                "replyto": "PixoG2ARBr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6687/Reviewer_kJCM"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6687/Reviewer_kJCM"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the detailed response and I will keep my positive score."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6687/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700578922740,
                "cdate": 1700578922740,
                "tmdate": 1700578922740,
                "mdate": 1700578922740,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]