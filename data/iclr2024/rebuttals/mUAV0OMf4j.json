[
    {
        "title": "Revisiting Subsampling and Mixup for WSI Classification: A Slot-Attention-Based Approach"
    },
    {
        "review": {
            "id": "0j9EIzLaxr",
            "forum": "mUAV0OMf4j",
            "replyto": "mUAV0OMf4j",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6751/Reviewer_Wtm8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6751/Reviewer_Wtm8"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes an original architecture for WSI classification which combine MIL (multi-instance learning) with multi-head attention, slot attention and pooling. The model can be seen as summarizing the WSI (split into M patches) into S slots where S is a fixed hyperparameter and is small (e.g. 16). Another model of similar architecture classifies these S slots into K classes. The M patches are first converted to codes using a ResNet-18 pre-trained on Imagenet. The paper also proposes a data augmentation scheme based on patch subsampling and MIXUP. Experiments show that the proposed architecture and data augmentation improves upon SOTA for 3 datasets (CAMELYON-16/17 and TCGA-NSCLC) for cancer/non-cancer or subtype classification. Ablation studies show the effect hyperparameter selection and combination of data augmentation."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The paper proposes a novel architecture for WSI classification that improves upon SOTA MIL. The architecure choices are well motivated and the results show a clear improvement. The idea of using a fixed number of attention-guided slots to summarize the important patches of a WSI prior to classification is original and powerful.\n* A typical drawback of MIL is that it tend to overtrain as the number of bags is generally small compared to the number of instances (a single WSI may generate several thousand patches). The proposed approach of subsampling and MIXUP augmentation appears to be effective at decreasing overtraining issues and improving classification measures. Again those approaches are relatively well motivated in the paper.\n* The choice of using ResNet-18 pre-trained on imagenet makes the approach simpler, faster and more reproducible."
                },
                "weaknesses": {
                    "value": "* The SOTA AUC for all 3 datasets, as reported on the current litterature and Grand-Challenge website is significantly higher than the baselines chosen in the paper. For TCGA-NSCLC, [Zhang22] reports 0.9377 AUC, while the top baseline in the paper is 0.893. For the CAMELYON datasets, the Grand Challenge leaderboard also outperforms the reported baselines. Furthermore, Transmil [2] reports 0.9309 AUC for CAMELYON-16, while the paper reports it at 0.834 ! And for TCGA-NSCLC, the discrepency is: 0.893 vs 0.9603 ! Those are significant differences, that make the proposed approach not SOTA anymore. [update: this issue has been cleared]\n[1] Zhang, Jingwei, et al. \"Gigapixel whole-slide images classification using locally supervised learning.\" MICCAI 2022.\n[2] Shao, Zhuchen, et al. \"Transmil: Transformer based correlated multiple instance learning for whole slide image classification.\" NIPS 2021.\n\n* Different subsampling rates are applied to the 3 datasets, based on information that is not generally known a-priori (the percentage of positive patches). This unfairly inflates the reported performance.\n\n* It is not clear how the optimal hyperparameters are obtained. At least the subsampling rate seems to be obtained heuristically (see bullet above), so it makes me suspicous about the others too. [update: this issue has been addressed by the authors in the rebuttal]\n\n* Since, one of the claim the authors make repeatedly is that their approach is more computationally efficient, it would be good to include some numbers and compare them to other approaches. [update: this issue has been addressed by the authors in the rebuttal]\n\nOverall, this is a technically solid paper presenting a novel approach to WSI classification using MIL sampling and augmentation. Unfortunately, despite its claims, it doesn't reach SOTA on the reported datasets."
                },
                "questions": {
                    "value": "see weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6751/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6751/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6751/Reviewer_Wtm8"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6751/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698098881209,
            "cdate": 1698098881209,
            "tmdate": 1700517642006,
            "mdate": 1700517642006,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gpPK3LrlkW",
                "forum": "mUAV0OMf4j",
                "replyto": "0j9EIzLaxr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6751/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6751/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer Wtm8 [1/2]"
                    },
                    "comment": {
                        "value": "- Weakness 1\n\nThank you for providing a detailed comparison. \n\nWe would like to highlight that **our method achieves an AUC of 0.975 for CAMELYON-16 and an AUC of 0.981 for TCGA-NSCLC, showcasing better-calibrated predictions with improved Negative Log-Likelihood (NLL) values.** These results are obtained using features extracted from SimCLR, provided by the DSMIL paper[4].\n\nAs previously outlined in our general response, the overall performance disparities observed in various papers can be attributed to the challenges posed by deficient and unbalanced datasets. The significance of a standardized evaluation protocol becomes crucial in such scenarios. Notably, TRANSMIL stands out as the most parameterized model in the baseline, making it particularly vulnerable to issues like overfitting and changes in the validation set. In the case of CAMELYON-16, TRANSMIL[3] exhibits varying AUC values of 0.877[1], 0.906[2], and 0.931[3]. \n\n**For further details on our efforts to ensure fair and reproducible comparisons, we invite you to refer to our general response.**\n\n- Weakness 2\n\nIn terms of the subsampling rate (p), the empirical evidence from Table 6 and Table 8 in the Appendix shows that **adopting any subsampling rate results in a performance gain compared to not adopting it.** This implies that the **benefits of subsampling can be harnessed without needing a prior** understanding of the ratio of positive patches. While we reported the optimal subsampling rate(p), it's worth noting that improvement from the baseline model can still be achieved without precisely tuning the subsampling rate(p).\n\n- Weakness 3\n\nThe main hyperparameters in our method comprise the number of slots (S), mixup beta distribution alpha (\u03b1), subsampling rate (p), and Late-mix (L), as elaborated in Section 4.3 of the paper. We fix the optimal hyperparameters based on the results of k-fold AUC. AUC is recognized as a more robust metric than ACC, as it is not influenced by threshold variations.\n\nFor Slot-MIL, where S is the sole hyperparameter, we observe minimal performance differences when the number of slots exceeds a certain threshold. Especially in TCGA-NSCLC, there is no discernible trend when varying the number of slots. It's important to highlight that our approach is based on learnable (implicit) clustering using attention scores, making **meticulous hyperparameter search unnecessary**, unlike non-learnable k-means clustering. Additional experiment results are provided in the table below.\n\n| # of Slots\\Dataset |             |   CAMELYON_16   |             |             |    TCGA-NSCLC   |             |\n|:------------------:|:-----------:|:---------------:|:-----------:|:-----------:|:---------------:|:-----------:|\n|                    |    ACC(\u2b61)   |      AUC(\u2b61)     |    NLL(\u2b63)   |    ACC(\u2b61)   |      AUC(\u2b61)     |    NLL(\u2b63)   |\n|        S = 4       | 0.841\u00b10.016 |   0.874\u00b10.026   | 1.000\u00b10.444 | 0.842\u00b10.022 |   0.906\u00b10.022   | 0.901\u00b10.480 |\n|        S = 8       | 0.846\u00b10.013 |   0.892\u00b10.024   | 1.221\u00b10.445 | 0.843\u00b10.021 |   0.910\u00b10.018   | 0.964\u00b10.337 |\n|       S = 16       | 0.834\u00b10.047 | **0.893**\u00b10.023 | 1.242\u00b10.979 | 0.852\u00b10.025 | **0.914**\u00b10.016 | 1.001\u00b10.202 |\n|       S = 32       | 0.852\u00b10.014 |   0.891\u00b10.031   | 1.438\u00b10.536 | 0.847\u00b10.032 |   0.905\u00b10.031   | 1.106\u00b10.409 |\n\nIn the case of Slot-MIL + SubMix, we **employ grid search to select hyperparameters**, with \u03b1 \u2208 {0.2, 0.5, 1.0}, p \u2208 {0.1, 0.2, 0.4}, and L \u2208 {0.1, 0.2, 0.3}, while maintaining the same number of slots as in Slot-MIL. Similar to subsampling rate (p) and the number of slots (S), the results show minimal changes with variations in \u03b1 and L. It's important to note not to use an excessively large \u03b1 (beyond 1.0) or to avoid using Late-mix. Detailed results can be found in Table 9 in our paper."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6751/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700061392386,
                "cdate": 1700061392386,
                "tmdate": 1700491126491,
                "mdate": 1700491126491,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "svsKQhGNki",
                "forum": "mUAV0OMf4j",
                "replyto": "JZgbOZ7Ncx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6751/Reviewer_Wtm8"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6751/Reviewer_Wtm8"
                ],
                "content": {
                    "title": {
                        "value": "On-point clarifications and additional results significantly improve the paper's contribution"
                    },
                    "comment": {
                        "value": "I appreciate the author's extensive response to the reviewers critiques. In my opinion, most points raised by the reviewers have been addressed constructively. In particular the misunderstanding related to SOTA numbers has been cleared. I will update my review accordingly."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6751/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700517402932,
                "cdate": 1700517402932,
                "tmdate": 1700517402932,
                "mdate": 1700517402932,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "R5D3QnMNAd",
            "forum": "mUAV0OMf4j",
            "replyto": "mUAV0OMf4j",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6751/Reviewer_vwP9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6751/Reviewer_vwP9"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose an efficient model called SlotMIL that leverages an attention-based mechanism to organize patches into a fixed number of slots. They demonstrate that combining the attention-based aggregation model with subsampling and mixup augmentation techniques enhances both generalization and calibration in WSI classification. \n\nA key contribution is subsampling/ mixup augmentation, which creates new bags of patches by randomly sampling subsets from the original slides. This helps restrict overfitting to the weak slide-level supervision. They also introduce an efficient model called SlotMIL that summarizes patches into a fixed number of slots using attention. Experiments show subsampling helps make more informative slots and improves generalization."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The subsampling augmentation is a simple but effective way to create new training bags that reduces overfitting, without altering underlying slide semantics or adding training cost. This is an improvement over complex augmentation techniques.\n- The SlotMIL model provides an efficient attention-based aggregation method to summarize patches into discriminative slots. This is more sophisticated than relying only on max pooling approaches commonly used.\n- The authors showed that subsampling plus mixup augmentation can work well together, whereas prior work found mixup had limited applicability in MIL frameworks. \n- Thorough experiments on multiple datasets demonstrate state-of-the-art performance, including on class imbalance and distribution shifts.\n- Authors considered various relevant baselines (ABMIL, DSMIL) and conducted rigorous ablation experiments to test the various components of the proposed model. \n- The paper is well written and easily to follow."
                },
                "weaknesses": {
                    "value": "1. MIL attention (https://arxiv.org/pdf/1802.04712.pdf) has been widely used for WSI analysis and several other extensions to the method have been explored in the field. While it is a relevant baseline, the proposed improvements do not significantly improve upon performance (<2-5%) and it is unclear if Mixup augmentation alone is better than other extensions to improve performance in the proposed binary classification task. \n2. The paper focuses solely on binary classification problems. Extending the approach to multi-class classification could be challenging. \n3. In some pathology samples (e.g., in cancer), very small proportion of patches might contain the relevant signal for classification. The subsampling augmentation could potentially discard useful patch information. Strategies that retain all patches may be able to learn more robust features."
                },
                "questions": {
                    "value": "- What impact does the subsampling rate have on model performance? Is there an optimal sampling fraction or range across datasets?\n- How well does the SlotMIL model and subsampling augmentation transfer to multi-class classification tasks? \nThe codebase is not public - making it opensource would help the reproducibility efforts."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6751/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698811148472,
            "cdate": 1698811148472,
            "tmdate": 1699636777182,
            "mdate": 1699636777182,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "thfUiT4O2H",
                "forum": "mUAV0OMf4j",
                "replyto": "R5D3QnMNAd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6751/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6751/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer vwP9 [1/2]"
                    },
                    "comment": {
                        "value": "* Weakness 1\n\nThanks for your comprehensive comparison. \n\nWe want to highlight that our enhancements amount to approximately 9.4%, 6.5%, and 3.1% when measured against the best baselines in terms of AUC for CAMELYON-16, CAMELYON-17, and TCGA-NSCLC, respectively. While this improvement might seem modest, it's crucial to note that we have concurrently improved calibration while enhancing both ACC and AUC, making this margin noteworthy. \n\nAs detailed in Table 3, we acknowledge that Slot-Mix doesn't yield a substantial improvement in terms of ACC and AUC. However, its contribution to calibration is significant, especially as it involves mixing inter-label slides using informative slots. Notably, our mixup approach stands out for its simplicity and efficiency, especially when compared to other augmentations such as DTFD-MIL[1] and Rank-Mix[2], which necessitate knowledge distillation based on grad-cam[3] or a pre-training phase to unify the number of patches between slides. It's also important to highlight that DFTD-MIL and Rank-Mix naturally pose similarities to subsampling, involving the division of WSIs into pseudo-bags and selecting a subset of patches. The comparison with these baselines becomes more fair in the context of Slot-MIL + SubMix. In response to reviewer Wtm8, we also incorporate additional metrics about the complexity at reply for Wtm8.\n\n* Weakness 3, Question 1\n\nThe ratio of positive patches in a positive slide varies across datasets, as it is known to be less than 10% in CAMELYON-16 and over 80% in TCGA-NSCLC[4]. The precise number of patches containing relevant signals for classification is naturally unknown. While subsampling may potentially discard some important patches in certain iterations, the stochasticity introduced by subsampling and the abundance of patches within a slide mitigate the likelihood of discarding all useful patches. This can be further adjusted by the subsampling rate (p).\n\n**Retaining all patches, as depicted in our \"Base train\" in Figure 3, exacerbates overfitting** since attention scores become excessively concentrated on specific patches. Additional details are provided in Section 4.1.2. Regarding the subsampling rate (p), empirical evidence in Table 6 and Table 8 in the appendix demonstrates that **adopting any subsampling rate leads to performance gain compared to not adopting it.** This implies that the **benefits of subsampling can be harnessed without requiring a prior** understanding of the ratio of positive patches.\n\nWe already validate the effectiveness of subsampling in CAMELYON-16, where only a small proportion of patches may contain relevant signals for classification. Also, at inference stage, we utilize whole patches in a slide, not a subset patches of a slide. The optimal range of subsampling rates might be higher in scenarios where the ratio of positive patches is low, as low subsampling rates may result in the exclusion of any positive patches (p=0.1 indicates the use of 10% of total patches per iteration)\n\n* Weakness 2, Question 2\n\nThe adoption of subsampling for multi-class scenarios is anticipated to pose no significant obstacle, as subsampling is an orthogonal augmentation with the number of classes, akin to dropout rates[5] and masking ratios in MAE[6]. While determining an optimal subsampling ratio (p) might require further elaboration in multi-class scenarios, our observations suggest that the performance is not overly sensitive to p, making this task more manageable. \n\nIt is worth noting that Slot-Mix originated from manifold mixup[7], where multi-class scenarios are the basic premise. Consequently, the adoption of Slot-MIL + SubMix in multi-class situations is not only feasible but also aligns with the method's conceptual foundations. \n\nDemonstrating the validity of our method on multi-class datasets, such as PANDA, where WSIs are categorized into 6 classes based on Gleason score, would indeed be valuable. However, given the large scale of PANDA with tens of thousands of WSIs, feature extraction might require a substantial amount of time over the rebuttal period. If you are aware of any open-source repositories providing extracted features from PANDA, we are keen to conduct experiments as time permits.\nRegarding the code, it has already been posted in the supplementary materials for Slot-MIL. We are in the process of making it open-source and endeavor to release it anonymously within the coming week including SubMix.\n\n***\nIf you have any further questions or need additional clarification, please feel free to reach out.\n\nSincerely, Authors"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6751/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699971976036,
                "cdate": 1699971976036,
                "tmdate": 1700491007068,
                "mdate": 1700491007068,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3Uh90lmfMx",
                "forum": "mUAV0OMf4j",
                "replyto": "AQAdSq6E4U",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6751/Reviewer_vwP9"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6751/Reviewer_vwP9"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for addressing my comments. The authors acknowledge that Slot-Mix doesn't yield a substantial improvement in ACC and AUC but argue for its significant contribution to calibration. However, the quantifiable evidence of this contribution is not well presented. The new results also point to the fact that SSL is more promising in certain experiments than MIL-based methods, somewhat undermining the relevance of the presented approach. Hence, I will keep the current score."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6751/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700604021933,
                "cdate": 1700604021933,
                "tmdate": 1700604021933,
                "mdate": 1700604021933,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "4xq0OU5b38",
            "forum": "mUAV0OMf4j",
            "replyto": "mUAV0OMf4j",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6751/Reviewer_5YV1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6751/Reviewer_5YV1"
            ],
            "content": {
                "summary": {
                    "value": "The papers proposes Slot-MIL, which incorporates the ideas related to inducing points and slot attention for simplifying pooling mechanisms in multiple instance learning for WSIs. Specifically, given two WSIs (bag of instance features), the model should be able to encode them into the identical number of slots for classification. This work assesses performance on CAMELYON and TCGA-NSCLC subtyping, with additional ablation studies performed related to impact of subsampling and number of slots."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- This work performs a comprehensive assessment into different subsampling approaches for WSIs, and how subsampling strategies perform as data augmentation methods for MIL. In addition to performing substantive ablation experiments for the main augmentation method, SubMix (different parameters and combinations of subsampling and Slot-Mixup), this work also thoroughly assesses ablation strategies in combination with other strategies such as RankMix. Additional figures and presentations in the supplement also convey the stability  of how subsampling affects training and validation loss."
                },
                "weaknesses": {
                    "value": "- Though adapting new techniques such as slot attention, I found this method to have limited novelty as it addresses common concerns such as patch redundancy in MIL. Many works such as DeepAttnMISL (Yao et al. 2020) achieve similar goals as Slot-MIL in filtering the bag to a smaller set of patches. Overall, relative to the performance improvement demonstrated, the contributions presented by the method may still be too limited and lack extensive validation with diverse downstream tasks.\n- One of the outlined contributions of this work (#3) is that Slot-MIL reaches state-of-the-art performance on CAMELYON and TCGA-NSCLC. Slot-MIL outperforms baselines relative to the comparisons developed in this work. However, when compared across studies, the reported best performance for C16 on the test set underperforms other reported results by a large margin. For example, on C16, whereas the accuracy / AUC for Slot-MIL+SubMix is 0.890 / 0.921, the reported best performances for ILRA-MIL (FRC) in Xiang et al. 2023 is 0.922 / 0.965. In other works such as MHIM-MIL (DSMIL) by Tang et al. 2023, the reported best performances is 0.925 / 0.965 (evaluated using cross-validation, not on official C16 test), and Bayes-MIL-APCRF by Cui et al. 2023 have best performances of 0.900 / 0.948. Though not using the same splits for TCGA-NSCLC, the AUCs for this task using 10-fold CV is generally 0.930+ (0.977 in Xiang et al. 2023).\n- Benchmarks such as C16 and TCGA-NSCLC lack difficulty and can be easily solved without adapting techniques related to WSI augmentation and slot attention. It would be interesting to explore this method on more diverse tasks that would benefit from data augmentation and \"sparsity\", such as gene mutation prediction (such as MSI prediction in TCGA-COADREAD), survival analysis, and other challenging tasks such as Gleason score grading in PANDA. The tasks evaluated in this work are limited to diagnostically-simple binary classification problems that do not need sparse MIL or virtual augmentation methods to see clinical translation.\n- - Additionally, tasks such as C16, TCGA-NSCLC, TCGA-RCC have been over-explored in computational pathology and should no longer be evaluated as the only tasks evaluated for MIL in the reviewer's opinion. C16 already has many state-of-the art performances and is nearly solved from both fully-supervised and weakly-supervised perspectives. Similarly, TCGA-NSCLC and TCGA-RCC can be generally solved without requiring sophisticated MIL approaches. Overall, it would be more interesting to demonstrate how this method would enable more challenging tasks to be solved in computational pathology.\n\n1. Xiang, J. and Zhang, J., 2022, September. Exploring low-rank property in multiple instance learning for whole slide image classification. In The Eleventh International Conference on Learning Representations.\n2. Yufei, C., Liu, Z., Liu, X., Liu, X., Wang, C., Kuo, T.W., Xue, C.J. and Chan, A.B., 2022, September. Bayes-MIL: A New Probabilistic Perspective on Attention-based Multiple Instance Learning for Whole Slide Images. In The Eleventh International Conference on Learning Representations.\n3. Tang, W., Huang, S., Zhang, X., Zhou, F., Zhang, Y. and Liu, B., 2023. Multiple Instance Learning Framework with Masked Hard Instance Mining for Whole Slide Image Classification. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 4078-4087).\n4. Yao, J., Zhu, X., Jonnagaddala, J., Hawkins, N. and Huang, J., 2020. Whole slide images based cancer survival prediction using attention guided deep multiple instance learning networks. Medical Image Analysis, 65, p.101789."
                },
                "questions": {
                    "value": "See above comments."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6751/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6751/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6751/Reviewer_5YV1"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6751/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698827697072,
            "cdate": 1698827697072,
            "tmdate": 1699641511957,
            "mdate": 1699641511957,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rBhbzEKsIA",
                "forum": "mUAV0OMf4j",
                "replyto": "4xq0OU5b38",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6751/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6751/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer 5YV1 [1/2]"
                    },
                    "comment": {
                        "value": "* Weakness 1\n\nThanks for your constructive review. \n\nHowever, we want to emphasize that our method is a novel approach and contributes a lot to MIL problems, while the underlying ideas may exist in other domains. You mentioned DeepAttnMISL, but we consider DeepAttnMISL to be notably different from ours for two main reasons: clustering mechanism and pooling strategy. \n\nFirstly, in the clustering phase, while seemingly achieving similar goals, DeepAttnMISL utilizes non-learnable k-means clustering, which cannot be trained with backpropagation or end-to-end. However, our approach employs a learnable O(n) complexity attention-based (implicit) clustering mechanism, which has already been explored in many different deep learning tasks including sets [7],  object-centric learning [8], etc. Also, our method is much simpler to implement, not needing a manual action to capture clusters. This crucial distinction allows our method to adapt more dynamically to the underlying data structure, enhancing its efficacy.\n\nMoreover, in the pooling phase, DeepAttnMISL relies on local attention, whereas our model integrates an O(n) complexity global attention pooling strategy. This strategic choice contributes to the model's ability to capture dependencies between clusters, which may be particularly valuable in tasks involving complex relationships. Again, our method is simple, computationally efficient, robust to distribution shifts, and able to capture underlying structures well, leading to state-of-the-art performance.\n\nIt's worth noting that non-learnable k-means clustering has its limitations, as some previous studies have indicated; its adaptation results in a marginal improvement of performance [1], and its application is confined solely to intra-label mixup[2]. Also, k-mean clustering shows optimal performance only in the limited range of the number of clusters (k) (as shown in Table 3 of [3], and Fig.A.1 of [2]), while our Slot-MIL shows the consistent performance when the number of slots (S) exceed a certain threshold.\n\nFurthermore, our novel attention clustering, with its linear time complexity, extends its utility to inter-label mixup, thereby addressing a broader spectrum of scenarios. To the best of our knowledge, the mixup between WSIs using a fixed number of attention-based clusters remains unexplored by other researchers. \n\n* Weakness 2\n\nThanks for your insightful comparison. \n\nWe forgot to mention the results with SimCLR features in the main paper, which was already detailed in Appendix B.6. With comparable contrastive-learning-based features, **our model achieves a notable performance of 0.923/0.975 (ACC/AUC), demonstrating superior results compared to ILRA-MIL and other papers discussed in the context of C16.** We'd like to underscore that the performance is significantly influenced by the type of pre-trained features and evaluation protocol considering the scarcity and imbalance in the overall train/validation/test sets. **We encourage you to refer to our general response at the top where we elaborate on our dedicated efforts to ensure fair and reproducible comparisons.**\n\n* Weakness 3,4\n\nWe appreciate your comments, and we find that applying our method to gene mutation prediction or multi-class Gleason grading in PANDA is very intriguing. **Recognizing the potential limitations of C16 and TCGA-NSCLC for evaluating sophisticated MIL approaches, we intentionally included the CAMELYON-17 (C17) experiment in Table 4.** \n\nC17, being a multi-center dataset with a substantial distribution shift, offers a more realistic reflection of real-world scenarios compared to other datasets. In the case of C17, both Slot-MIL and Slot-MIL + SubMix outperform the baselines by a large margin with well-calibrated predictions. It's worth noting that C17 has been recognized as challenging, even with pre-training[5] or surgical fine-tuning[6] has shown limited promise. Notably, our method addresses distribution shifts in C17 without the need for pre-training or fine-tuning. \n\nBayes-MIL[4] also reveals its performance on C17 in their appendix Table 5. However, they didn't specify whether they partitioned the multi-center data to make distribution shifts which is common practice on C17. It's noteworthy that our method achieves superior performance on C17 without relying on a slide-dependent Regularizer (SPDR) or incorporating spatial information (APCRF), both of which are essential components in Bayes-MIL.\n\n---\n\nYour understanding and consideration are greatly appreciated. By reflecting your feedback, we are planning to reorganize the updated version of the paper to highlight the results of C17, emphasizing the strong performance gap. If you have any further questions or require additional clarification, please don't hesitate to reach out. \n\nBest regards, Authors"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6751/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699970955562,
                "cdate": 1699970955562,
                "tmdate": 1700490933495,
                "mdate": 1700490933495,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "s1zIPFdx9T",
                "forum": "mUAV0OMf4j",
                "replyto": "4xq0OU5b38",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6751/Reviewer_5YV1"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6751/Reviewer_5YV1"
                ],
                "content": {
                    "comment": {
                        "value": "The reviewer thanks the authors for their thoughtful response. After reading the authors\u2019 rebuttal, thoughts of other reviewers, and inspecting the methodological details further (of this work and its preceding works), Presented below is a point-by-point reply:\n\n**Reply to Weakness 1 - Novelty of Slot-MIL:** Slot-MIL is indeed distinct from DeepAttnMISL (not requiring clustering beforehand) for addressing patch redundancy. At the same time, as the methodology of Slot-MIL is more of a direct extension of the already-existing Slot Attention paper, it would be nice to visualize and characterize which slots are associated with clinically relevant pathological features to further justify adapting this method from other literature. Any number of methods can be intuitively adapted from Transformer literature and broader works in deep learning literature for addressing redundancy (e.g. - I imagine Clustering Transformers would also work well here [1]). As the performance of Slot-MIL is ultimately modest, there does not seem to be a strong motivation to use Slot-MIL when ABMIL, DeepAttnMISL, TransMIL, and IRLA-MIL (combined with self-supervised features) already work very well. In addition to visualization of slots, other ways to demonstrate the applicability of Slot-MIL is with additional experimentation regarding: 1) solving harder tasks and 2) data-efficiency experiments. The improvement with SubMix (as raised by vwP9 and its reply) is only in NLL, which does not translate into any improvement of clinically relevant performance metrics.\n\n**Reply to Weakness 2 \u2013 On \u201cSOTA\u201d performance:** The reviewer understands that experimental designs (splits, parameters, implementation) differ across studies, especially when comparing MIL results with different pretrained encoders for feature extraction. The intention of my critique was to lessen the tone, as the previous (and also current versions) of this work continue to emphasize \u201cSOTA\u201d performance.\n\n**Reply to Weakness 3 \u2013 Diverse Benchmarks:** The reviewer understands that C17 is a good benchmark for studying distributional shift. At the same time, the reviewer contends that the experiments in this study are too limited to binary classification tasks. As utilized in many works, PANDA presents a more challenging task with 1) multi-class classification with label uncertainty, and 2) domain shift. The suggestion of PANDA (and other tasks) is two-fold in improving the study:\n1.\tFinding tasks where Slot-MIL would have better improvement: As the results stand, there is limited performance gain for Slot-MIL (and its combination with SubMix) (see below).\n2.\tTask diversity: Even with C17, the only tasks evaluated are lung cancer subtyping and breast cancer metastasis detection. These tasks are diagnostically simple and over-represented in computational pathology studies, relative to much harder tasks such as gene mutation prediction and even Gleason score grading.\n\n1. Vyas, A., Katharopoulos, A. and Fleuret, F., 2020. Fast transformers with clustered attention. Advances in Neural Information Processing Systems, 33, pp.21665-21674."
                    },
                    "title": {
                        "value": "Response to authors [1/2]"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6751/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700550236907,
                "cdate": 1700550236907,
                "tmdate": 1700551415699,
                "mdate": 1700551415699,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LBOS2yoK7w",
                "forum": "mUAV0OMf4j",
                "replyto": "4xq0OU5b38",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6751/Reviewer_5YV1"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6751/Reviewer_5YV1"
                ],
                "content": {
                    "title": {
                        "value": "Better MIL vs. Better SSL Encoder / Calibration Metric / Comparisons with CLAM-SB [2/2]"
                    },
                    "comment": {
                        "value": "Below are additional questions and concerns following the rebuttal:\n\n**Do we need better MIL if improving SSL encoder demonstrates the most performance gains?**: The reviewer thanks the authors for updating their results (Table 5) with self-supervised features using SimCLR. However, these results suggest a different finding \u2013 that developing more principled MIL architectures or augmentation strategies is not as significant as using a better SSL encoder. In CAMELYON-16 results in Table 5, ABMIL reaches the same AUC as base Slot-MIL (with smaller standard deviation) and with lower NLL (better supposed calibration). Slot-MIL is matched in AUC by DTFD-MIL. Potentially, ABMIL with subsampling can do better than Slot-MIL with SubMix (baselines presented in Table 5 do not seem to have subsampling). In TCGA-NSCLC results in Table 5, all MIL methods reach the same AUC as Slot-MIL (with or without Submix). The reviewer also acknowledges the author\u2019s rebuttal in that \u201cnot all tasks possess an optimal feature extractor\u201d. However, it is becoming increasingly prevalent with histopathology domain-specific encoders such as CTransPath [1] that SSL is the mainstay approach for extracting features in MIL. As suggested, evaluating on more difficult tasks may highlight better performance gains with Slot-MIL. However, the current results significantly diminishes Slot-MIL\u2019s contributions.\n\n**Confusion with calibration**: NLL is used as the main approach for measuring calibration. Upon  review, the reviewer is unsure why NLL is used, when metrics such as Brier score, conformal prediction, and Expected Calibration Error (ECE) are some of the main methods for evaluating calibration [2,3].\n\n**Missing comparison with CLAM-SB**: Upon review, the results of this work do not compare against CLAM-SB [4,5,6], which has been demonstrated to be a strong baseline (commonly evaluated in other MIL works on these tasks). As the evaluated tasks can be solved at the instance-level, comparing against CLAM-SB is necessary.\n\n**Concluding Thoughts**\nThe reviewer thanks the authors for the detailed reply. From reviewing the submission again, I did find the experimentation and investigation of subsampling as an augmentation strategy to be interesting and thoughtful. This is a worthwhile investigation, and I appreciate both positive and negative results that would make salient how subsampling works. However, the evaluation on NSCLC subtyping and CAMELYON-16, coupled with the updated results via SSL, may not be challenging enough to demonstrate the novelty of Slot-MIL and SubMix. The evaluation of SubMix itself is not convincing, as established calibration metrics are not used. The performance gains are extremely reduced following using SSL features, which emphasizes further interesting points on: 1) trade-off between MIL architecture and patch-level encoder, 2) what new clinical problems can MIL architectures solve that can't be solved with a strong patch-level encoder? At the moment, I am not changing my rating, but would encourage the authors to continue working on this problem as I believe it would have important results if evaluated on more diverse tasks.\n\n1. Wang, X., Yang, S., Zhang, J., Wang, M., Zhang, J., Yang, W., Huang, J. and Han, X., 2022. Transformer-based unsupervised contrastive learning for histopathological image classification. Medical image analysis, 81, p.102559.\n2. Naeini, M.P., Cooper, G. and Hauskrecht, M., 2015, February. Obtaining well calibrated probabilities using bayesian binning. In Proceedings of the AAAI conference on artificial intelligence (Vol. 29, No. 1).\n3. Minderer, M., Djolonga, J., Romijnders, R., Hubis, F., Zhai, X., Houlsby, N., Tran, D. and Lucic, M., 2021. Revisiting the calibration of modern neural networks. Advances in Neural Information Processing Systems, 34, pp.15682-15694.\n4. Lu, M.Y., Williamson, D.F., Chen, T.Y., Chen, R.J., Barbieri, M. and Mahmood, F., 2021. Data-efficient and weakly supervised computational pathology on whole-slide images. Nature biomedical engineering, 5(6), pp.555-570.\n5. Xiang, J. and Zhang, J., 2022, September. Exploring low-rank property in multiple instance learning for whole slide image classification. In The Eleventh International Conference on Learning Representations.\n6. Yufei, C., Liu, Z., Liu, X., Liu, X., Wang, C., Kuo, T.W., Xue, C.J. and Chan, A.B., 2022, September. Bayes-MIL: A New Probabilistic Perspective on Attention-based Multiple Instance Learning for Whole Slide Images. In The Eleventh International Conference on Learning Representations."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6751/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700550498584,
                "cdate": 1700550498584,
                "tmdate": 1700551555168,
                "mdate": 1700551555168,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5rGg1U6TWl",
                "forum": "mUAV0OMf4j",
                "replyto": "niDBiko9Y7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6751/Reviewer_5YV1"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6751/Reviewer_5YV1"
                ],
                "content": {
                    "title": {
                        "value": "Further response to authors"
                    },
                    "comment": {
                        "value": "To quickly follow-up, are the baselines presented in Table 5 trained with subsampling?"
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6751/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700700647778,
                "cdate": 1700700647778,
                "tmdate": 1700700647778,
                "mdate": 1700700647778,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "yf2R0kZ4hX",
                "forum": "mUAV0OMf4j",
                "replyto": "4xq0OU5b38",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6751/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6751/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Further response to reviewer 5YV1"
                    },
                    "comment": {
                        "value": "The baselines listed above the double line in Table 5 do not incorporate augmentation (and also subsampling), and the baselines below the double line use augmentation. To be clear,\n* DTFD-MIL employs a pseudo-bag with knowledge distillation based on the ABMIL model but does not exhibit significant improvement. Here, a pseudo-bag splits a single WSI into subset patches, which can be seen as a variant of subsampling.\n* RankMix, built on our Slot-MIL along with subsampling and patch-level mixup, displays inferior performance compared to SubMix, underscoring the importance of mixing at the attention-based clustered feature level. The rationale behind conducting RankMix on Slot-MIL lies in Slot-MIL being identified as the most powerful model. RankMix authors emphasized the significance of baseline performance in achieving optimal results.\n* Also, as demonstrated in **Table 6 (Appendix B.1), even when subsampling is applied to ABMIL and DSMIL, it does not align closely with the performance achieved by SubMix,** demonstrating a substantial margin.\n* We also conducted experiments with other baselines (TRANSMIL, ILRA) incorporating subsampling and found that they do not closely match SubMix's performance by a considerable margin. Additionally, applying subsampling to TRANSMIL is impractical due to its limited applicability, to be strict. The model relies on the ordering of full patches for the utilization of an additional positional encoding CNN module.\n\nWe can provide a further analysis within a day if needed.\n\nIn summary, our experiments indicate that even when subsampling is applied to the baseline, leading to performance improvement through the regularization of attention over-concentration on specific patches, it does not closely align with the performance achieved by SubMix."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6751/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700703336312,
                "cdate": 1700703336312,
                "tmdate": 1700706899951,
                "mdate": 1700706899951,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]