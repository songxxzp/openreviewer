[
    {
        "title": "Davidsonian Scene Graph: Improving Reliability in Fine-grained Evaluation for Text-Image Generation"
    },
    {
        "review": {
            "id": "WjY1aSyAxu",
            "forum": "ITq4ZRUT4a",
            "replyto": "ITq4ZRUT4a",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8654/Reviewer_p5Xv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8654/Reviewer_p5Xv"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the problem of fine-grained evaluation of text-to-image (T2I) alignment. Following a line of recent works, this paper formulates T2I alignment evaluation as visual question answering (VQA), which involves question generation from the text prompt using LLMs and question answering using VQA models (referred to as the QG/A framework).\n\nThe main contributions of this paper are:\n* It identifies the issues of existing QG/A methods regarding the question generation step and proposes four properties that the generated questions should satisfy.\n* It proposes a QG method that constructs the questions of a given text prompt as a Davidsonian Scene Graph (DSG). DSG considers the dependency between questions and is designed to satisfy the four properties.\n* It collects a test set of 1060 prompts, which covers different challenges, semantic categories and writing styles, together with Likert-scale T2I alignment rating by humans.\n* The experiments demonstrate that DSG, when combined with different VQA models, achieves a higher correlation with human evaluation compared with existing QG/A frameworks.\n* It further reveals two challenges of QG/A frameworks for T2I evaluation: (1) Some question categories (e.g., shape, style and text rendering) are beyond the capability of current SOTA VQA models to evaluate. (2) For questions that involve \u201csubjectivity\u201d and \u201cdomain knowledge\u201d, agreement is hard to achieve even between humans."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* This work reveals the issues of existing QG/A methods in terms of the QG step, which is well-motivated and the proposed four desired properties of QG are reasonable.\n* The experiments demonstrate that the proposed DSG achieves solid improvement over existing QG/A frameworks in terms of correlation with human evaluation, which advances the reliability of automatic T2I alignment evaluation.\n* This work is transparent about its limitations, shedding light on directions for future studies to work on.\n* The paper is well-written and easy to follow."
                },
                "weaknesses": {
                    "value": "### The design of DSG and DSG-1k\n* It is not well-explained which part of DSG is designed to address the **Atomic Question** and **Full Semantic Coverage** properties. Why the QG methods of TIFA and VQ^A cannot achieve these two properties?\n* Some categories are under-represented in the DSG-1k dataset. According to Table 9, six categories have less than 30 examples and VQA-Human correlation is not computed for these categories.\n\n### Evaluation\n* There is no ablation study showing which design choice in DSG contributes to each of the four properties.\n* It is unclear whether DSG correlates better with humans, compared with TIFA and VQ^A, in each fine-grained category.\n* The relationship between the claimed desired question properties and the final VQA-Human correlation is not well demonstrated. In other words, the relationship between Precision/Recall/Atomicity/Uniqueness in Table 2 and Spearman/Kendall correlation in Table 3 is unclear.\n* There is no comparison with TIFA and VQ^2A in terms of precision and recall in Table 2."
                },
                "questions": {
                    "value": "* It is stated on page 4 that DSG only involve binary yes/no questions. Why are there multi-choice questions in the human annotation UI in Figure 10?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8654/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8654/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8654/Reviewer_p5Xv"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8654/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697943426909,
            "cdate": 1697943426909,
            "tmdate": 1699637084204,
            "mdate": 1699637084204,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qaIan8vOZK",
                "forum": "ITq4ZRUT4a",
                "replyto": "WjY1aSyAxu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8654/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8654/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response to reviewer p5Xv (Part 1)"
                    },
                    "comment": {
                        "value": "We thank reviewer p5Xv for the valuable comments on the design and evaluation of DSG / DSG-1k. We address/clarify all the points and report the revision we applied to our submission.\n\n> **W1. It is not well-explained which part of DSG is designed to address the Atomic Question and Full Semantic Coverage properties. Why the QG methods of TIFA and VQ^A cannot achieve these two properties?**\n\nThanks for the great suggestion \u2014 we made a revision to explain the point more clearly (Sec. 3, the paragraph above the subsection on DSG-1k dataset).\n\nIn gist, to the question \u201cwhy the QG methods of TIFA and VQ^2 cannot achieve atomicity and semantic coverage?\u201d, the short answer is: because they do not have a structured intermediary representation in the question generation process (like DSG\u2019s semantic tuples) to a) define the semantic coverage for a prompt, and b) thereby allowing for the verification of atomicity.\n\nTo expand the answer in detail, DSG uses semantic tuples (e.g., attribute(bike, blue), entity(bike), relation(bike, wall, next to)) as an intermediary representation in question generation: prompt -> semantic tuples -> questions. As such, leveraging the structuredness of the semantic tuple intermediary representation, we can then clearly define \"atomicity\" and \"semantic coverage\" and thereby modulate DSG's behavior to generate questions accordingly:\n\n- Atomicity: an atomic question corresponds to only a single semantic tuple. For example, \"is there a bike?\" maps to one semantic tuple \"entity(bike)\", and is atomic; \"is there a blue bike?\" maps to two semantic tuples \"entity(bike)\" and \"attribute(bike, blue)\", and is thus NOT atomic.\n- Semantic coverage: the full semantics of a prompt is represented by a set of unique semantic tuples (no duplicates), such that no entities, attributes, relations, or global descriptions can be added. \n\nTIFA and VQ^2, on the other hand, generate questions directly, thus there is no means available to encourage atomicity and semantic coverage (i.e., with a direct prompt-to-questions configuration w/o semantic tuples intermediary). In our human evaluation of TIFA and VQ^2 questions (Table 2), we indeed find large numbers of non-atomic questions and violations of semantic coverage (with duplication and missing entities/attributes/relations).\n\n \n> **W2. Some categories are under-represented in the DSG-1k dataset. According to Table 9, six categories have less than 30 examples and VQA-Human correlation is not computed for these categories.**\n\nWe apologize for the confusion introduced here. Please allow us to clarify (also added clarification in the caption the table referred to \u2013 Table 13 (Table 9 in the original PDF)):\n\nSpecifically, Table 13 of the revised PDF (Table 9 in the original PDF) examines questions generated from **TIFA160 prompts** (160 in total), where we used DSG-1k\u2019s \u2018question types\u2019 to present specific scores,  rather than from the whole DSG-1k prompts. TIFA160 is a relatively small sample, so they do not reflect some types of questions such as text rendering. In Table 14 of the revised PDF (Table 10 in the original PDF), we illustrated three T2I models\u2019 VQA accuracy in the full range of DSG-1k\u2019s semantic categories.\n\n\n\n> **W3. There is no ablation study showing which design choice in DSG contributes to each of the four properties.**\n\nAs a similar point has also been raised by others, to avoid duplication, we reply in the common response. Please refer to the common response. Thanks.\n\n> **W4. It is unclear whether DSG correlates better with humans, compared with TIFA and VQ^A, in each fine-grained category.**\n\nWe agree that a by-category human correlation study is a valuable addition. Following the suggestion, we conducted an additional experiment using TIFA160 prompts, by calculating correlation Likert 1-5 scores with each of their 4 data splits:\n- COCO: general purpose;\n- PaintSkills: focusing on spatial relations;\n- PartiPrompts: focusing on attributional failure cases (e.g., color, counting, etc.);\n- DrawBench: focusing on simple compositionality.\n\nAs shown in the following table, DSG produces higher human correlation scores on 3 out of 4 splits, as well as the best overall performance. Even in the only data split it does not come on top (i.e., DrawBench), the DSG\u2019s correlation is very close to the top-performing TIFA\u2019s. We incorporated the new experiment result into Appendix D.2 in our revised PDF, thanks for the suggestion!\n\n|        | COCO     | PaintSkills | PartiPrompts | DrawBench | Overall  |\n|---|---|---|---|---|---|\n| TIFA   | 0.30/0.22| 0.56/0.40  | 0.49/0.37    | **0.59/0.46** | 0.43/0.32|\n| VQ^2A  | 0.17/0.13| 0.25/0.17  | 0.28/0.21    | 0.45/0.37 | 0.21/0.16|\n| DSG    | **0.53/0.42** | **0.64/0.53**  | **0.61/0.49**    | 0.56/0.44 | **0.57/0.46** |\n\n(continued below)"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8654/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700410154954,
                "cdate": 1700410154954,
                "tmdate": 1700410223213,
                "mdate": 1700410223213,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9I6mQ8z5Jw",
                "forum": "ITq4ZRUT4a",
                "replyto": "WjY1aSyAxu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8654/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8654/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response to reviewer p5Xv (Part 2)"
                    },
                    "comment": {
                        "value": "> **W5. The relationship between the claimed desired question properties and the final VQA-Human correlation is not well demonstrated. In other words, the relationship between Precision/Recall/Atomicity/Uniqueness in Table 2 and Spearman/Kendall correlation in Table 3 is unclear.**\n\nFor context, we would like to clarify that the Precision/Recall/Atomicity/Uniqueness validation (Table 2) is intended to check the quality of the generated questions on the extent to which they match human judgment, i.e., \u201care these the right validation questions to ask to check T2I alignment?\u201d, and importantly, achieving strong human correlation in the answers while maintaining the high quality in question generation.\n\nA natural doubt that we expect may arise is: without having the atomicity, semantic coverage, etc. quality we use to evaluate generated questions, TIFA and VQ^2 can still achieve decent human correlation, then what\u2019s the point in having these quality metrics?\n\nWe believe the answer lies with **interpretability** and **reliable diagnostics**, especially on the instance/question-level \u2014 for any individual question, we strive to achieve an unambiguous understanding of human judgments. For example, a response to a non-atomic question \u201cIs there a blue bike parked next to a door?\u201d would be hard to interpret \u2014 if the answer to this question is \u2018no\u2019, then is it that the \u2018bike\u2019 is not generated properly? or is it that the \u2018location of the bike\u2019 is wrong?\n\nTherefore, to summarize:\n- With the Precision/Recall/Atomicity/Uniqueness validation, we show that DSG produces high-quality questions per human intuition. This ensures interpretability and reliable diagnostics;\n- With the human correlation experiments, we show that DSG results at the same time correlate well (and better than previous work) with human judgments.\n\n\n> **W6. There is no comparison with TIFA and VQ^2A in terms of precision and recall in Table 2.**\n\nWe agree it is possible to evaluate TIFA and VQ^2 by eliciting human judgments on precision and recall, however in practice, there exists a failure mode that renders the evaluation not implementable (indicated by their low atomicity in Table 2) \u2014 the issue is rooted in how TIFA/VQ^2 and DSG are designed \u2013 we clarify as follows:\n\nNote that, due to the absence of the concept of atomicity, we frequently observe TIFA and VQ^2 questions that cover two or more semantic details (e.g. \u201cis there a blue bike next to the door?\u201d covers the entity \u201cbike\u201d, the color of it, and its relative position to another entity \u201cdoor\u201d). As we argue in the Sec. 1 introduction (the A-B-C-D bullet points on reliability), \u201cloaded questions\u201d like these do not lend themselves well to acquiring unambiguous answers (either by a VQA model or a human rater). \n\nIf we disregard the reliability concern to look at precision/recall alone, loaded questions can score high precision/recall, by simply adding a question mark to the original prompt to form a question (e.g., \u201cA blue motorcycle parked by paint chipped doors.\u201d $\\rightarrow$ \u201cA blue motorcycle parked by paint chipped doors?\u201d). This would render the precision/recall validation uninformative.\n\nOur DSG, on the other hand, uses semantic tuples as a structured intermediary semantic representation to handle atomicity \u2013 this wards it off the \u201cloaded question\u201d failure mode mentioned above in the precision/recall validation (e.g., if a \u201cloaded question\u201d that maps to more than 1 semantic tuple is produced, it is considered a wrong question).\n\n> **Q1. It is stated on page 4 that DSG only involves binary yes/no questions. Why are there multi-choice questions in the human annotation UI in Figure 10?**\n\nIt is true that DSG only involves binary yes/no questions. The questions inside Figure 10 are generated by TIFA, where multiple-choice questions are permitted. We use the same human annotation UI for questions generated by DSG and VQ2A, where we provide yes/no answer choices."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8654/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700410203378,
                "cdate": 1700410203378,
                "tmdate": 1700410203378,
                "mdate": 1700410203378,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3VY6qXpNTR",
                "forum": "ITq4ZRUT4a",
                "replyto": "9I6mQ8z5Jw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8654/Reviewer_p5Xv"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8654/Reviewer_p5Xv"
                ],
                "content": {
                    "title": {
                        "value": "Response to Author Rebuttal"
                    },
                    "comment": {
                        "value": "Thank you for the detailed reply. Most of my concerns have been addressed. I still have some further suggestions and comments:\n- It seems that the current paper lacks information on DSG-1k's data distribution over the categories (whole, spatial, color, count, etc). It is important to show that each category contains a sufficient number of examples to ensure statistically meaningful evaluation.\n- About W3: I'm afraid that there is a misunderstanding in your response. What I expect is ablating components in DSG (e.g., DSG w/o dep) and observing the impact on the four properties (i.e., Precision, Recall, Atomicity and Uniqueness in Table 2), instead of (+atomicity) and (+atomicity, +uniqueness), etc.\n- About W6: I agree that it is meaningless to compare precision/recall in the absence of atomicity. It would be better to explain this in the paper."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8654/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700662209440,
                "cdate": 1700662209440,
                "tmdate": 1700662209440,
                "mdate": 1700662209440,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Mq0dMoHaN7",
                "forum": "ITq4ZRUT4a",
                "replyto": "fWS74jJtoY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8654/Reviewer_p5Xv"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8654/Reviewer_p5Xv"
                ],
                "content": {
                    "title": {
                        "value": "Response to Author Rebuttal"
                    },
                    "comment": {
                        "value": "Thanks for the response. I still have a further question related to W1 and W3: TIFA also generates intermediary representation (extracting elements) before generating the questions. As such, I'm interested in understanding whether the better performance of DSG over TIFA stems from (1) the utilization of semantic tuples as a more effective representation than individual elements, (2) the incorporation of better in-context examples, or any other factors?\n\nI will consider raising the rating based on the response to this question."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8654/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700720953520,
                "cdate": 1700720953520,
                "tmdate": 1700720953520,
                "mdate": 1700720953520,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8QynHZrZrN",
                "forum": "ITq4ZRUT4a",
                "replyto": "WjY1aSyAxu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8654/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8654/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to the additional questions"
                    },
                    "comment": {
                        "value": "Thanks for your additional comments!\n\n> I'm interested in understanding whether the better performance of DSG over TIFA stems from (1) the utilization of semantic tuples as a more effective representation than individual elements, (2) the incorporation of better in-context examples, or any other factors?\n\nWe would like to point out that the most major difference between DSG and TIFA is that DSG has a dependency structure across each entity/attribute/relation, while the extracted elements in TIFA does not.  \n\nConsider the caption \"A blue motorcycle\". There are two questions, \"is there a motorcycle?\" and \"is the motorcycle blue?\". If there is no motorcycle in the image, then it is just unreasonable to ask \"is the motorcycle blue?\". The design of DSG makes sure that, if the entity \"motorcycle\" is not there, then the attribute \"blue\" is considered missing. In TIFA, there is no such guarantee, and we observe that the VQA models sometimes just hallucinate answers even if the question is not reasonable, which leads to bias in evaluation.\n\nAs such, the dependency structure of DSG is critical, and it is where DSG is making a difference compared with TIFA and VQ2. Besides, the improvement on precision, recall, atomicity, uniqueness are side benefits we observed, and the points you listed are possible explanations for these side benefits.\n\nWe hope our response has addressed your concerns. As it is the end of the discussion period, we do not have time to write more. It would be great if you increase the rating. Thanks!"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8654/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700725154633,
                "cdate": 1700725154633,
                "tmdate": 1700729198517,
                "mdate": 1700729198517,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "goqirvrQUN",
            "forum": "ITq4ZRUT4a",
            "replyto": "ITq4ZRUT4a",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8654/Reviewer_A5bo"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8654/Reviewer_A5bo"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on text-to-image generation evaluation by asking text-related questions and checking whether a VQA model can answer it given the generated image. It suggests previous QG/A frameworks usually ask ambiguous, duplicated, and invalid questions. The paper parses the input text input atomic entity/attribute/relation tuples, translates each tuple into questions, and obtains their dependencies through an LLM. The experimental results show the generated questions are unique, have valid entailment, and query atomic semantics. The paper lastly constructs a dataset with the proposed methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper has strong motivation. It first analyses the drawbacks of previous QG/A methods, then proposes some principles for the generated questions, and lastly introduces a three-step prompting method to resolve it.\n\nThe experimental results are strong enough to support the effectiveness of the proposed method in the proposed uniqueness, valid dependency, and human alignment.\n\nThe paper is well-written, and the related work is sufficient."
                },
                "weaknesses": {
                    "value": "The VQA model is not good enough and hinders the final alignment to humans in T2I evaluation."
                },
                "questions": {
                    "value": "Can a better VQA model lead to better alignment in Tables 3, 4, and 6?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8654/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698577069983,
            "cdate": 1698577069983,
            "tmdate": 1699637084058,
            "mdate": 1699637084058,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "L4f4mRC9CQ",
                "forum": "ITq4ZRUT4a",
                "replyto": "goqirvrQUN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8654/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8654/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response to reviewer A5bo"
                    },
                    "comment": {
                        "value": "We thank reviewer A5bo for the feedback and comments.\n\n> **W1. The VQA model is not good enough and hinders the final alignment to humans in T2I evaluation.**\n\nAs a similar point has also been raised by others, to avoid duplication, we reply in the common response. Please refer to the common response. Thanks.\n\n> **Q1. Can a better VQA model lead to better alignment in Tables 3, 4, and 6?**\n\nYes. This can be found in the results we already report in the paper. Please allow us to clarify:\n\nEmpirically, based on the result in Table 5 (per-question VQA-Human matching accuracy), we see a common trend in Tables 3, 4, and 6 \u2014 if a VQA model\u2019s result correlates better with human Likert scores, the model also tends to have a higher matching accuracy on the per-question level."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8654/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700409918970,
                "cdate": 1700409918970,
                "tmdate": 1700409918970,
                "mdate": 1700409918970,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "rXP9hxnXeK",
            "forum": "ITq4ZRUT4a",
            "replyto": "ITq4ZRUT4a",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8654/Reviewer_tDEJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8654/Reviewer_tDEJ"
            ],
            "content": {
                "summary": {
                    "value": "The paper addresses the task of evaluating text-to-image models, specifically focusing on the question generation and answering method. This method automatically generate questions and answers from the prompt, and the faithfulness of the image is assessed based on the consistency of the answers from both prompt and visual question answering models. The authors identify and tackle key reliability challenges in this approach, including the quality of generated questions and the consistency of visual question answering.\nTo overcome these challenges, the authors introduce the Davidsonian Scene Graph (DSG), which produces atomic and unique questions organized in dependency graphs, to ensure questions cover semantic of the prompt and that answers are consistent.\nThe paper provides experimental results and human evaluations, demonstrating that DSG addresses the reliability challenges mentioned earlier. Additionally, the authors introduce DSG-1k, an open-sourced evaluation benchmark with 1,060 prompts covering a wide range of fine-grained semantic categories."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper introduces Davidsonian Scene Graph (DSG) to improve the faithfulness of the text-to-image evaluation. Compared to previous QG/A methods, this framework generates  atomic questions with full semantic coverage and valid question dependency. The authors implement QG step as a Directed Acyclic Graph (DAG) where the nodes represent the unique questions and they explicitly model semantic dependencies with directed edges. Additionally, they collect a fine-grained human-annotated benchmark called DSG-1k including 1,060 diverse prompts with a balanced distribution to facilitate research in this area."
                },
                "weaknesses": {
                    "value": "1.\tThis paper improves the existing QG/A methods with Davidsonian Scene graph, which is generated based on LLMs. The approach of the work could be enriched with more details and techniques. \n\n2.\tAblation on separate steps of DSG should be presented. For example, without establishing dependencies, measure the changes of consistency between VQA score and the human 1-5 Likert Scores.\n\n3.\tApart from TIFA and VQ2A, more methods could be compared, including CLIPScore and caption based approaches."
                },
                "questions": {
                    "value": "1.\tIn 4.1, to validate the question dependencies, authors evaluate manually on 30 samples and automatically on the full TIFA160. However, the consistency of manual and automatic evaluation is not presented.\n\n2.\tThe comparison of runtime among different methods should be added. \n\n3.\tIn table 3, for Instruct-BLIP, the Spearman\u2019s \u03c1 of DSG is lower than that of TIFA, authors should explain this phenomenon briefly."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8654/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698638236334,
            "cdate": 1698638236334,
            "tmdate": 1699637083907,
            "mdate": 1699637083907,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "TZkAtBB8bz",
                "forum": "ITq4ZRUT4a",
                "replyto": "rXP9hxnXeK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8654/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8654/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response to reviewer tDEJ (Part 1)"
                    },
                    "comment": {
                        "value": "We thank reviewer tDEJ for the insightful and detailed comments. We address/clarify all the weaknesses they bring to our attention and report the revision we applied to our submission.\n\n> **W1. This paper improves the existing QG/A methods with Davidsonian Scene graph, which is generated based on LLMs. The approach of the work could be enriched with more details and techniques.**\n\nAs a similar point has also been raised by others, to avoid duplication, we reply in the common response. Please refer to the common response. Thanks.\n\n> **W2. Ablation on separate steps of DSG should be presented. For example, without establishing dependencies, measure the changes of consistency between VQA score and the human 1-5 Likert Scores.**\n\nAs a similar point has also been raised by others, to avoid duplication, we reply in the common response. Please refer to the common response. Thanks.\n\n> **W3. Apart from TIFA and VQ2A, more methods could be compared, including CLIPScore and caption based approaches.**\n\nPlease note that we mentioned \u201cFor human correlation, Hu et al. (2023) for example achieves 47.2 Kendall\u2019s \u03c4 in a sizable correlation analysis with human 1-5 Likert judgments, with CLIPScore \u03c4 = 23.1.\u2019\u2019,\nFor completeness, we also calculated and included correlation results on the 5 single-summary metrics considered in Hu et al. (2023), i.e., BLEU-4, ROUGE-L, METEOR, SPICE, (with BLIP-2 Captioning) and CLIPScore (with CLIP ViT-B/32) in Appendix E in the revised PDF.\n\n\n|  | Spearman's \u03c1 | Kendall's \u03c4 |\n|---|---|---|\n| **Captioning  (w/ BLIP-2)** | \n| BLEU-4 | 26.1 | 18.3 |\n| ROUGE-L | 34.2 | 23.9 |\n| METEOR | 37.9 | 26.9 |\n| SPICE | 32.9 | 23.6 |\n| **Cosine similarity (CLIP ViT-B/32)** | \n| CLIPScore | 27.6 | 19.1 |\n| **QG/A frameworks  (w/ PaLI)** | \n| VQ$^2$A | 20.7 | 15.7 |\n| TIFA | 43.1 | 32.3 |\n| DSG |  **57.0** | **45.8** |\n\n(NOTE: To ensure consistency, we did not directly report the result from Hu et al. (2023) Table 2. Our numbers are calculated with the same human 1-5 Likerts **we** collected \u2013 the same Likerts are used in calculating the correlation results in our Table 3. Further, also note that Hu et al. (2023) calculated human Likerts based (by rules, see their Appendix C.1) on raters\u2019 answers to the GPT-3 generated questions, whereas we collected them independently with a single 1-5 consistency question, please see our Figure 12.)\n\nFurther, we set focus on fine-grained QG/A as an overall more informative / diagnostic framework than single-summary metrics (represented by CLIPScore), and intend to show that, not only does QG/A work well on the coarse-grained model-level in evaluating performance (as TIFA/VQ^2 show in their comparison to a range of single-summary methods), we can improve it further (with DSG) to provide reliable fine-grained diagnostics.\n\n(continued below)"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8654/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700409748504,
                "cdate": 1700409748504,
                "tmdate": 1700409748504,
                "mdate": 1700409748504,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZS1IArnmyt",
                "forum": "ITq4ZRUT4a",
                "replyto": "rXP9hxnXeK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8654/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8654/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response to reviewer tDEJ (Part 2)"
                    },
                    "comment": {
                        "value": "> **Q1. In 4.1, to validate the question dependencies, authors evaluate manually on 30 samples and automatically on the full TIFA160. However, the consistency of manual and automatic evaluation is not presented.**\n\nFollowing the suggestion, we additionally report the the consistency of 30 samples of manual v.s automatic evaluation \u2014 dependency valid ratio of DSG is 100% in manual evaluation (30 samples), and 99% in automatic evaluation (full TIFA160). In the revised PDF, we added a sentence to present the result more clearly.\n\n> **Q2. The comparison of runtime among different methods should be added.**\n\nWe compare the runtime of TIFA and our DSG. For apple-to-apple comparison, we use the same LLM (gpt-3.5-turbo-16k) and VQA model (mPLUG-large). On a single A6000 GPU, we measure the time for obtaining the final T2I alignment score for 20 image-text pairs. We use 20 prompts sampled from TIFA-160 prompts and images generated by Stable Diffusion v2.1.\n\nAs in the following result, we find DSG takes a shorter time than TIFA, primarily because TIFA tends to generate more questions than DSG, increasing the inference time of VQA models.\n\n* TIFA: 481s for 20 image-text pairs (24s per image-text pair)\n* DSG: 332s for 20 image-text pairs (16s per image-text pair)\n\nPlease note that the cost for many image-text pairs could be saved in many ways: e.g., batch processing (separate QG and QA stages, processing multiple images), caching (re-using image features when a VQA model answers to questions paired with the same image), using smaller models (through quantization, distillation, compression), etc.\n\n> **Q3. In Table 3, for Instruct-BLIP, the Spearman\u2019s \u03c1 of DSG is lower than that of TIFA, authors should explain this phenomenon briefly.**\n\nWhile Spearman's rho shows \"Instruct-BLIP & TIFA\" > \"Instruct-BLIP & DSG\", Kendall's Tau shows \"Instruct-BLIP & TIFA\" < \"Instruct-BLIP & DSG\". There are some cases with tied ranks, where Kendall\u2019s Tau correlation is more reliable than Spearman\u2019s rho correlation when handling data with tied ranks [1,2].\n\n[1] Taylor, Wilson L. \u201cCorrecting the Average Rank Correlation Coefficient for Ties in Rankings.\u201d Journal of the American Statistical Association, vol. 59, no. 307, 1964, pp. 872\u201376. JSTOR, https://doi.org/10.2307/2283105. Accessed 19 Nov. 2023.\n\n[2] Marie-Therese Puth, Markus Neuh\u00e4user, Graeme D. Ruxton, \u201cEffective use of Spearman's and Kendall's correlation coefficients for association between two measured traits\u201d, Animal Behaviour, Volume 102, 2015, Pages 77-84, ISSN 0003-3472, https://doi.org/10.1016/j.anbehav.2015.01.010."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8654/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700409806061,
                "cdate": 1700409806061,
                "tmdate": 1700409806061,
                "mdate": 1700409806061,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]