[
    {
        "title": "SLiMe: Segment Like Me"
    },
    {
        "review": {
            "id": "deTlRptpNU",
            "forum": "7FeIRqCedv",
            "replyto": "7FeIRqCedv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3070/Reviewer_16rP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3070/Reviewer_16rP"
            ],
            "content": {
                "summary": {
                    "value": "This paper examines the problem of class-specific semantic part segmentation, under a one/few-shot data setting.\n\nThe method proposed by the paper is to leverage recent advances and findings in diffusion models, specifically in stable diffusion the cross-attention modules capturing relevant spatial regions and being used for semantic correspondence.\n\nLearning is done by fine-tuning the text embedding from a stable diffusion model on an image and a corresponding part segmentation mask.  The embedding is optimized under a loss with three components, encouraging the cross-attention map, and introduced weighted accumulated self-attention map, to match the ground-truth segmentation, while not straying too far from the original stable diffusion loss.\n\nExperimental validation is done on car, horse, and face part segmentation, with comparable to favorable results when compared against several recent state-of-the-art baselines."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- a nice framework for tackling the problem of one/few-shot semantic part segmentation, having clear benefits over existing proposed approaches in terms of amount of additional supervised annotations required for training, while maintaining similar performance\n\n- in general, paper details are clearly presented, including a thorough supplementary appendix"
                },
                "weaknesses": {
                    "value": "- one of the stated contributions is the introduced weighted accumulated self-attention map.  This seems important enough that the incremental contribution of this component to the overall performance should perhaps be added to the main text rather than deferred to the appendix.  \n\n- further, within the appendix, Table 9 shows an improvement from adding WAS-attention, from 62.7 on average to 68.3.  I'm a little confused, then, on how this differs from Table 5, as I would have though the last row, setting $\\alpha=0$, would also correspond to dropping WAS-attention, and here the average performance is 68.0\n\n- lastly, on initial read, I was unsure of how text/text prompt was being used within the proposed method.  This was clarified by the first ablation study in A.2, but perhaps a sentence mention of this in the main text would also be helpful."
                },
                "questions": {
                    "value": "see weaknesses above, in particular the issue raised in the second bullet"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3070/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698778490862,
            "cdate": 1698778490862,
            "tmdate": 1699636252647,
            "mdate": 1699636252647,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ZI6qItZhGy",
                "forum": "7FeIRqCedv",
                "replyto": "deTlRptpNU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3070/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3070/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Clarification on Tables 5 and 9**\n\nWe apologize for the confusion. We clarify that WAS-attention can be employed either solely during inference or during both optimization and inference. When used solely during inference, it has no impact on the optimization process. \n\nThe reference made by the reviewer to Table 5 refers to the scenario where WAS-attention is applied exclusively during inference. On the other hand, Table 9 presents results when WAS-attention is entirely excluded from both optimization and inference steps in the car parts segmentation task. Notably, this exclusion leads to a notable decrease in IoU, dropping from 68.3% to 62.7%.\n\nWe ran another experiment with completely removing WAS-attention from both optimization and inference steps on the horse parts segmentation task, where we also see a significant IoU drop (63.3% to 52.4%). \n\nThis table provides a clear breakdown of the values for each body part, comparing the results without and with the Weighted Average Score (WAS).\n\n| Part      | Without WAS   | With WAS      |\n|----------------|---------------|---------------|\n| Background     | 68.2% \u00b1 5.0   | 79.6% \u00b1 2.5   |\n| Head           | 47.8% \u00b1 2.8   | 63.8% \u00b1 0.7   |\n| Legs           | 50.1% \u00b1 0.6   | 59.5% \u00b1 2.1   |\n| Neck+Torso     | 57.3% \u00b1 13.1  | 68.1% \u00b1 4.4   |\n| Tail           | 38.6% \u00b1 1.4   | 45.4% \u00b1 2.4   |\n| Average        | 52.4% \u00b1 3.8   | 63.3% \u00b1 2.4   |\n\n==================\n\n**Clarification on prompt/tokens**\n\nPlease refer to our answer to Reviewer kAQy"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3070/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700271973099,
                "cdate": 1700271973099,
                "tmdate": 1700271973099,
                "mdate": 1700271973099,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VENrZvmLjo",
                "forum": "7FeIRqCedv",
                "replyto": "ZI6qItZhGy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3070/Reviewer_16rP"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3070/Reviewer_16rP"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the helpful replies, both to myself and the other reviewers, this clarifies my confusion and addresses my concerns."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3070/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700578846792,
                "cdate": 1700578846792,
                "tmdate": 1700578846792,
                "mdate": 1700578846792,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "bMp00kG2EX",
            "forum": "7FeIRqCedv",
            "replyto": "7FeIRqCedv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3070/Reviewer_VRf7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3070/Reviewer_VRf7"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes a one-shot segmentation approach using the stable diffusion model. They pose the problem as one-shot optimization to perform object segmentation at different granularity levels conditioned on a single segmentation map. They take advantage of the self / cross-attention layer in the diffusion model to optimize the text embeddings for the given semantic segmentation task. They evaluate the proposed method on two public datasets and show its outperformance against the recent related work."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The proposed idea of optimizing the text embedding based on the attention maps for semantic segmentation is interesting and novel.\n\n2. The proposed method is shown to be advantageous in two datasets both quantitatively and qualitatively.\n\n3. The paper is well-written and easy to follow. The theoretical background is explained well.\n\n4. The limitations are discussed.\n\n5. The method is well-ablated for the different components."
                },
                "weaknesses": {
                    "value": "1. The proposed method seems to be adapted for the segmentation task based on the work by Hedlin et al for unsupervised semantic correspondence.\n\n2. Related works which are not cited:\n[a] Burgert, Ryan, et al. \"Peekaboo: Text to image diffusion models are zero-shot segmentors.\" arXiv preprint arXiv:2211.13224 (2022).\n[b] Tian, Junjiao, et al. \"Diffuse, Attend, and Segment: Unsupervised Zero-Shot Segmentation using Stable Diffusion.\" arXiv preprint arXiv:2308.12469 (2023).\n\n3. The number of works that are compared against is limited.\n\nMinor:\n1. Making the best result in the ablation study tables bold would improve the readability."
                },
                "questions": {
                    "value": "1. Is there any specific reason behind not including the SegDDPM results in Tab. 1?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Other reasons (please specify below)"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "The paper is available on arXiv in ICLR template (mentioning \"Under review as a conference paper at ICLR 2024\"). I am not sure whether it is acceptable or not: https://arxiv.org/pdf/2309.03179.pdf"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3070/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699188853474,
            "cdate": 1699188853474,
            "tmdate": 1699636252577,
            "mdate": 1699636252577,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "VpDw1CF6ya",
                "forum": "7FeIRqCedv",
                "replyto": "bMp00kG2EX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3070/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3070/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Hedlin et al.\u2019s work**\n\nWe acknowledge that the high level idea of our work might have similarities to Hedlin et al.\u2019s work, but their objectives are completely different from ours: we perform image segmentation while their task is image correspondence. Furthermore, in terms of architectural similarities, the optimization of cross-attention maps has been previously explored for various downstream tasks, as demonstrated in the work by Hedlin et al. However, our current task differs, and adopting their approach for our segmentation task did not yield effective results. Hedlin et al.\u2019s exploration is confined to the optimization of a single text embedding, while our approach allows optimizing multiple text embeddings, enabling the simultaneous segmentation of various parts or objects. \n\nWe attribute the significantly enhanced performance of our method to the utilization of WAS-attention which we propose in this work, as demonstrated in Table 9 and also the following results on horse part segmentation which we ran to address reviewer Fjuo\u2019s comment. \n\n| Part      | Without WAS   | With WAS      |\n|----------------|---------------|---------------|\n| Background     | 68.2% \u00b1 5.0   | 79.6% \u00b1 2.5   |\n| Head           | 47.8% \u00b1 2.8   | 63.8% \u00b1 0.7   |\n| Legs           | 50.1% \u00b1 0.6   | 59.5% \u00b1 2.1   |\n| Neck+Torso     | 57.3% \u00b1 13.1  | 68.1% \u00b1 4.4   |\n| Tail           | 38.6% \u00b1 1.4   | 45.4% \u00b1 2.4   |\n| Average        | 52.4% \u00b1 3.8   | 63.3% \u00b1 2.4   |\n\n===================\n\n**Related works [a] and [b]**\n\nThank you for bringing this to our attention. We've noticed that neither of the two works are peer-reviewed. While we appreciate their contributions, it's essential to highlight the distinctions that make them not directly comparable to SLiMe. In summary, Peekaboo requires an object description for each segment, a requirement absent in SLiMe. Additionally, Peekaboo lacks the capability to perform segmentation at the object part level, which is a fundamental feature of SLiMe.\n\nRegarding the second work by Junjiao et al., as we understand it, the method primarily segments images into different parts based on feature similarity, akin to the approach in the Segment Anything paper. This method lacks control over segmentation granularity and cannot perform targeted object/part segmentation. For example, it cannot be directed to segment a specific part of an object. These distinctions emphasize the unique capabilities and advantages of SLiMe in comparison.\n\n\n=================\n\n**The number of methods that are compared against are limited**\n\nWe have conducted thorough comparisons with three _most_ relevant papers to SLiMe. Notably, these papers also provide comparisons with other existing methods, offering readers valuable insights into our performance relative to those cited works (although we didn't include them in our study as their setups differed from ours). In response to a reviewer's suggestion (please refer to our responses to reviewer Fjuo for details), we further conducted experiments on two additional datasets, providing readers with a broader context for evaluating the performance of our work. We are open to suggestions on specific papers that you believe warrant comparison, and we genuinely welcome any recommendations for meaningful comparisons to be incorporated into the final version of the paper. Thank you\n\n=================\n\n**Not including SegDDPM in Table 1**\n\nWe conducted a comparative analysis of SLiMe and SegDDPM on two segmentation tasks: _face_ and _horse_ part segmentation (Tables 2 and 3). It's worth noting that the SegDDPM paper did not report results for the _car_ part segmentation task (Table 1). Our attempts to run their method on this task revealed a challenge: for SegDDPM to segment cars into their parts, it requires a DDPM trained to generate car images. Unfortunately, the SegDDPM authors have not provided a DDPM checkpoint for car image generation, preventing us from assessing SegDDPM's performance in car part segmentation."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3070/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700271968408,
                "cdate": 1700271968408,
                "tmdate": 1700338338961,
                "mdate": 1700338338961,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wRXFaaxcya",
                "forum": "7FeIRqCedv",
                "replyto": "VpDw1CF6ya",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3070/Reviewer_VRf7"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3070/Reviewer_VRf7"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response. It clarified most of my questions and concerns."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3070/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700738049726,
                "cdate": 1700738049726,
                "tmdate": 1700738049726,
                "mdate": 1700738049726,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "doB5ePyED8",
            "forum": "7FeIRqCedv",
            "replyto": "7FeIRqCedv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3070/Reviewer_Fjuo"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3070/Reviewer_Fjuo"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes to retarget the Stable Diffusion model (SD) for few-shot semantic segmentation. Instead of taking the in-context learning or data generation approaches, this paper proposes a new pipeline which optimizes the text embedding in SD on the input image to \"find\" the text embedding to correspond to the segmented region. In addition to the cross-attention map of the optimized text token, it proposes a new self-attention map fusion module to regress the ground truth mask with a higher resolution. The proposed method achieves SOTA result on the benchmarks used in one previous work. The ablation studies shows the effectiveness of the model design."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- A novel and interesting idea. The idea of retargeting the feature representation in a pretrained generative model for few-shot semantic segmentation is not new. But it's novel to exploit the text branch in the text-based image generation model (i.e. Stable Diffusion). Instead of training the adaptor model to map between the generative features to the semantic masks which may overfit on input image features, the proposed method aims to find the text embedding which may be more generalizable.\n\n- The proposed method significantly outperforms SOTA result on the benchmarks used in the ReGAN paper. The ablation studies shows the effectiveness of the model design.\n\n- The paper provides enough details in the appendix for reproduction and the open-sourced code is straightforward to follow."
                },
                "weaknesses": {
                    "value": "- Unconvincing importance of WAS attention: Figure 1 shows a good intuition that we need WAS attention to refine the object boundary. However, In Table 5, the contribution of the WAS attention doesn't seem to be significant. For the without WAS attention results (fig. 2a, 2c), will they be much improved by using GrabCut or other methods for segmentation refinement post-processing?\n\n- Lack of comparison on benchmark datasets like ADE-Bedroom-30 (used by segDDPM) and FSS-1000 (used in segGPT). The current quantitative results are only on horse/car/face datasets used in ReGAN. The result can be more solid with evaluation on diverse types of objects/parts.\n\n- Figure 3 is a confusing. Currently, it seems like the predicted noise is from the cross-attention map and the WAS-attention map. It'll be better to put the Unet from SD in the box. Then from this Unet, one output is the predicted noise from the original SD and the other output is fed to the attention-extraction module which selects layers and combines attention maps."
                },
                "questions": {
                    "value": "- For the learned text embedding, are they interpretable? e.g. one can use the data-driven approach to find text tokens whose embedding are closest to the optimized ones.\n\n- How is the performance on other benchmark datasets like ADE-Bedroom-30 (used by segDDPM) and FSS-1000 (used in segGPT)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3070/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699297201402,
            "cdate": 1699297201402,
            "tmdate": 1699636252467,
            "mdate": 1699636252467,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "nfHfU74UNM",
                "forum": "7FeIRqCedv",
                "replyto": "doB5ePyED8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3070/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3070/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**WAS-attention's effect** \n\nWe would like to clarify that WAS-attention can be employed either solely during inference or during both optimization and inference. When used solely during inference, it has no impact on the optimization process. \n\nThe reference made by the reviewer to Table 5 refers to the scenario where WAS-attention is applied exclusively during inference. On the other hand, Table 9 presents results when WAS-attention is entirely excluded from both optimization and inference steps in the car parts segmentation task. Notably, this exclusion leads to a notable decrease in IoU, dropping from 68.3% to 62.7%.\n\nWe ran another experiment with completely removing WAS-attention from both optimization and inference steps on the horse parts segmentation task, where we also see a significant IoU drop (63.3% to 52.4%). \n\nThis table provides a clear breakdown of the values for each body part, comparing the results without and with the Weighted Average Score (WAS).\n\n| Part      | Without WAS   | With WAS      |\n|----------------|---------------|---------------|\n| Background     | 68.2% \u00b1 5.0   | 79.6% \u00b1 2.5   |\n| Head           | 47.8% \u00b1 2.8   | 63.8% \u00b1 0.7   |\n| Legs           | 50.1% \u00b1 0.6   | 59.5% \u00b1 2.1   |\n| Neck+Torso     | 57.3% \u00b1 13.1  | 68.1% \u00b1 4.4   |\n| Tail           | 38.6% \u00b1 1.4   | 45.4% \u00b1 2.4   |\n| Average        | 52.4% \u00b1 3.8   | 63.3% \u00b1 2.4   |\n\n=====================\n\n**Post-processing e.g., GrabCut** \n\nWhen it comes to post-processing techniques like traditional GrabCut, it's important to note that not only do they introduce additional computations during inference, but they may also necessitate user intervention for refining segmentations in complex cases (refer to Figure 5 in the GrabCut paper). These methods often struggle to generalize in scenarios where there is no distinct color difference between object and background, or in cases with subtle color/texture transitions between different parts of an object. For instance, in face part segmentation, distinguishing noise from the actual face becomes challenging due to their almost identical texture and color.\n\n=====================\n\n**Results on ADE-Bedroom-30 and FSS-1000**\n\nWe would like to emphasize that while the SegGPT model was tested on unseen samples/datasets, its foundational base model is trained on extensive supervised segmentation data\u2014comprising images and corresponding segmentation masks. This means that SegGPT's underlying supervised model may have encountered categories or samples resembling the test data. However, it's important to note that we do not utilize a base supervised segmentation model. Nevertheless, when applied to the FSS-1000 dataset, our method yielded a reasonable IoU:\n\nSLiMe (ours): 73.53 %\nPainter: 61.7%\nSegGPT: 85.6% \n\nPainter is cited in the SegGPT paper as a similar technique which also leverages a supervised segmentation base model (as SegGPT does). Notably, our SLiMe outperforms Painter.\n\nFor the ADE-Bedroom-30 dataset, we obtained an average mIoU of 32.3 using the same settings as SegDDPM, which is comparable to the 34.6 achieved by segDDPM. We identified a slight underperformance in SLiMe, attributed to its imperfect handling of small and featureless objects, such as man-made items like lamps in the ADE-Bedroom dataset. This limitation arises from the fact that these objects become tiny or even vanish in the cross-attention maps we employ, sized at 16x16 and 32x32, while the input images for this dataset are at a resolution of 256x256. Addressing this issue could be a valuable improvement for SLiMe in future work.\n\n=====================\n\n**Interpretability of learned embeddings** \n\nWe calculated cosine distance between some of the learned text embeddings with our SLiMe and the words/embeddings in the Stable Diffusion dictionary. The closest text tokens in the Stable Diffusion\u2019s dictionary to the learned ones using SLiMe do not necessarily match. For instance, the 10 closest words in the Stable Diffusion dictionary to the optimized embedding of 'bear' segmentation are:\n\n```\n{brasil, nightout, mccall, boudo, confirmation, oldschool, stockholm, regardless, beacon, tik}\n```\n\nwhich do not correspond to the concept of 'bear.' However, a future work could involve a deeper exploration of the initial and the evolution of the word embeddings during optimization, which may lead to improved segmentation results or more interpretable tokens."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3070/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700271948103,
                "cdate": 1700271948103,
                "tmdate": 1700272025281,
                "mdate": 1700272025281,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "EZY6t9ESeu",
            "forum": "7FeIRqCedv",
            "replyto": "7FeIRqCedv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3070/Reviewer_kAQy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3070/Reviewer_kAQy"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents SLiMe that allows for the segmentation of various objects or parts at different granularity levels with just one annotated example. The method leverages the knowledge embedded in pre-trained vision-language models and uses weighted accumulated self-attention maps and cross-attention maps to optimize text embeddings. The optimized embeddings then assist in segmenting unseen images, demonstrating the method's effectiveness even with minimal annotated data. The paper includes extensive experiments showing that SLiMe outperforms existing one- and few-shot segmentation methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1: SLiMe introduces a unique one-shot optimization strategy for image segmentation, which is useful when the available data is limited.\n\n2: The proposed method demonstrates superior performance over existing one- and few-shot segmentation methods in various tests, indicating its practical applicability and effectiveness.\n\n3: The paper showcases the method's versatility by successfully applying it to different objects and granularity levels, emphasizing its broad applicability."
                },
                "weaknesses": {
                    "value": "My main concerns focus on the **text prompt**.\n\n1: The introduction of the text prompt is quite abrupt. In the Introduction, SLiMe is described as requiring only an image and a corresponding mask to achieve segmentation of any granularity. However, immediately after, the author talks about fine-tuning text embeddings. What is the definition of 'text' in this task? How are text embeddings obtained? And do different granularities correspond to the same text? The author is encouraged to provide further clarification.\n\n2: The role of \"text prompt\" in the method. The authors claim that \"our novel WAS-attention map to fine-tune the text embeddings, enabling each text embedding to grasp semantic information from individual segmented regions\". However, I haven't found evidence from the main text to illustrate the correspondence between \"text embedding\" and \"individual segmented regions\", especially in the arbitary granularity situation, which is one of the most important parts of this paper. Moreover, how to construct the text is also ignored."
                },
                "questions": {
                    "value": "Please see the weakness part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3070/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3070/Reviewer_kAQy",
                        "ICLR.cc/2024/Conference/Submission3070/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3070/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699585948661,
            "cdate": 1699585948661,
            "tmdate": 1700429094184,
            "mdate": 1700429094184,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "aGBhMuCr7C",
                "forum": "7FeIRqCedv",
                "replyto": "EZY6t9ESeu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3070/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3070/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Clarification on prompt, tokens, and embedding**\n\nWe appreciate the reviewer's insight regarding the potentially confusing tokenization step. To address this concern, we plan to include a new figure similar to this: https://imgur.com/a/0dutj8r that provides a clearer illustration of how each part relates to tokens for granular level segmentation. As illustrated in the figure, each part (p) in the image corresponds to a token (t) in the embedding, which is initially set as a null text (i.e., \"\") to be later optimized. Consequently, we do not utilize any prompt as input. The inputs to SLiMe include empty token placeholder(s), an image, and a corresponding segmentation mask. \n\nWe utilize Stable Diffusion's text tokenizer to specify text tokens (i.e., breaking the sentence into tokens). Since Stable Diffusion is configured to operate with 77 tokens, if the length of the tokenized text is less than 77, the tokenizer pads it with a special token (e.g., end of string: EOS). If its length exceeds 77, the tokenizer crops it to a length of 77. We initialize the tokens with null text (i.e., \"\"). So they are like placeholders in the embedding.\n\nWe hope this clarification resolves any confusion. Please feel free to let us know if any ambiguity persists."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3070/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700271930951,
                "cdate": 1700271930951,
                "tmdate": 1700272373975,
                "mdate": 1700272373975,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0ZSb1Yea9I",
                "forum": "7FeIRqCedv",
                "replyto": "aGBhMuCr7C",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3070/Reviewer_kAQy"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3070/Reviewer_kAQy"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the detailed reply! The illustration in https://imgur.com/a/0dutj8r makes sense. For a better understanding, could authors provide some examples of how the learned text embeddings correspond to different parts of the real images? I think these samples could strengthen the claim."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3070/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700429289768,
                "cdate": 1700429289768,
                "tmdate": 1700429289768,
                "mdate": 1700429289768,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]