[
    {
        "title": "Enhancing Temporal Knowledge Graph Completion with Global Similarity and Weighted Sampling"
    },
    {
        "review": {
            "id": "ZFB7WJiHux",
            "forum": "wN9HBrNPSX",
            "replyto": "wN9HBrNPSX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2926/Reviewer_wF5W"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2926/Reviewer_wF5W"
            ],
            "content": {
                "summary": {
                    "value": "In summary, the paper presents an incremental learning approach for TKG completion that incorporates techniques like weighted sampling and a model-agnostic enhancement layer to address challenges of handling unseen and sparsely connected entities in a growing knowledge graph over time. Evaluation on the constructed benchmarks demonstrates the effectiveness of the proposed framework. The paper"
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper tackles the relatively new problem of incremental learning for temporal knowledge graph completion, which has practical values in real-world applications, as most real-world knowledge bases must address as data arrives continuously over time.\n2.  It proposes a novel model-agnostic enhancement layer that leverages global entity similarity, providing a creative approach beyond local neighborhood proximity used in existing methods.\n3. Results demonstrate substantial quantitative gains over baselines, indicating the high technical quality of the proposed framework components."
                },
                "weaknesses": {
                    "value": "1. Real-world deployment considerations are not discussed. For example, analyzing memory/compute needs and ability to handle streaming data.\n2. While weighted sampling is an intuitive idea, the sampling function used could be further explored and justified. For example, analyzing alternative frequency-based formulas or evaluating sampling directly from a learned importance weighting."
                },
                "questions": {
                    "value": "1. In Figure 2.a, it seems that the proposed method (Full) is outperformed by the proposed method (Enhancement Layer) on HITS@10, and the proposed method (Full) is outperformed by the proposed method (Weighted Sampling), any insights on that?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2926/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698442633442,
            "cdate": 1698442633442,
            "tmdate": 1699636236224,
            "mdate": 1699636236224,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "g4gw04drQl",
                "forum": "wN9HBrNPSX",
                "replyto": "ZFB7WJiHux",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2926/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2926/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate the reviewer's time and insightful feedback. \n\n>1. Real-world deployment considerations are not discussed. For example, analyzing memory/compute needs and ability to handle streaming data.\n\nThat is a great point. We have now added a table that report the memory and time complexity of each step of our approach both theoretically and in dataset ICEWS14. The table is copied below for ease of access. \n\n**Notation**:  $b$, $n$ and $d$ represent the batch size, number of similar entities, and the entity embedding dimensions. $|\\mathcal{Q}|$ and $|\\mathcal{E}|$ represent the number of entities and quadruples in the dataset respectively. \n\n| Methodology | Time Complexity | Memory Complexity | Per Epoch Runtime | Total Runtime |\n|-----------|-----------|-----------|-----------|-----------|\n| Naive Titer | - | - | ~9s | ~90m |\n| Enhancement Layer | $\\mathcal{O}(bnd)$ | $\\mathcal{O}(bnd)$  | ~12s | ~125m|\n| Weighted Sampling | $\\mathcal{O}(n)$ | $\\mathcal{O}(\\|\\mathcal{E}\\|)$ | ~9s | ~110m |\n| Full | $\\mathcal{O}(bnd)$ | $\\mathcal{O}(\\|\\mathcal{E}\\| + bnd)$ | ~12s | ~125m|\n\n>While weighted sampling is an intuitive idea, the sampling function used could be further explored and justified. For example, analyzing alternative frequency-based formulas or evaluating sampling directly from a learned importance weighting.\n\nWe thank the reviewer for this comment. We conducted experiments with various strategies, including mean, max, and min inverse frequencies of head and tail entities. During our hyperparameter selection process, the approach we ultimately chose was the one that yielded the best performance. In terms of exploring a learned importance weighting for each entity, we agree that this is a promising direction. We have mentioned this point in the future work section. \n\n>In Figure 2.a, it seems that the proposed method (Full) is outperformed by the proposed method (Enhancement Layer) on HITS@10, and the proposed method (Full) is outperformed by the proposed method (Weighted Sampling), any insights on that?\n\nThis is a great question. Our findings suggest that the enhancement layer improves performance for edges involving low and medium-degree entities, which aligns with its intended function. However, for edges involving higher-degree entities, the full methodology outperforms the individual components."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2926/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700704569330,
                "cdate": 1700704569330,
                "tmdate": 1700719964089,
                "mdate": 1700719964089,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "gJAUhmz5rF",
            "forum": "wN9HBrNPSX",
            "replyto": "wN9HBrNPSX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2926/Reviewer_eLoE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2926/Reviewer_eLoE"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents an incremental training framework for Temporal Knowledge Graph (TKG) completion, addressing the challenges of generalizing new knowledge and managing sparse connections. The framework combines a model-agnostic enhancement layer that leverages global entity similarity and a weighted sampling strategy to improve link prediction and handle long-tail entities."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "Clear writing, making it easy to understand the author's ideas."
                },
                "weaknesses": {
                    "value": "W1. The experimental dataset is very single. ICEWS14 and 18 are only different in years, and the homogeneity is serious. Moreover, the author reconstructed ICEWS without even giving a detailed data description of the new dataset.\n\nW2. The comparison method is single. The author mentioned a lot of related work, but in the end only one method was selected as the basic model for the experiment. Such an experiment does not prove that the enhancement method proposed by the author is universal.\n\nW3. The method is too simple. The global similarity and weighted sampling proposed in the paper are superficial training strategies. There is no in-depth discussion and unique insights into the problem, and there is a lack of breakthrough contributions."
                },
                "questions": {
                    "value": "Q1. Have you tried experimenting on more TKGs, such as WIKIDATA and GDELT?\n\nQ2. Have you tried more TKGC methods? For example, Cygnet, LCGE?\n\nQ3. Since global similarity is used to enhance the representation of entities, why not just cancel the incremental training setting and train the representations of all entities together from the beginning?\n\nQ4. Generally speaking, incremental training only reduces training costs and should not be more effective than training from scratch. If you want to use incremental training to enhance the effect of TKGC, is this starting point wrong?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2926/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2926/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2926/Reviewer_eLoE"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2926/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698457863630,
            "cdate": 1698457863630,
            "tmdate": 1699636236134,
            "mdate": 1699636236134,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "mJOsYB0NpB",
                "forum": "wN9HBrNPSX",
                "replyto": "gJAUhmz5rF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2926/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2926/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Experimental results of a new base model and further clarification of datasets"
                    },
                    "comment": {
                        "value": "We appreciate the reviewer's time and insightful feedback. The reviewer has thoughtfully highlighted important considerations regarding the datasets: \n>Q1. Have you tried experimenting on more TKGs, such as WIKIDATA and GDELT?\n\nWhile we agree that applying our approach to additional datasets would indeed be interesting, we were unfortunately unable to do so within the timeframe of this revision. However, it is important to note the inherent diversity of the two datasets currently included in our manuscript. Although ICEWS14 and ICEWS18 originate from the same ICEWS dataset, they are recognized as distinct benchmarks in the literature due to their differing distributions and densities. For example, ICEWS18 has 10 times more quadruples compared to ICEWS14 despite both of them covering a similar time duration within their respective years (see table below). The section 5.1 of our paper describes the dataset construction. We have now also revised this section to explain the dataset construction in more detail. The following table summarizes the statistics across these datasets:\n\n| Dataset | \\# Entities | \\# Relations | \\# Snapshots | \\# Quads ($G_1$) | Ave. Quads ($G_{i> 1}$)|\n|-----------|-----------|-----------|-----------|-----------|-----------|\n| ICEWS14 | 7128 | 230 | 33 | ~28k/3.7k/4k| ~1k/0.3k/0.3k |\n| ICEWS18 | 23039 | 230 | 16 | ~244k/45k/43k |  ~8K/1k/1k |\n\n\n> Q2. Have you tried more TKGC methods? For example, Cygnet, LCGE?\nWe are now adding another state-of-the-art model  TANGO [1] to the paper. Preliminary results show that even the addition of our enhancement layer alone improves the performance. The results are provided in the following table: \n\n|  Methodology |  | Current | | |  | Average | | |\n|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|\n|  | **Hit@10** | **Hit@3** | **Hit@1**| **MRR** | **Hit@10** | **Hit@3** | **Hit@1**| **MRR** |\nTANGO |0.519 |0.355 |0.232 |0.317 |0.534 |0.395 |0.270 |0.359 | \nTANGO + Enhancement Layer |0.720 |0.555 |0.379 |0.496 |0.715 |0.557 |0.386 |0.498 |\n\nFurther details about how the enhancement layer was incorporated to TANGO is provided in our paper.\n\n\n**References** \n\n[1] Han, Zhen, et al. \"Learning neural ordinary equations for forecasting future links on temporal knowledge graphs.\" Proceedings of the 2021 Conference on empirical methods in natural language processing. 2021. [(Code)](https://github.com/TemporalKGTeam/TANGO)"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2926/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700703784687,
                "cdate": 1700703784687,
                "tmdate": 1700723020795,
                "mdate": 1700723020795,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "3cICrVhvVZ",
            "forum": "wN9HBrNPSX",
            "replyto": "wN9HBrNPSX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2926/Reviewer_Pdc3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2926/Reviewer_Pdc3"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents an incremental training framework for temporal knowledge graphs by incorporating a model-agnostic enhancement layer and a weighted sampling strategy. The authors conduct extensive experiments on the two popular datasets and the results show the effectiveness of the proposed method. The target problem is interesting."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The authors conduct extensive experiments on the two popular datasets and the results show the effectiveness of the proposed method. \n2. The paper is well written and the target problem is interesting."
                },
                "weaknesses": {
                    "value": "1. The motivation is not well established.  It seems that the authors combine the two methods (global similarity and weighted sampling). \n2. In Table 1, many various recent works should be considered and discussed:\nXu et al., 2023. Temporal knowledge graph reasoning with historical contrastive learning.\nZhang et al., 2023. Learning Long- and Short-term Representations for Temporal Knowledge Graph Reasoning.\nZhu et al., 2021. Learning from History: Modeling Temporal Knowledge Graphs with Sequential Copy-Generation Networks."
                },
                "questions": {
                    "value": "1. The motivation is not well established.  It seems that the authors combine the two methods (global similarity and weighted sampling). \n2. In Table 1, many various recent works should be considered and discussed:\nXu et al., 2023. Temporal knowledge graph reasoning with historical contrastive learning.\nZhang et al., 2023. Learning Long- and Short-term Representations for Temporal Knowledge Graph Reasoning.\nZhu et al., 2021. Learning from History: Modeling Temporal Knowledge Graphs with Sequential Copy-Generation Networks."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2926/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698728230249,
            "cdate": 1698728230249,
            "tmdate": 1699636236040,
            "mdate": 1699636236040,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fuzxO5Ujxe",
                "forum": "wN9HBrNPSX",
                "replyto": "3cICrVhvVZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2926/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2926/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their time and their valuable feedback.\n\n> 1. The motivation is not well established. It seems that the authors combine the two methods (global similarity and weighted sampling).\n\nWe have modified the paper to better explain the motivation for our approach. Briefly, the motivation for integrating these two methods stems from their complementary strengths in addressing the challenges of Temporal Knowledge Graph (TKG) completion, including sparse or non-existent local connections and training biases resulting from unbalanced distribution. The enhancement layer addresses the issue of sparse or non-existent local connections for infrequent (long-tail) entities, an area where traditional TKG completion methods, which primarily relying on local neighborhood proximity for entity representation, fall short. On the other hand, weighted sampling ensures that long-tail entities are not overlooked by the model during training. Our ablation study, detailed in Section X of the paper, demonstrates how each component contributes to addressing these challenges. Figure 2.a illustrates the effectiveness of the enhancement layer for continual learning. As depicted in Figure 2.b, it also contributes to improving performance for entities with mid-degree nodes. The full method enhances hit@1 for low-degree nodes.\n\n>2. In Table 1, many various recent works should be considered and discussed: Xu et al., 2023. Temporal knowledge graph reasoning with historical contrastive learning. Zhang et al., 2023. Learning Long- and Short-term Representations for Temporal Knowledge Graph Reasoning. Zhu et al., 2021. Learning from History: Modeling Temporal Knowledge Graphs with Sequential Copy-Generation Networks.\n\nWe appreciate the reviewer's suggestion regarding the related work. To clarify, our experiments were designed to demonstrate the effectiveness of our framework and its ability to be used on top of any state-of-the-art TKG model. In Table 1, we compare the performance of a model before and after the application of our framework.  We have now added another state-of-the-art model  TANGO [1] to the paper. Preliminary results show that even the addition of our enhancement layer alone improves the performance. \n\nThe results are provided in the following table: \n\n|  Methodology |  | Current | | |  | Average | | |\n|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|\n|  | **Hit@10** | **Hit@3** | **Hit@1** | **MRR** | **Hit@10** | **Hit@3** | **Hit@1** | **MRR** |\nTANGO |0.519 |0.355 |0.232 |0.317 |0.534 |0.395 |0.270 |0.359 |\nTANGO + Enhancement Layer |0.720 |0.555 |0.379 |0.496 |0.715 |0.557 |0.386 |0.498 |\n\n\n\n**References** \n\n[1] Han, Zhen, et al. \"Learning neural ordinary equations for forecasting future links on temporal knowledge graphs.\" Proceedings of the 2021 Conference on empirical methods in natural language processing. 2021. [(Code)](https://github.com/TemporalKGTeam/TANGO)"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2926/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700699118514,
                "cdate": 1700699118514,
                "tmdate": 1700722998952,
                "mdate": 1700722998952,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "W9rgHopce6",
            "forum": "wN9HBrNPSX",
            "replyto": "wN9HBrNPSX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2926/Reviewer_1RrB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2926/Reviewer_1RrB"
            ],
            "content": {
                "summary": {
                    "value": "This paper is about the temporal knowledge graph completion problem and its challenges. Models proposed to solve this problem need to take into consideration the requirement for generalization and assimilation to new knowledge, and the sparseness or connection between newly introduced entities. To overcome these challenges, the authors propose an incremental training framework specifically designed for temporal knowledge graphs, a unique enhancement layer that can be integrated with various GNN-based temporal knowledge graph completion methods, and a weighted sampling strategy during the training process which emphasizes the connections of infrequent entities. The experimental setup contains two versions of ICEWS datasets, link prediction tasks (overall, inductive), hit@k and MRR evaluation metrics, and the comparison methods that include the baseline model (Titer) with three variations (FT, ER, EWC) and three proposed variations (weighted sampling, enhancement layer, full). The results show the overall improvement in the model when utilizing the proposed enhancement layer and the proposed weighted sampling."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1) The paper is about an interesting topic (TKG completion task). The authors highlight the challenges in this topic and provide with proposed solutions that can be adapted in existing TKG completion models.\n2) The results show the performance improvement when adding the proposed layer and sampling.\n3) The paper is well-written and well-structured. The authors have done a great job to describe the area of research, its limitations, provide the related works, give solutions for each limitation and support the proposed methodology with experiments around three research questions."
                },
                "weaknesses": {
                    "value": "1) The proposed methodology is incremental. The proposed layer and sampling are extensions to an existing model, Titer.\n2) It would be useful if the authors could add other state of the art models (from their related work) to the comparison models. The experiments would be more convincing and would make a stronger point if the authors could show how the proposed methodology can be added in other models too, and to be able to see the performance of other models with the proposed extensions.\n3) Another useful aspect on the proposed methodology is to add the run times and the time complexities when the layer and the sampling strategy are added."
                },
                "questions": {
                    "value": "The authors can respond to my 2) and 3) comments in the weaknesses section.\n\nMinor comment: In section 5.2, 3rd sentence, the citation was not added/compiled properly on LaTeX."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No concerns."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2926/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699054080110,
            "cdate": 1699054080110,
            "tmdate": 1699636235981,
            "mdate": 1699636235981,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0lxLqrZkxC",
                "forum": "wN9HBrNPSX",
                "replyto": "W9rgHopce6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2926/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2926/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Added experimental results of a new base model and runtime analysis"
                    },
                    "comment": {
                        "value": "> 2. It would be useful if the authors could add other state of the art models (from their related work) to the comparison models. The experiments would be more convincing and would make a stronger point if the authors could show how the proposed methodology can be added in other models too, and to be able to see the performance of other models with the proposed extensions.\n\nWe thank the reviewer for their suggestion. We are now adding another state-of-the-art model  TANGO [1] to the paper. Preliminary results show that even the addition of our enhancement layer alone improves the performance. The results are provided in the following table: \n\n|  Methodology |  | Current | | |  | Average | | |\n|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|\n|  | **Hit@10** | **Hit@3** | **Hit@1**| **MRR** | **Hit@10** | **Hit@3** | **Hit@1**| **MRR** |\nTANGO |0.519 |0.355 |0.232 |0.317 |0.534 |0.395 |0.270 |0.359 |\nTANGO + Enhancement Layer |0.720 |0.555 |0.379 |0.496 |0.715 |0.557 |0.386 |0.498 |\n\nTo concisely illustrate the integration of the enhancement layer in TANGO, which comprises two graph convolutional layers, we explored two approaches: (1) positioning the enhancement layer above the two convolutional layers, and (2) placing it between them. Our findings indicate that situating the enhancement layer atop the convolutional layers results in superior performance. Further details about this implementation have been elaborated in our paper.\n\n\n> 3. Another useful aspect on the proposed methodology is to add the run times and the time complexities when the layer and the sampling strategy are added.\n\nThis is a great suggestion. We have added a table that reports the runtime and memory complexity of different variations of our model (weighted sampling, enhancement layer, full) in comparison with the naive Titer, as well as the actual per-epoch and total runtime of our method on the ICEWS14 dataset. The table is copied below for ease of access. \n\n**Notation**:  $b$, $n$ and $d$ represent the batch size, number of similar entities, and the entity embedding dimensions. $|\\mathcal{Q}|$ and $|\\mathcal{E}|$ represent the number of entities and quadruples in the dataset respectively. \n\n| Methodology | Time Complexity | Memory Complexity | Per Epoch Runtime | Total Runtime |\n|-----------|-----------|-----------|-----------|-----------|\n| Naive Titer | - | - | ~9s | ~90m |\n| Enhancement Layer | $\\mathcal{O}(bnd)$ | $\\mathcal{O}(bnd)$  | ~12s | ~125m|\n| Weighted Sampling | $\\mathcal{O}(n)$ | $\\mathcal{O}(\\|\\mathcal{E}\\|)$ | ~9s | ~110m |\n| Full | $\\mathcal{O}(bnd)$ | $\\mathcal{O}(\\|\\mathcal{E}\\| + bnd)$ | ~12s | ~125m|\n\n> Minor comment: In section 5.2, 3rd sentence, the citation was not added/compiled properly on LaTeX.\n\nWe thank the reviewer for their comment. We have edited the paper and revised the issue. \n\n**References** \n\n[1] Han, Zhen, et al. \"Learning neural ordinary equations for forecasting future links on temporal knowledge graphs.\" Proceedings of the 2021 conference on empirical methods in natural language processing. 2021. [(Code)](https://github.com/TemporalKGTeam/TANGO)"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2926/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700698660832,
                "cdate": 1700698660832,
                "tmdate": 1700722981692,
                "mdate": 1700722981692,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]