[
    {
        "title": "Enhancing Medical Image Generation with Anatomical Precision: A Multi-Headed VAE-Based Diffusion Model"
    },
    {
        "review": {
            "id": "y8hwNa21px",
            "forum": "61TRLIS5A0",
            "replyto": "61TRLIS5A0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7146/Reviewer_PEau"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7146/Reviewer_PEau"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a unified framework for image segmentation and generation, where the segmentation is done by a VAE, and the generation is done by a VAE and Diffusion refinement."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Unify the segmentation and image reconstruction into a single VAE model with proposed location and style decomposition.\n2. Propose a conditional denoising diffusion probabilistic model (cDDPM) to refine the image from the VAE."
                },
                "weaknesses": {
                    "value": "1. The author proposed two types of spatial transformation, one is affine transform and the other is thin plate spline transform. Both types of transform only work well on simple segmentation like skin disease. I highly recommend the author test on some more complex anatomical organs, like the pancreas or cardiac ultrasound.\n2. In the image reconstruction of the VAE, the author only warped the first half of the channel using the deformation from the location branch. If the shape and style are fully decomposed, they should warp the entire features without the skip connection. Otherwise, the following decoder block might ignore the deformed feature and only utilize the skipped feature. Therefore, I want to author to add an experiment showing the performance of reconstruction without the skip connection. \n3. For the diffusion refinement, I'm not sure why the author wants to utilize such a complex framework. According to my experience, if you simply utilize the conditional DDPM by concatenating the noisy input with the blurred image from VAE, you're able to train a CDDPM following the traditional DDPM training framework. I'd like to see this extra experiment. \n4. I don't think this is a general approach for the medical images. If so, the author should utilize more popular datasets in the image segmentation from multi-domains."
                },
                "questions": {
                    "value": "Please add experiments and answer my question in the weakness part. \nFurthermore, I have some other questions:\n1. If this work is for the generation, I'd like to see some more quantitative results, like LPIPS and FID.\n2. What is the use case and benefit of this image generation framework? I don't get a clear motivation from the author."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No ethics concerns."
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7146/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7146/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7146/Reviewer_PEau"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7146/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698515450667,
            "cdate": 1698515450667,
            "tmdate": 1699636846171,
            "mdate": 1699636846171,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "AIOXm7KAjC",
            "forum": "61TRLIS5A0",
            "replyto": "61TRLIS5A0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7146/Reviewer_nez1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7146/Reviewer_nez1"
            ],
            "content": {
                "summary": {
                    "value": "In this work, the authors aim to disentangle position and style in variational autoencoders (VAE). This is so that the user can easily choose to change either the position or the style (eg: color) while generating medical images. To overcome the issue of blurry images produced by VAE, the authors also use diffusion models to enhance the quality of the image. The authors\u2019 method generates both an image and its corresponding ground truth segmentation mask. The segmentation task helps in the disentanglement process. The authors validate the segmentation performance on three datasets. They provide qualitative results showing that they are able to achieve disentanglement between position and style."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1) The authors have a clever design of injecting the position posterior into the intermediate layer in order to alleviate the style parameter from learning the position features.\n2) Through visual qualitative results, the authors demonstrate that they are able to disentangle the position and style parameters, and generate appropriate results by keeping one fixed and changing the other.\n3) The authors provide clear proof of their selection of loss function."
                },
                "weaknesses": {
                    "value": "1) Since the authors\u2019 main contribution is disentanglement, the authors should consider providing a comparison to or atleast a discussion of controllable generative models like ControlVAE [1], DynamicVAE [2], ControlNet [3]. The authors should provide comparison if possible, or, provide a discussion on how the mentioned methods are different / not applicable.\n2) One of the main limitations of this method is the need for a good reference mask. Could the authors provide a discussion on how their method would behave in the case of instance segmentation (for datasets like GLaS [4]) and curvilinear segmentation (for datasets like DRIVE [5])?\n3) It is unclear if the comparison of segmentation performance is fair or not. For non-generative models like UNet++, are the numbers on the test set? While for generative models like the proposed cDDPM, are the numbers on the generated images?\n\n\n**References**\n\n[1] Shao, Huajie, et al. \"Controlvae: Controllable variational autoencoder.\" International Conference on Machine Learning. PMLR, 2020.\n\n[2] Shao, Huajie, et al. \"Dynamicvae: Decoupling reconstruction error and disentangled representation learning.\" arXiv preprint arXiv:2009.06795 (2020).\n\n[3] Zhang, Lvmin, Anyi Rao, and Maneesh Agrawala. \"Adding conditional control to text-to-image diffusion models.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023. \n\n[4] Sirinukunwattana, Korsuk, et al. \"Gland segmentation in colon histology images: The glas challenge contest.\" Medical image analysis 35 (2017): 489-502.\n\n[5] Staal, Joes, et al. \"Ridge-based vessel segmentation in color images of the retina.\" IEEE transactions on medical imaging 23.4 (2004): 501-509."
                },
                "questions": {
                    "value": "1) Please also see the weakness above.\n2) The authors state, \u201cWhen our VAE employs affine warp, users can easily alter the estimated affine matrix\u201d. Since the authors use TPS as well, how should the users alter the position in TPS?\n3) In Section G, the authors mention that their method can also do pure sampling. How is this achieved, that is, how does the procedure change from controlled to pure sampling?\n4) In Figure 10, the pure sampling does not capture the lesion area well (which the authors state as a limitation of DiffuseVAE earlier). Please discuss.\n5) Please mention if the numbers in bold are just numerically better, or, if t-test [6] has been conducted to check if the performance improvement is statistically significant or not.\n\n**References**\n\n[6] Student, 1908. The probable error of a mean. Biometrika, pp.1\u201325."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7146/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7146/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7146/Reviewer_nez1"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7146/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698800974839,
            "cdate": 1698800974839,
            "tmdate": 1699636846046,
            "mdate": 1699636846046,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "pEc93NVxgD",
            "forum": "61TRLIS5A0",
            "replyto": "61TRLIS5A0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7146/Reviewer_etX6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7146/Reviewer_etX6"
            ],
            "content": {
                "summary": {
                    "value": "This article proposes a new diffusion model that guides the reverse diffusion process by using a specially designed multi head VAE, which generates images that display true style and anatomical accuracy. Solved the problem of difficulty in accurately capturing global anatomical priors in diffusion models and the lack of control over the ability to convert unrealistic anatomical images into real images due to recalibration.The proposed method has shown good performance in a series of medical image tasks, such as skin damage and fetal head. In addition, the model provides state-of-the-art segmentation performance."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.The Conditional DDPM (cDDPM) proposed in this article combines multi head VAE and continuous conditional diffusion models. It can effectively refine the blurred VAE reconstruction and control the generation of real images, allowing for more faithful reconstruction of image styles and segmented regions.\n2.The model proposed in this article decouples meaningful potential representations of style and position, and manipulating potential parameters of style or position only affects the corresponding parts of the generated image.\n3.This article proposes a new VAE model for image segmentation and generation, which achieves optimal performance compared to other advanced methods."
                },
                "weaknesses": {
                    "value": "1.The methods proposed in this article can mostly preserve image style or lesion areas during the interpolation process. However, the generation effect of certain small features (such as hair) is poor.\n2. limited novelty. it seems a combination of VAE and diffusion."
                },
                "questions": {
                    "value": "1.Why didn't ablation experiments be conducted, and how did the effectiveness of each module be verified?\n2.Why is there a problem of small feature loss? How to solve it?\n3.Why not compare the segmentation results with the latest article? Why is there no comparative experiment on the synthesis results?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7146/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698935578934,
            "cdate": 1698935578934,
            "tmdate": 1699636845947,
            "mdate": 1699636845947,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]