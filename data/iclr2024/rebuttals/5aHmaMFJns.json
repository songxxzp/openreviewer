[
    {
        "title": "Reason for Future, Act for Now: A Principled Architecture for Autonomous LLM Agents"
    },
    {
        "review": {
            "id": "Wwri4ghWNv",
            "forum": "5aHmaMFJns",
            "replyto": "5aHmaMFJns",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission755/Reviewer_3FoC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission755/Reviewer_3FoC"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a framework called RAFA that combines model-based reasoning and acting for autonomous LLM agents. The key idea is to use the LLM to plan a future trajectory that maximizes long-term rewards, take the first action from the planned trajectory, collect feedback, and replan at each new state. Theoretical analysis shows the regret bound under a series of assumptions. The method is evaluated on text-based environments like game of 24, ALFWorld, BlocksWorld, and TicTacToe."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The idea of combining model-based planning and short-term execution is logical and aligns well with model predictive control techniques in deep RL.\n- Theoretical analysis accounts for limitations like approximate planning and partial observability. Derives regret bound under stated assumptions.\n- Overall presentation, structure, and writing quality are good. Key ideas and algorithms are clearly explained."
                },
                "weaknesses": {
                    "value": "1. The proposed model-based planning framework is identical to existing techniques like PETS [1], Planet [2], and Plan2Explore [3] from the deep RL literature. These prior works are not mentioned or compared anywhere in the paper, which significantly weakens the novelty claims. PETS, Planet, and Plan2Explore also learn dynamics models to plan future trajectories within a model predictive control framework [4]. The lack of citation and comparison with these highly relevant prior works makes the technical contributions unclear. I would also expect to see the authors compare with these traditional approaches that do not rely on LLM.\n\n2. The assumptions required for the theoretical results are very strong and may not perfectly hold in practice. For instance, Assumption 4.1 requires the LLM to accurately reconstruct the true transition dynamics from the provided prompts. However, black-box LLM models have imperfect mathematical reasoning capabilities, so perfectly modeling the dynamics via prompting is unrealistic for complex environments with high-dimensional and highly stochastic dynamics. The authors provide no empirical evidence or analysis on how well this assumption actually holds. If the core assumptions are violated, then the theoretical results lose significance since similar analyses exist for traditional model-based RL methods.\n\n3. The experimental validation uses simple toy domains like game of 24, ALFWorld, BlocksWorld and TicTacToe. These environments seem trivial for planning and optimization algorithms to solve. The necessity of using a powerful LLM is unclear when traditional planners could potentially succeed. Moreover, the provided prompts contain many examples and solutions, which greatly simplifies the problem. The authors should test the approach on more complex planning tasks where traditional methods fail but the LLM succeeds, such as Minecraft. \n\n4. It is also necessary to study the sensitivity to the number of examples provided in the prompts. Since the prompts contain many examples in a few-shot learning set-up, ablation studies should analyze the scaling of success rate as the number of prompt examples is varied. This would shed light on how much the pre-provided solutions are aiding the LLM versus solving tasks from scratch (or maybe can not solve the problem with very few prompt examples).\n\nThe above concerns weaken the contributions and should be addressed.\n\n\n[1] Chua, Kurtland, et al. \"Deep reinforcement learning in a handful of trials using probabilistic dynamics models.\"\u00a0Advances in neural information processing systems\u00a031 (2018).\n\n[2] Hafner, Danijar, et al. \"Learning latent dynamics for planning from pixels.\"\u00a0International conference on machine learning. PMLR, 2019.\n\n[3] Sekar, Ramanan, et al. \"Planning to explore via self-supervised world models.\"\u00a0International Conference on Machine Learning. PMLR, 2020.\n\n[4] Moerland, Thomas M., et al. \"Model-based reinforcement learning: A survey.\"\u00a0Foundations and Trends\u00ae in Machine Learning\u00a016.1 (2023): 1-118."
                },
                "questions": {
                    "value": "Please see the weaknesses section for the main concerns. I also have some questions as follows:\n\n1. How long does it take to learn for each task? What computing resources are needed (such as memory) and how much would it cost to finish one task? How does the computation consumption compare to traditional planning/optimization methods?\n\n2. Can the authors provide the results on different base LLM models, including both API-based ones and open-source ones?\n\n3. I am also curious how much effort the authors spent on tuning the prompts. For example, how many trials of revising the prompts before it can work for a particular task? What are the observations and what is the strategy to tune the prompts?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission755/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698093365176,
            "cdate": 1698093365176,
            "tmdate": 1699636002982,
            "mdate": 1699636002982,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "prwkuSCq05",
                "forum": "5aHmaMFJns",
                "replyto": "Wwri4ghWNv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission755/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission755/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response for Reviewer 3FoC (Part I)"
                    },
                    "comment": {
                        "value": "Thank you for your review. We address your concerns as follows.\n\n**W1** Compare the novelty and performance of RAFA with Traditional RL agents (PETS, Planet, and Plan2Explore); Present tasks are too simple, and traditional RL can handle them.\n\n**A for W1** From the theoretical perspective, our analysis is aimed at discussing the theoretical performance of our proposed LLM mechanisms (RAFA), where we adopt the analysis tools from the Thomspon sampling (PSRL) in RL. Besides, our sample complexity analysis extends the present PSRL to an infinity horizon setting beyond the linear structure assumption, which is still missing in the current literature. Since we want to interpret the theoretical performance of LLM agents, it is important to let the analysis accommodate general function approximation. For the linear setting, we also refine the analysis in the previous literature ([6, 7]), which assumes a bounded reward while considering the reward to be Gaussian distributed. On the other hand, we claim that our major contribution is not on PSRL but on orchestrating reasoning (learning and planning) and acting following the principled approach in RL. Please see the Appendix A for more detailed discussions. Empirically, we do not need to update the parameter nor sample a parameter from the posterior as PETS, Planet, and Plan2Explore. LLM agent makes an implicit Bayesian inference via In-Context learning, which enables LLM agents to achieve good performance with a few interactions. Since our proposed method is a refinement of previously proposed LLM mechanisms, we use the same large language model for a fair comparison. We also add the citations to the work that you mentioned in Appendix A of the updated manuscript.\n\n**W2** Numerical verification of Assumption 4.1 and discussion of the effect when Assumption 4.1 fails in practice.\n\n**A for W2** We remark that we can extend our analysis to accommodate cases where LLM approximates the posterior within an error margin $\\iota$, resulting in a bounded additional regret of $\\iota \\cdot T$. Importantly, our novel RAFA algorithm (Algorithm 1) stands distinctly from prevalent LLM mechanisms ([1], [2], [3], [4]), and our analysis, guided by Algorithm 3 and Assumption 4.1, only elucidates the theoretical performance of RAFA.\n\n\nBesides, we conduct two experiments to numerically verify Assumption 4.1. Since the predictions given by posterior sampling have the contraction property and the variance heterogeneity, we examine these two properties of LLM predictions in multiple settings to study if the distribution of LLM predictions matches the one given by posterior sampling.\n\n**Experiment 1.** We test if GPT-3 can make an implicit Bayesian inference for the linear regression problem. We first generate i.i.d. data $\\{(x_i,y_i)\\}^N_{i = 1}$ from $x_i \\sim \\mathcal{N}(0.8,1),\\ y_i\\sim \\mathcal N(0.5x_i,0.04)$. Then, we feed the following prompt to GPT-3 API (temperature=1.0, top_p=0.8, these parameter configurations can be found in [7]) to obtain 200 predictions when $x_{\\text{pred}} = 1$. Specifically, the prompts are \"The following data is for linear regression: $(x_1, y_1), (x_2, y_2), \\cdots, (x_N, y_N), (1.000000, $\". Then, we plot the histogram of the LLM-generated predictions and the density function of the calculated $y_{\\text{pred}}$ by posterior sampling, where we choose the prior of the coefficient as $\\mathcal N(0,0.5)$. We report the results in Figure 8 of the updated manuscript. We find that the histogram of LLM predictions approximately matches the theoretical distribution of the prediction given by the posterior sampling, which supports Assumption 4.1. Also, with an increasing number of generated data, the variance of LLM predictions decays, which shows the contraction property. We also examine variance heterogeneity in the two-dimensional Bayesian linear regression setting. The variance heterogeneity of predictions helps LLMs give more diverse answers when the current prompt contains less relevant information. \\texttt{RAFA} relies on the diversity of LLM predictions to guarantee the exploration for those states less explored, hence we wish the variance of LLM predictions for the less explored states is higher. Please check more details and results in Appendix D of the updated manuscript."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission755/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700630798226,
                "cdate": 1700630798226,
                "tmdate": 1700641823006,
                "mdate": 1700641823006,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "PfQvIzR8GU",
            "forum": "5aHmaMFJns",
            "replyto": "5aHmaMFJns",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission755/Reviewer_Aro2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission755/Reviewer_Aro2"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a novel framework, \"Reason for Future, Act for Now\" (RAFA), aiming to optimize the actions of large language models (LLMs) in real-world scenarios. This framework is designed to achieve a task using the fewest possible interactions with the external environment by melding reasoning with acting. The theoretical analysis establishes a regret bound for the proposed framework, which is corroborated by superior empirical performance."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "**In-Context Learning Utilization**: RAFA astutely leverages the inherent in-context learning ability of LLMs to bolster reinforcement learning efficiency.\n\n**Comprehensive Experimental Evidence**: The paper furnishes extensive experimental evaluations that underscore RAFA's efficacy and robustness."
                },
                "weaknesses": {
                    "value": "**Irrelevant Theoretical Analysis**: Although the LLM is seemingly exploited in the theoretical analysis, what the theory really draws upon is actually a posterior inference oracle named LLM. Admittedly, a large transformer model pre-trained on carefully curated dataset may become such an oracle, but the assumption that a pre-trained large language model inherently serves this purpose is questionable. Besides, the analysis claims to draw connections between RAFA and Thompson sampling, but it is not clear how the posterior sampling is accomplished, which is absent from both the main text and the actual implementation in appendix.\n\n**Misrepresentation of Existing Methods**: The paper's assertion that existing techniques like graph-search or MCTS are akin to open-loop methodologies seems misleading. Techniques like MCTS can be effortlessly reconfigured for closed-loop control, wherein planning is grounded in the current knowledge base, and the inaugural action in the plan is executed. This approach essentially mirrors the RAFA framework, calling into question its claimed novelty."
                },
                "questions": {
                    "value": "1. How is the posterior sampling accomplished?\n2. How does the RAFA framework tackle the challenge of efficient exploration in its operations?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission755/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission755/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission755/Reviewer_Aro2"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission755/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698680967210,
            "cdate": 1698680967210,
            "tmdate": 1699636002904,
            "mdate": 1699636002904,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Hsg3bHN15v",
                "forum": "5aHmaMFJns",
                "replyto": "PfQvIzR8GU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission755/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission755/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response for Reviewer Aro2"
                    },
                    "comment": {
                        "value": "Thank you for your review. We address your concerns as follows.\n\n**Q1 and W1** No verification of Assumption 4.1. How is the posterior sampling accomplished?\n\n**A for Q1 and W1**  First, we want to clarify that Assumption 4.1 casts LLM's In-Context Learning as an implicit Bayesian inference, which finds support in various existing literature [1], [2], [3]. To reinforce its theoretical foundation, we have rigorously verified Assumption 4.1 in Appendix D, outlining the requisite regularity conditions and justifications for each. Furthermore, we extend our analysis to accommodate cases where LLM approximates the posterior within an error margin $\\iota$, resulting in a bounded additional regret of $\\iota \\cdot T$. Importantly, our novel RAFA algorithm (Algorithm 1) stands distinct from prevalent LLM mechanisms ([4], [5], [6], [7]), and our analysis, guided by Algorithm 3 and Assumption 4.1, only elucidates the theoretical performance of RAFA. Besides, we conduct two experiments to numerically verify Assumption 4.1. For more details, please see the first argument in the General Response.\n\n**W2** MCTS can also be implemented in a closed-loop manner.\n\n**A for W2** It is true that MCTS can be implemented in a closed-loop manner to converge for a deterministic environment, which is studied in [8]. However, there is no work that studies how to incorporate closed-loop MCTS with LLMs. Besides, it is still unclear if the closed-loop can achieve a $\\sqrt T$ Bayesian regret, as the sample complexity bound in [8] is $\\tilde{O} ((1/\\epsilon)^d)$ but \\texttt{RAFA} only requires $\\tilde{O}((1/\\epsilon)^2)$ samples to achieve the average $\\epsilon$ accuracy, where $d$ is the dimension of the state space. The key difference between \\texttt{RAFA} and the closed-loop MCTS is the method of model and critic learning. \\textt{RAFA} utilizes ICL to learn the unknown world model and generalizes to the unseen states, which leads to a more efficient exploration and a sharpened regret upper bound.  \n\n**Q2** How does the RAFA framework tackle the challenge of efficient exploration in its operations?\n\n**A for Q2** Since we cast the ICL of LLMs as an implicit Bayesian inference, RAFA explores the environment via the internal randomness of the LLM responses, which is similar to the posterior sampling methods. We also conduct additional experiments to better understand the connections between the randomness of LLM predictions and the exploration of the environment. Specifically, in the ALFWorld environment, the model LLM is prompted to predict the observation of a certain action \"go to countertop 1\". In Figure 9 of the updated manuscript, we report the diversity of LLM predictions. We find that as more action-observation paris are added to the GPT-3 prompt when timestep increases, the number of distinct prediction responses decreases, i.e., the model LLM has decreasing uncertainty of what objects a certain place has after observing the locations of objects at other places. This corresponds to a decreasing entropy, indicating that the variance heterogeneity of predictions LLMs gives more diverse answers when the current prompt contains less relevant information. In other words, \\texttt{RAFA} relies on the diversity of LLM predictions to guarantee the exploration for those states less explored. For more setup details and the results, please refer to Appendix D in the updated manuscript.\n\n\n[1] Zhang, Yufeng, et al. \"What and How does In-Context Learning Learn? Bayesian Model Averaging, Parameterization, and Generalization.\" arXiv preprint arXiv:2305.19420 (2023).\n\n[2] Lee, Jonathan N., et al. \"Supervised Pretraining Can Learn In-Context Reinforcement Learning.\" arXiv preprint arXiv:2306.14892 (2023).\n\n[3] Hollmann, Noah, et al. \"Tabpfn: A transformer that solves small tabular classification problems in a second.\" arXiv preprint arXiv:2207.01848 (2022).\n\n[4] Yao, Shunyu, et al. \"Tree of thoughts: Deliberate problem solving with large language models.\" arXiv preprint arXiv:2305.10601 (2023).\n\n[5] Yao, Shunyu, et al. \"React: Synergizing reasoning and acting in language models.\" arXiv preprint arXiv:2210.03629 (2022).\n\n[6] Shinn, Noah, Beck Labash, and Ashwin Gopinath. \"Reflexion: an autonomous agent with dynamic memory and self-reflection.\" arXiv preprint arXiv:2303.11366 (2023).\n\n[7] Sun, Haotian, et al. \"AdaPlanner: Adaptive Planning from Feedback with Language Models.\" arXiv preprint arXiv:2305.16653 (2023).\n\n[8] Shah, Devavrat, Qiaomin Xie, and Zhi Xu. \"Non-asymptotic analysis of monte carlo tree search.\" Abstracts of the 2020 SIGMETRICS/Performance Joint International Conference on Measurement and Modeling of Computer Systems. 2020."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission755/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700630716771,
                "cdate": 1700630716771,
                "tmdate": 1700639975686,
                "mdate": 1700639975686,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "A57GZlq2Ch",
                "forum": "5aHmaMFJns",
                "replyto": "Hsg3bHN15v",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission755/Reviewer_Aro2"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission755/Reviewer_Aro2"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your effort in addressing the concerns raised in my previous comments. While your rebuttal has shed some light on the validity of your assumptions, I feel that my primary concern regarding the posterior sampling in Thompson sampling was not fully addressed. Allow me to clarify further.\n\nThompson sampling necessitates the sampling of a plausible model from the posterior distribution and deriving its optimal policy. By setting a fixed model prompt and planning with it, RAFA is essentially trying to solve for the optimal policy of a posterior averaged model, rather than exploring one potential model. This fundamental discrepancy between theorety and implementation calls into question the relevance of your analysis.\n\nFrom a practical standpoint, RAFA, in its current form, appears to function similarly to a closed-loop MCTS integrated with an LLM. You assert that the critical distinction between RAFA and traditional closed-loop MCTS lies in the application of in-context learning. However, this claim is not entirely persuasive. It appears to me that any closed-loop MCTS implementation utilizing LLM would inherently involve some form of in-context learning.\n\nI hope these points provide a clearer picture of the areas where further explanation and evidence are needed. I look forward to your response."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission755/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700672992786,
                "cdate": 1700672992786,
                "tmdate": 1700672992786,
                "mdate": 1700672992786,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "kL5YdWXvpE",
            "forum": "5aHmaMFJns",
            "replyto": "5aHmaMFJns",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission755/Reviewer_n4NZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission755/Reviewer_n4NZ"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes to cast the problem of designing language model agents as an RL problem. The proposed algorithm RAFA leverages the in-context learning ability of LLM to do model-based posterior sampling reinforcement learning (PSRL). Specifically, RAFA builds a memory $\\mathcal{D}$ of environment transitions and then uses an LLM to infer the environment dynamic in a novel, \"non-parametric\" and \"in-context\" fashion.\n\nThe policy rollout involves three components that are all in-context learning algorithms that are conditioned on the current $\\mathcal{D}$:\n\n- A **model**  predicts the next state conditioned on an input state-action pair\n- A **critic** predicts the value of transitions/trajectories\n- An **elite** proposes a number of potential actions based on an input state.\n\nDuring planning, the algorithm uses an MPC-style action selection. At each step of the rollout of length $U$,  the elite proposes $B$ action for $s_u$ and the model predicts the next state for each action. At the end of the rollout, the critic evaluates the value of each trajectory and selects the initial action $a_0$ that leads to the highest cumulative return. Then the agent takes a_0 and adds the new transition to $\\mathcal{D}$.\n\nThe paper then proceeds to analyze the theoretical properties of the proposed algorithm and the associated assumptions and shows that the algorithm achieves $O(\\sqrt{T})$ regret where $T$ is the number of environment steps. The proof methodologies employed are reminiscent of those typically found in standard PSRL literature., although I have not thoroughly checked the correctness of every detail in the appendix due to the time constraint.\n\nThe paper concludes with empirical assessments across four benchmarks: Game of 24, AlfWorld, BlockWorld, and Tic-Tac-Toe. In these evaluations, RAFA demonstrates superior performance when juxtaposed with selected baseline algorithms, occasionally by a significant margin."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Overall, I think the idea of this paper is quite neat. Formalizing the LLM reasoning through RL could be the first step towards formalizing the emergent but rapidly growing fields and laying a good theoretical and conceptual foundation. The proposed algorithm makes intuitive sense and the empirical performance seems competitive. I can see that many future works could build on this framework of treating LLM reasoning as RL."
                },
                "weaknesses": {
                    "value": "The paper presents an interesting concept; however, there are areas that require further attention and clarification to fully realize its potential. I would like to offer a constructive critique on some aspects that, if addressed, could enhance the submission.\n\n**Theoretical contribution**\n\nOverall, my biggest concern is with the theoretical claims made by the paper. My concern is not with the correctness of the paper but how much utility the theoretical results have for the actual performance of RAFA. While the theoretical foundations of the paper appear sound, the practical implications of these theories on the RAFA algorithm's performance require further exploration. It is essential to clearly articulate the direct benefits of the theoretical results to the algorithm to prevent any misinterpretations about their significance.\nThe paper is currently written in a way that could lead to unknowledgeable readers thinking that this theory (e.g., regret guarantee) is crucial for the algorithm. My primary research area is not RL theory but I am reasonably familiar with the main developments in RL theory. It seems to me that the theoretical component is not particularly innovative in the context of existing PSRL literature ( for some examples, [1,2,3]). If the authors do not agree with my assessment, then I think it would be good to have a more detailed and explicit discussion about how the theoretical result of this work relates to existing works in PSRL.\n\nSince all of the theory is rooted in generic RL, where does LLM come in? I believe that the answer is entirely in the assumptions made by the theoretical results, specifically, Assumption 4.1 and those in Appendix D. Assumption 4.1 assumes that LLM is capable of doing generic posterior sampling given any $\\mathcal{D}$. This assumption seems overly optimistic, considering the fact that, in general, good posterior sampling is computationally challenging in all but the simplest cases. The paper cites [4] and [5] as supporting evidence that LLMs can do posterior sampling on natural text data, but [4] only uses a small-scale synthetic dataset, and [5] updates a continuous embedding to approximate $\\theta$, neither of which seems to be close to the kind of posterior sampling ability required by this work. Of course, it is entirely possible that LLMs can indeed do the kind of posterior sampling required here, but in that case, I think more thorough empirical verification is needed. For example, one can construct a simple environment where one can compute posterior sampling exactly and measure how well the results produced by the LLM match the analytical solution.\n\nMoreover, the empirical setting used to demonstrate the algorithm's performance involves a relatively small number of environment steps, which raises questions about the applicability of the regret analysis to the algorithm's real-world performance. Additional empirical evidence in this area could substantiate the theoretical claims.\n\n\n**Formatting**\n\nThe current formatting of the paper detracts from its readability and overall presentation. The text is densely packed, and the spacing between figures and tables is too narrow, often resulting in severe margin violation.  The absence of a conclusion section is also notable. I recommend a revision of the manuscript to address these formatting issues. This might involve condensing the theoretical content in the main body to allocate space for a conclusion and to resolve the formatting challenges. As I mentioned earlier, the regret guarantee, while interesting, may not be as central to the understanding and application of the algorithm as suggested. I am open to further discussion on this point if the authors have a different perspective.\n\n\n\n## Reference\n\n[1] Model-based RL with Optimistic Posterior Sampling: Structural Conditions and Sample Complexity. Agarwal et al.\n\n[2] Model-based Reinforcement Learning for Continuous Control with Posterior Sampling. Fan et al.\n\n[3] (More) Efficient Reinforcement Learning via Posterior Sampling. Osband et al.\n\n[4] An Explanation of In-context Learning as Implicit Bayesian Inference. Xie et al.\n\n[5] Large Language Models Are Latent Variable Models: Explaining and Finding Good Demonstrations for In-Context Learning. Wang et al."
                },
                "questions": {
                    "value": "- does this value iteration using LLM as the critic converge?\n\n- I don't fully understand how $\\mathcal{D}$ is being incorporated into the prompts for the model, critic, and elite even after looking at the example in the appendix. Could you give me a concrete example?\n\n- Why sometimes do you use MCTS and sometimes you use other tree searches? How should the users choose between them and what are the trade-offs? \n\n- Why do you say you search all sequences in the pseudocode? Wouldn't that create $B^U$ number of trajectories which is not scalable at all?\n\n- How many queries are needed for each run? In traditional RL, the number of environmental steps is important because that is the bottleneck. I am not sure if this is the case here so the number of environment steps seems less useful. For a more fair comparison to the baselines, I think it would be fairer to compare the number of queries vs return to rule out the possibility that other methods can get better performance via more queries."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission755/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission755/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission755/Reviewer_n4NZ"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission755/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698729708394,
            "cdate": 1698729708394,
            "tmdate": 1700716849224,
            "mdate": 1700716849224,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5FVIspFxGh",
                "forum": "5aHmaMFJns",
                "replyto": "kL5YdWXvpE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission755/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission755/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response for Reviewer n4NZ (Part I)"
                    },
                    "comment": {
                        "value": "Thank you for your review. We address your concerns as follows.\n\n**W1** This paper is less innovative in RL and lacks discussions with PSRL.\n\n**A for W1** From the theoretical perspective, our analysis is aimed at discussing the theoretical performance of our proposed LLM mechanisms (RAFA), where we adopt the analysis tools from the Thomspon sampling (PSRL) in RL. Besides, our sample complexity analysis extends the present PSRL to an infinity horizon setting beyond the linear structure assumption, which is still missing in the current literature. Since we want to interpret the theoretical performance of LLM agents, it is important to let the analysis accommodate general function approximation. For the linear setting, we also refine the analysis in the previous literature ([1, 2]), which assumes a bounded reward while considering the reward to be Gaussian distributed. On the other hand, we claim that our major contribution is not on PSRL but on orchestrating reasoning (learning and planning) and acting following the principled approach in RL. Please see the Appendix A for more detailed discussions. Empirically, we do not need to update the parameter nor sample a parameter from the posterior. LLM agent makes an implicit Bayesian inference via In-Context learning, which enables LLM agents to achieve good performance with a few interactions. Since our proposed method is a refinement of previously proposed LLM mechanisms, we use the same large language model for a fair comparison. \n\n**W2** This paper also lacks numerical verification of Assumption 4.1.\n\n**A for W2** \nWe conduct two experiments to numerically verify Assumption 4.1. Since the predictions given by posterior sampling have the contraction property and the variance heterogeneity, we examine these two properties of LLM predictions in multiple settings to study if the distribution of LLM predictions matches the one given by posterior sampling.\n\n**Experiment 1.** We test if GPT-3 can make an implicit Bayesian inference for the linear regression problem. We first generate i.i.d. data $[(x_i,y_i)]^N_{ i = 1}$ from $x_i \\sim \\mathcal{N}(0.8,1), y_i\\sim \\mathcal N(0.5x_i,0.04)$. Then, we feed the following prompt to GPT-3 API (temperature=1.0, top_p=0.8, these parameter configurations can be found in [7]) to obtain 200 predictions when $x_{\\text{pred}} = 1$. Specifically, the prompts are \"The following data is for linear regression: $(x_1, y_1), (x_2, y_2), \\cdots, (x_N, y_N), (1.000000, $\". Then, we plot the histogram of the LLM-generated predictions and the density function of the calculated $y_{\\text{pred}}$ by posterior sampling, where we choose the prior of the coefficient as $\\mathcal N(0,0.5)$. We report the results in Figure 8 of the updated manuscript. We find that the histogram of LLM predictions approximately matches the theoretical distribution of the prediction given by the posterior sampling, which supports Assumption 4.1. Also, with an increasing number of generated data, the variance of LLM predictions decays, which shows the contraction property. We also examine variance heterogeneity in the two-dimensional Bayesian linear regression setting. The variance heterogeneity of predictions helps LLMs give more diverse answers when the current prompt contains less relevant information. \\texttt{RAFA} relies on the diversity of LLM predictions to guarantee the exploration for those states less explored, hence we wish the variance of LLM predictions for the less explored states is higher. Please check more details and results in Appendix D of the updated manuscript."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission755/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700630611022,
                "cdate": 1700630611022,
                "tmdate": 1700639713904,
                "mdate": 1700639713904,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "IxD7O6fpHT",
                "forum": "5aHmaMFJns",
                "replyto": "kL5YdWXvpE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission755/Reviewer_n4NZ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission755/Reviewer_n4NZ"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the response and revision"
                    },
                    "comment": {
                        "value": "I sincerely thank the authors for their hard work on the revision and additional experiments. They certainly addressed some of my concerns (e.g., token efficiency).  The new verification (in particular, ALFworld) for Assumption 4.1, while still limited, at least provides some amount of evidence that LLM can do something similar to posterior sampling. I think the paper would be a lot stronger if this is done more thoroughly but I understand it's hard to do some in such short period of time. For example, I would like to see a head-to-head comparison on a simple RL task of LLM and actual posterior sampling (e.g., Figure 9). I would also somewhat disagree that the extension from finite horizon case to infinite horizon case is as signifianct as the authors claim, since one can apply the $\\frac{1}{1-\\gamma}$ horizon argument for the infinite horizon case (although this is a more subjective opinion). Not to mention that the all of the tested environment's horizons are very short.\n\nMore importantly, the formatting of the paper is still unacceptable. Margins are violated in pg 2, 8, and 9. On page 9, the top line of the ICLR template is completely missing and everythinhg is still crampped together. Even to make this little space for a conclusion, **the authors removed the anonymous author region from the first page**. Per ICLR guideline, the final paper *does not* have an additional page for the camera ready so there is no way that this paper will fit within 9 pages in its current form. Page limit exists for a reason and I would encourage the authors to adhere to the guidelines. In this current sate, I cannot in good conscience recommend acceptance, but I would consider raising my score to 6 if the author can at least make the paper follow the format guideline."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission755/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700641926680,
                "cdate": 1700641926680,
                "tmdate": 1700643810994,
                "mdate": 1700643810994,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "b1gRdN2ama",
                "forum": "5aHmaMFJns",
                "replyto": "kL5YdWXvpE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission755/Reviewer_n4NZ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission755/Reviewer_n4NZ"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for the clarification, the manuscript revision, and the new experiments. If the information gain is indeed novel, then I think it would be a good addition to the literature. Since I am not an expert in this area, I will defer this judgment to others.\n\nThe last page is still a bit rough but at least there are no egregious formatting violations anymore. The additional experiments (in particular contextual bandits) instill some confidence that LLMs can do posterior sampling in this case although it is evident that there is a visible gap.\n\nAs promised, I will raise my score to 6. However, I still strongly recommend that the authors make the last page more readable, and take other reviewers' feedback into consideration. I wish the authors the best of luck."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission755/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700716831562,
                "cdate": 1700716831562,
                "tmdate": 1700716905147,
                "mdate": 1700716905147,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "fTF0FdwjMG",
            "forum": "5aHmaMFJns",
            "replyto": "5aHmaMFJns",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission755/Reviewer_P2KV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission755/Reviewer_P2KV"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents an approach (RAFA) to use the pretrained LLMs as sequential decision-making agents, by designing a system in which LLM instances are asked to first generate and evaluate a plan, and then to execute it in the environment. The paper includes a theoretical analysis based on Bayesian regret bounds and an empirical investigation in text-based domains."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- I find the general perspective of the paper to be promising. Analyzing LLM-based agents using the conceptual and theoretical tools traditionally associated with reinforcement learning research can provide interesting insights and a more rigorous perspective on modern AI agents.\n- The discussion of related work is quite complete."
                },
                "weaknesses": {
                    "value": "- I am skeptical of how the theoretical background and analysis are related at all to the algorithmic and empirical setting. I would expect any meaningful theory about LLM-based agents to capture something about LLMs, but Assumption 4.1 basically says that the theory is going to assume access to the true posterior and ignores anything about the underlying models. I think this completely disconnects the theoretical results from the practice presented in the paper. In addition, I am not entirely sure of how the Bayesian perspective, in which the first part of the algorithm is supposed to estimate the posterior, is connected to the Model-Predictive Control-based algorithm that is actually employed in the paper.\n- The paper is so compressed and poor in terms of space and content organization that it is very hard to read. As a notable example, page 9 contains many plots too close one to the other and overflowing beyond the regular page limits and margins; moreover, it does not contain any conclusion, making it hard for the reader to further contextualize and understand these results.\n- The empirical evaluation does not seem to be rigorous. First, I do not fully understand why different models (GPT3.5, Vicuna, GPT4) have been employed in the different environments. What was the rationale behind the choice? Why one system and not the other for a specific task? Then, I do not see any error bars in the performance plots and table, nor any information about the number of repetitions of experiments."
                },
                "questions": {
                    "value": "- What are the empirical results under a single LLM model?\n- How many repetitions did you use for the empirical results? Do the results vary that much if you have more?\n- Is there a way to avoid the complete disconnect between theory and practice?\n- Can you reformat the paper to avoid the abuse of vspaces and other techniques that make it hardly readable?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission755/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698787355368,
            "cdate": 1698787355368,
            "tmdate": 1699636002737,
            "mdate": 1699636002737,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fQy8pnvsly",
                "forum": "5aHmaMFJns",
                "replyto": "fTF0FdwjMG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission755/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission755/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response for Reviewer P2KV"
                    },
                    "comment": {
                        "value": "Thank you for your review. We address your concerns as follows.\n\n**Q1** What are the empirical results under a single LLM model?\n\n**A for Q1** We have compared our proposed method (RAFA) with Reflexion [1] in all four experiments in this paper, where Reflexion only uses a single LLM with closed-loop interactions. Results can be found in Section 5 and Appendix G. We observed that Reflexion improves much slower compared to RAFA and has an inferior final performance. This is because Reflexion is an oversimplified version of RAFA, where the planning subroutine revises the current action to maximize the reward function (\u201creason for now\u201d) instead of planning multiple future steps to maximize the value function (\u201creason for future\u201d), which measures the expected cumulative future reward. Besides, in the ALFWorld environment, we compared RAFA and AdaPlanner [2], which is also a baseline that uses a single LLM model. We found that AdaPlanner has a higher initial performance due to a handcrafted set of programs for rejecting suboptimal candidate trajectories, which, however, is challenging to construct without the domain knowledge of a specific task. Besides, its final success rate is also lower than RAFA. In the BlocksWorld and Tic-Tac-Toe environments, we compared RAFA with the Chain-of-Thought and GPT-4 baselines, respectively. The results showed that RAFA outperforms these baselines by a remarkable margin.\n\n**Q2** How many repetitions did you use for the empirical results? Do the results vary that much if you have more?\n\n**A for Q2** We follow a similar reporting procedure as previous LLM work (such as [2], [3], and [4]) and do not repeat the experiments multiple times. The reason behind this choice is that the results do not vary too much, as opposed to experiments for reinforcement learning methods.  How the Bayesian perspective is connected to the Model-Predictive Control-based algorithm.\n\n**Q3 and W1** Is there a way to avoid the complete disconnect between theory and practice? This paper assumes the access to the true posterior and ignores anything about the underlying models. How to connect \n\n**A for Q3 and W1** First, we want to clarify that Assumption 4.1 casts LLM's In-Context Learning as an implicit Bayesian inference, which finds support in various existing literature [3], [4], [5]. In Appendix D in the manuscript, we prove that Assumption D.1. holds when we pose several regularity conditions on the model and data-generating process. Furthermore, we extend our analysis to accommodate cases where LLM approximates the posterior within an error margin $\\iota$, resulting in a bounded additional regret of $\\iota \\cdot T$. Importantly, our novel RAFA algorithm (Algorithm 1) stands distinctly from prevalent LLM mechanisms ([1], [2], [6], and [7]), and our analysis, guided by Algorithm 3 and Assumption 4.1, only elucidates the theoretical performance of RAFA.\n\n\nBesides, we conduct two experiments to numerically verify Assumption 4.1. Since the predictions given by posterior sampling have the contraction property and the variance heterogeneity, we examine these two properties of LLM predictions in multiple settings to study if the distribution of LLM predictions matches the one given by posterior sampling.\n\n**Experiment 1.** We test if GPT-3 can make an implicit Bayesian inference for the linear regression problem. We first generate i.i.d. data $[(x_i,y_i)]^N_{i = 1}$ from $x_i \\sim \\mathcal{N}(0.8,1),\\ y_i\\sim \\mathcal N(0.5x_i,0.04)$. Then, we feed the following prompt to GPT-3 API (temperature=1.0, top_p=0.8, these parameter configurations can be found in [7]) to obtain 200 predictions when $x_{\\text{pred}} = 1$. Specifically, the prompts are \"The following data is for linear regression: $(x_1, y_1), (x_2, y_2), \\cdots, (x_N, y_N), (1.000000, $\". Then, we plot the histogram of the LLM-generated predictions and the density function of the calculated $y_{\\text{pred}}$ by posterior sampling, where we choose the prior of the coefficient as $\\mathcal N(0,0.5)$. We report the results in Figure 8 of the updated manuscript. We find that the histogram of LLM predictions approximately matches the theoretical distribution of the prediction given by the posterior sampling, which supports Assumption 4.1. Also, with an increasing number of generated data, the variance of LLM predictions decays, which shows the contraction property. We also examine variance heterogeneity in the two-dimensional Bayesian linear regression setting. The variance heterogeneity of predictions helps LLMs give more diverse answers when the current prompt contains less relevant information. \\texttt{RAFA} relies on the diversity of LLM predictions to guarantee the exploration for those states less explored, hence we wish the variance of LLM predictions for the less explored states is higher. Please check more details and results in Appendix D of the updated manuscript."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission755/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700630513207,
                "cdate": 1700630513207,
                "tmdate": 1700641243164,
                "mdate": 1700641243164,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]