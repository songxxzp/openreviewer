[
    {
        "title": "DRMGuard: Defending Deep Regression Models against Backdoor Attacks"
    },
    {
        "review": {
            "id": "YulmE3JUUW",
            "forum": "AoRIT2Uzfg",
            "replyto": "AoRIT2Uzfg",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission301/Reviewer_tXTC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission301/Reviewer_tXTC"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a model-based backdoor defense method against the deep regression model. This paper proposes to leverage trigger reverse to detect and remove backdoors from the deep regression models. The experiments demonstrates its effectiveness."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1.This paper investigates backdoor defense in deep regression model, which has not been explored before."
                },
                "weaknesses": {
                    "value": "1. The novelty is limited. The trigger reverse method is similar to Neural Cleanse.\n2. This paper should emphasize the importance of the backdoor in the regression models. Why is it important in the regression area?\n3. The number of categories of compared methods and backdoor attacks is clearly below ICLR acceptance threshold."
                },
                "questions": {
                    "value": "Why is backdoor defense important in the regression area?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission301/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission301/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission301/Reviewer_tXTC"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission301/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698632053232,
            "cdate": 1698632053232,
            "tmdate": 1699635956538,
            "mdate": 1699635956538,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "lft0QIwfng",
                "forum": "AoRIT2Uzfg",
                "replyto": "YulmE3JUUW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission301/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission301/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply by authors"
                    },
                    "comment": {
                        "value": "We appreciate your time and comments on our submission. Please kindly find our response below.\n\n> Weakness 1: The novelty is limited. The trigger reverse method is similar to Neural Cleanse.\n\n**Response:** Thanks for your comments. **Our method is significantly different from Neural Cleanse (NC)**, as we have discussed in the third paragraph of the introduction and the third paragraph of the related work. Below, we briefly discuss the differences again.\n\nNC is designed for backdoored deep classification models (DCMs). First, different from DCMs for which the output space is discretized into a few class labels, the output space of DRMs is continuous. Thus, **it is infeasible to enumerate all the potential target vectors by NC to determine the infected target vector.** \n\nSecond, different from DCMs that adopt $\\arg\\max$ to obtain the final output, DRMs do not need $\\arg\\max$. This makes the backdoor behavior of the backdoored DRMs distinct from that of the backdoored DCMs. Specifically, for a backdoored DCM, the backdoor behavior is often triggered by the activation values of several neurons in the feature space, whereas for a backdoored DRM, it is triggered by the activation values of all the neurons. **Our method introduces feature-space regularization term based on the unique feature-space characteristics of backdoored DRMs, while NC does not utilize the feature-space characteristics of backdoored DCM for reverse engineering.**\n\nOur work is the first to detect backdoored DRMs in the image domain. We apply our method to a DRM to reverse engineer a potential trigger function, based on which we decide whether the model has been injected with a backdoor or not. A major challenge to reverse engineering the potential trigger function in the regression domain is that the output is defined in the continuous space. To address this challenge, we formulate reverse engineering as an optimization problem, which is based on both unique output-space and feature-space characteristics of the backdoored DRMs that are observed in this paper.\n\nThese differences make our work distinct from NC as well as all existing defenses that are designed for backdoored DCMs.\n\n> Weakness 2 and questions: This paper should emphasize the importance of the backdoor in the regression models. Why is it important in the regression area?\n\n**Response:** Thanks for your comments. We have discussed the motivation and importance of defending deep regression models against backdoor attacks in the first paragraph of the introduction section and the first paragraph of the evaluation section. Below, we briefly summarize the importance again.\n\n**Deep regression models are used in many safety-critical applications**, such as driver attention monitoring, head pose estimation, and facial landmark detection, among many others. In our evaluation, we consider gaze estimation and head pose estimation as two representative applications. Specifically, gaze estimation tracks where the subject is looking, and plays a key role in a series of safety-critical applications, such as user authentication, driver distraction detection, and user attention monitoring in AR/VR. Similarly, head pose estimation has also been used in many daily applications, such as the driver assistance system, pedestrian attention monitoring, and aliveness detection in facial recognition. \n\nUnfortunately, DRMs are also vulnerable to backdoor attacks, and existing defenses designed for classification models are ineffective in detecting backdoored regression models. **This research problem has been largely overlooked by the community and we are the first to detect backdoored DRMs in the image domain.** \n\n> Weakness 3: The number of categories of compared methods and backdoor attacks is clearly below ICLR acceptance threshold.\n\n**Response:** Thanks for your comments. Since **there is no existing defense designed for DRMs in the image domain**, we selected the most relevant and representative defenses originally designed for DCMs and adapted them to the regression problem for comparative analysis. Specifically, we compare DRMGuard with four compared methods, i.e., Neural Cleanse, FeatureRE [1], Fine-pruning [2], and ANP [3], and show that DRMGuard can defend against six backdoor attacks.\n\n[1] Zhenting Wang, et al. \"Rethinking the Reverse-engineering of Trojan Triggers.\" NeurIPS 2022.\n\n[2] Kang Liu, et al. \"Fine-pruning: Defending against backdooring attacks on deep neural networks.\" International symposium on research in attacks, intrusions, and defenses. Cham: Springer International Publishing, 2018.\n\n[3] Dongxian Wu, and Yisen Wang. \"Adversarial neuron pruning purifies backdoored deep models.\" NeurIPS 2021."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission301/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699904813191,
                "cdate": 1699904813191,
                "tmdate": 1699911829248,
                "mdate": 1699911829248,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "odY5kk35oN",
                "forum": "AoRIT2Uzfg",
                "replyto": "YulmE3JUUW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission301/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission301/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Invitation for More In-Depth Feedback"
                    },
                    "comment": {
                        "value": "Dear Reviewer tXTC,\n\nThank you for your valuable questions and comments. We're eager to continue this dialogue and are prepared to respond to any additional queries you may have. In light of our forthcoming responses, we kindly ask if you would be willing to reevaluate your score, should our answers help to alleviate your concerns. Your feedback is greatly appreciated.\n\nSincerely,\n\nThe authors"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission301/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700507403259,
                "cdate": 1700507403259,
                "tmdate": 1700507462995,
                "mdate": 1700507462995,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "3hCQXY5tCQ",
            "forum": "AoRIT2Uzfg",
            "replyto": "AoRIT2Uzfg",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission301/Reviewer_FMPV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission301/Reviewer_FMPV"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a model-level backdoor defense which tries to reverse-engine the trigger signal from backdoored model. Specifically, the authors considered a new scenario, i.e., deep regression task, where the output of the model is a vector instead of discrete output in the deep classification task. For instance, in a deep regression task, called gaze estimation, the deep regression model will output a vector to represent the direction of one person view. In this new scenario, the existing related work will not work since some of them [(Wang et al., 2019] recovered a suspect trigger signal for each class, and then determine the true trigger from all suspects. However, the existing works cannot be directly used in deep regression tasks since the defender cannot the infinite outputs. The main contribution of this work solves this problem."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The authors proposed new reverse engineering method to recover the backdoor attack in the deep regression task."
                },
                "weaknesses": {
                    "value": "The main idea to reconstruct the trigger is based on the generative model, which has been exploited following references:\n[1] Zhu, Liuwan, et al. \"Gangsweep: Sweep out neural backdoors by gan.\" Proceedings of the 28th ACM International Conference on Multimedia. 2020.\n[2] Chen, Huili, et al. \"DeepInspect: A Black-box Trojan Detection and Mitigation Framework for Deep Neural Networks.\" IJCAI. Vol. 2. No. 5. 2019\n\nSecondly, the new proposed method proposed a new regularization, which is $r_{f}$ shown in Equation 5. This regulation is only designed based on the empirical results, i.e., the angle of poisoned inputs is more concentrated than the benign data. There is not any theoretical proof to support this."
                },
                "questions": {
                    "value": "Is it possible to compare the ABS method, which is only mentioned in the introduction but isn\u2019t as a state-of-the-art to compare. The reason why I raise this question is that the ABS directly analyze the middle layer feature to reconstruct the trigger. It is will not affected by the infinite of output influence made by deep regression task."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission301/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698794308309,
            "cdate": 1698794308309,
            "tmdate": 1699635956458,
            "mdate": 1699635956458,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6wUljOKpE1",
                "forum": "AoRIT2Uzfg",
                "replyto": "3hCQXY5tCQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission301/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission301/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply by authors"
                    },
                    "comment": {
                        "value": "We appreciate your time and comments on our submission. Please kindly find our response below.\n\n> Weakness 1: The main idea to reconstruct the trigger is based on the generative model, which has been exploited following references: [1] Zhu, Liuwan, et al. \"Gangsweep: Sweep out neural backdoors by gan.\" Proceedings of the 28th ACM International Conference on Multimedia. 2020. [2] Chen, Huili, et al. \"DeepInspect: A Black-box Trojan Detection and Mitigation Framework for Deep Neural Networks.\" IJCAI. Vol. 2. No. 5. 2019\n\n**Response:** Thanks for your comments. The papers mentioned by the reviewer use generative models to perform trigger reverse engineering for classification models and are required to enumerate all possible labels. By contrast, our work is distinct from these works in the following ways.\n\nFirst, as the output vector of deep regression models (DRMs) is defined in the continuous space, it contains countless potential target vectors. **We minimize the variance of the output vectors to perform reverse engineering for backdoored DRMs without enumerating each potential target vector.**\n\nSecond, we analyze the difference in feature space between backdoored DRMs and backdoored deep classification models (DCMs), and identify the unique feature-space characteristics of backdoored DRMs by performing experiments and theoretical analysis. **Thus, the second major difference in our work is the observation of the unique feature-space characteristics of backdoored DRMs and the feature-space regularization term for reverse engineering based on this observation.**\n\n> Weakness 2: Secondly, the newly proposed method proposed a new regularization, which is $r_f$ shown in Equation 5. This regulation is only designed based on the empirical results, i.e., the angle of poisoned inputs is more concentrated than the benign data. There is not any theoretical proof to support this.\n\n**Response:** Thanks for your comments. In addition to the empirical results, **in Section 3.3 of the original submission, we have provided a comprehensive theoretical analysis to support the observation of the feature-space characteristics**. Please see the *Theoretical analysis and metrics* and *the last paragraph* of Section 3.3.\n\n> Questions: Is it possible to compare the ABS method, which is only mentioned in the introduction but isn\u2019t as state-of-the-art to compare? The reason why I raise this question is that the ABS directly analyze the middle layer feature to reconstruct the trigger. It is will not affected by the infinite of output influence made by deep regression task.\n\n**Response:** Thanks for your comments. Although ABS is not affected by the infinite output of DRMs, it is built upon the observation that successful backdoor attacks will entail one or a set of inner neurons, which will fall within a certain range to predict the target labels (see Section 3.2 of the ABS paper). This observation is similar to that in both Fine-pruning [1] and ANP [2]. As we have discussed in Section 3.3 (see *the difference between backdoored DCM and DRM* in Section 3.3), this observation does not hold for backdoored DRM. **We also conduct experiments in Section 4.3, which confirms our analysis that existing defenses built upon the feature-space characteristics of backdoored DCM cannot be applied to backdoored DRMs.**\n\n[1] Kang Liu, et al. \"Fine-pruning: Defending against backdooring attacks on deep neural networks.\" International symposium on research in attacks, intrusions, and defenses. Cham: Springer International Publishing, 2018.\n\n[2] Dongxian Wu, and Yisen Wang. \"Adversarial neuron pruning purifies backdoored deep models.\" NeurIPS 2021."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission301/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699914935560,
                "cdate": 1699914935560,
                "tmdate": 1699914935560,
                "mdate": 1699914935560,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "xLCHa5Qa0I",
            "forum": "AoRIT2Uzfg",
            "replyto": "AoRIT2Uzfg",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission301/Reviewer_WvrB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission301/Reviewer_WvrB"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a backdoor defense method named DRMGuard to tackle backdoor attacks of deep regression models. The core technique is to optimize the variance of feature representations when the cadidate outputs are uncountable. The authors conduct extensive experiments to demonstrate the effectiveness of DRMGuard."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.\tThe research problem is of great significance\n2.\tThe paper is well-structured and easy to follow.\n3.\tThe idea is novel and inspiring."
                },
                "weaknesses": {
                    "value": "1.\tTime complexity. I notice that the authors did not report the running time of DRMGuard. I am not sure whether the variance calculation is time-consuming and could be scaled to higher dimensional conditions. \n2.\tExtension to classification models (not very important). In my opinion, the key of this work is that we could use the variance of deep representations for trigger inverse optimization when the candidate labels are uncountable. I think this method could be extended to deep classification models. I suggest the authors conducting this method in classification experiments.\n3.\tTypos. In Table 8, MRT -> MTR.\n\nOverall, I like this paper. I think the authors study an important problem. Besides, the idea of optimizating the variance of deep representations will inspire more future works."
                },
                "questions": {
                    "value": "Please refer to Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission301/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission301/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission301/Reviewer_WvrB"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission301/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698984213368,
            "cdate": 1698984213368,
            "tmdate": 1699883249855,
            "mdate": 1699883249855,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Y2y1bf9kQ6",
                "forum": "AoRIT2Uzfg",
                "replyto": "xLCHa5Qa0I",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission301/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission301/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply by authors"
                    },
                    "comment": {
                        "value": "We really appreciate your constructive comments and insightful suggestions. Please kindly find our response below.\n\n> Time complexity. I notice that the authors did not report the running time of DRMGuard. I am not sure whether the variance calculation is time-consuming and could be scaled to higher dimensional conditions.\n\n**Response:** Thanks for your constructive comment. To investigate the processing time of our method, we perform experiments on a server installed with an NVIDIA A10 GPU. The input size for the generative model $G_{\\theta}$ and the deep regression model (DRM) is $224\\times 224 \\times3$. The generative model consists of 14 convolutional layers, and the DRM is implemented by ResNet18. The batch size is 50. **Given a DRM, DRMGuard takes about 16 minutes to train $G_{\\theta}$ for 2000 training steps.** We also measure the processing time in calculating the variance when the output is a 1000-dimensional vector. We repeat this process 1000 times, and **the average time used to calculate the variance over 50 1000-dimensional vectors is $5.91\\times 10^{-2}$ ms**. We think the results show that DRMGuard can be scaled to higher dimensional conditions.\n\n> Extension to classification models (not very important). In my opinion, the key of this work is that we could use the variance of deep representations for trigger inverse optimization when the candidate labels are uncountable. I think this method could be extended to deep classification models. I suggest the authors conducting this method in classification experiments.\n\n**Response:** This is a very insightful suggestion, and we really appreciate it. We analyze the behaviors of backdoored deep classification models (DCMs) and design a prototype to show that the idea can also be extended to DCMs. By doing so, we can perform trigger inverse optimization for backdoored DCMs without enumerating all the labels.\n\nSpecifically, we train a backdoored DCM on Cifar10 by BadNet. We use  $\\lbrace x_i \\rbrace _{i=1}^N$ and $\\lbrace\\mathcal{A}(x_i)\\rbrace _{i=1}^N$ to denote a set of benign images and poisoned images, respectively, where $\\mathcal{A}(\\cdot)$ is the function to apply the trigger to the image. We feed $\\lbrace x_i \\rbrace _{i=1}^N$ and $\\lbrace\\mathcal{A}(x_i)\\rbrace _{i=1}^N$ to the backdoored DCM $C(\\cdot)$, followed by a softmax function $S(\\cdot)$ to obtain the probability vectors $\\lbrace S(C(x_i))\\rbrace _{i=1}^N$ and $\\lbrace S(C(A(x_i)))\\rbrace _{i=1}^N$. We observe that poisoned images lead to similar probability vectors for the backdoored DCM. Formally, we have the following observation: $\\frac{1}{d}\\sum _{j=1}^d \\sigma^2(\\lbrace S_j(C(A(x_i))) \\rbrace _{i=1}^N) << \\frac{1}{d}\\sum _{j=1}^d \\sigma^2(\\lbrace S_j(C(x_i)) \\rbrace _{i=1}^N)$, where $S_j(C(A(x_i)))$ and $S_j(C(x_i))$ is the $j$th component of $S(C(A(x_i)))$ and $S(C(x_i))$ respectively. Specifically, we observe that $\\frac{1}{d}\\sum _{j=1}^d \\sigma^2(\\lbrace S_j(C(A(x_i))) \\rbrace _{i=1}^N) \\approx 9\\times 10^{-12}$  and $\\frac{1}{d}\\sum _{j=1}^d \\sigma^2(\\lbrace S_j(C(x_i)) \\rbrace _{i=1}^N) \\approx7\\times 10^{-2}$.\n\nBased on this observation, we propose the following optimization objective to generalize DRMGuard from the regression domain to the classification domain:\n$$\n\\theta^{*}=\\min_{\\theta} \\frac{\\lambda_1}{d}\\sum _{j=1}^d {\\sigma^2(\\lbrace S_j(C(G _{\\theta}(x_i))) \\rbrace _{i=1}^N)} + \\frac{1}{N} \\sum _{i=1}^N \\| G _{\\theta}(x_i)-x_i \\|_1\n$$\n\nThe first objective in the optimization problem aims to reverse engineer the poisoned images $\\lbrace G _{\\theta}(x_i)\\rbrace _{i=1}^N$ that lead to similar probability vectors, regardless of their actual contents. The second optimization term ensures the transformed image  $G _{\\theta}(x_i)$ is similar to the original image $x_i$. We remove the feature-space regularization term designed for DRMs, since backdoored DRMs and backdoored DCMs have different feature-space characteristics, as we discussed in our paper.\n\nBy solving the above optimization problem, $G_{\\theta}$ can reverse engineer the poisoned images without enumerating each label. Also, the probability vectors can converge to the vector that leads to the target label. We train ten backdoored DCMs by BadNets and ten benign DCMs on Cifar10. We report backdoor identification results in the following table.\n| TP | FP | FN | TN | Acc |\n|:--:|:--:|:--:|:--:|:---:|\n|  7 |  0 |  3 | 10 | 85% |\n\n**We also show reversed poisoned images in Appendix A.9 of the updated supplementary.** We think the results show that our idea of minimizing the variance can be extended to the classification domain for reverse engineering.\n\n>Typos. In Table 8, MRT -MTR.\n\n**Response:** Thanks for your detailed comments. We have fixed the typos in the updated version."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission301/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699873885682,
                "cdate": 1699873885682,
                "tmdate": 1699873885682,
                "mdate": 1699873885682,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tInnQIWd9t",
                "forum": "AoRIT2Uzfg",
                "replyto": "Y2y1bf9kQ6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission301/Reviewer_WvrB"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission301/Reviewer_WvrB"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your detailed response, which has addressed all my concerns. I appreciate this paper and have updated my confidence score. Good luck !"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission301/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699883531626,
                "cdate": 1699883531626,
                "tmdate": 1699883531626,
                "mdate": 1699883531626,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]