[
    {
        "title": "Multi-Objective Reinforcement Learning for Forward-Backward Markov Decision Processes"
    },
    {
        "review": {
            "id": "U4gMRkKCmp",
            "forum": "2boLXjsHsB",
            "replyto": "2boLXjsHsB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5148/Reviewer_EeKJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5148/Reviewer_EeKJ"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors have devised a new approach known as Forward Backward Multi-objective Reinforcement Learning (FB-MORL). The convergence behavior of the proposed approach to Pareto optimality is analyzed. The effectiveness of the proposed approach is evaluated on a use-case from wireless caching."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "A new approach is proposed for FB-MORL. A real-world wireless communication problem has been taken as a use case of the proposed methodology."
                },
                "weaknesses": {
                    "value": "The presentation of the paper needs to be improved. A small real-world example illustrating FBMDP would help strengthen the motivation. The algorithm may be computationally expensive."
                },
                "questions": {
                    "value": "My comments are as follows:\n\n1.\tHow do the forward and backward processes conflict with each other through the action space? A small real-world example illustrating this would help strengthen the motivation.\n\n2.\tThe authors stated that the backward dynamics in the backward Markov Decision Process (MDP)  result in a known final state. Can\u2019t it be analyzed by a forward MDP with an absorbing state?\n\n3.\tIn Section 3.2, it is considered that the backward and forward trajectories conflict with each other. However, the analysis does not reflect that. Rather, it seems the analysis presented in the paper replicates the existing forward MDP results for backward MDP. For example, Lemma 4.1 is just an extension of (5). What are the additional challenges in obtaining the convergence results?\n\n4.\tIn Equation (2), why is $\\gamma \\in (0,1]$? Usually for a finite $T$, $\\gamma=1$ is more appropriate. \n\n5.\tIn Equation (3), why is the $\\pi_\\theta$ conditioned on $s_t$ and not on $y_t$? \n\n6.\tThe computation of  $\\beta_f^*$ and $\\beta_b^*$ needs to be done at every iteration. Is it computationally feasible?\n\n7.\tCan\u2019t equation (18) be alternatively expressed as a forward MDP? In other words, can the problem in 5.2.1 be modeled with forward-only dynamics? \n\n8.\tThe presentation of the paper needs to be improved. The  FB-MOAC algorithm should be moved from the appendix to the main text since this is one of the main contributions of the paper. What are the additional challenges in obtaining the analytical results in Section 5.1 compared to the forward MDP only case?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5148/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5148/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5148/Reviewer_EeKJ"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5148/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698730231045,
            "cdate": 1698730231045,
            "tmdate": 1699636508864,
            "mdate": 1699636508864,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5CTxSGfghp",
                "forum": "2boLXjsHsB",
                "replyto": "U4gMRkKCmp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5148/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5148/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "\u201c_example illustrating FBMDP strengthen the motivation. algorithm may be computationally expensive_\u201d\n\nWe have included an example in the context of computation offloading in the revised submission to motivate the applicability of FB-MDPs to a different scenario. Computational complexity is determined by two factors: the selection of the learning rate and the episodic MCS-average mechanism. The former is not a concern, as the simulations suggest that even a fixed learning rate obtains convergence. The latter is primarily affected by the number $N_{MCS}$ of critic agents that should be implemented. Based on simulations, 3 or 4 are sufficient to reach convergence. As a consequence, the complexity of FB-MOAC is competitive with respect to a forward-only RL algorithm.\n\n\u201c_How do forward and backward processes conflict_\u201d\n\nThe forward and backward dynamics compete with each other as they depend on the action variables and they correspond to action-dependent rewards. Specifically, the optimal action from the perspective of forward rewards [Eqs. (20) and (21)] are not necessarily aligned with the optimal action from the perspective of backward reward [Eq. (22)]. This has been numerically evaluated by the ablation study in Section 4.2.3 of the revised manuscript. In particular, Figure 2 shows that merely optimizing the forward rewards causes the backward reward to diverge.\n\n\u201c_Can\u2019t it be analyzed by a forward MDP with an absorbing state_\u201d\n\nThe backward dynamics starts from a known state, however, it moves backwards over time: the current state depends on the future state and action, in contrast with the forward dynamics. Note that converting the backward dynamics by flipping the time does not solve the issue as the resulting state would now depend on the actions that happen in a far future. Hence, it cannot actually be formulated according to a forward-MDP with absorbing states.\n \n\u201c_The analysis does not reflect that conflict of trajectories_\u201d\n\nWe appreciate this remark. Lemma 3.1 differs from its counterpart for the forward dynamics as: it depends on the backward reward; the transition trajectory and the backward value function in Eq. (7) are obtained based on a policy distribution that only depends on the forward state. Hence, they are in conflict since the rewards compete, resulting in several challenges: jointly optimizing the action-coupled backward and forward rewards; devising a multi-objective algorithm that updates the policy _after_ the forward and backward dynamics are evaluated; and the convergence analysis for the devised multi-objective actor agent, shared between the forward and backward dynamics.\n\n\u201c_why is \u03b3 \u2208(0,1] ?_\u201d\n\nIn typical problems, setting $\\gamma=1$ generally suffices. In our case, however, we observed that the backward reward diverges for $\\gamma>0.97$ and the sample-efficiency of the algorithm remarkably worsens for $\\gamma<0.9$.\n\n\n\u201c_why is the \\pi(.) conditioned on s_t and not on y_t_\u201d\n\nThe backward state moves backwards over time and is not known in advance. The strategy $\\pi(. | s_t)$ simplifies the algorithm derivation. Moreover, it does not constrain solution optimality since it is updated after both the forward and backward procedures are evaluated, and it drives the backward state to _optimally_ evolve based on this optimal policy. Hence, both the backward and forward rewards are jointly optimized. Note that a policy equal to $\\pi(.| s_t, y_t)$ does not allow writing the Bellman equation for the backward dynamics, and the expression of the backward reward is complex, as indicated in Lemma 3.1 of the revised manuscript.\n\n\u201c_The computation of \u03b2_f\u2217 and \u03b2_b\u2217 needs to be done_\u201d\n\nThe analysis in Theorem C.1 provides the conditions that guarantee convergence. In practice, one could choose simple learning-rate settings with a confidence threshold such that these conditions are satisfied during the execution of the algorithm.\n\n\u201c_Can\u2019t equation (18) be expressed as a forward MDP?_\u201d\n\nThanks for the valuable comment. It is not possible to appropriately convert this controlled backward dynamics to a standard forward one. Specifically, if we flip the time-slot for the backward state we end up with a forward state which depends on the future actions that have not occurred yet and will happen in the far future. Hence, the states cannot be obtained by traversing onward over time.\n\n\u201c_Enhancement of presentation \u2026 challenges in obtaining the analytical results?_\u201d\n\nWe have improved the presentation in the revised submission. Our work entails solving several challenges related to the analysis. First, we had to derive a backward version of the Bellman equations by identifying an action-coupled class of FB-MDPs. Second, we devised a shared entity for the actor agent which is updated based on the forward and backward stochastic rewards. To prove its convergence, we proposed a novel approach called episodic MCS-average. Finally, we analyzed convergence for merely smooth (and not convex) objective functions."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5148/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700257144824,
                "cdate": 1700257144824,
                "tmdate": 1700257144824,
                "mdate": 1700257144824,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DkJKbw3oEL",
                "forum": "2boLXjsHsB",
                "replyto": "5CTxSGfghp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5148/Reviewer_EeKJ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5148/Reviewer_EeKJ"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response. It has resulted in a better understanding of the paper. Several concerns have been addressed in the revision."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5148/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700723741862,
                "cdate": 1700723741862,
                "tmdate": 1700723741862,
                "mdate": 1700723741862,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "6gALUeSFDr",
            "forum": "2boLXjsHsB",
            "replyto": "2boLXjsHsB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5148/Reviewer_g9Ad"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5148/Reviewer_g9Ad"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses sequential tasks that consider not only forward MDPs where states follow forward dynamics but also backward dynamics from which trajectories evolve in reverse-time order. In this setting, there can be conflicts between rewards from forward dynamics and those from backward ones. In this Forward-Backward Markov decision process (FB-MDP) setting, the authors propose an on-policy multi-objective reinforcement learning algorithm called Forward-Backward Multi-Objective Actor-Critic (FB-MOAC). The core idea is to use a multi-objective optimization technique so that the cumulative vector reward sum becomes a Pareto-optimal point. Numerical results show that the proposed FB-MOAC algorithm is applicable in the FB-MDP setting."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- This paper addresses sequential tasks on the Forward-Backward Markov decision process (FB-MDP) setting, which seems to provide a new perspective in (multi-objective) reinforcement learning.\n- The paper provides some mathematical proofs regarding convergence to Pareto-optimal points.\n- Based on previous works (Lemma 1), the proposed methodology gives technical soundness."
                },
                "weaknesses": {
                    "value": "1. The paper is not easy to follow. There are many notational errors in the core formula. I recommend checking them to improve the overall readability.\n\n2. The most critical weakness is that there are no baseline algorithms to compare with the proposed method in experiments. Readers cannot verify how much the FB-MOAC performs well.\n\n3. Another issue is about the setting: FB-MDP. When can we consider this backward dynamics setting? It was hard to understand. Could the authors provide an example or detailed explanation regarding the necessity of considering backward dynamics?\n\n4. Related to the above 3, the experiment part is hard to understand. Readers may not be familiar with wireless communication areas. Providing high-level pictures describing the environment is recommended.\n\n5. Regarding reproducibility, it would be better to provide anonymous source code to verify the proposed algorithm."
                },
                "questions": {
                    "value": "Please check the above weakness part. Additional questions are as follows.\n\n6.  There is another issue about Lemma 1 which is the core foundation of FB-MOAC. Lemma 1 tells us about the Pareto-optimal convergence, but not about 'which Pareto-optimal point' to converge to. In most multi-objective RL works, there is a preference function (either linear or non-linear) that the designer cares about. Can Lemma 1 explain the characteristics of the converged Pareto-optimal point?\n\n7. Conditions of Corollary 1 should be stated more precisely. There are inverse matrix operations and KKT conditions should be carefully considered.\n\n8. Does saving past model parameters raise any memory issues of the algorithm?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5148/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5148/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5148/Reviewer_g9Ad"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5148/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698756330465,
            "cdate": 1698756330465,
            "tmdate": 1700732102411,
            "mdate": 1700732102411,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "pdK5OtbOdL",
                "forum": "2boLXjsHsB",
                "replyto": "6gALUeSFDr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5148/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5148/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "\u201c_The paper is not easy to follow. There are many notational errors in the core formula_\u201d\n\nThanks for the valuable feedback. We carefully reviewed the notation and fixed some inconsistencies in the revised manuscript. \n\n\u201c_There are no baseline algorithms to compare_\u201d\n\nWe agree that the random approach is not enough for comparison. Therefore, we have included two additional schemes for comparison purposes:Least Frequently Used (LFU), a rule-based approach widely used in the literature on caching; and F-MDP, a learning-based algorithm that replaces the backward MDP with the forward-only formulation.\n\n\u201c_When can we consider this backward dynamics setting?_\u201d\n\nThe need for FB-MDPs depends on the problem environment under consideration. We have included an example in the context of computation offloading in the revised submission to motivate the applicability of FB-MDPs to a different scenario. Specifically, mobile devices offload computationally intensive tasks to an edge server, which processes them according to its computational capacity. The edge server has a buffer to store the tasks that cannot be immediately processed, which happens when there is no spare computational capacity. The new example has been mentioned in the introduction and detailed in the appendix.\n\n\u201c_Providing high-level pictures describing the environment is recommended_\u201d\n\nThank you for your insightful comment. The hybrid experiment considered a heterogeneous network with two different node tiers: Base-stations (BSs) and helper-nodes (HNs). There exists a set of users that request files from a database containing $N$ different files. These files are requested based on a time-varying file popularity distribution. The network serves the users with a hybrid content-delivery scheme by employing both BSs and HNs. BSs are connected to the core-network and HNs are equipped with caches to proactively store files. The hybrid scheme is based on multicast transmissions from HNs and unicast transmissions from BSs. The HNs cache most popular files and cooperatively transmit the cached files across the network. Conversely, the BSs individually serve the users that request files. To do so, the BS first fetches the file from the core network and then sends it to the user. Transmissions are repeated until the data are successfully received by the user.\nConsidering the popularity of content and the outage probability, a forward dynamics is found for the request probability which is expressed in Eq. (18). As an important metric for the transmission schemes, we consider the expected latency for successful content reception. Due to the outage possibility, a backward dynamics is extracted for the expected latency presented in Eq. (19). Then, based on the content popularity and expected latency, we are considering three conventional metrics to optimize the network performance. These metrics are as follows. A quality-of-service metric that indicates how much the users are satisfied, the bandwidth consumption that measures the required bandwidth of the whole network and the overall expected latency that shows the latency for all file requests.\n\n\u201c_provide anonymous source code_\u201d\n\nThis is indeed an interesting point. We are reviewing and improving the clarity of the code used for the paper and are committed to anonymously publish it by the end of the discussion phase.\n\n\u201c_Lemma 1 does not tell about which Pareto-optimal point it converges_\u201d\n\nThank you for the precise remark. Indeed, this Lemma does not specify which Pareto-optimal solution the devised algorithm converges to. However, the algorithm is guaranteed to converge to one of the Pareto-optimal points. As for our proposed method, we have proved convergence for both cases of strongly-convex and Lipschitz-smooth objectives and shown that all the forward and backward expected losses monotonically decrease as the iteration increases (Theorem C.1). A conventional approach for the multi-objective problems is scalarization, namely, by first constructing a scalar reward and then applying a single-objective algorithm. However, this approach makes the solution highly dependent on the selected scalarization technique. Instead, our approach aims to learn a scale-independent multi-objective learning approach. Furthermore, the considered methodology enables us to analyze the convergence of the proposed algorithm.\n \n\u201c_Conditions of Corollary 1 should be stated more precisely._\u201d\n\nWe have modified this Corollary to more precisely express these conditions, according to the comment.\n\n\u201c_Does saving past model parameters raise any memory issues of the algorithm?_\u201d\n\nThe FB-MOAC algorithm does not rely on storing the whole history of the model parameters: only the previous parameter is needed for the purpose of policy update. Our implementation ran on a laptop and did not experience any memory-related issues during numerical simulations."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5148/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700257084166,
                "cdate": 1700257084166,
                "tmdate": 1700257328314,
                "mdate": 1700257328314,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "k8Ll4gdfta",
                "forum": "2boLXjsHsB",
                "replyto": "pdK5OtbOdL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5148/Reviewer_g9Ad"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5148/Reviewer_g9Ad"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the reply"
                    },
                    "comment": {
                        "value": "Thank you for your response.\n\nI carefully reviewed the feedback and observed that several concerns have been addressed in the revision.\n\nHowever, a significant concern still remains regarding \"Lemma 1 does not specify which Pareto-optimal point it converges to.\" In the field of multi-objective RL, there are two subareas: single-policy and multi-policy. When focusing on a specific scalarization function or utility, single-policy techniques are employed. If the goal is to obtain the entire approximation of the Pareto front (e.g., convex coverage set), then multi-policy techniques are adopted.\n\nIt seems that the proposed method does not align with either of these settings. On the one hand, the algorithm is proven to converge to 'a' Pareto-optimal point but does not specify which Pareto-optimal point it converges to, i.e., which utility (either linear or non-linear) it optimizes. In some cases, understanding the optimized utility is crucial. On the other hand, the proposed algorithm does not consider covering the entire approximation of the Pareto front. In certain scenarios, covering various Pareto points (with linear preference) is essential. Moreover, it is unclear what the main factor is that differentiates the converged optimal point and how much of the (true) Pareto front it can cover.\n\nIn summary, the proposed algorithm fails to (i) specify the characteristics of the converged Pareto point and (ii) guarantee coverage of (most of) the Pareto front. This aspect still raises concerns about the contribution of the paper."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5148/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700529820557,
                "cdate": 1700529820557,
                "tmdate": 1700529820557,
                "mdate": 1700529820557,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VZu5q0495x",
                "forum": "2boLXjsHsB",
                "replyto": "HyfWYi6D2P",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5148/Reviewer_g9Ad"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5148/Reviewer_g9Ad"
                ],
                "content": {
                    "title": {
                        "value": "Thanks"
                    },
                    "comment": {
                        "value": "Thanks for the additional comments. Although the comment attempted to address my concern about the contribution, I believe it is a repetition of the previous reply. The paper neither (i) specifies the non-decreasing utility regarding which the converged Pareto point optimizes nor (ii) guarantees coverage of (most of) the Pareto front \"without repeating the training procedure\". This aspect continues to raise concerns about the contribution of the paper.\n\nNonetheless, with the two replies, most of the other concerns have been appropriately addressed. Therefore, I have adjusted my rating accordingly."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5148/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700732185757,
                "cdate": 1700732185757,
                "tmdate": 1700732185757,
                "mdate": 1700732185757,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "YsaAk5dFQ0",
            "forum": "2boLXjsHsB",
            "replyto": "2boLXjsHsB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5148/Reviewer_AwDv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5148/Reviewer_AwDv"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces Forward-Backward Multi-Objective Reinforcement Learning (FB-MORL), an approach for addressing a special family of multi-task control problems. It presents the concept of the Forward-Backward Markov Decision Process (FB-MDP). Unlike the existing RL algorithms, this work primarily focuses on environments that cannot be modeled solely by a forward Markov decision process. This paper derives the policy gradient in the FB-MDP setting and proposes FB-MOAC, a stochastic actor-critic implementation of the policy gradient approach. In the policy update step, the author employs Monte Carlo Sampling (MCS) with an exponential moving average called episodic MCS-average to estimate the gradient. The proposed method is evaluated in wireless communication environments, and an ablation study highlights the importance of backward optimization and episodic MCS-average. The paper also provides theoretical analysis of FB-MOAC, demonstrating its convergence capability."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The main contributions of this paper are mainly two-fold:\n1. To the best of my knowledge, this paper offers the first formal study on the forward-backward MDPs and the resulting learning problem.\n2. Accordingly, from the perspective of multi-objective RL, this paper proposes FB-MOAC, which is a policy-gradient-based actor critic method. This is built on the derived policy gradient in the FB-MDP setting.\n- Convergence results of FB-MOAC are also provided (convergence to optimal total return under strict convexity and convergence to a local optimum in the general case).\n- FB-MOAC is empirically evaluated on a hybrid delivery environment against a randomized baseline. A brief ablation study is also provided."
                },
                "weaknesses": {
                    "value": "- The formulation of forward-backward MDP could be further justified. Despite that there is a whole line of research on backward SDEs, the use of the coupled forward and backward dynamics in the context of Markov processes and RL is quite rare and shall be justified. A motivating example early on in the paper would be helpful for readers to better appreciate this problem setting. \n\n- The FB-MOAC algorithm is mainly built on Lemma 3.1, which suggests a first-order iterative descent approach to solve the general multi-objective optimization problems. However, this does not guarantee that an optimal policy could be achieved under FB-MOAC in general. While the authors did provide some convergence results in Section 5.1 and Appendix B, the best that FB-MOAC could achieve is convergence to a first-order stationary point (as the RL objective in general is not strictly convex). \n\n- Based on the above, one important missing piece is the characterization of an optimal policy or the optimal value function in FB-MDPs. In the standard forward MDP, the widely known Bellman optimality equations offer a salient characterization of what an optimal value function and an optimal policy shall look like. It remains unclear what such conditions shall look like in the FB-MDP setting. In my opinion, further efforts on this is needed to design an algorithm that provably finds an optimal policy in general.\n\n- Another related concern is on the policy class needed in the FB-MDP setting. In the standard forward (tabular) MDP setting, it is known that the class of (randomized) stationary policies is large enough to contain an optimal policy. However, it is rather unclear whether this is still true in FB-MDPs. However, this paper appears to directly presume that the policy is stationary. More discussion on this would be helpful and needed.\n\n- Regarding the experiments of the hybrid delivery problem, while I could understand that this problem could be formulated as an FB-MDP (for keeping track of the latency), an alternative forward MDP formulation could still be used to solve this problem (for example, instead of keeping track of the latency of file n, the agent could look at the \u201ctime since the first transmission of file n\u201d, which could be compatible with the standard forward MDP). Then, one natural question is: is there any benefit of using an FB-MDP instead of the (simpler) forward MDP formulation?\n\n- Moreover, the experimental results are not very strong for two reasons: (1) It is unclear how far the performance of FB-MOAC is from an optimal policy (this is also related to one of my comments above). (2) The baseline (a simple randomized policy) is certainly not very strong. Given the plethora of research on unicast and multicast control in the wireless networking community, I would expect that there are some more competitive benchmark methods included in the experiments, either rule-based or learning-based. Also, it would be good to strengthen the experimental results by evaluating FB-MOAC on other RL environments. Otherwise, the application scope of FB-MOAC is somewhat limited."
                },
                "questions": {
                    "value": "Please see the above for the main questions.\n\nHere are some additional detailed questions:\n- In Eq. (18), shall the 1/2 in the second term be just 1 (as Ln(t) denotes the latency)?\n\n- While the whole Section 5.2.1 is used to describe the environment setup, it is still somewhat a bit difficult to parse (probably due to the use of terminology). For example, \n    - How to define the event of multicast outage? \n    - And accordingly how to derive the probability of this event? \n    - How does the \u201crequest process\u201d of each user work? \n\n- The referencing numbers in the caption in Figure 1 and Figure 2 appear misplaced."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5148/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699116209051,
            "cdate": 1699116209051,
            "tmdate": 1699636508666,
            "mdate": 1699636508666,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "93jr62w7Ku",
                "forum": "2boLXjsHsB",
                "replyto": "YsaAk5dFQ0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5148/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5148/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "\u201c_forward-backward MDP could be further justified_\u201d\n\nWe have included an example in the context of computation offloading in the revised submission to motivate the applicability of FB-MDPs to a different scenario. Specifically, mobile devices offload computationally intensive tasks to an edge server, which processes them according to its computational capacity. The edge server has a buffer to store the tasks that cannot be immediately processed, which happens when there is no spare computational capacity. The new example has been mentioned in the introduction and detailed in the appendix.\n\n\u201c_Lemma 3.1 suggests a first-order iterative descent_\u201d\n\nWe appreciate the remark and the opportunity to clarify. The convergence of optimization algorithms based on this Lemma can be proven under the assumption of Lipschitz-smooth functions (Fliege et al., 2019)*. Our paper proves that for two cases: convexity and smoothness alone. Specifically, a locally Pareto-optimal solution with a convergence rate of $O(1/\\sqrt{K})$ for the Lipschitz-smooth case is guaranteed, where $K$ is the number of updates. Section 4.1 provides a concise summary of these results, whereas Appendix C details the related derivation.\n* J. Fliege, A. I. F. Vaz & L. N. Vicente. \u201cComplexity of gradient descent for multiobjective optimization,\u201d Optimization Methods and Software, 34:5, 949-959, 2019.\n\n\u201c_Bellman optimality equations offer a salient characterization_\u201d\n\nThe Bellman optimality equation characterizes the solution of dynamic programming algorithms and our work targets RL algorithms for action-coupled FB-MDPs. However, we here derive an alternative equation for FB-MDPs, namely, the Bellman Pareto-optimality equation. Please refer to the Lemma 3.1 of the revised manuscript and the following Remark.\n\n\u201c_the class of stationary policies is large enough to contain an optimal policy_\u201d\n\nWe consider the forward and backward transition probabilities as stationary. We also assume that the forward and backward rewards are only coupled within the action space. Hence, the forward and backward value functions are stationary. Therefore, designing an optimal policy does not need to consider the time or the previous history of forward states. The same also holds for the backward reward. The evolution of the backward state is optimally determined by the forward state throughout the action space. As a result, a non-stationary policy distribution is not beneficial to jointly optimize the forward and backward rewards. \n\n\u201c_forward MDP formulation could still be used to solve this problem_\u201d\n\nThanks for your suggestion. The backward reward is a part of the environment in the considered experiment. Therefore, replacing only that with another one would lead to a sub-optimal policy. In line with your suggestion, we consider an additional formulation to replace the backward dynamics. For this, we utilize this fact that minimizing both the outage probability and time-slot duration result in minimizing the overall latency, based on Eq.(19). Please refer to the revised manuscript for the additional results of the new formulation.\n\n\u201c_ experimental results are not strong _\u201d\n\nWe point out that the experiments already considered two different cases: a hybrid and a multicast scheme. They differ from each other in terms of the system model, action parameters, reward function, and MDPs. Moreover, we do agree that the random approach is not a compelling baseline. Hence, we have included two additional schemes for comparison purposes: Least Frequently Used (LFU), a rule-based approach widely used in the literature on caching; and F-MDP, a learning-based algorithm that replaces the backward MDP with the forward-only formulation mentioned above.\n\n\u201c_In Eq.(18), shall the 1/2 in the second term be just 1_\u201d\n\nThanks for the precise comment. The 1/2 factor is used under the assumption that requests in a time-slot are submitted according to a uniform distribution, corresponding to completely uncoordinated operations among users.\n\n\u201c_How to define the event of multicast outage?_\u201d\n\nThe multicast outage happens when a user cannot successfully receive the requested file because the received signal quality is too low. The probability that such an outage occurs is given by $\\mbox{Pr} ( B \\log_2(1+\\gamma)< R )$, where $\\gamma$ is the Signal-to-Noise Ratio associated with the user, $B$ the bandwidth associated with the multicast scheme, and $R$ the desired transmission rate.\n\n\u201c_how to derive the probability_\u201d\n\nA closed-form expression of the probability has been obtained in (Amidzadeh et al., 2022). The derivation relies on leveraging stochastic geometry under the assumption that the base-stations are distributed according to a Poisson process. \n\n\u201c_How does the \u201crequest process\u201d of each user work?_\u201d\n\nThe users send requests to retrieve files towards the BSs by uplink transmissions. Each BS is responsible to deliver the file to the users associated with it and by using the conventional unicast scheme."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5148/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700257022279,
                "cdate": 1700257022279,
                "tmdate": 1700257022279,
                "mdate": 1700257022279,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]