[
    {
        "title": "Vision-Language Models Provide Promptable Representations for Reinforcement Learning"
    },
    {
        "review": {
            "id": "YCh26JGSxU",
            "forum": "DQCZiKb3Uy",
            "replyto": "DQCZiKb3Uy",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6621/Reviewer_AR8Q"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6621/Reviewer_AR8Q"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes a general task-related visual representation as input to the policy, aiming to improve the efficiency of reinforcement learning. By inputting task-related information (including auxiliary information) into VLM as prompts, efficient fusion with the current visual state is achieved. This paper verifies the effectiveness of this approach through experiments."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "[1] The method proposed in this paper, PR2L, is very straightforward, and the story sounds reasonable. The organization of the paper is very clear, and it is polished well. \n\n[2] The experimental analysis is quite comprehensive, and ablation experiments have demonstrated the effectiveness of each part of the proposed method. Conducting experiments in a challenging environment like Minecraft is persuasive."
                },
                "weaknesses": {
                    "value": "[1] Concerns about computational cost. Obtaining promptable representation requires running a complex VLM on every image returned from the environment, along with the prompt and answer. This cost is prohibitively high and not practical.\n\n[2] Concerns about the performance of the learned policy. The visually rich representation obtained at such a high computational cost should greatly improve the performance of the policy. However, the authors did not provide videos of rollouts on relevant tasks, making it difficult to judge the effectiveness of the promptable representation in a real Minecraft environment (the three tasks used in the paper are not very complex). \n\n[3] Insufficient literature review. Since the authors conducted experiments in Minecraft, they should have provided a more comprehensive discussion of articles that control and plan within Minecraft. However, the authors left out the following important literature. \n\n1. Open-world multi-task control through goal-aware representation learning and adaptive horizon prediction. \n2. Video pretraining (vpt): Learning to act by watching unlabeled online videos. \n3. Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents. \n4. CLIP4MC: An RL-Friendly Vision-Language Model for Minecraft. \n5. GROOT: Learning to Follow Instructions by Watching Gameplay Videos. \n6. Learning from Visual Observation via Offline Pretrained State-to-Go Transformer."
                },
                "questions": {
                    "value": "My main concerns are presented in the \"Weaknesses\" box, please refer to it."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6621/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6621/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6621/Reviewer_AR8Q"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6621/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698143988892,
            "cdate": 1698143988892,
            "tmdate": 1699636755872,
            "mdate": 1699636755872,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "o7q5oREBNJ",
                "forum": "DQCZiKb3Uy",
                "replyto": "YCh26JGSxU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6621/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6621/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your detailed feedback and for a positive assessment of our work. We have addressed your main concerns by adding the suggested related works, video rollouts and an explanation on why the computational cost of our method is actually much more practical than prior works that typically train entire VLMs. In the following we will answer your questions in detail.\n\n> Concerns about computational cost. Obtaining promptable representation requires running a complex VLM on every image returned from the environment, along with the prompt and answer. This cost is prohibitively high and not practical.\n\nWe agree that the computational complexity of large VLMs is a major limiting factor for their adoption into control tasks and that lowering the compute required is difficult, barring architectural changes or lowered precision. Nevertheless, several prior works have used a large (V)LM in an RL loop [ https://arxiv.org/abs/2305.13301, https://llm-rl.github.io/static/images/llarp.pdf ] or even used a full pretrained VLM to initialize a policy (where the VLM is trained as part of the policy) [ https://arxiv.org/abs/2307.15818 ]. Compared to the latter, our method actually requires less compute, as it does not require training an entire VLM (only the lightweight policy network head). \nWe also find that PR2L is reasonable in terms of wall-clock speed, achieving ~4 Hz inference speed (sufficient for playing Minecraft in real time). Training speed is also ~4 Hz, as gradients do not flow through the VLM (only the small policy network, which adds much less overhead). Moreover, training wall clock time is further improved in offline approaches, as all offline data can be embedded by VLMs in parallel and then reused in future epochs or runs. Again, this does not lower the compute required, but drastically decreases training time. We found this to be true in our new BC experiments (see Appendix E.1), wherein we trained near-expert level policies with comparatively small training time/data and used multiple VLMs to parallelize observation embedding generation.\n\n> Concerns about the performance of the learned policy. The visually rich representation obtained at such a high computational cost should greatly improve the performance of the policy. However, the authors did not provide videos of rollouts on relevant tasks, making it difficult to judge the effectiveness of the promptable representation in a real Minecraft environment (the three tasks used in the paper are not very complex).\n\nWe include new videos visualizing 10 combat spider rollouts (here)[ https://drive.google.com/drive/folders/1GFN4C6qAEciGEK3phcqgYs3V-HxJpAOy?usp=sharing ] for both our approach and the image encoder baseline (achieving 7 and 4 successes respectively). We do note that it\u2019s difficult to observe qualitative differences between the PR2L and baseline policies \u2013 they are more clear in reported aggregate performance metrics.\n\n> Insufficient literature review. Since the authors conducted experiments in Minecraft, they should have provided a more comprehensive discussion of articles that control and plan within Minecraft. However, the authors left out the following important literature.\n\nThank you for these suggested citations \u2013 we have included them in the extended literature review in Appendix G.\n\nPlease let us know if we have addressed your concerns, or if there is anything else we should consider. Otherwise, we would be extremely grateful if you could raise our score!"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6621/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700294394491,
                "cdate": 1700294394491,
                "tmdate": 1700506082159,
                "mdate": 1700506082159,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "93GqhVdXFG",
                "forum": "DQCZiKb3Uy",
                "replyto": "YCh26JGSxU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6621/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6621/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer AR8Q, \n\nWe were wondering if you have had a chance to read our reply to your feedback. As the time window for the rebuttal is closing soon, please let us know if there are any additional questions we can answer to help raise the score!"
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6621/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700514241789,
                "cdate": 1700514241789,
                "tmdate": 1700514241789,
                "mdate": 1700514241789,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "oo0kELg1Ca",
                "forum": "DQCZiKb3Uy",
                "replyto": "o7q5oREBNJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6621/Reviewer_AR8Q"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6621/Reviewer_AR8Q"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your responses. I appreciate the submitted rollout videos, it helps me to understand how the agent works. I agree that the proposed method is interesting and reasonable. I understand the authors have done a lot to speed up the training. However, the computational cost is still my major concern. 4Hz is too slow to scale up this method to large-scale tasks in Minecraft. After careful consideration, I decided to hold my original rating (5)."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6621/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700535383092,
                "cdate": 1700535383092,
                "tmdate": 1700535383092,
                "mdate": 1700535383092,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bGoRsx32tB",
                "forum": "DQCZiKb3Uy",
                "replyto": "YCh26JGSxU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6621/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6621/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your prompt response! We are glad that you find our method interesting. We would first like to clarify that our goal in this paper was to demonstrate our method PR2L can provide for an effective way to extract useful representations from pre-trained VLMs for downstream reinforcement learning. This goal realizes our long-term vision of using off-the-shelf pre-trained VLMs for RL, even though these models were never trained on data from the specific domain. As general-purpose VLMs start to become useful,  we believe it is crucial to develop approaches for extracting representations from them, targeted towards downstream RL, even though this comes at a larger inference cost with currently available VLMs. While we do agree that these computational costs can be reduced specifically for the domain of Minecraft, **we reiterate that the goal of this paper is not to develop better approaches for Minecraft, but to develop methodology to utilize pre-trained VLMs in RL.** Developing approaches for utilizing VLMs for downstream control has been an important theme in recent works (e.g., [RT-2](https://arxiv.org/abs/2307.15818), [InstructBLIP for Robotics](https://arxiv.org/pdf/2309.02561.pdf)), despite the associated computational costs.\n\n**Specifically, for our experiments, please note that a frequency of 4 Hz is already better than prior works that combine VLMs and RL:** [RT-2](https://arxiv.org/abs/2307.15818) uses a 65B parameter model and only achieves a worse 1-3 Hz control frequency on a real robot, which they still find to be sufficient for the real world. \n\n**Likewise, we use less compute and achieve greater sample efficiency than similar works:** As with our online RL experiments, [LLaRP](https://arxiv.org/abs/2310.17722)\u2019s most performant model calls LLaMA-13B for 2e8 environment steps, thereby using a larger model (13B vs. our 7B parameters), with a longer history-dependent context length, for 1000 times more steps than PR2L. These prior approaches incur significantly larger computational costs, but are of interest to the community as they integrate foundation models with downstream RL / control algorithms. \n\nIn addition, please note that when PR2L is applied with BC, as we demonstrate in our [newly added experiments](https://ibb.co/MRHrv03), the computational cost of encoding collected offline observations with the pre-trained VLM is incurred only once and can be parallelized. Encoding observations in the offline dataset takes ~2 hours with two VLMs (and only needs to be done once), which is comparable to running BC training itself (which takes around an hour for each run).\n\n**We would appreciate it if you are willing to upgrade your rating in the light of these clarifications. We are happy to discuss further. Thank you so much!**"
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6621/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700546580971,
                "cdate": 1700546580971,
                "tmdate": 1700546730780,
                "mdate": 1700546730780,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "xI0hli5jGN",
            "forum": "DQCZiKb3Uy",
            "replyto": "DQCZiKb3Uy",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6621/Reviewer_Cm8x"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6621/Reviewer_Cm8x"
            ],
            "content": {
                "summary": {
                    "value": "Paper presents an approach to provide task-relevant visual representations for RL, especially in an open-world environment.The main idea is to take a pre-trained VLM (vision-language model), feed it with the current visual observation, and a meticulously picked prompt about the current task, and then use the resulting embeddings produced by the last two layers of the VLM transformer as the representation. The authors also propose some additional techniques that could be helpful: 1) the VLM has to generate some text out of the prompt and use the embedding to correspond to both the prompt and the produced text, not just the embedding of the prompt only; 2) prompt engineering; 3) an encoder-decoder transformer is used as the policy to distill the representations into a summary embedding. Experiments and ablations on three Minecraft tasks show some promises."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+The study is relevant and could be of interest to many audiences with a background in large models, reinforcement learning, and representation learning.\n\n+The method is well-motivated and technically sound. Pretrained VLM indeed provides open vocabulary and even knowledge-aided representations for multimodal input, which can be quite beneficial to open-world environments. Plus, it is plausible to tweak the representation further via prompting. This is a very neat idea.\n\n+The results look impressive. Although the method is only evaluated on limited (3) tasks in a single environment (Minecraft), the advantages over the baselines and ablative approaches are significant. I do think the authors did a good job of comparing it against several interesting baselines, including no generation, no prompt, etc."
                },
                "weaknesses": {
                    "value": "Having said those above, I do have some major concerns about the evaluation part of this paper. I also would like to point out some minor issues as well.\n\n-Albeit the promises shown by the results on 3 tasks on Minecraft, I don't think the approach is thoroughly evaluated, especially given their claim on \"leverage contextual prior information\" and \"visually-complex RL tasks\" (see abstract). I have the following suggestions:\n\n1) There are some other approaches that are designed to tackle similar issues, especially in Minecraft, ex. [1,2,3]. Although I agree some settings could be different (RL vs. IL, etc), they all deliver some backbone design or objective functions that could facilitate better representations. Comparisons against these missing baselines would help the reader with a better understanding of the significance of the proposed method.\n\n2) Minecraft is indeed a challenging domain in terms of open-world and complex visual observations. However, the tasks being evaluated here (spider, cow, sheep) do not seem to be challenging enough to justify the effectiveness of the proposed method, especially on the claimed \"leverage contextual prior information\". These mobs are indeed very common and the tasks themselves do not seem to involve complex visual stimuli. My suggestion is to try some long-term and open-ended tasks like surviving, collecting items, etc. [7] offers a few of them and worth taking a look at.\n\nMinor: some references on open-world representation learning and Minecraft agents should be cited: [1-6].\n\n\n[1] open-world control: https://arxiv.org/abs/2301.10034\n\n[2] VPT: https://arxiv.org/abs/2206.11795\n\n[3] STG transformer: https://arxiv.org/abs/2306.12860\n\n[4] DEPS: https://arxiv.org/abs/2302.01560\n\n[5] Plan4MC: https://arxiv.org/abs/2303.16563\n\n[6] GITM: https://arxiv.org/abs/2305.17144\n\n[7] MCU: https://arxiv.org/abs/2310.08367"
                },
                "questions": {
                    "value": "-In Figure 2 and 3, why do some curves not have shadows?\n\n-Some prompts shown in Table 1 require hand-crafted domain knowledge, ex. \"Spiders in Minecraft are black\". Is is possible to avoid this?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6621/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698482236372,
            "cdate": 1698482236372,
            "tmdate": 1699636755676,
            "mdate": 1699636755676,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Ivf3MO1Lai",
                "forum": "DQCZiKb3Uy",
                "replyto": "xI0hli5jGN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6621/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6621/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your detailed feedback and for a positive assessment of our work. We have addressed your main concerns by adding offline imitation learning experiments and more baselines (R3M, VC-1), to which our method favorably compares. In the following we will answer your questions in detail.\n\n> There are some other approaches that are designed to tackle similar issues, especially in Minecraft, ex. [1,2,3]. Although I agree some settings could be different (RL vs. IL, etc), they all deliver some backbone design or objective functions that could facilitate better representations. Comparisons against these missing baselines would help the reader with a better understanding of the significance of the proposed method.\n\nThank you for the suggestion! We have run additional baselines (using VC-1 or R3M as encoders or giving the policy ground-truth entity detection) and have found PR2L outperforms all of them in all tasks. Additionally, we have also trained policies with BC on top of PR2L and image encoder representations, finding that PR2L especially excels at the \u201ccombat spider\u201d task (doubling the baseline\u2019s success rate and achieving near-expert performance in only one epoch).\n\nAdditionally, our primary goal was not to build a SOTA system for Minecraft tasks, but to show that the representations produced by prompting a VLM with task context are better for learning control tasks than equivalent, non-prompted representations.\n\nIn principle, our approach can be combined with other approaches for improving performance in Minecraft tasks (e.g., using a better reward function like MineCLIP or CLIP4MC). However, as we wanted to isolate performance differences to better representations elicited via prompting, we did not include such approaches. \n\n> Minecraft is indeed a challenging domain in terms of open-world and complex visual observations. However, the tasks being evaluated here (spider, cow, sheep) do not seem to be challenging enough to justify the effectiveness of the proposed method, especially on the claimed \"leverage contextual prior information\". These mobs are indeed very common and the tasks themselves do not seem to involve complex visual stimuli. My suggestion is to try some long-term and open-ended tasks like surviving, collecting items, etc. [7] offers a few of them and worth taking a look at.\n\nAs we just wished to investigate the effectiveness of promptable representations (rather than developing an agent that is generally better at playing Minecraft), we chose to evaluate on the same task suite that the original MineDojo paper because they were solved by simple RL algorithms (off-the-shelf PPO) with little Minecraft-specific system engineering, meaning that our approach could likewise be less Minecraft-specific. We have likewise run PR2L and the VLM image encoder baseline on all five of the remaining programmatic tasks that MineDojo evaluated on to complete the analysis. For longer-horizon or more open-ended tasks, we likely would need to adopt similar approaches to the ones presented in your suggested citations. \n\n> Some prompts shown in Table 1 require hand-crafted domain knowledge, ex. \"Spiders in Minecraft are black\". Is it possible to avoid this?\n\nWe find that the spider task\u2019s domain knowledge is helpful, but not necessary. Removing it degrades performance slightly, but still outperforms using the non-promptable VLM image encoder representations (green vs. blue curves in Figs. 4 and 7\u2019s combat spider plots). However, it does even better if this auxiliary text is included (red curve). We view the ability for VLMs to use auxiliary text as an advantage: it allows them to recognize stylized Minecraft spiders despite them being visually different from most natural images of spiders seen during pre-training. which intuitively explains why giving it this domain knowledge is helpful.\n\nAdditionally, we recognize that re-running PR2L to optimize the prompt is computationally expensive. However, we find that the simple prompt evaluation scheme in Sec 5.2 and Appendix A is an effective proxy measure for how good a prompt is for PR2L.Finally, we also acknowledge that this evaluation requires domain-specific data, but note that it uses a miniscule amount of it compared to fully fine-tuning a model \u2013 we collected all prompt evaluation data in under an hour.\n\n[Continued in follow-up response due to character limit]"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6621/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700295548178,
                "cdate": 1700295548178,
                "tmdate": 1700295600770,
                "mdate": 1700295600770,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "J9ke9Ifejh",
                "forum": "DQCZiKb3Uy",
                "replyto": "xI0hli5jGN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6621/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6621/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer Cm8x, \n\nWe were wondering if you have had a chance to read our reply to your feedback. As the time window for the rebuttal is closing soon, please let us know if there are any additional questions we can answer to help raise the score!"
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6621/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700514215566,
                "cdate": 1700514215566,
                "tmdate": 1700514215566,
                "mdate": 1700514215566,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JD9wYk6KnZ",
                "forum": "DQCZiKb3Uy",
                "replyto": "J9ke9Ifejh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6621/Reviewer_Cm8x"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6621/Reviewer_Cm8x"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the reply"
                    },
                    "comment": {
                        "value": "Thank you for the detailed reply. My concerns on baselines, handcrafted knowledge, plots, and citations are basically addressed.\n\nMy concern about the task complexity remains. I can understand the need for evaluating tasks that can be solved by RL, but this seems to contradict your claim on \"leverage contextual prior information\", which I believe, requires evaluation on more challenging tasks.\n\nImitation learning seems to be a better choice for more challenging tasks. Some other domains, ex. Atari, could be worth a try, as they also offer rich visual stimuli and likely can benefit from \"contextual prior information\"."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6621/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700560923015,
                "cdate": 1700560923015,
                "tmdate": 1700560923015,
                "mdate": 1700560923015,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PLrAUCb6XN",
                "forum": "DQCZiKb3Uy",
                "replyto": "xI0hli5jGN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6621/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6621/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the prompt reply! We are glad we were able to address most of your concerns. We agree that there are more complex Minecraft tasks that could be tested on. Nevertheless, we find that the tasks we considered (all from the MineDojo paper) still pose an interesting and non-trivial challenge for reinforcement learning. In particular, we note that the original MineDojo paper found they needed a more complex learning algorithm than pure RL with PPO \u2013 namely, they require self-imitation learning to achieve good performance on shear sheep and milk cow (their approach fails otherwise, as shown in [Figure A.8](https://ibb.co/x2cSz7Q) of that paper). \n\nWe empirically confirm that RL on top of MineCLIP does poorly on these tasks in particular, despite being trained to produce representations on domain-specific data (in contrast to our VLMs, which are not trained on Minecraft data). **Since these RL tasks are non-trivial even for more complex agent architectures/training algorithms that use domain-specific representations, we believe that our evaluation and contribution \u2013 showing a way to extract representations from _general purpose_ VLMs that match or significantly exceed the performance of domain-specific ones \u2013 is still valuable.**\n\nThis level of performance is not present when using the VLM image encoder baseline, which suggests that the performance increase is because of the LLM parsing the image encoder\u2019s representations based on its prior understanding of (1) what things look like and (2) how that relates to task context, provided as a prompt. **We show in our follow-up BC experiments that this results in representations that are particularly conducive to learning good actions** (with our combat spider agent achieving [expert-level performance after 1 epoch, doubling the success rate of the VLM image encoder baseline](https://ibb.co/MRHrv03)).\n\nAn additional point regarding evaluating on other RL benchmarks such as Atari or dm control is that, as our method does not fine-tune a VLM on domain specific data (as prior works do), we are constrained to environments that produce somewhat more realistic observations, as the VLMs are trained on natural images.\n\nWhile we believe this all demonstrates how RL performance can be improved by using VLM representations that \u201cleverage contextual prior information,\u201d we would be happy to change the wording of that claim to something you believe to be a more accurate description of the above phenomenon \u2013 e.g., we could say that they \u201cdraw out relevant representations based on task context\u201d (or some other suggested phrasing). Otherwise, if we have addressed your concern, we would be extremely grateful if you increase your rating to reflect this!"
                    }
                },
                "number": 24,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6621/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700601459017,
                "cdate": 1700601459017,
                "tmdate": 1700601500293,
                "mdate": 1700601500293,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "nZPUgdAv07",
            "forum": "DQCZiKb3Uy",
            "replyto": "DQCZiKb3Uy",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6621/Reviewer_qGuw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6621/Reviewer_qGuw"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces promptable representations for reinforcement learning (PR2L), which uses the semantic features of pre-trained VLM as state representation for reinforcement learning; the main advantage of PR2L to other pre-trained representations is that PR2L allows extract task-specific features from a generic pre-trained models by injecting task knowledge via prompting. PR2L outperforms both domain-specific representations and instruction-following methods on several tasks in MineCraft domain."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "PR2L presents an interesting and creative way of utilizing pre-trained VLMs as representations for visual policy learning. It is unlike prior pre-trained representations for control work in which the features are generic (i.e., directly encoding the image observation); it is also different from recent Vision-Language-Action work (Brohan et al., 2023) in that it does not require fine-tuning a pre-trained VLM and enable high-frequency policies that are disentangled from the VLM backbone. \n\nAs VLMs are increasingly adopted in decision-making pipelines, PR2L is a timely work that presents a lightweight and simple alternative to the existing literature. \n\nThe paper itself is generally well-written and free of grammatical errors."
                },
                "weaknesses": {
                    "value": "This paper's weaknesses mainly lie in its experimental methodologies. \n\n1. The only form of prompt that PR2L uses essentially amounts to object detection in the scene. This introduces a confounding factor of whether PR2L outperforms baselines because it is able to recognize objects better in the scene. \n\n2. The paper claims that the prompts are different from instruction following; however, the prompts are still manually constructed and task-specific. It's unclear the advantage of doing so as instructions, by construction, should exist as it is a direct form of task specification.\n\n3. The improvements of PR2L over its various ablations appear only moderate. Furthermore, the best prompt format for the tasks are not consistent; for Spider, it is helpful to include contextual information of what a spider looks like in MineCraft, whereas for the other two tasks, it is more helpful to disregard such information. Therefore, applying PR2L to a new task may require substantial prompt engineering for the best performance.\n\n4. PR2L is only evaluated on 3 tasks; these tasks are also the simplest in the MineDojo benchmark. The paper would be strengthened if more tasks and domains are evaluated. Currently, it is not convincing that PR2L can be generally applied to other visuomotor control domains. Relatedly, PR2L does not outperform MineCLIP on most tasks; given that MineCLIP exists and is open-sourced, PR2L's stated advantages can be better demonstrated via a new domain in which foundation pre-trained representations do not already exist. \n\n5. BLIP-2's vision encoder may not be the strongest baseline for pre-trained vision encoders. Several prior works such as VC-1, R3M, MVP, VIP are trained for decision-making and robotics tasks and may constitute stronger baselines in that category."
                },
                "questions": {
                    "value": "1. Could a baseline that somehow incorporates oracle object detection information be included? This will test whether PR2L does better because it detects the object of task interest in the scene.\n\n2. Could more tasks and qualitatively different prompts be tested in the paper? Ideally, some tasks in MineCraft requires more than just object detection as auxillary information that may be implicitly captured by a VLM.\n\n3. Could additional pre-trained vision encoder baselines be included? \n\nI am willing to improve my assessment of the paper if these questions as well as the points in the Weakness section can be adequately addressed."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6621/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6621/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6621/Reviewer_qGuw"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6621/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698589937830,
            "cdate": 1698589937830,
            "tmdate": 1700671052844,
            "mdate": 1700671052844,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "lGpFFKemTF",
                "forum": "DQCZiKb3Uy",
                "replyto": "nZPUgdAv07",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6621/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6621/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your detailed feedback. We have addressed your main concerns by visualizing the representations of instructions vs. prompts and running the suggested baselines with oracle object detection and running with image encoders (R3M, VC-1) that are more suited to embodied control, to which PR2L compares favorably. In the following we will answer your questions in detail.\n\n> The only form of prompt that PR2L uses essentially amounts to object detection in the scene. This introduces a confounding factor of whether PR2L outperforms baselines because it is able to recognize objects better in the scene.\nCould a baseline that somehow incorporates oracle object detection information be included? This will test whether PR2L does better because it detects the object of task interest in the scene.\n\nThank you for the suggestion! To address the concern regarding object detection, we train baselines with oracle object detection and find that they perform worse than PR2L in all cases as seen [here](https://ibb.co/PrkhKFB) or Appendix E.2. Additionally, while we agree that our prompts essentially involve task-specific object detection, it's essential to highlight that this aligns with the specific requirements of our considered tasks. Detecting spiders, cows, and sheep in observations is a critical feature for accomplishing said tasks. The advantages of PR2L is that prompting (1) specifies what entities are important to detect for a task and (2) improves representations + detection of potentially OOD entities (like cartoony Minecraft spiders), obviating the need for domain-specific classifiers/representations by using general-purpose VLMs.\n\n> Relatedly, PR2L does not outperform MineCLIP on most tasks; given that MineCLIP exists and is open-sourced, PR2L's stated advantages can be better demonstrated via a new domain in which foundation pre-trained representations do not already exist.\n> BLIP-2's vision encoder may not be the strongest baseline for pre-trained vision encoders. Several prior works such as VC-1, R3M, MVP, VIP are trained for decision-making and robotics tasks and may constitute stronger baselines in that category.\n> Could additional pre-trained vision encoder baselines be included?\n\nThank you for the suggestion, to address the concern regarding more baseline comparisons, we trained additional baselines on VC-1 and R3M and found that they perform worse than PR2L in all cases (see [the prior link](https://ibb.co/PrkhKFB)). We note that VC-1 is the most recent of these models, and seems to be more performant than MVP and VIP, so we train on its representations rather than the latter two.\n\nWe chose to use InstructBLIP\u2019s image encoder not because it was the best encoder for Minecraft, but to **illustrate that representations that are suboptimal for control are improved by passing them through an LLM with the task-relevant prompt**. The observed performance improvement of PR2L over the encoder baseline can thus entirely be attributed to the prompt extracting task-relevant information from the subpar generic encoder representations.\n\n> The paper claims that the prompts are different from instruction following; however, the prompts are still manually constructed and task-specific. It's unclear the advantage of doing so as instructions, by construction, should exist as it is a direct form of task specification.\n\nThanks for bringing this up. We agree that instruction-following and our prompts both specify the task and require hand-designing. However, the former often cannot be done by our chosen VLM zero-shot, resulting in lower quality representations. This is reflected in the lower performance when the VLM is given instructions in our RT-2-style baseline.\n\nFurthermore, we find that the distribution of representations the VLM yields is very different depending on if it\u2019s prompted with instructions or our approach, with the latter seemingly being more predictive of \u201cgood\u201d actions to take. We observe this by plotting the first 2 principal components of the VLM features of observations from an expert policy when given instructions or our prompt, as shown [here](https://ibb.co/p1CL8Nx). We find that the representations from our prompt are bimodal (as the VLM usually answers \u201cyes\u201d or \u201cno\u201d) and most expert functional actions (attacking or using items) that result in rewards correspond to embeddings when the VLM answers \u201cyes\u201d (the left-hand clusters). This is most clear for milk cow \u2013 see the large orange points. In contrast, the PCA plots of instruction representations are much less structured; rewarding functional actions are much closer to the overall mean, and seem qualitatively harder to separate out and identify.\n\n**In summary: with our prompts, VLM representations of observations correspond strongly to actions that yield rewards for our considered tasks. This is not true of representations from instructions**. More details can be found in Appendix F.\n\n[Continued in follow-up response due to character limit]"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6621/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700295389910,
                "cdate": 1700295389910,
                "tmdate": 1700295389910,
                "mdate": 1700295389910,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DLPTSBgQaX",
                "forum": "DQCZiKb3Uy",
                "replyto": "nZPUgdAv07",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6621/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6621/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer qGuw, \n\nWe were wondering if you have had a chance to read our reply to your feedback. As the time window for the rebuttal is closing soon, please let us know if there are any additional questions we can answer to help raise the score!"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6621/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700514192711,
                "cdate": 1700514192711,
                "tmdate": 1700514192711,
                "mdate": 1700514192711,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "31zik2GnLG",
                "forum": "DQCZiKb3Uy",
                "replyto": "nZPUgdAv07",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6621/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6621/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer qGuw,\n\nAs the rebuttal window closes tomorrow, we would again like to ask if you have had a chance to consider our reply to your feedback. Please let us know if you have any additional concerns we can address to help raise the score!"
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6621/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700600466655,
                "cdate": 1700600466655,
                "tmdate": 1700600466655,
                "mdate": 1700600466655,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9dEnyF9Y25",
                "forum": "DQCZiKb3Uy",
                "replyto": "31zik2GnLG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6621/Reviewer_qGuw"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6621/Reviewer_qGuw"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Authors,\n\nThank you for your responses and I have carefully read through them. Most of my concerns have been adequately address except the concern over \"the best prompt format for the tasks are not consistent\" and that only one additional environment has been added. In addition, it's not entirely clearly to me why the benefit of the proposed approach appears to show only later during training. That said, despite the room for improvement for the empirical results and their analysis, I do think that the paper has been strengthened and presents an interesting and novel idea that can be explored further in research, so I have decided to increase my score."
                    }
                },
                "number": 26,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6621/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700671034058,
                "cdate": 1700671034058,
                "tmdate": 1700671034058,
                "mdate": 1700671034058,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "BZDCpA1kdy",
            "forum": "DQCZiKb3Uy",
            "replyto": "DQCZiKb3Uy",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6621/Reviewer_sWjV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6621/Reviewer_sWjV"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a simple but effective approach of improving visual embeddings from pre-trained VLMs by providing VLMs with useful prompts. For example, to detect spider in Minecraft, instead of directly encoding the scene using a vision encoder, the authors extract more meaningful representations from the VLM by giving auxiliary information like \u201cSpiders in Minecraft are black. Is there a spider in the image?\u201d. This can help the VLM to focus on task-specific/domain-specific information and produce more meaningful embeddings that are useful for training a RL policy. The authors show that the proposed approach outperforms policies trained using generic image features from a vision encoder ( on 3/3 tasks), as well as domain specific image features (on 2/3 tasks)."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The main contribution of the paper \u201cPrompting VLM via auxiliary information and task context\u201d allows extracting more meaningful representation from a VLM is quite interesting and easily applicable to a range of tasks. Instead of fine-tuning VLM for specific domains, it\u2019s easier to plug-and-play existing VLM and extract meaningful representations via prompting.\n- Overall, the paper is well written and is systematic in its evaluation. I also appreciate the authors willingness to address concerns preemptively (lack of visual tokens as input to the policy MLP, not fine-tuning VLM similar to RT-1)."
                },
                "weaknesses": {
                    "value": "- Given that the approach is using a VLM, it\u2019d be nice to test the model for \u201cunseen\u201d tasks, containing objects and instructions not seen during training. For instance, does the policy generalise form \u201cCombat Spider\u201d to \u201cCombat Zombie\u201d?\n- I also recommend a stronger evaluation on Minecraft benchmark consistent with the evaluations done in MineDOJO. Currently, the paper shows result on only three tasks. For a more exhaustive evaluation, MineDOJO recommends evaluation on a collection of starter tasks (32 programmatic and 32 creative tasks).\n- While I understand that the authors didn\u2019t have the resources to train a RT-1 style baseline, would it still be possible to train an action decoder on top of the VLM to produce actions. I think having a strong RT-1 style baseline is very important to properly evaluate the question (2) mentioned in the paper \u2014 \u201cHow does PR2L compare to approaches that directly \u201cask\u201d the VLM to generate the best possible actions for a task specified in the prompt?\u201d\n\nOverall, I liked the main contribution of the paper. But I believe, in its current form, the evaluation in the paper is a bit weak and could be made more exhaustive (unseen tasks, more exhaustive MineCraft evaluation).\n\n\n**Update**: After reading the rebuttal, most of my questions are adequately answered. I still believe that directly training an action decoder (while keeping VLM frozen is a good baseline) and should be included in the paper. I also feel that the paper will be stronger if they include more exhaustive minecraft experiments (to check for generalisation on unseen tasks / objects) and more environments which are visually more complex like Habitat / AI2 Thor. Based on authors response, I am increasing my score."
                },
                "questions": {
                    "value": "Apart from questions asked in the weaknesses section, I have additional questions: \n\n- The proposed architecture compresses the task-relevant features from the VLM into a single CLS token which can severely restrict the information available to the policy. While this may work for simpler environments like Minecraft which doesn\u2019t have a lot of clutter, it might not work for other environments / tasks (rearrangement tasks in indoor environments). Did the authors try an approach like Perceiver IO (Jaegle et al, 2021) which encodes N tokens to K (1\u2264K\u2264N) tokens?\n- I think the first question \u2014 \u201cCan promptable representations obtained via task-specific prompts enable more efficient learning than those of pretrained image encoders?\u201d is not really answered. It\u2019s unclear what efficiency mean (faster to train in FLOPS? faster to train measured by amount of training steps?). I think the paper can be made stronger by comparing training efficiency when using the proposed approach vs using VLM image-representations directly.\n- I didn\u2019t fully understand the various ablations done. Specifically, did the authors try just giving task context (and no auxiliary information)? Similarly, did the authors try giving just auxiliary information without giving task context.\n- While I don\u2019t expect this experiment to be performed for rebuttal, I really wish the authors evaluated their approach on tasks that are visually more complex (or the environments are more cluttered). For [e.g](http://e.gm)., using the approach to perform rearrangement tasks in indoor environments like Habitat / AI2 Thor."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6621/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6621/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6621/Reviewer_sWjV"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6621/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698871874923,
            "cdate": 1698871874923,
            "tmdate": 1700714952526,
            "mdate": 1700714952526,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "dSdKhL6s2J",
                "forum": "DQCZiKb3Uy",
                "replyto": "BZDCpA1kdy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6621/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6621/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your detailed feedback. We have addressed your main concerns by adding all remaining programmatic Minecraft tasks from the original MineDojo paper and three additional baselines to which our method favorably compares. In the following we will answer your questions in detail.\n\n> I also recommend a stronger evaluation on the Minecraft benchmark consistent with the evaluations done in MineDOJO. Currently, the paper shows results on only three tasks. For a more exhaustive evaluation, MineDOJO recommends evaluation on a collection of starter tasks (32 programmatic and 32 creative tasks). \n\nThank you for the suggestion! Unfortunately, to the best of our knowledge, we were unable to find the 64 core tasks mentioned in the original MineDojo paper. As the MineDojo paper evaluates their agent on only 12 tasks (four of which were \u201ccreative,\u201d and thus do not have associated concrete reward functions), we assume that said tasks are part of the core set. Thus, we run PR2L and the VLM image encoder baseline on all 8 of the \u201cprogrammatic\u201d tasks considered in the MineDojo paper. Only \u201ccombat zombie\u201d is currently available (as our computing cluster\u2019s file system is down), but we find that PR2L outperforms the baseline in that case, as shown [here](https://ibb.co/jggxzD9). \nWe have also run some additional baselines / ablations on the original three tasks, wherein the model is trained on VC-1 and R3M representations or is supplemented by ground-truth entity detection. In all cases, these perform worse than our approach, as shown [here](https://ibb.co/PrkhKFB) and discussed in Appendix E.2.\n\n> While I understand that the authors didn\u2019t have the resources to train a RT-1 style baseline, would it still be possible to train an action decoder on top of the VLM to produce actions. I think having a strong RT-1 style baseline is very important to properly evaluate the question (2) mentioned in the paper \u2014 \u201cHow does PR2L compare to approaches that directly \u201cask\u201d the VLM to generate the best possible actions for a task specified in the prompt?\u201d\n\nGreat point! While we did not exactly match the architecture of RT-1/-2, we already addressed question (2) in our baseline (b), wherein the VLM was given a prompt similar to the ones used in the RT-1 and RT-2  with the task instruction and possible actions, and was then asked to produce an action. The trained Transformer-based policy then decodes the instructions and generated text into actions. Of course, unlike RT-2, this approach does not fine-tune the VLM itself on data from MineCraft, but please note that none of our methods use a fine-tuned VLM: our goal is to compare different approaches to query representations from a pre-trained model in terms of their downstream RL performance. We apologize for any confusion and have updated the paper to clarify this point.\n\n> The proposed architecture compresses the task-relevant features from the VLM into a single CLS token which can severely restrict the information available to the policy ... Did the authors try an approach like Perceiver IO (Jaegle et al, 2021) which encodes N tokens to K (1\u2264K\u2264N) tokens?\n\nFor a fair comparison against MineCLIP that produces a fixed-length representation of an observation,  we chose to convert the VLM representations to a single-token bottleneck. This allows us to roughly match the size of the policy network that converts the VLM representation into an action distribution. In our experiments, we do not find this bottleneck to hinder our policy\u2019s ability to achieve high task performance. That said, of course, PR2L can use more advanced policy architectures which do not involve such bottlenecks and we would only expect these architectures to improve performance.\n\n> It\u2019s unclear what efficiency means (faster to train in FLOPS? faster to train measured by amount of training steps?). I think the paper can be made stronger by comparing training efficiency when using the proposed approach vs using VLM image-representations directly.\n\nThank you for this suggestion. We apologize for the confusion \u2013 we use the term efficiency to denote sample efficiency, following the standard textbook definitions of sample efficiency in RL. Sample efficiency is distinct from computation efficiency (measured in FLOPs), and is a standard comparison metric for RL algorithms. We have updated the paper to reflect this. The reviewer-suggested comparison is shown in the red (PR2L) and blue (VLM image representation) curves in all performance figures \u2013 when considering sample efficiency, we can simply check the returns or successes after a fixed number of training samples.\n\n[Continued in follow-up response due to character limit]"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6621/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700294497960,
                "cdate": 1700294497960,
                "tmdate": 1700294497960,
                "mdate": 1700294497960,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PEcO1KLToz",
                "forum": "DQCZiKb3Uy",
                "replyto": "BZDCpA1kdy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6621/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6621/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer sWjV, \n\nWe were wondering if you have had a chance to read our reply to your feedback. As the time window for the rebuttal is closing soon, please let us know if there are any additional questions we can answer to help raise the score!"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6621/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700514157524,
                "cdate": 1700514157524,
                "tmdate": 1700514255848,
                "mdate": 1700514255848,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qF6l0o5QwI",
                "forum": "DQCZiKb3Uy",
                "replyto": "BZDCpA1kdy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6621/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6621/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer sWjV,\n\nAs the rebuttal window closes tomorrow, we would again like to ask if you have had a chance to consider our reply to your feedback. Please let us know if you have any additional concerns we can address to help raise the score!"
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6621/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700600446629,
                "cdate": 1700600446629,
                "tmdate": 1700600446629,
                "mdate": 1700600446629,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]