[
    {
        "title": "Attribute Based Interpretable Evaluation Metrics for Generative Models"
    },
    {
        "review": {
            "id": "yd4aqKBfop",
            "forum": "VZVXqiaI4U",
            "replyto": "VZVXqiaI4U",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4880/Reviewer_rWJQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4880/Reviewer_rWJQ"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose to evaluate the quality of image generation approaches by comparing the distribution of attributes (and attribute pairs) scores across the training dataset and the generated one. This is intended to capture whether the generative model has captured well the training distribution.\nThe requires either an auxiliary model trained for attribute detection, either ad hoc or a generic one like CLIP."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The method is well motivated."
                },
                "weaknesses": {
                    "value": "1- The experiments aimed at investigating the behaviour of the approach seem to be, in their current form, inconclusive.\n1.1 \u2013 In 5.1, it is not explained how the \u201cnormal images\u201d are obtained. This prevents us from discerning whether it really is the out-of-distribution attributes that increase the scores, or simply the difference between the generated images and the normal ones.\n1.2 \u2013 In 5.2, the authors intend to highlight the need of PaD over SaD. However, they do not actually compare them, with no results for SaD to be found in this section.\n1.3 \u2013 The approach would be, by nature, sensitive to the quality of the attribute detector, and only attributes that are consistently visually detectable should be used, since the metrics would be a mixture of the quality of the generator and that of the detector.\n2- The paper needs to be improved in terms of writing and structure.\n2.1 \u2013 With respect to the writing, there are many sentences that need some improvement. Just some examples:\n- \u201c They provide which attributes the models struggle.\u201d\n- \u201call SaD top-rank attributes have negative mean differences that mean SDs tend to omit some objects\u201d\n- \u201cWe infer that the color-related attributes are the inferiority of DMs\u201d\n2.2 \u2013 The mean difference (Eq 7), seems to be an element of the methodology but appears in the experiments section.\n2.3 \u2013 Many of the figures (like Fig 4a) display text that is impossible to read to to its size (and when zooming in it actually becomes pixelated).\n3 \u2013 Some of the comparisons could be more comprehensive. For instance, Table 4 shows no other metrics than the proposed ones."
                },
                "questions": {
                    "value": "Please see the weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4880/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697916627867,
            "cdate": 1697916627867,
            "tmdate": 1699636472164,
            "mdate": 1699636472164,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "nmF3dwkEnT",
                "forum": "VZVXqiaI4U",
                "replyto": "yd4aqKBfop",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4880/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4880/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate Reviewer rWJQ for the constructive feedback, especially highlighting the sound motivation of our method.\n\nBelow, we carefully address the concerns.\n\n### 1.1. Explanation for \"normal images\"\n> In 5.1, it is not explained how the \u201cnormal images\u201d are obtained. This prevents us from discerning whether it really is the out-of-distribution attributes that increase the scores, or simply the difference between the generated images and the normal ones. Please review the final version.\n\n \nNormal images and the initial 30K images are IID sampled from *the same distribution*, i.e., the training set, without repetition. It works as a controlled counterpart of adding images with OOD pairs of attributes. We added this in the revised version. Thank you for providing preciseness for our paper.\n\n### 1.2. Necessity of PaD over SaD \n> In 5.2, the authors intend to highlight the need of PaD over SaD. However, they do not actually compare them, with no results for SaD to be found in this section. \n\n\nWe respectfully remind that PaD detects correlation errors such as \"babies with beard\", while SaD lacks this capability *by design*. \n\nBesides, we added SaD in Appendix Figure S9. It only reveals divergence in single attribute, e.g., *smile*, while PaD reveals correlation errors.\n\n### 1.3. Dependency on the attribute detector\n> The approach would be, by nature, sensitive to the quality of the attribute detector, and only attributes that are consistently visually detectable should be used, since the metrics would be a mixture of the quality of the generator and that of the detector\n> \n\nThis is the characteristics shared by most existing metrics that use feature extractor including FID, precision, recall, LPIPS, etc. We respectfully suggest that designing metrics without dependency is an orthogonal research direction.\n\n### 2.1. Writing improvements\n\nThank you for helping us improve readability. Changes are summarized below.\n\n> Single-attribute Divergence (SaD) measures the divergence regarding PDFs of a single attribute. Paired-attribute Divergence (PaD) measures the divergence regard- ing joint PDFs of a pair of attributes. *They provide which attributes the models struggle*. \n\n$\\rightarrow$ Single-attribute Divergence (SaD) *reveals the attributes that are generated excessively or insufficiently by measuring* the divergence of PDFs of individual attributes. Paired-attribute Divergence (PaD) *reveals such pairs of attributes by measuring* the divergence of joint PDFs of pairs of attributes. \n\n\n> *all SaD worst-rank attributes have negative mean differences that mean SDs tend to omit some objects*\n\n$\\rightarrow$ *mean difference of attribute strengths are below zero. It implies that SDs tend to omit some objects rather than over-generating them.*\n\n\n> *We infer that the color-related attributes are the inferiority of DMs.*\n\n$\\rightarrow$ *It implies that DMs fall short on modeling color-related attributes.*\n\n\nBesides, we have revised writings that were difficult to comprehend. Please refer to the revised version.\n\n### 2.2.-2.3. Structure improvements\n\n\n> The mean difference (Eq 7), seems to be an element of the methodology but appears in the experiments section. \n\nThank you for the great suggestion. We moved equation for mean difference to methology section 4.1.\n\n> Many of the figures (like Fig 4a) display text that is impossible to read to to its size (and when zooming in it actually becomes pixelated). \n\nSuch figures illustrate the key observation in the enlarged insets and the content of tiny labels does not need to be read because they are peripheral attributes without problems. \n\n\n### 3. Additional experiments\n> Some of the comparisons could be more comprehensive. For instance, Table 4 shows no other metrics than the proposed ones.\n\nIn Table 4, other metrics are irrelevant because they have nothing to do with attributes. The message of Table 4 is that Stable Diffusions omit or over-generate specific attributes, and these differences vary across the versions. Nevertheless, we report them below.\n\n\nTable 4: SaD and PaD of different versions of Stable Diffusion\n\n\n|        | FID    | FID_CLIP | Precision | Recall    | Density   | Coverage  |\n|--------|--------|----------|-----------|-----------|-----------|-----------|\n| SDv1.5 | **14.748** | **8.902**    | **0.707** | 0.962     | **0.567** | **0.869** |\n| SDv2.1 | 15.203 | 10.659   | 0.663     | **0.969** | 0.414     | 0.672     |"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4880/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699968675901,
                "cdate": 1699968675901,
                "tmdate": 1699968675901,
                "mdate": 1699968675901,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Z821Wid4B4",
                "forum": "VZVXqiaI4U",
                "replyto": "nmF3dwkEnT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4880/Reviewer_rWJQ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4880/Reviewer_rWJQ"
                ],
                "content": {
                    "comment": {
                        "value": "I greatly appreciate the effort of the authors in updating the paper. However, I still find the main elements of my review to be unaddressed.\n\n## 1.1. Explanation for \"normal images\"\nI thank the authors for clarifying that the \"normal\" images come from the real dataset. This reinforces my concern that this experiment doesn't allow to draw any conclusions, since the injections are of different nature: 1) biased generated images and 2) real images. In order to understand whether this difference is important, this would need to be compared against 3) unbiased generated images (since biased real images are not really an option).\n\n## 1.2. Necessity of PaD over SaD\nI don't think I follow the authors' argument here. Section 5.2 seems to be written as a comparison between SaD and PaD, in order to highlight the cases in which PaD is able to detect deviations from the original distribution that are due to pairwise interactions between attributes. It would, therefore, be sensible, and even necessary for the argument, to compare both. In case the contamination only affects the pairwise relations, and not the individual abundance of each attribute, as in described in Section 5.2, we would expect PaD to pick up the difference, while SaD may not. Currently, the result shown in Fig. 4 (and those in Fig. S9), don't allow to understand whether PaD would indeed allow to pick up this difference without assuming that we know the nature of the contamination (the fact that it relates to gender and smiling). This results are, therefore, inconclusive as well. \n\n## 1.3. Dependency on the attribute detector\nI agree with the authors that this limitation is intrinsic to the approach. I only meant that this should be acknowledged and, ideally, investigated: how does the quality of the attribute detector affect the methods?\n\n## 2. Writing and structure\nI appreciate the improvements based on the suggestions. I find that there are still some writing details that need to be polished, though, and I strongly recommend the authors to edit it carefully. For instance: in Section 5.2, \"non-smiling men\" and \"smiling men\" are written in very different styles (not clear if for a reason); in Section 5.3, the plural of phenomenon is phenomena (but also, why is iDDPM called a phenomenon?).\n\nWith respect to the figures, I still find that they need to be substantially improved. I strongly disagree with the argument that the tiny text in the figures is fine because it \"does not need to be read\". If that is the case, please don't add it to the figure. Keeping with the example of Fig. 4a, not only is the label text impossible to read, but even the actual result (the bars), are tiny and hard to see, while most of the space is used for \"peripheral attributes without problems\". I find this quite unacceptable for a final version of a paper."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4880/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700461159689,
                "cdate": 1700461159689,
                "tmdate": 1700461159689,
                "mdate": 1700461159689,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "plbj6XZxiU",
            "forum": "VZVXqiaI4U",
            "replyto": "VZVXqiaI4U",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4880/Reviewer_B3SF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4880/Reviewer_B3SF"
            ],
            "content": {
                "summary": {
                    "value": "- This work proposes two new evaluation metrics, named Single-attribute Divergence (SaD) and Paired-attribute Divergence (PaD), that measure the divergence of a set of generated images from the training set.\n- SaD measure the divergence of the marginal PDF of a single attribute, while PaD measures the divergence of the joint distribution of two attributes between a set of generated and ground truth images. \n- To measure the attribute strengths of an image, authors propose Heterogenous CLIP score which is based on heterogenous starting points. This formulation avoids the narrow range of values that result from a CLIP score achieving scores that are unrestricted and flexible.\n- Finally, authors perform experiments comparing some popular GAN and Diffusion models, revealing some interesting properties, which can be attributed to the interpretability of the proposed metrics."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "- Evaluating generative models is a very important area of research given the exponential progress achieved recently in this space. Researchers have identified several shortcomings with existing automatic evaluation methods of generative models which requires more analysis and research. This work tackles a very important research problem and proposes simple and effective evaluation metrics for generative models.\n- Authors identify an important shortcoming of CLIP similarity score, a popular score used for evaluating conditioned generative models, and propose an alternative which is more interpretable. \n- The SaD and PaD metrics are theoretically well grounded and interpretable, unlike existing automatic evaluation metrics, making it a good diagnostic tool as well.\n- Authors show some interesting preliminary analysis that agree with the general consensus of the public (SD 1.5 > SD 2.1)."
                },
                "weaknesses": {
                    "value": "- My major concern with the proposed metrics is with respect to resolution. How does the difference in absolute values translate to actually seeing a difference in the generations. For example, in [1] authors identify, with the help of human evaluation, that the resolution of FVD is 50, i.e. a human rater can tell the difference between generations of two models if their corresponding FVD scores differ by atleast 50 points. \n- How does SaD or PaD correlate with human evaluation? Do humans agree that the attributes identified by SaD and PaD are indeed misrepresented in the generations of the model?\n\n\n[1] Thomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach, Rapha\u00ebl Marinier, Marcin Michalski, Sylvain Gelly, FVD: A new Metric for Video Generation, ICLRW 2019"
                },
                "questions": {
                    "value": "- Do these metrics correlate well with any existing evaluation metrics? \n- Do these metrics work with any other features than CLIP features? For example, for textures, do features from DINO/SAM (get reference features from some texture dataset and compare to features of generated images) work as well or is the joint embedding of CLIP necessary for the success of SaD and PaD.\n- Can these metrics be used to measure quality and faithfulness of text to image generative models? If so, how would one go about that?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "I do not foresee any immediate ethical concerns pertaining to this work."
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4880/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4880/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4880/Reviewer_B3SF"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4880/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698696797903,
            "cdate": 1698696797903,
            "tmdate": 1699636472070,
            "mdate": 1699636472070,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "oKg24MJZAJ",
                "forum": "VZVXqiaI4U",
                "replyto": "plbj6XZxiU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4880/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4880/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate Reviewer B3SF for the constructive feedback, highlighting our strengths in\n1. Identifying shortcomings in existing evaluation methods\n2. Proposing a more interpretable alternative to CLIP similarity\n3. Theoretically grounded and interpretable SaD and PaD\n4. Preliminary analysis aligning with public consensus \n\n\nBelow, we carefully address the concerns. \n\n\n### Resolution of the proposed metrics\n> My major concern with the proposed metrics is with respect to resolution. How does the difference in absolute values translate to actually seeing a difference in the generations. For example, in [1] authors identify, with the help of human evaluation, that the resolution of FVD is 50, i.e. a human rater can tell the difference between generations of two models if their corresponding FVD scores differ by atleast 50 points.\n\nThank you for the nice suggestion.\n\nResolution of FVD can be measured by a few samples because most users have common sense for high quality video. On the other hand, SaD and PaD require users to assess the distribution of attributes in the training set and the generated set. As it is prohibitive, to make the problem easier, we will ask users to rank the four sets of generated images from LDM-20/-50/-100/-200 which are used in Table 2. We expect the users to be able to tell the difference between sets with large SaD/PaD gaps and vice versa.\n\n\n|  | SaD | PaD |\n| -------- | -------- | -------- |\n| LDM_20steps     | 21.07     | 50.06     |\n| LDM_50steps     | 10.42     | 25.36     |\n| LDM_100steps     | 11.91     | 27.26     |\n| LDM_200steps     | 14.04     | 30.71     |\n\nWe will share the results as soon as the user study is completed.\n\n\n### Correlation with human judgements\n> How does SaD or PaD correlate with human evaluation? Do humans agree that the attributes identified by SaD and PaD are indeed misrepresented in the generations of the model?\n\nThank you for the nice suggestion. We will conduct two user studies with toy experiments as follows.\n\nWith the training set having a medium *smile*, we will ask users to rank three sets with strong, medium, and no smile. We expect the rank to agree to SaD.\n\nWith the training set having a strong positive correlation between *man* and *smile*, we will ask users to rank three sets with strong positive, no, and strong negative correlations. We expect the rank to agree to PaD.\n\nWe will share the results as soon as the user study is completed.\n\n\n### Correlation with existing metrics\n> Do these metrics correlate well with any existing evaluation metrics?\n\n\nFrechet Inception Distance[FID][FID_clip] shows a consistent trend with our metrics across most generative models, while other metrics[Precision&Recall][Density&Coverage] showed no correlations with proposed metrics in Experiment 5.2.\n\nWe found notable differences in PaD with FIDs, particularly revealing significant penalties for [ProjectedGAN]. We attribute ProjectedGAN's inferior performance in PaD to its ignorance to attribute correlations, generating 'babies with beards'. \n\n[FID] Heusel et al., Gans trained by a two time-scale update rule converge to a local nash equilibrium, NeurIPS, 2017\n\n[FID_CLIP] Kynk\u00e4\u00e4nniemi et al., The Role of ImageNet Classes in Fr\u00e9chet Inception Distance, ICLR, 2023\n\n[Precision&Recall] Sajjadi et al., Assessing generative models via precision and recall. NeurIPS, 2018\n\n[Density&Coverage] Naeem et al., Reliable fidelity and diversity metrics for generative models, ICML, 2020\n\n\n### Other feature extractors\n> Do these metrics work with any other features than CLIP features? For example, for textures, do features from DINO/SAM (get reference features from some texture dataset and compare to features of generated images) work as well or is the joint embedding of CLIP necessary for the success of SaD and PaD.\n\nDINO/SAM with texture datasets would work but they would measure the strength of textures instead of attributes. It is a natural extension of our metrics when a user wants to evaluate the distribution of textures in the generated images. This generalization is a virtue of our metric. Using multi-modal feature extractors such as [ImageBind] also could be an interesting research topic. We added these discussion in the revised version.\n\n[ImageBind] ImageBind_ One Embedding Space To Bind Them All, Rohit Girdhar et al., CVPR2023, https://imagebind.metademolab.com/\n\n\n\n### Evaluating text-to-image model\n> Can these metrics be used to measure quality and faithfulness of text to image generative models? If so, how would one go about that?\n\n\n_1) We are afraid but our metrics are not for measuring quality. 2) Heterogeneous CLIP score better measures faithfulness to the text than the original CLIP. 3) For the text-to-image task, SaD and PaD measure the divergence of the generated images from the training images regarding the attributes in the text prompts. Rather than the text prompts, SaD and PaD consider the training images as the reference."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4880/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699968591059,
                "cdate": 1699968591059,
                "tmdate": 1699968591059,
                "mdate": 1699968591059,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rbCGZAj1br",
                "forum": "VZVXqiaI4U",
                "replyto": "oKg24MJZAJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4880/Reviewer_B3SF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4880/Reviewer_B3SF"
                ],
                "content": {
                    "title": {
                        "value": "Response to Author's comments"
                    },
                    "comment": {
                        "value": "I would like to thank the authors for their detailed response. \n1. How was the correlation with other metrics measured?\n2. Regarding resolution, the human evaluation can be done within each attribute as well right? Show users two images with \"smile\" attribute which get scores within a range. You can choose different range of values and decide at which point, humans can stop telling the difference."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4880/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700064243694,
                "cdate": 1700064243694,
                "tmdate": 1700064243694,
                "mdate": 1700064243694,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "yXQQYi4jrO",
                "forum": "VZVXqiaI4U",
                "replyto": "x4lkRLlJqc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4880/Reviewer_B3SF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4880/Reviewer_B3SF"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors"
                    },
                    "comment": {
                        "value": "I thank the authors for their detailed response.\n- It makes sense since the baseline metrics are more generic and do not explicitly measure the quantities measured by SaD and PaD.\n- Thank you for taking the suggestion. That would be interesting thing to try.\n- One thing I feel which is not emphasized strongly is why do we need both PaD and SaD. Authors show an experiment that PaD can circumvent some issues of SaD. Then why not just replace SaD with PaD? I think authors should emphasize these points much stronger in the next version."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4880/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700163357939,
                "cdate": 1700163357939,
                "tmdate": 1700163357939,
                "mdate": 1700163357939,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gas79m5yYi",
                "forum": "VZVXqiaI4U",
                "replyto": "FKsMkdWXS0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4880/Reviewer_B3SF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4880/Reviewer_B3SF"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors"
                    },
                    "comment": {
                        "value": "I thank the reviewers for their detailed response. After carefully looking through the results I am convinced and I believe the authors have addressed my questions sufficiently. However, the authors need to carefully plan how to add this analysis to the main paper which I believe requires significant effort. I vote to keep my rating with the understanding that authors will add this and other discussions raised by other reviewers in the paper."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4880/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700696052052,
                "cdate": 1700696052052,
                "tmdate": 1700696052052,
                "mdate": 1700696052052,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "pP39r6gVD8",
            "forum": "VZVXqiaI4U",
            "replyto": "VZVXqiaI4U",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4880/Reviewer_7B16"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4880/Reviewer_7B16"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes two evaluation metrics, single-attribute divergence and paired-attribute divergence, to measure the divergence of a set of generated images with respect to the distribution of attribute strengths. The proposed metrics are defined based on heterogeneous CLIPScore, an enhanced measure from CLIPScore. THe metrics are verified on a few generative models, including PrpjectedGAN and diffusion models, to show the effectiveness and explainability."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- This paper is clearly written and easy to follow.\n- Evaluation of generative models is a crucial problem which will attract wide research interest.\n- Defining evaluation metrics based on attributes to measure the divergence between image sets is a novel and reasonable idea."
                },
                "weaknesses": {
                    "value": "- The overall contribution is incremental, though the research motivation is fairly clear and reasonable. \n  - The heterogeneous CLIPScore is a simple extension from CLIPScore by using the centralized encodings. \n  - The proposed SaD and PaD are straightforward to measure the divergences of single and paired attributes.\n\n- As for the interpretability, I have some concerns:\n  -  For me, the interpretability comes from the attributes, which are obtained from annotation or large models. So the interpretability of the evaluation metrics are limited by the set of attributes.\n  - Interpretation based on attributes is only one possible solution, which may be not complete or accurate to characterize the capability of generative models.\n\n- The attributes selection methods are somewhat simple. Little insight can be gained from this process.\n  -- What if the attributes are biased due to biased annotations or large models?"
                },
                "questions": {
                    "value": "Please refer to \"weaknesses\" part for my concerns."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4880/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698728747297,
            "cdate": 1698728747297,
            "tmdate": 1699636471964,
            "mdate": 1699636471964,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0bI1nR0QSC",
                "forum": "VZVXqiaI4U",
                "replyto": "pP39r6gVD8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4880/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4880/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate Reviewer 7B16 for the constructive feedback, highlighting our strengths in \n1. Clear writing \n2. Addressing the crucial issues in the evaluation\n3. Novelty and soundness of the proposed metrics regarding attributes to measure the divergence between image sets\n\nBelow, we carefully address the concerns.\n\n\n### Compared to the motivation being clear and reasonable, Heterogeneous CLIPScore, SaD, and PaD are too simple.\n>The overall contribution is incremental, though the research motivation is fairly clear and reasonable.\nThe heterogeneous CLIPScore is a simple extension from CLIPScore by using the centralized encodings.\nThe proposed SaD and PaD are straightforward to measure the divergences of single and paired attributes.\n\nWe sincerely appreciate recognizing our motivation. \nWe respectfully suggest that *it is desirable to resolve problems using simple solutions because they are easy to adopt in future research*. Especially, *our solutions are helpful for the research community and practitioners* as follows:\n\n- Heterogeneous CLIPScore indicates the presence of attributes in a given image, while CLIPScore does not.\n- Single and Paired attribute Divergence (SaD and PaD) reveal the problematic attributes, while existing metrics do not.\n\n### Interpretability is limited by the set of attributes.\n\n> As for the interpretability, I have some concerns:\nFor me, the interpretability comes from the attributes, which are obtained from annotation or large models. So the interpretability of the evaluation metrics is limited by the set of attributes.\nInterpretation based on attributes is only one possible solution, which may be not complete or accurate to characterize the capability of generative models.\n\nThank you for the constructive feedback. Indeed the interpretability of our metrics depends on the choice of attributes. As different users want different capabilities of the generative models, our metrics provide *customizable* measures that capture attributes designated by users. It would be beneficial to extend our work with different encoders for different modalities other than textual attributes as stated by reviewer B3SF. We added this discussion in the revised version.\n\n\n\n### What if the attributes are biased?\n> The attributes selection methods are somewhat simple. Little insight can be gained from this process. -- What if the attributes are biased due to biased annotations or large models?\n\nLarge models or annotations are merely options to ease users' burden. If a user cares about fairness, one can choose the target attributes by oneself. *Resolving the bias of annotations or large models is an orthogonal research topic.* We respectfully suggest that such bias should not be a reason for rejecting this paper. Nevertheless, we agree that we should be careful not to be biased in choosing the target attributes. This discussion is in Appendix A.1. and we added it in the main paper as well in the revised version."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4880/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699968428178,
                "cdate": 1699968428178,
                "tmdate": 1699968428178,
                "mdate": 1699968428178,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VqoAsxX4Vp",
                "forum": "VZVXqiaI4U",
                "replyto": "pP39r6gVD8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4880/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4880/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate with your thoughtful comments. Could you check our response? We will be happy to answer follow-up questions if any."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4880/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700524397728,
                "cdate": 1700524397728,
                "tmdate": 1700524397728,
                "mdate": 1700524397728,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qNoYnuWcKy",
                "forum": "VZVXqiaI4U",
                "replyto": "VqoAsxX4Vp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4880/Reviewer_7B16"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4880/Reviewer_7B16"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the authors' response"
                    },
                    "comment": {
                        "value": "Thanks for the effort in replying my comments and updating the submission. My concerns are partially addressed."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4880/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700722064063,
                "cdate": 1700722064063,
                "tmdate": 1700722064063,
                "mdate": 1700722064063,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]