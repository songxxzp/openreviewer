[
    {
        "title": "FacTool: Factuality Detection in Generative AI - A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios"
    },
    {
        "review": {
            "id": "pO6Swc9Mev",
            "forum": "jolYuxpVn1",
            "replyto": "jolYuxpVn1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3682/Reviewer_wbTV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3682/Reviewer_wbTV"
            ],
            "content": {
                "summary": {
                    "value": "This paper describes a factuality evaluation framework comprising five steps: claim extraction, query generation, tool querying, evidence collection and agreement verification. The main novelty of the proposed framework allows the incorporation of external tools such as Google Search, Google Scholar, and code interpreters. It can be applied to multiple tasks (QA, math, code generation, scientific literature reviewing). The authors construct a dataset covering the four tasks mentioned earlier and compare the proposed framework against the baseline: Self-Check."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Reasons To Accept\n* The studied topic is important, and the proposed framework is a valuable contribution to the community\n* Extensive analysis covering multiple tasks, although some interesting content is in the appendix"
                },
                "weaknesses": {
                    "value": "Although the described framework makes sense to me, the authors highly rely on ChatGPT to implement their framework (even if the function is just to check whether list A is a subset of list B); this may hinder other researchers from reproducing their results and additional costs."
                },
                "questions": {
                    "value": "N/A"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3682/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698731373735,
            "cdate": 1698731373735,
            "tmdate": 1699636324964,
            "mdate": 1699636324964,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2Rey6fRh9H",
                "forum": "jolYuxpVn1",
                "replyto": "pO6Swc9Mev",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3682/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3682/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Weakness 1"
                    },
                    "comment": {
                        "value": "Thanks for the comment!\n\n> Although the described framework makes sense to me, the authors highly rely on ChatGPT to implement their framework (even if the function is just to check whether list A is a subset of list B); this may hinder other researchers from reproducing their results and additional costs.\n\nWe are currently working on integrating FacTool with Llama2 and various other open-source models. Detailed results are:\n\n| Methods | Tasks        | Foundation Model | Claim-Level |      |      |      | Response-Level |      |      |      |\n|---------|--------------|------------------|-------------|------|------|------|----------------|------|------|------|\n|         |              |                  | Accuracy    | Recall | Precision | F1   | Accuracy    | Recall | Precision | F1   |\n| FACTOOL | Knowledge-QA | llama2-70b-chat  | 73.99       | 93.49 | 77.07      | 84.49| 52.00       | 78.26 | 48.65      | 60.00|\n| FACTOOL | Code         | llama2-70b-chat  | 55.75       | 50.00 | 76.00      | 60.32| 55.75       | 50.00 | 76.00      | 60.32|\n| FACTOOL | Math         | llama2-70b-chat  | 86.62       | 100.00| 86.62      | 92.83| 47.00       | 100.00| 47.00      | 63.95|\n| FACTOOL | Scientific   | llama2-70b-chat  | 96.77       | 81.82 | 100.00     | 90.00| 98.00       | 80.00 | 100.00     | 88.89|\n\nWe note that the error detection ability of FacTool powered by Llama-2-70b-chat is slightly constrained due to Llama-2-70b's limited coding capabilities (Llama-2-70b scores only 29.9% on the HumanEval benchmark, in contrast to ChatGPT-3.5's 48.1%)."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3682/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700640871015,
                "cdate": 1700640871015,
                "tmdate": 1700642392634,
                "mdate": 1700642392634,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "80G1jxyDwB",
                "forum": "jolYuxpVn1",
                "replyto": "2Rey6fRh9H",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3682/Reviewer_wbTV"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3682/Reviewer_wbTV"
                ],
                "content": {
                    "title": {
                        "value": "acknowledgement"
                    },
                    "comment": {
                        "value": "I have read the author's response and other reviews. Thanks for providing additional results using open-source models."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3682/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700689638882,
                "cdate": 1700689638882,
                "tmdate": 1700689638882,
                "mdate": 1700689638882,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "50Ih1QMoIP",
                "forum": "jolYuxpVn1",
                "replyto": "pO6Swc9Mev",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3682/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3682/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the positive comment!"
                    },
                    "comment": {
                        "value": "Dear reviewer,\n\nWe are glad to hear your positive comments! We will greatly appreciate if you could consider raising the review score if you find our response helpful!"
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3682/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700708419211,
                "cdate": 1700708419211,
                "tmdate": 1700708419211,
                "mdate": 1700708419211,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ETdkibDuHg",
            "forum": "jolYuxpVn1",
            "replyto": "jolYuxpVn1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3682/Reviewer_fmJJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3682/Reviewer_fmJJ"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes a tool to detect factual errors in generated text which can be used across multiple domains. The authors claim that existing fact-checking methods are 1.) limited and task-specific 2.) use disjoint pieces of text as claims which might lose the contextual information especially in long form answers. They propose custom modularize pipelines for factuality error detection for knowledge based QA, arithmetic problem solving, language to code generation and scientific literature summarization. At a high level, they first use a claim extraction module to state claims, in a task-dependent manner. Then they use query generation to test the validity of these claims against retrieved evidences, calculator, interpreter etc. depending on each task. Finally they collate the results to evluate factuality of the LLM response."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.) The authors develop a one-stop factuality tool which can be useful for the community.\n2.) The authors have invested in extensive prompt development for claim extraction in various use cases.\n3.) They do an extensive analysis of their system on four tasks -  KBQA, code generation, scientific literature survey and arithmetic problems"
                },
                "weaknesses": {
                    "value": "1.) The technical contributions are somewhat limited and mostly incremental.\n2.) The intermediate question generation module to validate any claims made by the LLM against an external evidence has been proposed in other works (https://arxiv.org/pdf/2210.08726.pdf, https://arxiv.org/pdf/2309.11495.pdf). The novelty as far as I understand is mostly in claim generation which is different for each tasks because the inputs are no longer just declarative statements but also things like code snippets, questions, etc.\n3.) The authors stretch the definition of factuality to new tasks like code generation which are not necessarily considered \"factual\". For instance, there can multiple ways to write a logical code snippet and not necessarily a universal way for it to be deemed as a fact.\n4.) The evaluations mostly focus on different language models and not existing methods for factual error detection like RARR or SELF-REFINE which could have been used for at least KBQA."
                },
                "questions": {
                    "value": "1.) Could you please decsribe how the baseline method Self-Check(*) was setup? Does it use some existing code or were the prompts developed by the authors?\n2.) In question generation module for code generation task, how will the model know what are the right corner case inputs for unit testing the piece of code?\n3.) In Appendix C (Fig 6) some characters in the prompt are all-caps, was there any specific reason for that. Out of curiosity, does standard casing yield bad performance?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3682/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698818049379,
            "cdate": 1698818049379,
            "tmdate": 1699636324888,
            "mdate": 1699636324888,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "iS1vQZhIQ9",
                "forum": "jolYuxpVn1",
                "replyto": "ETdkibDuHg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3682/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3682/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Weakness 1"
                    },
                    "comment": {
                        "value": "Thanks for the comment!\n\n> The technical contributions are somewhat limited and mostly incremental. \n\nWe want to emphasize the novelty of our paper:\n\n(i) **Motivation**: Given that a wider range of tasks now face an increasing risk of containing factual errors when handled by generative AI. We aim to provide a unified framework for detecting factual errors across various tasks and domains. \n\n(ii) **A Unified Framework for Multi-Task and Multi-Domain Scenarios**: FacTool presents a unified tool-augmented framework for detecting factual errors across diverse tasks and domains, including KBQA, coding, math, and scientific. \n\n(iii) **An automatic factuality evaluator for modern chatbots across different scenarios**: FacTool is essentially a automate factuality evaluator that can be used to evaluate the factuality of modern chatbots across various scenarios. By utilizing factool, we maintain a factuality leaderboard for chatbots. We believe factool can facilitate the development of modern LLMs by helping developers detect potential factual errors generated by their LLMs.\n\n(iv) **An user-friendly API interface and supports ChatGPT plugin**: We have made significant efforts to ensure that FacTool is user-friendly and supports ChatGPT plugin, enabling both technical and general users to benefit from our research."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3682/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700640136514,
                "cdate": 1700640136514,
                "tmdate": 1700640136514,
                "mdate": 1700640136514,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rHVDdDejBs",
                "forum": "jolYuxpVn1",
                "replyto": "ETdkibDuHg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3682/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3682/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Weakness 2"
                    },
                    "comment": {
                        "value": "Thanks for the comment!\n\n> The intermediate question generation module to validate any claims made by the LLM against an external evidence has been proposed in other works (https://arxiv.org/pdf/2210.08726.pdf, https://arxiv.org/pdf/2309.11495.pdf). The novelty as far as I understand is mostly in claim generation which is different for each tasks because the inputs are no longer just declarative statements but also things like code snippets, questions, etc. \n\nWe would like to emphasize three main difference between FacTool and RARR:\n\n(i) FacTool provides versatile claim extraction, enabling not only the provision of fine-grained factuality information to help users more easily detect potential factual errors but also the integration of factuality detection across various domains and scenarios. In contrast, RARR neither supports fine-grained factuality nor multi-task factuality.\n\n(ii) FacTool offers a more intuitive and straightforward agreement verification process. After evidence collection, we leverage the strong reasoning capabilities of LLMs to directly feed LLMs with the collected evidence, eliminating the need for extra steps in relevance matching. RARR requires an extra step to parse over webpages and uses a relevant matching algorithm to find the most relevant snippets, which is more complicated compared to FacTool.\n\n(iii) FacTool focuses on \u201cfactuality detection\u201d, i.e., detecting factual errors from the generated text from LLMs, while RARR focuses on \u201ctext editing\u201d, i.e., editing text that may contain hallucinations.\n\nAs for https://arxiv.org/pdf/2309.11495.pdf , this work was completed after FacTool (and actually cites a pre-print of the FacTool paper)."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3682/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700640272409,
                "cdate": 1700640272409,
                "tmdate": 1700640272409,
                "mdate": 1700640272409,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "asgYtf4NPm",
                "forum": "jolYuxpVn1",
                "replyto": "ETdkibDuHg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3682/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3682/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Weakness 3"
                    },
                    "comment": {
                        "value": "Thanks for the comment!\n\n> The authors stretch the definition of factuality to new tasks like code generation which are not necessarily considered \"factual\". For instance, there can multiple ways to write a logical code snippet and not necessarily a universal way for it to be deemed as a fact. \n\nWe agree that there can be multiple ways to write a logical code snippet. In fact, this argument is consistent with our framework since we are adopting an **execution-based approach** to identify whether a code snippet is factual or not. As defined in our paper, the factuality of a code snippet is grounded in an execution-based approach to code evaluation. This approach measures the correctness of generated code by executing it against test case inputs and comparing its output to the expected (or golden) output."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3682/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700640402883,
                "cdate": 1700640402883,
                "tmdate": 1700640402883,
                "mdate": 1700640402883,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bUL2rIKf0U",
                "forum": "jolYuxpVn1",
                "replyto": "ETdkibDuHg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3682/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3682/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Weakness 4"
                    },
                    "comment": {
                        "value": "Thanks for the comment!\n\n> The evaluations mostly focus on different language models and not existing methods for factual error detection like RARR or SELF-REFINE which could have been used for at least KBQA.\n\nAs mentioned in the response of Weakness 2, RARR focuses on text editing, which is different from our focus on factuality detection. Also, the workflow of RARR doesn\u2019t include a claim extraction process, which is very different from our focus to provide fine-grained claim-level information to users. Thus, we did not include RARR in our baseline.\n\nOn the other hand, we included 'Self-Check', essentially the first step of 'Self-Refine', as our baseline. It's important to note that our focus is on 'detection' rather than 'refinement'. Therefore, we use the first step of 'Self-Refine' as our baseline and refer to it as 'Self-Check'."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3682/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700640477788,
                "cdate": 1700640477788,
                "tmdate": 1700640477788,
                "mdate": 1700640477788,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LzFE8PeUyT",
                "forum": "jolYuxpVn1",
                "replyto": "ETdkibDuHg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3682/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3682/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Question 1"
                    },
                    "comment": {
                        "value": "Thanks for the comment!\n\n> Could you please describe how the baseline method Self-Check(*) was setup? Does it use some existing code or were the prompts developed by the authors? \n\nThe prompts are developed by the authors, based on the intuition of Self-Refine. Self-Refine has been acknowledged as a strong baseline for LLM to refine their own response. Recall that the first step of SELF-REFINE begins by generating an output, which is then fed back into the same LLM to obtain feedback. This is coherent with our focus, factuality detection. Therefore, we utilize the first step of Self-Refine as our baseline and named it as \u201cSelf-Check\u201d."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3682/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700640545047,
                "cdate": 1700640545047,
                "tmdate": 1700640545047,
                "mdate": 1700640545047,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GYPaAhhD2U",
                "forum": "jolYuxpVn1",
                "replyto": "ETdkibDuHg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3682/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3682/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Question 2"
                    },
                    "comment": {
                        "value": "Thanks for the comment!\n\n> In the question generation module for code generation task, how will the model know what are the right corner case inputs for unit testing the piece of code? \n\nThe test case inputs are generated automatically by LLMs through prompt-based learning. We acknowledge that, at times, the variety of generated test cases can be limited, as mentioned in Figure 16 of Appendix D. However, we believe that further prompt engineering and advancements in more sophisticated LLMs can mitigate this issue."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3682/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700640621411,
                "cdate": 1700640621411,
                "tmdate": 1700640621411,
                "mdate": 1700640621411,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WerfJh9KcO",
                "forum": "jolYuxpVn1",
                "replyto": "ETdkibDuHg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3682/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3682/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Question 3"
                    },
                    "comment": {
                        "value": "Thanks for the comment!\n\n> In Appendix C (Fig 6) some characters in the prompt are all-caps, was there any specific reason for that. Out of curiosity, does standard casing yield bad performance?\n\nWe follow the same intuition as in https://github.com/openai/evals, which uses capitalization or brackets to emphasize certain parts of the prompt."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3682/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700640701582,
                "cdate": 1700640701582,
                "tmdate": 1700640701582,
                "mdate": 1700640701582,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "F4Mp8j8pvk",
                "forum": "jolYuxpVn1",
                "replyto": "ETdkibDuHg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3682/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3682/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking forward to your helpful feedback!"
                    },
                    "comment": {
                        "value": "Dear reviewer,\n\nWe look forward to your helpful response and feedback! We are happy to provide further clarifications if you have any more questions."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3682/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700713576930,
                "cdate": 1700713576930,
                "tmdate": 1700713576930,
                "mdate": 1700713576930,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "78b19KLsxn",
            "forum": "jolYuxpVn1",
            "replyto": "jolYuxpVn1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3682/Reviewer_tg4t"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3682/Reviewer_tg4t"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes FacTool, a tool-augmented framework for detecting factual errors in texts generated by large language models (LLMs) like ChatGPT. The key ideas are:\n- Connects the concept of \"tool use\" by modern LLMs with factuality detection, using tools like search engines, code interpreters, etc to gather evidence and fact-checking for diverse tasks like QA, code, math, and scientific review.\n- Proposes a 5-step process: claim extraction, query generation, tool querying, evidence collection, agreement verification. Uses LLMs for claim extraction, query generation, and verifying claims.\n- Evaluate FACTOOL on 4 tasks - KB-QA, code, math, and scientific review. Shows it outperforms baselines like self-checking and supervised models on accuracy.\n- Uses FACTOOL to evaluate modern chatbots - finds GPT-4 has highest weighted accuracy, while supervised Vicuna-13B underperforms on math, code, and scientific review."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper intertwines the notion of \"tool use\" in modern LLMs with factuality detection. It showcases the versatile application of LLMs for auxiliary tasks, such as claim extraction and query generation, for fact-checking pipeline.\n2. An exhaustive evaluation was undertaken to assess the fact-checking capabilities of LLM responses across four distinct tasks and five datasets.\n3. The authors have made the codebase publicly available and introduced a ChatGPT plugin for enhanced accessibility."
                },
                "weaknesses": {
                    "value": "1. The novelty of the approach appears limited. While the paper employs LLMs to generate search queries and refers to search engines as tools, this methodology isn't drastically different from prior evidence-based fact-checking methods that use search engines for evidence retrieval.\n2. The study lacks comparative baselines using non-LLM models or previous LLM-based system for evidence-based fact-checking. This omission makes it challenging to ascertain the specific advantages of incorporating LLMs into the fact-checking process, or referring to search engines as tools.\n3. Despite the paper's emphasis on fact-checking, notable fact-checking datasets like FEVER and SciFact are absent from the evaluation."
                },
                "questions": {
                    "value": "1. In the \"Related Work\" section, could you provide a detailed comparison between your approach and the RARR presented by Gao et al. in 2022a? It would be beneficial to understand the distinctiveness of your work in relation to theirs.\n2. Were any experiments conducted using preivous LLMs or non-LLM-based fact-checking systems within the framework of your experimental settings?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3682/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3682/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3682/Reviewer_tg4t"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3682/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698825100753,
            "cdate": 1698825100753,
            "tmdate": 1699636324817,
            "mdate": 1699636324817,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "lK13kNff6l",
                "forum": "jolYuxpVn1",
                "replyto": "78b19KLsxn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3682/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3682/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Weakness 1"
                    },
                    "comment": {
                        "value": "Thank you for the comment!\n\n> The novelty of the approach appears limited. While the paper employs LLMs to generate search queries and refers to search engines as tools, this methodology isn't drastically different from prior evidence-based fact-checking methods that use search engines for evidence retrieval.\n\nThe fact checking methodology of FacTool differs from prior works that use search engines for evidence retrieval in evidence-based fact checking, with its **versatile claim extraction** module. This module effectively provides **fine-grained factuality information for long-form text across various domains and scenarios** that enables a **unified framework for factuality detection across different scenarios**. \n\nClassical evidence-based fact-checking methods [1, 2, 3, 4] typically follow a three-step pipeline: document retrieval, sentence retrieval, and claim verification. This pipeline is designed to address scenarios where the **claims are already given**. In contrast, FacTool is capable of fact-checking **long-form texts in which claims are not explicitly given**.\n\nMore recent prior works such as RARR [5] utilize search engines for evidence retrieval but also do not employ a claim extraction process to collect detailed factuality information. Other concurrent works like FactScore [6] employ a claim extraction module to extract atomic facts from long-form texts but are limited to the Knowledge-Based Question Answering (KBQA) task. Our work expands the definition of factuality beyond the KBQA task and does not solely focus on using search engines for evidence retrieval. FacTool is designed to be versatile and capable of handling multiple tasks, integrating various tools such as search engines, Python executors, and Google Scholar, as demonstrated in our paper. This comprehensive integration enables FacTool to tackle a wider spectrum of factuality issues in generative AI (GAI), overcoming the limitations of existing works that are focused on single tasks or specific scenarios.\n\nReferences:\n\n[1] Bekoulis et al.. A review on fact extraction and verification\n\n[2] Thorne et al. FEVER: a large-scale dataset for fact extraction and VERification\n\n[3] Zhong et al. Reasoning Over Semantic-Level Graph for Fact Checking\n\n[4] Zhou et al. GEAR: Graphbased Evidence Aggregating and Reasoning for Fact Verification\n\n[5] Gao et al. Rarr: Researching and revising what language models say, using language models\n\n[6] Min et al. FACTSCORE: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3682/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700638296922,
                "cdate": 1700638296922,
                "tmdate": 1700639752296,
                "mdate": 1700639752296,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "MLkEaGJv4B",
                "forum": "jolYuxpVn1",
                "replyto": "78b19KLsxn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3682/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3682/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Weakness 2"
                    },
                    "comment": {
                        "value": "Thank you for the comment!\n\n> The study lacks comparative baselines using non-LLM models or previous LLM-based systems for evidence-based fact-checking. This omission makes it challenging to ascertain the specific advantages of incorporating LLMs into the fact-checking process, or referring to search engines as tools.\n\n**Regarding previous LLM-based systems**\n\nWe are currently working on integrating FacTool with Llama2 and various other open-source models. Detailed results are:\n\n| Methods | Tasks        | Foundation Model | Claim-Level |      |      |      | Response-Level |      |      |      |\n|---------|--------------|------------------|-------------|------|------|------|----------------|------|------|------|\n|         |              |                  | Accuracy    | Recall | Precision | F1   | Accuracy    | Recall | Precision | F1   |\n| FACTOOL | Knowledge-QA | llama2-70b-chat  | 73.99       | 93.49 | 77.07      | 84.49| 52.00       | 78.26 | 48.65      | 60.00|\n| FACTOOL | Code         | llama2-70b-chat  | 55.75       | 50.00 | 76.00      | 60.32| 55.75       | 50.00 | 76.00      | 60.32|\n| FACTOOL | Math         | llama2-70b-chat  | 86.62       | 100.00| 86.62      | 92.83| 47.00       | 100.00| 47.00      | 63.95|\n| FACTOOL | Scientific   | llama2-70b-chat  | 96.77       | 81.82 | 100.00     | 90.00| 98.00       | 80.00 | 100.00     | 88.89|\n\nWe note that the error detection ability of FacTool powered by Llama-2-70b-chat is slightly constrained due to Llama-2-70b's limited coding capabilities (Llama-2-70b scores only 29.9% on the HumanEval benchmark, in contrast to ChatGPT-3.5's 48.1%).\n\n**Regarding non-LLM-based system**\n\nIt is challenging to find a non-LLM approach capable of performing the task defined here: the fine-grained assessment of factuality in long texts from various scenarios. Previous methods often focused on specific aspects. For example, FEVER judges the factuality of a given claim, a relatively simpler task than the one defined here. Developing a non-LLM method from scratch to conduct fine-grained assessment of factuality in long texts from various scenarios would require a substantial engineering effort. This effort could be so significant that it might be considered a separate project."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3682/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700639262665,
                "cdate": 1700639262665,
                "tmdate": 1700642381266,
                "mdate": 1700642381266,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Vci0dNfsyK",
                "forum": "jolYuxpVn1",
                "replyto": "78b19KLsxn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3682/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3682/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Weakness 3"
                    },
                    "comment": {
                        "value": "Thank you for the comment!\n\n> Despite the paper's emphasis on fact-checking, notable fact-checking datasets like FEVER and SciFact are absent from the evaluation.\n\nIn our paper, we focus on the growing challenges of factual errors generated by models in generative AI like ChatGPT. To this end, we have constructed a dataset based on the natural distribution of errors generated by these models. Fact-checking datasets like FEVER and SciFact are not natural distribution of errors generated by LLMs, so we didn\u2019t focus on these datasets. Additionally, both FEVER and SciFact provide claims, which do not align with our task setting: offering an end-to-end system capable of delivering detailed, claim-level factuality information from long-form context."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3682/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700639425769,
                "cdate": 1700639425769,
                "tmdate": 1700639740574,
                "mdate": 1700639740574,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "v873CA8R1o",
                "forum": "jolYuxpVn1",
                "replyto": "78b19KLsxn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3682/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3682/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Question 1"
                    },
                    "comment": {
                        "value": "Thanks for the comment! \n\n> In the \"Related Work\" section, could you provide a detailed comparison between your approach and the RARR presented by Gao et al. in 2022a? It would be beneficial to understand the distinctiveness of your work in relation to theirs.\nResponse:\n\nWe would like to emphasize three main difference between FacTool and RARR:\n\n(i) FacTool provides versatile claim extraction, enabling not only the provision of fine-grained factuality information to help users more easily detect potential factual errors but also the integration of factuality detection across various domains and scenarios. In contrast, RARR neither supports fine-grained factuality nor multi-task factuality.\n\n(ii) FacTool offers a more intuitive and straightforward agreement verification process. After evidence collection, we leverage the strong reasoning capabilities of LLMs to directly feed LLMs with the collected evidence, eliminating the need for extra steps in relevance matching. RARR requires an extra step to parse over webpages and uses a relevant matching algorithm to find the most relevant snippets, which is more complicated compared to FacTool.\n\n(iii) FacTool focuses on \u201cfactuality detection\u201d, i.e., detecting factual errors from the generated text from LLMs, while RARR focuses on \u201ctext editing\u201d, i.e., editing text that may contain hallucinations.\n\n**We will update the related work section in our final revision.**"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3682/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700639720066,
                "cdate": 1700639720066,
                "tmdate": 1700639720066,
                "mdate": 1700639720066,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "t9tjDDTc4O",
                "forum": "jolYuxpVn1",
                "replyto": "78b19KLsxn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3682/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3682/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Question 2"
                    },
                    "comment": {
                        "value": "> Were any experiments conducted using preivous LLMs or non-LLM-based fact-checking systems within the framework of your experimental settings?\n\n**Please refer to the response of Weakness 2.**"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3682/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700639826699,
                "cdate": 1700639826699,
                "tmdate": 1700639826699,
                "mdate": 1700639826699,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nha9o4qo36",
                "forum": "jolYuxpVn1",
                "replyto": "78b19KLsxn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3682/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3682/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking forward to your helpful feedback!"
                    },
                    "comment": {
                        "value": "Dear reviewer,\n\nWe look forward to your helpful response and feedback! We are happy to provide further clarifications if you have any more questions."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3682/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700713543850,
                "cdate": 1700713543850,
                "tmdate": 1700713543850,
                "mdate": 1700713543850,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "eKgKoaSZqq",
                "forum": "jolYuxpVn1",
                "replyto": "lK13kNff6l",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3682/Reviewer_tg4t"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3682/Reviewer_tg4t"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the author's response.\n\n- Although the author states that their work is different from the prior arts with the claim extraction module, it's hard to agree that simply adding such a module is novel.\n- Thanks for adding new experiments with Llama-2 70b. For non-LLM methods, there are open-source code including both claim extraction (https://github.com/titipata/detecting-scientific-claim) and fact-checking pipeline (https://github.com/dwadden/multivers)\n- If it's important to distinguish detecting the factual error in generated text rather than other text, is it better to phrase the main task of this paper as hallucination detection rather than fact-checking? As fact-checking is commonly known as a task to check facts in text no matter if it's machine-generated or human-written.\n- Thanks for the details comparison with previous work RARR."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3682/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700714823256,
                "cdate": 1700714823256,
                "tmdate": 1700714823256,
                "mdate": 1700714823256,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kmdQWmsEzA",
                "forum": "jolYuxpVn1",
                "replyto": "78b19KLsxn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3682/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3682/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer tg4t Part 1"
                    },
                    "comment": {
                        "value": "> Although the author states that their work is different from the prior arts with the claim extraction module, it's hard to agree that simply adding such a module is novel.\n\nFirst of all, we appreciate your acknowledgment of our versatile claim extraction module as a differentiating factor for FacTool from prior art. However, our framework's innovation **extends beyond \u201csimply\u201d adding this module**. As mentioned in our response to weakness 1 and question 1, besides our \u201cversatile claim extraction\u201d module, FacTool's factuality detection framework distinguishes itself in two other key areas compared to prior art:\n\n(i) FacTool simplifies the agreement verification process. After gathering evidence, we utilize the advanced reasoning abilities of LLMs to directly process the collected evidence, thereby avoiding the complexities of relevance matching that systems like RARR entail.\n\n(ii) FacTool facilitates the integration of various tools into the factuality detection pipeline. Unlike prior art, which typically focuses on a single task and scenario, FacTool is a more versatile approach capable of factuality detection across multiple scenarios and tasks."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3682/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700730207826,
                "cdate": 1700730207826,
                "tmdate": 1700730207826,
                "mdate": 1700730207826,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zqLqxnk3Ne",
                "forum": "jolYuxpVn1",
                "replyto": "78b19KLsxn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3682/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3682/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer tg4t Part 2"
                    },
                    "comment": {
                        "value": "> Thanks for adding new experiments with Llama-2 70b. For non-LLM methods, there are open-source code including both claim extraction (https://github.com/titipata/detecting-scientific-claim) and fact-checking pipeline (https://github.com/dwadden/multivers)\n\nThanks for acknowledging our effort on adding new experiments with Llama-2 70b. Also, thanks for providing the references. However, we believe that both https://github.com/titipata/detecting-scientific-claim and https://github.com/dwadden/multivers are not suitable for the main objective of our paper: factuality detection in Generative AI across multi-task and multi-domain scenarios.\n\nUpon trying to run the open-source code, we believe that both of the repos are not suitable for our propose, detailed below:\n\n**Regarding https://github.com/titipata/detecting-scientific-claim**\n\nThe claim extraction setting of https://github.com/titipata/detecting-scientific-claim is totally different from our claim extraction setting. According to their paper [1], the claim extraction task is defined as to predict whether a \u201csentence\u201d in a paragraph is a \u201cclaim\u201d or not, which is a \u201cbinary classification\u201d task. However, in our paper, \n\n(i) We define \u201cclaim\u201d as either atomic facts, code snippets, math calculations, or Tuple (paper title, year, authors) extracted from long-from text. This is clearly different from simply a \u201csentence\u201d as defined in [1].\n\n(ii) Also, our claim extraction task is to extract all verifiable claims within the generated text $x$, denoted as $c_1, c_2, \\cdots, c_n$\n. This is a \u201cgenerative\u201d task, not a \u201cbinary classification\u201d task as defined in [1].\n\n**Regarding https://github.com/dwadden/multivers**\n\nThe scientific claim verification task defined in [2] is: Given a claim $c$ and a collection of candidate abstracts which may contain evidence relevant to $c$, the scientific claim verification task requires a system to predict a label $y(c, a) \\in \\{\\text{SUPPORTS}, \\text{REFUTES}, \\text{NOT ENOUGH INFO}\\}$. **This is different from our task, in which claims are not provided**. The task of factuality detection in Generative AI requires a pipeline that can provide fine-grained claim-level factuality WITHOUT given claims.\n\nMoreover, directly applying the MultiVerS model for agreement verification is impractical, as it is trained on specific datasets (HealthVer, COVIDFact, SCIFACT), and its effectiveness on general domains and tasks like KBQA is unknown. It is very likely that further fine-tuning is required. Given the superior capabilities of open-source LLMs, we believe that fine-tuning open-source LLMs would be a better choice, instead of using non-LLM based methods.\n\n\u201cVersatile claim extraction\u201d is highly non-trivial and was very challenging before the era of LLMs. The limitations of non-LLM methods in achieving this task underscore its complexity, justifying the non-trivialness of \u201cversatile claim extraction\u201d. LLM-based methods enable claim extraction through customizable prompts, offering a generalizable solution across various scenarios. In contrast, non-LLM based methods are limited to each specific scenario, significant effort is needed to annotate a dataset to train a claim extractor with limited generalization ability.\n\n[1] Achakulvisut et al. Claim Extraction in Biomedical Publications Using Deep Discourse Model and Transfer Learning\n\n[2] Wadden et al. MULTIVERS: Improving scientific claim verification with weak supervision and full-document context"
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3682/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700731322458,
                "cdate": 1700731322458,
                "tmdate": 1700731322458,
                "mdate": 1700731322458,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JpkRkhHWcF",
                "forum": "jolYuxpVn1",
                "replyto": "78b19KLsxn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3682/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3682/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer tg4t Part 3"
                    },
                    "comment": {
                        "value": "> If it's important to distinguish detecting the factual error in generated text rather than other text, is it better to phrase the main task of this paper as hallucination detection rather than fact-checking? As fact-checking is commonly known as a task to check facts in text no matter if it's machine-generated or human-written.\n\nThe title of our paper, FacTool: Factuality Detection in Generative AI - A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios, highlights our focus on detecting diverse factual errors generated by generative AI (LLMs). The task that we focus on is \"Factuality Detection in Generative AI across Multi-Task and Multi-Domain Scenarios\"."
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3682/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700731455250,
                "cdate": 1700731455250,
                "tmdate": 1700732077450,
                "mdate": 1700732077450,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]