[
    {
        "title": "TRAM: Benchmarking Temporal Reasoning for Large Language Models"
    },
    {
        "review": {
            "id": "eDYKJ6d642",
            "forum": "EJvFFedM2I",
            "replyto": "EJvFFedM2I",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2864/Reviewer_PXAF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2864/Reviewer_PXAF"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on introducing a new benchmark for temporal reasoning tasks in a multi-choice format to evaluate the temporal reasoning capabilities of LLMs. There are 10 tasks collected in the benchmark covering Ordering, Frequency, Duration, Typical Time, Ambiguity Resolution, Arithmetic, Temporal Relation, Temporal NLI, Temporal Causality and Temporal Storytelling. The authors evaluated several leading LLMs including Llama2, PaLM2, GPT-3.5 and GPT-4 against the tasks, and reported the evaluations results. In addition, the authors also conducted error analysis of the results to understand where current LLMs are still struggling in temporal reasoning."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The set of tasks is very comprehensive to cover a wide variety of temporal reasoning task types. The dataset size is quite large, bigger than 3K samples for 9 out of 10 tasks, making the evaluation results robust against variance. The LLMs evaluated are also quite comprehensive covering both the SOTA proprietary and the SOTA open-source LLMs. The error analysis is also quite useful for understanding where the current LLMs still fall short in temporal reasoning."
                },
                "weaknesses": {
                    "value": "There are two major weaknesses to this work:\n1. The task difficulty of the TRAM benchmark is not enough to serve the purpose for ongoing evaluations of future more powerful LLMs. 8 out of 10 tasks have close to or better than 90% accuracy when evaluated on the SOTA LLM GPT-4 (5S, CoT). For the same 8 tasks, the gap between GPT-4 and Human is within 5%. This leaves very little headroom for improvement to validate future more powerful LLMs. \n\n2. While the benchmark is designed specific for temporal reasoning, it is not entirely clear how much failure in the errors are due to the model's lack of general reasoning capability, rather than specific to the time dimension. A more careful separation and attribution to either general reasoning and temporal reasoning failures is needed to make the benchmark really useful for gauging temporal reasoning progress."
                },
                "questions": {
                    "value": "1. The difference between TRAM and previous temporal reasoning benchmarks is unclear: for instance Duration and Frequency are already covered by earlier benchmarks. A more detailed and convincing comparison is needed to establish the necessity of the newly introduced TRAM benchmark.\n\n2. In Section 4.3, the authors claimed that they prompted the model to explain its decisions, and used these explanations to identify errors, understand the reasons, and categorize error types. This is very concerning as LLMs are known to not know what they don\u2019t know. Relying on LLMs to provide the explanations for error analysis puts a big question mark on the reliability of the error analysis conclusions themselves."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2864/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698638247280,
            "cdate": 1698638247280,
            "tmdate": 1699636230094,
            "mdate": 1699636230094,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "j55ognjB4n",
                "forum": "EJvFFedM2I",
                "replyto": "eDYKJ6d642",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2864/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2864/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer PXAF (Part 1)"
                    },
                    "comment": {
                        "value": "Reviewer PXAF, thank you for providing the insightful suggestions. We hope the following responses address your concerns.\n\n**Weakness 1: Difficulty of the benchmark.**\n\nWe appreciate your concern about the difficulty level of our TRAM benchmark. The purpose of this benchmark is to provide a comprehensive test that covers a wide range of temporal reasoning abilities for LLMs. As detailed in our paper, most tasks consist of multiple subtasks. Although the average SOTA performance is high, among the eight tasks you mentioned, which total 34 subtasks, there are 12 out of 34 subtasks where the GPT-4 (5S, CoT) setting performs at or below 80%. For instance, in this setting, the Ambiguity Resolution (calendar-shift) task achieves 53% accuracy, the Typical Time (comparison) task 77%, and the Arithmetic (date computation) task 78%. These results underscore specific areas that require further refinement, even in SOTA models.\n\nMoreover, the performance gap between GPT-4 and human benchmarks warrants particular attention. While this gap may seem modest (within 5%), achieving even incremental improvements is notably challenging as models approach human-level accuracy. Therefore, these seemingly small gaps are, in fact, substantial indicators of areas where future LLMs can and need to improve.\n\nMeanwhile, to address your concern and challenge the models more rigorously, we have considered some straightforward and effective methods, including introducing additional options and modifying question formats. To test the efficacy of these methods, we conducted case studies on the Causality and Storytelling tasks, which initially showed high performance. Under GPT-4 (5S, CoT), we reevaluated 50 samples from each task that were previously predicted correctly. When the distracting choices were contextually close to the correct answer, the model's performance dropped, with 9 out of 50 incorrect predictions for Causality and 17 out of 50 for Storytelling. In contrast, when distractors were clearly differentiated from the correct answer, the model had 5 out of 50 incorrect predictions for Causality and 13 out of 50 for Storytelling.\n\nEven in a SOTA setting, our deliberate introduction of additional options can decrease model performance by more than 10%. Additionally, converting some tasks to a short-answer format has been shown to affect performance; for example, the SOTA performance on the Arithmetic task dropped from 94.3% to 89.6%. We are dedicated to continuously refining the tasks in our benchmark and to providing a variety of question formats to meet different levels of model testing. Updates to question sets and results will be regularly shared on our GitHub page.\n\n**Weakness 2: Temporal vs. general reasoning in model errors.**\n\nWe designed our temporal reasoning (TeR) benchmark, recognizing the importance in understanding and reasoning about time. Although TeR is a specialized form of general reasoning, our aim is to isolate and evaluate this unique aspect due to its broad use in everyday life. In our error analysis, we distinguish between general reasoning and TeR when evaluating LLM capabilities. For example, in the Frequency task with the question \u2018If a person\u2019s job contract has a renewal every 4 years, and they started working in 1857 and renewed it 3 times without gaps, until what year is their current contract valid\u2019, GPT-4\u2019s error was categorized as \u2018Contextual Misjudgment\u2019. The model incorrectly assumed that each 4-year contract period concludes at the beginning of the following year, instead of at the end of the fourth year. This error demonstrates a misunderstanding in processing temporal information, rather than a general reasoning flaw. While time-related mistakes may occur within a broader reasoning context, our primary objective is to gauge a model\u2019s progress in TeR.\n\n**Question 1: TRAM vs. existing temporal reasoning benchmarks.**\n\nThe following table provides a comparison between our TRAM and previous temporal reasoning datasets, including task coverage and data size. The order of the prior work listed is from the most recent to the earlier ones."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2864/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700546618836,
                "cdate": 1700546618836,
                "tmdate": 1700546618836,
                "mdate": 1700546618836,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "T0DlWl48zT",
                "forum": "EJvFFedM2I",
                "replyto": "j55ognjB4n",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2864/Reviewer_PXAF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2864/Reviewer_PXAF"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you authors for the response. I am still not convinced that the tasks are hard enough to be useful for benchmarking future model developments. Therefore, I will still keep my original score.\n\nRegarding the task difficulty, I am not convinced by the argument that some subtasks are difficult and hence the whole benchmark has enough headroom for measuring progress. One can always pick the tail distribution and get low performance from the models. The numbers in Table 2 for 8 out of 10 tasks are close to or higher than 90%, leaving little headroom for improvements. For instance, Caus. has accuracy over 99% for most GPT, Llama2 and PaLM2 models. What is the point of using it to evaluate future models?\n\nRegarding the point of upon perturbing the dataset, the model performance drops. I don't think that is just an artificial way to make the tasks look harder for the model."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2864/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700616700240,
                "cdate": 1700616700240,
                "tmdate": 1700616700240,
                "mdate": 1700616700240,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "I7aB4AM2oe",
                "forum": "EJvFFedM2I",
                "replyto": "dMDCrVqVbI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2864/Reviewer_PXAF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2864/Reviewer_PXAF"
                ],
                "content": {
                    "comment": {
                        "value": "While I acknowledge that self-refine/self-debug/self-reflection could help the model to recover from the errors, there is typically some feedback provided such as execution traces. I am not convinced that the same model can reason about its own error without high-quality feedbacks. The authors mentioned expert review, which can serve as a guard, but doing so on each example level is simply impossible. I still have a lot of concerns on the reliability of the error analysis conducted in this paper through model explanations."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2864/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700617077333,
                "cdate": 1700617077333,
                "tmdate": 1700617077333,
                "mdate": 1700617077333,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "j2VYYixK75",
            "forum": "EJvFFedM2I",
            "replyto": "EJvFFedM2I",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2864/Reviewer_Z4nr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2864/Reviewer_Z4nr"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a new LLM-focused benchmark suite named TRAM. It consists of a variety of temporal reasoning tasks, and is aimed at driving progress. Experimental results demonstrate a gap between human-level and machine-level performance, suggesting progress to be had."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+ I generally like good benchmarks. I think they serve an important purpose in the community, which is to systematize comparisons (and ideally, drive progress).\n\n+ This benchmark seems well thought out, with a thoughtful and diverse set of temporal reasoning tasks.\n\n+ The empirical results suggest that (1) the benchmark discriminates between different models, highlighting their performance discrepancies, and (2) shows a gap between human and machine performance."
                },
                "weaknesses": {
                    "value": "- I like good benchmarks to be hard. I'm a bit concerned that SOTA performance on this benchmark starts at 84%; this perhaps suggests that the benchmark isn't hard enough.\n\n- I'm a bit concerned that some of the questions were sourced from other benchmarks. This could be problematic if it were, for example, included in a larger suite of benchmarks such as Google's BigBench.  I worry that some questions would be double-counted, leading to incorrect conclusions about model performance."
                },
                "questions": {
                    "value": "What percentage of questions, exactly, come from other benchmarks?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "n/a"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2864/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698807460381,
            "cdate": 1698807460381,
            "tmdate": 1699636230025,
            "mdate": 1699636230025,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Q2S2D1kG0u",
                "forum": "EJvFFedM2I",
                "replyto": "j2VYYixK75",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2864/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2864/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Z4nr (Part 1)"
                    },
                    "comment": {
                        "value": "Reviewer Z4nr, thank you for providing the insightful suggestions. We hope the following responses address your concerns.\n\n**Weakness 1: Difficulty of the benchmark.**\n\nWe appreciate your concern about the difficulty level of our TRAM benchmark. The purpose of this benchmark is to provide a comprehensive test that covers a wide range of temporal reasoning abilities for LLMs. As detailed in our paper, most tasks consist of multiple subtasks. Although the average state-of-the-art (SOTA) performance is high, particularly for 8 out of 10 tasks with accuracies close to or exceeding 90%, there are 12 out of 35 subtasks where the GPT-4 (5S, CoT) setting performs at or below 80%. For instance, in this setting, the Ambiguity Resolution (calendar-shift) task achieves 53% accuracy, the Typical Time (comparison) task 77%, and the Arithmetic (date computation) task 78%. These results underscore specific areas that require further refinement, even in SOTA models.\n\nMeanwhile, to address your concern and challenge the models more rigorously, we have considered some straightforward and effective methods, including introducing additional options and modifying question formats. To test the efficacy of these methods, we conducted case studies on the Causality and Storytelling tasks, which initially showed high performance. Under GPT-4 (5S, CoT), we reevaluated 50 samples from each task that were previously predicted correctly. When the distracting choices were contextually close to the correct answer, the model's performance dropped, with 9 out of 50 incorrect predictions for Causality and 17 out of 50 for Storytelling. In contrast, when distractors were clearly differentiated from the correct answer, the model had 5 out of 50 incorrect predictions for Causality and 13 out of 50 for Storytelling.\n\nEven in a SOTA setting, our deliberate introduction of additional options can decrease model performance by more than 10%. Additionally, converting some tasks to a short-answer format has been shown to affect performance; for example, the SOTA performance on the Arithmetic task dropped from 94.3% to 89.6%. We are dedicated to continuously refining the tasks in our benchmark and to providing a variety of question formats to meet different levels of model testing. Updates to question sets and results will be regularly shared on our GitHub page.\n\n**Weakness 2: Questions from other benchmarks.**\n\nThank you for your feedback regarding the use of questions from existing datasets in our benchmark. We acknowledge this as a limitation of our work and understand the potential issues it might pose, particularly in the context of larger benchmark suites where double-counting could lead to skewed conclusions about model performance. Similar to the GLUE and SuperGLUE benchmarks, our adoption of pre-existing datasets is based on their implicit acceptance within the NLP community. For questions sourced from existing datasets, we conducted significant preprocessing and reformulation to align them with the specific objectives and format of TRAM. Our preprocessing involved adapting the questions to effectively test temporal reasoning capabilities and reformatting them to fit the structure of our benchmark. This process included manually adding distracting or confusing options, filtering out irrelevant questions for quality control, and reformulating problems. Due to this extensive manual preprocessing, the overlap with larger benchmark suites like Google\u2019s BigBench should be minimal. To further address your concern and enhance LLM evaluation, we have undertaken the following actions and plans:\n\n1. We have conducted some case studies on tasks in our benchmark, including Causality and Storytelling, by a) adding additional options and b) modifying question formats. The reason for experimenting with a) is to challenge models, potentially confuse them, and test the nuanced reasoning capabilities of LLMs. On the other hand, b) is implemented to avoid the possibility that models can guess instead of truly reasoning. These direct methods have proven effective in our case studies, as demonstrated by the decrease in model performance. Meanwhile, even if models like GPT-4 have been exposed to existing datasets during pretraining, we can still present additional challenges, such as confusing options and challenging problem formats, to test their abilities."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2864/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700545828340,
                "cdate": 1700545828340,
                "tmdate": 1700546881289,
                "mdate": 1700546881289,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "PB7SPEt3Yc",
            "forum": "EJvFFedM2I",
            "replyto": "EJvFFedM2I",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2864/Reviewer_v5US"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2864/Reviewer_v5US"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes  a benchmarking dataset for temporal reasoning named TRAM. The TRAM benchmark evaluation is exemplified with BERT-style pretrained-finetuned models  and GPT style prompting-based LLMs. The authors intended to provide TRAM as a comprehensive benchmark to spur the LLM research progress in temporal reasoning capabilities."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "\u2022 A comprehensive benchmark covers various temporal reasoning abilities: ordering, frequency, duration, typical time, ambiguity, arithmetic, relation, temporal NLI, causality, storytelling. \n\u2022 The overall  size of the dataset is big, being 526,068 problems for benchmarking.\n\u2022 Pretraining-finetuning and prompting paradigms of LLMs are both evaluated using the benchmarking providing reasoning evaluation conclusions. It is a good starting point from which the community can evolve the LLM techniques  or other LLM alternatives for temporal reasoning."
                },
                "weaknesses": {
                    "value": "\u2022 The benchmark currently is only in the form of multi-choice questions.\n\u2022  The sizes of different categories of problems are imbalanced. For example, causality is of only 600 problems. This might render the benchmarking evaluation results misguiding. Especially for the pretraining-finetuning paradigms. \n\u2022  The texts are mostly from existing datasets. Latest LLMs might have seen them through the pretraining phrase crawled dataset. It might make the benchmarking results over-estimate the performance of LLMs in the temporal abilities. It is an issue beyond just the temporal reasoning abilities extending to all other LLM benchmarking datasets. It calls for more organic benchmarking approaches for LLMs and their iteration which can be pretrained with all kind of available data in human world including benchmarking data."
                },
                "questions": {
                    "value": "1. For the terms \"commonsense\", \"analogy inference\", \"comparison\" and so on, it would be better to have a formal definition and ensure that the datasets and the reasoning follow the formal definitions with verifiable criteria (automatically verifiable would be even better).\n2. Page 7, it would be better to include a more comprehensive list of the settings for human expert annotators evaluation. For example, how the experts are drawn from the population, how to ensure they are capable experts or their level of expertise. How will an ordinary person performance comparing to experts? Such a systematic study of human experiments might also provides hints for comparing human performance variation with LLM performance variation.\n3. Page 18, appendix B, for self-contained please detail a little bit more for SP with examples. The whole section is essentially about CoT with little information regarding how SP is constructed."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2864/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699168276411,
            "cdate": 1699168276411,
            "tmdate": 1699636229960,
            "mdate": 1699636229960,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9RNIWofD5r",
                "forum": "EJvFFedM2I",
                "replyto": "PB7SPEt3Yc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2864/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2864/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer v5US (Part 1)"
                    },
                    "comment": {
                        "value": "Reviewer v5US, thank you for providing the constructive suggestions. We hope the following responses address your concerns.\n\n**Weakness 1: Question format.**\n\nWe employ multiple-choice questions (MCQs) in TRAM to align with established practices in LLM benchmarks, such as the AI2 Reasoning Challenge (ARC) and Massive Multitask Language Understanding (MMLU), which also utilize MCQs. Additionally, the MCQ format is the most straightforward method for evaluating LLMs. Nevertheless, we recognize the potential limitations of this format. On our GitHub page, we have provided an alternative version of the benchmark in a short-answer format where appropriate, except for tasks like storytelling, ambiguity resolution, and causality. For these three types of questions, I have retained the MCQ format, as other formats could lead to answers that are too open-ended and difficult to evaluate. As part of our ongoing research, we remain open to incorporating more diverse question formats and adding more challenging questions to each task. These updates will be made available on our GitHub page, adapting to the evolving needs of the research community and the developing capabilities of LLMs.\n\n**Weakness 2: Imbalanced problems per category.**\n\nOur causality task is inspired by the Choice of Plausible Alternatives (COPA) dataset, which itself comprises only 1,000 questions. Given that COPA has set a precedent in the field, particularly for tasks centered around causal reasoning, the number of questions it contains serves as a benchmark for what can be considered sufficient for a robust evaluation. Furthermore, 93.5% of the questions in our original causality task are meticulously crafted by hand. This manual process, while ensuring the precision and applicability of each question, also limits the feasible scale of our dataset. To address the limited size of the causality category, we have added an additional, mirrored instance for each original instance. This expands our causality task to a total of 1,200 questions. Each mirrored instance uses the same alternatives as the corresponding original instance but introduces a new premise that matches the incorrect alternative of the original instance. We have updated the relevant information on this specific task in our revised paper, mainly including updated BERT-based results for causality and the task description. We are committed to continuously refining and potentially expanding this task, with updates to the question set being regularly posted on our GitHub page. This effort is specifically aimed at mitigating issues associated with the pretraining-finetuning paradigms, particularly those arising from limited training samples. \n\n**Weakness 3: Texts based on existing datasets.**\n\nThank you for your feedback regarding the use of questions from existing datasets in our benchmark. We acknowledge this as a limitation of our work and understand the issues it might pose, particularly in terms of potential overestimation of model capabilities. Similar to the GLUE and SuperGLUE benchmarks, our adoption of pre-existing datasets is based on their implicit acceptance within the NLP community. For questions sourced from existing datasets, we conducted significant preprocessing and reformulation to align them with the specific objectives and format of TRAM. Our preprocessing involved adapting the questions to effectively test temporal reasoning capabilities and reformatting them to fit the structure of our benchmark. This process included manually adding distracting or confusing options, filtering out irrelevant questions for quality control, and reformulating problems. Due to this extensive manual preprocessing, the overlap with larger benchmark suites like Google\u2019s BigBench should be minimal. To further address your concern and enhance LLM evaluation, we have undertaken the following actions and plans:\n\n1. We have conducted some case studies on tasks in our benchmark, including Causality and Storytelling, by a) adding additional options and b) modifying question formats. The reason for experimenting with a) is to challenge models, potentially confuse them, and test the nuanced reasoning capabilities of LLMs. On the other hand, b) is implemented to avoid the possibility that models can guess instead of truly reasoning. These direct methods have proven effective in our case studies, as demonstrated by the decrease in model performance. Meanwhile, even if models like GPT-4 have been exposed to existing datasets during pretraining, we can still present additional challenges, such as confusing options and challenging problem formats, to test their abilities."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2864/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700545175502,
                "cdate": 1700545175502,
                "tmdate": 1700545175502,
                "mdate": 1700545175502,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "68n9OgNv0v",
                "forum": "EJvFFedM2I",
                "replyto": "76oKIRjGma",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2864/Reviewer_v5US"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2864/Reviewer_v5US"
                ],
                "content": {
                    "comment": {
                        "value": "Thank the authors for clarifying my questions. Please improve the paper as other reviewers suggested. I don't have further questions. I will keep my original rating."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2864/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700727932613,
                "cdate": 1700727932613,
                "tmdate": 1700727932613,
                "mdate": 1700727932613,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "BLgmhQjU2n",
            "forum": "EJvFFedM2I",
            "replyto": "EJvFFedM2I",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2864/Reviewer_rYw6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2864/Reviewer_rYw6"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a temporal reasoning benchmark for large language models. The benchmark is run on a collection of ten datasets containing thirty-eight subtasks related to time reasoning problems. The datasets used for the temporal reasoning task are formulated as multiple-choice problems, reconstructed from several existing datasets. The paper evaluates the performance of several LLMs on the curated datasets in few-shot learning settings. The experimental results show that there is still room for improvement in enhancing the temporal reasoning abilities of these models."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1, The author introduces a new dataset and benchmark for evaluating the temporal reasoning capabilities of large language models with sufficient amounts of data in different time-related domains, including duration, frequency, ordering, etc. \n\n2, The author provides an in-detail description of the format of the benchmark dataset. \n\n3, The author provides a comprehensive experimental evaluation of popular LLMs, including GPT-4, GP3-3.5, and Llamma2 on the TRAM benchmark. \n\n4, the author provides error analysis on different task groups, this can help researchers prioritize their efforts and further improve the temporal reasoning abilities of LLMs in the future."
                },
                "weaknesses": {
                    "value": "1, As the paper primarily focuses on the area of datasets and benchmarks in large language models, it is better to provide an anonymous GitHub page, for example (https://anonymous.4open.science/) with code for dataset curation and empirical evaluation, as well as simple documentation on running the LLM\u2019s assessments. \n\n---\n\n2, At this point, the overall contribution of the dataset curation done by the authors is unclear. It is better to provide some examples for comparing the differences between the source dataset and the provided curated dataset. \nWith some manual comparison between the shared supplementary materials and the source dataset repos (https://github.com/CogComp/MCTACO/tree/master) and (https://rajpurkar.github.io/SQuAD-explorer/). The author seems to simply reformulate multiple original Yes/No questions in one multiple-choice question. For example, in the \u2018frequency\u2019 task, the original question from MCTACO looks like:\n\nQ1: For example, buy local produce at a farmers market, like the one in Figure 2.20. How often do they buy produce from the farmers market?\t\ntwice a week\tyes\tFrequency\n\nQ2: For example, buy local produce at a farmers market, like the one in Figure 2.20. How often do they buy produce from the farmers market?\the says for example\tno\tFrequency\n\n\u2026\u2026\n\nQ8: For example, buy local produce at a farmers market, like the one in Figure 2.20. How often do they buy produce from the farmers market?\tonce a second\tno\tFrequency\n\nHowever, the provided dataset in this paper takes the above 8 original questions and reformulates the question as (in file frequency.csv): \n\nQ1: For example, buy local produce at a farmers market, like the one in Figure 2.20. How often do they buy produce from the farmers market?\t\n\nAnswer A: twice a second\n\nAnswer B: he says for example\t\n\nAnswer C: twice a week\t\n\nCorrect Answer: C\n\nFirst, the reconstructed candidate choices contain answers that do not correlate with time (Answer B), which is caused by the error from the original data source. It\u2019s better to provide an overall evaluation of the data quality.  Second, it is better to explain the advantages of converting several existing Yes/No questions to one multiple-choice question. \n\n---\n\n3, It is better to provide an overall description of the dataset following the datasheet for datasets [1] (or other similar sources), I believe this may address most of the concerns. \n\n---\n\n4, The experimental results show a disparity in performance across different Large Language Models (LLMs). In addition, with the integration of chain-of-thought prompting, the results show only minor improvements. In addition, GPT-4 appears to outperform every other model by a large margin. It is unclear whether the provided model is already trained on the source dataset (MCTACO) from which this benchmark dataset is derived. Here is the answer I got from the GPT-3.5 and GPT-4 by prompting the question: \u2018Please provide a detailed description of the MCTACO dataset for temporal reasoning.\u2019 \n\nGPT-3.5: \nI'm sorry, but as of my last knowledge update in January 2022, I do not have specific information about the \"MCTACO\" dataset for temporal reasoning. It's possible that this dataset was created or became publicly available after my last update, or it may not be a well-known dataset in the field of natural language processing\u2026\u2026.. \n\nGPT-4:\nThe MCTACO (Multiple Choice Temporal Commonsense Reasoning Assessment) dataset is a collection of questions designed for evaluating the temporal commonsense reasoning abilities of machine learning models. This dataset is particularly focused on the aspect of understanding time-based common sense or temporal common sense, which is essential for natural language understanding systems\u2026\u2026.\nIt is better to provide some justifications for the problem mentioned above.\n\n---\n\n[1]: Gebru, Timnit, et al. \"Datasheets for datasets.\" Communications of the ACM 64.12 (2021): 86-92."
                },
                "questions": {
                    "value": "My questions are listed in the weakness, and can be summarized into three folds:\n\n1, Did the author do a lot of data preprocessing on curating the benchmark dataset?\n\n2, What are the advantages of reformulating the original data sources into the TRAM dataset? What is the major differences between the source dataset and the curated dataset? \n\n3, What causes the significant disparity in model performance? Is it because some LLMs are already trained on the source dataset?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2864/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2864/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2864/Reviewer_rYw6"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2864/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699224124386,
            "cdate": 1699224124386,
            "tmdate": 1700647669510,
            "mdate": 1700647669510,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "G1f4axseeL",
                "forum": "EJvFFedM2I",
                "replyto": "BLgmhQjU2n",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2864/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2864/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer rYw6 (Part 1)"
                    },
                    "comment": {
                        "value": "Reviewer rYw6, thank you for providing the detailed suggestions. We hope the following responses address your concerns.\n\n**Weakness 1: GitHub page.**\n\nFollowing your suggestion, we have set up an anonymous GitHub page. Here is the link: https://anonymous.4open.science/r/TRAM-Benchmark-596D/.\n\n**Weakness 2/Question 2: Overall contribution of the work.**\n\n Thank you for your insightful suggestion about clarifying the contribution of our work and the rationale behind the transformation of existing data sources into the TRAM dataset. The following table provides a comparison between our TRAM and previous temporal reasoning datasets, including task coverage and data size. The order of the prior work listed is from the most recent to the earlier ones.\n\n| Dataset/Benchmark | Size  | Task Coverage                                                  |\n|-------------------|-------|---------------------------------------------------------------|\n| TRAM              | 526.7k| Ordering, Frequency, Duration, Typical Time, Ambiguity Resolution, Arithmetic, Relation, Temporal NLI, Causality, Storytelling |\n| MenatQA           | 2.9k  | Scope, Order, Counterfactual                                   |\n| TEMPREASON        | 52.8k | Facts                                                          |\n| TEMPLAMA          | 50k   | Facts                                                          |\n| Time-Sensitive QA | 41.2k | Facts                                                          |\n| TIMEDIAL          | 1.1k  | Commonsense                                                    |\n| MCTACO            | 13k   | Commonsense (duration, temporal ordering, typical time, frequency, stationarity) |\n\nCompared to earlier works, our dataset offers a much larger data size and broader task coverage, encompassing 10 main tasks and 38 subtasks. For instance, in the classic tasks of \u2018Duration\u2019 and \u2018Frequency\u2019, earlier works mainly cover Facts and Commonsense. In contrast, our work includes 7 subtasks for \u2018Duration\u2019 and 6 subtasks for \u2018Frequency\u2019, delving into more nuanced aspects of these temporal elements, which are less emphasized by earlier works. Consequently, the extensive data size of TRAM and the intricate design of its subtasks represent a notable change in both the depth and breadth of temporal reasoning (TeR) evaluation, compared to existing benchmarks.\n\nFollowing your suggestion, we have included examples in Appendix F (pages 23-24) that compare our curated dataset with the source datasets, specifically MCTACO and SQuAD. For information on data quality, please refer to the datasheets available on our anonymous GitHub page. The reformulation of original data sources into the TRAM benchmark offers several advantages:\n\n**Increased Complexity and Depth**: By converting simple Yes/No questions to multiple-choice format, we inherently increase the complexity of each question. This change demands that models not only identify the correct answer but also discern and reject multiple plausible but incorrect options, thereby enhancing the depth of temporal reasoning evaluation.\n\n**Reduction in Guesswork**: The multiple-choice format reduces the likelihood of correct guesses compared to Yes/No questions, demanding a more nuanced understanding and reasoning, and thus providing a clearer assessment of a model's temporal reasoning capabilities.\n\nIn summary, we have chosen to use existing benchmarks due to their wide acceptance within the NLP community. The multiple-choice format, effective for evaluating LLMs as seen in benchmarks like the AI2 Reasoning Challenge (ARC) and Massive Multitask Language Understanding (MMLU), introduces more complexity for the model and challenges it to discern subtle differences in temporal reasoning contexts. Our benchmark combines the strengths of existing benchmarks with extensive manual work, setting a new standard for assessing temporal reasoning in LLMs. To address the evolving needs of advanced model evaluation, we will regularly share updates to our question sets and results on our GitHub page.\n\n**Weakness 3/Question 1: Overall dataset description.**\n\nThank you for providing the insightful advice. Following your suggestions, we have created a datasheet for the TRAM benchmark, which is available on the anonymous GitHub page (datasheet_for_TRAM_benchmark.pdf). Please let us know if there are any additional aspects you think would be worthwhile to incorporate into the datasheet. We are open to suggestions and will gladly update it accordingly."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2864/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700547671598,
                "cdate": 1700547671598,
                "tmdate": 1700547671598,
                "mdate": 1700547671598,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "62wLL020ao",
                "forum": "EJvFFedM2I",
                "replyto": "BLgmhQjU2n",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2864/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2864/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer rYw6 (Part 2)"
                    },
                    "comment": {
                        "value": "**Weakness 4/Question 3: Performance disparity among LLMs.**\n\nThank you for your feedback regarding the use of questions from existing datasets in our benchmark. We acknowledge this as a limitation of our work and understand the issues it might pose, particularly in terms of performance disparity among different LLMs. Similar to the GLUE and SuperGLUE benchmarks, our adoption of pre-existing datasets is based on their implicit acceptance within the NLP community. For questions sourced from existing datasets, we conducted significant preprocessing and reformulation to align them with the specific objectives and format of TRAM. Our preprocessing involved adapting the questions to effectively test temporal reasoning capabilities and reformatting them to fit the structure of our benchmark. This process included manually adding distracting or confusing options, filtering out irrelevant questions for quality control, and reformulating problems. The performance disparity, especially the superior results of GPT-4, can be attributed to varying architectures, training dataset scales, and sizes of the models. While GPT-4 may have been exposed to these datasets during pretraining, our manual work in reformulating the tasks aims to introduce new dimensions and challenges, thus minimizing any potential advantage. To address both this issue and the need for advanced LLM evaluation, we have undertaken the following actions and plans:\n\n1.  We have conducted some case studies on tasks in our benchmark, including Causality and Storytelling, by a) adding additional options and b) modifying question formats. The reason for experimenting with a) is to challenge models, potentially confuse them, and test the nuanced reasoning capabilities of LLMs. On the other hand, b) is implemented to avoid the possibility that models can guess instead of truly reasoning. These direct methods have proven effective in our case studies, as demonstrated by the decrease in model performance. Meanwhile, even if models like GPT-4 have been exposed to existing datasets during pretraining, we can still present additional challenges, such as confusing options and challenging problem formats, to test their abilities.\n\n2. We will prepare and craft more challenging questions for our benchmark, such as reframing existing tasks by introducing novel contexts or counterfactual elements, and introducing a set of novel tasks that require non-linear temporal reasoning. These tasks might include analyzing events in reverse chronological order or understanding cyclical time concepts, which are less likely to be present in standard training datasets. We are dedicated to continuously refining the tasks in our benchmark and providing a variety of question formats to meet different levels of model testing. Updates to question sets and results will be regularly shared on our GitHub page."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2864/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700547784392,
                "cdate": 1700547784392,
                "tmdate": 1700602299720,
                "mdate": 1700602299720,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "19L91gpxrx",
                "forum": "EJvFFedM2I",
                "replyto": "DyGBzRhJJm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2864/Reviewer_rYw6"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2864/Reviewer_rYw6"
                ],
                "content": {
                    "title": {
                        "value": "Response to the author"
                    },
                    "comment": {
                        "value": "Thank you so much for the detailed clarification and thank you again for providing an anonymous GitHub repository and datasheet for the TRAM benchmark. \n\n---\nHowever, I still have the following concerns: \n\nI ran several data generation codes, and I noticed that for some topics, a proportion of the questions are generated in a simple format. \n\nFor example, In Arithmetic tasks (https://anonymous.4open.science/r/TRAM-Benchmark-596D/data_processing/arithmetic.ipynb)\nI have the following output questions from \u2018df_month\u2019: \n\n---\n\nQuestion 1: Which month comes 3 months after April?\n\nQuestion 2: Which month was 4 months before December?\n\n\u2026\u2026\n\nQuestion 500: Which month was 2 months before February?\n\nAnd \u2018df_time_op\u2019 gives questions like:\n\nQuestion 1: Convert 214 minutes into hours.\n\nQuestion 2: Convert 398 minutes into hours.\n\n\u2026\u2026\n\nQuestion 1500: Convert 4 days into minutes.\n\n---\n\nIn addition, some questions are generated with limited templates, for example in frequency tasks (https://anonymous.4open.science/r/TRAM-Benchmark-596D/data_processing/frequency.ipynb): \n\nI have the following outputs from \u2018df_abstract_frequency\u2019:\n\n---\n\nQuestion 1: If an athlete trains every 2 days, starting on Wednesday, on which day will they train next?\n\nQuestion 2: A town hosts a carnival every 4 years in November. If the preceding carnival was in 1937, when will the subsequent one occur?\n\nQuestion 3: On Planet Alpha, 1 day is equivalent to 80 Earth days. How many Earth days elapse between daily events on Alpha?\n\n\u2026\u2026\n\nQuestion 997: A town hosts a carnival every 2 years in September. If the preceding carnival was in 1895, when will the subsequent one occur?\n\nQuestion 998: A town hosts a carnival every 3 years in February. If the preceding carnival was in 1942, when will the subsequent one occur?\n\nQuestion 999: In a spaceship experiencing time dilation, 1 year inside equates to 37 Earth years. If an event occurs inside every year, how frequently is it observed from Earth?\n\nQuestion 1000: A town hosts a carnival every 5 years in January. If the preceding carnival was in 1845, when will the subsequent one occur?\n\n---\n\nWhere these 1000 questions are generated using 10 templates. Similarly, the Ambiguity resolution tasks: (https://anonymous.4open.science/r/TRAM-Benchmark-596D/data_processing/ambiguity_resolution.ipynb), where 300 questions are constructed by converting calendars between Gregorian, Julian, Hebrew, and Islamic:\n\nQuestion 1: If the date is 9/6/1868 in the Gregorian, what is the date in the Hebrew?\n\nQuestion 2: If the date is 5/7/1805 in the Gregorian, what is the date in the Islamic?\n\n\u2026\u2026\n\nQuestion 300: If the date is 3/23/1814 in the Gregorian, what is the date in the Julian?\n\n---\n\n**However, I do appreciate the efforts in some other subtask datasets like:**\n\n'df_facts', 'df_multistep_comparison' and 'df_inference' from duration (https://anonymous.4open.science/r/TRAM-Benchmark-596D/data_processing/duration.ipynb)\n\n'df_implicit_phrases' from ambiguity resolution (https://anonymous.4open.science/r/TRAM-Benchmark-596D/data_processing/ambiguity_resolution.ipynb)\n\n---\n\nAlso, according to the provided source code, I believe the author has made enough effort to curate this dataset. Therefore I adjusted my score from 5 to 6 to recommend borderline acceptance."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2864/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700647621633,
                "cdate": 1700647621633,
                "tmdate": 1700647621633,
                "mdate": 1700647621633,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "HOWhURTrwH",
            "forum": "EJvFFedM2I",
            "replyto": "EJvFFedM2I",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2864/Reviewer_fFct"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2864/Reviewer_fFct"
            ],
            "content": {
                "summary": {
                    "value": "This work introduces various temporal reasoning tasks in MCQ format with a single correct answer. These tasks include Ordering, Frequency, Duration, Time, Ambiguity Resolution, Arithmetic, Relation, Temporal NLI, Causality, Storytelling. This work also includes the results of these datasets on various LLMs like GPT-4, GPT-3.5, Llama2, and Palm2 in settings like zero-shot, few-shot with standard prompting, and Chain of Thought. Furthermore, they also include results from BERT and RoBERTa. The authors also did a thorough error analysis to find out where the models were going wrong and gain a better understanding of the mistakes the models were making."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* Detailed description of dataset creation, sources, templates, and prompts used.\n* Insightful error analysis, which investigated every specific error type at a task level.\n* Results on several LLMS like GPT-4/3.5, Llama2, Palm2"
                },
                "weaknesses": {
                    "value": "* There are many specifically designed models to solve temporal reasoning. None of these models are included in the benchmarks. Without these, it is difficult to compare results between LLMs and RoBERTa or BERT. What goodness that LLMs bring in which tasks compared to these special models which are smaller compared to LLMs?\n\n[1] Yuan, Weizhe, and Pengfei Liu. \"reStructured Pre-training.\"  (2022) \n\n[2] Ben Zhou, Kyle Richardson, Qiang Ning, Tushar Khot, Ashish Sabharwal, and Dan Roth. (2021). \"Temporal Reasoning on Implicit Events from Distant Supervision.\""
                },
                "questions": {
                    "value": "* How does the length of questions in MCQ affect performance? And adding more options, will it confuse models?\n* In error analysis, how many mistaken samples were analyzed for each task?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2864/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699293354084,
            "cdate": 1699293354084,
            "tmdate": 1699636229798,
            "mdate": 1699636229798,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jqAWIH0bz0",
                "forum": "EJvFFedM2I",
                "replyto": "HOWhURTrwH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2864/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2864/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer fFct (Part 1)"
                    },
                    "comment": {
                        "value": "Reviewer fFct, thank you for providing the insightful suggestions. We hope the following responses address your concerns.\n\n**Weakness 1: Temporal reasoning model comparison.**\n\nThank you for highlighting the need for a comparison with domain-specific models. Following your suggestion, we have conducted additional experiments using a temporal reasoning model (https://huggingface.co/GAIR/rst-temporal-reasoning-11b), referred to as RST-TeR below, where the training and testing sets are the same as those used for BERT-based models. We have also included results from RoBERTa-large and GPT-4 for reference and comparison:\n\n| Model            | Order | Freq. | Dur. | Typical Time | Amb. Res. | Arith. | Rel.       | NLI        | Caus.      | Story | Average | \n|-------------------|-------|------|------|--------------|-----------|--------|------------|------------|------------|-------|---------|\n| GPT-4 (5S, CoT)  | 71.5        | 93.7 | 93.0         | 90.2      | 89.8   | 94.3       | 69.5/69.1  | 90.7/91.0  | 100.0 | 96.3    | 87.4       |\n| RoBERTa-large    | 55.5        | 57.7 | 55.4         | 60.0      | 41.0   | 29.1       | 90.0/90.0  | 70.0/70.3  | 88.0  | 84.0    | 65.9       |\n| RST-TeR          | 54.5        | 56.2 | 52.3         | 58.7      | 39.8   | 31.6       | 91.5/91.6  | 68.2/68.7  | 87.5  | 82.2    | 65.2       |\n\nBased on these results, RST-TeR achieves performance on par with the RoBERTa-large model. However, there is a significant gap between this domain-specific model and LLMs like GPT-4, except in the Relation (Rel.) task, where RST-TeR is specifically trained. GPT-4\u2019s extensive training across varied contexts contributes to its robust performance, showcasing its superior generalization capabilities. Overall, LLMs bring depth and versatility to temporal reasoning, enabling them to handle a range of tasks that demand a comprehensive and integrative approach, an aspect where specialized models may be more limited.\n\n**Question 1: Impact of length of questions and number of options on model performance.**\n\nThank you for raising the insightful question about how the length of questions and the number of options in MCQs affect model performance. Here are the statistics for the average length of questions, options, and their combined average for each task:\n\n| Category             | Avg. Question Length | Avg. Option Length | Avg. Combined Length |\n|----------------------|----------------------|--------------------|----------------------|\n| Ordering           |        56.8                 | 7.8                | 64.6                 |\n| Frequency            | 25.3                 | 9.3                | 34.6                 |\n| Duration             | 41.7                 | 8.0                | 49.7                 |\n| Typical Time         | 98.1                 | 4.6                | 102.7                |\n| Ambiguity Resolution | 20.7                 | 5.2                | 25.9                 |\n| Arithmetic           | 13.3                 | 6.9                | 20.2                 |\n| Relation             | 53.9                 | 3.3                | 57.2                 |\n| NLI                  | 44.1                 | 3.0                | 47.1                 |\n| Causality            | 14.2                 | 16.5               | 30.7                 |\n| Storytelling               | 48.3                 | 17.7               | 66.0                 |\n\nBased on our experiments, we have the following observations:\n\n**Length of questions vs. Performance**: There is no direct correlation between question/option length and model performance. For instance, tasks with longer questions such as Typical Time and Storytelling have some of the highest accuracies, indicating that the additional context from longer questions can be beneficial. Conversely, shorter tasks like Arithmetic and Ambiguity Resolution do not necessarily result in higher performance, suggesting that brevity, which may reduce context, does not automatically improve performance. For the tasks in our study, as there is no substantial difference in length, the intrinsic task complexity appears to have a more pronounced effect on the final performance rather than the question length.\n\n**Length of options vs. Performance**: Longer option lengths in tasks like Causality and Storytelling do not lead to uniform decreases in performance, implying that detailed options may provide valuable context that helps the model deduce the correct answer. However, if the added length introduces ambiguity or irrelevant details, it could confuse the model."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2864/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700544562804,
                "cdate": 1700544562804,
                "tmdate": 1700544562804,
                "mdate": 1700544562804,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "g0n9H9MM6u",
                "forum": "EJvFFedM2I",
                "replyto": "hTcVzywBLb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2864/Reviewer_fFct"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2864/Reviewer_fFct"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the very detailed comments.\n\nI have a few questions\n* In the above results, Is RST-TeR fine-tuned or prompted in a zero-shot or few-shot way?"
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2864/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700724583644,
                "cdate": 1700724583644,
                "tmdate": 1700724583644,
                "mdate": 1700724583644,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]