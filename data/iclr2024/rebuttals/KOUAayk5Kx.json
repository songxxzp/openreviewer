[
    {
        "title": "Defying Multi-model Forgetting: Orthogonal Gradient Learning to One-shot Neural Architecture Search"
    },
    {
        "review": {
            "id": "191DUp6PgC",
            "forum": "KOUAayk5Kx",
            "replyto": "KOUAayk5Kx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6907/Reviewer_9XeX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6907/Reviewer_9XeX"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the main objective of the research is to train a supernet effectively to overcome the problem of multi-model forgetting in one-shot Neural Architecture Search (NAS). To address this issue, the authors propose a method called Orthogonal Gradient Learning (OGL) to update the weights of the current architecture in a way that they become orthogonal to the constructed gradient space. A series of experiments are conducted in this paper on multiple datasets to evaluate the effectiveness of OGL in addressing the multi-model forgetting problem."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "\uff081\uff09The logic of this paper is clear and it is easy to read.\n\uff082\uff09OGL offers a fresh perspective for one-shot NAS, especially in addressing the multi-model forgetting issue. Compared to existing suboptimal methods, it exhibits superior performance across multiple datasets."
                },
                "weaknesses": {
                    "value": "\uff081\uff09Figure 4: (a) is quite messy, and the curves for different network architectures cannot be clearly distinguished and compared. Please provide a more intuitive explanation and presentation method.\n\uff082\uff09Although RandomNAS-OGL has a slight advantage in test error rate in Table 2, PDARTS is superior in terms of model size and computational complexity.All things considered, I believe the latter is more superior than your model. Please find a better approach to optimize your model.\n\uff083\uff09The storage and computational overhead are issues you need to consider at the moment, as they will greatly limit you in real-world application scenarios.\n\uff084\uff09When comparing performance, many methods were not reprouced or their true performance metrics were not obtained. Therefore, I believe your comparison is lacking and not comprehensive."
                },
                "questions": {
                    "value": "Please refer to the weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "I do not find any ethical problem in this paper."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6907/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698463363286,
            "cdate": 1698463363286,
            "tmdate": 1699636803384,
            "mdate": 1699636803384,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "WhjbSd2uwc",
                "forum": "KOUAayk5Kx",
                "replyto": "191DUp6PgC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6907/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6907/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors"
                    },
                    "comment": {
                        "value": "**Weakness1**: Thanks for your comments! It is true that Fig.4 is unable to see the relationships between different architectures, and we refer to [1] and find a new visualization method to show the changes of the validation accuracy of architectures and the impact between different architectures. New Figures and related descriptions have been correspondingly updated in our paper.\n\n[1] Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Yong Guo, Peilin Zhao, Junzhou Huang, and Mingkui Tan. Disturbance-immune weight sharing for neural architecture search. Neural Networks, 144: 553\u2013564, 2021.\n\n**Weakness2**: Thanks for your insightful and constructive advice. We agree with your opinion. The use of traditional RandomNAS as the baseline in this study is to verify the effectiveness of OGL in solving the multi-model forgetting issue. However, we think OGL will get better results with PDARTS for the following reasons. 1. PDARTS is an improvement over DARTS, especially on the storage to train large models. PDARTS divides the search process into three stages, and increases the depth of the network while reducing the types of operations. Therefore, PDARTS has lower search costs and smaller model size than others; 2. OGL projects the direction of weight update in baselines on the orthogonal direction of the gradient space, which has been proved in Lemma 1 that the orthogonal direction of weight update shows slightly influence on the performance of the previously trained architectures. In other words, the multi-model forgetting issue is largely alleviated. \n\nBased on the reasons above. We believe that applying OGL to PDARTS (PDARTS-OGL) can get better results. We will consider PDARTS-OGL in our future work and apply OGL to more other excellent methods."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6907/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700476449633,
                "cdate": 1700476449633,
                "tmdate": 1700476449633,
                "mdate": 1700476449633,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PDmEAErHc9",
                "forum": "KOUAayk5Kx",
                "replyto": "191DUp6PgC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6907/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6907/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking forward to your feedback"
                    },
                    "comment": {
                        "value": "Dear Reviewer,\n\nWe would like to express our sincere gratitude for taking the time to review our paper and providing valuable feedback. We appreciate your expertise and insights, which are significant in enhancing the quality of our work.\n\nWe are eagerly awaiting your response to our rebuttal, as your comments and suggestions are crucial to the further development of our research. We understand that your time is precious, and we respect your commitment to this process.\n\nIn our rebuttal, we have thoroughly addressed the issues raised and provided comprehensive discussions and revisions. If there is any additional information you require or further clarification needed, please do not hesitate to reach out to us. We are more than willing to explain in any way possible.\n\nOnce again, we appreciate your dedication and effort in reviewing our paper. Your prompt attention to our rebuttal would be highly appreciated, as it greatly contributes to the timely progress of our research. \n\nThank you for your consideration, and we look forward to receiving your valuable feedback.\n\nSincerely,\n\nThe authors of Submission6907"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6907/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700704025165,
                "cdate": 1700704025165,
                "tmdate": 1700704025165,
                "mdate": 1700704025165,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "aDPZeWvQ34",
            "forum": "KOUAayk5Kx",
            "replyto": "KOUAayk5Kx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6907/Reviewer_iP4N"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6907/Reviewer_iP4N"
            ],
            "content": {
                "summary": {
                    "value": "The work focus on solving the problem of multi-model forgetting in neural architecture search (NAS) . To address this problem, the authors propose an Orthogonal Gradient Learning (OGL) for one-shot NAS-guided supernet training. This method updates the weights of the overlapping structures of the current architecture in directions orthogonal to the gradient space of these structures in all previously trained architectures.\n\nThe authors provide experimental evidence supporting the effectiveness of the proposed paradigm in mitigating multi-model forgetting."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "the authors propose the Orthogonal Gradient Learning (OGL) . This method updates the weights of the overlapping structures of the current architecture in directions orthogonal to the gradient space of these structures in all previously trained architectures.\n\nThe authors provide experimental evidence supporting the effectiveness of the proposed paradigm in mitigating multi-model forgetting."
                },
                "weaknesses": {
                    "value": "The proposed orthogonal gradient learning (OGL) guided supernet training method may be sensitive to hyperparameters. The paper should conduct a more detailed analysis of the impact of hyperparameters on the robustness of the method.\n\nThis paper mentions the theoretical support for the proposed approach, but the assumptions made in these theoretical proofs and their relevance to actual NAS scenarios should be detailed."
                },
                "questions": {
                    "value": "This paper aims to reduce the computational budget in NAS; it should provide a more complete analysis of the computational cost introduced by the OGL approach, as this can be an issue in resource-constrained environments."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6907/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6907/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6907/Reviewer_iP4N"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6907/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698808213936,
            "cdate": 1698808213936,
            "tmdate": 1699636803243,
            "mdate": 1699636803243,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "QiML1DLDUB",
                "forum": "KOUAayk5Kx",
                "replyto": "aDPZeWvQ34",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6907/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6907/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors"
                    },
                    "comment": {
                        "value": "**Weakness1**: Many thanks for your comments! OGL is involved in a hyperparameter $d$ in $G_{dim}\\in \\mathbb{R}^{h \\times d}$, which is the column dimension of the gradient space, i.e., the number of the gradient vectors. However, the parameter $d$ is self-adjusted with the optimization process. Specifically, it is known that a space can be spanned by the maximally linear independent set, and PCA used in this study is to find the independent set of the gradient space. Once the independent set is found, $d$ here is determined since it is set to the number of vectors in the maximally linear independent set. Accordingly, we did not perform the parameter analysis of the proposed method.\n\n**Weakness2**:  Many thanks for your valuable feedback on the assumptions made in the theoretical proofs.\n\nIn the proof of lemma 1, there is a remainder term (i.e.,$R_1(w)$) of Taylor expansion of the loss function, namely, the infinitesimal of the first order.  In practice, we ignored this term for theoretical proof and drew a conclusion that the update of weights along the orthogonal direction of the gradient of this operation will slightly change the accurancy of the trained architecture. However, we must admit that ignoring $R_1(w)$ will impact the accuracy of the trained architecture to some extent, but the impact is tiny. Accordingly, we have emphasized the conclusion in the manuscript that the OGL guided training paradigm enables the training of the current architecture to largely eliminate the impact to the performance of all previously trained architectures."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6907/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700476291599,
                "cdate": 1700476291599,
                "tmdate": 1700476291599,
                "mdate": 1700476291599,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kyHcVDfW44",
                "forum": "KOUAayk5Kx",
                "replyto": "aDPZeWvQ34",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6907/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6907/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking forward to your feedback"
                    },
                    "comment": {
                        "value": "Dear Reviewer,\n\nWe would like to express our sincere gratitude for taking the time to review our paper and providing valuable feedback. We appreciate your expertise and insights, which are significant in enhancing the quality of our work.\n\nWe are eagerly awaiting your response to our rebuttal, as your comments and suggestions are crucial to the further development of our research. We understand that your time is precious, and we respect your commitment to this process.\n\nIn our rebuttal, we have thoroughly addressed the issues raised and provided comprehensive discussions and revisions. If there is any additional information you require or further clarification needed, please do not hesitate to reach out to us. We are more than willing to explain in any way possible.\n\nOnce again, we appreciate your dedication and effort in reviewing our paper. Your prompt attention to our rebuttal would be highly appreciated, as it greatly contributes to the timely progress of our research. \n\nThank you for your consideration, and we look forward to receiving your valuable feedback.\n\nSincerely,\n\nThe authors of Submission6907"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6907/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700703995508,
                "cdate": 1700703995508,
                "tmdate": 1700703995508,
                "mdate": 1700703995508,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "bc0VoIIdhW",
            "forum": "KOUAayk5Kx",
            "replyto": "KOUAayk5Kx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6907/Reviewer_74aa"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6907/Reviewer_74aa"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new method called Orthogonal Gradient Learning (OGL) to overcome multi-model forgetting in one-shot NAS. It updates weights of overlapped structures in the orthogonal direction to the gradient space of previously trained architectures. This avoids overwriting well-trained models while training new architectures sequentially. A PCA-based projection is used to find orthogonal directions without storing all past gradient vectors. OGL is integrated into RandomNAS and GDAS one-shot NAS baselines. Experiments show OGL reduces forgetting, leading to better final architectures and stronger supernet predictive ability."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "**Strengths**:\n\n- Original idea of using orthogonal gradient updates to avoid catastrophic forgetting in NAS.\n\n- Technically sound approach grounded in theory with clear algorithm design and experimental methodology.\n\n- Strong empirical results demonstrating reduced forgetting and improved search performance compared to baselines.\n\n- The PCA-based projection to compute orthogonal gradients is creative and helps address a key limitation.\n\n- OGL seems widely applicable to enhance different one-shot NAS methods as shown by results on two baselines."
                },
                "weaknesses": {
                    "value": "**Weaknesses**:\n\n- Theoretical analysis is limited, more formal convergence guarantees could strengthen the approach.\n\n- Certain details like schedule for gradient space updates are unclear. Sensitivity to hyper-parameters not fully studied.\n\n- Experiments focus on small CNN search spaces, evaluating on larger spaces like transformers could be useful.\n\n- Qualitative analysis into why and how OGL architectures differ from baseline NAS would provide more insight. \n\n- Extending OGL to other architecture search domains like hyperparameter optimization could further demonstrate generality."
                },
                "questions": {
                    "value": "see Weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "no"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6907/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699101938093,
            "cdate": 1699101938093,
            "tmdate": 1699636803122,
            "mdate": 1699636803122,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "eLKSfdDjqp",
                "forum": "KOUAayk5Kx",
                "replyto": "bc0VoIIdhW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6907/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6907/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Weakness1**: Thanks for your constructive comments. We provide with the convergence analysis of the update of the weights as follows:\n\n***Theorem* 1 :** Given a $l$-smooth and convex loss function $L(w)$, $w^*$ and $w_0$ are the optimal and initial weights of $L(w)$, respectively. If we let the learning rate $\\eta = 1/l$, then we have:\n\n$$\nL(w_t) - L(w^*) \\leq \\frac{2l}{t} \\left\\lVert w_0 - w^* \\right\\rVert_F^2,\n$$\nwhere $w_t$ is the weights of architecture after $t$-th training.\n\nAccording to *Theorem*  1, the OGL has a convergence rate of $O(1/t)$. The main proof of Theorem 1 is as follows:\n\n**Proof.** The update of the weights of architecture $w$ can be represented as follows:\n\n$$\nq(w_t) = \\arg\\min_{w \\in Q} (L(w_t) + \\langle L^{'}(w_t), w - w_t \\rangle + \\frac{\\beta}{2}\\left\\lVert w_t - w \\right\\rVert_F^2)\n$$\n$$\nw_{t+1} = w_t - \\eta\\beta(w_t - q(w_t)),\n$$\nwhere $\\eta$ and $\\beta$ are two hyperparameters.\n\nLet $Q$ is a closed convex set, $w^+ \\in Q$ and $\\beta \\geq l$. We denote $Q_w = q(w^+)$ and $g_Q = g_Q(w^+) = \\beta(w^+ - q(w^+))$, then we have:\n\n$$\n\\langle L^{'}(w^+) - g_Q, w - Q_w \\rangle = \\langle L^{'}(w^+), w - Q_w \\rangle - \\langle g_Q, w - Q_w \\rangle \\geq 0.\n$$\n\nAnd combined with the property of convex function, we have:\n\n$$\nL(w) \\geq L(Q_w) + \\frac{1}{2\\beta}\\left\\lVert g_Q \\right\\rVert_F^2 + \\langle g_Q, w - w^+ \\rangle.\n$$\n\nLet $\\beta = l$, $w = w^+ = w_t$, we have:\n\n$$\nL(w_t) \\geq L(w_{t+1}) + \\frac{1}{2l}\\left\\lVert g_Q(w_t) \\right\\rVert_F^2,\n$$\nwhere $g_Q(w_t) = \\beta(w_t - q(w_t))$.\n\nLet $\\beta = l$, $w = w^*$, $w^+ = w_t$, we have:\n\n$$\nL(w^*) \\geq L(w^*) + \\frac{1}{2l}\\left\\lVert g_Q(w_t) \\right\\rVert_F^2 - \\langle g_Q(w_t), w^* - w^t \\rangle.\n$$\n\nWe denote $r_t = \\left\\lVert w_t - w^* \\right\\rVert_F$ and $g_{Q,t} = g_Q(w_t)$, then if $\\eta \\leq \\frac{1}{\\beta}$ we have:\n\n$$\nr_{t+1}^2 \\leq r_t^2 + \\eta (\\eta - \\frac{1}{\\beta}) \\left\\lVert g_{Q,t} \\right\\rVert_F^2 \\leq r_t^2 \\leq \\ldots \\leq r_0^2.\n$$\n\nWe denote $\\Delta_t = L(w_t) - L(w^*)$, and $\\Delta_{t+1} \\leq \\Delta_t$, we have:\n\n$$\n\\frac{1}{\\Delta_{t+1}} \\geq \\frac{1}{\\Delta_{t}} + \\frac{1}{2l} \\frac{1}{r_0^2} \\frac{\\Delta_{t}}{\\Delta_{t+1}} \\geq \\frac{1}{\\Delta_t} + \\frac{1}{2l} \\frac{1}{r_0^2} \\geq \\ldots \\geq \\frac{1}{\\Delta_0} + \\frac{1}{2l} \\frac{1}{r_0^2}.\n$$\n\nThen we have:\n\n$$\n\\Delta_t = \\frac{2l (L(w_0) - L(w^*)) \\left\\lVert w_0 - w^* \\right\\rVert_F^2}{2l \\left\\lVert w_0 - w^* \\right\\rVert_F^2 + t (L(w_0) - L(w^*))}.\n$$\n\nBased on the property of $l$-smooth function, we have:\n\n$$\nL(w_0) \\leq L(w^*) + \\langle L^{'}(w^*), w_0 - w^* \\rangle + \\frac{l}{2} \\left\\lVert w_0 - w^* \\right\\rVert_F^2.\n$$\n\nBased on the inequation above, we have:\n\n$$\nL(w_t) - L(w^*) \\leq \\frac{2l \\left\\lVert w_0 - w^* \\right\\rVert_F^2}{2l \\frac{\\left\\lVert w_0 - w^* \\right\\rVert_F^2}{L(w_0) - L(w^*)} + t} \\leq \\frac{2l}{t} \\left\\lVert w_0 - w^* \\right\\rVert_F^2.\n$$\n\nThus the Theorem 1 is proven.\n \nThe details of the proof of Theorem 1 is provided in the section 1.2 of the Supplementary material.\n\n\n**Weakness2**: Sorry for the unclear statement and we detailed the update of the gradient space as follows:\n  1. Initialization of the gradient spaces: We firstly design a gradient space $G$ for each operation to save the gradient vectors of corresponding operation, then initialize each gradient space as the identity matrix $I$ (i.e.,  $G = I$);\n  2. Training of architectures: During the process of supernet training, an architecture is sampled in each supernet training epoch, and then the weights of the architecture is updated through OGL;\n  3. Calculation of the gradient vectors: We calculate the gradient of the weights for each operation in the supernet after the update of the architecture, and then obtain the gradient vectors of each operation (i.e., $g_1, g_2, ..., g_n$ where $n$ is the number of gradient vectors);\n  4. Concatenation of gradient vectors:  We concatenate the gradient vectors $g_1, g_2, ..., g_n$ to the gradient space $G$ (i.e., $G\\leftarrow [G, g_1, g_2, ..., g_n]$);\n  5. Dimension reduction of gradient space through PCA: We extract a set of base vectors of each gradient space by PCA (i.e., $G\\in \\mathbb{R}^{h \\times n} \\rightarrow G_{dim}\\in \\mathbb{R}^{h \\times d}$, where $h$ and $d$ are the dimension of the gradient space and the number of base vectors, respectively).\n\nAbout the hyperparameters. OGL is involved in a hyperparameter $d$ in $G_{dim}\\in \\mathbb{R}^{h \\times d}$, which is the column dimension of the gradient space, i.e., the number of the gradient vectors. However, the parameter $d$ is self-adjusted with the optimization process. Specifically, it is known that a space can be spanned by the maximally linear independent set, and PCA used in this study is to find the independent set of the gradient space. Once the independent set is found, $d$ here is determined since it is set to the number of vectors in the maximally linear independent set. Accordingly, we did not perform the parameter analysis of the proposed method."
                    },
                    "title": {
                        "value": "Rebuttal by Authors"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6907/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700475074777,
                "cdate": 1700475074777,
                "tmdate": 1700476262064,
                "mdate": 1700476262064,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ud3e6xRjXG",
                "forum": "KOUAayk5Kx",
                "replyto": "bc0VoIIdhW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6907/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6907/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Weakness3**: Yes. We totally agree with your opinion and thanks for your comments. Theoretically, our method is more competitive by using a larger search space than a smaller one. Specifically,  more candidate architectures will be sampled from a larger search space to train the supernet to get high performance, leading to more coupling candidate architectures. Therefore,  the multi-model forgetting issue is more likely to occur. Hence, our method is more competitive by alleviating the issue by using a larger search space than a smaller one. \n\nHere, we in this study did not use a large search space and aim to get a fair comparison with the up-to-date methods in handling the multi-model forgetting issue, where most of them use small search space. However, we will experimentally investigate the performance of our method by using a larger search space to evaluate the comprehensive performance of our method.\n\n**Weakness4**: Many thanks and the differences between OGL and baseline NAS are as follows:\n\nThe difference results from the multi-model forgetting issue in the baseline NAS. The key of the baseline NAS and other one-shot NAS methods is weight sharing, where the weights of all candidate architectures directly inherit from a supernet without training from scratch. However, it may introduce multi-model forgetting. During the training of a supernet, a number of architectures are sequentially sampled from the supernet and trained independently. Once the architectures have partially overlapped structures, the weights of these overlapped structures of the previously well-trained architecture will be overwritten by the weights of the newly sampled architectures. In this way, the performance of the previously well-trained architecture may be decreased. Therefore, we proposed a strategy called OGL to solve the multi-model forgetting exists in baseline NAS and other one-shot NAS methods.\n\nThe difference between OGL and baseline NAS is on the weight update. OGL firstly detects the structures of current architecture whether they are ever sampled or not. If not, then the weights of the architecture are updated with back-propagation algorithm (e.g., SGD) and a pre-constructed gradient space is updated by the gradient direction of the structure. If yes, then the weights of the overlapped structures of the current architecture are updated in the orthogonal direction to the gradient space of all previously trained architectures. Following the OGL guided training paradigm, the training of the current architecture will largely eliminate the influence to the performance of all previously trained architectures, which has been proved in Lemma 1. In other words, the multi-model forgetting issue is largely alleviated.\n\n**Weakness5**: Many thanks for your insightful comments, and we will explore the generality of our method on other architecture search domains. It will be great interesting to extend OGL to other architecture search if the previously-trained and newly-sampled architectures have overlapped structures during the training."
                    },
                    "title": {
                        "value": "continue to Rebuttal Part 2"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6907/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700475205026,
                "cdate": 1700475205026,
                "tmdate": 1700476237741,
                "mdate": 1700476237741,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "eZr73PU4Tx",
                "forum": "KOUAayk5Kx",
                "replyto": "bc0VoIIdhW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6907/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6907/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking forward to your feedback"
                    },
                    "comment": {
                        "value": "Dear Reviewer,\n\nWe would like to express our sincere gratitude for taking the time to review our paper and providing valuable feedback. We appreciate your expertise and insights, which are significant in enhancing the quality of our work.\n\nWe are eagerly awaiting your response to our rebuttal, as your comments and suggestions are crucial to the further development of our research. We understand that your time is precious, and we respect your commitment to this process.\n\nIn our rebuttal, we have thoroughly addressed the issues raised and provided comprehensive discussions and revisions. If there is any additional information you require or further clarification needed, please do not hesitate to reach out to us. We are more than willing to explain in any way possible.\n\nOnce again, we appreciate your dedication and effort in reviewing our paper. Your prompt attention to our rebuttal would be highly appreciated, as it greatly contributes to the timely progress of our research. \n\nThank you for your consideration, and we look forward to receiving your valuable feedback.\n\nSincerely,\n\nThe authors of Submission6907"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6907/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700703957275,
                "cdate": 1700703957275,
                "tmdate": 1700703957275,
                "mdate": 1700703957275,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]