[
    {
        "title": "Xformer: Hybrid X-Shaped Transformer for Image Denoising"
    },
    {
        "review": {
            "id": "Sr3XTfyuxg",
            "forum": "vXrIQLzIKY",
            "replyto": "vXrIQLzIKY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission937/Reviewer_Md3A"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission937/Reviewer_Md3A"
            ],
            "content": {
                "summary": {
                    "value": "In the proposed architecture, the authors design two branches to conduct these interaction modes. Both branches use an encoder-decoder setup to capture multi-scale features. An essential addition to this structure is the \"Bidirectional Connection Unit (BCU)\", which couples the learned representations from the two branches and facilitates better information fusion.\n\nThe combined designs enable the Xformer to effectively model global information in both spatial and channel dimensions. Through extensive experiments, the authors demonstrate that the Xformer achieves state-of-the-art performance on both synthetic and real-world image denoising tasks, all while maintaining comparable model complexity."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "The paper stands out in its innovative approach to image denoising by proposing a hybrid X-shaped Transformer. The clear presentation, combined with extensive experiments and state-of-the-art results, underscores its significance in the domain. The novel components, especially the BCU, and the creative combination of spatial and channel-wise blocks, emphasize its originality. The potential impact of this work on the broader image processing community is considerable."
                },
                "weaknesses": {
                    "value": "The hybrid nature of the model, with its dual branches and BCU, might be challenging for some readers to grasp fully. While the description seems structured, visual aids might be lacking.\n7.Questions\uff1aCould you provide more insights into the design rationale behind the Bidirectional Connection Unit (BCU)? How does it ensure efficient information fusion between the two branches?"
                },
                "questions": {
                    "value": "Please see the weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission937/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698589900835,
            "cdate": 1698589900835,
            "tmdate": 1699636020454,
            "mdate": 1699636020454,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "oGWgcINDTe",
                "forum": "vXrIQLzIKY",
                "replyto": "Sr3XTfyuxg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission937/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission937/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Md3A (denoted as R4)"
                    },
                    "comment": {
                        "value": "`Q1`: The hybrid nature of the model, with its dual branches and BCU, might be challenging for some readers to grasp fully. While the description seems structured, visual aids might be lacking.\n\n`A1`: Thanks for your suggestions. We have tried to introduce our work using simple and effective descriptions. But it might be challenging for some readers to understand. Therefore, we will consider adding visual aids in the supplementary material. At the same time, we will try to improve the writing of the main paper.\n\n`Q2`: Could you provide more insights into the design rationale behind the Bidirectional Connection Unit (BCU)? How does it ensure efficient information fusion between the two branches?\n\n`A2`: Thanks for your questions. We will give our replies as follows.\n\n**(1) The BCU is proposed for the information fusion between two types of feature extraction branches.** Theoretically, the gaps exist between the spatial-wise self-attention mechanism and channel-wise self-attention mechanism. Therefore we try to utilize the dual-branch network. In order to construct an effective hybrid network, simple concatenating operation is not able to effectively utilize information from different branches . Therefore, we consider proposing the Bidirectional Connection Unit (BCU) to couple the deep features from their respective feed-forward processes for feature complementarity.\n\n**(2) The BCU provides effective refinements for different types of features.** The purpose of BCU is enabling the dual branches to capture the effective feature information from the opposite branch. Therefore, the refinements of different types of features are very important. However, a simple and effective way is that using light-weight module to finish effective information processing. Since the convolution operation is commonly-used and able to meet our needs, we considering using it to finish the feature refinements. Since the BCU uses simple convolution operations, it can be thought as efficient.\n\n**(3) The experimental results validate the effectiveness of the BCU.** Under the premise that the proposal of BCU is reasonable, we conduct extensive experiments to validate its effectiveness. We provide fair ablation study results to demonstrate that the usage of BCU can bring better performance improvement. Besides, we analyze the effectiveness of  the single-direction connection unit. Lastly, we provide visual results to compare the output features of the models with BCU and without BCU. All the experimental results support that the BCU can provide efficient information fusion between the two branches."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission937/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700014785538,
                "cdate": 1700014785538,
                "tmdate": 1700014785538,
                "mdate": 1700014785538,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "4i7Ud1Jozd",
            "forum": "vXrIQLzIKY",
            "replyto": "vXrIQLzIKY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission937/Reviewer_wiTn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission937/Reviewer_wiTn"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors propose a hybrid X-shaped vision transformer for image denoising, named Xformer. Xformer has two branches with one containing the spatial-wise transformer blocks and the other containing the channel-wise transformer blocks. Between these two branches, there are the bidirectional connection units which couple the learned representations from these two branches. The experimental results show that the proposed method performs well on the synthetic image denoising dataset, but the method does not achieve the SOTA on the real-world image denoising dataset."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The X-shaped architecture is elegant and reasonable.\n2. The experimental results on the synthetic dataset are good.\n3. The overall paper writing is good."
                },
                "weaknesses": {
                    "value": "There are several places that are not intuitive or clear:\n1. The authors claim that \"we make the last encoder involving STBs of two branches share parameters for the purpose of computational efficiency.\" However, it is unclear how much the performance will be influenced by the parameter-sharing strategy. It is also not clear why it is critical to share parameters for this place in the network. Why not share parameters in other places?\n2. The authors claim that \"In short, the STB utilizes non-overlapping windows to generate shorter token sequences for the self-attention computation, which can enable the network to obtain fine-grained local patches interactions.\" Shorter token sequences? Compared to what? Why does the shorter token sequences can enable the network to obtain fine-grained local patches interactions?\n3. The authors claim that \"In order to introduce contextualized information into self-attention computation, we choose to use 3\u00d73 depth-wise convolution (Conv) following 1\u00d71 Conv to generate query (Q), key (K), and value (V).\" Why not directly use a vanilla 3\u00d73 convolution?\n4. The authors claim that \"Specifically, we use a 3\u00d73 depth-wise convolution layer to refine the deep features from the spatial-wise branch for the purpose of saving computational consumption.\" Why not also using the 3\u00d73 depth-wise convolution layer to refine the deep features from the channel-wise branch? Will it influence the performance compared to using the 3\u00d73 vanilla convolution?"
                },
                "questions": {
                    "value": "See the weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission937/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698811028391,
            "cdate": 1698811028391,
            "tmdate": 1699636020384,
            "mdate": 1699636020384,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "lB9Y6xYaQd",
                "forum": "vXrIQLzIKY",
                "replyto": "4i7Ud1Jozd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission937/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission937/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer wiTn (denoted as R3)"
                    },
                    "comment": {
                        "value": "`Q1`:  The experimental results show that the proposed method performs well on the synthetic image denoising dataset, but the method does not achieve the SOTA on the real-world image denoising dataset.\n\n`A1`: Thanks for your comments. It is worth determining that our Xformer performs well on the synthetic image denoising dataset. At the same time, our Xformer has much better performance on DND dataset and sightly limited performance on SIDD dataset while solving real-world image denoising. **We want to give some explanations here.**\n\n(1) The commonly-used testing datasets on real-world image denoising task include SIDD and DND. We should attach importance to both of them. In our experiments, **the performance of Xformer on DND is much better than Restormer.** Our Xformer obtains 0.16dB higher PSNR score over Restormer on DND. \n\n(2) Furthermore, **the performance of Xformer on SIDD is influenced by the characteristics of the data itself.** As we know, the test process on SIDD is based on 1280 image patches of size 256\u00d7256 pixels from 40 high-resolution images. Since we carefully investigate these 1280 image patches, we find that they have very smooth texture features and most of them are from the background with unity color. **The evaluation on these images has limited ability to mine the strengths of our model.** It is because that our Xformer shows the strengths by combine the local fine-grained features and global features across channels. Therefore, the testing results on SIDD of Xformer is slightly below the results of Restormer since Restormer does not consider the feature extraction on spatial dimension.\n\n`Q2`: The authors claim that \"we make the last encoder involving STBs of two branches share parameters for the purpose of computational efficiency.\" However, it is unclear how much the performance will be influenced by the parameter-sharing strategy. It is also not clear why it is critical to share parameters for this place in the network. Why not share parameters in other places?\n\n**(1) The performance is not influenced by the parameter-sharing strategy.** In our work, we use the parameter-sharing strategy in the last encoder of U-Net structure network to reduce the model complexity and improve the computing efficiency. During our study, we validate that this strategy has no negative influence on the model performance. We construct two types of neural networks. One is using the parameter-sharing strategy and the other is not. We conduct fair experimental comparison on the gaussian color image denoising task with noise level 15. We keep the same experimental settings. We show the experimental results here.\n| PSNR results (dB)| Params (M) |FLOPs (G)| CBSD68 | Kodak24 | McMaster| Urban100 |\n|-|-|-|-|-|-|-|\n|w/o params-sharing|37.18|40.07|32.42|35.38|35.65|35.20|\n|w/ params-sharing|25.07|39.11|34.42|35.37|35.65|35.22|\n\nAccording to the table results above, **we demonstrate that the parameter-sharing strategy has little influence on the model performance**.\n\n**(2) Explain the position of parameter-sharing strategy.** We choose to use the parameter-sharing strategy at the lowest layer of U-Net network. On the one hand, the module at the lowest layer processes the feature graph with the lowest resolution, so **the computational complexity is low when calculating self-attention interaction.** On the other hand, we use the STB to replace CTB by using parameter-sharing since the input feature of the lowest layer includes enough global information. Therefore, the STB can also capture the global feature by conducting self-attention interactions in the spatial dimension. **In short, the best position to using parameter-sharing strategy is just the last encoder of U-Net network.**"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission937/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700014582355,
                "cdate": 1700014582355,
                "tmdate": 1700014582355,
                "mdate": 1700014582355,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VA6EL8j6L2",
                "forum": "vXrIQLzIKY",
                "replyto": "4i7Ud1Jozd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission937/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission937/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer wiTn (denoted as R3)"
                    },
                    "comment": {
                        "value": "`Q3`: The authors claim that \"In short, the STB utilizes non-overlapping windows to generate shorter token sequences for the self-attention computation, which can enable the network to obtain fine-grained local patches interactions.\" Shorter token sequences? Compared to what? Why does the shorter token sequences can enable the network to obtain fine-grained local patches interactions?\n\n`A3`: Thanks for your questions. We give our explanations as follows.\n\n**(1) About the shorter token sequences.** We claim the shorter token sequence because **we compare it to the token sequence obtained by the common self-attention block of original vision Transformer**. Since we utilize non-overlapping windows to extract tokens, we obtain the token sequence within a local window, not the whole image area. Therefore the sequence is shorter.\n\n**(2) About fine-grained local patches interactions.** At first, it does not matter whether we utilize shorter token sequence. The network can obtain fine-grained local patches interactions because it extracts the tokens from the spatial dimension. Each token represents a local image patch. Therefore, the interactions among tokens can be thought as the interactions of local patches. **From this point of view, we think that the proposed STB can enable the network to obtain fine-grained local patches interactions.**\n\n`Q4`: The authors claim that \"In order to introduce contextualized information into self-attention computation, we choose to use 3\u00d73 depth-wise convolution (Conv) following 1\u00d71 Conv to generate query (Q), key (K), and value (V).\" Why not directly use a vanilla 3\u00d73 convolution?\n\n`A4`: Thanks for your comments. We choose to use 3\u00d73 depth-wise Conv and 1\u00d71 Conv because we want to reduce the computational cost and model parameters. In practice, it is called the depthwise separable convolution and has been widely used in recent works. Therefore, we do not use a vanilla 3\u00d73 convolution.\n\n`Q5`: The authors claim that \"Specifically, we use a 3\u00d73 depth-wise convolution layer to refine the deep features from the spatial-wise branch for the purpose of saving computational consumption.\" Why not also using the 3\u00d73 depth-wise convolution layer to refine the deep features from the channel-wise branch? Will it influence the performance compared to using the 3\u00d73 vanilla convolution?\n\n`A5`: Thanks for your questions. We give our replies as follows.\n\n(1) Firstly, since the spatial information of features passed to STB is important, we use vanilla convolutions to process the deep features from the channel-wise branch. **It is because the vanilla convolution has more learnable parameters and thus extract more spatial information.** Therefore, we choose to use the 3\u00d73 vanilla convolution.\n\n(2)Secondly, we are going to provide specific experimental results to demonstrate the necessity of using 3\u00d73 vanilla convolution. We train the new network named Xformer-change by replacing the 3\u00d73 vanilla convolution in BCU with 3\u00d73 depth-wise convolution on the gaussian color denoising task with noise level 15. We compare it to our current model Xformer. We keep the same experiment settings while training these two compared models. For ablation study, We train these models for 150k iterations with batch size 16. We report the compared results here.\n| PSNR results (dB)| Params (M) |FLOPs (G)| CBSD68 | Kodak24 | McMaster| Urban100 |\n|-|-|-|-|-|-|-|\n|Xformer-change|24.71|40.90|34.27|35.15|35.25|34.76|\n|Xformer|25.23|42.24|34.38|35.41|35.55|35.08|\n\nAs we can see, the model Xformer-change without using 3\u00d73 vanilla convolution has limited performance. **Therefore, we demonstrate that using 3\u00d73 vanilla convolution in BUC is necessary.**"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission937/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700014703679,
                "cdate": 1700014703679,
                "tmdate": 1700221699009,
                "mdate": 1700221699009,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ou9GWsi75i",
            "forum": "vXrIQLzIKY",
            "replyto": "vXrIQLzIKY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission937/Reviewer_GouV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission937/Reviewer_GouV"
            ],
            "content": {
                "summary": {
                    "value": "The authors proposed a hybrid X-shaped transformer for high-quality image denoising. The idea is good. Specifically, the technique consists of spatial-wise transformer blocks (STB) and channel-wise transformer blocks (CTB) to model global information. The authors provide extensive ablation studies to support the effectiveness of each proposed component, like STB, CTB, and BCU etc. The main comparisons with recent methods further show that the proposed method Xformer achieves better performance than others quantitatively and visually."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The idea is good and novel. The proposed Xformer exploits stronger global representation of tokens with a hybrid implementation of spatial-wise and channel-wise Transformer.\n\nThe bidirectional connection unit (BCU) is proposed to couple the learned representations from two branches of Xformer. It is simple but effective according to the ablation.\n\nThe authors provide extensive ablations to show the effects of some key components, like STB, CTB, BCU, and shift operation.\n\nThe main comparisons are also extensive. The authors provide both Gaussian and real image denoising results, where the proposed Xformer achieves better average quantitative results and also shows better visual results.\n\nThe writing is good and the work is well-prepared. The overall paper framework is well-organized.\n\nThe authors provide more results and analyses in supplementary file, where a sample code is also available. Such a code makes the reproducibility more faithful."
                },
                "weaknesses": {
                    "value": "Some details are not clear enough for better understanding. How did the authors determine the final model when training is finished? For example, did the authors choose the model based on the best validation performance or just use the model from the final iteration?\n\nIn the ablation study, Table 1 (b), it seems that w/o BCU and BCU-1 is comparable, BCU-2 and Complete BCU is comparable. Please give more analyses about their difference.\n\nIf the proposed method could be used for other image restoration tasks? If so, please give some comments and discussions. Or is it specifically designed for image denoising?\n\nThe Xformer shows very good performance. Are there any failure cases for image denoising? Namely, the proposed method can hardly recover good details either."
                },
                "questions": {
                    "value": "Please refer to the Weaknesses for details."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission937/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698851014925,
            "cdate": 1698851014925,
            "tmdate": 1699636020314,
            "mdate": 1699636020314,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GeyFprAV8k",
                "forum": "vXrIQLzIKY",
                "replyto": "ou9GWsi75i",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission937/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission937/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer GouV (denoted as R2)"
                    },
                    "comment": {
                        "value": "`Q1`: Some details are not clear enough for better understanding. How did the authors determine the final model when training is finished? For example, did the authors choose the model based on the best validation performance or just use the model from the final iteration?\n\n`A1`: Thanks for your questions. In our experiments, **we choose the model from the final training iteration**. We find that our model has stable performance after the given training steps by watching the loss curves and validation curves. Therefore, we guarantee the unity of training and choose to use the final training iteration as the last model.\n\n`Q2`: In the ablation study, Table 1 (b), it seems that w/o BCU and BCU-1 is comparable, BCU-2 and Complete BCU is comparable. Please give more analyses about their difference.\n\n`A2`: Thanks for your comments. We will give our replies as follows.\n\n**(1) About the differences.** The differences of them are given in the ablation study of the main paper. We will give more details here. **(a) The model without BCU, named w/o BCU, means that the BCU is not used in the constructed network.** Attention that the BCU has two independent fusion modules, which enable the bilateral interactions. **(b) BCU-1 and BCU-2 are thought as using the single-direction BCU.**  Note that using single-direction BCU means that we only use the depth-wise convolution (DWConv) or the vanilla convolution (Conv) to provide the information transmission from a single direction. Furthermore, BCU-1 denotes the model using DWConv to refine the deep features from the spatial-wise branch. BCU-2 denotes the model using Conv to refine the deep features from the channel-wise branch. **(c) Complete BCU means that we use the whole BCU in the network.**\n\n**(2) More analyses.** We analyze the results in the ablation study of the main paper. It is right that w/o BCU and BCU-1 is comparable, BCU-2 and Complete BCU is comparable. **The experimental results demonstrate that the information flow from the channel-wise branch has a bigger impact.** Since the spatial-wise branch deals with the local patches interactions, it requires guidance from globally distributed feature information cross channels.\n\n`Q3`: If the proposed method could be used for other image restoration tasks? If so, please give some comments and discussions. Or is it specifically designed for image denoising?\n\n`A3`: Thanks for your comments. Our Xformer is not specifically designed for image denoising.  **We have tried Xformer on the motion deblurring task.** We keep the training settings the same with Restormer. Besides, we do not change our Xformer model and use the same architecture. We provide the comparative results with Restormer here.  PSNR and SSIM are reported on four benchmarks. More detailed contents can be found in the supplementary materials.\n| Motion Deblurring| Metrics | Gopro | HIDE\t | RealBlur_J | RealBlur_R |\n|-|-|-|-|-|-|\n| Restormer | PSNR(dB)/SSIM | 32.92/0.961 |31.22/0.942|28.96/0.879|36.19/0.957|\n| Xformer (ours)| PSNR(dB)/SSIM |33.06/0.962 |31.19/0.942|29.02/0.883|36.19/0.957|\n\nAs we can see, our Xformer achieves the comparable performance with Restormer. **On Gopro, Xformer has PSNR gain of 0.14 dB over Restormer.** It indicates that our Xformer can also perform well on some other image restoration tasks.\n\n`Q4`: The Xformer shows very good performance. Are there any failure cases for image denoising? Namely, the proposed method can hardly recover good details either.\n\n`A4`: Thanks for your comments. As we provide extensive visual examples in the supplementary material, we validate that our Xformer can solve many challenging denoising cases. Of course, Xformer also has some failure cases for image denoising. We are willing to discuss and share some failure cases of Xformer. Because of the limited space, **we will add these contents in the revised supplementary material**."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission937/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700014500108,
                "cdate": 1700014500108,
                "tmdate": 1700014500108,
                "mdate": 1700014500108,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "JnabSIzSCZ",
            "forum": "vXrIQLzIKY",
            "replyto": "vXrIQLzIKY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission937/Reviewer_V2SV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission937/Reviewer_V2SV"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes XFormer for image denoising, which aims to combine SwinIR which utilizes spatial-wise self attention and Restormer designing channel-wise self attention for image denoise, thereby leveraging the advantages from both methods. The designs, including dual-branch architecture and the bidirectional connection unit for bilateral interactions between two branches, are straightforward.m The paper is motivated well, whilst the technical novelty is incremental, especially compared to SwinIR and Restormer."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper is motivated well. It is reasonable to combine the advantages of both spatial-wise self attention and channel-wise self attention to capture both the local fine-grained features and global features across channels.\n\n2. The paper is organized well and easy to follow despite some typos."
                },
                "weaknesses": {
                    "value": "1. The technical novelty is incremental. There are two core designs: the dual-branch architecture and the bilateral interactions between two branches, which are both typical designs and have been extensively explored in other work. Thus, the technical novelty is limited, especially compared to SwinIR and Restormer.\n\n2. Compared to Restormer, Xformer has limited performance improvement, especially on real image denoising scenarios which is more important for evaluation."
                },
                "questions": {
                    "value": "It is suggested to investigate both theoretically and experimentally what kind of denoising scenarios (noises) are STB and CTB suitable for, respectively."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission937/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission937/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission937/Reviewer_V2SV"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission937/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699108588462,
            "cdate": 1699108588462,
            "tmdate": 1700742032136,
            "mdate": 1700742032136,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wDUhm8i5vj",
                "forum": "vXrIQLzIKY",
                "replyto": "JnabSIzSCZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission937/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission937/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer V2SV (denoted as R1)"
                    },
                    "comment": {
                        "value": "`Q1`: The technical novelty is incremental. There are two core designs: the dual-branch architecture and the bilateral interactions between two branches, which are both typical designs and have been extensively explored in other work. Thus, the technical novelty is limited, especially compared to SwinIR and Restormer.\n\n`A1`: Thanks for your comments. We consider your concern carefully and give our replies as follows.\n\n**(1) The technical novelty is well.**  (a) At first, we should know that **the technical novelty should be consistent with the research motivation and background**. According to our research, creating a dual-branch network using different Transformer blocks is not common in the low-level vision field. (b) Besides, our motivation is not to validate the effectiveness of the dual-branch network. We want to explore stronger learning representation ability by utilizing different types of self-attention mechanisms in a unity neural network. **Although the dual-branch architecture and the bilateral interactions are not the first try, they have not been investigated to the exploit the advantages of both spatial-wise self attention and channel-wise self attention.** Therefore, our core designs are novel.\n\n**(2) Compared to SwinIR and Restormer.** (a) We are willing to discuss the differences between our proposed method and these two works while considering the research motivation. In fact, it is important to further understand our motivation. In our work, the research subjects are the core self-attention mechanisms, but not the specific modules which have been proposed in SwinIR and Restormer. (b) **Our starting point is not to improve existing Transformer blocks for better performance.** In other words, we do not focus on using which kind of self-attention module proposed by previous works. **We consider more improvements from the level of feature capture and interaction.** Since existing Transformer-based denoising methods consider feature extraction from a single dimension, spatial or channel-wise, our method creatively uses a concurrent network to model global information from two dimensions. Besides, our proposed BCU is proved to effectively fuse two types of features. Extensive experimental results validate that our method has promising denoising performance. Here, we provide the compared results with SwinIR and Restormer on gaussian color image denoising.\n| PSNR results (dB)| CBSD68 (15/25/50) | Kodak24 (15/25/50) | McMaster (15/25/50) | Urban100 (15/25/50) |\n|-|-|-|-|-|\n| SwinIR | 34.42/31.78/28.56| 35.34/32.89/29.79| 35.61/33.20/30.22| 35.13/32.90/29.82|\n| Restormer| 34.40/31.79/28.60| 35.35/32.93/29.87| 35.61/33.34/30.30| 35.13/32.96/30.02|\n| Xformer (ours)| **34.43/31.82/28.63** | **35.39/32.99/29.94** | **35.68/33.44/30.38** | **35.29/33.21/30.36** |\n\nAs we can see, our Xformer achieves the best performance on all benchmark datasets across three noise levels. It validates that the technical novelty brings Xformer the obvious performance improvement.\n\n**(3)Conclusion.** In our work, **the core issues we want to solve are about the extraction and fusion of different types of features from spatial or channel dimensions.** Therefore, we design Xformer to improve the global information modeling from two dimensions. We conduct extensive experiments to demonstrate its promising denoising performance. **From this perspective, our work does not lack novelty.**"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission937/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700014225777,
                "cdate": 1700014225777,
                "tmdate": 1700014225777,
                "mdate": 1700014225777,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Lu95E2MRkJ",
                "forum": "vXrIQLzIKY",
                "replyto": "JnabSIzSCZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission937/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission937/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer V2SV (denoted as R1)"
                    },
                    "comment": {
                        "value": "`Q2`: Compared to Restormer, Xformer has limited performance improvement, especially on real image denoising scenarios which is more important for evaluation.\n\n`A2`: Thanks for your comments. We propose Xformer and evaluate its performance on the synthetic and real-world image denoising tasks. Our replies are as follows.\n\n**(1) About the performance on the synthetic datasets.** (a) Compared to Restormer, Xformer achieves obviously better performance on the synthetic image denoising tasks. For example, our Xformer obtains 0.34 dB higher PSNR score over Restormer on Urban100 for gaussian color image denoising with noise level 50. (b) Besides, we provide extensive visual results in the main paper and supplementary material to validate the superior of Xformer. **Therefore, Xformer has obvious performance improvement on the synthetic image denoising tasks.** \n\n**(2) About the performance on real image denoising.** (a) The commonly-used testing datasets on real image denoising task include SIDD and DND. We should attach importance to both of them. In fact, **the performance of Xformer on DND is much better than Restormer.** Our Xformer obtains 0.16dB higher PSNR score over Restormer on DND. (b) Furthermore, **the performance of Xformer on SIDD is influenced by the characteristics of the data itself.** As we know, the test process on SIDD is based on 1280 image patches of size 256\u00d7256 pixels from 40 high-resolution images. Since we carefully investigate these 1280 image patches, we find that they have very smooth texture features and most of them are from the background with unity color. **The evaluation on these images has limited ability to mine the strengths of our model.** It is because that our Xformer shows the strengths by combine the local fine-grained features and global features across channels. Therefore, the testing results on SIDD of Xformer is slightly below the results of Restormer since Restormer does not consider the feature extraction on spatial dimension.\n\n**(3) Conclusion.** We explain the reasons of the limited performance of Xformer on SIDD dataset. According to the analyses above, **we confirm that our Xformer has obvious performance improvement over Restormer.**\n\n`Q3`: It is suggested to investigate both theoretically and experimentally what kind of denoising scenarios (noises) are STB and CTB suitable for, respectively.\n\n`A3`: Thanks for your suggestions. In our paper, we discuss the performance of different network structures while using STB or CTB. The corresponding results can be found in the ablation study. (a) **Theoretically, STB is good at solving the denoising scenario which involves dealing with some spatially uniform distributed noise, such as the synthetic gaussian noises.** CTB is good at solving the denoising scenario which involves dealing with some global consistent distributed noise. (b) Experimentally, we find that the network using STB has better performance than that using CTB while solving gaussian color image denoising task. In the future, we will conduct more experiments to investigate the suitiable denosing scenarios of different Transformer blocks."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission937/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700014291267,
                "cdate": 1700014291267,
                "tmdate": 1700014291267,
                "mdate": 1700014291267,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OIoTt9sIxe",
                "forum": "vXrIQLzIKY",
                "replyto": "Lu95E2MRkJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission937/Reviewer_V2SV"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission937/Reviewer_V2SV"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the response, which addresses most of my concerns."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission937/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700742008106,
                "cdate": 1700742008106,
                "tmdate": 1700742008106,
                "mdate": 1700742008106,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]