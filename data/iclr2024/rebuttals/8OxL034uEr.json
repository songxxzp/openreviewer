[
    {
        "title": "MgNO: Efficient Parameterization of Linear Operators via Multigrid"
    },
    {
        "review": {
            "id": "uHBVFh2Vor",
            "forum": "8OxL034uEr",
            "replyto": "8OxL034uEr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3863/Reviewer_4o6c"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3863/Reviewer_4o6c"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors introduce a novel formulation for neural operators and establish (in Theorem 3.1) the corresponding universal approximation property.\n\nThe authors then propose a multi-grid-based parameterization of linear operators, called MgNO, that can be efficiently parameterized and naturally accommodates various boundary conditions.\n\nNumerical results are provided on several popular PDEs, including Darcy, Helmholtz, and Navier-Stokes equations, with different boundary conditions, showing the superiority in prediction accuracy and efficiency of the proposed approach."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper provides novel insights and novel methodology of neural operators.\nThe numerical results on different popular PDEs with various boundary conditions look compelling."
                },
                "weaknesses": {
                    "value": "I do not see particular weakness for this paper. See below for some comments."
                },
                "questions": {
                    "value": "I do not have specific questions but the following general comments for the authors:\n\n1. just being curious, can something similar to Theorem 3.1 be said about deep networks as defined in Section 3.2? How the network depth may play a role here?\n2. it would be helpful to discuss also the limitation of the proposed approach."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3863/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698753522899,
            "cdate": 1698753522899,
            "tmdate": 1699636344712,
            "mdate": 1699636344712,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "TcvBXobpLy",
                "forum": "8OxL034uEr",
                "replyto": "uHBVFh2Vor",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3863/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3863/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "## Response to Questions:\n\n**Question 1**:\nThank you for raising this interesting question. Indeed, Theorem 3.1 pertains solely to shallow neural operators. The theoretical impact of deep neural operator architecture on performance, as well as the role of depth in neural operators, remain open questions.\n\nIn fact, the role of depth in traditional deep Neural Networks is not yet fully understood. A vast amount of literature has explored this topic, primarily focusing on the following aspects:\n1. Expressivity: 2. Approximation power; 3. Implicit bias and regularization effects; 4. The landscape of loss and its impact on training.\nI conjecture that depth in neural operators might play similar roles as in traditional neural networks. However, no rigorous theoretical framework has yet been established to confirm this.\n\nIn our experiments, we observed a rapid increase in performance when the depth of MgNO was increased from one hidden layer to 4, 5, or 6 layers. Beyond this point, however, the benefits of increased depth diminish, weighed against the costs of computational complexity and training challenges.\n\nFrom both theoretical and practical standpoints, whether a significantly deeper neural operator architecture, such as one with 100 layers, can substantially enhance neural operator performance remains an open question.\n\n\n**Question 2**:\nThank you for drawing attention to this aspect. We acknowledge the need for a discussion on limitations or drawbacks in our concluding remarks section. Accordingly, we incorporate a discussion on the limitations or drawbacks there. One limitation of the current implementation of MgNO is its restriction to inputs and outputs on uniform meshes; otherwise, mesh data has to be integrated into the input such that the input is akin to pixel grids in image data. To address this, we plan to integrate ideas from algebraic multigrid methods to\nenhance our current MgNO framework. This enhancement will enable it to handle inputs and outputs on more\ngeneral meshes, and even extend to arbitrary sampling data points or point clouds."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3863/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700069492532,
                "cdate": 1700069492532,
                "tmdate": 1700069492532,
                "mdate": 1700069492532,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "dZsnNG6n5G",
            "forum": "8OxL034uEr",
            "replyto": "8OxL034uEr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3863/Reviewer_RPM7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3863/Reviewer_RPM7"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new neural operator architecture that does not require lifting and projecting operators. It uses multigrid structures (V-cycle multigrid) to parameterize the linear operators between neurons. Then this paper proves the universal approximation of their proposed parameterization and uses experiments to show the efficiency and accuracy of the method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The neural operator proposed in this paper uses the multi-scale method to get a better parameterization of the weights. It is applied to the spatial domain hence it seems like it won't significantly increase the complexity. From the experimental results, MGNO works quite well compared to other models, which provides convincing evidence that MGNO can be useful."
                },
                "weaknesses": {
                    "value": "1. I am confused about the discretization of the input. It seems like the input still needs to be discretized (Eq.(6)) and the multigrid is applied to the spatial domain. Then I suppose the performance and the efficiency of MGNO are affected by the mesh size $h=1/d$. The performance of MGNO with respect to $d$ is missing in the paper.\n\n2. What about the parameterization with multi-channel inputs? Can this V-cycle be applied to multi-channel inputs? If there is no easy extension, I believe the significance of this work is compromised."
                },
                "questions": {
                    "value": "1. What is the dependence of $n$ with $\\epsilon$ in Theorem 3.1?\n2. Why are the approximation capabilities so important? It might be a necessary condition for neural operators to learn but is definitely not sufficient. In my understanding, FNO is good at working with different resolutions which probably is not caused by its approximation capability."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3863/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3863/Reviewer_RPM7",
                        "ICLR.cc/2024/Conference/Submission3863/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3863/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698788502790,
            "cdate": 1698788502790,
            "tmdate": 1700517815729,
            "mdate": 1700517815729,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kw5nl3i0DY",
                "forum": "8OxL034uEr",
                "replyto": "dZsnNG6n5G",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3863/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3863/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your feedback on our paper. We appreciate your recognition of the strengths of our proposed Multigrid Neural Operator (MgNO) architecture and your comments on areas needing clarification. Below, we address each of your concerns and questions.\n\n### Addressing Weaknesses:\n\n1. **Discretization of Input**: \n\n - **Clarification**: \n\nYou are correct that the input requires discretization, as shown in Eq.(6). MgNO applies multigrid structures to the spatial domain, which inherently involves a discretization process. For irregular domains, the mesh structure data has to be further integrated into the input data.\n   - **Impact of Mesh Size**: \n\nThe performance and efficiency of MgNO are influenced by the mesh size. While discretization invariance, as observed in FNO, suggests that a model's performance is not sensitive to input resolutions for certain tasks, it's important to recognize the limitations of this concept. Discretization invariance can result in uniformly good or uniformly bad performance across different input resolutions. The uniformly good scenario implies that the underlying operator is low-dimensional, where coarse information suffices to capture its behavior. However, this is not typically the case in complex problems, such as fluid flow at high Reynolds numbers or wave propagation at high wave numbers. For instance, in turbulent flows, fine resolution is crucial to accurately represent small-scale phenomena, and failure to capture these details can significantly alter long-range predictions. Thus, expecting uniform performance across various resolutions is unrealistic, particularly for complex real-world problems.\n   \nTo illustrate this, experiments in Table 6 of Appendix D, compare the performance of MgNO and FNO across different mesh sizes. These experiments reveal that for tasks where fine-scale information is crucial, both models exhibit deteriorated performance compared to the results shown in Table 1. This demonstrates the importance of resolution in such complex problems. \n \nFurthermore, in the study \"Representation Equivalent Neural Operators: a Framework for Alias-free Operator Learning,\" it is proven that discretization invariance is achievable only within a band-limited function space (low dimensional space), which is a rather restrictive condition. Our experiments, therefore, provide valuable insights into the practical implications of mesh size on model performance.\n\n2. **Parameterization with Multi-Channel Inputs**:\n\nWe appreciate your query regarding the handling of multi-channel inputs in our model. This aspect, focusing on both expressivity and complexity, is thoroughly discussed in Section 4.1 of our manuscript.\n\nIn our experiments, the model indeed utilizes a multi-channel parametrization, and notably, the input for the Navier-Stokes task is multi-channel, demonstrating the model's capability to handle such data. This approach aligns with classical multigrid methods applied to solve elliptic partial differential equations (PDEs), including linear elasticity problems in 2D or 3D scenarios. In such cases, the V-cycle multigrid methods can be naturally represented as linear MgNO with multi-channel inputs."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3863/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699989526478,
                "cdate": 1699989526478,
                "tmdate": 1699989526478,
                "mdate": 1699989526478,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "AB1Cr2qjJL",
                "forum": "8OxL034uEr",
                "replyto": "RJlHmYQwyZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3863/Reviewer_RPM7"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3863/Reviewer_RPM7"
                ],
                "content": {
                    "comment": {
                        "value": "Thank the authors for addressing my concerns. I have increased the score."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3863/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700517802781,
                "cdate": 1700517802781,
                "tmdate": 1700517802781,
                "mdate": 1700517802781,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "JK2mVhiFel",
            "forum": "8OxL034uEr",
            "replyto": "8OxL034uEr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3863/Reviewer_H6UK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3863/Reviewer_H6UK"
            ],
            "content": {
                "summary": {
                    "value": "The authors present a concise neural operator architecture designed for operator learning. This architecture draws inspiration from a conventional fully connected neural network, with each neuron in a nonlinear operator layer being determined by a bounded linear operator connecting input and output neurons, in addition to a function-based bias term. To address the efficient parameterization of these bounded linear operators within the architecture, the authors introduce MgNO (Multigrid Neural Operator). MgNO utilizes multigrid structures to effectively model and approximate these linear operators, eliminating the need for conventional lifting and projecting operators while accommodating diverse boundary conditions. Numerical experiments highlight the advantages of MgNO over CNN-based models and spectral-type neural operators, demonstrating improved ease of training and reduced susceptibility to overfitting."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The authors introduced a novel formulation for neural operators that characterizes neuron connections as bounded linear operators within function spaces. This eliminates the need for traditional lifting and projecting operators, simplifying the architecture.\n\n2. The MgNO architecture, which leverages multigrid structure and multi-channel convolutional form, efficiently parameterizes linear operators while accommodating various boundary conditions. This approach enhances both accuracy and efficiency.\n\n3. Empirical evaluations demonstrate superior performance of MgNO across multiple partial differential equations (PDEs), including Darcy, Helmholtz, and Navier-Stokes equations. This indicates the effectiveness and versatility of the proposed approach."
                },
                "weaknesses": {
                    "value": "The limitations and drawbacks of the proposed methods are not explicitly mentioned"
                },
                "questions": {
                    "value": "1. The proposed method may not scale well to larger datasets or more complex problems. Any remark for this?\n\n2. Is it possible to extend the universal approximation theorem to encompass general Banach spaces for X and Y?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3863/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698824063277,
            "cdate": 1698824063277,
            "tmdate": 1699636344531,
            "mdate": 1699636344531,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "DhkU7Hbxfq",
                "forum": "8OxL034uEr",
                "replyto": "JK2mVhiFel",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3863/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3863/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "## Addressing Weakness:\n\nThank you for highlighting this aspect. We acknowledge the need for a discussion on limitations or drawbacks in our concluding remarks section. Accordingly, \nwe incorporate a discussion on the limitations or drawbacks there. One limitation of the current implementation of MgNO is its restriction to inputs\nand outputs on uniform meshes; otherwise, mesh data has to be integrated into the input such that the input is akin to pixel grids in image data. To address this, we plan to integrate ideas from algebraic multigrid methods to\nenhance our current MgNO framework. This enhancement will enable it to handle inputs and outputs on more\ngeneral meshes, and even extend to arbitrary sampling data points or point clouds.\n\n\n## Response to questions\n**Question 1**:\nWe thank the reviewer for raising concerns regarding the scalability of our proposed method. The MgNO architecture, central to our approach, is specifically tailored for efficient parameterization of linear operators within function spaces. This is achieved through a multigrid structure and multi-channel convolutional form, enabling scalability to high-resolution inputs typically encountered in complex problems.\n\nImportantly, the multigrid structure of MgNO is recognized for its effectiveness in handling large-scale problems, particularly in numerical solutions to PDEs. This aspect of our architecture underpins our confidence in its scalability.\n\nEmpirically, MgNO has demonstrated superior performance in both accuracy and efficiency, as evidenced in our results. Notably, it features the smallest number of parameters as shown in Table 1 of our paper. To directly address the scalability concern, we conducted additional experiments to assess MgNO's performance at various input resolutions -- $64 \\times 64$, $128 \\times 128$, $256 \\times 256$, and $512 \\times 512$ -- while monitoring memory usage during both forward and backward passes. For MgNO, the recorded memory usages were 145.04 MB, 574.16 MB, 2290.66 MB, and 9156.64 MB, respectively. In comparison, for the Fourier Neural Operator (FNO), the memory usage were 184.25 MB, 630.16 MB, 2342.80 MB, and 9051.43 MB at the same resolutions. These results indicate a near-linear relationship between memory usage and the square of the resolution $(n^2)$, where $ n$ represents the resolution. This linear trend in resource usage, coupled with the comparative efficiency over FNO, further substantiates our method's scalability.\n\n\nIn conclusion, both the architectural design of MgNO and the empirical evidence from our experiments reinforce our confidence in its scalability to more demanding scenarios, thereby addressing the concerns raised.\n\n\n\n**Question 2**:\nThank you for the insightful point. Indeed, we can extend the universal approximation theorem to encompass more general Banach or Hilbert spaces, which may not necessarily be spaces of functions. As evident from the neural operator architecture in Equation (1), the definitions of $\\mathcal W_i$, $\\mathcal B_i$, and $\\mathcal A_i$ are independent of the structures of $\\mathcal{X}$ and $\\mathcal{Y}$. This allows for a direct extension to abstract Banach spaces.\n\nHowever, there are two critical aspects to consider. Firstly, we must assume the existence of at least one Schauder basis, enabling the uniform approximation of $\\mathcal{X}$ by finite-dimensional spaces. This assumption is crucial for achieving the approximation rate as outlined in the second step of the original proof in Appendix A.\n\nSecondly, the definition of the activation function needs attention. In the initial version of our manuscript, we proposed a property for the nonlinear activation operator ($\\widetilde{\\sigma}: \\mathcal{Y} \\rightarrow \\mathcal{Y}$): there exist two elements $\\xi, \\zeta \\in \\mathcal{Y}$ such that $\\widetilde{\\sigma}(a \\xi) = \\sigma(a) \\zeta$, where $\\sigma: \\mathbb{R} \\rightarrow \\mathbb{R}$ is a real, non-polynomial function. This approach facilitates a proof analogous to the third step in Appendix A. \n\nWith these assumptions for $\\mathcal{X}$, $\\mathcal{Y}$, and the activation operator $\\widetilde{\\sigma}$, we can replicate the proof in Appendix A to achieve similar universal approximation results for highly general or abstract Banach spaces $\\mathcal{X}$ and $\\mathcal{Y}$. In the initial version of our manuscript, we included this result. However, we later decided to omit it, concerned that its generality might lead to misunderstandings among readers."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3863/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700061003110,
                "cdate": 1700061003110,
                "tmdate": 1700061003110,
                "mdate": 1700061003110,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "qWMNqrQqS0",
            "forum": "8OxL034uEr",
            "replyto": "8OxL034uEr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3863/Reviewer_5jaY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3863/Reviewer_5jaY"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new neural network architecture based on analogies between convnets and the multigrid method (these analogies have been previously identified with an architecture called MgNet). The authors prove a universality theorem for a network whose neurons are operators and use this theorem to motivate a neural operator which is a deep network with neurons being (elliptic) operators implemented via multigrid. Numerical experiments show competitive performance against existing neural operator architectures."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper is very clearly written, the ideas are solid and the numerics are strong. I like the spirit of an architecture where neurons become operators: this appears very natural and the right thing to do.\n\n- Using a multigrid (or in general multiscale) representation of operators is also the right thing to do. Similar ideas go back to efficient wavelet approximations of operators of Beylkin, Coifman, and Rokhlin. It is nice to see this done explicitly and clearly it yields strong performance.\n\n- The experiments are very well executed."
                },
                "weaknesses": {
                    "value": "- Many of the ideas might have already been present in MgNet; it would be nice to see a comment.\n\n- It is not clear to me what (9) is stating since you don't state the typical values of c. How should we use / interpret (9)?\n\n- The motivation via exact encoding of boundary conditions is sound but I would also say quite easy to implement with other neural operators. For example the FNO by default works on the torus but via zero padding could be easily adapted to Neumann or Dirichlet boundaries (though not as easily as MgNO)."
                },
                "questions": {
                    "value": "- Why is there no comparison with FNO in Figure 7 (Helmholtz)? It might be nice to compare / combine with https://arxiv.org/pdf/2301.11509\n- The sentence \"Regarding our training setup, we adopted a meticulous approach.\" is strange."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3863/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699249406813,
            "cdate": 1699249406813,
            "tmdate": 1699636344442,
            "mdate": 1699636344442,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "lcxBpdOxLH",
                "forum": "8OxL034uEr",
                "replyto": "qWMNqrQqS0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3863/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3863/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "## Addressing Weakness\n**Weakness 1**:\nWe appreciate your valuable feedback. For initial remarks on MgNet, please refer to the 'Background and Related Work' section, particularly the subsection titled 'Multigrid'. We include a more comprehensive discussion here and also in the revised manuscript.  MgNet is the pioneering work to introduce multigrid to neural network architecture. The authors rigorously demonstrated that the linear V-cycle multigrid structure for the Poisson equation,  can be represented as a convolutional neural network, despite their experimental focus on nonlinear multigrid structures for vision-related tasks. Our main idea is that the key of operator learning is how to efficiently parametrize a rich family of linear operators integrated with specific boundary conditions. The impressive performance outcomes we've recorded offer robust evidence in favor of this approach.  The insight of MgNet forms a cornerstone in the implementation level of our approach in utilizing multigrid to parameterize such operators.  However, as we have discussed in the \"Background and Related Work\" section under \"Multigrid,\" the original MgNet and its subsequent adaptations in the context of numerical PDEs and forecasting scenarios, have not provided a simple but effective solution in the realm of operator learning.\n\n\n**Weakness 2**:\nThank you for highlighting this aspect. We should have added a few more words for this in our paper. The constant $c$ in Equation (9) is determined by the dimensions and properties of $a(x)$ but remains independent of the mesh size $h$. Typically, a classical iterative method (denoted as $ \\mathcal{W} $) for solving discrete elliptic PDEs (such as $ A_h u_h = f_h $) will exhibit a convergence rate of $ 1-\\frac{1}{c_h} $, where $ c_h = \\mathcal{O}(h^{-2}) $ with mesh size (resolution) $ h $. This indicates that the approximation rate of $ \\mathcal{W} $ for $ A_h^{-1} $ (i.e., the inverse of discrete elliptic operators, which is the discrete Green's function) is not uniform and deteriorates when $ h $ is small. However, Equation (9) reveals that if we employ the operator implemented by a multigrid iterative method (denoted as $ W_{Mg} $), the convergence rate becomes $ 1-\\frac{1}{c} $. This implies that the approximation rate of $ W_{Mg} $ to $ A_h^{-1} $ (Green's function) is uniformly bounded by $ 1-\\frac{1}{c} $, independent of the mesh size $ h $. Therefore, Equation (9) demonstrates that $ W_{Mg} $ has significantly enhanced expressivity for general linear operators between two finite element spaces. Specifically, it achieves a uniform approximation rate for a special type of kernel functions, namely, Green's functions, which are the inverses of elliptic operators. This insight is crucial in motivating the use of multigrid methods to parameterize the linear operator $ W_i $. \n\nIn Section C of the Appendix, we provide a detailed numerical example to quantify the constant $c$ and approximation rate.\n\n**Weakness 3**:\nWe agree that other neural operators can always zeros pad the input to a larger enough (for example padding size 7 for the baseline FNO) domain such that the output functions space is redefined on larger domain. Without this strategy, FNO would be constrained to learning mappings to periodic function space on the target domain. However, it's important to note that padding (always zero-padding for any boundary conditions), is not reflective of an operator's intrinsic ability to learn specific boundary conditions and should be considered more as a workaround than a solution. \n\nIn contrast, our distinct parametrization method enables the explicit expression of the multigrid solver.  Consequently, the inverse of  elliptic operators $\\mathcal L$ under a linear FEM framework can be approximated by  MgNO operators rigorously. This includes, but is not limited to, the inverse Laplacian $ \\Delta^{-1}$ with any boundary conditions.  Furthermore, the constructive approximation of the inverse Laplacian $ \\Delta^{-1}$ with any boundary conditions using MgNO explicitly requires the padding mode to be zero, reflect, or periodic (with a padding size of 1), corresponding to Dirichlet, Neumann, and periodic boundary conditions, respectively. This is achieved without the requirement to expand the domain via padding.  In Section C of the Appendix, we have further provided a detailed numerical example illustrating the construction of a linear MgNO operator $\\mathcal{W}_{Mg}$, showcasing the integration of boundary conditions into MgNO and its approximation capabilities.  In summary, we assert our approach with MgNO inherently integrates boundary conditions into its structure without resorting to padding to larger domain, offering a more natural and mathematically sound method for handling various boundary conditions."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3863/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700130280670,
                "cdate": 1700130280670,
                "tmdate": 1700492710466,
                "mdate": 1700492710466,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Xx0ECYd3ST",
                "forum": "8OxL034uEr",
                "replyto": "qWMNqrQqS0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3863/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3863/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "## Response to Questions\n**Question 1**:\nThank you for your valuable suggestion. In the revised version of our manuscript, we have now included a comparison with FNO in Figure 7 (Helmholtz). Additionally, we have incorporated a comparison with sFNO-v2 as referenced in the paper you mentioned [https://arxiv.org/pdf/2301.11509]. For this comparison, we employed the default hyperparameter settings as provided in the paper, using their official implementation.\n\nHowever, it's important to note that the results we obtained did not precisely mirror the performance reported in the paper. This discrepancy could potentially be attributed to the hyperparameters not being fine-tuned for the specific task in our study. \n\n| **Model**       | Time | Params(m) | $L^2 (\\times 10^{-2})$ | $H^1 (\\times 10^{-2})$ |\n|-----------------|------|-----------|------------------------|------------------------|\n| **FNO2D**       | 5.1  | 1.33      | 1.69                   | 9.92                   |\n| **UNet**        | 7.1  | 17.26     | 3.81                   | 23.31                  |\n| **DilResNet**   | 10.8 | 1.03      | 4.34                   | 34.21                  |\n| **U-NO**        | 21.5 | 16.39     | 1.26                   | 8.03                   |\n| **sFNO-v2**     | 30.1 | 12.0      | 1.72                   | 10.40                  |\n| **LSM**         | 28.2 | 4.81      | 2.55                   | 10.61                  |\n| **MgNO**        | 15.1 | 7.58      | **0.71**               | **4.02**               |\n\n_Performance on Helmholtz._\n\n**Question 2**:\nThank you for your careful review. We have revised the sentence accordingly to \"In our training setup, we implemented a comprehensive approach.\""
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3863/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700131103243,
                "cdate": 1700131103243,
                "tmdate": 1700493108799,
                "mdate": 1700493108799,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]