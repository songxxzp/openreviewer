[
    {
        "title": "$\\pi$2vec: Policy Representation with Successor Features"
    },
    {
        "review": {
            "id": "F0YBppZ3ly",
            "forum": "o5Bqa4o5Mi",
            "replyto": "o5Bqa4o5Mi",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1825/Reviewer_Kqe9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1825/Reviewer_Kqe9"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents an approach to build a compact, vectorial representation of policies, that can be used for off-policy evaluation. The idea is to leverage pre-trained image-based models to generate state-based features to be used for training successor features for policy evaluation. The policy features ($\\pi 2vec$) is obtained by averaging the successor features over a set of states sampled from the offline dataset.\n\nThe authors showed that these features are effective in the task of off-policy evaluation."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is clear and easy to follow. The approach is simple and intuitive, and, as far as I know, it is novel."
                },
                "weaknesses": {
                    "value": "- I would appreciate more details about the relevance of this setting. What are the use cases you have in mind?\n- Could you clarify the metrics used to evaluate the approach? You should report (at least in appendix) the equations.\n- Why don't you use ranking metrics (Mean Average Precision, DCG, etc)? These seems quite relevant for evaluating methods for offline policy selection. As you mentioned, absolute error in terms of value prediction may not be always relevant.\n- You decided to focus on visual representation without clearly explaining why. I think it is important to evaluate the approach also on state-based observation. How would you select base state features in this setting?\n- Similarly, I would have expected an evaluation on standard offline benchmarks (e.g., D4RL or ExoRL). For example, if I'm not mistaken, ExORL provides also image-based observations."
                },
                "questions": {
                    "value": "See above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1825/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698745924330,
            "cdate": 1698745924330,
            "tmdate": 1699636112001,
            "mdate": 1699636112001,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8zpMj6OKyH",
                "forum": "o5Bqa4o5Mi",
                "replyto": "F0YBppZ3ly",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1825/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1825/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer *Kqe9*, thanks for your constructive suggestions. We report our answer about the relevance of this setting and a discussion on the metrics in our general comment, and we answer the remaining questions below. \n\n**Ranking metrics in the experiments:** We add additional details about metrics in our general comment and update the Appendix accordingly. To answer the reviewer\u2019s specific question, Correlation is a ranking metric that evaluates the ranking of the estimated policies\u2019 values, which we clarify in the manuscript. It is commonly adopted for Offline Policy Selection [1] and can be used for comparing policies. We report correlation for all our experiments to assess $\\pi$2vec\u2019s performances on ranking policies. \n\n**Visual representations**: Commonly available foundation models are usually trained on visual (e.g., VIT, TAP) and text-vision data (e.g., CLIP). Those features are commonly adopted in robotics tasks [2]. Motivated by the fact that learning RL policies from vision is hard for such a task at hand, we explore how $\\pi$2vec for vision-based RL benefits from visual foundation models thanks to their geometrical (TAP), semantical (CLIP), and classification-related (VIT) features. Extending $\\pi$2vec for state-based representations requires foundation models that include proprioception or other state-based information as part of their input. Although state-based features are relevant for future work, in this paper, we focused on showing that visual representations are sufficient for policy representations in the domains that we considered.\n\n**Evaluation on standard offline benchmarks**: We are interested in vision-based robotic environments, and we leave further evaluation of $\\pi$2vec on different scenarios as future work. [3] is limited in scope to synthetic environments with a focus on state-based RL (e.g., CartPole, Walker), while [4] evaluates a broader range of environments, including synthetic (Maze2D), navigation (CARLA, AntMaze), and robotic manipulation (Kitchen) environments. In our experiments, we included the Kitchen environment along with other real and synthetic environments for vision-based robotics (Metaworld, RGBStacking, Insert gear sim and real)\n\n\n[1] Justin Fu, Mohammad Norouzi, Ofir Nachum, George Tucker, Ziyu Wang, Alexander Novikov, Mengjiao Yang, Michael R Zhang, Yutian Chen, Aviral Kumar, et al. Benchmarks for deep off-policy evaluation. arXiv preprint arXiv:2103.16596, 2021\n\n[2] Mohit Sharma, Claudio Fantacci, Yuxiang Zhou, Skanda Koppula, Nicolas Heess, Jon Scholz, and Yusuf Aytar. Lossless adaptation of pretrained vision models for robotic manipulation. ICLR 2023.\n\n[3] Yarats, Denis, et al. \"Don't change the algorithm, change the data: Exploratory data for offline reinforcement learning.\" arXiv preprint arXiv:2201.13425 (2022).\n\n[4] Fu, Justin, et al. \"D4rl: Datasets for deep data-driven reinforcement learning.\" arXiv preprint arXiv:2004.07219 (2020)."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1825/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700127492709,
                "cdate": 1700127492709,
                "tmdate": 1700127492709,
                "mdate": 1700127492709,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "75gtjRgkBn",
            "forum": "o5Bqa4o5Mi",
            "replyto": "o5Bqa4o5Mi",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1825/Reviewer_eE8e"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1825/Reviewer_eE8e"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a method for representing reinforcement learning policies as comparable feature vectors. The idea is to use successor features to capture how a policy changes the states of the environment over time. In particular, the method leverages pretrained foundation models to encode individual states into feature vectors. After training the successor features on offline datasets of trajectories, the method aggregates the state-dependent successor features into a state-independent policy embedding that summarizes the policy's behavior. Finally, the embeddings are used to predict policy performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Novel policy representation method: The combination of successor features and foundation models provides a new way to summarize and compare policies based on their effects on the environment. Representing policies by their induced state changes rather than their parameters or actions is an interesting idea.\n\n2. Strong empirical results: The paper presents extensive experiments across 5 domains, including real robots. The method outperforms the baseline policy representation method in predicting held-out policy performance."
                },
                "weaknesses": {
                    "value": "1. Limited theoretical analysis: The paper shows empirically that the obtain successor representations are effective, but provides limited insight/intuition or analysis into why combining successor features and foundation models results in good policy embeddings.\n\n2. Restricted to offline setting: The method seems to require pre-collected offline datasets and cannot be applied in an online setting where policies interact with the environment. The offline assumption limits the applicability."
                },
                "questions": {
                    "value": "1. Why do successor features plus foundation models work well for policy representation? More analysis on why this combination is effective compared to other representations would be useful.\n\n2. Can this method be extended to an online setting where policies interact with the environment? \n\n3. How does dataset composition impact the quality of the policy embeddings? Is there any further analysis on dataset requirements and relationships between dataset and representation quality?\n\n4. Have you considered any other methods to aggregate the state-dependent successor features? Averaging seems effective but overly simplistic; are there alternatives that may capture policies better?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1825/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1825/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1825/Reviewer_eE8e"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1825/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698792283301,
            "cdate": 1698792283301,
            "tmdate": 1699636111938,
            "mdate": 1699636111938,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Gy6hYBvQuQ",
                "forum": "o5Bqa4o5Mi",
                "replyto": "75gtjRgkBn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1825/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1825/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer *eE8e*, thank you for the comments and suggestions to improve our paper. We address the question about online representation in the general comment. We proceed to answer the remaining concerns below.\n\n**Limit analysis on why combining successor features and foundation models is effective:**\n$\\pi$2vec is theoretically justified by the successor feature framework [1]. We based the method on the assumption that visual foundation models consistently represent pertinent features of the environment and are adaptable as reward predictors[2]. Our focus is, therefore, on the empirical evaluation of $\\pi$2vec\u2019s representations. \nWe validate this assumption in paragraph (iii) of the experimental Section when we validate $\\pi$2vec for fully offline policy evaluation. In this setting, we fit the reward predictor $\\hat{r}=\\langle\\phi(s), w_\\text{task}\\rangle$ from offline data, where $\\phi$ is a foundation model, and we estimate $V^\\pi(s)$ by injecting $w_\\text{task}$ in Equation 1 of the main manuscript. Even if this assumption might not be true in general, we show in Table 3 that it is reasonable for the settings we consider. Table 3 reports that estimating value functions with $\\pi$2vec representations is a strong offline policy evaluation method compared to FQE, a state-of-the-art method for OPE. \n\n**Comparing feature representations for $\\pi$2vec:**\nWe clarify our insights about $\\pi$2vec representations. We show that $\\pi$2vec with foundation models always outperforms random features (see Tab. 1 and 2). We compare geometrical (TAP) and semantical (CLIP) features in Table 1 for the task Insert Gear (sim). We show that geometrical representations improve for correlation, while semantical task helps for Regret@1. Furthermore, we discuss our insight by comparing CLIP and VIT for policy representations in paragraph (iv) of the Appendix. From our analysis, we suggest that $\\pi$2vec with CLIP features performs better when the historical policies do not solve tasks effectively. This is often the case when starting working on a new robotic task, e.g., insert gear (real). On the other hand, VIT features help find the best policy among a set of well-performing policies, which is ideal in ablating policies that achieve good performances for the task.\n\n**Dataset quality**: we explore dataset quality in paragraph (v) in Section 7.4 of the Appendix. We collect an enhanced dataset of trajectories for all Metaworld tasks and point-of-views (12 settings). A dataset of trajectories better resembles the behavior of historical policies that are used for $\\pi$2vec\u2019s representations. We report results in Table 5, showing that $\\pi$2vec improves performance when adopting an improved dataset. Similar findings hold when adopting $\\pi$2vec on online data. \nOther aggregation methods: we thank the reviewer for the suggestion. In this initial work, we focus on proposing $\\pi$2vec\u2019s representations, and we leave aggregation strategies as further work.\n\n[1] Barreto, Andr\u00e9, et al. \"Successor features for transfer in reinforcement learning.\" Advances in neural information processing systems 30 (2017).\n\n[2] Du, Yuqing, et al. \"Vision-language models as success detectors.\" CoLA (2023)."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1825/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700145212062,
                "cdate": 1700145212062,
                "tmdate": 1700145212062,
                "mdate": 1700145212062,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "wh6hdgRzbB",
            "forum": "o5Bqa4o5Mi",
            "replyto": "o5Bqa4o5Mi",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1825/Reviewer_4YPE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1825/Reviewer_4YPE"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a new off-policy evaluation apporach. \nThe problem setting is that given a set of historical trajectories sampled from other policies and its corresonding performance, it predicts the performance of a unseen policy. \n\nThe main idea is using the average of successor features over a set of canonical states of each policy, to predict the policy performance.\nThe successor features are obtained via Fitted Q learning via a policy-agnositic state encoder.\n\nThe authors show that proposed approach is usually better than baseline across different domains and metrics."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- This paper studied an important problem of off-policy evaluation.\n- As far as I k now, the idea of leveraging successor feature for the purpose of OPE is new.\n- The authors conducted extensive set of experiments."
                },
                "weaknesses": {
                    "value": "- Potentially missing baseline: I am not closely follow the OPE literature, but could the authors explain why [1] is not suitable for this setting? \nWe should be able to get ranking results by that approach.\n- Missing key ablations: I think there are implicit key assumptions such as \n\t- 1) the historical policies' performance should mostly likely cover the unseen policy. If not, the unseen policy's performance is way better, I hardly expect this approach would work. \n\t- 2) the canonical states coverage. How does it affect the performance if state coverage of MDP is small? More specifically, if the historical dataset only cover 50% of\n\tstates that the unseen policy would visit in the MDP, how does it affect the performance?\n\n\tIt would be good to have a comprehensive understanding when this approach can be effective.\n- In section 3.4, \\Phi_\\pi represents an aggregated average effect of the behavior of \u03c0, I think this highly relies on the implicit assumptions mentioned above. It would be \ngood that authors can clarify.\n\n\n[1] Alexander Irpan, Kanishka Rao, Konstantinos Bousmalis, Chris Harris, Julian Ibarz, and Sergey Levine. Off-policy evaluation via off-policy classification. Advances in Neural Information Processing Systems, 32, 2019."
                },
                "questions": {
                    "value": "Please refer to the weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1825/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698978100296,
            "cdate": 1698978100296,
            "tmdate": 1699636111874,
            "mdate": 1699636111874,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "AmhmeOTg0g",
                "forum": "o5Bqa4o5Mi",
                "replyto": "wh6hdgRzbB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1825/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1825/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer *4YPE*, thank you for the comments and suggestions to improve our paper. We report our answer about when $\\pi$2vec is effective by motivating the use cases of $\\pi$2vec in the general comment, and we address the remaining concerns below. \n\n**Comment on [1]**: Irpan et al. [1] propose an OPE approach for robotics. Authors assume that reward is sparse and binary (success/failure in an episode), and they cast OPE as a binary classification problem. Moreover, their assumption about sparse and binary rewards does not hold in our setting, as the environments that we adopted have continuous and dense rewards. In experiment (iii) of the main paper, we compare our method to FQE, which is a strong baseline in  OPE (similar to [1] but without the assumption of binary rewards), showing promising results.\nCoverage of historical policies: While it is true that very differently performing policies are unlikely to provide useful information for policy evaluation of the new policies, we have some experiments that look at the question of how the difference in the policies affects the performance of the method. We explore the point raised by the reviewer in paragraph (vi) in the Appendix. We adopt $\\pi$2vec for representing intermediate checkpoints for a set of policies for Metaworld assembly task, and we test on held-out policies. Intermediate checkpoints for the same policy are similar to each other and different from checkpoints of other policies, especially at the beginning of training [2].  $\\pi$2vec greatly outperforms actions in this setting for all metrics, proving its robustness to policy coverage. \n\n**Canonical state coverage**: We sample canonical states uniformly from the dataset that was used for training offline RL policies. Even though selecting canonical states to represent the environment better is intuitively beneficial, even simple sampling at random worked well in our experiments. We conduct further experiments to ablate the state coverage. We adopt demonstrations from Metaworld Assembly task and adopt the initial state of each trajectory for $\\pi$2vec\u2019s and Action\u2019s representation. By using the initial state of a trajectory, $\\pi$2vec cannot rely on the state coverage. We report the results in the following table. We show that $\\pi$2vec is robust to state coverage, showing SOTA performances even when only initial states are used. We add this experiment in the Appendix of the revised manuscript.\n\n*Assembly Left*\n| Feature  \t|\tNMAE | Correlation | Regret@1 |\n|--------------|---------|-------------|----------   |\n| CLIP     \t|   0.381 |   \t0.359 |\t0.287 |\n| $\\Delta$CLIP | **0.260** | \t**0.592** |  **0.087** |\n| Random   \t|   0.422 |   \t0.252 | \t0.46 |\n| VIT      \t|   0.366 |    \t0.26 |\t0.347 |\n| Actions  \t|   0.356 |   \t0.503 |\t0.222 |\n\n*Assembly Right*\n| Feature   |\tNMAE | Correlation | Regret@1 |\n|-----------|---------|-------------|----------   |\n| CLIP  \t|   0.363 |   \t0.023 |\t0.365 |\n| $\\Delta$CLIP      \t| **0.242** | \t**0.582** |  **0.096** |\n| Random\t|   0.334 |   \t0.313 |\t0.212 |\n| VIT   \t|\t0.27 |   \t0.345 |\t0.304 |\n| Actions   |   0.405 |   \t0.369 |\t0.263 |\n\n*Assembly Top*\n| Feature  \t|\tNMAE | Correlation | Regret@1 |\n|--------------|---------|-------------|----------   |\n| CLIP     \t|   0.463 |   \t0.270 |\t0.330 |\n| $\\Delta$CLIP | **0.305** | \t**0.594** |  **0.078** |\n| Random   \t|   0.394 |   \t0.277 |\t0.328 |\n| VIT      \t|   0.418 |   \t0.020 |\t0.417 |\n| Actions  \t|   0.414 |   \t0.554 |\t0.106 |\n\n**Interpretation for $\\Phi_\\pi$**: As mentioned by reviewer *4YPE*, $\\Phi_\\pi$ represents policy $\\pi$ by aggregating the successor feature representation for policy $\\pi$ over a canonical set of states. $\\pi$2vec performances are influenced by (i) historical policy coverage, (ii) coverage of the training data, and (iii) canonical states. Our experiments show that (i) $\\pi$2vec is robust to historical policy coverage (paragraph *vi* and *x*, Section 7.4 in the revised Appendix), (ii) including online trajectories in the training set improves $\\pi$2vec\u2019s representations (paragraph *v*, section 7.4 in Appendix), and (iii) canonical state coverage has a limited impact on $\\pi$2vec (paragraph ix in the revised manuscript in the Appendix). We hope our answers address the question and are open for further discussion.\n\n[1] Alexander Irpan, Kanishka Rao, Konstantinos Bousmalis, Chris Harris, Julian Ibarz, and Sergey Levine. Off-policy evaluation via off-policy classification. Advances in Neural Information Processing Systems, 32, 2019.\n\n[2] Konyushova, Ksenia, et al. \"Active offline policy selection.\" Advances in Neural Information Processing Systems 34 (2021): 24631-24644."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1825/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700127397646,
                "cdate": 1700127397646,
                "tmdate": 1700145130599,
                "mdate": 1700145130599,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qAKHrYDHrq",
                "forum": "o5Bqa4o5Mi",
                "replyto": "wh6hdgRzbB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1825/Reviewer_4YPE"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1825/Reviewer_4YPE"
                ],
                "content": {
                    "title": {
                        "value": "Further questions"
                    },
                    "comment": {
                        "value": "Thanks for the detailed response from the authors.\n\n> canonical state coverage has a limited impact \n\nI didn't really understand the intuition behind this. To do a thought experiment, if we only hav one state, how could this approach work? I am a bit skeptical on this conclusion.\n\n> even simple sampling at random worked well in our experiments.\n\nI am not sure if using different selection approach to approve the state coverage is a valid reasoning process. it would be great if we know how many unique states are covered, and what is percentage of those `canonical states/full state`, more importantly  `canonical states/full state sampled from high quality policy` I might be missing sth but I couldn't find such info in the paper and author response."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1825/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700633541034,
                "cdate": 1700633541034,
                "tmdate": 1700633558330,
                "mdate": 1700633558330,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "KFfoFAWsQH",
            "forum": "o5Bqa4o5Mi",
            "replyto": "o5Bqa4o5Mi",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1825/Reviewer_81Fm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1825/Reviewer_81Fm"
            ],
            "content": {
                "summary": {
                    "value": "The ability to represent reinforcement learning policies in a vector space would allow for quickly evaluating policies' performance offline. Successor features is a learning paradigm that defines a policy's performance (i.e., expected reward) as being linear in the features of the policy. The authors propose to leverage ideas from successor features by predicting the performance of unknown policies with the performance of known policies based on joint learned successor feature embedding. This would allow for evaluating unknown policy's performance offline without needing online interactions."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The idea of storing an embedding space where you can look up policies could be beneficial for OPE and imitation learning.\n- The combination of exploring foundation model features and successor features is interesting.\n- The authors provide a very thorough empirical investigation of the performance of the proposed idea across several tasks. Furthermore, they investigated various representation learning ideas beyond changing the underlying foundational model, which provided insight into which representation is important for each task.\n- The evaluation metrics chosen provide insight between relative performance (i.e., correlation), absolute performance (i.e., NMAE), and performance against the best policy (i.e., regret). All three metrics captured three different and important aspects of the learning problem, providing a lot of insight."
                },
                "weaknesses": {
                    "value": "- This paper needs more analysis regarding why certain features work better for certain settings. At the moment, if I implement this idea, I would have to enumerate all possible pairs.\n- The authors only compare to 1 baseline algorithm that only depends on the actions of the policies in the offline dataset. Meanwhile, the proposed method uses the state-action in the offline dataset to learn the successor feature components. I don't know if the performance increase of the baseline is due to the proposed idea or the additional information the proposed idea has access to."
                },
                "questions": {
                    "value": "- If you had online policy trajectories from behavior policies, how would that affect the proposed idea?\n- How did you find the best policy to compare against for the regret metric? Was the best policy feature-dependent or feature-agnostic?\n- Why is the correlation metric related to how many evaluations on the reboot are required to find the best policy? I thought correlation was the relationship between the set of predicted values and ground-truth values. This relationship could be arbitrarily bad; no matter how many evaluations you do, the underlying feature may not provide a reasonable signal that relates to the ground truth.\n- Can I assume that correlation means relative performance (i.e.,  the ordering of values between the prediction and ground are the same), while NMAE is absolute performance (i.e., the predicted and ground values are exactly the same)?\n- What is significant of NMAE? The performance values of Table-1 and Table-2 imply that correlation indicates regret. This means that representations with low regret have a high correlation, but NMAE does not have a relationship to regret.\n- Why is action representation the only baseline applicable baseline? The underlying data has states and actions. Would it be unfair to condition a baseline on both state-action pairs?\n- The discussion from (i), (ii), and (iii) in the results section is confusing. In results (ii), the authors raise the point that NMAE is better, but in (iii), the authors raise the point that their approach is better in regret. What metric is the most important across these metrics presented?\n\n\nMissing cites:\n\n- Original Successor feature paper:  Improving generalization for temporal difference learning: The successor representation by Dayan, et al. 1993.\n- Successor Feature Representations by Reinke et al 2023\n- Successor Feature Sets: Generalizing Successor Representations Across Policies by Brantley et al. 2021\n- Successor Features Combine Elements of Model-Free and Model-based Reinforcement Learning by Lehnert et al 2020"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1825/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699463994202,
            "cdate": 1699463994202,
            "tmdate": 1699636111801,
            "mdate": 1699636111801,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "3utjLHjGJf",
                "forum": "o5Bqa4o5Mi",
                "replyto": "KFfoFAWsQH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1825/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1825/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer *81Fm*, we appreciate the insightful feedback and reference to missing citations, which helped improve our manuscript. We address the question about online policy representation and metrics in the general comment, and next, we address the remaining concerns below. \n\n**Better analysis of the features**: we present our intuition regarding the features to adopt in paragraph (iv) in the Appendix. We compare CLIP and VIT across all environments. Tables 1 and 2 show a correlation between the choice of the feature encoder and policies\u2019 true return values. CLIP performs better than VIT in Insert Gear (Sim), Insert Gear (Real), and Metaworld. Table 4 reports policy performances in these environments, showing they are the hardest to solve, as the standard deviation among policies is high. These results demonstrate that CLIP is robust when the task is hard, and most of the policies fail to solve it. We stress that $\\pi$2vec might be especially useful for this use case, as corresponds to a scenario where there is an ongoing research effort to train policies for a particular task on a real robotic platform. On the other hand, VIT is the best option when the standard deviation of policy returns are low or negligible (e.g., Kitchen and RGB stacking), making this feature most suited for ranking policies that consistently solve the task. \n\n**Baseline comparison**: We point out that the action representations are extracted by running the historical policies on the set of canonical states to compute their actions. We adopt the same set of canonical states for $\\pi$2vec and action representations. In this regard, the baseline and $\\pi$2vec have access to the same data, and the baseline uses state information implicitly by deciding which actions should be compared. Active Offline Policy Selection (AOPS) [1] stands alone as a notable work that delves into policy representation from offline data with the task of deciding which policies should be evaluated in priority to gain the most information about the system. We argue that this is a strong baseline, as demonstrated in [1] \"actions baseline\" proves to be insightful for understanding policy behavior (Figure 3 in [1]) and predicting policy performance (Figure 5 in [1]).  We are not aware of any previous work that addresses this particular problem by bringing foundation model representations into offline policy representation and, in particular, any other prior method that would use states in a more explicit way. \n\n**Finding the best policy**: when reporting average results across tasks\u2013Metaworld and Kitchen\u2013we cross-validate $\\pi$2vec with various features on the historical policies for each task. Next, for each task, we pick $\\pi$2vec with the feature representation that achieves the lowest regret. We report the average of the per-task best representations as Best-* in Table 2. \n\n[1] Konyushova, Ksenia, et al. \"Active offline policy selection.\" Advances in Neural Information Processing Systems 34 (2021): 24631-24644."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1825/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700127286401,
                "cdate": 1700127286401,
                "tmdate": 1700127286401,
                "mdate": 1700127286401,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]