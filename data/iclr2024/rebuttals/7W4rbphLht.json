[
    {
        "title": "A Semi-smooth, Self-shifting, and Singular Newton Method for Sparse Optimal Transport"
    },
    {
        "review": {
            "id": "IspXF5H670",
            "forum": "7W4rbphLht",
            "replyto": "7W4rbphLht",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5242/Reviewer_35vU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5242/Reviewer_35vU"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors introduce a novel Newton-type algorithm called S5N, designed to tackle problems with potentially non-differentiable gradients and non-isolated solutions. This is particularly relevant for the sparse optimal transport problem. The S5N algorithm stands out from existing Newton-type methods due to its wide applicability, absence of hyperparameter tuning, and robust global and local convergence guarantees. Numerical experiments demonstrate that S5N outperforms in terms of convergence speed and computational efficiency when applied to sparse optimal transport problems."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Some of the strengths of the paper are \n* The paper is well written and the contributions are defined clearly \n* In this paper the authors measure the ratio between the actual and predicted reduction of function values similar to trust-region methods to decide whether to accept the next step or not\n* The proposed algorithm can handle non-smooth gradients and singular Hessian matrices\n* Does not rely on line search to guarantee global convergence \n* The stepsize $\\eta^k$ does not need to satisfy sufficient decrease or curvature conditions as line search-based methods. It only needs to be bounded to guarantee global convergence \n* The authors also show that under mild assumptions the proposed algorithm has local quadratic convergence\n\nThank you for providing the code!"
                },
                "weaknesses": {
                    "value": "I want to know why the authors used only MNIST and Fashion-MNIST apart from Examples 1 and 2 to compare their algorithm experimentally. There are plenty of different datasets that the authors could explore and share the results of how their algorithm behaves compared to other algorithms. Moreover, why did they choose only image data to test their algorithm in addition to Examples 1 and 2?"
                },
                "questions": {
                    "value": "Mentioned on weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5242/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5242/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5242/Reviewer_35vU"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5242/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698510303173,
            "cdate": 1698510303173,
            "tmdate": 1699636523504,
            "mdate": 1699636523504,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wtroaWmMQ8",
                "forum": "7W4rbphLht",
                "replyto": "IspXF5H670",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5242/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5242/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> I want to know why the authors used only MNIST and Fashion-MNIST apart from Examples 1 and 2 to compare their algorithm experimentally. There are plenty of different datasets that the authors could explore and share the results of how their algorithm behaves compared to other algorithms. Moreover, why did they choose only image data to test their algorithm in addition to Examples 1 and 2?\n\nWe appreciate the various helpful comments. Firstly, OT is not sensitive to the type of data. Its input only contains two probability vectors and a cost matrix, and all types of data are eventually summarized into these three quantities. In this sense, we are more interested in designing the input probability vectors and the cost matrix, rather than choosing the original data set. For instance, our Example 1 studies two similar distributions, and Example 2 is for two distinct distributions.\n\nThe reason for choosing MNIST and Fashion-MNIST is that they have a simple and natural cost matrix, thus convenient for demonstration, enabling us to focus on algorithm development and validation. For higher-dimensional image data, it is commonly suggested that the cost matrix should be computed from some feature space rather than from the pixel space. Therefore, for more complicated image data, typically a feature extraction step is required before applying OT, which introduces another factor that needs to be controlled. Regarding this work, we focus more on the computing of sparse OT than on the application to various data types, but of course, we would be glad to explore more data sets later, once the technical questions have been properly handled."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5242/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700092572799,
                "cdate": 1700092572799,
                "tmdate": 1700092572799,
                "mdate": 1700092572799,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "a9cSzUT6iC",
                "forum": "7W4rbphLht",
                "replyto": "IspXF5H670",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5242/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5242/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Added experiments on ImageNet data"
                    },
                    "comment": {
                        "value": "Dear reviewer, per the suggestion we have added more experiments on OT applied to the higher-dimensional and larger-scale ImageNet data. In particular, we compute sparse OT between two categories of images from a subset of the ImageNet data set. The subset contains ten classes of ImageNet images, and approximately 1000 images per category are selected. We map each image to a 30-dimensional feature vector by first passing the image to a ResNet18 network, resulting in a 512-dimensional vector, then followed by a dimension reduction by principal component analysis. Let $x_i\\in\\mathbb{R}^{30}$ be the feature vector of an image in the first category, $i=1,\\ldots,n$, and $y_j\\in\\mathbb{R}^{30}$ be the feature vector of an image in the second category, $j=1,\\ldots,m$. Then $\\nu=n^{-1}\\mathbf{1}_n$, $\\mu=m^{-1}\\mathbf{1}\\_m$, and the cost matrix is  $C\\_{ij}= \\Vert x_i-y_j\\Vert^2$.\n\nThe results are displayed in Figures 19 and 20 in the revised article."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5242/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700383014645,
                "cdate": 1700383014645,
                "tmdate": 1700383014645,
                "mdate": 1700383014645,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5UvgDjM7Pf",
                "forum": "7W4rbphLht",
                "replyto": "a9cSzUT6iC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5242/Reviewer_35vU"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5242/Reviewer_35vU"
                ],
                "content": {
                    "comment": {
                        "value": "Dear authors, \n\nThank you for taking into consideration my question and adding more experiments."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5242/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700400763081,
                "cdate": 1700400763081,
                "tmdate": 1700400763081,
                "mdate": 1700400763081,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ULwdRs2pjk",
            "forum": "7W4rbphLht",
            "replyto": "7W4rbphLht",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5242/Reviewer_h4ty"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5242/Reviewer_h4ty"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a Newton method to minimize functions with non-smooth gradients, more specifically,  differentiable functions, not differentiable twice. The main application of such an algorithm is to solve the dual of the quadratically regularized optimal transport problem. The authors provide convergence rates for the proposed algorithm and optimal transport experiments."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The algorithm seems very tailored for the considered problem."
                },
                "weaknesses": {
                    "value": "- 1 In equation 2, how is the linear system solved? Do you solve it exactly (with usual decomposition techniques) or approximately, using iterative algorithms? If you use iterative algorithms, which algorithms do you use? To which tolerance / number of iterations do you solve the linear system?\n\n- 2 The step size.\nFirst, in the main theorem 3, no clear assumption is given on the step size $\\eta_k$. I think the authors should write clearly under which assumptions the proposed algorithm converges.\n\nThen, I do not understand how the condition on the step size (loosly given in the main text) can be so loose: \"In fact, the only requirement is that $\\eta_k$ needs to be\nbounded\", how is it possible??\nIf I take a sequence of exponentially decreasing step size, how can the proposed algorithm converge? This is usually why one has to resort to costly line-search techniques.\nCould authors comment on this?\n\n- 3 Experiments. I think experiments should be reshaped: I do not know how significant/useful the experiments with run time below a second are: I guess that for these timings, implementation matters much more than the algorithms themselves: I would remove experiments with run time below 1 second.\nFor clarity, I would select around 5 algorithms, and display all the benchmark algorithms on a single graph, and multiple values of the regularization parameter $\\gamma$, like in Figure 1 of [1].\nThe block coordinate descent algorithm does not seem to converge, could you comment on this?\n\n[1] Lin, H., Mairal, J. and Harchaoui, Z., 2015. A universal catalyst for first-order optimization. Advances in neural information processing systems, 28."
                },
                "questions": {
                    "value": "See Weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5242/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698691978857,
            "cdate": 1698691978857,
            "tmdate": 1699636523379,
            "mdate": 1699636523379,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wI7AH6nrqp",
                "forum": "7W4rbphLht",
                "replyto": "ULwdRs2pjk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5242/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5242/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Clarification on the step sizes"
                    },
                    "comment": {
                        "value": "> In equation 2, how is the linear system solved? Do you solve it exactly (with usual decomposition techniques) or approximately, using iterative algorithms? If you use iterative algorithms, which algorithms do you use? To which tolerance / number of iterations do you solve the linear system?\n\nThanks for raising this question. Equation (2) is a generic statement for solving the linear system. Specific to sparse OT, the computation method is given in Appendix C, which is based on the conjugate gradient (CG) method. Regarding the setting of precision, in our implementation, the tolerance for CG is globally set to $10^{-6}$.\n\n> The step size. First, in the main theorem 3, no clear assumption is given on the step size. I think the authors should write clearly under which assumptions the proposed algorithm converges.\n\nOur assumption on the step size is described in Section 2.2. Specifically, we require that the step sizes are bounded from both below and above: $\\eta_k\\in[\\tilde{m},\\tilde{M}]$, $0<\\tilde{m}<\\tilde{M}<+\\infty$.\n\n> Then, I do not understand how the condition on the step size (loosely given in the main text) can be so loose: \"In fact, the only requirement is that needs to be bounded\", how is it possible?? If I take a sequence of exponentially decreasing step size, how can the proposed algorithm converge? This is usually why one has to resort to costly line-search techniques. Could authors comment on this?\n\nWe apologize for the confusion on the term \"bounded\". A more precise statement is that we require the step sizes to be **bounded from both below and above**: $\\eta_k\\in[\\tilde{m},\\tilde{M}]$, $0<\\tilde{m}<\\tilde{M}<+\\infty$. We have clarified this point in the revision to avoid confusions.\n\n> Experiments. I think experiments should be reshaped: I do not know how significant/useful the experiments with run time below a second are: I guess that for these timings, implementation matters much more than the algorithms themselves: I would remove experiments with run time below 1 second. For clarity, I would select around 5 algorithms, and display all the benchmark algorithms on a single graph, and multiple values of the regularization parameter $\\gamma$ , like in Figure 1 of [1]. The block coordinate descent algorithm does not seem to converge, could you comment on this?\n\nThanks for the suggestions. In fact, as can be inferred from Figures 1-3, for most first-order methods, they take much longer than one second to converge, and we truncate the iteration number for better illustration. Under the same setting, Newton-type methods are able to finish in shorter times.\n\nWe think our timing results are comparable, as we spent lots of efforts in implementing every algorithm in efficient C++ code, and tried our best to reduce the overhead caused by programming language.\n\nFor the illustration of results, since we have considered many existing algorithms, it would be visually undesirable if we put all curves in the same plot. Instead, we categorize the algorithms into three groups based on their characteristics, and then compare their performance with the S5N algorithm.\n\nFor the BCD algorithm, it is normal to see a very long 'flat' stage, as this is also observed in other articles such as Figures 1 and 2 in [2]. It will converge after a sufficiently large number of iterations.\n\n[1] Lin, H., Mairal, J. and Harchaoui, Z., 2015. A universal catalyst for first-order optimization. Advances in neural information processing systems, 28.\n\n[2] Pasechnyuk, D.A., Persiianov, M., Dvurechensky, P. and Gasnikov, A., 2023. Algorithms for euclidean regularised Optimal Transport. arXiv preprint arXiv:2307.00321."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5242/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700092515075,
                "cdate": 1700092515075,
                "tmdate": 1700092515075,
                "mdate": 1700092515075,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lcvoC69Ky8",
                "forum": "7W4rbphLht",
                "replyto": "wI7AH6nrqp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5242/Reviewer_h4ty"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5242/Reviewer_h4ty"
                ],
                "content": {
                    "title": {
                        "value": "Response to Rebutal"
                    },
                    "comment": {
                        "value": "I thank the authors for their time and effort in the rebuttal.\n\nI still think that the graphs with timing in milliseconds should be removed for graphs in larger dimension. I really believe that such timing does not reflect which algorithms is better, but much more implementation tricks\n\nI do not see vanilla coordinate descent in Figure 1 of [2], but a variance-reduced variation. Could authors comment on this?\n\n[2] Pasechnyuk, D.A., Persiianov, M., Dvurechensky, P. and Gasnikov, A., 2023. Algorithms for euclidean regularised Optimal Transport. arXiv preprint arXiv:2307.00321."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5242/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700606592730,
                "cdate": 1700606592730,
                "tmdate": 1700606592730,
                "mdate": 1700606592730,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "IIx9pUU2Ed",
            "forum": "7W4rbphLht",
            "replyto": "7W4rbphLht",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5242/Reviewer_7Z3B"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5242/Reviewer_7Z3B"
            ],
            "content": {
                "summary": {
                    "value": "In this paper authors, motivated by optimal transport problems, propose Newton-type algorithm for objectives, that may have non-smooth gradients and singular Hessians. The proposed algorithm converges globally and has local quadratic convergence."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper proposes new Newton-type method, that can work with non-smooth gradients and singular Hessian matrices\n2. The experimental results show, that proposed method is superior over other existing approaches\n3. The results are mostly presented in a clear, understandable way."
                },
                "weaknesses": {
                    "value": "1. Authors use grid-search to find the best step size on each step, which can be time-consuming in practice.\n2. Proposed algorithm converges globally, but it is not clear, what is the speed of global convergence.\n3. Lack of definitions of some important concepts. Maybe it is clear for those, who are very familiar with this topic, but I think, introduction of such definitions may improve the overall look of papers (see questions)."
                },
                "questions": {
                    "value": "1. Probably, in the abstract you have a typo and you meant \"possibly non-smooth gradients\" instead of \"possibly non-differentiable gradients\"\n2. Please, increase the font size of text in the figures\n3. Please, provide the definitions of local Lipschitz continuity and upper semi-continuity."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5242/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5242/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5242/Reviewer_7Z3B"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5242/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698822497263,
            "cdate": 1698822497263,
            "tmdate": 1699636523291,
            "mdate": 1699636523291,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "b4RNYujZSm",
                "forum": "7W4rbphLht",
                "replyto": "IIx9pUU2Ed",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5242/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5242/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "### Weakness part\n\n> Authors use grid-search to find the best step size on each step, which can be time-consuming in practice.\n\nThanks for the comments. We shall point out that unlike classical line search methods, the choice of the step size in S5N is **optional**, since the theory only requires that the step sizes are bounded from both below and above. Therefore, users are allowed to set the number of candidates, and can keep it small. Also, Algorithm 2 is **not** a full grid search. It immediately stops when function value decreases, and it typically only takes one or two steps.\n\n> Proposed algorithm converges globally, but it is not clear, what is the speed of global convergence.\n\nThanks for raising this question. To the best of our knowledge, existing global convergence results for semi-smooth Newton methods are almost all asymptotic. On the other hand, although first-order methods have non-asymptotic rates, the empirical results shown in Figures 1-3 suggest that they are much slower than Newton-type methods. We agree that the global convergence rate of semi-smooth Newton is indeed a topic that deserves future research.\n\n> Lack of definitions of some important concepts. Maybe it is clear for those, who are very familiar with this topic, but I think, introduction of such definitions may improve the overall look of papers (see questions).\n\nThanks for the suggestions. We have added the relevant definitions in Appendix A in the revision.\n\n### Question part\n\n> Probably, in the abstract you have a typo and you meant \"possibly non-smooth gradients\" instead of \"possibly non-differentiable gradients\"\n\nThanks for pointing out this issue. In our context, \"non-smooth\" and \"non-differentiable\" have the same meaning. For consistency we have changed \"non-differentiable\" to \"non-smooth\" (in the PDF, as we are no longer able to edit the abstract shown in the page).\n\n> Please, increase the font size of text in the figures\n\n> Please, provide the definitions of local Lipschitz continuity and upper semi-continuity.\n\nThanks for all these helpful suggestions. We have updated the definitions related to local Lipschitz continuity and upper semi-continuity in Appendix A, and have enlarged the font size of the text in the figures."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5242/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700092126319,
                "cdate": 1700092126319,
                "tmdate": 1700092126319,
                "mdate": 1700092126319,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "en0BVAMddy",
            "forum": "7W4rbphLht",
            "replyto": "7W4rbphLht",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5242/Reviewer_cPiV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5242/Reviewer_cPiV"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a general Newton-type algorithm named\nS5N, to solve problems that have possibly non-differentiable gradients and non-isolated solutions, a setting highly motivated by the sparse optimal transport problem. Compared with existing Newton-type approaches, the proposed S5N algorithm has broad applicability, does not require hyperparameter tuning, and possesses rigorous global and local convergence guarantees. Extensive numerical\nexperiments show that on sparse optimal transport problems, S5N gains superior performance on convergence speed and computational efficiency."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Compared with existing Newton-type approaches, the proposed S5N algorithm has broad applicability, does not require hyperparameter tuning, and possesses rigorous global and local convergence guarantees. Extensive numerical\nexperiments show that on sparse optimal transport problems, S5N gains superior performance on convergence speed and computational efficiency."
                },
                "weaknesses": {
                    "value": "1. The results in Sec. 2.4 seems not new. They seem to be standard results of semi-smooth Newton. Thus, the author should cite some references.\n2. It seems that $f(x)$ in Eq. (4) is not a smooth function because $()_+$ operator is not smooth. Thus, Proposition 1 can not hold since $\\nabla f(\\alpha, \\beta)$ does not exist.\n3. The convergence analysis of this paper lies on Assumption 1 and Assumption 2 which requires that $f(x)$ is differentiable, so the convergence analysis can not be used in the problem that $f(x)$ is not smooth.\n4. This paper claims that S5N is used to solve problems that have possibly \\emph{non-differentiable} gradients and non-isolated solutions.\nHowever, the convergence analysis requires Assumption 1which supposes that $f(x)$ is differentiable."
                },
                "questions": {
                    "value": "No"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5242/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5242/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5242/Reviewer_cPiV"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5242/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699070087897,
            "cdate": 1699070087897,
            "tmdate": 1700224252464,
            "mdate": 1700224252464,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "40C9CvYpLo",
                "forum": "7W4rbphLht",
                "replyto": "en0BVAMddy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5242/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5242/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "First of all, we thank the reviewer for the comments, and would like to respectfully correct a factual misunderstanding in the review.\n\n> It seems that $f(x)$ in Eq. (4) is not a smooth function because $()_+$ operator is not smooth. Thus, Proposition 1 can not hold since $\\nabla f(\\alpha, \\beta)$ does not exist.\n\n> The convergence analysis of this paper lies on Assumption 1 and Assumption 2 which requires that $f(x)$ is differentiable, so the convergence analysis can not be used in the problem that $f(x)$ is not smooth.\n\n> This paper claims that S5N is used to solve problems that have possibly *non-differentiable* gradients and non-isolated solutions. However, the convergence analysis requires Assumption 1 which supposes that $f(x)$ is differentiable.\n\nWe would like to point out that $f(x)$ in Eq. (4) in fact involves $h(x)=[(x)\\_+]^2$, not $(x)\\_+$. It can be verified that the former **is differentiable**, with the derivative $h'(x)=2\\cdot(x)_+$. Therefore, $\\nabla f(\\alpha,\\beta)$ in Proposition 1 **indeed exists**, and Assumption 1 holds for the problem we consider.\n\nWe humbly hope the reviewer could re-evaluate our main results and raise the score when appropriate, if this misunderstanding is resolved. \n\n> The results in Sec. 2.4 seems not new. They seem to be standard results of semi-smooth Newton. Thus, the author should cite some references.\n\nThanks for the comments. While the expressions of the main conclusions in Section 2.4 appear to be standard, we have proved them for our new algorithm, S5N, under completely new settings (non-smooth gradient **and** singular Hessian). Therefore, the proof is specific to S5N, which is not a trivial consequence of existing results.\n\nWe have added references to standard results, and have made our contributions clearer in the revision."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5242/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700091782647,
                "cdate": 1700091782647,
                "tmdate": 1700091782647,
                "mdate": 1700091782647,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vTeA6EUjAn",
                "forum": "7W4rbphLht",
                "replyto": "40C9CvYpLo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5242/Reviewer_cPiV"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5242/Reviewer_cPiV"
                ],
                "content": {
                    "comment": {
                        "value": "Since $(x)_+$ is not differentiable, why $h(x) = [(x)_+]^2$ is differentiable? Chain Rule!"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5242/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700105714226,
                "cdate": 1700105714226,
                "tmdate": 1700105714226,
                "mdate": 1700105714226,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6Q8Ofxy3qG",
                "forum": "7W4rbphLht",
                "replyto": "ZJhjma8Ftw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5242/Reviewer_cPiV"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5242/Reviewer_cPiV"
                ],
                "content": {
                    "comment": {
                        "value": "I think you are right. I raise the score to 6."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5242/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700224215962,
                "cdate": 1700224215962,
                "tmdate": 1700224215962,
                "mdate": 1700224215962,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "P1xxiTIs7R",
            "forum": "7W4rbphLht",
            "replyto": "7W4rbphLht",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5242/Reviewer_tikC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5242/Reviewer_tikC"
            ],
            "content": {
                "summary": {
                    "value": "In the paper, the authors propose a new version of the gradient regularized Newton method with singular Hessian. It is applied to solve quadratically regularized optimal transport problems. It was proven that the method has asymptotical convergence of the gradient norm to zero. Additionally, the local quadratic convergence was proved under certain additional assumptions. The paper includes experiments with various data and setups."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The problem studied in the paper seems quite interesting, with real-life applications in optimal transport. The experimental results seem promising, and the code is implemented in an efficient, high-performance manner."
                },
                "weaknesses": {
                    "value": "Unfortunately, I find it challenging to identify new and distinct results and contributions in the presented paper. Allow me to clarify my perspective. The paper consists of three main parts: 1) The S5N method for general problems with semi-smooth Hessian; 2) Application of S5N to quadratically regularized optimal transport; 3) Experiments. Next, I will discuss all of them in detail.\n\n1) I begin with the general problems and S5N; the presented results are very similar to those in papers [1],[2]. In both papers, gradient regularization is employed with some adaptive procedures. Across all three papers, the global rates are asymptotical, and the local rates are quadratic (in 2, it is even cubic). Moreover, all these rates are relatively weak compared to modern optimization methods with non-asymptotical global convergence rates. For example, first-order methods offer guaranteed non-asymptotical global rates with a much cheaper computational cost of every iteration. Thus, from a theoretical perspective, the first part does not seem to contribute significantly compared to the existing literature from my point of view. Please correct me if I overlooked any substantial improvement and theoretical challenge over the previous papers. \n\n2)  In the optimal transport (OT) section, the authors describe the OT problem and why it has the semi-smooth Hessian, as discussed in [3]. Therefore, the application of semi-smooth Newton to OT is also not novel (note that the authors do not claim it to be new).\n\n3) Numerical Experiments: I appreciate the inclusion of a large number of competitors' methods.  However, the presentation and comparison seem unfair to me. I believe that comparing the Iteration Number may be biased in the context of first-order and second-order methods. I would recommend using gradient computations as a more appropriate metric for comparison. Also, theoretical per-iteration computational complexities could help. I listed more questions in the next section to clarify the methods\u2019 performances. \n\nTo sum up, the presented contribution is not enough to be accepted for the ICLR, from my point of view. \n\n[1] Xiantao Xiao, Yongfeng Li, Zaiwen Wen, and Liwei Zhang. A regularized semi-smooth Newton method with projection steps for composite convex programs. Journal of Scientific Computing, 76:364\u2013389, 2018.\n\n[2] Weijun Zhou and Xinlong Chen. On the convergence of a modified regularized Newton method for convex optimization with singular solutions. Journal of Computational and Applied Mathematics, 239:179\u2013188, 2013.\n\n[3] Dirk A Lorenz, Paul Manns, and Christian Meyer. Quadratically regularized optimal transport. Applied Mathematics & Optimization, 83(3):1919\u20131949, 2021."
                },
                "questions": {
                    "value": "1) It is quite counterintuitive for me: why do we want to use second-order methods for almost quadratic problems (quadratic + linear)? Why not use simple CG or PCG to solve it? \n\n2) Why in time plots the first-order methods are slower than S5N? \n\n3) Why do you not compare S5N with the classical baseline for OT, the Sinkhorn method?\n\n4) Why does the gradient time computation differ for different methods? \u201cthe semi-dual L-BFGS has a fast convergence in iteration number, it suffers from a long run time since its gradient computation is more difficult and time-consuming than other methods.\u201d\n\n5) Do you have any intuition: why \u201cASSN has a very slow convergence\u201d in practice? It seems to have the same structure with gradient regularization."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "n\\a"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5242/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5242/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5242/Reviewer_tikC"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5242/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699556427849,
            "cdate": 1699556427849,
            "tmdate": 1699636523120,
            "mdate": 1699636523120,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1cDCx63FAj",
                "forum": "7W4rbphLht",
                "replyto": "P1xxiTIs7R",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5242/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5242/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to the Weakness part"
                    },
                    "comment": {
                        "value": "> Relation to papers [1], [2]; comparison to first-order methods; improvement and theoretical challenge over the previous papers.\n\nThanks for the comments. Regarding the two articles mentioned above, we would like to make the following clarifications:\n\n1. [1] introduced the ASSN algorithm, which only applies to fix-point problems of the form $T(z)=z$, where $T=(1-\\alpha)I+\\alpha R$, and $R$ is a nonexpansive operator. This imposes a restriction on the problem to solve, whereas in contrast, S5N has substantially broader use.\n2. In addition, the local convergence result of [1] relies on the BD-regular condition, which essentially does not support singular problems. We have reflected this point in Table 1.\n3. Also shown in Table 1, [2] assumes that the objective function is twice continuously differentiable and the Hessian is Lipschitz continuous. These are quite strong assumptions, and that is why it can achieve local cubic convergence.\n\nIn this sense, neither [1] nor [2] really applies to the general setting of non-smooth gradient and singular Hessian. In constract, we have obtained both global and local convergence properties of S5N under significantly weaker assumptions, and we think this is a major theoretical contribution that differs from existing literature.\n\nAs for first-order methods, although they have non-asymptotic global convergence rates, they are mostly sub-linear, thus resulting in slower convergence speed as evidenced in Figures 1-3.\n\nMore importantly, the per-iteration cost of first-order methods is not necessarily smaller than that of Newton-type methods for sparse OT problems. For example, block coordinate descent (BCD) needs to sort length-$n$ vectors $n$ times, with a total cost of $\\mathcal{O}(n^2\\log n)$. Newton-type methods constructs the $\\sigma$ sparse matrix in Proposition 1 in $\\mathcal{O}(n^2)$ time, and solves the linear equation using conjugate gradient (CG) in $\\mathcal{O}(nnz\\cdot\\log(1/\\varepsilon))$ time, where $nnz$ is the number of nonzero elements in $\\sigma$, and $\\varepsilon$ is the tolerance. Their total per-iteration cost can be substantially smaller than $\\mathcal{O}(n^2\\log n)$.\n\n> In the optimal transport (OT) section, the authors describe the OT problem and why it has the semi-smooth Hessian, as discussed in [3].\n\nAgain shown in Table 1, the algorithm used by [3], named GRSSN, does not provide proofs for the global and local convergence guarantees. Crucially, GRSSN introduces a very sensitive hyperparameter $\\lambda$, which we have discussed in Figures 1-3, Section 5. S5N is shown to perform better than GRSSN with no hyperparameter tuning.\n\n> Numerical Experiments\n\nTo compare different algorithms, we not only consider the iteration number, but also the run time as a metric for the overall performance. We have implemented every algorithm in efficient C++ to minimize the overhead and to make the run times comparable. Also, the theoretical complexity analysis from our response above also indicates that second-order methods may even have a smaller per-iteration cost than some first-order methods in the context of sparse OT.\n\n[1] Xiantao Xiao, Yongfeng Li, Zaiwen Wen, and Liwei Zhang. A regularized semi-smooth Newton method with projection steps for composite convex programs. Journal of Scientific Computing, 76:364\u2013389, 2018.\n\n[2] Weijun Zhou and Xinlong Chen. On the convergence of a modified regularized Newton method for convex optimization with singular solutions. Journal of Computational and Applied Mathematics, 239:179\u2013188, 2013.\n\n[3] Dirk A Lorenz, Paul Manns, and Christian Meyer. Quadratically regularized optimal transport. Applied Mathematics & Optimization, 83(3):1919\u20131949, 2021."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5242/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700091498883,
                "cdate": 1700091498883,
                "tmdate": 1700091498883,
                "mdate": 1700091498883,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4tH4mDjmdv",
                "forum": "7W4rbphLht",
                "replyto": "P1xxiTIs7R",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5242/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5242/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to the Question part"
                    },
                    "comment": {
                        "value": "> It is quite counterintuitive for me: why do we want to use second-order methods for almost quadratic problems (quadratic + linear)? Why not use simple CG or PCG to solve it?\n\nWe would like to point out that this problem is **piecewise** quadratic, which has properties quite distinct from quadratic functions. In particular, its KKT condition is not a linear system, so CG or PCG is not applicable here.\n\nWe focus on second-order methods here since (1) second-order methods in general exhibit fast convergence; (2) for sparse OT problems, the generalized Hessian is sparse, making the per-iteration cost very small.\n\n> Why in time plots the first-order methods are slower than S5N?\n\nThere are two major factors: (1) Newton-type methods have faster convergence speed in terms of iteration number; (2) according to our analysis in the response above, first-order methods might even have a higher per-iteration cost. Overall, S5N exhibits visible advantage over first-order methods.\n\n> Why do you not compare S5N with the classical baseline for OT, the Sinkhorn method?\n\nThanks for raising this question. Given that sparse OT is a highly structured problem, and Sinkhorn represents another approximation to OT that results in a dense optimal plan, our focus is on the computation of quadratically regularized OT. It is not comparable to Sinkhorn when sparsity is required.\n\n> Why does the gradient time computation differ for different methods? \"the semi-dual L-BFGS has a fast convergence in iteration number, it suffers from a long run time since its gradient computation is more difficult and time-consuming than other methods.\"\n\nAlthough L-BFGS can be applied to both the dual and semi-dual objective functions, their gradient computation is different. Semi-dual L-BFGS has a similar computation as BCD, and according to our analysis above, it takes $\\mathcal{O}(n^2\\log n)$ time in each gradient evaluation. In contrast, dual L-BFGS is $\\mathcal{O}(n^2)$. Therefore, although the total number of iterations for semi-dual might be smaller than dual, the longer per-iteration time of semi-dual results in a higher overall run time.\n\n> Do you have any intuition: why \u201cASSN has a very slow convergence\u201d in practice? It seems to have the same structure with gradient regularization.\n\nWe have two conjectures here: (1) although algorithmically ASSN can be applied to the QROT problem, the nature of the QROT objective function does not satisfy the convergence assumptions of the algorithm (fixed-point operator, singular Hessian, etc.); (2) from the perspective of ASSN's structure, it has no flexibility in choosing the step size. As a result, we have observed that the algorithm undergoes many rejection steps (i.e., Newton steps that do not behave well) during iterations, resulting in a slow convergence speed."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5242/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700091640438,
                "cdate": 1700091640438,
                "tmdate": 1700091640438,
                "mdate": 1700091640438,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FTOs9lCl3T",
                "forum": "7W4rbphLht",
                "replyto": "4tH4mDjmdv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5242/Reviewer_tikC"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5242/Reviewer_tikC"
                ],
                "content": {
                    "title": {
                        "value": "Theory + Duality Gap"
                    },
                    "comment": {
                        "value": ">**1:** *\"[1] introduced the ASSN algorithm, which only applies to fix-point problems of the form ... This imposes a restriction on the problem to solve, whereas in contrast, S5N has substantially broader use.\u201d*\n\nA) It seems to me that this statement is not true. In Sections 3.3-3.4, the authors are solving a monotone variation inequality. I haven\u2019t seen any usage of $T(z)$ there. Monotone variation inequality is a more general problem than a convex optimization problem. Hence, It seems fair to me to compare S5N with them. Moreover, Lemma 3.7 in [1] claims that the ASSN method is monotone in terms of distance to the solution, which is stronger than just asymptotic gradient convergence.\n\nB) Also, for Theorem 3, it seems that $V_k$ could be any bounded positive semi-definite matrix. I haven\u2019t found any usage of $V_k$ as a generalized Hessian. Could you provide lines in the proof of the global convergence where it is significant that $V_k$ is a generalized Hessian?\n\n>**2:** *\u201cAs for first-order methods, although they have non-asymptotic global convergence rates, they are mostly sub-linear, thus resulting in slower convergence speed as evidenced in Figures 1-3.\u201d*\n\nI kindly disagree with this statement. From the theoretical point of view, sublinear global convergence rates are still much better and faster than global asymptotical rates. For example, from $||g_k|| \\leq O(k^{-1/1000})$, one can conclude that $||g_k|| \\rightarrow 0$. But the rate $O(k^{-1/1000})$ is much slower than $O(k^{-1/2})$ for the first-order methods.\n\t\n>**3:** *\u201cMore importantly, the per-iteration cost of first-order methods is not necessarily smaller than that of Newton-type methods for sparse OT problems...\u201d*\n\nI was talking about the very classic first-order methods, like the Gradient Descent or Fast Gradient Method. For them, one has to calculate $g(\\alpha,\\beta)$ from Proposition 1. I believe for the S5N, one also has to calculate $g(\\alpha,\\beta)$, additionally, a sparse matrix should be constructed and CG should be performed. Hence, the iteration of S5N is not cheaper than classical first-order methods. Also, it has a worse theoretical convergence rate, which makes first-order methods dominant to S5N. \n\n>**4:** *\u201cWe would like to point out that this problem is piecewise quadratic, which has properties quite distinct from quadratic functions. In particular, its KKT condition is not a linear system, so CG or PCG is not applicable here.\u201d*\n\nCG could be applicable for any convex and smooth function. Moreover, it has local $n$-step local quadratic convergence (The end of Section 1.3.2, Page 49 from [2]). I believe in the case of piecewise quadratic function it should move away from linear areas as the GD method and has fast convergence in quadratic areas. \n\n**5. Duality Gap.** \nDo you have proof that Assumption 3 is satisfied for the problem (4)? Is the proposed method primal-dual (by solving the dual problem, the method solves the primal problem as well)? I believe for the experiments the duality gap metric is required as the main goal is to solve the primal problem, not only the dual problem.\n\n[1]  Xiantao Xiao, Yongfeng Li, Zaiwen Wen, and Liwei Zhang. A regularized semi-smooth Newton method with projection steps for composite convex programs. Journal of Scientific Computing, 76:364\u2013389, 2018.\n\n[2] Nesterov, Yurii. Lectures on convex optimization. Vol. 137. Berlin: Springer, 2018."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5242/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700166329233,
                "cdate": 1700166329233,
                "tmdate": 1700166329233,
                "mdate": 1700166329233,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UDApFvdpi8",
                "forum": "7W4rbphLht",
                "replyto": "Z40KmeH7wj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5242/Reviewer_tikC"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5242/Reviewer_tikC"
                ],
                "content": {
                    "title": {
                        "value": "Duality Gap 2"
                    },
                    "comment": {
                        "value": "**1A, 1B, 2, 4:** Thank you for the clarification and your comments. \n\n**3:** Thank you for the GD graphics. Finally, I found out why the time plots were so confusing to me. In Figure 1, the upper plot shows 200 iterations for every method. I assumed that the lower graphic shows the time of the same 200 total iterations for every method. That is how you can approximate time per iteration for every method. But now it seems that some methods have more iterations in the time graphic, which is confusing. So, maybe it would be better to show 200 iterations of every method for the time plots.\n\n**5:** When one wants to solve the primal problem $\\min f(x)$ by solving the dual problem $\\max g(y)$, there are some conditions to satisfy for the method. Unfortunately it is not enough to have $f(x^{\\ast})=g(y^{\\ast})$. The main issue is that for a convex problem, the convergence by gradient or function value does not guarantee the convergence by argument. Hence, it is possible that $||y_k-y^{\\ast}||$ is still big enough, hence $||x_k-x^{\\ast}||$ is big, and $f(x_k)-f^{\\ast}$ is also big, which means that it does not guarantee the convergence for the primal problem. To show such convergence, one has to show either the method converges by argument to $||x_k-x^{\\ast} || \\leq \\varepsilon$ or the method is primal-dual, which means the duality gap $f(x_k)-g(y_k)\\leq \\varepsilon$. In the experimental setup, it means that one either shows the graphic for $||y_k-y^{\\ast}||$, which is hard to do, or $f(x_k)-g(y_k)$, which is much easier and it shows that the method solves both primal and dual problems."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5242/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700237370367,
                "cdate": 1700237370367,
                "tmdate": 1700237370367,
                "mdate": 1700237370367,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]