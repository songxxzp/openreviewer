[
    {
        "title": "SemiReward: A General Reward Model for Semi-supervised Learning"
    },
    {
        "review": {
            "id": "XulcOMYW2P",
            "forum": "dnqPvUjyRI",
            "replyto": "dnqPvUjyRI",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4230/Reviewer_DH5L"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4230/Reviewer_DH5L"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the problem of semi-supervised learning, where the labeled data is limited and unlabeled data are massive during training. Previous pseudo labeling based methods are limited to pre-defined schemes or complex hand-crafted policies specially designed for classification. To address this, the authors present the Semi-supervised Reward framework (SemiReward), which assesses and chooses high-quality pseudo labels through predicted reward scores and is adaptable to mainstream SSL methods for both classification and regression tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper is well-written and easy to understand.\n2. The proposed integration of a reward model with an SSL model enhances pseudo label quality evaluation, with extensive experiments demonstrating its effectiveness across diverse tasks.\n3. The experimental protocol is detailed and conducive to reproduce the results."
                },
                "weaknesses": {
                    "value": "1. The method involves multiple models and training processes, adding complexity.\n2. The manuscript would benefit from pseudo codes for improved clarity.\n2. The design of reward model and hyper-parameter selection influence the outcomes, affecting the method's feasibility in practical scenarios."
                },
                "questions": {
                    "value": "See weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4230/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4230/Reviewer_DH5L",
                        "ICLR.cc/2024/Conference/Submission4230/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4230/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698645447050,
            "cdate": 1698645447050,
            "tmdate": 1700475538014,
            "mdate": 1700475538014,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5KSDW7MYIn",
                "forum": "dnqPvUjyRI",
                "replyto": "XulcOMYW2P",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4230/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4230/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer DH5L PART 1"
                    },
                    "comment": {
                        "value": "**W1&W3: The method's complexity, involving multiple models and training processes and the design of the reward model and hyper-parameter selection, raises concerns about the method's practical feasibility.**\n\n**R:** From the complexity perspective, our algorithmic flow might be complex at first glance, but it is actually quite clear, concise, and user-friendly. We discuss the complexity of SemiReward from five aspects (for **W1**). *As for the complexity of user usage*, the most essential component in the inference process is the Rewarder, as described in the pseudocode (provided in PART 3) and Figure 3. It is a pluggable module to assess the quality of pseudo labels, subsequently filtering them based on predicted reward scores, which can be treated as previous pseudo-label selection strategies and utilized in various pseudo-label methods. *As for the two-stage training process*, it can be regarded as another line of self-teaching process involving two models (the Rewarder and the Generator). The role of the Generator is similar to the teacher model in the self-training pipeline to auxiliarily optimize the Rewarder. And, these two stages are actually the same process but use different input datasets, i.e., the second stage further utilizes the selected reliable pseudo labels $\\hat{\\mathcal{D}}_{U}$ in additional to the first stage. *As for the complexity of the model itself*, our Rewarder is relatively lightweight, and its architecture is easy to explain (as discussed in Sec. 3.1). The relevant parameters and computational costs of our model are provided in Table 5 of the main text as follows:\n\n|     Model     |          Params. (M)          |            FLOPs (M)           |\n|:-------------:|:-----------------------------:|:------------------------------:|\n| Student Model |              21.7             |              607.9             |\n|    Rewarder   |             0.140             |              0.198             |\n|   Generator   |             0.137             |              0.139             |\n|   Proportion  |**1.28** |**0.056** |\n\n(view PART 2 to continue)"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4230/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700147259786,
                "cdate": 1700147259786,
                "tmdate": 1700160730510,
                "mdate": 1700160730510,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3YIihCez4L",
                "forum": "dnqPvUjyRI",
                "replyto": "XulcOMYW2P",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4230/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4230/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer DH5L PART 2"
                    },
                    "comment": {
                        "value": "Furthermore, *as for the practical usage of SemiReward*, the results in Table A9 of the appendix indicate that the inclusion of our model significantly enhances the overall training speed of the student model. Note that the numbers in parentheses represent the acceleration factor. This indirectly underscores the lightweight and fast-convergence nature of our model (for **W3**).\n\nTable: Training speedup times on nine SSL classification datasets with CV, NLP, and Audio modalities in various label settings.\n\n| Modality | Dataset (Setting) | Pseudo Label Base | Pseudo Label +SR | FlexMatch Base | FlexMatch +SR | Soft/FreeMatch Base | Soft/FreeMatch +SR | **Avg. Speedup** |\n| ---------| -------------------| :----------------: | :---------------: | :------------: | :------------: | :-----------------------: | :---------------------: | :---------------: |\n| Audio    | ESC-50 (250)        | 5.700              | 7.125 **(\u00d70.8)**  | 10.053         | 3.142 **(\u00d73.2)**  | 9.100                    | 7.583 **(\u00d71.2)**       | **(\u00d71.73)**       |\n|          | ESC-50 (500)        | 6.750              | 3.214 **(\u00d72.1)**  | 10.806         | 4.912 **(\u00d72.2)**  | 10.751                   | 5.658 **(\u00d71.9)**       | **(\u00d72.07)**       |\n|          | FSDnoisy18k (1773)  | 7.467              | 8.297 **(\u00d70.9)**  | 12.133         | 8.089 **(\u00d71.5)**  | 11.467                   | 7.645 **(\u00d71.5)**       | **(\u00d71.34)**       |\n|          | UrbanSound8k (100)  | 5.250              | 5.833 **(\u00d70.9)**  | 4.728          | 1.525 **(\u00d73.1)**  | 6.167                    | 5.606 **(\u00d71.1)**       | **(\u00d71.70)**       |\n|          | UrbanSound8k (400)  | 4.217              | 6.024 **(\u00d70.7)**  | 2.833          | 2.361 **(\u00d71.2)**  | 3.033                    | 2.757 **(\u00d71.1)**       | **(\u00d71.08)**       |\n| NLP      | AG News (40)        | 2.400              | 1.714 **(\u00d71.4)**  | 6.267          | 1.333 **(\u00d74.7)**  | 13.333                   | 6.060 **(\u00d72.2)**       | **(\u00d72.77)**       |\n|          | AG News (200)       | 2.889              | 1.699 **(\u00d71.7)**  | 3.556          | 1.693 **(\u00d72.1)**  | 4.444                    | 1.434 **(\u00d73.1)**       | **(\u00d72.30)**       |\n|          | Yahoo! Answer (500) | 0.178              | 0.445 **(\u00d70.4)**  | 8.711          | 5.807 **(\u00d71.5)**  | 9.000                    | 2.571 **(\u00d73.5)**       | **(\u00d71.80)**       |\n|          | Yahoo! Answer (2000)| 8.689              | 1.889 **(\u00d74.6)**  | 8.122          | 1.692 **(\u00d74.8)**  | 9.919                    | 8.266 **(\u00d71.2)**       | **(\u00d73.53)**       |\n|          | Yelp Review (250)   | 22.400             | 22.400 **(\u00d71.0)** | 20.066         | 20.066 **(\u00d71.0)** | 22.400                   | 10.667 **(\u00d72.1)**      | **(\u00d71.39)**       |\n|          | Yelp Review (1000)  | 1.822              | 4.673 **(\u00d70.4)**  | 21.411         | 16.470 **(\u00d71.3)** | 19.133                   | 16.394 **(\u00d71.2)**      | **(\u00d71.00)**       |\n| CV       | CIFAR-100 (200)     | 9.320              | 11.314 **(\u00d70.8)** | 54.280         | 49.345 **(\u00d71.1)** | 54.889                   | 49.899 **(\u00d71.1)**      | **(\u00d71.04)**       |\n|          | CIFAR-100 (400)     | 14.920             | 13.564 **(\u00d71.1)** | 100.240        | 45.564 **(\u00d72.2)** | 94.044                   | 67.174 **(\u00d71.4)**      | **(\u00d71.57)**       |\n|          | STL-10 (20)          | 0.528              | 1.320 **(\u00d70.4)**  | 11.760         | 8.400 **(\u00d71.4)** | 19.360                   | 15.600 **(\u00d71.3)**      | **(\u00d71.07)**       |\n|          | STL-10 (40)          | 0.268              | 0.693 **(\u00d70.4)**  | 9.556          | 7.351 **(\u00d71.3)** | 20.267                   | 13.889 **(\u00d71.5)**      | **(\u00d71.11)**       |\n|          | Euro-SAT (20)        | 1.196              | 5.980 **(\u00d70.2)**  | 14.320         | 17.900 **(\u00d70.8)** | 10.755                   | 5.121 **(\u00d72.1)**       | **(\u00d71.03)**       |\n|          | Euro-SAT (40)        | 1.092              | 5.460 **(\u00d70.2)**  | 21.040         | 23.378 **(\u00d70.9)** | 16.800                   | 7.304 **(\u00d72.3)**       | **(\u00d71.13)**       |\n\nLastly, *from the perspective of tunable hyperparameters*, our model actually incorporates fewer hyperparameters compared to current state-of-the-art methods such as FreeMatch and SoftMatch. We have fixed the network architectures of the Rewarders and Generator and only tuned three hyperparameters for the Rewarder training. Meanwhile, ablation and analysis experiments (e.g., Sec. 4.3 and Appendix B) have demonstrated the hyper-parameter robustness of our SemiReward, whereas manually designed methods often require extensive tuning for different task types. The comparison table of tunable hyperparameters is provided for reference (for **W3**):\n\n|  FlexMatch+SR  |  FreeMatch  |  SoftMatch  |\n| :-----: | :-----: | :-----: |\n| start_timing | label_hist | ema_p |\n| sr_lr | self-adaptive fairness (SAF) L_f | n_sigma |\n| N_k | w_u | per_class |\n| - | w_f | dist_align |\n| - | - | dist_uniform |\n\n(view PART 3 to continue)"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4230/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700147355125,
                "cdate": 1700147355125,
                "tmdate": 1700162504226,
                "mdate": 1700162504226,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "G8dNq5rt5Z",
                "forum": "dnqPvUjyRI",
                "replyto": "XulcOMYW2P",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4230/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4230/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer DH5L PART 3"
                    },
                    "comment": {
                        "value": "For instance, as demonstrated in the experiments outlined in Table A6 of the appendix, our model exhibits robustness within a certain range, as indicated by start_timing. Therefore, the user can apply the provided empirical hyperparameters to use SemiReward without hesitation (for **W3**).\n\n|      Start Timing     |      CV      |              |     Audio    |             |\n|:---------------------:|:------------:|:------------:|:------------:|:-----------:|\n|                       | Accuracy(\\%) |   iteration  | Accuracy(\\%) |  iteration  |\n|          0\\%          |     80.35    | 159743 iters |     62.86    | 96255 iters |\n|          5\\%          |     82.11    | 169984 iters |     65.59    | 65535 iters |\n| **10\\%** |   **83.35**  | **100352** iters |   **67.42**   | **38911** iters |\n|          15\\%         |     83.18    | 174080 iters |     67.10    | 69631 iters |\n\n**W2: Offer clarity-enhancing pseudo codes.**\n\n**R:** Thanks for your suggestions on improving the readability of our manuscript, and we provide the relevant pseudocode below. It has been incorporated into the appendix of the revision:\n\n```\n# SR_Train Stage 1 \niteration < T:\n\t# set SemiReward data loader\n\tfor x_l,y_l in loader:\n\t\tx_r,y_r,B_R = x_l,y_l,B_l\n  # load data in B_R size batch, x_r is labeled data and y_r is ground truth label\n\tfor x_r,y_r,B_R in loader:\n\t\tfeat(x_r) = f_s.feat(x_r) # get feature\n\t\ty_f,r = G(feat(x_r)),S(feat(x_r),y_r) # get fake label and reward\n\t\tS = cosine_similarity_n(y_r,y_f)) #get label similarity as targte\n\t# calculate loss\n\t\tL_R += MSE(r,S)\n\t\tL_G += MSE(r,1)\n\tL_aux = (L_R+L_G)/B_R\n\t# adam update\n  L_aux.backward\n\tupdate(G,R)\n# SR_Train Stage 2 \niteration >= T:\n\t# set SemiReward data loader\n\tfor x_u,x_l in loader:\n\t\tx_r = cat(x_u,x_l)\n\t\t# get pseudolabel y_p, Pseudolabel method can see in Pseudo Label algorithm (https://arxiv.org/abs/1908.02983)\n\t\ty_p = Pseudolabel(f_s(x_r))\n\t\tr = R(y_p,x_r) # calculate reward for each pseudolabel in N\n\t\ty_k = Topk(y_p,r) # select top k reward in N by reward\n\t\tB_R = (B_l+B_u)*k/N # get loader batch size B_R\n  # load data in B_R size batch, x_r is unlabeled data\n\tfor x_r,y_k,B_R in sr_dataloader: \n\t\tfeat(x_r) = f_s.feat(x_r) # get feature\n\t\ty_f,r = G(feat(x_r)),S(feat(x_r),y_k) # get fake label and reward\n\t\tS = cosine_similarity_n(y_k,y_f)) #get label similarity as target\n\t# calculate loss\n\t\tL_R,L_G = MSE(r,S),MSE(r,1)\n\tL_aux = (L_R+L_G)/B_R\n\t# adam update\n  L_aux.backward\n\tupdate(G,R)\n# SR_Inference\niteration > T:\n\tfor x_u,x_l,y_l in loader:\n\t\t# get pseudolabel y_p, Pseudolabel method can see in Pseudo Label algorithm (https://arxiv.org/abs/1908.02983)\n\t\ty_p = Pseudolabel(f_s(x_u))\n\t\tfeat(x_u) = f_s.feat(x_u) # get feature\n\t\tr = R(feat(x_u),y_p) # evaluate score\n\t\tmask_r = where(r>r.mean,1,0) # get threshold by T = r.mean\n\t\tL_u = CrossEntropy(y_p,f_s(x_u))*mask # filter label\n\t\tL_l = CrossEntropy(y_l,f_s(x_l))\n\t# calculate loss\n\tL =  L_u/B_U+L_l/B_L+L_aux # total loss\n\t# adamW update\n  L.backward\n\tupdate(f_s)\n```"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4230/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700147607202,
                "cdate": 1700147607202,
                "tmdate": 1700161253995,
                "mdate": 1700161253995,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WUis1NR9Mu",
                "forum": "dnqPvUjyRI",
                "replyto": "XulcOMYW2P",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4230/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4230/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer,\n\nWe appreciate your valuable feedback. In our response, we added pseudocode-related content (more specific has been updated in the manuscript). At the same time, we have explained through discussions from three angles that our Rewarder is actually not complicated and does not reflect additional hyperparameters, and is very easy to follow due to its end-to-end and plug-and-play features.\n\nAs you may know, unlike previous years, this year's discussion period only lasts until November 22nd, and we are getting closer to that deadline. We hope to discuss this with you in the meantime and will be happy to provide more information based on your feedback or further questions.\n\nIf you are satisfied with our response, please consider updating your score. If you need any clarification, please feel free to contact us."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4230/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700382123965,
                "cdate": 1700382123965,
                "tmdate": 1700382857436,
                "mdate": 1700382857436,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gwo9tU26Gm",
                "forum": "dnqPvUjyRI",
                "replyto": "WUis1NR9Mu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4230/Reviewer_DH5L"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4230/Reviewer_DH5L"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response. I have carefully reviewed the rebuttal provided, as well as the insights from other reviews. I\u2018ve decided to uphold my original score and raise my confidence from 4 to 5."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4230/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700475508750,
                "cdate": 1700475508750,
                "tmdate": 1700475508750,
                "mdate": 1700475508750,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nrFyOFdFhT",
                "forum": "dnqPvUjyRI",
                "replyto": "XulcOMYW2P",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4230/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4230/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your reply and efforts in going through our response! Since the rebuttal period is approaching the end, feel free to discuss them with us if you have any other questions. We are glad to further improve the quality of our manuscript. Thanks again for your help!\n\nBest regards,\n\nAuthors"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4230/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700477241278,
                "cdate": 1700477241278,
                "tmdate": 1700513945566,
                "mdate": 1700513945566,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "d9CREuLSJn",
            "forum": "dnqPvUjyRI",
            "replyto": "dnqPvUjyRI",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4230/Reviewer_uket"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4230/Reviewer_uket"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes Semi-supervised Reward (SemiReward), an add-on module that uses a reward network and a generator network in a two-stage training pipeline, to train a student model with labeled and unlabeled data. The rewarder network predicts scores to filter pseudo labels for the student training, and the generator network generates fake labels (to provide the rewarder with a variety of scenarios, including incorrect ones, so that it learns to distinguish good data from bad more effectively without affecting the training of the student network) that only train the rewarder. The paper proposes a two-stage training pipeline: 1) pre-training both the rewarder network and a generator network on a labeled dataset, 2)  the reward network is trained on a smaller, random subset of the labeled data along with selected unlabeled data. Empirical results on NLP, Vision and Audio datasets show the effectiveness of their framework."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+ This paper is well-written and easy to follow, the motivation and formulation of the paper are sound\n\n+ Figures and examples are informative and help understand the paper\n\n+ Numerous ablation studies and qualitative comparisons help in understanding the setting\n\n+ This paper shows strong empirical results that surpass current state-of-the-art methods on SSL\n\n+ This paper adds non-trivial contributions and builds on prior work to present a novel framework for SSL"
                },
                "weaknesses": {
                    "value": "- Missing experiments: recent works show the effectiveness of their proposed methods by varying the number of labeled samples, given the sensitivity of available data and their impact, it seems imperative to include such experiments to asses the proposed method.\n\n- Missing benchmarks: it is hard to asses the impact of the proposed framework without at least a large-scale experiment (e.g., ImageNet)\n\n- Some gains, especially in NLP and CV datasets seem marginal (e.g., TL +0.30)"
                },
                "questions": {
                    "value": "- The NLP experiments are performed using 1\\% labels, what is the labeled \\% used in the CV and Audio datasets?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4230/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4230/Reviewer_uket",
                        "ICLR.cc/2024/Conference/Submission4230/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4230/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698744833509,
            "cdate": 1698744833509,
            "tmdate": 1700508889026,
            "mdate": 1700508889026,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "lY2uwIfd6S",
                "forum": "dnqPvUjyRI",
                "replyto": "d9CREuLSJn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4230/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4230/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer uket"
                    },
                    "comment": {
                        "value": "**W1: Lack of experiments with samples of different labeling levels (missing experiments).**\n\n**R:** In several settings of CIFAR-100, we have augmented the relevant tasks. As illustrated in the table, the backbone for the student model is set as pretrained VIT:\n\n|  | CIFAR-100(1000) | CIFAR-100(2500) | CIFAR-100(10000) |\n| --- | --- | --- | --- |\n| FlexMatch | 11.19\u00b10.79 | 10.82\u00b11.90 | 10.22\u00b11.21 |\n| FlexMatch+**SR** | **9.94\u00b10.23** | **9.42\u00b10.66** | **8.99\u00b10.42** |\n| Avg. Gain| |  **1.29**   ||\n\n**W2: Lack of large-scale benchmarks like ImageNet (missing benchmarks).**\n\n**R:** Following FlexMatch and SoftMatch, we have provided results for ImageNet with 100 labels per class **in Table 3 of the main text**. Additionally, we have supplemented the data results for 1% and 10% labeled datasets (i.e., 13 and 128 labels per class) following CoMatch, as presented below. We find that applying the proposed SemiReward (+SR) upon FixMatch and CoMatch can achieve around 1.3% performance gains, and CoMatch+SR outperforms the existing SOTA method, SimMatch (CVPR\u20192022). Note that all these experiments train the student model from scratch.\n\n| Labels | FixMatch | FixMatch+**SR** | CoMatch | CoMatch+**SR** | SimMatch | Gain |\n| --- | :---: | :---: | :---: | :---: | :---: | :---: |\n| 1% | 53.5 | **55.1** | 66.0 | **67.4** | 67.2 | **1.5** |\n| 10% | 71.6 | **72.8** | 73.6 | **74.5** | 74.4 | **1.1** |\n\n**W3: Marginal gains in NLP and CV datasets.**\n\n**R:** Due to the relatively antiquated nature and the performance saturation on the STL-10 and AG News datasets, our approach did not achieve quite large gains on the average of multiple trials with different random seeds. But we can find out the fast-convergence and lightweight characteristics of our SemiReward. Consequently, we have supplemented our study with datasets from the CV and NLP domains with more difficult experimental settings (e.g., more class numbers and less labeled data) that exhibit remarkably superior performances.\n\n| NLP | PseudoLabel | PseudoLabel+**SR** | FlexMatch | FlexMatch+**SR** | SoftMatch | SoftMatch+**SR** | Gain |\n| --- | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| amazon_review_250 | 53.45\u00b11.90, | **49.13\u00b10.77** | 45.73\u00b11.60 | **43.08\u00b10.11** | 45.29\u00b10.95 | **42.98\u00b10.24** | **3.09** |\n| amazon_review_1000 | 47.00\u00b10.79 | **44.21\u00b10.64** | 42.25\u00b10.33 | **41.11\u00b10.89** | 42.21\u00b10.20 | **39.17\u00b10.32** | **2.32** |\n| **CV** |  |  | |  |  |  |  |\n| semi_aves_3959_0 | 40.35\u00b10.3 | **37.93\u00b10.45** | 32.48\u00b10.15 | **31.23\u00b10.09** | 32.85\u00b10.31 | **31.02\u00b10.15** | **1.82** |\n| tissuemnist_80 | 56.92\u00b14.54 | **53.06\u00b10.11** | 58.36\u00b13.8 | **54.27\u00b10.71** | 58.24\u00b13.08 | **53.52\u00b11.07** | **4.22** |\n\n**Q1: Lack of explanation of the percentage of labels in the experimental setup.**\n\n**R:** The configuration of settings is guided by the approach outlined in the USB paper. Specific details and settings information can be found in Appendix Table A1. For your convenience in reference, we present them below:\n\n| Domain |    Dataset    | \\#Label per class | \\#Training data | \\#Validation data | \\#Test data | \\#Class |\n|:------:|:-------------:|:-----------------:|:---------------:|:-----------------:|:-----------:|:-------:|\n|        |   CIFAR-100   |       2 / 4       |      50,000     |         -         |    10,000   |   100   |\n|   CV   |     STL-10    |       4 / 10      | 5,000 / 100,000 |         -         |    8,000    |    10   |\n|        |    EuroSat    |       2 / 4       |      16,200     |         -         |    5,400    |    10   |\n|        |    ImageNet   |        100        |     1,28,167    |         -         |    5,0000   |   1000  |\n|        |  Yelp Review  |      50 / 200     |     250,000     |       25,000      |    50,000   |    5    |\n|   NLP  |    AG News    |      10 / 50      |     100,000     |       10,000      |    7,600    |    4    |\n|        | Yahoo! Answer |      50 / 200     |     500,000     |       50,000      |    60,000   |    10   |\n|        |     ESC-50    |       5 / 10      |      1,200      |        400        |     400     |    50   |\n|  Audio |  UrbanSound8k |      10 / 40      |      7,079      |        816        |     837     |    10   |\n|        |  FSDnoisy18k  |       52-171      |  1,772 / 15,813 |         -         |     947     |    20   |"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4230/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700147059239,
                "cdate": 1700147059239,
                "tmdate": 1700162365732,
                "mdate": 1700162365732,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gSFZ2WS5fA",
                "forum": "dnqPvUjyRI",
                "replyto": "d9CREuLSJn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4230/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4230/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer,\n\nWe appreciate your valuable feedback. In our response, we supplemented the relevant experiments you mentioned and clarified the settings of the relevant data sets for our experiments. Several new sets of CV and NLP data sets illustrate the effectiveness of our gain. At the same time, the actual number of different labels has also been added. If you have more suggestions, please feel free to make them.\n\nAs you may know, unlike previous years, this year's discussion period only lasts until November 22nd, and we are getting closer to that deadline. We hope to discuss this with you in the meantime and will be happy to provide more information based on your feedback or further questions.\n\nIf you are satisfied with our response, please consider updating your score. If you need any clarification, please feel free to contact us."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4230/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700382095498,
                "cdate": 1700382095498,
                "tmdate": 1700382693478,
                "mdate": 1700382693478,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wr6r9PjhUL",
                "forum": "dnqPvUjyRI",
                "replyto": "gSFZ2WS5fA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4230/Reviewer_uket"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4230/Reviewer_uket"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your detailed response. I carefully read the rebuttal, and concerns were addressed and included in the main paper. Thus, I'm raising my confidence score."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4230/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700508918209,
                "cdate": 1700508918209,
                "tmdate": 1700508918209,
                "mdate": 1700508918209,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "elHL0mpBTu",
            "forum": "dnqPvUjyRI",
            "replyto": "dnqPvUjyRI",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4230/Reviewer_xKm4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4230/Reviewer_xKm4"
            ],
            "content": {
                "summary": {
                    "value": "This work addressed the major challenge in semi-supervised learning, i.e. evaluating the quality of pseudo labels in anomaly detection. A rewarder network is introduced to predict good quality pseudo labels by training on real and generated samples. The proposed reward network is demonstrated to improve multiple semi-supervised learning baselines."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Strength:\n\n1. Evaluating the quality of pseudo label is a real and major challenge for semi-supervised learning.\n\n2. The experiment evaluation is extensive covering audio, nlp and computer vision tasks. The proposed rewarder also consistently improves the performance of multiple state-of-the-art semi-supervised learning baselines."
                },
                "weaknesses": {
                    "value": "Weakness:\n\n1. It is not clear why this rewarder design could be effective. If such a rewarder function is effective can one infer the labels for unlabelled data by simply choosing the label that maximise the rewards score given an input sample? In addition, one may also infer the labels for testing data in the similar way. If this is true, does it mean one can use directly use the rewarder function for inference?\n\n2. According to Fig. 7, the semi-supervised training stage still requires a hard threshold to generate pseudo labels. The proposed method does not reduce the overall amount of hyperparameters to tune compared against existing SSL baselines.\n\n3. It is unclear the definition of fake label. Does it mean the fake label must deviate from the ground-truth label? But the objective of Eq (7) seems to encourage the fake label to be consistent with ground-truth label. There is not enough clarifications here.\n\n4. The current design seems to mix student model f_S with generator. Such a design may affect the training of student model."
                },
                "questions": {
                    "value": "A more clear analysis of why the rewarder can predict the label quality and why the rewarder is not used for inference are encouraged.\n\nMore discussions on hyper-parameters and the definitions of fake label are recommended."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4230/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4230/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4230/Reviewer_xKm4"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4230/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698821793153,
            "cdate": 1698821793153,
            "tmdate": 1700617051783,
            "mdate": 1700617051783,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6B9MQAC8uv",
                "forum": "dnqPvUjyRI",
                "replyto": "elHL0mpBTu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4230/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4230/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer xKm4 PART 1"
                    },
                    "comment": {
                        "value": "**W1:** **Lack of clarity on the validity of the rewarder design and inference strategy.**\n\n**R:** Thanks for your concerns, which are the most challenging problems we tackled with the Rewarder. As you have considered, directly predicting pseudo-labels with the algorithmic flow of the Rewarder will fall back to the basic pseudo-labeling algorithm, where the teacher model generates pseudo-labels and confident scores (can be regarded as the quality in Thresholding methods). It cannot provide a credible evaluation of generated pseudo-labels, which are coupled with the student training and affected by the confirmation bias. Therefore, our Rewarder serves as the pseudo-label selection method that evaluates the quality of the given pseudo-label of unlabeled data. As introduced in Sec. 3.1, our Rewarder is learned to compute the reward score, and we will select the relatively high-quality pseudo-labels with reward scores above the average in a mini-batch. We have shown the proposed reward score is calibrated for pseudo-label evaluation and is easy to fit by the lightweight Rewarder. Meanwhile, whether we can filter out high-quality depends on both the student & teacher models and the Rewarder. At the early stage of the student training, the generated pseudo-labels might not be reliable due to the weak student & teacher models. It will cost impractical computational overhead if you try to generate high-quality pseudo-labels by maximizing the reward score. When the student & teacher models are knowledgeable during the middle training period, the high-quality pseudo-labels will be discriminated by the Rewarder since it is online optimized. Moreover, you can refer to the General Response for the summary of our algorithm.\n\n**W2: Introduce new hyperparameters in rewarder filtering in semi-supervised training.**\n\n**R:** In the context of $\\tau$, it is important to note that it does not represent a new hard threshold hyperparameter; rather, it serves as a scoring threshold marker for the Rewarder. Our inference process entails scoring multiple candidate pseudo-labels for a given sample using the Rewarder and subsequently filtering them. This involves scoring pseudo-label candidates collectively, computing individual rewards for each, and then calculating the average reward score. The average reward score, denoted by the symbol $\\tau$, serves as the filtering threshold. During this process, pseudo-labels with scores below the average reward (i.e., the threshold $\\tau$) are excluded. As this threshold is the mean of a set of pseudo-label candidate lists, it remains controllable and adaptable to changes in the training process. This adaptability implies that we do not introduce additional hyperparameters through hard threshold methods."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4230/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700146662782,
                "cdate": 1700146662782,
                "tmdate": 1700172164083,
                "mdate": 1700172164083,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "T5h6MhUoKJ",
                "forum": "dnqPvUjyRI",
                "replyto": "elHL0mpBTu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4230/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4230/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer xKm4 PART 2"
                    },
                    "comment": {
                        "value": "**W3: Ambiguity in the definition of fake labels.**\n\n**R:** Throughout the training process, please refer to the general response. The primary function of the Generator is to generate a **fake label** distribution, aiding the Rewarder in learning high-quality mapping relationships. Importantly, the goal of generating **fake labels** is not to produce a set of high-quality pseudo-labels for the Rewarder to learn. Such a learning distribution tends to exhibit **confirmation bias**, which is _**why the pseudo-label results directly generated by the student model are not employed**_. The purpose of learning this mapping, which decouples the training processes of the student model and the Rewarder, is to mitigate the aforementioned this **confirmation bias**. In finer detail, by inputting a pair of **fake label** and sample, the Rewarder assigns a score, and the training objective involves calculating the cosine similarity between the **fake label** of a data sample and its ground-truth label. Throughout the entire training process, in the initial stage, the Rewarder learns from the sample and its corresponding **fake label** as input. The learning objective is the difference between the true label of the sample and the **fake label** (reduction is applied here), normalized using cosine similarity (as depicted in Figure 4 of the main text, where the calibration curve explains the rationale behind using cosine similarity). The Generator generates a **fake label** distribution with random quality. These **fake labels** are not incorporated into the training process of the student model but serve to assist in the training of the Rewarder. This approach enables the Rewarder to comprehensively explore various scenarios of the score mapping, enhancing its ability to evaluate pseudo-labels effectively.\n\n**W4: Potential negative impact of mixing current designs with Student model with generator.**\n\n**R:** The proposed training pipeline effectively severs the interdependence of the student and rewarder training processes, a measure taken to mitigate confirmation bias. Through the generator generating multiple sets of **fake labels** with varying qualities and computing their respective scores, the rewarder initializes its mapping from pseudo-labels and unlabeled data to reward scores. In the first stage, these candidate training samples aid the Rewarder in its initial convergence, as elaborated in **W3 response** and the **general response**. Moving to the second stage, the rewarder necessitates a more diverse set of samples for further optimization, having already undergone substantial training on labeled samples. Once again, we employ samples and their corresponding pseudo-labels as input. However, in this stage, the learning objective shifts to the disparity between the sample's high-quality pseudo-labels (utilized as true labels) and the pseudo-labels generated by the generator. This approach facilitates a more extensive exploration of additional unlabeled samples to refine the rewarder's optimization."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4230/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700172587072,
                "cdate": 1700172587072,
                "tmdate": 1700172587072,
                "mdate": 1700172587072,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ypEyh6GXES",
                "forum": "dnqPvUjyRI",
                "replyto": "elHL0mpBTu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4230/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4230/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer,\n\nWe appreciate your valuable feedback. In our response, we have tried our best to answer your questions about the Rewarder evaluation method and fake label. We have restated the problem through the general response and elaborated more specifically on the part where you gave us a weakness. If you have more questions, please inform us freely.\n\nAs you may know, unlike previous years, this year's discussion period only lasts until November 22nd, and we are getting closer to that deadline. We hope to discuss this with you in the meantime and will be happy to provide more information based on your feedback or further questions.\n\nIf you are satisfied with our response, please consider updating your score. If you need any clarification, please feel free to contact us."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4230/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700382076906,
                "cdate": 1700382076906,
                "tmdate": 1700382509625,
                "mdate": 1700382509625,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cPPcQWTktw",
                "forum": "dnqPvUjyRI",
                "replyto": "ypEyh6GXES",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4230/Reviewer_xKm4"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4230/Reviewer_xKm4"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks very much for the detailed clarifications. However, I still have the concern over the design of rewarder which takes image and label as inputs and outputs a normalized score. If such a rewarder can really quantify the quality of label, a simple way to infer the best label could be taking the gradient of $\\partial R(x^u,y^u)/\\partial y^u$ and use gradient ascent to estimate the $y^u$ that maximize $R(x^u,y^u)$. I do not see any high computation cost for such an inference. Basically, my major concern is, if the rewarder can well quantify the quality of pseudo label what prevents us from using the rewarder function to further refine the pseudo labels?"
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4230/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700588351150,
                "cdate": 1700588351150,
                "tmdate": 1700588351150,
                "mdate": 1700588351150,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VeZGzcc539",
                "forum": "dnqPvUjyRI",
                "replyto": "elHL0mpBTu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4230/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4230/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Concerns of the Rewarder Design"
                    },
                    "comment": {
                        "value": "Thanks for your insightful comment. We answer your concerns from two aspects, i.e., the necessity of decoupled two-stage training and the possibility of further optimizing pseudo labels with the Rewarder. We also added relevant discussion in Appendix B.7 of the latest revision for your reference.\n\n### **Why is the rewarder not trained together with the student model?**\nHere, the first type of training process is to optimize the student and the Rewarder together without the Generator, where the teacher model generates candidate pseudo labels for the student and the Rewarder. We call the first type coupled training and the proposed two-stage training decoupled training. The following table shows the categories of the input, output results, and functions of the relevant models. \nThere are two reasons for decoupling the training process of the student and the Rewarder. Firstly, the Rewarder requires diverse pseudo-labels as the training data to fit the ground truth reward scores rather than deterministic high-performance labels. Secondly, the student and the Rewarder might suffer from confirmation bias. Therefore, if we directly utilize pseudo labels generated by the teacher model as the couple training, the Rewarder might be unstable during the early training period and cannot learn more information in the second stage.\n\n| Model | Input | Output | Effects on Student Training | Effects on Rewarder Training |\n|---|:---:|:---:|:---:|:---:|\n| Student Model $f_S$ | $\\Omega(x^l)$ | Predicted label | - | - |\n| Teacher Model $f_T$ | $\\Omega(x^u)$ | Pseudo label | Generating candidate pseudo-labels | - |\n| Rewarder $\\mathcal{R}$ | $f(x^u),y^u$ | Reward score | Selecting reliable pseudo-labels | Training objective |\n| Generator $\\mathcal{G}$ | $f(x^u)$ | Fake label | - | Assist Rewarder training |\n\nWe conduct comparison experiments of the two training processes on CIFAR-100 (400 labels) based on FlexMatch in the following table (report top-1 accuracy). When using the coupled training of the student and the Rewarder, FlexMatch+SR yields worse performance than the baseline (82.12 vs. 82.20). Meanwhile, the proposed two-stage training decouples the student and the Rewarder by the Generator (aiming to maximize the reward score) and achieves a great trade-off between performance gains and speedup. As shown in Figure A2(a), in coupled training, the selected pseudo-labels are occasionally of slightly higher quality, but are unstable and affected by the student model, while decoupled training stably produces high-quality pseudo-labels.\n\n| Method | FlexMatch+SR |\n|---|:---:|\n| Coupled Training | 82.12 ($\\times 1.0$) |\n| +Gradient Ascent |  82.23 ($\\times 1.2$) |\n| Decoupled Training | 83.11 ($\\times$**2.2**) |\n| +Gradient Ascent | **83.25**($\\times 1.7$) |\n\n### **Why is SemiReward unable to maximize the reward score to generate better pseudo labels?**\nCurrently, the reward score output by the rewarder only evaluates the pseudo label. Thus, we can obtain the pseudo labels with a gradient ascent trick (GA) as you suggested: Given a mini-batch of selected pseudo labels, we modify them towards more high-quality pseudo labels (or fake labels) by maximizing the reward scores with a step of gradient ascent in the inference process of the Rewarder. In the coupled training process, the Rewarder can directly affect the pseudo labels generated by the teacher model. However, in the decoupled training, the pseudo label and reward score have no gradient backpropagation relationship with each other, while fake labels generated by the Generator are related to the reward scores. Therefore, we add these gradient-ascent enhanced fake labels to the selected pseudo labels for the student training.\n\nWe also implement the gradient ascent trick on CIFAR-100 in the table above. When using the coupled training, FlexMatch+SR (GA) can only obtain a limited performance gain and speedup over the baseline. Applying the gradient ascent to the decoupled training will yield a little performance gain with more extra computational costs and cause unstable training. As shown in Figure A2 (b), the quality of fake labels is relatively random and diverse, and it is difficult to steadily obtain high-quality labels through the gradient ascent and directly apply to the student training. Therefore, we chose the proposed decoupled training without GA as the final version.\n\n**In conclusion**, on the one hand, the first coupled training process is sub-optimal for SemiReward, and we avoid the existence of confirmation bias through decoupling method training. On the other hand, if we try to enhance the pseudo labels with the gradient ascent trick, we need a more complex loss design and violate the original intention of making a fast-convergence and plug-and-play method. The construction idea of SemiRewarder can be more widely used in other fields.\nThanks again for your efforts and valuable feedback! We are looking forward to your further response."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4230/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700616273895,
                "cdate": 1700616273895,
                "tmdate": 1700616907852,
                "mdate": 1700616907852,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UWwLeXTqcA",
                "forum": "dnqPvUjyRI",
                "replyto": "VeZGzcc539",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4230/Reviewer_xKm4"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4230/Reviewer_xKm4"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the additional explanations and evaluations. I think the overall idea is novel despite some designs deserve more investigations. I would like to raise my score."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4230/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700617031926,
                "cdate": 1700617031926,
                "tmdate": 1700617031926,
                "mdate": 1700617031926,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]