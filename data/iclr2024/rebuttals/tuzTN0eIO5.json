[
    {
        "title": "Zero Bubble Pipeline Parallelism"
    },
    {
        "review": {
            "id": "9j2AzBWAmS",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7283/Reviewer_xbKE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7283/Reviewer_xbKE"
            ],
            "forum": "tuzTN0eIO5",
            "replyto": "tuzTN0eIO5",
            "content": {
                "summary": {
                    "value": "* The author proposed a new schedule in Pipeline Parallelism (PP) to reduce pipeline bubbles to close to zero\n* The key idea is instead of the 1F1B schedule of earlier work, splitting B to two stages (aka activation gradient and weight gradient) and a schedule like 1F1B1W can further reduce bubbles of 1F1B\n* The intuition behind it is only Forward calcluation and activation gradient calculation have a pipeline/stage dependency, while weight gradient calculation are not, so activation gradient calcluation should be eagerly scheduled while weight gradient calculation can be rescheduled to fill/balance pipeline bubbles\n* The authors have proposed two schedule algorithms to adapt for real-world workloads when F/B/W can have different runtime."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The author made a key observation that the backprop stage in PP can be split into two parts and scheduled in a finer granularity, i.e., `any pipeline (or producer/consumer) independent workload can be rescheduled to balance/fill pipeline bubbles`, which is not only benifitical to train NN with PP, but should hold generally true for any PP workloads\n\n* The author didn't limit themselves to just a simple TF = TB  = TW assumption but also proposed two optimization implementations to optimize the schedule for potentially unbalanced TF/TB/TW\n\n* The authors have conducted representative experiements and showed that their methods can outperform earlier methods by up to 20% and more importantly close to the upperbound (zero bubble) of PP\n\n* The presentation/feagures are easy to follow and illustrative"
                },
                "weaknesses": {
                    "value": "* While I agree the authors have discovered a valuable new angle optimizing PP and indeed bring the bubble rate close to zero, I think it is still a bit overselling to claim the method achives zero bubble PP, given:\n  1. The method still require a large enough number of microbatches, e.g., m >= 2p to achive an amortized zero bubble PP, how does it perform under m < p is rarely discussed. Experiments (at least a roofline analysis) with m < p can make the methods more sound\n  2. There are other methods that can achieve zero bubble PP like PipeDream and PipeMare. These are more strictly flushless (thus bubble-free) methods though they have altered the math of backprop and poses potential accuracy loss or incurs more weight memory. So it is a bit overclaim that this work is the first to achieve pippeline bubbles. Discussions on these previous related works are also encouraged.\n  3. Therefore \"near zero bubble\" is probably more accurate IMHO, emphasis on \"zero bubble\" actually overshadows the solid intuition mentioned as 1st bullet point in Strengths\n\n* Even though authors proposed two algos to schedule unbalanced TF/TB/TW, the experimetns however are not designed to show how their advantages, e.g., with sentence-size = 1024, TF/TB/TW are almost equal (24h >> 8s in table 1) for all model sizes, experiments or roofline analysis with 8s >> 24h (or any models with sifinicantly different T) could be helpful to evaluate the efficacy of these algos"
                },
                "questions": {
                    "value": "1. when authors talk about peak memory I assume they refer to max of peak memory of each worker, rather than the entire pipeline, if it is the latter case even ZB-H1 should have a higher peak memory than 1F1B based on figure 3, can you make this more explicit in the texts?\n\n2. what's a \"local search\" in 3.1?\n\n3. what's the runtime of ILP, how much does it improve over HEURISTIC? can you do an ablation with HEURISTIC algo only?\n\n4. My understad is section 4 BYPASSING OPTIMIZER SYNCHRONIZATIONS only works for INF/NAN, how does it work for grad-norm when each stage need the global reduced norm? If the optimizer depends on partially reduced state how can it provide same numerical result of baselines like 1F1B?\n\n5. can you try to recompute the activation gradient (not activation), e.g., redo B during W to save some peak memory in H2/2P cases?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7283/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7283/Reviewer_xbKE",
                        "ICLR.cc/2024/Conference/Submission7283/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7283/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697336029473,
            "cdate": 1697336029473,
            "tmdate": 1700276414007,
            "mdate": 1700276414007,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ctHtsLGOf9",
                "forum": "tuzTN0eIO5",
                "replyto": "9j2AzBWAmS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7283/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7283/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the valuable feedback and suggestions for improvement. We respond to individual points from your review below.\n\n> **Weaknesses:\nQ: The method still require a large enough number of microbatches, e.g., m >= 2p to achive an amortized zero bubble PP, how does it perform under m < p is rarely discussed. Experiments (at least a roofline analysis) with m < p can make the methods more sound**\n\nBy the nature of PP, $m < p$ would cause extremely large bubble rate ($\\ge 50%$). Previous PP strategies reduce bubble rate by increasing m, for example, in Megatron-LM, all configurations have $m \\ge 8p$, while still having a notable bubble rate. Therefore we consider $m < p$ as a very uncommon setting for PP. We want to highlight that compared to previous methods, ZB-2p requires a much smaller number of microbatches to achieve \u201cnear zero bubble\u201d (typically $m=3p$ is enough).\n\n> **Weakness:\nQ: There are other methods that can achieve zero bubble PP like PipeDream and PipeMare. These are more strictly flushless (thus bubble-free) methods though they have altered the math of backprop and poses potential accuracy loss or incurs more weight memory. So it is a bit overclaim that this work is the first to achieve pippeline bubbles. Discussions on these previous related works are also encouraged.**\n\nWe were aware that asynchronous PP achieves zero bubbles, and intended to briefly touch this point and state that ours is the first to achieve zero bubble under synchronous setting. Somehow we forgot to do so, thanks for pointing this out, we added this discussion in the updated version.\n\n> **Weakness:\nQ: Therefore \"near zero bubble\" is probably more accurate IMHO, emphasis on \"zero bubble\" actually overshadows the solid intuition mentioned as 1st bullet point in Strengths**\n\nTo avoid over-claiming, we changed the title to \u201cNear Zero Bubble Pipeline Parallelism\u201d in our updated pdf.\n\n> **Weakness:\nQ: Even though authors proposed two algos to schedule unbalanced TF/TB/TW, the experimetns however are not designed to show how their advantages, e.g., with sentence-size = 1024, TF/TB/TW are almost equal (24h >> 8s in table 1) for all model sizes, experiments or roofline analysis with 8s >> 24h (or any models with sifinicantly different T) could be helpful to evaluate the efficacy of these algos**\n \n- We show our profiled value of $T_F/T_B/T_W$ across different settings in Appendix D. From the table, we can see $T_F=T_B$, but $T_W$ is significantly different, with $T_W/T_B$ varies from 0.516 to 0.756.\n- Our algos don\u2019t rely on any assumption on the value of $T_F/T_B/T_W$. The reason why the heuristic works well is because of the flexibility of W and the pattern of \u201c1F-1B-1W\u201d.\n- Actually by dividing W into a finer granularity, e.g. each single weight gradient calculation, it\u2019s easy and straightforward to fill the bubble because the FLOPs of each weight gradient calculation $\\ll T_F/T_B$. In practice, we just found that scheduling based on grouping W is already good enough. So we didn\u2019t bring this complexity."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7283/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699958174163,
                "cdate": 1699958174163,
                "tmdate": 1699958174163,
                "mdate": 1699958174163,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kBS0HWP1SP",
                "forum": "tuzTN0eIO5",
                "replyto": "9j2AzBWAmS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7283/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7283/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Additional info for:\n> **Weaknesses:\nQ: The method still require a large enough number of microbatches, e.g., m >= 2p to achive an amortized zero bubble PP, how does it perform under m < p is rarely discussed. Experiments (at least a roofline analysis) with m < p can make the methods more sound**\n\nWe compared ZB methods when $m\\le p$ and added the result to appendix. Roughly the run time of 1F1B and ZB methods when $m\\le p$ are $(m+p-1)(T_F+T_B+T_W)$  vs  $(m+p-1)(T_F+T_B)+T_W$. The experiment also shows that we can still get approximately 20%~30% improvement in these settings."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7283/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700106328231,
                "cdate": 1700106328231,
                "tmdate": 1700106328231,
                "mdate": 1700106328231,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aY3XYXDVrJ",
                "forum": "tuzTN0eIO5",
                "replyto": "KgvAlzaRBx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7283/Reviewer_xbKE"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7283/Reviewer_xbKE"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for all the insightful answers. Regarding Q4, if the model just want to do a grad-norm (e.g., torch.nn.utils.clip_grad_norm_) instead of a conditional grad-clipping, would it still work? since you can't perform a grad-norm clipping (rather than grad clipping) until you get an all-reduced global grad-norm?"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7283/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700276247318,
                "cdate": 1700276247318,
                "tmdate": 1700276247318,
                "mdate": 1700276247318,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4N8kE5yns4",
                "forum": "tuzTN0eIO5",
                "replyto": "ctHtsLGOf9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7283/Reviewer_xbKE"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7283/Reviewer_xbKE"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the edits and additional experiments. I am happy to revise my score to 8 and look forward to the acceptance of your work."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7283/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700276381520,
                "cdate": 1700276381520,
                "tmdate": 1700276381520,
                "mdate": 1700276381520,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Z4XAaJhEsM",
                "forum": "tuzTN0eIO5",
                "replyto": "UuJ0CaCeaY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7283/Reviewer_xbKE"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7283/Reviewer_xbKE"
                ],
                "content": {
                    "comment": {
                        "value": "Okay, somehow I forgot when clip_coef < 1, grad is not scaled. Good point, thanks."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7283/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700333369982,
                "cdate": 1700333369982,
                "tmdate": 1700333369982,
                "mdate": 1700333369982,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "X16nB2rALc",
            "forum": "tuzTN0eIO5",
            "replyto": "tuzTN0eIO5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7283/Reviewer_b5Up"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7283/Reviewer_b5Up"
            ],
            "content": {
                "summary": {
                    "value": "Pipeline parallelism is a widely used technique in distributed training. However, the efficiency of pipeline parallelism suffers from pipeline bubbles. In view of this, this paper designs a new strategy to reduce the bubble rates. The idea behind the new pipeline strategy is to split the backward computation into two parts, which could reduce the bubble in the 1F1B strategy."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The problem studied in this paper is very important. With the increase of the model parameters, pipeline parallelism is a popular strategy for training large models. However, the bubble in the 1F1B strategy affects the efficiency of the pipeline strategy.\n2. The idea behind this paper is very easy. It is a good signal, I think the paper develops an easy solution with great performance.\n3. This paper contains a mathematical analysis."
                },
                "weaknesses": {
                    "value": "1. The authors miss an important point in the experiment section.\n2. The writing of this paper can be slightly improved.\n3. The performance of the method developed in this paper is limited."
                },
                "questions": {
                    "value": "The problem studied in this paper is significant in distributed training and the authors give an excellent solution to improve the existing pipeline strategy. \n\nHowever, the main issue of this paper is that the solution mentioned in this paper is likely to increase the memory usage of pipeline parallelism training. Moreover, the experimental results introduce bubble rate which is not important in the pipeline parallelism training. It is known to all that the efficiency of pipeline parallelism suffers from pipeline bubbles. But the bubble rate is not important. We only care about the throughput and memory usage of the pipeline parallelism strategy. Since the pipeline strategy developed in this paper will increase the memory usage of pipeline parallelism training, large memory usage is likely to decrease the size of the micro-batch. A small micro batch will decrease the efficiency of the pipeline parallelism strategy. The experimental results show the developed algorithm has limited improvement.\n\nThe writing of this paper could be improved. For example, we can use different colors to represent B in Figure 3 and Backward in Figure 2. Moreover, the title of this paper is too large. I do not think the method in this paper is zero bubble."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7283/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7283/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7283/Reviewer_b5Up"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7283/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698196391292,
            "cdate": 1698196391292,
            "tmdate": 1700635617682,
            "mdate": 1700635617682,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rC0HSTzl6Z",
                "forum": "tuzTN0eIO5",
                "replyto": "X16nB2rALc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7283/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7283/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the valuable feedback. We respond to individual points from your review below.\n\n> **Q1 & Q2. \u2026 likely to increase the memory usage of pipeline parallelism training, \u2026 care about memory usage**\n\nWe want to highlight the automatic scheduling algorithm which provides the flexibility for users to trade-off between memory usage and bubble rate by themselves. Both ZB-1p and ZB-2p are special cases generated from this algorithm with different memory limits. Although ZB-2p indeed needs a doubled memory, ZB-1p still significantly outperforms 1F1B under a similar memory constraint.\n\n\n\n> **Q2. bubble rate is not important. We only care about the throughput and memory usage.**\n\nAs throughput $\\propto$ (1 - bubble rate) * (kernel efficiency), where bubble rate measures how good the pipeline schedule is, and kernel efficiency measures how a single op is utilizing the GPU. We report bubble rate separately to reflect how efficient our pipeline schedule is, as this is the main contribution of this work. The comparison is under the same batch size and thus the same kernel efficiency.\n\nWe also presented throughput in Table 3.\n\n> **Q3. A small micro batch will decrease the efficiency**\n\nWe double the size of each microbatch for 1F1B and compare its throughput with ZB-2p. The experimental results show that ZB-2p also holds a higher throughput than 1F1B by about 14% on average (min 4%, max 22%), even with a half size for each microbatch. Empirically, a larger batch size increases the utilization rate of the GPU and thus improves the efficiency. However, it is less of a concern for large models because the hidden dimension is big enough to saturate device utilization. Based on this consideration and our experimental results, we believe ZB-2p is preferred over increasing the batch size for 1F1B in LLM training. For more details of the experiments, please see the Appendix F in our updated pdf.\n\n> **Q4. The experimental results show the developed algorithm has limited improvement.**\n\nDespite the memory concern, in an apple to apple comparison where our memory usage is similar to the baselines, our method ZB-1p improves the throughput by 9.5% on average (min 4%, max 15%, row 2 and 3 in table 4). With larger models (14.6B & 28.3B), the average throughput improvement is 11.7%. We think these numbers are already significant considering the cost of GPU. Furthermore, when memory allows, our method pushes the throughput almost to the theoretical upper limit (red bar in figure 5 and bubble rates of ZB-2p in Table 5). \n\nAnother advantage of ZB-2p is that it can achieve near zero bubble with a smaller number of micro-batch (typically m=3p is enough), for 1F1B in Megatron the setting is m>=8p yet still has notable bubbles. This means more micro-batch can be partitioned over Data Parallelism (DP) dimension which brings a better scalability for the training of LLM. Given a fixed PP size and a fixed global batch size, we can reduce #micro-batch of PP to a relatively small number (e.g. 3p) and increase the size of DP, thus shortening the training time by using more devices. When integrated with ZeRO, we can further partition the parameter memory (including optimizer states) to compensate for the extra activation memory introduced by ZB-2p.\n\n\n> **Q5. use different colors to represent B in Figure 3 and Backward in Figure 2.**\n\nWe adjusted accordingly in the updated PDF.\n\n> **Q6. title of this paper is too large**\n\nIt is true that the bubble rate can never go to zero because practically there\u2019s always a difference in computation time of F/B/W, introducing gaps in between. As Reviewer xbKE also suggested \u201cNear Zero Bubble\u201d could be more accurate and also well supported by our experimental evidence, we updated the title accordingly in the PDF."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7283/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699957731296,
                "cdate": 1699957731296,
                "tmdate": 1699957731296,
                "mdate": 1699957731296,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QUDt3kXxnf",
                "forum": "tuzTN0eIO5",
                "replyto": "rC0HSTzl6Z",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7283/Reviewer_b5Up"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7283/Reviewer_b5Up"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your reply! I am happy to raise my score to 6."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7283/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700635602798,
                "cdate": 1700635602798,
                "tmdate": 1700635602798,
                "mdate": 1700635602798,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "OVttDPdQNE",
            "forum": "tuzTN0eIO5",
            "replyto": "tuzTN0eIO5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7283/Reviewer_Uec3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7283/Reviewer_Uec3"
            ],
            "content": {
                "summary": {
                    "value": "The manuscript titled \"ZERO BUBBLE PIPELINE PARALLELISM\" presents a novel strategy for achieving pipeline parallelism with the primary goal of eliminating pipeline bubbles. This approach includes the development of an automatic scheduling algorithm, which is then compared with established methods like 1F1B and interleaved 1F1B. The key contributions of the paper are as follows:\n\n1. It introduces a pipeline parallelism strategy designed to eliminate pipeline bubbles, assuming that forward, backward, and weight gradient calculations all take the same amount of time.\n\n2. It presents an automatic scheduling algorithm that consistently outperforms traditional methods, with the highlight being the achievement of a zero bubble rate, indicating optimal computational resource utilization. However, this comes at the cost of nearly doubling memory usage.\n\n3. The paper delves into the relationship between memory constraints and bubble rates, providing insights into the trade-offs between memory requirements and scheduling efficiency."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper introduces a novel strategy for pipeline parallelism aimed at completely eliminating pipeline bubbles. Leveraging this strategy, an automatic scheduling algorithm is developed and benchmarked against existing approaches, such as 1F1B and interleaved 1F1B. \n\noriginality: fair. The paper separated the backward loss and weight gradient calculation, and achieved zero bubble pipeline parallelism under the assumption that forward, backward loss, and weight gradient calculation time are identical.\nquality: good. The paper has theoretical calculation and also experimental data to support the zero bubble pipeline parallelism algorithm proposed in this paper.\nclarity: good. \nsignificance: medium. The algorithm can achieve zero bubble pipeline parallelism but with memory usage nearly doubled. This will limit the application of this method for larger LLM given larger models will use larger memory."
                },
                "weaknesses": {
                    "value": "The algorithm can achieve zero bubble pipeline parallelism but with memory usage nearly doubled. This will limit the application of this method for larger LLM given larger models will use larger memory."
                },
                "questions": {
                    "value": "For Figure 5, sometimes ZB-1p is better than 1F1B-I, and sometime not. Can you explain the reason in the paper?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7283/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698730996549,
            "cdate": 1698730996549,
            "tmdate": 1699636869803,
            "mdate": 1699636869803,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6fbo1yDioY",
                "forum": "tuzTN0eIO5",
                "replyto": "OVttDPdQNE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7283/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7283/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the valuable feedback. We respond to individual points from your review below.\n\n> **Weakness: memory usage limit application to LLM**\n\nFor the concern of application to LLM, we want to highlight the following points.\n* We want to highlight the automatic scheduling algorithm which provides the flexibility for users to trade-off between memory usage and bubble rate by themselves. Both ZB-1p and ZB-2p are special cases generated from this algorithm with different memory limits. Although ZB-2p indeed needs a doubled memory, ZB-1p still significantly outperforms 1F1B under a similar memory constraint.\n* We double the size of each microbatch for 1F1B and compare its throughput with ZB-2p. The experimental results show that ZB-2p also holds a better performance even with a half size for each microbatch. Empirically, a larger batch size increases the utilization rate of the GPU and thus improves the efficiency. However, it is less of a concern for large models because the hidden dimension is big enough to saturate device utilization. Based on this consideration and our experimental results, we believe ZB-2p is preferred over increasing the batch size for 1F1B in LLM training. For more details of the experiments, please see the Appendix F in our updated pdf.\n* Another advantage of ZB-2p is that it can achieve near zero bubble with a smaller number of micro-batch (typically m=3p is enough), for 1F1B in Megatron the setting is m>=8p yet still has notable bubbles. This means in ZB-2p, more micro-batch can be partitioned over Data Parallelism (DP) dimension which brings a better scalability for the training of LLM. Given a fixed PP size and a fixed global batch size, we can reduce #micro-batch of PP to a relatively small number (e.g. 3p) and increase the size of DP, thus shortening the training time by using more devices. When integrated with ZeRO, we can further partition the parameter memory (including optimizer states) to compensate for the extra activation memory introduced by ZB-2p.\n\n\n> **Question: For Figure 5, sometimes ZB-1p is better than 1F1B-I, and sometime not. Can you explain the reason in the paper?**\n\nFrom Table 5, we can find that 1F1B-I has lower bubble rates for models of 1.5B, 6.2B and 28.3B, and a bit higher bubble rate for the 28.3B model. However, 1F1B-I is generally under-performed than theoretical efficiency in our experiments, especially in multi-node setups. We guess the reason is that 1F1B-I has more communication and computation passes than both 1F1B and ZB-1p, so it is more likely to be affected by the fluctuations in the time cost of each pass. For multi-node setups, the time cost of inter-node communication is usually larger than intra-node communication, which also decreases the implemental efficiency of 1F1B-I because we don\u2019t take this difference into account."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7283/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699957684435,
                "cdate": 1699957684435,
                "tmdate": 1699957684435,
                "mdate": 1699957684435,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ORu62cRJu6",
            "forum": "tuzTN0eIO5",
            "replyto": "tuzTN0eIO5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7283/Reviewer_hNFq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7283/Reviewer_hNFq"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a novel approach, called ZeroBubble, for eliminating bubbles in pipeline parallelism in order to training efficiency. There are two key aspects to the proposal: (1) splitting the backward pass into the B and W components to increase scheduling flexibility, and (2) eliminating synchronization of optimizer step with a rollback mechanism to preserve semantics. The paper also presents various analysis to illustrate the memory consumption effects of different ZeroBubble strategies. Overall, the evaluation results show that ZeroBubble provides up 30% throughput improvement over 1F1B schedules for Megatron GPT models."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "I think splitting the backward pass into B (activation gradient computation) and W (weight gradient computation) as a way of improving scheduling flexibility is a nice touch of creativity. \n\nThe paper includes ample analytical details that foster intuition and understanding of the relevant memory and TFLOPs consideration of pipeline parallelism schedules. \n\nThe authors did a really great job of writing and organizing the paper."
                },
                "weaknesses": {
                    "value": "My main concern relates to breaking the synchronization of optimizer step because it complicates the synchronous training semantics, and adoption requires close interaction of rollback and model checkpointing.  Moreover, I think there are few design alternatives that the paper failed to explore or discuss. \n1. Evaluating the performance of synchronous optimizer step as way of understanding the trade-off between the simplicity and throughput. \n2. Scheduling W before B, which is less memory-efficient but allows staggering the optimizer step of the stages while preserving the synchronous semantics. \n\nAnother concern, or perhaps a question is whether the more memory efficient schemes (1F1B* and ZB-1p) be evaluated with larger micro-batch sizes than ZB-2p. This would help to confirm that the evaluation is not biased towards ZB-2p."
                },
                "questions": {
                    "value": "See weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7283/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698849835935,
            "cdate": 1698849835935,
            "tmdate": 1699636869685,
            "mdate": 1699636869685,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "faC2sYlbeE",
                "forum": "tuzTN0eIO5",
                "replyto": "ORu62cRJu6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7283/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7283/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the positive feedback and suggestions for improvement. We respond to individual points from your review below.\n\n>**adoption requires close interaction of rollback and model checkpointing**\n\nWe guess the \u201cmodel checkpointing\u201d you mentioned here means saving previous model parameters/optimizer states and restoring them when rollback.\n- For efficiency, we recommend the in-place optimizer rollback proposed in Appendix C, which does not require checkpointing or extra copy of parameters and should be more efficient.\n- For simplicity, from our study, gradient clipping and NAN are mainly activated during the initial stage of training, which we believe is more a choice for training stability than model performance. Therefore, instead of post validation, there\u2019re indeed other simpler choices that may achieve the same. For example, clipping the gradients locally. Since in this work we would like to focus on the systems aspect, we did not explore the numerical/algorithmic choices, but just align it with our baselines.\n\n> **Weakness 1:\n> Evaluating the performance of synchronous optimizer step as way of understanding the trade-off between the simplicity and > throughput.**\n\nThanks for the valuable suggestion. We add some new experiments in Appendix E, which shows that the synchronous optimizer under-performs our post-validation strategy by about 8.8% on average.\n\n> **Weakness 2:\n> Scheduling W before B, which is less memory-efficient but allows staggering the optimizer step of the stages while preserving the synchronous semantics.**\n\nThanks for the suggestion, we interpret your point as that if W goes earlier than B, then the optimizer step can start earlier and overlap with B, is it correct? However, for a neural network W in layer i is dependent on B in layer i+1, this dependency prevents us from shifting W to the front enough to achieve the described benefit.\n\n\n> **Another concern, or perhaps a question is whether the more memory efficient schemes (1F1B\\* and ZB-1p) be evaluated with larger micro-batch sizes than ZB-2p. This would help to confirm that the evaluation is not biased towards ZB-2p.**\n\nWe add experiments comparing ZB-2p and 1F1B (with double micro-batch size) under the same memory consumption. The results show that ZB-2p also has a higher throughput than 1F1B by about 14% on average (min 4%, max 22%). Please find related data in Appendix F in the updated manuscript."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7283/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699957598043,
                "cdate": 1699957598043,
                "tmdate": 1699957598043,
                "mdate": 1699957598043,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]