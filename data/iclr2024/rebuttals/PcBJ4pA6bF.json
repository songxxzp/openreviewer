[
    {
        "title": "Overcoming Data and Model heterogeneities in Decentralized Federated Learning via Synthetic Anchors"
    },
    {
        "review": {
            "id": "kHapeueync",
            "forum": "PcBJ4pA6bF",
            "replyto": "PcBJ4pA6bF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission545/Reviewer_phME"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission545/Reviewer_phME"
            ],
            "content": {
                "summary": {
                    "value": "This paper studied the decentralized federated learning with both data and model heterogeneity. To solve this problem, the authors introduced a novel DESA method, which generated global synthetic anchors to guide the local model training. For each client, in addition to standard supervised classification loss, it would also consider the classification loss over synthetic anchors and cross-client knowledge distillation losses for improving the model's generalization performance. Experimental results validated the effectiveness of DESA over baselines with respect to both inter- and intra-client prediction performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "**(1) Originality:** This paper handled both data and model heterogeneity in decentralized federated learning problems without public data. Technically, it proposed to generate synthetic anchor data from each client. Besides, supervised contrastive loss between client data and synthetic data was introduced to mitigate the data heterogeneity among clients. The knowledge distillation loss over synthetic data was designed to mitigate the model heterogeneity among clients. The generalization performance of the proposed DESA method was theoretically analyzed.\n\n**(2) Quality:** DESA used synthetic anchors to solve the issues of decentralized federated learning without public data. It further leveraged contrastive loss and knowledge distillation loss to handle data and model heterogeneity. The hyperparameter analysis in the experiments also validated the impact of those two losses on the proposed DESA method.\n\n**(3) Clarity:** Overall, the presentation of this paper is easy to follow. This paper illustrated the three crucial components of DESA in different subsections. The effectiveness of DESA was evaluated on a variety of benchmarks, including both heterogeneous and homogeneous model settings.\n\n**(4) Significance:** The studied decentralized federated learning is practical in real-world applications, especially when no public data is available among local clients. Thus, the developed method without leveraging public data in this paper can be applied to more general FL problems compared to previous works relying on public data."
                },
                "weaknesses": {
                    "value": "The weaknesses of this paper are summarized below.\n\n(1) The research question is not well motivated. This paper studied decentralized federated learning regarding the performance of every client model on other clients. Traditional FL settings might focus only on the performance of every client model on its own client domain. Thus, it might be more convincing to provide some practical examples to illustrate why inter-client test accuracy should be emphasized in real FL scenarios.\n\n(2) The introduction shows that the proposed approach aims to generate minimal synthetic anchor data to enhance client-model generalization. However, this \"minimal\" property of generated data is not discussed in the experiments. The ablation study in subsection 5.4 shows that the size of synthetic data can significantly affect the inter-accuracy. Thus, there might exist a trade-off between the model performance and the size of synthetic data. More explanations can be provided here.\n\n(3)  Subsection 3.1 shows that synthetic anchor data is shared amongst the client\u2019s neighbors. However, it is unclear how the neighbor information is defined in the experiments. In addition, it seems that the synthetic anchor data $D^{Syn}$ in Subsection 3.2 simply combines all the anchors $D^{Syn}_i$ within each client.\n\n(4) The definition of distribution matching in Eq. (3) is confusing. First, it is unclear why this term can guarantee the class-imbalanced anchors. How is the function $\\psi^{rand}(x|y)$ affected by the class labels? Second, it is defined over all clients $i=1,\\cdots, N$. Then why does the generation of anchors within client $i$ rely on the data on other clients? Third, it is not explained whether the minimization of MMD between true data and anchor data would increase the risk of privacy leakage. That is, when anchor data becomes more similar to the true data, it is more likely to include the private domain information.\n\n(5) In the derived generalization in Theorem 1, it assumes (i)  real labeling and synthetic data labeling are similar, and (ii) real labeling and distillation data labeling are also similar. It is confusing how both assumptions can always be guaranteed in real scenarios.\n\n(6) The experimental settings show that for heterogeneous model experiments, multiple baselines are compared, including FedMD, FedDF, FCCL, FedGen, and VHL. But Table 2 only lists the results of FedHe, FedDF, and FCCL."
                },
                "questions": {
                    "value": "(1) The client structure information is not provided in the experiments. Does it imply that all the clients are connected with each other in all experiments?\n\n(2) In subsection 3.3., \"$P(\\cdot)$ index class category\" is confusing. Where is $P(\\cdot)$ used in this section?\n\n(3) The communication costs of DESA can be analyzed, because it might include additional anchor sharing and logits sharing compared to baselines.\n\n(4) Some notations used in Theorem 1 are undefined, e.g., $d_{H\\Delta H}$, $\\lambda(P_i)$, etc. In addition, what does \"if $\\psi_i \\circ P^{Syn} \\to \\psi_i \\circ P^T$ for any $\\psi_i$\" imply?\n\n(5) Below Proposition 2, it is shown that when the local data heterogeneity is severe, the model learning should rely more on the centralized data, e.g., synthetic data and the extended KD data. This can be verified in the experiments, e.g., how the hyperparameters $\\lambda_{REG}$ and $\\lambda_{KD}$ can be changed with respect to the data heterogeneity.\n\n(6) \"FedSAB\" in subsection 5.3 is undefined."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "n/a"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission545/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698020567923,
            "cdate": 1698020567923,
            "tmdate": 1699635981647,
            "mdate": 1699635981647,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GiexGjmqkm",
                "forum": "PcBJ4pA6bF",
                "replyto": "kHapeueync",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission545/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission545/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Justification of our motivation for federated mutual learning"
                    },
                    "comment": {
                        "value": "We thank the reviewer for raising this concern. Let us think about an essential scenario: In 2020, hospitals from different regions want to collaboratively train a COVID-19 classification model. Due to quarantine, people cannot travel outside the local area, so the local hospital can only collect local cases with local strains of Covid viruses. If they train a model that only cares about intra-domain, this *personalized* model will suffer from a performance drop when the quarantine restriction is revoked and people from one region (with its local virus strains) will travel to other regions and use other hospitals\u2019 models. Thus, we consider *intra* and *inter*-domain performances are both  important, and allowing each client model to generalize well to other client domains can potentially improve model robustness. Thus, In this work, we consider a different FL setting without sever and define it as  *decentralized federated mutual learning*, which shares the same motivation as the existing work [1].\n\n[1] Huang W, Ye M, Du B. Learn from others and be yourself in heterogeneous federated learning. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition 2022 (pp. 10143-10153)."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission545/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700470494521,
                "cdate": 1700470494521,
                "tmdate": 1700470494521,
                "mdate": 1700470494521,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DX2Czh4slt",
                "forum": "PcBJ4pA6bF",
                "replyto": "kHapeueync",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission545/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission545/Authors"
                ],
                "content": {
                    "title": {
                        "value": "The effect of IPC"
                    },
                    "comment": {
                        "value": "Finding the minimal amount synthetic data is not the focus of this work. We have replaced \u201cminimal\u201d with \u201csmall\u201d in our revision for clarification. Following your suggestion, we have performed a more detailed hyperparameter search on IPC. Specifically, we search IPC from {5, 10, 20, 50, 100, 200} and the experimental results are updated in our revision. \nOverall, blindly increasing the IPC does not guarantee to obtain optimal intra- and inter-accuracy. It will cause the loss function to be dominated by the last 2 terms of Eq. 8, \\textit{i.e.,} by synthetic data rather than minimizing the empirical risk of classification on cross-entropy loss. However, synthesizing larger number of synthetic data may degrade its quality, and the sampled batch for $\\mathcal{L}_{REG}$ may fail to capture the distribution."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission545/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700470622588,
                "cdate": 1700470622588,
                "tmdate": 1700470622588,
                "mdate": 1700470622588,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zLl6c0H7Px",
                "forum": "PcBJ4pA6bF",
                "replyto": "kHapeueync",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission545/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission545/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Explanation of the global synthetic data and the neighbor information"
                    },
                    "comment": {
                        "value": "Regarding how we merge D^{syn}: We perform simple interpolation (averaging) among clients as it is shown that using this mixup strategy can improve model fairness [1]. \n\nRegarding how we define our neighbors: We assume the client nodes form a complete graph in DIGITS and OFFICE experiments. For CIFAR10C experiments, we randomly sample neighboring clients for each round.\nWe have added the clarification in our revision.\n\n\n[1]  Chuang CY, Mroueh Y. Fair Mixup: Fairness via Interpolation. InInternational Conference on Learning Representations 2020 Oct 2."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission545/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700470656467,
                "cdate": 1700470656467,
                "tmdate": 1700470656467,
                "mdate": 1700470656467,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1JL7IwcFAn",
                "forum": "PcBJ4pA6bF",
                "replyto": "kHapeueync",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission545/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission545/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Clarification of Eq. 3 and synthetic data generation"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the careful review and clarification questions. \n\n**Typo on notation:** We apologize for the typo. There shouldn\u2019t be a summation over i to N, so the client i\u2019s data is not affected by client j, $\\forall i \\neq j$. We thank the reviewer for the careful review and have corrected it in our revision, and we sincerely apologize for causing your confusion.\n\n**Class-balanced guarantee:** We synthesize data based on each class, i.e., for synthesizing data for class c, we will sample real data from P(x|y=c). As a result, we can control the number of synthetic data for each class c, and generate a class-balanced synthetic data set. \n\n**Potential privacy leakage:** As discussed in [2], using MMD distilled data to train a model can defend against reconstruction attacks and membership inference attacks, which is empirically justified in our Appendix B. For further privacy preservation, we also show we can apply Differential Privacy when distilling synthetic data in Appendix C.\n\n[1] Bo Zhao and Hakan Bilen. Dataset condensation with distribution matching. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 6514\u20136523, 2023.\n\n[2] Dong T, Zhao B, Lyu L. Privacy for free: How does dataset condensation help privacy?. InInternational Conference on Machine Learning 2022 Jun 28 (pp. 5378-5396). PMLR."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission545/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700470713048,
                "cdate": 1700470713048,
                "tmdate": 1700470713048,
                "mdate": 1700470713048,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VbhoNeT9ZM",
                "forum": "PcBJ4pA6bF",
                "replyto": "kHapeueync",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission545/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission545/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Justification and explanation of the labelling function for synthetic data"
                    },
                    "comment": {
                        "value": "We thank the reviewer for his astute observation about the labelling functions.The only mathematical assumption that the theorem makes is that the encoded distributions of the source $P_s$ and the global data domain $P_t$ converge to each other. Practically, this is enforced by the anchor term in the loss function, $\\mathcal{L}_{REG}$. \n\nComing to the interpretation of the bound through the similarity of the labeling functions.\n\nFirstly, it is important to note that the theorem does not require both of the error terms to be small. As long as any single one of the terms is low, we can change the component weights $\\alpha$ to rely more on that term , so that we get a tighter generalization guarantee. These component weights alpha are directly tied to the regularization weights $\\lambda_{REG}$ and $\\lambda_{KD}$.\nThe explanation for either of the labelling functions being close to $f_T$ is given below, \n\n1. The real data and the synthetic data will have similar labeling functions because their distributions are enforced to be similar under an MMD metric during the data generation process. - (Eq. 3) . Therefore $f^{syn}$ will be close to $f_T$ \n\n2. The statement about the extended KD data is a bit more subtle. It is crucial to note that $D_{KD}^{syn}$ depends on the client models $M_j$  and **is not a static distribution.**  Towards the end of training, when the empirical KD loss consistently decreases and generalized models are learned, the labeling function of the extended KD data $D_{KD}^{syn}$, which represents the consensus knowledge of all the domains,  will correctly classify any example drawn from the global data  domain  $P_T$.\nTherefore, we project that the labeling function $f_{KD}^{syn}$ will be close to $f_T$."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission545/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700470884295,
                "cdate": 1700470884295,
                "tmdate": 1700470884295,
                "mdate": 1700470884295,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "M0T19TMsrl",
                "forum": "PcBJ4pA6bF",
                "replyto": "kHapeueync",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission545/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission545/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Explanation of the method selections for model heterogeneous experiments"
                    },
                    "comment": {
                        "value": "The comparison results with FedGen are presented in Homogeneous Model Experiments (Section 5.3). We did not include FedGen in Table 2 as it is not applicable to model heterogeneous scenarios discussed in table 2. FedMD[1] was not considered to be included due to its poor performance, consistent with the observation report in its similar method FedHe [3]. We have corrected the paragraph by removing FedGen and FedMD and adding FedHe in our revision.\n\n[1] Li D, Wang J. Fedmd: Heterogenous federated learning via model distillation. arXiv preprint arXiv:1910.03581. 2019 Oct 8.\n\n[2] Zhu Z, Hong J, Zhou J. Data-free knowledge distillation for heterogeneous federated learning. InInternational conference on machine learning 2021 Jul 1 (pp. 12878-12889). PMLR.\n\n[3] Chan YH, Ngai EC. Fedhe: Heterogeneous models and communication-efficient federated learning. In2021 17th International Conference on Mobility, Sensing and Networking (MSN) 2021 Dec 13 (pp. 207-214). IEEE."
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission545/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700470949112,
                "cdate": 1700470949112,
                "tmdate": 1700470949112,
                "mdate": 1700470949112,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "m5q4NPYFQQ",
                "forum": "PcBJ4pA6bF",
                "replyto": "kHapeueync",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission545/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission545/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Clarification on the client structure"
                    },
                    "comment": {
                        "value": "We assume the client nodes form a complete graph in DIGITS and OFFICE experiments. For CIFAR10C experiments, we randomly sample neighboring clients for each round."
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission545/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700471036560,
                "cdate": 1700471036560,
                "tmdate": 1700471036560,
                "mdate": 1700471036560,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "IAm9pRXYoM",
                "forum": "PcBJ4pA6bF",
                "replyto": "kHapeueync",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission545/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission545/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Clarification on $P(\\cdot)$"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the careful review. The $P(\\cdot)$ is unused and we have removed $P(\\cdot)$ in our revision."
                    }
                },
                "number": 24,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission545/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700471090808,
                "cdate": 1700471090808,
                "tmdate": 1700471090808,
                "mdate": 1700471090808,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Bi4FPhod38",
                "forum": "PcBJ4pA6bF",
                "replyto": "kHapeueync",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission545/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission545/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Clarification on the communication cost of DeSA"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the suggestion. As noted in our Section 3.1, DeSA only requires sharing logits w.r.t. Global synthetic data during model training. Thus it has a relatively low communication overhead compared to baseline methods which require sharing model parameters. For example, if we use ConvNet as our model and set IPC=50, the number of parameters for communication for DeSA will be 10(number of classes) x 50(images/class) x 10(logits/image) = 5k. In comparison, baseline methods need to share 320K parameters (the number of parameters of ConvNet model), which is much larger than DeSA. We also add the discussion in Appendix D in our revision."
                    }
                },
                "number": 25,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission545/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700471135994,
                "cdate": 1700471135994,
                "tmdate": 1700471135994,
                "mdate": 1700471135994,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XVJNkUl3V3",
                "forum": "PcBJ4pA6bF",
                "replyto": "kHapeueync",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission545/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission545/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Explanation of the notations in Theorem 1"
                    },
                    "comment": {
                        "value": "We thank the reviewer for pointing this out.\n\nThe exact mathematical definitions have been added to our revision.\nBelow, we explain their meaning in words.\n\t\n1. $d_{H \\Delta H}$ is a distance metric that relates to the maximum difference in probability assigned to the space of disagreements between any two hypothesis functions drawn from H.\n2. $\\lambda(P_i)$ refers to the combined generalization loss of the best joint model on both local and client domains.\n3. The condition implies that there should exist a function $\\psi$  such that the encoded distributions of $P_s$ and $P_t$  converge to each other in probability.\nThis convergence condition is enforced by the loss $\\mathcal{L_reg}$, which acts as an anchor to pull all encoded distributions together."
                    }
                },
                "number": 26,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission545/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700471231199,
                "cdate": 1700471231199,
                "tmdate": 1700471231199,
                "mdate": 1700471231199,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "MGFH3n97lw",
                "forum": "PcBJ4pA6bF",
                "replyto": "kHapeueync",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission545/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission545/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Clarification of Proposition 2"
                    },
                    "comment": {
                        "value": "We clarify that the statement \u201cwhen the local data heterogeneity is severe, the model learning should rely more on the centralized data\u201d is on the assumption  that \u201c synthetic and the extended KD datasets are similar to the global ones\u201d Thus the LHS of Eq(9) will be dominated by $ \\epsilon_{{P}^T}(f^{Syn})$ and $ \\epsilon_{{P}^T}(f_{\\text{KD}}^{Syn})$. It is intuitive that if synthetic data is ideal aligned with global distribution and we fully rely on synthetic data in local training, the performance gain between training on synthetic data vs on local data is expected to be larger when local data is more heterogeneous. Also, as noted in Theorem 1, the performance is not solely related to hyperparameters $\\lambda_{REG}$ and $\\lambda_{KD}$. It will be also affected (or dominated) by the fourth term of Eq 8, when we rely on sufficient number of ideal synthetic data (aligned with global distribution)."
                    }
                },
                "number": 27,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission545/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700471266166,
                "cdate": 1700471266166,
                "tmdate": 1700471266166,
                "mdate": 1700471266166,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zZ95oAdB5Y",
                "forum": "PcBJ4pA6bF",
                "replyto": "kHapeueync",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission545/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission545/Authors"
                ],
                "content": {
                    "title": {
                        "value": "FedSAB in Subsection 5.3"
                    },
                    "comment": {
                        "value": "Sorry for the typo, we have corrected it in our revision."
                    }
                },
                "number": 28,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission545/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700471292945,
                "cdate": 1700471292945,
                "tmdate": 1700471292945,
                "mdate": 1700471292945,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mpoOGnafg5",
                "forum": "PcBJ4pA6bF",
                "replyto": "zZ95oAdB5Y",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission545/Reviewer_phME"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission545/Reviewer_phME"
                ],
                "content": {
                    "title": {
                        "value": "Comments"
                    },
                    "comment": {
                        "value": "Thanks for the rebuttals. But I have other follow-up concerns.\n\n(1) It shows the impact of IPC on the proposed DESA framework. \n- It affects the trade-off between inter- and intra-performance. In the context of decentralized federated mutual learning, how should the optimal trade-off between inter- and intra-performance be defined? For example, in COVID-19 classification, larger IPC can lead to improved inter-accuracy but degraded intra-accuracy. In this case, how will IPC be selected?\n- The results of Figure 4(c) are also affected by the hyperparameters $\\lambda_{KD}$ and $\\lambda_{REG}$. Does Figure 4(c) consider the same hyperparameters for IPC?\n\n(2) The explanation of the labeling function is still unclear to me.\n- What does the assumption of \"the encoded distributions of the source $P_s$ and the global data domain $P_t$ converge to each other\" mean? What are \"encoded distributions of the source\" here? How can this be enforced by $\\mathcal{L}_{REG}$?\n- How can the MMD in Eq. (3) guarantee that $f^{syn}$ is similar to $f_T$?\n\n(3) The computation of the communication cost of DeSA is confusing. It is shown that DESA only requires sharing logits. But all the three terms in Eq. (7) involve the global synthetic data $D^{Syn}$. Would $D^{Syn}$ be shared in FL?\n \n(4) The experiments consider only synthetic structure information on common image data sets. It might be more convincing to include some real-world data sets (e.g., COVID-19 classification) which indicate both inter- and intra-accuracy should be emphasized. This can also verify the impact of structure information on the proposed DeSA method in real scenarios."
                    }
                },
                "number": 31,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission545/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700516959144,
                "cdate": 1700516959144,
                "tmdate": 1700516959144,
                "mdate": 1700516959144,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tiXtsIiXC5",
                "forum": "PcBJ4pA6bF",
                "replyto": "kHapeueync",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission545/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission545/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Explanation of the labeling function"
                    },
                    "comment": {
                        "value": "> What does the assumption of \"the encoded distributions of the source $P_s$ and the global data domain $P_t$ converge to each other\" mean? \n\nIn our paper, we represent the $i$th client model as two parts: the feature encoder $\\psi_i$ and classification head $\\rho_i$. $\\psi_i$ will map the raw data (i.e. the image data in our experiments) to the embedding representation, and $\\rho_i$ will output the prediction on top of the embeddings. In our theoretical analysis, the core assumption is that the distributions of embedding representations under the synthetic dataset $P^{Syn}$ and global dataset $P^T$ across clients are similar. We express this mathematically as $\\psi_i \\circ P^{Syn} \\rightarrow \\psi_i \\circ P^{T}$, where $\\psi_i \\circ P^{Syn}$ is the distribution of the embeddings over the synthetic dataset and $\\psi_i \\circ P^{T}$ is that over the global dataset.\n\n> What are \"encoded distributions of the source\" here?\n\nThe encoded distributions represent the embeddings derived from the global synthetic dataset. In this context, we use the term 'source data domain' to specifically refer to the synthetic dataset of all the clients.\n\n> How can this be enforced by $\\mathcal{L}_{REG}$\n\nIf we go back to the definition of $L_{REG}$ (i.e., Eq (4) in the paper), it minimizes the per-class distances of the embeddings over the synthetic dataset and the true datasets. When it is approaching zero, it means that the per-class averaged embeddings over the synthetic and true datasets are the same. When we minimize $\\mathcal{L}_{REG}$ for every client model, the synthetic data behaves as an anchor, pulling each domain\u2019s encoded representation close to the encoded representation of the synthetic data.\nTherefore, as the models learn $\\psi_i$, they will learn to project the synthetic data distribution $P_s$ into a domain invariant representation $\\psi_i \\circ P_T$\n\n> How can the MMD in Eq. (3) guarantee that $f^{syn}$ is similar to $f_T$?\n\n1. The MMD in Eq. (3) is a label-conditional distance. It requires that the generated local synthetic data and local client data behave similarly under each class.  \n\n2. The global synthetic data $D^{syn}$ is the aggregation of all the local synthetic data. Since eq(3) guarantees that the local synthetic data distribution is similar to the local client data distribution, the global synthetic data distribution becomes representative of $P_T$\n\n3. The labelling function from [1] is characteristic to the distribution it is defined upon, therefore, the labelling function $f_{syn}$ is close to $f_T$.\n\n[1] Ben-David S, Blitzer J, Crammer K, Kulesza A, Pereira F, Vaughan JW. A theory of learning from different domains. Machine learning. 2010 May;79:151-75."
                    }
                },
                "number": 33,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission545/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700547103399,
                "cdate": 1700547103399,
                "tmdate": 1700612399314,
                "mdate": 1700612399314,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OKhzECCVzB",
                "forum": "PcBJ4pA6bF",
                "replyto": "kHapeueync",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission545/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission545/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Clarification of DeSA and its experiments"
                    },
                    "comment": {
                        "value": "We wish to clarify that for this study, **our choice of datasets was guided by the objective to align with the widely recognized standards** in dataset distillation literature [1,2,3]. The selected datasets, commonly used in FL research, enable direct comparisons with existing methods and model both feature and label heterogeneities, ensuring our findings are relevant and interpretable in current research contexts.\n\nWe acknowledge that the distribution-based distillation method employed in our study is optimized for the datasets we selected and may not be directly applicable to high-resolution images, which often require more sophisticated distillation strategies, such as [1,2].  Due to time constraints and the specialized nature of medical image processing for COVID19 classification task, it wasn't feasible to include these in this rebuttal phase.\n\nNotably, **obtaining good distilled datasets can be a standalone step to be optimized**, but not the focus of our work. **Our primary focus was to handle both model and data heterogeneity in a challenging serverless decentralized FL setting** and **develop a theoretically solid framework to justify the proposed loss functions given being able obtain ideal synthetic data**. Our theoretical findings are versatile and can be applied to other distillation strategies, potentially including those suitable for high-dimensional data.\n\nFinally, we would like to **reiterate the contributions of our study**: we develop a theoretically solid framework to handle both model and data heterogeneity in a serverless decentralized FL scenario, and, empirically, we apply dataset distillation to generate synthetic data that fits our theory. The experimental result on benchmark datasets out-performs existing decentralized FL methods, which validates the effectiveness of DeSA.\n\nWe believe our framework **lays a solid foundation for future exploration in data and model heterogeneous FL**, and we are hopeful that subsequent research will build on our theoretical insights to address a broader range of applications, including those involving high-dimensional real data.\nWe appreciate the reviewer's insights and will highlight potential areas for future exploration. \n\n[1] Cazenavette G, Wang T, Torralba A, Efros AA, Zhu JY. Dataset distillation by matching training trajectories. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition 2022 (pp. 4750-4759).\n\n[2] Yu R, Liu S, Wang X. Dataset distillation: A comprehensive review. arXiv preprint arXiv:2301.07014. 2023 Jan 17.\n\n[3] Zhao B, Bilen H. Dataset condensation with distribution matching. InProceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision 2023 (pp. 6514-6523)."
                    }
                },
                "number": 35,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission545/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700547392640,
                "cdate": 1700547392640,
                "tmdate": 1700611295783,
                "mdate": 1700611295783,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "MSoz8qiKYc",
                "forum": "PcBJ4pA6bF",
                "replyto": "UTafCc5Dpu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission545/Reviewer_phME"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission545/Reviewer_phME"
                ],
                "content": {
                    "title": {
                        "value": "Comments"
                    },
                    "comment": {
                        "value": "(1) Regarding the assumption on $P^{Syn}$ and $P^T$, it may not hold that they are similar when true client distribution is class-imbalanced. In this case, $P^{Syn}$ would always be given by class-balanced synthetic samples. Then it is not reasonable to assume that $P^{Syn}$ and $P^T$ are similar distributions (balanced vs. imbalanced distributions).\n\n(2) More theoretical analysis on the similarity between $f^{Syn}$ and $f^T$ will be more convincing. It directly affects the tightness of the generalization error bound in Theorem 1. For example, MMD corresponds to the empirical distribution similarity of synthetic samples and real samples. But labeling functions in [Ben-David S, 2010] are defined over the expected functions.\n\n(3) For a fair comparison, it is better to consider all the involved communication costs in a FL method, including pre-processing and online information sharing during training."
                    }
                },
                "number": 37,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission545/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700603473582,
                "cdate": 1700603473582,
                "tmdate": 1700603473582,
                "mdate": 1700603473582,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "hh6lbWywV1",
            "forum": "PcBJ4pA6bF",
            "replyto": "PcBJ4pA6bF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission545/Reviewer_xb5U"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission545/Reviewer_xb5U"
            ],
            "content": {
                "summary": {
                    "value": "The paper studied data and model heterogeneities in decentralized federated learning (FL), which is a serverless FL setting. In particular, the paper focused on the generalization, beyond personalization, of client models. The proposed method, DeSA, leverages synthetic anchors using data generation techniques to introduce two effective regularization terms for local training: anchor loss that matches the distribution of the client\u2019s latent embedding with an anchor, and KD loss that enables clients learning from others. Experiments demonstrated the effectiveness of DeSA on intra- and inter-domain tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper considered a complex setting where both data and model heterogeneities are present, which can be hard to tackle in general. New loss terms are introduced to deal with the heterogeneities, and data synthesis technique are used to avoid sharing real data. The approach is reasonable and justified.\n\n2. The paper provided extensive experimental results to demonstrate the effectiveness of DeSA, which is compared against methods from both model heterogenous and homogeneous settings."
                },
                "weaknesses": {
                    "value": "1. The motivation of considering generalization ability of client models on inter-domain tasks is not clear. In the model heterogenous setting, each client may process a model with a different architecture,  which is compatible with its own configuration. While the client can benefit from other clients\u2019s data to train a personalized model, why does this model have to perform well on other clients\u2019 tasks too? Other clients may not be able to acquire or deploy the model.\n\n2. In experiments, from Table 2 it seems that data heterogeneity and model heterogeneity are correlated. That is, each dataset is assigned one model architecture. The results would be more interesting if both different datasets and models are assigned independently (by dividing a dataset into multiple clients)."
                },
                "questions": {
                    "value": "1. In DIGITS and OFFICE experiments, what is the number of clients? Is a client identified as a dataset?\n\n2. How does DeSA perform in cases where each client has limited training data, e.g., a few samples per class? Can each model benefit more from the global synthetic dataset and KD?\n\n3. Minor issues:\n- In Equation (3), the definition of D_i^{Syn} involves summation over i.\n- In Equation (6), L_{CE} is not used (but introduced right after the equation).\n- First line of Section 5.3: FedSAB?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission545/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission545/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission545/Reviewer_xb5U"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission545/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698734439237,
            "cdate": 1698734439237,
            "tmdate": 1699635981571,
            "mdate": 1699635981571,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6z56Vht7tU",
                "forum": "PcBJ4pA6bF",
                "replyto": "hh6lbWywV1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission545/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission545/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Justification of our motivation for federated mutual learning"
                    },
                    "comment": {
                        "value": "We thank the reviewer for raising this concern. Let us think about an essential scenario: In 2020, hospitals from different regions want to collaboratively train a COVID-19 classification model. Due to quarantine, people cannot travel outside the local area, so the local hospital can only collect local cases with local strains of Covid viruses. If they train a model that only cares about intra-domain, this *personalized* model will suffer from a performance drop when the quarantine restriction is revoked and people from one region (with its local virus strains) will travel to other regions and use other hospitals\u2019 models. Thus, we consider *intra* and *inter*-domain performances to be both important, and allowing each client model to generalize well on other client domains can potentially improve model robustness. Thus, In this work, we consider a different FL setting without sever and define it as  *decentralized federated mutual learning*, which shares the same motivation as the existing work [1].\n\n[1] Huang W, Ye M, Du B. Learn from others and be yourself in heterogeneous federated learning. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition 2022 (pp. 10143-10153)."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission545/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700470262285,
                "cdate": 1700470262285,
                "tmdate": 1700470262285,
                "mdate": 1700470262285,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lpz1aFJZoG",
                "forum": "PcBJ4pA6bF",
                "replyto": "hh6lbWywV1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission545/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission545/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Explanation of the model heterogeneous setup"
                    },
                    "comment": {
                        "value": "We thank the reviewer for suggesting that we should split a dataset and assign different models to subsets. In fact, we implement this strategy in our CIFAR10C experiments, where we split Cifar10C into 57 clients (subsets) and randomly assigned different model architectures to them. The objective of Table 2 is to show that our method can obtain competitive inter and intra-domain performance under both data and model heterogeneous scenarios."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission545/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700470314142,
                "cdate": 1700470314142,
                "tmdate": 1700470314142,
                "mdate": 1700470314142,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bWNxB0ErvV",
                "forum": "PcBJ4pA6bF",
                "replyto": "hh6lbWywV1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission545/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission545/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Clarification of the client setting for DIGITS and OFFICE experiments"
                    },
                    "comment": {
                        "value": "For DIGITS and OFFICE experiments, we assume each dataset represents one client following [1].  We further clarify the setting in our revision.\n\n[1] Li X, JIANG M, Zhang X, Kamp M, Dou Q. FedBN: Federated Learning on Non-IID Features via Local Batch Normalization. InInternational Conference on Learning Representations 2020 Oct 2."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission545/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700470386073,
                "cdate": 1700470386073,
                "tmdate": 1700470386073,
                "mdate": 1700470386073,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aeJqsvgVg6",
                "forum": "PcBJ4pA6bF",
                "replyto": "hh6lbWywV1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission545/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission545/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Explanation of how limited number of local data affects DeSA"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the great question. In Eq. 8, if we have limited local data, the bound will be dominated by $\\alpha^{Syn}$ since $\\alpha$ is small. Thus, the performance depends on the quality of global synthetic data. However, we would also like to point out limited local training data may affect the generation of synthetic data and further decrease its utility."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission545/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700470424268,
                "cdate": 1700470424268,
                "tmdate": 1700470424268,
                "mdate": 1700470424268,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RXsJt4EbGl",
                "forum": "PcBJ4pA6bF",
                "replyto": "hh6lbWywV1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission545/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission545/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Correction of the typos"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the careful review and pointing out the typos. We have addressed the typos as follows. We have also done more careful proofreading in our revision.\n- There shouldn\u2019t be a summation over i to N, and we have corrected it. \n- The L_{CE} description shouldn\u2019t be there and we have removed it.\n- We corrected it into DeSA."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission545/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700470448461,
                "cdate": 1700470448461,
                "tmdate": 1700470448461,
                "mdate": 1700470448461,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Duru2NjnZp",
                "forum": "PcBJ4pA6bF",
                "replyto": "hh6lbWywV1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission545/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission545/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you and look forward to your feedback to our rebuttal"
                    },
                    "comment": {
                        "value": "Dear Reviewer xb5U,\n\nAs the rebuttal deadline is approaching, we would like to know if our rebuttals have addressed your concerns and questions. Also, we have tried our best to reflect your brilliant feedback to our revision. We appreciate the opportunity to discussing during this stage and are delighted to address your further question if there is any. If you are satisfied with our response and revision, we are grateful if you could kindly re-consider the rating for DeSA.\n\nBest Regards,\n\nDeSA authors"
                    }
                },
                "number": 39,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission545/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700686566789,
                "cdate": 1700686566789,
                "tmdate": 1700686651880,
                "mdate": 1700686651880,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "9wzohjNdAY",
            "forum": "PcBJ4pA6bF",
            "replyto": "PcBJ4pA6bF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission545/Reviewer_VrA4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission545/Reviewer_VrA4"
            ],
            "content": {
                "summary": {
                    "value": "The paper explores decentralized federated learning, focusing on both data and model heterogeneity\u2014a notably challenging context where traditional FedAVG and its derivatives fall short. The authors introduce a novel approach, DESA (Decentralized FL with Synthetic Anchors), which employs synthetic anchors to act as class-specific feature centers. To generate these synthetic anchors, the authors utilize randomly sampled feature extractors and optimize data points using the empirical maximum mean discrepancy (MMD) loss. Subsequently, each client is trained using anchor loss and knowledge distillation loss to combat data and model heterogeneity, respectively. Experimental validation is conducted on domain-shifted datasets: DIGITS, OFFICE, and CIFAR10c, where DESA shows superior performance."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The problem formulation is both rigorous and practically relevant.\n\n2. Experimental evidence substantiates the efficacy of DESA on DIGITS, OFFICE, and CIFAR10c datasets."
                },
                "weaknesses": {
                    "value": "1. Certain aspects of the paper remain ambiguous.\n1-1. Equation (2) introduces an objective that encompasses all clients for defining inter-client loss. However, the decentralized nature of the problem implies that each client can communicate only with adjacent nodes, raising questions about the feasibility of this objective.\n1-2 Equation (3) suffers from unclear terminology; specifically, the meaning of the representation (x\u2223y) is not explained. Additionally, the methodology for generating \"randomly sampled feature extractors\" is also unclear.\n1-3. Equation (4) contains undefined notations, requiring clarification.\n2. The paper touches upon privacy concerns arising from the sharing of synthetic data but fails to delve deep enough into this critical issue. Given the importance of privacy in federated learning algorithms, the authors should offer a more comprehensive discussion, preferably in the main text rather than relegating it to the appendix.\n3. The theoretical results are primarily based on Ben-David et al. (2010), a fact highlighted in the appendix but missing from the main text. This could potentially weaken the paper\u2019s contribution.\n4. The paper struggles to bridge the gap between theoretical claims and empirical results. The presented theorems are contingent upon strong assumptions, and their relevance to the experimental findings is not intuitively obvious."
                },
                "questions": {
                    "value": "ould you please clarify the issues listed under \"Cons\"?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission545/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698846497709,
            "cdate": 1698846497709,
            "tmdate": 1699635981498,
            "mdate": 1699635981498,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "S9r6q3UMOU",
                "forum": "PcBJ4pA6bF",
                "replyto": "9wzohjNdAY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission545/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission545/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Explanation of the ambiguous parts in the paper"
                    },
                    "comment": {
                        "value": "> Clarification of client structure and the objective of Eq. 2\n\nWe thank the reviewer for pointing out the typo of Eq. 2. The summation should be from 1 to N(C_i), and we have corrected it in our revision. As shown in Algorithm 1, our method is designed to work only to receive neighbor node information, and DeSA is designed for peer-to-peer decentralized network. Although we are working on a peer-to-peer decentralized network but the loss requires all nodes\u2019 information, we can leverage the FastMix algorithm to aggregate all nodes\u2019 information [1,2]. This method can aggregate all nodes' information via adjacent nodes\u2019 communication at a linear speed. It is very common in fully decentralized optimization. In fact, our method can also work if each node can only receive neighbor nodes\u2019 information, and we empirically show the feasibility in our CIFAR10C experiments by sampling neighboring clients.\n\n> Clarification of (x|y)\n\nWe thank the reviewer for the clarification question. $\\psi(x|y)$ means the comparison is conditional on label y , which allows us to distill synthetic data based on each class c. \n\n> Clarification of \u201crandomly sampled feature extractors\u201d\n\nWe thank the reviewer for the clarification question. Using randomly sampled feature extractors allows us to perform empirical MMD loss as described in [3]. To implement it, we simply randomly initialize the model parameters for each round with PyTorch.\n\n> Clarification of Eq. 4\n\nWe appreciate reviewer\u2019s careful review. . We have removed the unused notation $P$ and add description of $K$ in our revision.  \n\n[1] Ye H, Zhou Z, Luo L, Zhang T. Decentralized accelerated proximal gradient descent. Advances in Neural Information Processing Systems. 2020;33:18308-17.\n\n[2] Luo L, Ye H. Decentralized Stochastic Variance Reduced Extragradient Method. arXiv preprint arXiv:2202.00509. 2022 Feb 1.\n\n[3] Bo Zhao and Hakan Bilen. Dataset condensation with distribution matching. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 6514\u20136523, 2023."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission545/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700470077923,
                "cdate": 1700470077923,
                "tmdate": 1700479104722,
                "mdate": 1700479104722,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gb8IISESgL",
                "forum": "PcBJ4pA6bF",
                "replyto": "9wzohjNdAY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission545/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission545/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Justification of privacy preservation in DeSA"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the suggestion. Since our main focus is to solve the challenging issue - decentralized federated learning with data and model heterogeneities, we decided to put more focus on how our method can utilize data distillation to solve the problem in our manuscript. \n\nWe agree that showing the privacy guarantee is important, so we have discussed the empirical privacy guarantee in Section 3.2 and pointed the readers to our Appendix B and C for our privacy-preservation results. Specifically, we provide a DP version for synthetic data in our Appendix C. In addition, we have shown our method can defend against Membership Inference Attack (MIA) [1] in Appendix B. We share the concern that incorporating a comprehensive discussion into the main text might limit the space available for elaborating on the technical details essential for understanding our innovative method. Our priority is to ensure that the core aspects of our approach are communicated effectively and clearly. Other reviewers seem to like our current presentation. In the revised manuscript, we underscored the sentence in the main text that directs readers to the appendix. \n\n\n\n[1] Nicholas Carlini, Steve Chien, Milad Nasr, Shuang Song, Andreas Terzis, and Florian Tramer. Membership inference attacks from first principles. In 2022 IEEE Symposium on Security and Privacy (SP), pp. 1897\u20131914. IEEE, 2022a."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission545/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700470124298,
                "cdate": 1700470124298,
                "tmdate": 1700470124298,
                "mdate": 1700470124298,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9FF9K4LAxe",
                "forum": "PcBJ4pA6bF",
                "replyto": "9wzohjNdAY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission545/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission545/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Discussion regarding Ben-David et al (2010)\u2019s work"
                    },
                    "comment": {
                        "value": "We would like to point out that Ben - David 2010 et al\u2019s work was a general bound for learning on new domains. Our theory\u2019s  contribution is not only applying Ben-David et al\u2019s work to our scenario, but also connecting it to our main algorithm through the addition of terms like $f^{syn}$ and $f^{syn}_{KD}$.This addition introduces novel mathematical insights into the connection between labelling functions and the anchor and KD regularization losses.\n\nWe also provide a novel stronger version of our theorem in Proposition 2, that holds under the mild conditions enforced by our algorithm, and other novel and useful Lemmas in the appendix that theoretically motivate our algorithm.\n\nFollowing your valuable suggestion, we have edited the reference for Ben-David et al. in our revision."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission545/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700470181769,
                "cdate": 1700470181769,
                "tmdate": 1700479125118,
                "mdate": 1700479125118,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ofLW2hrmNs",
                "forum": "PcBJ4pA6bF",
                "replyto": "9wzohjNdAY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission545/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission545/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Discussion about the gap between theoretical claims and empirical results"
                    },
                    "comment": {
                        "value": "We thank the reviewer for his interesting remark. However, we strongly believe that our theorem gives remarkable insights into the empirical success of our algorithm.\n\n- The only assumption that our theorem makes is the convergence in probability of the encoded target and source domains, which is empirically enforced by our anchor loss regularizer $L_{reg}$. This is a novel condition in theorem 1, that inspired our design of the empirical anchor regularization term in Equation (7).\n\n- It is of paramount importance to note that the previous work of Ben-David 2010 et al, whose shoulders we build Theorem 1 upon, has no connections to any practical algorithm at all. It was our novel contribution to develop a bound that incorporated empirical connections like the labelling function of synthetic data $f^{syn} $ and the Extended KD data $f^{syn}_{KD}$. These labelling functions connect deeply with our empirical anchor and KD regularization losses . \nTheorem 1 helps us predict the global performance of models using the component weights $\\alpha$ given to the empirical regularization terms.\nWe have further clarified these connections in the appendix of our revision.\n\n- Our newly added ablation studies clearly show that improving the quality and quantity of synthetic data ($D^{syn}$) boosts both inter and intra accuracy. Remarkably, Theorem 1 accurately predicted this outcome. In simpler terms, the theorem suggests that better synthetic data reduces the generalization error of labeling functions ($f^{syn}$ and $f^{syn}_{KD}$), tightening the upper bound on global generalization error. This alignment between theory and experiment underscores the reliability of Theorem 1 in predicting empirical outcomes.\n\n- The connection between the quality of synthetic data and the upper bound in Theorem 1 is further confirmed during experiments in Section C of the Appendix. The injection of noise reduces the quality of the global synthetic data $D_{syn}$, which thereby worsens the upper bound guarantee in Theorem 1. This conforms with the empirical observation of a drop in inter and intra-client accuracy.\n\nTherefore, we strongly believe that our theory is connected with the empirical side of our problem, and helps in explaining it\u2019s results."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission545/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700470229312,
                "cdate": 1700470229312,
                "tmdate": 1700470229312,
                "mdate": 1700470229312,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xBfKsYCr2s",
                "forum": "PcBJ4pA6bF",
                "replyto": "9wzohjNdAY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission545/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission545/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you and look forward to your feedback to our rebuttal"
                    },
                    "comment": {
                        "value": "Dear Reviewer VrA4,\n\nAs the rebuttal deadline is approaching, we would like to know if our rebuttals have addressed your concerns and questions. Also, we have tried our best to reflect your brilliant feedback to our revision. We appreciate the opportunity to discussing during this stage and are delighted to address your further question if there is any. If you are satisfied with our response and revision, we are grateful if you could kindly re-consider the rating for DeSA.\n\nBest Regards,\n\nDeSA authors"
                    }
                },
                "number": 40,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission545/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700686619792,
                "cdate": 1700686619792,
                "tmdate": 1700686643991,
                "mdate": 1700686643991,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "AbDFbrzG5C",
                "forum": "PcBJ4pA6bF",
                "replyto": "xBfKsYCr2s",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission545/Reviewer_VrA4"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission545/Reviewer_VrA4"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your reply. I have reviewed all of your responses and will take them into account when determining my final score and in discussions with reviewers and AC."
                    }
                },
                "number": 42,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission545/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700729348355,
                "cdate": 1700729348355,
                "tmdate": 1700729348355,
                "mdate": 1700729348355,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "nAD3hcHU1C",
            "forum": "PcBJ4pA6bF",
            "replyto": "PcBJ4pA6bF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission545/Reviewer_2BZe"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission545/Reviewer_2BZe"
            ],
            "content": {
                "summary": {
                    "value": "Decentralized FL enables clients to own different local models and separately optimize local data. How can every client's local model learn generalizable representation is unknown. To address this question, This paper proposes a Decentralized FL technique by introducing Synthetic Anchors, as DESA. Authors leverage the synthetic anchors to implement 1) anchor loss that matches the distribution of the client's latent embedding with an anchor and 2) KD loss that enables clients learning from others. The proposed method doesn't presume access to real public or a global data generator."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The studied problem is novel and well motivated.\n2. Distilling local synthetic anchor is interesting.\n3. There are theoretical analysis of the proposed methods, in which the new generalization bound is better.\n4. Figure 3 is interesting, jointly considering worst local accuracy and global accuracy. Experiment results show significant improvements of the proposed method."
                },
                "weaknesses": {
                    "value": "1. The local synthetic anchor dataset iss shared. Thus, the privacy of the synthesized anchor should be considerred. Although the DP is used to protect synthetic anchor. But could this defend against recovering the raw data?\n2. It would be better to conduct a more ablation study to decouple the effect of the sythetic anchor and the KD loss."
                },
                "questions": {
                    "value": "See weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission545/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698847465472,
            "cdate": 1698847465472,
            "tmdate": 1699635981435,
            "mdate": 1699635981435,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "DYHTwf0zPP",
                "forum": "PcBJ4pA6bF",
                "replyto": "nAD3hcHU1C",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission545/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission545/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Privacy guarantee for sharing synthetic data"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the clarification question. We agree the privacy of the synthetic data is important, and we provide a DP version for synthetic data in our Appendix C. In addition, we have shown our method can defend against Membership Inference Attack (MIA) [1] in Appendix B. Regarding raw data recovery, since we don\u2019t share gradients among clients (we only share logits w.r.t. global synthetic data), the commonly used gradient inversion attacks [2,3] may not be able to recover original raw data.\n\n[1] Nicholas Carlini, Steve Chien, Milad Nasr, Shuang Song, Andreas Terzis, and Florian Tramer. Membership inference attacks from first principles. In 2022 IEEE Symposium on Security and Privacy (SP), pp. 1897\u20131914. IEEE, 2022a.\n\n[2] Geiping J, Bauermeister H, Dr\u00f6ge H, Moeller M. Inverting gradients-how easy is it to break privacy in federated learning?. Advances in Neural Information Processing Systems. 2020;33:16937-47.\n\n[3] Huang Y, Gupta S, Song Z, Li K, Arora S. Evaluating gradient inversion attacks and defenses in federated learning. Advances in Neural Information Processing Systems. 2021 Dec 6;34:7232-41."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission545/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700469945385,
                "cdate": 1700469945385,
                "tmdate": 1700469945385,
                "mdate": 1700469945385,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fkmxMvCbwG",
                "forum": "PcBJ4pA6bF",
                "replyto": "nAD3hcHU1C",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission545/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission545/Authors"
                ],
                "content": {
                    "title": {
                        "value": "More fine-grained ablation study on Reg loss and KD loss"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the constructive comment. Following your suggestion, we have performed a more fine-grained hyperparameter search on Reg loss and KD loss. Specifically, we search $\\lambda_{KD}$ from {0, 0.1, 0.5, 1, 2} and $\\lambda_{REG}$ from {0, 0.01, 0.02, 0.05, 0.1}, and the experimental results are updated in our revision. The findings are aligned with our original version that $\\lambda_{KD}$ helps improve the inter-accuracy increases, and $\\lambda_{REG}$ helps improve the intra-accuracy as well as the inter-accuracy within a certain range (observe the peak inter-accuracy at $\\lambda_{REG}=0.01$)."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission545/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700469971270,
                "cdate": 1700469971270,
                "tmdate": 1700469971270,
                "mdate": 1700469971270,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QS4XlXDA0s",
                "forum": "PcBJ4pA6bF",
                "replyto": "fkmxMvCbwG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission545/Reviewer_2BZe"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission545/Reviewer_2BZe"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the responses"
                    },
                    "comment": {
                        "value": "Based on the contributions of this work and responses, I'd like to keep my original scores."
                    }
                },
                "number": 30,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission545/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700471898002,
                "cdate": 1700471898002,
                "tmdate": 1700471898002,
                "mdate": 1700471898002,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "yk8h7pEK1k",
            "forum": "PcBJ4pA6bF",
            "replyto": "PcBJ4pA6bF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission545/Reviewer_R93g"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission545/Reviewer_R93g"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the problem of decentralized mutual learning. The challenges of decentralized mutual learning, other than the ordinary data non-iid issue, include model-heterogeneity and no server-coordination. This paper tackles this problem via constructing synthetic anchor data, whose information is shared across clients to bridge the large gap among data distributions. The paper further designs novel losses including regularization loss for representations of both anchor and true data; and a knowledge distillation loss to tackle model heterogeneity issue. Some theoretical insight is provided and numerical experiments on several benchmarks show convincing results."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Disclaimer: the reviewer is not very familiar with the anchor data generation in federated learning. Thus, I may not accurately assess the novelty of the technique proposed by this paper.\n\n- the problem this paper considers is interesting and important. Features like no central server coordination and model heterogeneity make practical sense.\n\n- the proposed algorithm is intuitive, has theoretical insight. And it seems to be also communication-efficient since only logits of anchor data require to be transmitted across clients.\n\n- the overal presentation is very good, and I find enjoyable to read the paper.\n\n- experimental results seem to be convincing."
                },
                "weaknesses": {
                    "value": "- overall I find the designed model contains a lot of subtlety, as it is quite complex and contains many components. So it appears a bit difficult to probe what really works and what does not.\n\nFor example, \n\n(a) how difficult is the data synthesis process (i.e. eq. 3) when the data is highly non-iid across clients. Since it basically minimize discrepancy between representations of local data and global data, does this process always successfully generate satisfactory anchor regardless of how data is partitioned? there is some visualization of synthetic anchor in appendix, but the quality of synthetic anchor still seems to be mysterious.\n\n(b)  the losses are not dissected well enough so that readers can make sure each loss is orthogonal, and plays its desirable role. the losses are designed based on intuitive heuristics. However, what role does each loss exactly play is not clear enough. For example, the anchor loss defined in eq 4, is that a bit overlapping with what eq 3 (i.e. anchor data synthesis)? basically, if data is generated from eq 3, will eq 4 automatically be relatively small? \n\nBasically, whether these losses are overlapping, and whether these losses have monotonic correlation, is difficult to determine.\n\n- following up on the subtlety of the model components, the ablation studies for DESA is not comprehensive enough to help. the hyperparameters (e.g. $\\lambda_{KD}$, $\\lambda_{REG}$, and IPC) are not searched comprehensively. For example, the inter accuracy vs. $\\lambda_{KD}$ is still monotonic with the three data points, and readers cannot grasp a full picture of the role of $\\lambda_{KD}$ or KD loss."
                },
                "questions": {
                    "value": "Please see weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission545/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699303998009,
            "cdate": 1699303998009,
            "tmdate": 1699635981360,
            "mdate": 1699635981360,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "toHNXOfFyl",
                "forum": "PcBJ4pA6bF",
                "replyto": "yk8h7pEK1k",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission545/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission545/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Explanation of the data synthesis and its quality"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the constructive question. Let us think about an extreme non-iid scenario where clients lack data in class c, our Eq. 2 intuitively suggests that client i, lacking samples in class c, cannot reasonably generate data samples with label c. In such instances, we adjust the interpolation ratio according to the class prior, thus deweighting client i on synthetic data class c and adding weight to the clients with abundant data in class i. Consequently, this implies client i will have a greater reliance on other clients\u2019 data in class c, thereby addressing heterogeneity and enhancing overall generalizability.\n\n**Regarding the quality of synthetic data:**\nWe hope to clarify that the purpose of data distillation is not to generate hight-fidelity images that look similar to the original image, but rather the ones keeping the similar level of utilities. Our generated distilled data share similar patterns as the ones presented in the original distribution-based dataset distillation paper [1] (see its Fig. 2). \n\n[1] Bo Zhao and Hakan Bilen. Dataset condensation with distribution matching. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 6514\u20136523, 2023."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission545/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700469745400,
                "cdate": 1700469745400,
                "tmdate": 1700469745400,
                "mdate": 1700469745400,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "NmEFQyiprK",
                "forum": "PcBJ4pA6bF",
                "replyto": "yk8h7pEK1k",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission545/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission545/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Correction and explanation of the non-overlapped characteristic of Eq. 3 and Eq. 4"
                    },
                    "comment": {
                        "value": "We apologize for the typo in Eq. 3 which may lead to misunderstanding. There should not be a summation of clients in Eq. 3, and we have corrected it in our revision. \n\nTo answer your question regarding Eq. 3 and Eq. 4: No, Eq. 3 and Eq. 4 are not overlapped. \nFirst, we hope to correct the typo in Eq. 3 \u2013 there should not be a summation of clients in Eq.3 as we stated it as a local data distillation loss. Sorry for leading the misunderstanding. The objective of Eq. 3 is to distill *local synthetic data* by minimizing the MMD of synthetic data and real data; thus, it is used to update client i\u2019s local synthetic data ($D^{syn}_i). Differently, Eq. 4 aims to update the feature extractor parameters using Supervised Contrastive Loss. Since the inputs of Eq. 4 is *global synthetic data*($D^{syn}$) (the interpolation of every client\u2019s *local synthetic data*) instead of client i\u2019s *local synthetic data* ($D^{syn}_i), minimizing Eq. 3 does not imply Eq. 4 will be relatively small."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission545/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700469787105,
                "cdate": 1700469787105,
                "tmdate": 1700469787105,
                "mdate": 1700469787105,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "sj4o7raimZ",
                "forum": "PcBJ4pA6bF",
                "replyto": "yk8h7pEK1k",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission545/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission545/Authors"
                ],
                "content": {
                    "title": {
                        "value": "The inter accuracy vs. $\\lambda_{KD}$ is monotonic"
                    },
                    "comment": {
                        "value": "Thanks for noticing the results, which is in fact align with our theoretical analysis. As shown in Theroem 1, if we have a large $\\lambda_{KD}$, the error bound is dominated by the KD loss, which is for improving the inter accuracy purpose. However, a too large KD loss will hurt intra accuracy."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission545/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700469816504,
                "cdate": 1700469816504,
                "tmdate": 1700469830350,
                "mdate": 1700469830350,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3sVCQA2pLh",
                "forum": "PcBJ4pA6bF",
                "replyto": "yk8h7pEK1k",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission545/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission545/Authors"
                ],
                "content": {
                    "title": {
                        "value": "More fine-grained ablation study on Reg loss and KD loss"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the constructive comment. Following your suggestion, we have performed a more fine-grained hyperparameter search on Reg loss and KD loss. Specifically, we search $\\lambda_{KD}$ from {0, 0.1, 0.5, 1, 2} and $\\lambda_{REG}$ from {0, 0.01, 0.02, 0.05, 0.1}, and the experimental results are updated in our revision. The findings are aligned with our original version that $\\lambda_{KD}$ helps improve the inter-accuracy increases, and $\\lambda_{REG}$ helps improve the intra-accuracy as well as the inter-accuracy within a certain range (observe the peak inter-accuracy at $\\lambda_{REG}=0.01$)."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission545/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700469848108,
                "cdate": 1700469848108,
                "tmdate": 1700469848108,
                "mdate": 1700469848108,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "E0HsJDWIOJ",
                "forum": "PcBJ4pA6bF",
                "replyto": "yk8h7pEK1k",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission545/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission545/Authors"
                ],
                "content": {
                    "title": {
                        "value": "The effect of IPC"
                    },
                    "comment": {
                        "value": "Finding the minimal amount synthetic data is not the focus of this work. We have replaced \u201cminimal\u201d with \u201csmall\u201d in our revision for clarification. Following your suggestion, we have performed a more detailed hyperparameter search on IPC. Specifically, we search IPC from {5, 10, 20, 50, 100, 200} and the experimental results are updated in our revision. \n\nOverall, blindly increasing the IPC does not guarantee to obtain optimal intra- and inter-accuracy. It will cause the loss function to be dominated by the last 2 terms of Eq. 8, \\textit{i.e.,} by synthetic data rather than minimizing the empirical risk of classification on cross-entropy loss. However, synthesizing larger number of synthetic data may degrade its quality, and the sampled batch for $\\mathcal{L}_{REG}$ may fail to capture the distribution."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission545/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700469877462,
                "cdate": 1700469877462,
                "tmdate": 1700469877462,
                "mdate": 1700469877462,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HVq0bPFHzL",
                "forum": "PcBJ4pA6bF",
                "replyto": "yk8h7pEK1k",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission545/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission545/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you and look forward to your feedback for our rebuttal"
                    },
                    "comment": {
                        "value": "Dear Reviewer R93g,\n\nAs the rebuttal deadline is approaching, we would like to know if our rebuttals have addressed your concerns and questions. Also, we have tried our best to reflect your brilliant feedback to our revision. We appreciate the opportunity to discussing during this stage and are delighted to address your further question if there is any. If you are satisfied with our response and revision, we are grateful if you could kindly re-consider the rating for DeSA.\n\nBest Regards,\n\nDeSA authors"
                    }
                },
                "number": 41,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission545/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700686729767,
                "cdate": 1700686729767,
                "tmdate": 1700686729767,
                "mdate": 1700686729767,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]