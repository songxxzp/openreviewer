[
    {
        "title": "PanoDiffusion: 360-degree Panorama Outpainting via Diffusion"
    },
    {
        "review": {
            "id": "6sgtseAPeN",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission821/Reviewer_dEjQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission821/Reviewer_dEjQ"
            ],
            "forum": "ZNzDXDFZ0B",
            "replyto": "ZNzDXDFZ0B",
            "content": {
                "summary": {
                    "value": "The paper presents a diffusion model for panoramic image generation. A two-stage RGB-D PanoDiffusion model is proposed for indoor RGB-D panorama outpainting. The model taks depth information as input and processed through a bi-modal LDM structure. As the results shown, the use of depth information enhances the generation of RGB panoramas, and the alignment mechanism ensures wraparound consistency in the results. The method can be used to generate RGB-D panoramas at 512\u00d71024 resolution."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The RGB-D fusion and combination mechanism is brought to the field of panoramic image generation via using the latent diffusion models. A RGB-D panoramic outpainting model is proposed to perform indoor 360-degree image generation. \n\nA bi-modal latent diffusion structure is proposed to combine RGB and depth information during training the diffusion model for panoramic image generation. \n\nA camera-rotation method is proposed to perform a stronger data augmentation. The two-end method can be used to crop a 90-degree equivalent area and stitch to the opposite sides to perform additional data augmentation to improve the wraparound consistency."
                },
                "weaknesses": {
                    "value": "As compared to the previous methods, the depth information is additionally added to train the diffusion model. Even though the depth information is not needed for inference. \n\nThe proposed RGB-D framework is constructed by using two parallel LDM to reconstruct the depth and RGB images separately. This structure might result in a larger and more complex model architecture than using a shared or depth-conditional LDM.  \n\nAn additional module is needed to refine and upscale the low-resolution image output to a high-resolution image. However, a pre-trained super-resolution GAN model is needed to perform such a refinement."
                },
                "questions": {
                    "value": "Apart from the visualization results, how about the evaluation results of using concatenation or cross-attention in the so-called depth-conditional diffusion model? \n\nHow is the cross-attention operation used in the depth-conditional method? Whether the authors try other advanced multimodal fusion methods to better combine the features from RGB and depth? \n\nIt would be better to ablate the camera-rotation data augmentation method. \n\nIn the depth panoramic synthesis, how about the comparison to some RGB-based depth estimation methods? For example, in the case of using fully masked input. \n\nHow about the runtime or complexity analysis of the proposed method, since the parallel LDM are used for RGB and depth separately."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission821/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission821/Reviewer_dEjQ",
                        "ICLR.cc/2024/Conference/Submission821/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission821/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697383074682,
            "cdate": 1697383074682,
            "tmdate": 1700653520439,
            "mdate": 1700653520439,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "z5rrVjBZwi",
                "forum": "ZNzDXDFZ0B",
                "replyto": "6sgtseAPeN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission821/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission821/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Authors (1/2)"
                    },
                    "comment": {
                        "value": "We appreciate the reviewer's valuable feedback and suggestions. We would like to provide a detailed response to each comment.\n\n> Q1: The proposed RGB-D framework is constructed by using two parallel LDM to reconstruct the depth and RGB images separately. This structure might result in a larger and more complex model architecture than using a shared or depth-conditional LDM.\n\nWe do not use two parallel LDM's. As is claimed in Section 3.3, using two parallel LDMs to reconstruct Depth and RGB images separately will incur excessive computational resources. To avoid this, **our PanoDiffusion uses two VAE encoders, but only one LDM, with depth info as concatenated channels**. Therefore, the computational cost does not increase significantly compared to the original conditional LDM (see Q6).\n\n> Q2: Apart from the visualization results, how about the evaluation results of using concatenation or cross-attention in the so-called depth-conditional diffusion model?\n\nWe've additionally tested depth-conditional diffusion models that use the usual concatenation and cross-attention conditioning on all types of masks. The results show our PanoDiffusion consistently outperforming the traditional depth-conditional LDM formulation.\n\n|                          | Camera    |           |           |           | NFoV       |           |           |           |\n| ------------------------ | --------- | --------- | --------- | --------- | ---------- | --------- | --------- | --------- |\n| Method                   | FID       | sFID      | D         | C         | FID        | sFID      | D         | C         |\n| PanoDiffusion            | **21.55** | **26.95** | **0.867** | **0.708** | **21.41**  | **27.80** | **0.790** | **0.669** |\n| DC-LDM (cross-attention) | 51.77     | 37.96     | 0.275     | 0.280     | 48.14      | 39.17     | 0.248     | 0.271     |\n| DC-LDM (concatenation)   | 114.59    | 51.96     | 0.023     | 0.018     | 112.54     | 64.80     | 0.046     | 0.006     |\n|                          | **Layout**   |           |           |           | **Random Box** |           |           |           |\n| Method                   | FID       | sFID      | D         | C         | FID        | sFID      | D         | C         |\n| PanoDiffusion            | **23.06** | **22.39** | **1.000** | **0.737** | **16.13**  | **20.39** | **1.000** | **0.883** |\n| DC-LDM (cross-attention) | 40.63     | 25.52     | 0.476     | 0.450     | 30.25      | 23.65     | 0.597     | 0.719     |\n| DC-LDM (concatenation)   | 105.79    | 32.90     | 0.060     | 0.054     | 64.49      | 30.38     | 0.237     | 0.471     |\n\n\n\n> Q3: How is the cross-attention operation used in the depth-conditional method? Whether the authors try other advanced multimodal fusion methods to better combine the features from RGB and depth?\n\nFor depth-conditional LDM's, we pretrained a VQVAE for depth, and kept the encoder. The encoded depth is fed into the LDM as a condition in the usual two options, either via concatenation, or by cross-attention. Additionally, to improve robustness, we allowed the LDM to update the depth-condition encoder weights during training. However, these methods led to unsatisfactory outpainting results, often resulting in blurry content.\n\nWe did not investigate more advanced multimodal fusion baselines, because our main task is RGB **outpainting without any depth input at inference time**."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission821/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700468258503,
                "cdate": 1700468258503,
                "tmdate": 1700468258503,
                "mdate": 1700468258503,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JWnbSl1PG7",
                "forum": "ZNzDXDFZ0B",
                "replyto": "0Qb1IwLcvm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission821/Reviewer_dEjQ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission821/Reviewer_dEjQ"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the response"
                    },
                    "comment": {
                        "value": "Thanks for the detailed information provided by the authors. \n\nMy concerns have been addressed from the response, for example, the evaluation results of using concatenation or cross-attention. The authors also provide a further comparison to some RGB-based depth estimation methods like PanoFormer and ACDNet. Therefore, I would like to increase my score to support this work."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission821/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700653480585,
                "cdate": 1700653480585,
                "tmdate": 1700653480585,
                "mdate": 1700653480585,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "rctI0Aksf1",
            "forum": "ZNzDXDFZ0B",
            "replyto": "ZNzDXDFZ0B",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission821/Reviewer_MEmh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission821/Reviewer_MEmh"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a latent diffusion model (LDM) for indoor RGB panaroma inpainting and depth map generation. During training stage, the input to the bi-modal LDM structure are RGB images and corresponding depth maps, which improve the performance of panaroma inpainting. At each stage of the denoising process in the diffusion model, the proposed alignment mechanism enhances the wraparound consistency of the results. The results indicate that the proposed PanoDiffusion excels not only by achieving a substantial performance advantage over state-of-the-art techniques in RGB-D panorama outpainting, yielding diverse and well-structured results for various mask types, but also by demonstrating the capability to generate high-quality depth panoramas."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- During inference process, there is no need to input the depth map as a guidance, only training process requires depth maps as input. It is significantly different from the previous approaches.\n- This paper proposed a noval approach that involves gradually introducing camera rotations at each stage of the diffusion denoising process, resulting in a notable enhancement in achieving seamless panorama wraparound consistency.\n- With the clear description of each module and step, this approach ensures that the framework is not only effective but also easily comprehensible and straightforward to follow."
                },
                "weaknesses": {
                    "value": "- There is not many noval changes to the LDM framework. The authors only add a pretrained depth map encoder as a guidance.\n- There are only one visual result (Figure 6) in the paper, which is not convincing. The authors should add more visual results for comparison in the supplementary materials."
                },
                "questions": {
                    "value": "- How about the training and inference time compared to other frameworks? Since LDM requires a step-by-setp mechanism to generate the final results, the efficiency may be a problem.\n- There are only 4 types of maskes. Each type of mask covers different portion of the whole panaroma. I recommand the authors to add a chart that describes the performance changing along the percentage of the mask. According to my understanding, with the increase of the percentage of the mask, the performance will drop.\n- The mask are all continuous presented in this paper, I am curious when there are several separate masks in one panaroma, what will the performance become?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission821/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission821/Reviewer_MEmh",
                        "ICLR.cc/2024/Conference/Submission821/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission821/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698732271543,
            "cdate": 1698732271543,
            "tmdate": 1700664300335,
            "mdate": 1700664300335,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "oxsMZqXxB6",
                "forum": "ZNzDXDFZ0B",
                "replyto": "rctI0Aksf1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission821/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission821/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Authors (1/2)"
                    },
                    "comment": {
                        "value": "We appreciate the reviewer's valuable feedback and suggestions. We would like to provide a detailed response to each comment.\n\n> Q1: There is not many noval changes to the LDM framework. The authors only add a pretrained depth map encoder as a guidance.\n\nThe more obvious approach is to train an LDM with depth masks as condition, particularly during training. We avoided this, and instead chose to use a RePaint-based structure, because we want to free the model from *presuming mask distributions*. This is critical for our two-end alignment design, as masks will continually change due to rotation. As LDM or Repaint cannot individually handle this challenge properly, it provides ample motivation for us to fuse these two components, naturally combined with our novel rotational denoising. Furthermore, we advanced our model to a bi-modal structure, where depth maps prove to have significant auxiliary effect in generating well-structured results. We also consider that our discovery that the *asymmetric* condition, where having a training setting (use of depth and without masking) that is substantially *different* from the inference setting (exclusion of depth and with masking), still leads to improved performance, is an interesting novel finding.\n\n> Q2: The authors should add more visual results for comparison in the supplementary materials.\n\nPerhaps it was overlooked, but we had previously included more visual results for comparison in our **Appendix, previously Fig. 12, now 13**, including two groups of examples for each mask type. Additionally, we had also provided some examples where we project different panorama images to 3D scenes in our supplementary video.\n\n> Q3: How about the training and inference time compared to other frameworks?\n\nFor training, we compared average training time (minutes) for one epoch of PanoDiffusion against baseline models on the same devices, using the same batch size 4 and the same training dataset. For inference, we compared the time (seconds) required to infer a single image.\n\n| Method        | Type                | Depth | Training (mins/epoch) | Inference (sec/image) |\n| ------------- | ------------------- | ----- | --------------------- | --------------------- |\n| PanoDiffusion | Bi-modal LDM        | +     | 82                    | 5                     |\n| BIPS          | GAN                 | +     | 131                   | <1                    |\n| RePaint       | Diffusion model     | -     | 78                    | 45                    |\n| LDM           | LDM                 | -     | 72                    | 4                     |\n| OmniDreamer   | Transformer + VQGAN | -     | 158                   | 61                    |\n\nThe results show that while our model is not the fastest, it remains within a reasonable range. It's also noteworthy that, compared to the original LDM framework, **our bi-modal structure achieves a significant improvement in the quality of outpainting.** **This improvement comes without a proportionate increase in resource consumption** \u2013 we observed only a modest increase of 13.8% in training time and 25% in inference time."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission821/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700468068656,
                "cdate": 1700468068656,
                "tmdate": 1700468068656,
                "mdate": 1700468068656,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "NAvfrXfS26",
                "forum": "ZNzDXDFZ0B",
                "replyto": "0Ilv88bngk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission821/Reviewer_MEmh"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission821/Reviewer_MEmh"
                ],
                "content": {
                    "title": {
                        "value": "Concerns Resolved"
                    },
                    "comment": {
                        "value": "After checking the response from the authors, most of my concerns are resolved. The authors add additional experiments and analysis to demonstrate the effectiveness of the proposed method. Based on their response and changes, I would like to increase my rating.\n\nBest,"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission821/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700664257075,
                "cdate": 1700664257075,
                "tmdate": 1700664257075,
                "mdate": 1700664257075,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "msuGy8LJJ4",
            "forum": "ZNzDXDFZ0B",
            "replyto": "ZNzDXDFZ0B",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission821/Reviewer_inST"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission821/Reviewer_inST"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a way to outpaint a near field-of-view image (i.e. a normal image from a normal camera) to a panorama, which they represent as a equirectangular projected image. They use latent diffusion models to do so. Their latent model is trained on RGB-D panoramic data, but works on just RGB inputs. \n\nOne problem with outpainting panoramic images is that the left and right side of the equirectangular project image need to map to each other. The authors introduce a novel technique to do this. It works by rotating the image by 90 degrees in each denoising step in the diffusion model.\n\nThe authors compare to a variety of other techniques and show that their method is better than competing methods."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "The paper is well-written and the results seem pretty good. The author\u2019s idea to rotate the image 90 degrees in each denoising step seems very novel. \n\nThere are lots of comparisons to many other methods and the proposed method is better based on a variety of techniques. \n\nI appreciate that the authors include the code as supplementary material!"
                },
                "weaknesses": {
                    "value": "It is somewhat hard to evaluate the generated panoramic images based on looking at images or fixed rotations. I would be interested in seeing the panoramic images of both the proposed method and competing method in a viewer like threejs. See https://threejs.org/examples/webgl_panorama_equirectangular.html"
                },
                "questions": {
                    "value": "There are many ways to represent 360 degree panorama images. The authors should clarify that using the equirectangular projection is a choice they are making.\n\nDo the authors only rotate the camera in the horizontal direction? Or are vertical rotations allowed as well?\n\nDuring inference, did the authors try 180 degree shifts instead of 90 degree shifts? Or some other rotation?\n\nWhat\u2019s the difference between Fig. 2 and Fig. 3? They seem like they are both figures about how training and inference are done, but they seem to be different. Specifically, Fig. 2b does not add noise and does not reference the circular shift, while Fig. 3b does. Is Fig. 2b incorrect?\n\nIn the supplemental video, do the shown 3D Scenes at 00:12 use the generated depth maps?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission821/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission821/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission821/Reviewer_inST"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission821/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698772561444,
            "cdate": 1698772561444,
            "tmdate": 1699636009258,
            "mdate": 1699636009258,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8MS2YdXUHN",
                "forum": "ZNzDXDFZ0B",
                "replyto": "msuGy8LJJ4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission821/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission821/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Authors"
                    },
                    "comment": {
                        "value": "We appreciate the reviewer's valuable feedback and suggestions. We would like to provide a detailed response to each comment. For Q3 and Q4, much of the replies here are also included in our response to reviewer SqKp.\n\n> Q1: I would be interested in seeing the panoramic images of both the proposed method and competing method in a viewer like threejs.\n\nThank you for your suggestion! In fact, we've already completed a demo that transforms panorama images into interactive threejs for users to check the results, and we plan to release the examples upon the acceptance of our paper. As proof, we provided some examples at 00:12 in our supplemental video for a preview, which are outpainted panoramas of our PanoDiffusion.\n\n> Q2: There are many ways to represent 360 degree panorama images. The authors should clarify that using the equirectangular projection is a choice they are making.\n\nThank you, we have clarified the choice of using equirectangular projection in the dataset section of our revised paper.\n\n> Q3: Do the authors only rotate the camera in the horizontal direction? Or are vertical rotations allowed as well?\n\nFor both training and inference stages, we only consider rotations in the horizontal direction. As panorama images typically follow equirectangular projection, the distortion increases non-uniformly towards the top and bottom poles. **Introducing angle rotations in the vertical direction would lead to substantial changes in the projection results**, and require more complex preprocessing, which will increase both training and inference costs. We have added two examples where the camera has a 90-degree vertical rotation in our Appendix 1.3 Fig.~10.  Conversely, **distortion is uniform horizontally** -- image manipulation here only involves horizontal cropping and splicing, without the need for reprojection. Since the main objective of PanoDiffusion is to generate coherent and realistic panorama images, we have focused more on the spatial wrap-around consistency in the horizontal direction only. In the future, we will consider better approaches to support wrap-around consistency in all directions.\n\n> Q4:  During inference, did the authors try 180 degree shifts instead of 90 degree shifts? Or some other rotation?\n\nDuring inference, we have explored the effect of different rotation angles, including 180\u00b0, 90\u00b0 and 45\u00b0, on the outpainting results.  The results show that the wrap-around consistency of outpainting results is improved across all settings.  Compared to 180\u00b0, 90\u00b0 leads to better consistency. However, diminishing the angle further to 45\u00b0 did not lead to additional improvements. We believe this is reasonable, as the model is expected to generate coherent content when the two ends are in contact for enough denoising steps. Therefore, smaller rotation angles than 90\u00b0 and longer connections do not necessarily lead to more consistent results. We have incorporated this part into the Appendix of our revised paper.\n\n| Methods \\ Mask Type | Camera | NFoV   | Layout | Random Box | End    |\n| ------------------- | ------ | ------ | ------ | ---------- | ------ |\n| w/o rotation        | 125.82 | 128.33 | 128.10 | 128.19     | 132.69 |\n| 180\u00b0                | 95.11  | 96.57  | 90.93  | 85.23      | 119.60 |\n| 90\u00b0                 | 90.41  | 89.74  | 88.01  | 85.04      | 116.77 |\n| 45\u00b0                 | 90.67  | 90.25  | 87.65  | 86.50      | 112.47 |\n\n> Q5: What\u2019s the difference between Fig. 2 and Fig. 3?\n\nFig. 2 is intended to provide an overview of our method. We had previously deliberated on integrating the circular shifts, masking operations and iteration loop into Fig. 2, but eventually felt it became excessively cluttered. So we created Fig. 3 to illustrate those operations in more detail, while excluding them from Fig. 2. \n\nMore specifically, Fig. 2 presents an overview of the entire model pipeline, **where we only show the input and output of each stage and omit the specific details of circular shift and adding noise**. This is to make our two-stage bi-modal structure clearer and to highlight the difference between the inputs we use for training and inference.\n\nFor Fig. 3, we instead focus on explaining how the camera rotation mechanism is implemented.  We detailed the flow of the rotation process, **including the incorporation of noisy images with different transparency to represent different noising steps.**\n\nWe have clarified this difference in Fig. 2 caption in our revised paper.\n\n> Q6: In the supplemental video, do the shown 3D Scenes at 00:12 use the generated depth maps?\n\nThe 3D scenes at 00:12 do not use the generated depth maps.  We only use the outpainted panorama images of PanoDiffusion and project them onto spherical coordinates using threejs, which enabled their display in an immersive 3D format."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission821/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700467827717,
                "cdate": 1700467827717,
                "tmdate": 1700467827717,
                "mdate": 1700467827717,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3fb7EjF98a",
                "forum": "ZNzDXDFZ0B",
                "replyto": "8MS2YdXUHN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission821/Reviewer_inST"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission821/Reviewer_inST"
                ],
                "content": {
                    "comment": {
                        "value": "I am satisfied with the authors rebuttal to my concerns as well as the concerns of other reviewers and maintain my positive rating."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission821/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700663695792,
                "cdate": 1700663695792,
                "tmdate": 1700663695792,
                "mdate": 1700663695792,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "r5eWG5kDMB",
            "forum": "ZNzDXDFZ0B",
            "replyto": "ZNzDXDFZ0B",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission821/Reviewer_SqKp"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission821/Reviewer_SqKp"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new diffusion based method to tackle panorama image generation task. The proposed method utilizes the latent diffusion models to outpaint the area that is not originally taken by the camera. Relying on the powerful generation capability of diffusion models, the proposed method proposes to progressively apply camera rotations during the image generation process in order to enhance the generalizability. Extensive experiments show that the proposed method is able to outperforms the baseline methods significantly."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper is generally well written with strong motivation and well-organized writing. It clearly demonstrates the problem of  current pano generation and implies the proposed method is tackling the problems stated. \n2. The proposed method significantly outperforms the baseline methods on the selected benchmarks."
                },
                "weaknesses": {
                    "value": "1. During the training stage, does the random angle rotation only apply to horizontal direction?\n2. Depth map actually provides rich information indicating the scales of the objects in the images. In order to test the generalization capability, can you provide more results of running the proposed method on other datasets?\n3. This paper claims the camera rotations as one of the contributions, please provide an ablation study on how the camera rotation actually works effectively."
                },
                "questions": {
                    "value": "The authors are suggested to address the concerns raised in the weaknesses section during the rebuttal period."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission821/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698831748974,
            "cdate": 1698831748974,
            "tmdate": 1699636009175,
            "mdate": 1699636009175,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "lpPoowcLUx",
                "forum": "ZNzDXDFZ0B",
                "replyto": "r5eWG5kDMB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission821/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission821/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Authors"
                    },
                    "comment": {
                        "value": "We appreciate the reviewer's valuable feedback and suggestions. We would like to provide a detailed response to each comment.\n\n> Q1: During the training stage, does the random angle rotation only apply to horizontal direction? \n\nDuring the training stage, we restrict random angle rotation solely to the horizontal direction. As panorama images typically follow equirectangular projection, the distortion increases non-uniformly towards the top and bottom poles. **Introducing random angle rotation in the vertical direction would lead to substantial changes in the projection results, and require more complex preprocessing, which will increase training costs.** We have added two examples where the camera has a 90-degree vertical rotation in our Appendix 1.3 Fig.~10. Conversely, **distortion is uniform horizontally** - image manipulation here only involves horizontal cropping and splicing, without the need for reprojection.\n\nIn terms of training efficacy, without providing camera viewpoints as part of the input, **adding vertical rotations does not improve model performance**. Instead, it can complicate the learning of equirectangular projection patterns, reducing overall effectiveness. Since the main objective of PanoDiffusion is to generate coherent and realistic panorama images, we focus more on data augmentation in the horizontal direction - the spatial wrap-around consistency.\n\n> Q2: In order to test the generalization capability, can you provide more results of running the proposed method on other datasets?\n\nWe conducted additional tests using a set of 2305 panorama images from Matterport3D [1] dataset. These tests were performed without re-training or fine-tuning PanoDiffusion, which was trained on the Structured3D dataset.\n\n|                  | Camera Mask  |        |       |       | NFoV Mask |        |       |       |\n| ---------------- | --------------- | ------ | ----- | ----- | ------------- | ------ | ----- | ----- |\n| Method \\ Metrics | FID \u2193           | sFID \u2193 | D \u2191   | C \u2191   | FID \u2193         | sFID \u2193 | D \u2191   | C \u2191   |\n| PanoDiffusion | **45.52**       | **37.34** | **0.639** | **0.622** | **49.86**     | **38.31** | **0.740** | **0.649** |\n| BIPS | 83.23 | 45.97 | 0.113 | 0.098 | 97.13 | 57.56 | 0.082 | 0.045 |\n| Inpaint Anything | 65.69 | 50.67 | 0.049 | 0.121 | 79.48 | 56.57 | 0.014 | 0.031 |\n\nMatterport3D differs from Structured3D with greater complexity and diversity, including multi-story and outdoor scenes. Despite these challenges, PanoDiffusion outperforms the baseline models and is able to outpainting reasonable panorama images in most of the cases. We have provided some qualitative results in the Appendix of our paper for your reference.\n\n> Q3: This paper claims the camera rotations as one of the contributions, please provide an ablation study on how the camera rotation actually works effectively.\n\nWe have previously provided some ablation results on this. **Please see Fig. 8 and Table 3, where we compared PanoDiffusion with and without the use of rotations.**\n\nTo further shed light on the reviewer's question, we **additionally explored the effect of the rotation angle, including 180\u00b0, 90\u00b0 (chosen for our final result) and 45\u00b0, on the outpainting results.** The results show that the wrap-around consistency is improved across all settings. Compared to 180\u00b0, 90\u00b0 leads to better consistency. However, diminishing the angle further to 45\u00b0 did not lead to additional improvements. We believe this is reasonable, as the model is expected to generate coherent content when the two ends are in contact for enough denoising steps. Therefore, smaller rotation angles and longer connections do not necessarily lead to more consistent results. \n\n| Methods \\ Mask Type | Camera | NFoV   | Layout | Random Box | End    |\n| ------------------- | ------ | ------ | ------ | ---------- | ------ |\n| w/o rotation        | 125.82 | 128.33 | 128.10 | 128.19     | 132.69 |\n| 180\u00b0                | 95.11  | 96.57  | 90.93  | 85.23      | 119.60 |\n| 90\u00b0                 | 90.41  | 89.74  | 88.01  | 85.04      | 116.77 |\n| 45\u00b0                 | 90.67  | 90.25  | 87.65  | 86.50      | 112.47 |\n\n[1] Chang A, Dai A, Funkhouser T, et al. Matterport3d: Learning from rgb-d data in indoor environments. arXiv preprint arXiv:1709.06158, 2017"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission821/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700467726653,
                "cdate": 1700467726653,
                "tmdate": 1700467726653,
                "mdate": 1700467726653,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]