[
    {
        "title": "Domain-Agnostic Molecular Generation with Self-feedback"
    },
    {
        "review": {
            "id": "yd9UMIuydz",
            "forum": "9rPyHyjfwP",
            "replyto": "9rPyHyjfwP",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2312/Reviewer_13gf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2312/Reviewer_13gf"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a pre-trained molecular language model called MOLGEN to generate molecules from SELFIES with self-feedback. Specifically, MOLGEN contains two pre-training stages: 1) molecular language syntax learning using the BART model; and 2) domain-agnostic molecular prefix tuning using two sets of tunable prefix vectors in multi-head attention layers. Then, the authors use a self-feedback paradigm to align the pre-training language model with the anticipated chemical preferences in the downstream phase (i.e., the authors align the model\u2019s probabilistic rankings of diverse molecular responses with preference rankings observed in actual chemical contexts)."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Strengths:\n\n- This paper is well organized and written.\n\n- A two-stage domain-agnostic molecular pre-training model based on BART with SELFIES is proposed. Then, a self-feedback paradigm is used to alleviate the molecular hallucination problem."
                },
                "weaknesses": {
                    "value": "Weaknesses:\n- The experiment is incomplete. I suggest that the authors conduct more comprehensive experiments (i.e., baseline comparisons and ablation studies) to demonstrate the effectiveness of the proposed MOLGEN model. For example,\n    - $\\bf baseline~comparison$: most of the baseline models in the experiment are from before 2022 (e.g., JT-VAE (2018), GCPN (2018), GraphAF (2020), and GraphDF (2021)). I suggest the authors to compare more latest baselines to validate the MOLGEN.\n    - $\\bf ablation~study$: the authors only performed ablation study on the self-feedback mechanism. In Eq. 8, the authors used the \"soft\" label to smooth the one-hot distribution (Eq. 2) into the target distribution. This is one of the contributions of the paper. The effectiveness of the \"soft\" label also needs to be verified in the ablation study. Also, the hyper-parameter \"$\\alpha$\" controls the balance of the two losses and has an important impact on the performance of the model. How to choose a and the effect of different $\\alpha$ on the model performance should also be demonstrated in the experiments.\n\n- The description of the model (Section 2) is not clear. More details need to be presented. For example,\n    - Does the \"$l$\" in the paragraph above Eq. (1) indicate the total number of SELFIES in the dataset? Is it the same as the \"$l$\" in \"two sets of \"$l$\" tunable prefix vectors\" above  Eq. (3)?\n    - What do the two tunable prefix vectors refer to, and why are these two vectors used in multi-head? How to prove that the performance is improved after using them?"
                },
                "questions": {
                    "value": "Questions:\n\n- Please see the above.\n- In the sentence \"Finally, we encode the corrupted SMILES using a bidirectional model...\" above Eq. (1), should it refer to \"SMILES\" or \"SELFIES\"?\n- How $(S_i, S_j)$ pair in $\\bf S^{*}$ are selected?\n\n- Some typos:\n    - Figures 2 and 3 have opposite locations.\n    - It would be preferable to replace \"ours\" in tables 1, 2 and 3 with \"MOLGEN\". In addition, both \"MOLGEN\" and \"MolGen\" appear in the context, which should be consistent.\n    - The masking and start tokens are denoted by \"[MASK]\" and \"[S]\" in the context, but are \"[Mask]\" and \"[s]\" in Figure 3. Consistency is needed."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2312/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2312/Reviewer_13gf",
                        "ICLR.cc/2024/Conference/Submission2312/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2312/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698641812111,
            "cdate": 1698641812111,
            "tmdate": 1700632408123,
            "mdate": 1700632408123,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "JNUerwy6U3",
                "forum": "9rPyHyjfwP",
                "replyto": "yd9UMIuydz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2312/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2312/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We sincerely thank you for your insightful feedback. We have addressed your concerns below and hope our responses provide clarity:\n\n**1. More baselines**\n\nOur selection of baseline models included notable ones like LIMO, Chemformer, and RetMol from post-2022, chosen for their well-recognized effectiveness in this field. It's important to note that finding universally recognized and effective baseline models in this rapidly evolving field can be challenging, especially with some newer models lacking publicly available code or comprehensive documentation. According to your recommendation, we have now incorporated a new baseline model, RT[1], into our analysis. **(Highlighted in Table 4 Page 7, and Appendix E Page 18)**\n\n**2. More ablation studies**\n\nIn response to your suggestions, we have added the results of these ablation studies to **Appendix H.5 (Highlighted in Page 23)**.\n\n- **Effect of label smoothing**\n\nWe investigate the impact of label smoothing on the diversity of molecules generated by the model, employing IntDiv as the metric. IntDiv assesses the chemical diversity of generated molecules by calculating the average Tanimoto coefficient within the generated set. As shown in the following table, the model with label smoothing does not overly rely on singular, frequently occurring patterns learned from the training data. Consequently, this enhances the diversity and creativity of the molecules generated.\n\n|Model|Synthetic|Natural Product|\n|-|-|-|\n|MolGen (w/o label smoothing)|.8556|.8769|\n|MolGen|.8567 **(+0.12%)**|.8878 **(+1.24%)**|\n\n- **Effect of $\\alpha$ parameter**\n\nWe also explore the impact of the hyperparameter $\\alpha$. As illustrated in the following tables, an $\\alpha$ value of 0 indicates no self-feedback.  When $\\alpha$ is increased to 3, the model demonstrates a superior ability to optimize molecules compared to an $\\alpha$ of 1. However, increasing $\\alpha$ to 5 does not necessarily lead to better performance than at an $\\alpha$ of 3. During actual operation, it is necessary to adjust parameters according to specific requirements. Based on experience, setting $\\alpha$ to either 3 or 5 is recommended. \n\n|P-logP|Synthetic|Natural Product|\n|-|-|-|\n|$\\alpha$=0|-13.152|-7.031|\n|$\\alpha$=1|0.128|1.235|\n|$\\alpha$=3|0.569|1.762|\n|$\\alpha$=5|0.860|1.569|\n\n|QED|Synthetic|Natural Product|\n|-|-|-|\n|$\\alpha$=0|0.488|0.767|\n|$\\alpha$=1|0.532|0.802|\n|$\\alpha$=3|0.675|0.883|\n|$\\alpha$=5|0.811|0.795|"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2312/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700220906013,
                "cdate": 1700220906013,
                "tmdate": 1700220906013,
                "mdate": 1700220906013,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Y8PRDEd81I",
                "forum": "9rPyHyjfwP",
                "replyto": "sxQopS4Kub",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2312/Reviewer_13gf"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2312/Reviewer_13gf"
                ],
                "content": {
                    "title": {
                        "value": "After rebuttal"
                    },
                    "comment": {
                        "value": "Thanks for the effort the authors made to clarify and improve the manuscript.\nI would like to rise my score."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2312/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700632352712,
                "cdate": 1700632352712,
                "tmdate": 1700632352712,
                "mdate": 1700632352712,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "yoHgxmnvrT",
            "forum": "9rPyHyjfwP",
            "replyto": "9rPyHyjfwP",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2312/Reviewer_Hnvb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2312/Reviewer_Hnvb"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces MolGen, a molecule language model with 700M parameters built on the Bart architecture. Unlike its predecessor, Chemformer, MolGen employs SELFIES representations for molecules, ensuring 100% validity in molecule generation. Additionally, the paper presents a fine-tuning technique called 'self-feedback,' which aligns MolGen's generation probabilities with the chemical properties of the generated molecules.\n\nThe proposed method has shown significant improvement over previous methods. However, I have some major concerns on the paper's presentation, see the weakness section for details."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* The model shows significant improvements in the compared benchmarks, including distribution learning, targeted molecule discovery and constrained optimization.\n* The proposed pretraining method is simple and plausible."
                },
                "weaknesses": {
                    "value": "The submission contains significant issues, including the misuse of two major concepts and some questionable claims. Given the centrality of these concepts to the paper's overall argument, I recommend that the paper is not suitable for publication in its current form.\n\n* **Concept Misuse: Molecular Hallucination:** The authors introduce the notion of \"molecular hallucination\" as the generation of molecules that do not exhibt the desired properties or follow human preferences. The term appears to be a misnomer, as it does not align with the traditional understanding of \"hallucination\" in the NLP domain. In NLP literature, hallucination generally refers to the generation of fictitious or nonexistent entities or events [1,2,3]. Conversely, the authors assert that their \"hallucinated\" molecules are chemically valid, meaning they are neither fictitious nor nonexistent. Therefore, the use of the term \"hallucination\" could be misleading and should be reconsidered for clarity and conceptual consistency.\n* The related works on hallucination are missing.\n* **Concept Misuse: Self-feedback:** The authors refer to their fine-tuning approach as the \"self-feedback paradigm.\" However, the proposed method is more like \"**external-feedback**\", instead of \"self-feedback\". In short, the proposed fine-tuning method is to align the molecule generation probabilities to chemical property scores **that are measured by an external model**. Given this external dependency, the term \"external-feedback paradigm\" would be a more accurate and descriptive name for the fine-tuning method. \n* **Missing Ablation Study:** MolGen is pretrained in two phases. In the second phase, MolGen is adapted on two datasets using a method called domain-agnostic molecular prefix tuning. After reading the submission, it remain unclear to me how the second phase can help other downstream tasks. Moreover, the ablation study on the second pretraining phase is missing. It is also unclear how is the proposed prefix tuning method compared against other parameter efficient tuning methods, like LoRA?\n* **Unclear Figures:** \n  * What are the x-axis of the two subfigures on the right of Figure 7? \n  * What are the x-axis and y-axis of the two subfigures on the right of Figure 8?\n  * What are the x-axis of the Appendix Figure 2?\n\n* **Novelty:** This is not the first molecule language model pretrained using molecule's SELFIE representations. [4] is also pretrained using SELFIES. The relevance and diffierence to [4] should be discussed.\n* **Dubious claims:**\n  * In the second paragraph of introduction, the authors argue that previous works `are limited by their heavy reliance on task-specific tuning`. However, the proposed method also relies on a fine-tuning stage for downstream tasks.\n  * In the third paragraph of introduction, the authors claims that `almost all previous studies have focused primarily on synthetic molecules, neglecting natural products`. However, the literature review to support this claim is missing.\n  * The opening paragraph of Section 2.2 describes a scenario where a molecule has one substructure that is effective for a specific task but is counteracted by another, ineffective substructure. However, it remains unclear how the proposed method addresses this issue.\n  * In the second paragraph of the introduction, the authors claim that `the brittleness of SMILES may lead to a high proportion of generated chemically invalid strings`. Are there any citation to support this claim? \n\n\n\n\n\n\n\n\n**Reference:**\n\n[1] On Faithfulness and Factuality in Abstractive Summarization. In ACL 2020.\n\n[2] Neural Path Hunter: Reducing Hallucination in Dialogue Systems via Path Grounding. In EMNLP 2021.\n\n[3] Retrieval Augmentation Reduces Hallucination in Conversation. In EMNLP Findings 2021 \n\n[4] Regression Transformer enables concurrent sequence regression and generation for molecular language modelling. In NMI 2023."
                },
                "questions": {
                    "value": "* Is the synthetic dataset in Section 3.1 referring to ZINC250K? If it is, directly use the name ZINC250k can improve the clarity."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2312/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2312/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2312/Reviewer_Hnvb"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2312/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698673111280,
            "cdate": 1698673111280,
            "tmdate": 1700645564754,
            "mdate": 1700645564754,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ARrhdVVyKr",
                "forum": "9rPyHyjfwP",
                "replyto": "yoHgxmnvrT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2312/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2312/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We sincerely thank you for your insightful feedback. We have addressed your concerns below and hope our responses provide clarity:\n\n**1. Concept of molecular hallucination**\n\nWe apologize for any ambiguity in our expression, but we wish to clarify that our concept of \"molecular hallucination\" is indeed aligned with the NLP concept of hallucination. In NLP, hallucination refers to generating text or responses that, while grammatically correct, fluent, and natural, deviate from the provided source inputs (faithfulness) or lack factual accuracy (factualness) [1] [2] [3] [4].\n\nTherefore, in our study, \"molecular hallucination\" is used to describe the phenomenon where molecules created by language models, although chemically valid and adhering to basic structural rules (similar to producing grammatically correct sentences), fail to possess the anticipated chemical properties (akin to producing text that deviates from the intended message or content of the source input). This concept is consistent with the understanding of hallucination in NLP, and as you may have noticed, other reviewers have also concurred with this viewpoint.\n\nTo enhance clarity, we have included related works about hallucination and further clarified the concept of \"molecular hallucination\" **(Highlighted in Appendix D.3)**. \n\n**2. Concept of self-feedback**\n\nWe chose the term \"self-feedback\" to underscore the model's self-driven learning and optimization process. In this process, the model guides its own learning and optimization based on the evaluation results of molecules it autonomously generates, though these evaluations are aided by an external scoring function. This term is specifically chosen to differentiate our work from studies that use responses from other models as their feedback signal sources [5]. In contrast, our feedback mechanism is based on the responses generated by our own model. \n\nIn response to your feedback, we have revised our wording to better convey this concept to readers, emphasizing the model's self-reflective learning cycle, despite incorporating external scoring **(Highlighted in Section 2.2, Page 5)**. \n\n**3. Additional ablation study**\n\nWe apologize for any confusion that may have arisen from our presentation and would like to clarify that the domain-agnostic molecular prefix-tuning in the second phase is distinct from parameter-efficient tuning methods. As mentioned in Page 3, the penultimate paragraph, this approach impacts the entire model's parameters,  boosting its ability to understand and adapt across various molecular domains.\n\nIn response to your valuable suggestion, we have now incorporated an ablation study in our manuscript **(Highlighted in Appendix H.5, Page 24)**. To investigate the impact of prefix tuning on the model, we present the mean (and standard deviation) of penalized logP improvement for molecules generated compared to inputs under varying similarity constraints, as detailed in the following table.  This data demonstrates that prefix tuning enhances the model's performance in optimizing molecules.\n\n|Method|sim=0.6|sim=0.4|\n|-|-|-|\n|MolGen (w/o prefix)|11.63\u00b10.18|10.23\u00b11.47|\n|MolGen|12.08\u00b10.82 **(+0.45)**|12.35\u00b11.21 **(+2.12)**|\n\nAdditionally, **Figure 8 (Page 9)** provides a comparison of the model's attention to key substructures, both with and without prefix tuning. For enhanced clarity, we have also included a tabular representation of the data visualized in this figure, offering a more direct interpretation of the impact of prefix tuning.\n\n|Model|SAL|\n|-|-|\n|SMILES-based PLM|0.396\u00b10.054|\n|MolGen(w/o prefix)|0.476\u00b10.095 **(+20.2%)**|\n|MolGen|0.495\u00b10.105 **(+25.0%)**|\n\nAs indicated in the aforementioned tables, the implementation of domain-agnostic prefix tuning not only enables the model to more effectively adapt to downstream tasks but also improves its interpretability.\n\n**4. Modifications to the figures**\n\nWe have revised these figures to better facilitate reader understanding.\n\n- For **Figure 7 (Highlighted in Page 8)**, the x-axis represents the QED values, and the y-axis shows their distribution.\n- For **Figure 8 (Highlighted in Page 9)**, the first subfigure has the x-axis representing the Substructure Attention Level (SAL) values, while the second subfigure's x-axis also represents SAL values and the y-axis shows their distribution.\n- For **Appendix Figure 2 (Highlighted in Page 21)**, the x-axis represents the property values and the y-axis shows their distribution."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2312/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700220546998,
                "cdate": 1700220546998,
                "tmdate": 1700220546998,
                "mdate": 1700220546998,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BEHUmFaNG1",
                "forum": "9rPyHyjfwP",
                "replyto": "yoHgxmnvrT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2312/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2312/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**5. Related paper**\n\nThank you for bringing this to our attention. While it is true that this study [6] also uses molecule's SELFIES representations for pretraining, the focus and methodology differ significantly from our work. In the referenced paper, the approach reinterprets regression as a sequence modeling task, where training combines property tokens with text tokens. Their model is engineered to generate molecules by inputting expected molecular property values along with a given molecular scaffold (with the generated molecules incorporating this scaffold), or to predict molecular property values based on an input molecule.\n\nIn contrast, our model is specifically designed to generate entirely new molecules based on existing ones, without being confined to a predetermined scaffold. This key difference in our objectives and techniques distinctly sets our research apart from the approach outlined in the cited study. \n\nFollowing your suggestion, we have now included the referenced paper as a new baseline in **Table 4 (Highlighted in Page 7)** and a discussion of this study in **Appendix E (Highlighted in Page 18)** of our revised manuscripts.\n\n**6. Other revisions**\n\nWe are grateful for your attention to these details and pointing out these oversights. In response to your valuable suggestions, we have made the following revisions:\n\n- **Task-specific tuning:** We changed the phrase \"heavy reliance on task-specific tuning\" to \"hard to train due to the high variance of RL\". This alteration underscores our method's aim to alleviate the instability and difficulty associated with training reinforcement learning models, thereby offering a more stable and efficient process for downstream applications. **(Highlighted in Section 1, Page 1)**\n\n- **Focus on synthetic molecules:** We included additional references to support our claim about previous studies. **(Highlighted in Section 1, Page 2)**\n\n- **Examples of molecular hallucination:** To avoid any ambiguity, we have rephrased the description in the scenario where the model generates a molecule with an additional side chain. The revised text emphasizes that the perceived structural robustness is illusory and does not guarantee desirable properties, exemplifying \"molecular hallucination\". **(Highlighted in Section 2.2, Page 4)**\n\n- **Brittleness of SMILES:** We added necessary references to support the statement about SMILES potentially leading to meaningless sequences. **(Highlighted in Section 1, Page 2)**\n\nThank you again for your constructive suggestions! Your insights have significantly contributed to enhancing the completeness and persuasiveness of our paper. We hope that the revisions and clarifications provided in our responses adequately address your concerns.\n\n[1] Survey of Hallucination in Natural Language Generation. In ACM Computing Surveys 2023.  \n[2] A Survey of Hallucination in \u201cLarge\u201d Foundation Models. 2023.  \n[3] Siren\u2019s Song in the AI Ocean: A Survey on Hallucination in Large Language Models. 2023.  \n[4] Cognitive Mirage: A Review of Hallucinations in Large Language Models. 2023.  \n[5] Contrastive Post-training Large Language Models on Data Curriculum. 2023.  \n[6] Regression Transformer enables concurrent sequence regression and generation for molecular language modelling. In NMI 2023."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2312/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700220665603,
                "cdate": 1700220665603,
                "tmdate": 1700220765453,
                "mdate": 1700220765453,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "d8TVMjAI7u",
                "forum": "9rPyHyjfwP",
                "replyto": "yoHgxmnvrT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2312/Reviewer_Hnvb"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2312/Reviewer_Hnvb"
                ],
                "content": {
                    "title": {
                        "value": "Followup questions"
                    },
                    "comment": {
                        "value": "Thanks for the authors' efforts on rebuttal. I appreciate them. The response has resolved my concern on the molecule hallucination problem, and I have raised my rating accordingly. I still have the following questions about the submission.\n\n* **Regarding the concept of self-feedback.** As the authors agree, the objective function is to align the generation probability to an external model. In [1], the external model is a RLHF reward model; in this submission, the external model is the QED or plop score function. In concept, it is still external feedback but not self-feedback, because the training signal does not come from MolGen itself.\n\n* **Regarding the prefix tuning method.**  `this approach impacts the entire model's parameters`. Does it mean that the MolGen's other parameters (not the prefix) are also tuned in fine-tuning? \n\n  \n\n**Reference:**\n\n[1] CONTRASTIVE POST-TRAINING LARGE LANGUAGE MODELS ON DATA CURRICULUM. 2023"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2312/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700638507665,
                "cdate": 1700638507665,
                "tmdate": 1700638584034,
                "mdate": 1700638584034,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4lRbOGd2eF",
                "forum": "9rPyHyjfwP",
                "replyto": "Yx1GjggOeo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2312/Reviewer_Hnvb"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2312/Reviewer_Hnvb"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the prompt response"
                    },
                    "comment": {
                        "value": "Thank the authors for their efforts in rebuttal. I have raised my recommendation score."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2312/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700645645055,
                "cdate": 1700645645055,
                "tmdate": 1700645645055,
                "mdate": 1700645645055,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "sl9KRYzdig",
            "forum": "9rPyHyjfwP",
            "replyto": "9rPyHyjfwP",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2312/Reviewer_A1g6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2312/Reviewer_A1g6"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents MOLGEN, a new pre-trained molecular language model dedicated to molecule generation. By reconstructing over 100 million molecular SELFIES, MOLGEN has gained in-depth knowledge of molecular structures and grammar. This understanding is amplified by the domain-agnostic molecular prefix tuning, ensuring better knowledge transferability across a wide range of domains. A crucial feature of the model is the self-feedback mechanism, which safeguards against \"molecular hallucinations\" by ensuring that the model's estimated probabilities align with real-world chemical propensities. Comprehensive evaluation on established benchmarks highlights MOLGEN's superior performance in properties like penalized logP, QED, and molecular docking. Further analysis confirms its adeptness in accurately capturing molecule distributions, discerning intricate structural patterns, and efficiently exploring the chemical space."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper introduces a language model designed for molecule generation, adeptly capturing deep structural and grammatical insights through the reconstruction of over 100 million molecular SELFIES. \n\n2. The paper is well-written and easy to follow. Figures and Tables are very good. \n\n3. Compared to previous baselines, the proposed approach showcases impressive performance. Through comprehensive experiments, the paper convincingly demonstrates that SELFIES is a superior molecular representation to SMILES for 2D molecule generation tasks. This paper also provides very insightful discussions, which can help to understand the model behaviors."
                },
                "weaknesses": {
                    "value": "What potential limitations might the self-feedback mechanism introduce in molecular generation?"
                },
                "questions": {
                    "value": "See Weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2312/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698723011263,
            "cdate": 1698723011263,
            "tmdate": 1699636163553,
            "mdate": 1699636163553,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "52iUssW4M8",
                "forum": "9rPyHyjfwP",
                "replyto": "sl9KRYzdig",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2312/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2312/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you very much for your insightful comments. Below are our responses:\n\n- **Limitations about self-feedback**\n\nCurrently, the self-feedback mechanism in our study is tailored for optimizing a single target objective. As we mentioned in **Appendix B**, this approach exhibits reduced efficacy when applied to scenarios involving multiple objectives. This is primarily due to the generation of divergent preference rankings, which leads to uncertainty in the model's decision-making process about which optimization directions to prioritize. Consequently, an area for future research could be the development of self-feedback techniques that are more adept at handling and optimizing multiple objectives simultaneously.\n\nThank you again for your constructive suggestions! We hope our responses address your concerns."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2312/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700220221120,
                "cdate": 1700220221120,
                "tmdate": 1700220221120,
                "mdate": 1700220221120,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "NohzLY9KqA",
            "forum": "9rPyHyjfwP",
            "replyto": "9rPyHyjfwP",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2312/Reviewer_Vj81"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2312/Reviewer_Vj81"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents MolGen, a domain-agnostic molecular generation model, and its application in generating molecules using the SELFIES molecular language. The paper discusses the ability of MolGen to discern essential substructures and compares it with other molecular generation approaches using the SMILES language. The paper also introduces a self-feedback mechanism to mitigate \"molecular hallucinations\" and improve the generation of molecules with desired properties."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is well-structured with clear writing and is supported by rich and lucid illustrations.\n\n2. The application of SELFIES, as opposed to SMILES, is more concise and effective in deep generative models, facilitating the analysis of generated results. \n\n3. The paper delves deep into the concept of 'Molecular Hallucinations' and attempts to address it, which is a beneficial discussion for the field of molecule generation."
                },
                "weaknesses": {
                    "value": "1. Although an ablation study was conducted to check the self-feedback paradigm, there was no ablation experiment carried out to assess the use of SELFIES over SMILES. Instead, only a comparison was made between Chemformer and MolGen. Technically, a SELFIES-based MolecularLM implemented on BART does not seem irreplaceable. Given that many works involving molecule generation are still based on SMILES (such as MoMu, MolT5), the effectiveness of SELFIES in the work lacks further experimental verification.\n\n2. The significance of the 'domain-agnostic molecular prefix tuning' step is questionable. It seems to be merely a measure to avoid overfitting in the overall model. Whether synthetic molecule generation and natural product generation in drug discovery can be considered two different tasks, and whether other dataset partitioning methods would have similar effects, are not explained. Therefore, the comparison of molecular distribution learning in the paper lacks persuasiveness."
                },
                "questions": {
                    "value": "1. It would be highly beneficial if the authors could conduct further experiments to address the issues raised in the 'Weaknesses' section of this review.\n\n2. Given that molecule generation typically needs to cater to a variety of requirements, have the authors considered other metrics beyond penalized logP, QED, and binding affinity for two human proteins? More persuasive experiments addressing a broader range of molecular properties could significantly enhance the applicability and robustness of the proposed model.\n\n3. The correspondence between the attention scores and the specific molecular structures in Figure 8 and Appendix Figure 5 is not very intuitive. The current figures do not convincingly demonstrate that the SMILES-based PLM is focusing attention on less relevant positions. It would be beneficial if the authors could revise this figure to improve its clarity and interpretability, thereby aiding readers in better understanding the model's inner workings."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2312/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2312/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2312/Reviewer_Vj81"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2312/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699326557927,
            "cdate": 1699326557927,
            "tmdate": 1700703353151,
            "mdate": 1700703353151,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "YicHYzgbS7",
                "forum": "9rPyHyjfwP",
                "replyto": "NohzLY9KqA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2312/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2312/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We sincerely thank you for your insightful feedback. Below are our detailed responses to your concerns: \n\n**1. Effectiveness of SELFIES**\n\nWe apologize for any confusion that may have arisen from our presentation. \n\n- To further clarify, as detailed in Appendix F, we have conducted a comparative analysis using both SMILES and SELFIES formats. In **Table 1, Figure 6, and Figure 8**, Chemformer (S-PLM), which is pre-trained on SMILES and employs a BART architecture, serves as our baseline for comparison. This setup, with MolGen not utilizing the self-feedback mechanism, ensures an unbiased evaluation. The experimental results demonstrate that using SELFIES as the molecular language enables the generation of 100% valid molecules, while also facilitating a more focused attention mechanism in the model.\n- The decision to use SELFIES is based on its standardized and more simplified syntax, which we find to be particularly well-suited for processing by language models. This choice is reflected in our results, where SELFIES ensured the generation of 100% valid molecules, aligning well with our study's objectives. We wish to underscore that SMILES remains a vital and widely adopted format for molecular representation, and our study does not intend to diminish its importance. Instead, our utilization of SELFIES represents a novel exploration, driven by the specific objectives of our research.\n\n**2. Synthetic molecule v.s. natural product generation**\n\nThe division of the molecular dataset into \"synthetic\" and \"natural product\" domains is to effectively explore and understand molecules of varying complexities and origins. The \"synthetic\" domain encompasses artificially synthesized chemical molecules tailored for specific needs, e.g., drug development. On the other hand, the \"natural product\" domain covers molecules naturally occurring, which are pivotal in biological activities and often provide insights for drug development. Natural product molecules generally exhibit greater structural complexity and diversity (further data statistics are available in Appendix C), often resulting from the myriad of unique chemical structures produced through natural biological processes. This classification helps us better understand the unique challenges and features of each domain. \n\nIn our research, we follow the methodologies of prior works[1] for distribution learning, where the baselines focus on synthetic molecule generation. Building upon this foundation, we have extended our scope by including the generation of natural products as a new and more challenging task. This expansion not only enhances the complexity of the tasks we address but also broadens the applicability of our model to a wider range of molecular structures encountered in various scientific domains. Given that both synthetic and natural molecules represent a significant portion of molecules in daily life and research, we posit that our model\u2019s proficiency in these domains indicates its capability to handle various molecular tasks effectively. In response to your suggestion, we have elaborated on this aspect in **Appendix C (Page 16)**.\u000b\n\n**3. Effectiveness of prefix tuning**\n\nWe performed an additional experiment to investigate the impact of prefix tuning **(Highlighted in Appendix H.5, Page 24)**. Here, we present the mean (and standard deviation) of penalized logP improvement for molecules generated compared to inputs under varying similarity constraints, as detailed in the following table.  This data demonstrates that prefix tuning enhances the model's performance in optimizing molecules.\n\n|Method|sim=0.6|sim=0.4|\n|-|-|-|\n|MolGen (w/o prefix)|11.63\u00b10.18|10.23\u00b11.47|\n|MolGen|12.08\u00b10.82 **(+0.45)**|12.35\u00b11.21 **(+2.12)**|\n\nAdditionally, **Figure 8 (Page 9)** provides a comparison of the model's attention to key substructures, both with and without prefix tuning. For enhanced clarity, we have also included a tabular representation of the data visualized in this figure, offering a more direct interpretation of the impact of prefix tuning.\n\n|Model|SAL|\n|-|-|\n|SMILES-based PLM|0.396\u00b10.054|\n|MolGen(w/o prefix)|0.476\u00b10.095 **(+20.2%)**|\n|MolGen|0.495\u00b10.105 **(+25.0%)**|\n\nAs indicated in the aforementioned tables, the implementation of domain-agnostic prefix tuning not only enables the model to more effectively adapt to downstream tasks but also improves its interpretability."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2312/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700220143512,
                "cdate": 1700220143512,
                "tmdate": 1700220143512,
                "mdate": 1700220143512,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XnpKNc3qoc",
                "forum": "9rPyHyjfwP",
                "replyto": "YicHYzgbS7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2312/Reviewer_Vj81"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2312/Reviewer_Vj81"
                ],
                "content": {
                    "title": {
                        "value": "Raise Score"
                    },
                    "comment": {
                        "value": "Thanks to the authors for their response. I have raised my score to 6."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2312/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700703325256,
                "cdate": 1700703325256,
                "tmdate": 1700703325256,
                "mdate": 1700703325256,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]