[
    {
        "title": "Generating Images in Context with Multimodal Large Language Models"
    },
    {
        "review": {
            "id": "PNPZEtkquG",
            "forum": "he6mX9LTyE",
            "replyto": "he6mX9LTyE",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8897/Reviewer_J1rb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8897/Reviewer_J1rb"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a framework that combines MLLM and SD to perform image generation/editing with multimodal input. To better bridge the MLLM output space and SD input space, AlignerNet is introduced for feature alignment. Additionally, a large-scale object compositional image generation data is collected and used for training."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The idea of bridging MLLM and SD for versatile image generation is interesting. MLLM naturally can accept both image and text input, which can provide a more diverse signal to the image generation module and therefore enable new applications.\n2. The newly collected compositional image generation dataset should be useful to the community."
                },
                "weaknesses": {
                    "value": "1. I don't see much novelty from AlignerNet.  Compared with GlueNet, AlignerNet merely replaces the MLP with encoder-decoder Transformers but they have the same loss and the same domains (both aligning text embedding). AlignerNet is useful from the experiments, but not novel IMO.\n2. Since the training data includes the image editing dataset from InstructPix2Pix. A comparison between previous works on image editing benchmarks should also be conducted. Similarly how is kosmos-g compared with GILL in visual storytelling?\n3. In AlignerNet, both MSE and REC losses are used. However, no ablation is done about those two losses. \n4. In Tab2, it seems the E2E Fine-tuning fails. However, recent works such as BLIP-Diffusion, EMU, and MGIE can successfully connect MLLM with SD via E2E fine-tuning without any specific alignment. Why Kosmos-G's behavior is different from others and relies on additional alignment?"
                },
                "questions": {
                    "value": "1. When constructing the compositional generation dataset, what if multiple objects of the same class exist in the same image? Would the corresponding segmentation mask cover multiple instances in the same mask?\n\n-------------- After rebuttal ----------------\nThank the authors for the last-minute efforts. I have raised the rating to 6. Please stick to this new manuscript and the new title in the camera-ready version if accepted, and further improvement in corresponding writing is also encouraged."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8897/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8897/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8897/Reviewer_J1rb"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8897/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698822146752,
            "cdate": 1698822146752,
            "tmdate": 1700764777177,
            "mdate": 1700764777177,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "cnthImh8NH",
                "forum": "he6mX9LTyE",
                "replyto": "PNPZEtkquG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8897/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8897/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response to Reviewer J1rb (Part 1)"
                    },
                    "comment": {
                        "value": "Dear Reviewer J1rb,\n\nThanks a lot for your valuable questions. We will address your concerns one by one.\n\n**Q1:** AlignerNet has the same loss and same domains as GlueGen, which is not novel.\n\n**A1:** Thanks for your comments that \"The idea of bridging MLLM and SD for versatile image generation is interesting. MLLM naturally can accept both image and text input, which can provide a more diverse signal to the image generation module and therefore enable new applications.\" We acknowledge that AlignerNet may not be novel, it is adopted from GlueNet [1] for alignment and modified to deal with variable-length outputs. However, we would like to emphasize that for Kosmos-G, the key idea is not about how to align MLLM with SD. As shown in Figure 6, the rough semantic alignment is far from achieving our objective of \"Image as a foreign language in image generation\". Our main contribution is the idea of leveraging the advanced multimodal perception of MLLM for subject-driven generation. Through instruction tuning the MLLM, we can approach images as a foreign language when prompting Stable Diffusion. We also include additional examples with 3 to 4 image inputs and diverse text interleaving in our revised Appendix D and [here (https://i.ibb.co/BstzDqL/challenge.jpg)](https://i.ibb.co/BstzDqL/challenge.jpg), which shows more about this capability.\n\n**Q2:** Lacks the comparison with previous works on image editing, and comparison with GILL on visual storytelling.\n\n**A2:** We utilize the data from InstructPix2Pix to enable attribute and accessory editing, yet it wouldn't be appropriate to compare with InstructPix2Pix on image editing task. Because we are focusing on subject-driven generation, which does not require preserving the layout of the original input, unlike typical image editing task that requires layout consistency. They are completely two tasks for different purposes. For visual storytelling, we argue that Kosmos-G and GILL [2] are instruction fine-tuned for different purposes. Kosmos-G is fine-tuned on a compositional generation dataset for subject-driven generation, while GILL is fine-tuned on Conceptual Captions (CC3M). Considering the composition nature of our data, our model will not be suitable for visual storytelling, while GILL also struggles with subject-driven generation and composition. However, given the aligned multimodal perception, our method possesses the potential to deal with visual storytelling when trained with interleaved images and captions following GILL.\n\n**Q3:** No ablation is done about MSE and REC loss\n\n**A3:** As shown below or in the revised Appendix C, we present a comprehensive set of ablation studies on the COCO-30k, focusing on the model design and supervision methods used for AlignerNet. We observed that using MSE loss alone leads to slightly worse results, which is consistent with the observation from GlueGen [1].\n\n### Table 1: FID Score $\\downarrow$ on COCO-30k with a guidance scale of 3.0: measuring the fidelity of generated images\n\n| Methods                 | MSE + REC | MSE    | Diff   |\n| ----------------------- | --------- | ------ | ------ |\n| Linear                  | Failed    | Failed | Failed |\n| MLP                     | Failed    | Failed | Failed |\n| Perceiver Resampler [2] | Failed    | Failed | Failed |\n| 12-Layers Decoder       | Failed    | -      | -      |\n| 12-Layers AlignerNet    | 9.89      | 10.23  | 11.30  |\n| 24-Layers AlignerNet    | 9.55      | -      | -      |\n\n### Table 2: CLIP Score $\\uparrow$ on COCO-30k with a guidance scale of 3.0: measuring the alignment between text input and image output\n\n| Methods                 | MSE + REC | MSE    | Diff   |\n| ----------------------- | --------- | ------ | ------ |\n| Linear                  | Failed    | Failed | Failed |\n| MLP                     | Failed    | Failed | Failed |\n| Perceiver Resampler [3] | Failed    | Failed | Failed |\n| 12-Layers Decoder       | Failed    | -      | -      |\n| 12-Layers AlignerNet    | 25.48     | 25.31  | 24.22  |\n| 24-Layers AlignerNet    | 25.57     | -      | -      |\n\n**Q4:** Why E2E Fine-tuning fails.\n\n**A4:** Thanks a lot for this important inquiry, we will clarify it here. E2E Fine-tuning in the paper means we directly connect MLLM with SD and e2e fine-tuning the MLLM. As shown in the above table, direct e2e fine-tuning is feasible with the help of AlignerNet. While it is more costly because we need to pass it through U-Net, leading to a worse performance under the same GPU days. So in practice, we choose to add a specific alignment to accelerate training."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8897/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700414094189,
                "cdate": 1700414094189,
                "tmdate": 1700507467741,
                "mdate": 1700507467741,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gv2TgDWiTH",
                "forum": "he6mX9LTyE",
                "replyto": "PNPZEtkquG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8897/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8897/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response to Reviewer J1rb (Part 2)"
                    },
                    "comment": {
                        "value": "**Q5:** When constructing the compositional generation dataset, what if multiple objects of the same class exist in the same image? Would the corresponding segmentation mask cover multiple instances in the same mask?\n\n**A5:** Thank you for this insightful question. There do exist cases that multiple instances of the same object class appear in a single image. We simply choose the largest connected sub-region. This approach is intuitive as it picks the clearest, unobstructed object for training. It is also consistent with the inference usage, as we represent each object in the prompt using a single image.\n\n## Reference\n\n[1] Qin, C., Yu, N., Xing, C., Zhang, S., Chen, Z., Ermon, S., ... & Xu, R. (2023). GlueGen: Plug and Play Multi-modal Encoders for X-to-image Generation. *arXiv preprint arXiv:2303.10056*.\n\n[2] Koh, J. Y., Fried, D., & Salakhutdinov, R. (2023). Generating images with multimodal language models. *arXiv preprint arXiv:2305.17216*.\n\n[3] Alayrac, J. B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., ... & Simonyan, K. (2022). Flamingo: a visual language model for few-shot learning. *Advances in Neural Information Processing Systems*, *35*, 23716-23736."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8897/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700414110144,
                "cdate": 1700414110144,
                "tmdate": 1700414110144,
                "mdate": 1700414110144,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VMSq4d9mgg",
                "forum": "he6mX9LTyE",
                "replyto": "PNPZEtkquG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8897/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8897/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reviewer-Author Discussion Period Ends in TWO Days"
                    },
                    "comment": {
                        "value": "Thanks again for reviewing our paper. We hope that we were able to address your concerns in our response. As the deadline is approaching, please let us know if you have any further questions before the reviewer-author discussion period ends. We are glad to address your further concerns."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8897/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700531555611,
                "cdate": 1700531555611,
                "tmdate": 1700531555611,
                "mdate": 1700531555611,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8OfpFZsRse",
                "forum": "he6mX9LTyE",
                "replyto": "PNPZEtkquG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8897/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8897/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Final Day for Discussion: We Hope to Have You in Our Discussion"
                    },
                    "comment": {
                        "value": "Thank you again for dedicating your time to reviewing our paper. We understand the discussion time is limited, yet we do hope to have your participation in our Reviewer-Author discussion. It means a lot to us to fully address your concern and improve our paper during the discussion period. Even within the remaining one-day discussion period, we are still happy to know your further concerns and will do our utmost to resolve them."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8897/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700619571313,
                "cdate": 1700619571313,
                "tmdate": 1700619571313,
                "mdate": 1700619571313,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FsoJHs4bcn",
                "forum": "he6mX9LTyE",
                "replyto": "PNPZEtkquG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8897/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8897/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Final Reminder for Discussion: 7 Hours Left until the Deadline"
                    },
                    "comment": {
                        "value": "Dear reviewer: As the deadline draws near, we kindly request your feedback on our rebuttal. We are eager to engage in further discussion and address any additional concerns you may have."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8897/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700716233341,
                "cdate": 1700716233341,
                "tmdate": 1700716233341,
                "mdate": 1700716233341,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DlNp6uh7Gj",
                "forum": "he6mX9LTyE",
                "replyto": "gv2TgDWiTH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8897/Reviewer_J1rb"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8897/Reviewer_J1rb"
                ],
                "content": {
                    "title": {
                        "value": "Re: Official Response"
                    },
                    "comment": {
                        "value": "Thank the authors for the detailed response. I am satisfied with the clarification about E2E fine-tuning as well as the main focus of this paper being positioned as `subject-driven generation`. \n\nHowever, the title `GENERATING IMAGES IN CONTEXT WITH MULTIMODAL LARGE LANGUAGE MODELS` is a bit too broad, more than just subject-driven generation. GILL and many other works can also be umbrellaed under this title. Also, many related descriptions in the introduction/abstract need to be revised to more faithfully reflect the uniqueness and main functionality of your work, eg, the caption in Fig1: `It can perceive generalized vision-language inputs that span multiple images and faithfully generate images.` might not be accurate since it cannot conduct faithful image editing and visual story-telling with multiple-image input. \n\nOverall, I appreciate the author(s)' efforts and insights in this work. **I would raise my score if the author(s) can revise their manuscript correspondingly.**"
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8897/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700720174562,
                "cdate": 1700720174562,
                "tmdate": 1700720174562,
                "mdate": 1700720174562,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "8HHYBUN6YG",
            "forum": "he6mX9LTyE",
            "replyto": "he6mX9LTyE",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8897/Reviewer_VSX4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8897/Reviewer_VSX4"
            ],
            "content": {
                "summary": {
                    "value": "This paper explores image generation from generalized vision-language inputs, especially involving multiple images. Named KOSMOS-G, a model that leverages the advanced perception capabilities of MLLMs, and aligns the output space of MLLM with CLIP using the textual modality as an anchor and performs compositional instruction tuning on curated data."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The problem of image generation conditioning on generalized vision-language inputs is an interesting problem, and the proposed approach seems to show some promising results.\nThe idea of aligning KOSMOS-G Space with the CLIP-T Space and then directly leveraging the stable diffusion models seems a valid approach.\nThe qualitative results show some good capabilities of the proposed method."
                },
                "weaknesses": {
                    "value": "The ablation study seems not very comprehensive, for example, if the goal is to align the two representation spaces, there should be other options to achieve the alignment design, so why is the current AlignerNet design the best, maybe more justification and ablation study are needed here."
                },
                "questions": {
                    "value": "see the weakness part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8897/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698908518284,
            "cdate": 1698908518284,
            "tmdate": 1699637119450,
            "mdate": 1699637119450,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "L6mbnTD0IN",
                "forum": "he6mX9LTyE",
                "replyto": "8HHYBUN6YG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8897/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8897/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response to Reviewer VSX4"
                    },
                    "comment": {
                        "value": "Dear Reviewer VSX4,\n\nThanks a lot for your valuable feedback and advice.\n\n**Q:** More justification and ablation study for AlignerNet\n\n**A:** In the revised Appendix C, we have expanded the ablation study for AlignerNet on the COCO-30k dataset, focusing on its model design and supervision. We investigated various configurations, including the use of MSE loss, reconstruction loss, and diffusion loss in supervising the aligned network, while keeping other modules such as the MLLM frozen. \n\nThe results demonstrate that simple networks such as Linear, MLP, and Resampler fail to achieve satisfactory alignments. Moreover, due to MLLM's variable-length output, MLP-based GlueNet [1] and encoder-only Transformer models are unsuitable for learning alignment. Decoder-only architecture also fails due to its inefficiency in modeling dimension transformations.\n\nFor loss functions, using only MSE loss results in worse results, which is consistent with findings from GlueGen [1]. Direct end-to-end training is more costly because we need to pass it through U-Net, leading to worse performance under the same GPU days. \n\nNotably, AlignerNet surpasses the performance of GlueNet+T5. For instance, GlueNet + T5 attained CLIP scores of 20.67, 23.24, and 23.74 on the COCO-5k dataset at guidance scales of 1.5, 5, and 7.5 respectively, and a FID score of 14.32 at a guidance scale of 3.0 on COCO-30k. These results highlight the effectiveness of AlignerNet in achieving superior alignment, validating the soundness of its design and supervision.\n\n### Table 1: FID Score $\\downarrow$ on COCO-30k with a guidance scale of 3.0: measuring the fidelity of generated images\n\n| Methods               | MSE + REC | MSE   | Diff   |\n| --------------------- | --------- | ------ | ------ |\n| Linear                | Failed    | Failed | Failed |\n| MLP                   | Failed    | Failed | Failed |\n| Perceiver Resampler [2] | Failed    | Failed | Failed |\n| 12-Layers Decoder     | Failed | -      | -      |\n| 12-Layers AlignerNet  | 9.89     | 10.23 | 11.30 |\n| 24-Layers AlignerNet  | 9.55     | -      | -      |\n\n### Table 2: CLIP Score $\\uparrow$ on COCO-30k with a guidance scale of 3.0: measuring the alignment between text input and image output\n\n| Methods               | MSE + REC | MSE   | Diff |\n| --------------------- | --------- | ------ | ------ |\n| Linear                | Failed    | Failed | Failed |\n| MLP                   | Failed    | Failed | Failed |\n| Perceiver Resampler [2] | Failed    | Failed | Failed |\n| 12-Layers Decoder     | Failed | -      | -     |\n| 12-Layers AlignerNet  | 25.48   | 25.31 | 24.22 |\n| 24-Layers AlignerNet  | 25.57   | -      | -     |\n\n## Reference\n\n[1] Qin, C., Yu, N., Xing, C., Zhang, S., Chen, Z., Ermon, S., ... & Xu, R. (2023). GlueGen: Plug and Play Multi-modal Encoders for X-to-image Generation. *arXiv preprint arXiv:2303.10056*.\n\n[2] Alayrac, J. B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., ... & Simonyan, K. (2022). Flamingo: a visual language model for few-shot learning. *Advances in Neural Information Processing Systems*, *35*, 23716-23736."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8897/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700414049132,
                "cdate": 1700414049132,
                "tmdate": 1700414049132,
                "mdate": 1700414049132,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CrXjBlYWxx",
                "forum": "he6mX9LTyE",
                "replyto": "8HHYBUN6YG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8897/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8897/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reviewer-Author Discussion Period Ends in TWO Days"
                    },
                    "comment": {
                        "value": "Thanks again for reviewing our paper. We hope that we were able to address your concerns in our response. As the deadline is approaching, please let us know if you have any further questions before the reviewer-author discussion period ends. We are glad to address your further concerns."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8897/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700531537235,
                "cdate": 1700531537235,
                "tmdate": 1700531537235,
                "mdate": 1700531537235,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bmGFAjQTs5",
                "forum": "he6mX9LTyE",
                "replyto": "8HHYBUN6YG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8897/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8897/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Final Day for Discussion: We Hope to Have You in Our Discussion"
                    },
                    "comment": {
                        "value": "Thank you again for dedicating your time to reviewing our paper. We understand the discussion time is limited, yet we do hope to have your participation in our Reviewer-Author discussion. It means a lot to us to fully address your concern and improve our paper during the discussion period. Even within the remaining one-day discussion period, we are still happy to know your further concerns and will do our best to resolve them."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8897/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700619557893,
                "cdate": 1700619557893,
                "tmdate": 1700619557893,
                "mdate": 1700619557893,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "IPr9lZts8l",
                "forum": "he6mX9LTyE",
                "replyto": "bmGFAjQTs5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8897/Reviewer_VSX4"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8897/Reviewer_VSX4"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the clarifications. I've read other reviews and rebuttals."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8897/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700695446162,
                "cdate": 1700695446162,
                "tmdate": 1700695446162,
                "mdate": 1700695446162,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aaUXUtL73n",
                "forum": "he6mX9LTyE",
                "replyto": "8HHYBUN6YG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8897/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8897/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Manuscript Updated"
                    },
                    "comment": {
                        "value": "Dear reviewer VSX4:\n\nThanks again for your valuable feedback, we have updated our manuscript to reflect your feedback with all changes highlighted using blue color. You can find the changelog in the main thread. We are still eager to engage in further discussion and address any additional concerns you may have. We would be very grateful if you could raise your rating accordingly."
                    }
                },
                "number": 24,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8897/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700733844224,
                "cdate": 1700733844224,
                "tmdate": 1700733886820,
                "mdate": 1700733886820,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "NIU5SmWmPy",
            "forum": "he6mX9LTyE",
            "replyto": "he6mX9LTyE",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8897/Reviewer_hrQd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8897/Reviewer_hrQd"
            ],
            "content": {
                "summary": {
                    "value": "Kosmos-G aligns the outputs of MLLM to the embedding space of CLIP text encoder, which can be fed into Stable Diffusion model for image generation with context of any form."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The KOSMOS-G's ability to achieve zero-shot multi-entity subject-driven generation is notable. The method addresses an underexplored area in image generation by focusing on generalized vision-language inputs and multiple images, the method leverage existing advancements in both multimodal language models and image generation. \n\n2. By the alignment of the output space of MLLMs with CLIP and Score distillation instruction tuning, KOSMOS-G can achieve subject-driven generation and image editing without any training on diffusion models, highlighting its potential for integration into different frameworks."
                },
                "weaknesses": {
                    "value": "1. The paper repeatedly mentions KOSMOS-G's ability to master zero-shot multi-entity generation and handle interleaved image-text input. However, the practical cases presented in the paper seem to focus primarily on image editing capabilities. Look forward to showing more cases with complex and rich scenarios to further illustrate the capabilities of the model. : 1\uff09the paper only demonstrates cases with a maximum of two images, failing to showcases with more than two images as inputs.  2) the paper predominantly showcases and evaluates image-text-image input scenarios, leaving more diverse multi-image and text interleaving cases unexplored. \n\n2. Section 2.3 discusses the \"Score distillation instruction tuning\" technique, but the description lacks clarity. The paper should provide a more precise definition of the entities involved in calculating the KL divergence, along with any specific mathematical formulas or equations for better understanding.  In addition, is KL divergence loss necessary? Is it feasible to directly apply diffusion model's loss for training?\n\n3. The paper highlights the exceptional subject-driven generation capabilities of KOSMOS-G, particularly when not training the diffusion model.   I would like to ask if the authors have explored the possibility of further enhancing subject-driven generation, like training the diffusion model."
                },
                "questions": {
                    "value": "See above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8897/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8897/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8897/Reviewer_hrQd"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8897/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699534602875,
            "cdate": 1699534602875,
            "tmdate": 1699637119297,
            "mdate": 1699637119297,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "bk1cgwETm0",
                "forum": "he6mX9LTyE",
                "replyto": "NIU5SmWmPy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8897/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8897/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response to Reviewer hrQd (Part 1)"
                    },
                    "comment": {
                        "value": "Dear Reviewer hrQd,\n\nThanks a lot for your insightful feedback and kind advice. We would like to address your concerns one by one.\n\n**Q1:** The paper does not demonstrate scenarios involving more than two images and lacks exploration of diverse multi-image and text interleaving cases.\n\n**A1:** Thank you for your valuable suggestion. We acknowledge the importance of showcasing more complex scenarios in demonstrating the capabilities of Kosmos-G. It's worth noting that subject-driven generation across multiple images, especially more than two, is challenging. This is true even for state-of-the-art fine-tuning methods like Break-a-Scene [1], which supports up to three subjects. Zero-shot methods like FastComposer [2] and Subject-Diffusion [3] can handle up to two images but struggle with attribute and accessory editing and also require attention map manipulation. Owning to our collected training data, it is possible to treat the image input as a text token and prompt Kosmos-G like Stable Diffusion. This unique feature allows for more diverse and complex multi-image and text interleaving scenarios. We have included additional examples with 3 to 4 image inputs and diverse text interleaving in our revised Appendix D and [here (https://i.ibb.co/BstzDqL/challenge.jpg)](https://i.ibb.co/BstzDqL/challenge.jpg). These examples demonstrate Kosmos-G's robust performance in handling these challenging cases.\n\n**Q2:** \"Score distillation instruction tuning\" lacks clarity. Is it feasible to directly apply diffusion model's loss for training?\n\n**A2:** Thank you for your question regarding the clarity of \"Score Distillation Instruction Tuning\". We would like to emphasize that during the instruction tuning stage, we still utilize the diffusion loss for model training. However, this loss can also be regarded as score distillation, and it is effectively equivalent to optimizing the diffusion loss as the SDS loss. We will elaborate on these in the following text.\n\nOur objective is to distill the learned score function from the Stable Diffusion U-Net into Kosmos-G. This process enables Kosmos-G to encode image features into embeddings that the Stable Diffusion model can understand for subject-driven generation. Essentially, this approach is like pre-training a generalized textual inversion [4] model, with all conditions learnable by model-seeking.\n\nConsider the Kosmos-G model, denoted as $\\phi$, which takes an input $\\mathbf{x}$ and produces an output $\\mathcal{C}=\\phi(\\mathbf{x})$. Alongside this, we have the Diffusion U-Net, represented as $\\theta$. In our process, we optimize the Kosmos-G model $\\phi$ using the diffusion loss, while keeping the parameters of the Diffusion U-Net $\\theta$ frozen. The diffusion loss is expressed as follows:\n\n$$\n\\mathcal{L} _{diff}(\\phi) = \\mathbb{E} _{\\mathbf{z} _0, \\boldsymbol{\\epsilon} \\sim \\mathcal{N}(0, 1), t} \\Big[w(t)\\|\\boldsymbol{\\epsilon} _\\theta(\\mathbf{z} _t; \\mathcal{C}, t) - \\boldsymbol{\\epsilon}\\|^2 \\Big]\n$$\n\nconsider the gradient of $\\mathcal{L} _{diff}$:\n$$\n\\nabla _\\phi \\mathcal{L} _{diff}(\\phi) = \\mathbb{E} _{\\mathbf{z} _0, \\boldsymbol{\\epsilon} \\sim \\mathcal{N}(0, 1), t} \\Bigg[w(t)\n\\underbrace{\\left(\\boldsymbol{\\epsilon} _\\theta(\\mathbf{z} _t; \\mathcal{C}, t)  - \\boldsymbol{\\epsilon} \\right) \\vphantom{\\partial \\mathcal{C} \\over \\partial \\phi}} _{\\text{Noise Residual}} \n\\underbrace{{\\partial \\boldsymbol{\\epsilon} _\\phi(\\mathbf{z} _t; \\mathcal{C}, t) \\over \\mathcal{C}} \\vphantom{\\partial \\mathcal{C} \\over \\partial \\phi}} _{\\text{U-Net Jacobian}}\n\\underbrace{{\\partial \\mathcal{C} \\over \\partial \\phi}} _{\\text{Kosmos-G Jacobian}}\\Bigg]\n$$\n\nFollowing the approach in Dreamfusion [6], we can simplify this equation by omitting certain terms, leading to the SDS loss for Kosmos-G:\n$$\n\\nabla _\\phi \\mathcal{L} _{SDS}(\\phi) \\triangleq \\mathbb{E} _{\\mathbf{z} _0, \\boldsymbol{\\epsilon} \\sim \\mathcal{N}(0, 1), t} \\Bigg[w(t)\\left(\\boldsymbol{\\epsilon} _\\theta(\\mathbf{z} _t; \\mathcal{C}, t) - \\boldsymbol{\\epsilon} \\right) {\\partial \\mathcal{C} \\over \\partial \\phi}\\Bigg]\n$$\n\nAs established in Dreamfusion [6], optimizing the SDS loss is effectively equivalent to optimizing the diffusion loss when the U-Net $\\theta$ is frozen. From the perspective of score distillation, when using diffusion loss, the KL divergence defined by conditions and the pre-learned score function is equivalently minimized for distilling learned probability density in conditional image synthesis [5, 7]:\n$$\n\\mathop{\\min}\\limits _{\\phi} \\mathcal{L} _{Diff}(\\phi) = \\mathbb{E} _{\\mathbf{z} _0, t, \\mathcal{C}} \\Big[D _{\\rm{KL}}\\big(q(\\mathbf{z} _{t-1}|\\mathbf{z} _t, \\mathbf{z} _{0})~\\| ~p _\\theta(\\mathbf{z} _{t-1}|\\mathbf{z} _t; \\mathcal{C})\\big)\\Big]\n$$"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8897/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700413946575,
                "cdate": 1700413946575,
                "tmdate": 1700413946575,
                "mdate": 1700413946575,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ucZ7el4eMo",
                "forum": "he6mX9LTyE",
                "replyto": "NIU5SmWmPy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8897/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8897/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response to Reviewer hrQd (Part 2)"
                    },
                    "comment": {
                        "value": "**Q3:** The possibility of further enhancing subject-driven generation, like training the diffusion model.\n\n**A3:** Thank you for your valuable suggestion regarding the enhancement of subject-driven generation through training the diffusion model. We designed Kosmos-G to function as a minimal, plug-and-play module within Stable Diffusion, offering a more community-friendly approach compared to a fully modularized system. This design choice allows Kosmos-G to seamlessly replace CLIP, facilitating easier integration with other U-Net techniques, such as ControlNet and LoRA, as demonstrated in our paper. While we acknowledge that some research, like BLIP-Diffusion [8], involves training the diffusion model, our approach as outlined in Table 1 achieves superior performance without the need to train the U-Net. We agree the subject-driven generation capability can be improved by tuning more parameters. However, our current resources, specifically V100 GPUs with 32GB VRAM, constrain our ability to fully explore the outcomes of training Kosmos-G with an unlocked U-Net.\n\n## Reference:\n\n[1] Avrahami, O., Aberman, K., Fried, O., Cohen-Or, D., & Lischinski, D. (2023). Break-A-Scene: Extracting Multiple Concepts from a Single Image. *arXiv preprint arXiv:2305.16311*.\n\n[2] Xiao, G., Yin, T., Freeman, W. T., Durand, F., & Han, S. (2023). FastComposer: Tuning-Free Multi-Subject Image Generation with Localized Attention. *arXiv preprint arXiv:2305.10431*.\n\n[3] Ma, J., Liang, J., Chen, C., & Lu, H. (2023). Subject-diffusion: Open domain personalized text-to-image generation without test-time fine-tuning. *arXiv preprint arXiv:2307.11410*.\n\n[4] Gal, R., Alaluf, Y., Atzmon, Y., Patashnik, O., Bermano, A. H., Chechik, G., & Cohen-Or, D. (2022). An image is worth one word: Personalizing text-to-image generation using textual inversion. *arXiv preprint arXiv:2208.01618*.\n\n[5] Dong, R., Han, C., Peng, Y., Qi, Z., Ge, Z., Yang, J., ... & Yi, L. (2023). Dreamllm: Synergistic multimodal comprehension and creation. *arXiv preprint arXiv:2309.11499*.\n\n[6] Poole, B., Jain, A., Barron, J. T., & Mildenhall, B. (2022). Dreamfusion: Text-to-3d using 2d diffusion. *arXiv preprint arXiv:2209.14988*.\n\n[7] Luo, C. (2022). Understanding diffusion models: A unified perspective. *arXiv preprint arXiv:2208.11970*.\n\n[8] Li, D., Li, J., & Hoi, S. C. (2023). Blip-diffusion: Pre-trained subject representation for controllable text-to-image generation and editing. *arXiv preprint arXiv:2305.14720*."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8897/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700413979174,
                "cdate": 1700413979174,
                "tmdate": 1700413979174,
                "mdate": 1700413979174,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "yElz8uGBpS",
                "forum": "he6mX9LTyE",
                "replyto": "NIU5SmWmPy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8897/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8897/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reviewer-Author Discussion Period Ends in TWO Days"
                    },
                    "comment": {
                        "value": "Thanks again for reviewing our paper. We hope that we were able to address your concerns in our response. As the deadline is approaching, please let us know if you have any further questions before the reviewer-author discussion period ends. We are glad to address your further concerns."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8897/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700531517669,
                "cdate": 1700531517669,
                "tmdate": 1700531517669,
                "mdate": 1700531517669,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Cl7Br2tPRX",
                "forum": "he6mX9LTyE",
                "replyto": "NIU5SmWmPy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8897/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8897/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Final Day for Discussion: We Hope to Have You in Our Discussion"
                    },
                    "comment": {
                        "value": "Thank you again for dedicating your time to reviewing our paper. We understand the discussion time is limited, yet we do hope to have your participation in our Reviewer-Author discussion. It means a lot to us to fully address your concern and improve our paper during the discussion period. Even within the remaining one-day discussion period, we are still happy to know your further concerns and will do our utmost to resolve them."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8897/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700619534409,
                "cdate": 1700619534409,
                "tmdate": 1700619534409,
                "mdate": 1700619534409,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "oTCvToSYw5",
                "forum": "he6mX9LTyE",
                "replyto": "NIU5SmWmPy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8897/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8897/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Final Reminder for Discussion: 7 Hours Left until the Deadline"
                    },
                    "comment": {
                        "value": "Dear reviewer: As the deadline draws near, we kindly request your feedback on our rebuttal. We are eager to engage in further discussion and address any additional concerns you may have."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8897/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700716213339,
                "cdate": 1700716213339,
                "tmdate": 1700716213339,
                "mdate": 1700716213339,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HpffCrg8Eg",
                "forum": "he6mX9LTyE",
                "replyto": "NIU5SmWmPy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8897/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8897/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Manuscript Updated"
                    },
                    "comment": {
                        "value": "Dear reviewer hrQd:\n\nThank you once again for your valuable feedback, we have updated our manuscript to reflect your feedback with all changes highlighted using blue color. You can find the changelog in the main thread. We are still eager to engage in further discussion and address any additional concerns you may have. If you find our response addressed your concern, we would deeply appreciate it if you could consider raising our rating."
                    }
                },
                "number": 25,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8897/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700733996493,
                "cdate": 1700733996493,
                "tmdate": 1700733996493,
                "mdate": 1700733996493,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]