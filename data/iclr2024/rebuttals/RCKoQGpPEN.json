[
    {
        "title": "MaXTron: Mask Transformer with Trajectory Attention for Video Panoptic Segmentation"
    },
    {
        "review": {
            "id": "sOzYFG5NiQ",
            "forum": "RCKoQGpPEN",
            "replyto": "RCKoQGpPEN",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission971/Reviewer_tzju"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission971/Reviewer_tzju"
            ],
            "content": {
                "summary": {
                    "value": "This paper tackles the well-known video task, video panoptic segmentation, and presents MaXTron. From the inherent property of the VPS task that comprises other video segmentation tasks such as VSS and VIS, MaXTron can be considered as a general video segmentation framework.\nThe authors points out a couple of challenges of the video segmentation tasks, and target to alleviate such challenges. Specifically, per-clip segmentation methods (which MaXTron also belongs to) have put efforts in improving inter-clip and intra-clip predictions. In order to improve inter-clip predictions, the authors suggest the Within-Clip Tracking Module, which consists of a stack of multi-scale deformable attention followed by Axial-Trajectory Attentions. For intra-clip association, MaXTron fully leverages the object queries that possess object-level information, and insert into the Cross-Clip Tracking Module that has Trajectory Attention and Temporal ASPP.\nFinally, utilizing the presented modules, MaXTron achieves compelling results, demonstrating state-of-the-art accuracy."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "This paper has a clear structure that ease the readers to follow and understand which components are being used. \nThe authors points out important problems of the video segmentation tasks and each module is designed with a specific goal.\nCombining them all, MaXTron achieves improvements in the accuracy on multiple benchmarks, highlighting the effectiveness of each module."
                },
                "weaknesses": {
                    "value": "The major weakness of this paper is lack of novelty. Each component used in the design of MaXTron is mostly brought from existing literatures or with a subtle modification.\nFor instance, Multi-level Deformable Attention, Transformer Decoder, and ASPP are brought from previous works.\n\nThe used modules with slight changes are 1) Axial-Trajectory Attention and 2) Cross Clip Tracking Module.\nThe Axial-Trajectory Attention manipulates the set of tokens fed into transformer attentions, which has been widely used in lots of different areas.\nAdditionally, Cross Clip Tracking Module is a simple modification to VITA, using clip-level outputs instead of frame-level outputs.\n\nAs this paper used a number of components that are already proven effective, it is rather expected to see the gain in the accuracy."
                },
                "questions": {
                    "value": "How much are the FLOPs and FPS of MaXTron compared to other methods?\nWhat's the statistical significance, i.e. how many runs were executed for reporting the numbers? Are the numbers mean/median of multiple trials?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission971/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission971/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission971/Reviewer_tzju"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission971/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698034161558,
            "cdate": 1698034161558,
            "tmdate": 1699636023084,
            "mdate": 1699636023084,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "RXRidseV7B",
                "forum": "RCKoQGpPEN",
                "replyto": "sOzYFG5NiQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission971/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission971/Authors"
                ],
                "content": {
                    "title": {
                        "value": "To Reviewer tzju (1/2)"
                    },
                    "comment": {
                        "value": "We thank Reviewer tzju for the review, and we address the concerns below.\n\n> W1: Some components are borrowed from literature.\n\nWe thank the reviewer for the question. We respectfully disagree with the reviewer and clarify the concerns of each component as below:\n\n>> W1.1: Multi-scale Deformable Attention\n\nMulti-scale Deformable Attention (MSDeformAttn) have been introduced in [1], and used in most recent works. We introduce MSDeformAttn in our within-clip tracking module for a fair comparison to other state-of-the-art methods. Most importantly, we show below that though MSDeformAttn brings improvement to performance, the most significant improvement is still brought by our own proposed axial-trajectory attention.\n\nFirstly, we provide additional results on VIPSeg with ResNet-50 as backbone and ablate on the effectiveness of multi-scale deformable attention and the proposed axial-trajectory attention in terms of VPQ:\n\nMSDeformAttn | axial-trajectory attention | VPQ |\n|:---------:|:----------:|:--------------:|\nX     |    X    |      42.7      |\n$\\checkmark$     |    X    |      44.5      |\nX     |    $\\checkmark$    |      44.9      |\n$\\checkmark$     |    $\\checkmark$    |      46.1      |\n\nAs can be seen, when compared with the baseline Video-kMaX, adding MSDeformAttn only brings 1.8 VPQ improvement, adding axial-trajectory attention brings 2.2 VPQ improvement. When combining together, we get the final 3.4 VPQ improvement. We also emphasize that Video-kMaX with axial-trajectory attention alone already achieves significant improvement compared to other state-of-the-art methods (Video-kMaX with axial-trajectory attention alone 44.9 vs. current state-of-the-art DVIS 39.2). Please refer to Tab. 5 for more comparisons with different within-clip tracking module designs.\n\nSecondly, for video instance segmentation, please note that Tube-Link already contains MSDeformAttn in their own clip-level segmenter design and we simply add the proposed axial-trajectory attention after their MSDeformAttn. As a result, all performance gains on the near-online setting are brought by the proposed axial-trajectory attention. To be concrete, it brings 0.6 AP, 4.4 $AP^\\text{long}$, 5.8 AP improvement when evaluating on Youtube-VIS-21, Youtube-VIS-22 and OVIS dataset with ResNet-50, ResNet-50, and Swin-L as backbone, respectively. Please refer to Tab. 2, 8, 9 and 10 for more comparisons.\n\nIn summary, MSDeformAttn does help with the within-clip tracking module, but it is not the core component in it and we do not claim it as our novelty as well. Most importantly, even without MSDeformAttn, our model still brings non-trivial improvement.\n\n>> W1.2: Transformer Decoder\n\nWe kindly request the reviewer for clarification on the point that *'Transformer Decoder is borrowed from the literature'.* As far as we are concerned, the transformer decoder is an essential and non-removable component for modern video segmentation models and all other works use it. If the reviewer insists on this point, we kindly request the reviewer for more clarifications, e.g., why the follow-up works should not (and never) use the Transformer Decoder.\n\n>> W1.3: ASPP\n\nASPP is introduced by [2], and originally operates on the dense pixels in the ***spatial*** domain. As far as we are concerned, we are the first to extend it to the ***temporal*** domain. Motivated by its property in capturing local information, we extend it to operate on the clip object query to reason about the temporal information. This is a non-trivial adaptation of ASPP.\n\nBesides, Tab. 6 (6) in the paper shows that even without ASPP, our usage of trajectory attention on clip object query still brings performance improvement due to the capture of global cross-clip interactions. \n\n[1] Zhu, Xizhou, et al. \"Deformable DETR: Deformable Transformers for End-to-End Object Detection\". ICLR 2021 (Oral)\n\n[2] Chen, Liang-Chieh, et al. \"DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs\". TPAMI 2017"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission971/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700081768434,
                "cdate": 1700081768434,
                "tmdate": 1700084594737,
                "mdate": 1700084594737,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "EEIdJcwwmv",
                "forum": "RCKoQGpPEN",
                "replyto": "sOzYFG5NiQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission971/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission971/Authors"
                ],
                "content": {
                    "title": {
                        "value": "To Reviewer tzju (2/2)"
                    },
                    "comment": {
                        "value": "> W2: Some components are modified with slight changes.\n\nWe thank the reviewer for the question. We respectfully disagree with the reviewer and clarify the concerns of each component as below:\n\n>> W2.1: Axial-Trajectory Attention\n\nWe respectfully disagree with the reviewer with the point that *'the Axial-Trajectory Attention manipulates the set of tokens fed into transformer attentions, which has been widely used in lots of different areas'.* As far as we are concerned, ***all*** attention mechanisms manipulate the set of tokens thus it is unfair to blame the design on this. Most importantly, it is non-trivial to adapt trajectory attention to video segmentation task. Though different self-attention mechanisms have been explored a lot in **video classification**, few works try to explore it in **video segmentation** due to the intolerant complexity. Our novel design effectively addresses this key challenge and improves within-clip consistency. Please refer to C1 in common concerns for more discussion on this.\nFinally, we kindly request the reviewer to re-evaluate our contributions. If the reviewer insists on the point that it is trivial to design ***axial-trajectory attention***, we kindly request the reviewer for any reference papers that have ever successfully done so in literature.\n\n>> W2.2: Cross-clip Tracking Module (Comparison with VITA)\n\nOur cross-clip tracking module is not a simple modification from VITA. We clarify the detailed differences compared to VITA below:\n\n1. VITA introduces an **additional** set of video object queries on top of the existing frame object queries, while MaXTron instead directly manipulates the clip object queries.\n\n2. VITA contains both an encoder and a decoder to encode the information from frame object queries and decode the information from them via video object queries. As a comparison, MaXTron only employs a simple encoder to aggregate information from clip object queries. \n\nConcretely, the operation flow of VITA is:\nframe query self-attention $\\rightarrow$ frame query feed-forward network $\\rightarrow$ video query cross-attention to frame query $\\rightarrow$ video query self-attention $\\rightarrow$ video query feed-forward network\n\nThe operation flow of cross-clip tracking module in MaXTron is:\nclip query trajectory-attention $\\rightarrow$ clip query Temporal-ASPP\n\nAs can be seen, the proposed cross-clip tracking module enjoys a ***much simpler*** pipeline compared to VITA.\n\n3. VITA uses their proposed video object queries to predict both the whole video mask and class directly, while MaXTron exploits clip object queries to predict clip mask ***separately*** (the final whole video mask is obtained by concatenating the clip masks) and predicts the class by ***weighted mean*** of aligned clip queries, motivated by the fact that the aligned clip queries should have consistent class prediction. \nWe argue that the proposed per-clip mask prediction scheme leads to better video mask prediction, since it is challenging to use only one set of video queries to directly predict the mask for the whole video. We provide additional ablation study on this to support our claim by experimenting with MaXTron w/ Video-kMaX with ResNet-50 as backbone on VIPSeg:\n\nmask prediction scheme | VPQ |\n|:---------:|:----------:|\nper-clip   (adopted by us)  |    46.7    |\nper-video (adopted by VITA)    |    46.4    |\n\n4. Finally, VITA introduces additional similarity loss after introducing their video object queries to better supervise the training, while MaXTron does not need that. Besides, VITA takes frame object queries from the last three layers of the transformer decoder as input, while MaXTron only needs the clip object queries from the last layer.\n\nIn the end, we provide experimental results when replacing our cross-clip tracking modules with VITA below (all based on the same Video-kMaX baseline to ensure a fair comparison):\n\ncross-clip tracking design | backbone | input size | GPU days (A100) | params | GFlops (cross-clip tracking module only) | VPQ |\n|:---------:|:----------:|:----------:|:----------:|:----------:|:----------:|:----------:|\nMaXTron     |    ResNet-50    |    $24 \\times 769 \\times 1345$    |    3.5    |    7.7M    |    32    |    46.7    |\nVITA     |    ResNet-50    |    $24 \\times 769 \\times 1345$   |    5.2    |    12.9M    |    47    |    46.3   |\n\nAs shown in the table, our cross-clip tracking module is ***faster*** in training, ***more lightweight*** in params and FLOPs and achieves ***better*** performance compared to VITA.\n\n> Q1: FLOPs and FPS of MaXTron.\n\nWe thank the reviewer for the question. Please refer to common concerns C3 for the FLOPs and FPS comparison.\n\n> Q2: Statistical significance.\n\nWe thank the reviewer for the question. Yes, all experiments are conducted 3 times and the reported number is the mean of the results. Please note that we do not report this statistical significance as most works in this literature do not report this in their papers."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission971/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700082075649,
                "cdate": 1700082075649,
                "tmdate": 1700082658139,
                "mdate": 1700082658139,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nUrmYrAiWD",
                "forum": "RCKoQGpPEN",
                "replyto": "sOzYFG5NiQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission971/Reviewer_tzju"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission971/Reviewer_tzju"
                ],
                "content": {
                    "title": {
                        "value": "To authors"
                    },
                    "comment": {
                        "value": "I have read the authors\u2019 response and the reviews from fellow reviewers. \n\n----\n\n- The main concern that has not fully been addressed is the significance of the claimed contributions. For the past few years, numerous works have been published to address the quadratic computation issue of self-attention. There are many approaches that can handle the CUDA OOM issue such as reducing the scope of attention, decomposing the attention, reducing the number of tokens by taking hierarchical approach, and so on. A lot of those works have already presented such approaches, and they also provide customized CUDA codes that actually makes the model feasible. Compared to those works, I believe the axial-trajectory attention of this paper is much of a naive extension of self-attention: limiting the number of visiting tokens. In order to prove its effectiveness, the authors should have provided thorough analysis of the module and a comparison between temporal extension of existing transformer variants, not only the MSDeformAttn which does only 2D spatial encoding. To list a few, here are some of the references that I believe that could have been simply extended to a spatio-temporal version, and be applied to the VPS task.\n    - Bertasius et al. Is Space-Time Attention All You Need for Video Understanding\n    - Ramachandran et al. Stand-Alone Self-Attention in Vision Models\n    - Hassani et al. Neighborhood Attention Transformer\n    - Beltagy et al. Longformer: The Long-Document Transformer\n    - Xie et al. SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers\n    - Pan et al. Slide-Transformer: Hierarchical Vision Transformer with Local Self-Attention\n\nEspecially, since the first reference can be directly applied, what is the benefit (accuracy & efficiency?) of the proposed module over Bertasius et al?\n\n----\n\n- Thank you for pointing out that the authors do not claim ASPP as their novelty. However, I do not view the application of ASPP as an enough contribution. It is true that there are not many applications to the dense video pixel-level prediction tasks. However, given that the VPS task is not a significantly popular task, and ASPP is an extremely well-known module in the vision domain, I cannot agree with the authors that it can be considered as a major contribution. Indeed, it is very obvious that ASPP can be seamlessly integrated into any module. I do not mean that this work should not have used the transformer decoder. To clarify, I believe the contributions for the overall architectural design is limited, e.g., using the transformer decoder is to be expected.\n\n-----\n\n- Thanks for providing the comparison to VITA. Despite I understand that the time can be limited for experimenting during the rebuttal period, my remaining concern is that it is only experimented on top of Video-kMaX.\n\n----\n\n- From the 15 GFlops reduction over VITA, the authors mentioned that the presented module is more computationally efficient. Then, as shown in Table C3, it seems like MaXTron is extremely heavier than Video-kMaX (more than 30% increase).\n    - What\u2019s the GFlops and FPS of other state-of-the-art methods such as TarVIS, VITA, DVIS? Referring to the DVIS paper, DVIS has much less params even than Video-kMaX (which is much lighter than MaXTron).\n        - I believe the FPS comparison can be reported if they are experimented on VIS benchmarks.\n        - Since MaXTron also provides accuracies on VIS datasets, it might be easier and more straight-forward to compare on the VIS benchmarks."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission971/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700347331594,
                "cdate": 1700347331594,
                "tmdate": 1700347873750,
                "mdate": 1700347873750,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RZVY5S0H5a",
                "forum": "RCKoQGpPEN",
                "replyto": "sOzYFG5NiQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission971/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission971/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Second Rebuttal to Reviewer tzju (1/3)"
                    },
                    "comment": {
                        "value": "We thank Reviewer tzju for the follow-up reviews, and we address the concerns below.\n\n> T1 The main concern that has not fully been addressed is the significance of the claimed contributions.\n\nWe thank the reviewer for the question and the provided references. We are happy to cite and add a brief discussion about them in the revised version.\nBefore we start, we would like to clarify that the main aim of this work is not just designing an efficient attention mechanism. Our core objective lies in enhancing the ***tracking ability of clip-level segmenters***, encompassing improvements on both within-clip and cross-clip levels. The introduction of axial-trajectory attention is a non-trivial exploration in advancing this specific pursuit.\n\n> > T1.1 Should compare not only the MSDeformAttn which does only 2D spatial encoding\n\nWe note that in our previous rebuttal, we offered the detailed comparison to MSDeformAttn mainly to address your previous concern *\"MSDeformAttn is borrowed from literature\"* and showed that the proposed axial-trajectory attention brings more significant improvement than MSDeformAttn. We are ***not*** merely comparing with MSDeformAttn, but instead we have carefully compared with the mostly related works to support our claim (***enhancing the tracking ability of clip-level segmenters via axial-trajectory attention***) as shown below:\n- Video segmentation:\n  - TarVIS (for within-clip tracking comparison), VITA (for cross-clip tracking comparison) \n- Video classification\n  -  Divided Space-Time Attention [1].\n\n[1] Bertasius et al. Is Space-Time Attention All You Need for Video Understanding"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission971/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700385198463,
                "cdate": 1700385198463,
                "tmdate": 1700385673650,
                "mdate": 1700385673650,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "e8OQyVLNyO",
            "forum": "RCKoQGpPEN",
            "replyto": "RCKoQGpPEN",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission971/Reviewer_LGcv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission971/Reviewer_LGcv"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes a novel panoptic segmentation method, namely MaXTron, which enhances temporal consistency by the proposed within-clip and cross-clip tracking modules. Axial-trajectory attention is the essential component of the introduced tracking modules, which aims at associating objects meanwhile reducing computational complexity.Experimental results have shown state-of-the-art performance on video segmentation benchmarks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "It sounds interesting to conduct in-clip tracking and cross-clip tracking via axial-trajectory attention.\n\nAssociation with non-overlapping clips is more efficient than previous overlapping-based methods. \n\nThe results are promising."
                },
                "weaknesses": {
                    "value": "The writing of sec.3 (method) should be improved. It\u2018s a bit confusing about the implementation details.\n\nBesides the performance, it is suggested to provide the cost, e.g. training cost and inference speed, of integrating the proposed model into existing methods.\n\nBesides the overall performance, a deeper analysis is expected. For instance, how does the association capability improve after integrating the proposed modules into an off-the-shelf method?"
                },
                "questions": {
                    "value": "see weakness"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission971/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698240903848,
            "cdate": 1698240903848,
            "tmdate": 1699636023016,
            "mdate": 1699636023016,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "TbJ89x12uI",
                "forum": "RCKoQGpPEN",
                "replyto": "e8OQyVLNyO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission971/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission971/Authors"
                ],
                "content": {
                    "title": {
                        "value": "To Reviewer LGcv"
                    },
                    "comment": {
                        "value": "We thank Reviewer LGcv for the review, and we address the concerns below.\n\n> W1: Writing should be improved. confusing about the implementation details.\n\nWe thank the reviewer for pointing this out and for the suggestions. Please refer to Q3, W1 and W2 in response to Reviewer TBSQ in case there are any confusions regarding axial-trajectory attention, trajectory attention and Temporal-ASPP, respectively.\n\nAdditionally, as promised in our abstract, we will open-source all our implementations and checkpoints, allowing the community to scrutinize the details (for both training and inference).\n\n> W2: Provide the cost, e.g. training cost and inference speed\n\nWe thank the reviewer for pointing this out. Please refer to common concerns C3 for detailed training cost and inference speed.\n\n> W3: Deeper analysis of how the association capability is improved by the proposed modules.\n\nWe thank the reviewer for pointing this out. The association capability is improved with two modules: within-clip tracking and cross-clip tracking.\n\nIn the within-clip tracking module, MaXTron effectively exploits axial-trajectory attention to reason about the pixel-level temporal association. Concretely, axial-trajectory attention finds the probabilistic path of each pixel along time as shown in Fig. 1. It then aggregates information along this probabilistic path. Intuitively, this helps to capture pixel correspondence and avoids inconsistent prediction within a clip. As a result, the within-clip tracking module brings **3.4** and **3.5** VPQ improvement on VIPSeg, with ResNet50 and ConvNeXt-L, respectively (see Tab. 1 in the paper).\n\nIn the cross-clip tracking module, MaXTron exploits trajectory attention along with Temporal-ASPP to capture both the global and local cross-clip connections. Intuitively, the per-clip prediction is not perfect and simply associating them with Hungarian Matching as done in most existing online/near-online methods might lead to incorrect matching. This might further lead to problems such as ID switching or miss detection. By introducing the cross-clip tracking module, we aim at aligning the ***clip object queries*** more precisely by modeling the connections between them. As a result, our offline method with the proposed cross-clip tracking module is able to provide more consistent prediction at video-level and thus brings an additional **0.6** and **0.9** VPQ improvement on VIPSeg, with ResNet50 and ConvNeXt-L, respectively (see Tab. 1 in the paper)."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission971/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700081185752,
                "cdate": 1700081185752,
                "tmdate": 1700082454157,
                "mdate": 1700082454157,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "lbBP7GSLCt",
            "forum": "RCKoQGpPEN",
            "replyto": "RCKoQGpPEN",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission971/Reviewer_tzTu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission971/Reviewer_tzTu"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors proposed trajectory attention based mask transformer for video panoptic segmentation. Specifically, two types of tracking modules (within-clip and cross-clip tracking) are proposed to improve the temporal consistency by leveraging trajectory attention. The within-clip tracking module, an axial-trajectory attention is proposed for effectively computing the trajectory attention for tracking dense pixels sequentially along the height- and width-axes, while the cross-clip tracking module is used to capture the long-term temporal connections by applying trajectory attention to object queries. The experimental shows that the proposed solution is able to help boost the performance of existing solutions (e.g., Video-kMax and Tube-Link) on multiple datasets."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The proposed solution sounds solid. (1) Using trajectory attention to force the model pay more attention spatially and temporally on trajectories (maybe simply on pixel trajectories) while doing video segmentation sound solid in theory. The attention should be able to provide extra useful information to the model. (2) Splitting the trajectory attention along different axes (horizontal and vertical) indeed helps reduce the complexity while calculating attention. \n2. The experimental results on multiple datasets and models prove that the proposed solution works in varying application scenarios. \n3. This paper is well-organized, which help readers easy to read and understand. Expecially, there are more implementation details and results reported in the appendix, which helps readers better understand their work and the performance."
                },
                "weaknesses": {
                    "value": "1. It will be better to report some failure cases. It will be helpful if the authors could report some failure cased that caused by applying the proposed MaxTron. In this case, readers will better understand their work and the performance, which may inspire more ideas along this direction. \n2. The proposed solution sounds like an add-on to the existing solutions, which was inspired by other works (e.g., Patrick et al. 2021). The novelty may be incremental."
                },
                "questions": {
                    "value": "1. What if we change the number of frames within one video clip? Is there any positive / negative impact on the model performance? Is there any guidances (or suggestion) of how many frames should be selected while spliting the video?\n2. Does the proposed solution perform differently if we (1) process all continuous frames or (2) only process key frames (with some down-sampling temporally)? The later operation will speed up motions in videos."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission971/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission971/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission971/Reviewer_tzTu"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission971/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698271135732,
            "cdate": 1698271135732,
            "tmdate": 1700512802448,
            "mdate": 1700512802448,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Rpt5GOULbN",
                "forum": "RCKoQGpPEN",
                "replyto": "lbBP7GSLCt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission971/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission971/Authors"
                ],
                "content": {
                    "title": {
                        "value": "To Reviewer tzTu"
                    },
                    "comment": {
                        "value": "We thank Reviewer tzTu for the review, and we address the concerns below. \n\n> Q1: What if we change the number of frames within one video clip?\n\nWe thank the reviewer for bringing up this question. We observe performance change when varying the number of frames within one video clip. Most likely, it is due to the fact that the off-the-shelf clip-level segmenter itself performs differently with different clip sizes. Concretely, for Video-kMaX and MaXTron w/ Video-kMaX on VIPSeg with ResNet50 as backbone in terms of VPQ, we have:\n\nclip size | Video-kMaX | MaXTron online | MaXTron offline |\n|:---------:|:----------:|:--------------:|:---------------:|\n2     |    42.7    |      46.1      |       46.7      |\n3     |    42.1    |      45.1      |       45.5      |\n4     |    41.4    |      44.2      |       44.7      |\n\nAs we can see, with the increase of clip size, Video-kMaX performance gradually decreases. However, both our MaXTron online (w/ within-clip tracking module) and MaXTron offline (w/ within-clip + cross-clip tracking module) models bring steady and consistent improvements. We hypothesize that the performance difference of Video-kMaX is due to the lack of a temporal module in its framework and the temporal connections are forced to be modeled by the transformer decoder only. As a result, with the increase of clip size, the transformer decoder can not handle the features properly, leading to the decrease in performance. Our axial-trajectory attention alleviates this problem and achieves slightly larger improvement when clip size = 2. Besides, our cross-clip tracking module is less sensitive to clip size as it directly learns to model video-level input.\n\n> Q2: Does the proposed solution perform differently if we (1) process all continuous frames or (2) only process key frames (with some down-sampling temporally)?\n\nWe thank the reviewer for bringing up this question. We would like to clarify the challenges of dense prediction is to label all pixels in the video, and it is very challenging to only process key frames and propagate the key frame results to other frames during testing. \n\nHowever, as suggested in C2 in common concerns where we discuss possible failure cases, one main challenge that still remains for MaXTron is to model fast-moving objects. As a result, we hypothesize that MaXTron will perform worse if the video itself contains fewer frames, which might be caused by down-sampling with a large factor in the temporal domain. This is partially supported by our experiments where we train the model with discontinuous sampled frames (specified by the frame sampling range below):\n\nframe sampling range | VPQ |\n|:---------:|:----------:|\n$\\pm1$     |    46.1    |\n$\\pm2$     |    45.9    |\n$\\pm3$     |    45.8    |\n$\\pm5$     |    45.3    |\n$\\pm10$     |    43.9    |\n\nAbove experiments are done with MaXTron w/ Video-kMaX on VIPSeg with ResNet-50 as backbone, clip size = 2 and are evaluated in terms of VPQ. As shown in the table, training with continuous frames brings the best performance. While slightly increasing the sampling range does not affect the performance a lot, increasing it to $\\pm10$ greatly hampers the learning of the within-clip tracking module.\n\n> W1: Report some failure cases.\n\nWe thank the reviewer for pointing this out. Please refer to common concerns C2 for the discussions on common error patterns of MaXTron."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission971/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700081086947,
                "cdate": 1700081086947,
                "tmdate": 1700084195773,
                "mdate": 1700084195773,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Wlz4bgYl9k",
                "forum": "RCKoQGpPEN",
                "replyto": "Rpt5GOULbN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission971/Reviewer_tzTu"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission971/Reviewer_tzTu"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you so much for the authors' responses. The extra results are helpful, and the analysis is sound reasoning. I do not have extra questions. Thanks."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission971/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700512768945,
                "cdate": 1700512768945,
                "tmdate": 1700512768945,
                "mdate": 1700512768945,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "N2TuJWBJmx",
            "forum": "RCKoQGpPEN",
            "replyto": "RCKoQGpPEN",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission971/Reviewer_TBSQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission971/Reviewer_TBSQ"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors mainly study the clip-level video panoptic segmentation. They propose a new framework using Mask Xformer with trajectory attention, named MaXTron. It includes within-clip and cross-clip tracking modules, to use trajectory attention. The experimental results show the effectiveness of their proposed model."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper is well-written and easy to follow. The idea of using trajectory information to help the segmentation and decompose the attention into height and width in two directions, greatly reducing the computational complexity. They have done comparison and ablation studies to validate their proposed components."
                },
                "weaknesses": {
                    "value": "The figures might not be easy to follow. For example, in Fig. 3, they show H and W-axis attention maps of one point. It would be much better, if they also show how to get the probabilistic path of a point between frames and what the whole attention maps for static and dynamic points. Besides, the authors should show the details in trajectory attention module and temporal ASPP in Fig. 4 and it can help readers to understand. \n\nThe main contribution of this work is the trajectory based within-clip and cross-clip module, which might be limited and insufficient for this conference, even if the authors could clearly introduce their modules using Fig. 3 and 4 after revision. \n\nIn the experiment, the authors are suggested to add some examples to show the attention maps and how to get the trajectories or the trajectories might not be perfect."
                },
                "questions": {
                    "value": "The main contribution of this work is the trajectory-based within-clip and cross-clip modules, which might be limited and insufficient for this conference, even if the authors could clearly introduce their modules using Fig. 3 and 4 after revision.\n\nIn the experiment, the authors are suggested to add some examples to show the attention maps and how to get the trajectories or the trajectories might not be perfect.\n\nIt would be much better if they also showed how to get the probabilistic path of a point between frames and what the whole attention maps for static and dynamic points."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission971/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission971/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission971/Reviewer_TBSQ"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission971/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698498136663,
            "cdate": 1698498136663,
            "tmdate": 1700451157058,
            "mdate": 1700451157058,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "n0GwpnWxCG",
                "forum": "RCKoQGpPEN",
                "replyto": "N2TuJWBJmx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission971/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission971/Authors"
                ],
                "content": {
                    "title": {
                        "value": "To Reviewer TBSQ (1/2)"
                    },
                    "comment": {
                        "value": "We thank Reviewer TBSQ for the review, and we address the concerns below. \n\n> Q1: Main contribution?\n\nAs properly phrased in the submission, we ***never*** claim trajectory-attention as our main contributions. Instead, as mentioned in the ***common concerns C1***, our contribution lies in the fact that we are the first to successfully adapt trajectory-attention to the dense pixel prediction tasks, by designing ***axial-trajectory-attention*** for tracking pixel movement in *H* -axis and *W*-axis separately to model the within-clip temporal cues as well as exploiting trajectory-attention on ***clip object queries*** to model the cross-clip temporal connections. \n\nTo the best of our knowledge, none of the existing works have successfully explored trajectory-attention for dense prediction tasks and very few works ever explored temporal attention for dense video prediction. As shown in the experimental results, our attempt has successfully led to a significant boost. We kindly request the reviewer to re-evaluate our contributions. If the reviewer insists on the point that it is ***limited*** and ***insufficient*** to design trajectory-based within-clip and cross-clip modules for dense prediction, we kindly request the reviewer to elaborate more on the reasons.\n\n> Q2: Examples of attention maps and how to get the trajectories or the trajectories might not be perfect.\n\n>> Q2.1 More examples of attention maps\n\nPlease see the revised version for more examples of attention maps of selecting either dynamic or static points as reference points. Concretely, please refer to Fig. 9, 10, 11, 12 and Sec. F. 1 in Appendix for the detailed discussion.\n\n>> Q2.2 How to get the trajectories?\n\nPlease see Q3 for detailed explanation.\n\n>> Q2.3 Trajectories might not be perfect.\n\nPlease see the revised version for failure cases (Fig. 13 and 14). Besides, as suggested by the discussions in C2 in common concerns, the trajectories might also fail when the selected reference points are not discriminative enough.\n\n> Q3: How to get the probabilistic path of a point between frames?\n\n**Overview:** We first provide an overview of how to get the probabilistic path. Conceptually, the affinity logits of pixels across different frames are computed within the process of axial-trajectory attention. As a result, simply exploiting the computed attention map can bring us the visualizations of the probabilistic path. For your convenience, we now explain it in detail below.\n\nWe take *H*-axis trajectory attention for example. We first reshape the feature map extracted by the backbone $F \\in \\mathbb{R}^{\\mathit{T} \\times \\mathit{D} \\times \\mathit{H} \\times \\mathit{W}}$ into $F_{h} \\in \\mathbb{R}^{\\mathit{W} \\times \\mathit{TH} \\times \\mathit{D}}$ before conducting *H*-axis trajectory attention. In this way, we obtain a sequence of $\\mathit{TH}$ pixel features $x_{th} \\in \\mathbb{R}^{D}$.\n\nThen the *H*-axis trajectory attention can be divided into two steps.\n\n1. **Trajectory along *H*-axis (Eq. (2) in the paper):** Given a reference point at a specific time-height th position, we compute its possible time-height locations at each frame separately followed by extracting per-frame information (i.e., trajectories) with the computed probability.\n\n2. **Temporal attention along the trajectory (Eq. (4) in the paper):** Given the trajectories, we aggregate the information along them using 1D time attention to reason about temporal connections.\n\nThe construction of the probabilistic path lies in step 1. Concretely, we use linear projection to project all features $x_{th}$ into their corresponding queries, keys and values. Then for a given new time $t'$ (i.e., the frame index), the trajectory tokens are computed by (Eq. (2) in the paper): $\\widetilde y_{tt'h} = \\sum_{h'}v_{t'h'} \\cdot \\frac{\\exp{\\langle q_{th},k_{t'h'} \\rangle}}{\\sum_{\\overline{h}}\\exp{\\langle q_{th}, k_{t'\\overline{h}}\\rangle}}$.\n\nIntuitively, the operation in the numerator $\\langle q_{th},k_{t'h'} \\rangle$ yields an attention map $A \\in \\mathbb{R}^{\\mathit{T} \\times \\mathit{H}}$, which computes the similarity logits of the reference point th and pixel features at all other possible TH locations $t'h'$.\n\nAs a result, to draw the *H*-axis trajectory attention map across frames, we first pick a reference point th (e.g. the basketball in Fig. 1) followed by computing the attention map $A$ as above. We can then set the new time point to $t+1$, $t+2$, ... $t+k$ and retrieve the corresponding position at *H*-axis iteratively. By putting them together, we get the trajectory path along *H*-axis. Similarly, the same operation is conducted for the *W*-axis to obtain the trajectory path along the *W*- axis. As explained in the caption of Fig. 1, for the purpose of visualization, we multiply the *H*-axis and *W*-axis trajectory attentions to visualize the trajectory of the reference point over time (i.e., a bright point corresponds to a high attention value in both the H- and W-axis trajectory attention)."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission971/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700080440973,
                "cdate": 1700080440973,
                "tmdate": 1700080440973,
                "mdate": 1700080440973,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3mpAYEUbTs",
                "forum": "RCKoQGpPEN",
                "replyto": "mlAlyqX3pn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission971/Reviewer_TBSQ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission971/Reviewer_TBSQ"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors."
                    },
                    "comment": {
                        "value": "I have carefully read other reviews and the authors' feedback. Thanks for the great length and detailed feedback. For questions about the lack of details of the trajectory attention and Temporal-ASPP module, the authors are suggested replacing Fig. 3 and 4 with Fig. 15 and 16, since Fig. 15 and 16 have more details not just a bounding box in Fig. 3 and 4. However, after reading the all responses, I decided to hold the rating mainly on the novelty and contributions. This work has extended or stepped forward on the current method, like trajectory attention for video or ASPP, in terms of reducing computer costs. But as pointed out by reviewer tzju, the authors might consider other quadratic computation issues of self-attention."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission971/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700451109625,
                "cdate": 1700451109625,
                "tmdate": 1700451109625,
                "mdate": 1700451109625,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]