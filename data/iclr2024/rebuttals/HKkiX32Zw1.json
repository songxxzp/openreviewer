[
    {
        "title": "Promptbreeder: Self-Referential Self-Improvement via Prompt Evolution"
    },
    {
        "review": {
            "id": "kreDhX4CXq",
            "forum": "HKkiX32Zw1",
            "replyto": "HKkiX32Zw1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5283/Reviewer_AFtS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5283/Reviewer_AFtS"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes Promptbreeder (PB), a general-purpose self-referential self-improvement mechanism of LLMs that evolves and adapts prompts for a given domain.\nThis mechanism mutates a population of task-prompts, evaluates them for fitness on a training set, and repeats this process over multiple generations to evolve task-prompts.\nAuthors shows that PB outperforms state-of-the-art prompt strategies such as Chain-of-Thought and Plan-and-Solve Prompting on commonly used arithmetic and commonsense reasoning benchmarks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* This paper focuses on the problems of prompt strategies that hand-crafted prompt-strategies are often sub-optimal,\nand present Promptbreeder (PB).\n* A self-referential self-improvement mechanism is promissing approach as the prompt optimization. \n* The authors conducted  extensively survey and support their originality."
                },
                "weaknesses": {
                    "value": "**The comparative study of alternative methods is weak and does not fully support the validity of the proposed method**\n* While Promptbreeder is an important approach as the prompt strategies, it is complex in its composition such as a mutation prompt, a hyper mutation prompt,  a domain-specific problem description, and a seed thinking-styles, and then lacks the ablation analysis to show which components are effective and how effective they are.\n* Promptbreeder appears to rely on past prompt strategies or their combination, lacks its motivation and theoretical considerations, and lacks sufficient experimental results to support them.\n* As authors use only two LLMs as baselines, the generality of the proposed method cannot be determined."
                },
                "questions": {
                    "value": "* What is the rationale for the baseline selection in Table 1?\n* Can you explain the result that PS+ does not show a better performance than PS for PaLM 2-L than text-davinci-003, in Table 1?\n* Which resullt supports your claim ``we investigate the various self-referential components of Promptbreeder and their contribution to our results.''?\n* Can you show how effective it is compared to LLaMA or OPT as baselines?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5283/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698157194652,
            "cdate": 1698157194652,
            "tmdate": 1699636528601,
            "mdate": 1699636528601,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "w7ncl5TiEA",
                "forum": "HKkiX32Zw1",
                "replyto": "kreDhX4CXq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5283/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5283/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you reviewer AFtS"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their detailed comments to improve our paper. We are glad the reviewer found Promptbreeder promising and our survey of related work extensive, supporting the originality of our approach. We address your concerns below.\n\n**Comprehensive Comparative Study:**\nWe respectfully disagree with your claim that our paper lacks sufficient experimental results. Promptbreeder was benchmarked against various algorithms across nine distinct problems, in fact going beyond the standards as seen in published state-of-the-art work like APE [1] and Plan-and-Solve [2]. However, to address your point, we've expanded the comparison in Table 1 of our revised draft even further, adding three more control groups to reinforce the validity of Promptbreeder.\n\n[1] Zhou, Y., Muresanu, A. I., Han, Z., Paster, K., Pitis, S., Chan, H., & Ba, J. (2023). Large Language Models Are Human-Level Prompt Engineers. arXiv. https://doi.org/10.48550/arXiv.2211.01910\n\n[2] Wang, L., Xu, W., Lan, Y., Hu, Z., Lan, Y., Lee, R. K.-W., & Lim, E.-P. (2023). Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models. arXiv. https://doi.org/10.48550/arXiv.2305.04091\n\nWe also point to the fact that Promptbreeder outperforms APE on 21/24 instruction induction tasks which are outlined in the Appendix L. \n\n**Ablation Analysis:**\n\nYou state that our paper \u201clacks the ablation analysis to show which components are effective and how effective they are\u201d. You will be glad to hear that our original submission already contained a comprehensive ablation analysis in Appendix L, however, we have moved this to Appendix B as it was easily overlooked. It shows that every component of Promptbreeder is important, and the interplay of different mutation operators is what makes Promptbreeder effective. We agree that this ablation analysis is crucial to show which components of Promptbreeder are effective, so to decrease the chances of other readers overlooking this important part of our work, we have made the reference to Appendix B in bold in the main text. We thank you for bringing this to our attention and improving the paper in this way.\n\n**Theoretical Motivation:**\n\nYour observation on Promptbreeder's reliance on past prompt strategies is valuable. We designed Promptbreeder to not only incorporate but also evolve beyond these strategies. The addition of genetic algorithms to our framework is grounded in existing literature, advocating for diverse and non-greedy exploration that circumvents local optima. We've elaborated on this in the revised draft, highlighting how Promptbreeder transcends mere aggregation of existing strategies. Control 1 in our paper demonstrates that initializing task prompts with these strategies is less effective than employing evolutionary evaluations.\n\n**Rationale for Baseline Selection:**\n\nThe selection of baselines was driven by a desire to compare Promptbreeder with the latest state-of-the-art prompting strategies. Additionally, two control experiments were conducted to validate the efficacy of evolutionary processes in Promptbreeder over random search and to demonstrate improvements over the initial problem descriptions used as prompts.\n\n**Summary:**\n\nIn conclusion, we respectfully ask you to consider the enhancements made in our revised draft, which address your concerns in detail, and to increase your support for our paper."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5283/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700232710090,
                "cdate": 1700232710090,
                "tmdate": 1700232710090,
                "mdate": 1700232710090,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BgSjDbwLRh",
                "forum": "HKkiX32Zw1",
                "replyto": "kreDhX4CXq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5283/Reviewer_AFtS"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5283/Reviewer_AFtS"
                ],
                "content": {
                    "title": {
                        "value": "Reply to authors"
                    },
                    "comment": {
                        "value": "Thank you for your answer. \nI found it very interesting. \n\nCould you explain clearly me a little more about the following points? \nI would like to check the effectiveness and novelty of individual proposals.\n* How insensitive is the proposed methodology to existing strategies and types of LLM? Please provide any evidence of this.\n* Also, there are several search methods to prevent genetic algorithms from becoming a local optimum solution, can you compare them and highlight what you have devised for Promptbreeder's?\n* Please tell us about your evaluation methodology for reproducibility. Which tools or library do you use?"
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5283/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700614140274,
                "cdate": 1700614140274,
                "tmdate": 1700636167342,
                "mdate": 1700636167342,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "74Ta7ndIJd",
                "forum": "HKkiX32Zw1",
                "replyto": "nYqNYhGLjO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5283/Reviewer_AFtS"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5283/Reviewer_AFtS"
                ],
                "content": {
                    "title": {
                        "value": "Reply to authors"
                    },
                    "comment": {
                        "value": "Thank you for your answer. \nI found it very interesting, too.\n\nCan you tell me a little more about your evaluation methods and tools?\nAs we can have available prompt evaluation tools, but have you considered using or modifying these tools? \nIf so, can you tell us about those tools and the results of your consideration for reproducibility?"
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5283/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700709409821,
                "cdate": 1700709409821,
                "tmdate": 1700709409821,
                "mdate": 1700709409821,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ZrPWvvJlZ0",
            "forum": "HKkiX32Zw1",
            "replyto": "HKkiX32Zw1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5283/Reviewer_DPYS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5283/Reviewer_DPYS"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces PromptBreeder (PB), a novel prompt evolution system that automates the exploration of prompts within a specific domain, thereby improving a language model's ability to answer questions in that domain. PB employs a self-referential self-improvement mechanism to evolve and adapt task-prompts. By mutating a population of task-prompts, evaluating their fitness on a training set, and iteratively repeating this process, PB successfully evolves task-prompts. The empirical evidence presented in the paper provides strong support for the effectiveness of PB in enhancing the language model's performance."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper introduces an innovative automatic strategy for prompt discovery, eliminating the requirement for manual engineering and design. This approach streamlines the process and saves time and effort.\n2. The proposed prompt strategy showcases exceptional performance when compared to currently available state-of-the-art approaches. \n3. The paper is commendable for its clear and well-structured organization. The logical flow of the content enhances readability and comprehension, contributing to a more effective communication of the research findings."
                },
                "weaknesses": {
                    "value": "1. The proposed PB algorithm appears to rely heavily on interactions with the LLM compared to the baselines. As a result, solely evaluating its performance based on accuracy may not provide a fair assessment of its capabilities.\n2. While the authors acknowledge that hand-crafted prompt-strategies are often sub-optimal, they do not offer a guarantee or highlight any asymptotic properties for the PB algorithm, leaving room for uncertainty regarding its long-term effectiveness.\n3. The main text of the paper is relatively concise, with several crucial aspects relegated to the appendix. This arrangement can disrupt the smoothness of the reading experience."
                },
                "questions": {
                    "value": "1. I am quite curious about the sample efficiency of the algorithm. As evolutionary algorithms often suffer from the poor sample efficiency.\n2. I hope that authors will provide how PB perforems if the size of train dataset is limited.\n3. It will be nice to provide guarantee or asymptotic property for the PB."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5283/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5283/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5283/Reviewer_DPYS"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5283/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698668836775,
            "cdate": 1698668836775,
            "tmdate": 1699636528494,
            "mdate": 1699636528494,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "75XgH8PtUm",
                "forum": "HKkiX32Zw1",
                "replyto": "ZrPWvvJlZ0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5283/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5283/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you reviewer DPYS"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their insightful feedback and support for our paper. We are glad to hear they found the method innovative, the empirical performance exceptional, and the write up clear and well structured. We address your concerns below.\n\n**Reliance on LLM Interactions:**\n\nWe appreciate your point about Promptbreeder\u2019s interaction intensity with LLMs. As a general-purpose self-referential self-improvement mechanism, Promptbreeder leverages LLMs to mutate a population of task-prompts, evaluating their fitness on a training set over multiple generations. This process inherently involves significant interaction with LLMs, but it is crucial for the evolutionary approach that underpins Promptbreeder's effectiveness\u200b\u200b. The sample efficiencies of the algorithm for each dataset are shown in the Appendix K for each problem. For example, on AQuA the best individual was produced after 1400 mutations which equates to 1400 x 2 x 100 Q:A pairs tested. While this may seem costly at first, we believe Promptbreeder will have important practical implications as the time and cost to find a superior prompt is a one-time cost for a given domain, and amortized with respect to many LLM inferences at deployment time of the prompt.\n\n**Lack of Asymptotic Guarantees:**\n\nNote that such guarantees are generally not available for evolutionary algorithms in realistic large and noisy problem spaces. However, we have demonstrated empirical superiority to other optimization techniques such as APE, random task prompt initialization (see our own controls in Table 1), and ORPO on GSM-8k. We have shown that using the diverse set of initial task prompts improves Promptbreeder\u2019s search for effective prompts in various domains.  \n\n**Content Distribution in Main Text and Appendix:**\n\nThank you for your suggestion to improve our paper. We have moved Figure 3 into the main paper in improved form. \n\n\nAgain, thank you for your valuable contribution to improving our paper."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5283/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700232653957,
                "cdate": 1700232653957,
                "tmdate": 1700232653957,
                "mdate": 1700232653957,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "v0C52YKQAS",
            "forum": "HKkiX32Zw1",
            "replyto": "HKkiX32Zw1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5283/Reviewer_ceRr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5283/Reviewer_ceRr"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes Promptbreeder, a self-referential self-improvement method to evolve prompts for a specific domain. Given some seed prompts, domain description, and thinking-styles, Promptbreeder can generate variations of both task prompts and mutation prompts.  Experiments on various benchmarks have verified that the method outperforms other prompt strategies like CoT and Plan&Solve."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper proposes a systematic framework to evolve domain-specific prompts, and shows better results compared to other prompt strategies."
                },
                "weaknesses": {
                    "value": "1. The experiment is not extensive. In Table 1, the compared LLMs do not involve the most recognized models like gpt-3.5 or gpt-4, and the compared methods should contain CoT on PaLM 2-L.\n2. The proposed method Promptbreeder still requires initial information for specific task (like description or mutation prompts), where worse initialization may lead to worse performance. This makes the method may not generalize to various tasks."
                },
                "questions": {
                    "value": "None"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5283/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5283/Reviewer_ceRr",
                        "ICLR.cc/2024/Conference/Submission5283/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5283/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698822054327,
            "cdate": 1698822054327,
            "tmdate": 1700738421591,
            "mdate": 1700738421591,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KZZ5SL6ZLL",
                "forum": "HKkiX32Zw1",
                "replyto": "v0C52YKQAS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5283/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5283/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you reviewer ceRr"
                    },
                    "comment": {
                        "value": "Thank you for your thoughtful review of our paper on Promptbreeder. We appreciate your recognition of our systematic framework's ability to evolve domain-specific prompts and its comparative effectiveness. We address your concerns below and offer clarifications and updates to our methodology and results.\n\n**Regarding the Experiment's Extensiveness:**\n\nWe acknowledge your observation about the lack of inclusion of prominent models like GPT-3.5 or GPT-4 in our initial experiments. In response, we have carried out experiments with GPT-3.5 on GSM-8k showing successful improvement of prompts to 65.5% test set accuracy, see Appendix M for further details. Additionally, as per your suggestion, we have re-run the experiments using Chain-of-Thought (CoT) and other methods on PaLM 2-L. Our updated results demonstrate that Promptbreeder shows consistent improvements. \n\n**Concerning the Requirement of Initial Information:**\n\nYou rightly pointed out that the effectiveness of Promptbreeder depends on the initial information provided, such as task descriptions or mutation prompts. To address this, we have included a new section in the Appendix of our revised draft and carried out two more control experiments showing that even with poor and general problem descriptions, PB shows consistent improvements in fitness. \n\n- **Generation of Thinking Styles and Mutation Prompts from Problem Descriptions:** We have developed a method by which thinking styles and mutation prompts can be autonomously generated from the problem description itself, reducing the dependency on external initialization.\n- **Concerns about the initial problem description information provided to Promptbreeder:** We demonstrate (Appendix N) that even with neutral or misleading problem specification (task descriptions) Promptbreeder achieves substantial improvement in terms of the test set fitness of evolved prompts.\n\nWe respectfully disagree with the implication that our method may not generalize across various tasks. The aforementioned enhancements and experimental results strongly suggest that Promptbreeder is not only adaptable but also capable of performing effectively across a diverse range of tasks, even with minimal or suboptimal initial information. Furthermore, the fact that Promptbreeder outperforms APE on 21/24 instruction induction tasks also highlights this, see Appendix L. \n\n**Summary:**\n\nWe believe these updates and clarifications address your concerns regarding the breadth of our experiments and the generalizability of Promptbreeder. We kindly request you to reconsider the contribution score in light of these significant improvements and additional data. Moreover, if there are any further queries or points of contention, we are more than willing to provide additional clarifications to strengthen the case for acceptance of our paper."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5283/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700232600248,
                "cdate": 1700232600248,
                "tmdate": 1700232600248,
                "mdate": 1700232600248,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fEIFaV2f9y",
                "forum": "HKkiX32Zw1",
                "replyto": "7xuTPNugDO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5283/Reviewer_ceRr"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5283/Reviewer_ceRr"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the Responses"
                    },
                    "comment": {
                        "value": "I have carefully read the responses from authors, they have addressed most of my concerns. I am willing to increasing my score to 6. Thanks!"
                    }
                },
                "number": 26,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5283/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700738399570,
                "cdate": 1700738399570,
                "tmdate": 1700738399570,
                "mdate": 1700738399570,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "cztDNzwPBW",
            "forum": "HKkiX32Zw1",
            "replyto": "HKkiX32Zw1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5283/Reviewer_7CXL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5283/Reviewer_7CXL"
            ],
            "content": {
                "summary": {
                    "value": "This manuscript proposed PROMPTBREEDER, a new self-referential self-improvement method using an LLM to generate and refine both task- and mutation- prompts over multiple generations. PROMPTBREEDER incorporated nine mutation operators falling into five broad classes to promote varied and robust prompt evolution. This method has been evaluated in 8 tasks with promising performance using the PaLM 2-L model, surpassing serval established baselines such as CoT, APE, and Plan-and-Solve."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- PROMPTBREEDER showed promising performance and outperformed competing baseline approaches in 7 out of 8 tasks with a large margin.\n- The PROMPTBREEDER method was well elaborated in the manuscript and in the supplementary material."
                },
                "weaknesses": {
                    "value": "- PROMPTBREEDER aimed to concurrently refine both task and mutation prompts, considerably expanding the search space. However, the absence of navigation during each evaluation often resulted in unpredictable performance for the successive generated prompts. This is evidenced by the persistence of less effective prompts after extensive evaluations, as illustrated in Figure 3.\n- In light of the above, PROMPTBREEDER appears to rely on an extensive series of trial-and-error iterations to identify an optimized prompt, raising concerns about the method's efficiency in exploring potential solutions. It would be helpful if the authors can include a comparative analysis detailing the correlation between the number of prompts generated and the performance for each evaluated baseline and for PROMPTBREEDER itself.\n- It is not clear how the \"Mutator Prompts\" (Table 2) and \"Thanking Styles\" (Section D) are created. Are they derived from pre-existing prompt strategies? Are these prompts hand-crafted?"
                },
                "questions": {
                    "value": "1. This study exclusively shows the performance of PROMPTBREEDER with PaLM 2-L, raising questions about its generalization ability to other LLMs. Specifically, \n\n- Whether PROMPTBREEDER method can be effectively utilized to enhance prompts for LLMs?\n\n- Whether the prompts refined using PROMPTBREEDER in conjunction with PaLM 2-L can yield improved results when employed with alternative LLMs.\n\n2. In Figure 3, the y-axis label is not visible. Additionally, what is the relationship between \"number of evaluations\" and \"number of generations\"? It is confusing since Section 4 reported that the populations \"evolved for typically 20-30 generations,\" but there are 2000 evaluations in Figure 3.\n\n3. Please clarify why OPRO was only evaluated on GSM8K in Table 1.\n\nMinor Comment:\n\nFor the sake of readability, it would be beneficial if the color coding is consistent across different figures and texts. For example, the \"mutation prompt\" is color-coded as red in the text on Page 6, yet appears in shades of blue (and not the exact same blue) in Figures 1 and 2."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5283/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699575565239,
            "cdate": 1699575565239,
            "tmdate": 1699636528307,
            "mdate": 1699636528307,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kdummo7aLM",
                "forum": "HKkiX32Zw1",
                "replyto": "cztDNzwPBW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5283/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5283/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you reviewer 7CXL"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their detailed comments to improve our paper. We are glad to hear the reviewer found our empirical results promising and the method well elaborated. Below, we are addressing the reviewer\u2019s concerns.\n\n\n**Generalization:**\n\nMany thanks for the expressing the concern that this study exclusively shows the performance of PROMPTBREEDER with PaLM 2-L. To address this we ran PB on GSM-8k using GPT3.5-Turbo achieving test set scores of 65.5% and  63.9% accuracy respectively. Results are shown in Appendix M. The PS+ prompt only achieved 44.7% with GPT-3.5-Turbo. This provides evidence that PB generalizes to other language models. \n\n**Absence of Navigation:**\n\nThe absence of directed navigation is an important component of an evolutionary algorithm. The success of evolution relies on sequences of random mutations that, when accumulating over time, lead to increased fitness. Promptbreeder, an evolutionary algorithm, makes use of this property. In fact, even keeping around less fit (but less specialized) individuals in an evolving population can be important for the entire evolutionary system to arrive at better solutions in the long term [1]. A more directed Promptbreeder approach is conceivable, but it would move Promptbreeder away from an evolutionary approach which is outside of the scope of our paper.\n\n[1] Sudholt, D. The Benefits of Population Diversity in Evolutionary Algorithms: A Survey of Rigorous Runtime Analyses, 2018. https://arxiv.org/abs/1801.10087\n\n**Efficiency in Prompt Optimization:**\n\nIn response to your concern about efficiency, we have introduced two control experiments: the Random Baseline and the PD baseline. The Random baseline evaluates 2,000 prompts generated by our initialization method without evolution, demonstrating that evolution contributes more than a random search. The PD baseline, where task-prompts are set to the problem description, shows that evolution improves performance beyond the initial problem description. \n\n**Origin of \"Mutation Prompts\" and \"Thinking Styles\":**\n\nWe appreciate your inquiry about the creation of the \"Mutation Prompts\" and \"Thinking Styles.\" These were hand-designed for general problem-solving domains and are thus widely applicable (as demonstrated by our empirical gains on multiple diverse reasoning benchmarks). We have added a new section (Appendix G) detailing how an LLM can generate these lists from a problem description using a hierarchical generation method.\n\n**Transfer of Promptbreeder Prompts to other LLMs:**\n\nTo address your question about Promptbreeder's applicability to other LLMs, note that we do not claim prompt transferability across LLMs\u2014Promptbreeder is designed to find prompts that are tailored to the unique characteristics of each LLM for optimal task performance through automated prompt engineering. We will clarify this in a revised version of the paper. \n \n**Clarifications on Figure 3, OPRO Evaluation, and Color Coding:**\n\nWe have corrected and enhanced Figure 3, providing clarity on the relationship between evaluations and generations (a generation being POP_SIZE evaluations). Regarding OPRO's evaluation on GSM8K, we suggest the authors of OPRO may consider evaluating it on a broader range of tasks, as we have done in our study. We have standardized the color coding for task-prompts and mutation-prompts throughout the paper to enhance readability and coherence.\n\n**Summary:**\n\nWe again thank the reviewer for helping improve our paper. We hope that our clarifications will lead you to consider increasing your rating of our paper or detailing what still stands in the way of increasing your support."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5283/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700232495563,
                "cdate": 1700232495563,
                "tmdate": 1700232495563,
                "mdate": 1700232495563,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XjzFPvhi8K",
                "forum": "HKkiX32Zw1",
                "replyto": "cztDNzwPBW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5283/Reviewer_7CXL"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5283/Reviewer_7CXL"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the responses"
                    },
                    "comment": {
                        "value": "Thank you for your thorough responses, which have addressed many of my initial concerns through additional experimental results and analysis.\n\nHowever, I continue to have reservations about the *efficiency* of the proposed PB. Although the use of an evolutionary algorithm demonstrates certain advantages over the '2000 random prompts' approach, the marginal improvement of PB over the Random baseline is less pronounced when compared to other prompting strategies except in the GSM8K. As evidenced in Table 1, the Random baseline outperforms most other baselines in 7/8 datasets, suggesting that a large number of various prompts plays a more significant role in most tasks, which lacks novelty, in my opinion.\n\nMoreover, I am concerned about the *scalability* of experiments using PB, especially when 2k prompts are needed to be evaluated. Factors such as dataset size, input length, and the use of different LLMs when opting for paid APIs like GPT-4 over GPT-3.5-Turbo, could substantially increase the time and cost of experimentation. This could pose a disadvantage for researchers and practitioners with limited resources.\n\nIn conclusion, while the revised manuscript shows commendable improvements and the authors have provided detailed clarifications, the persistent concerns mentioned above led me to **maintain my initial rating**."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5283/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700693561433,
                "cdate": 1700693561433,
                "tmdate": 1700693825445,
                "mdate": 1700693825445,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "kieYYEkh3E",
            "forum": "HKkiX32Zw1",
            "replyto": "HKkiX32Zw1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5283/Reviewer_PsCD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5283/Reviewer_PsCD"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, authors propose a genetic algorithm to improve both task prompts and additional \u201cmutation\u201d prompts for the specific downstream task. In details, authors utilize direct mutation, estimation of distribution mutation, hypermutation, Lamaerckian mutation and prompt crossover and context shuffling to randomly change either only task prompt or both task prompt and mutation prompts. The overall genetic algorithm is based on a binary tournament genetic algorithm framework where two individuals are sampled and the worse one is replaced with the mutated version of the better one. In most experiments, authors show that their method achieved better results in the targeted downstream tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1: How to generate appropriate LLM prompt for the target downstream task is indeed a very important research question and still lacks a solid answer. I agree with authors that LLM are qualitiifly different from other deep learning models as they have the potential to self-improve their thinking process (e.g. self-generating prompts).\n\n2: From the point of view of genetic algorithm, authors propose a comprehensive set of mutation stratergy, which includes certain extent of \"self-referential'/self-improvement mutation stratergy. Overall it seems interesting in general."
                },
                "weaknesses": {
                    "value": "1: The major concern I have is whether evolution algorithm framework in general is not capable enough for the large prompt space for LLM. Overall from the examples provided by the authors in Figure3 appear to show not much different between prompts, which might suggest under-explored prompt space. Personally I feel certain level learning/gradient signal is needed to better explore and generate the prompts for complex LLM models. It will be very interesting (but not necessay) to have some comparision with prompt tunning algorithms if white box LLM models are used.\n\n2: Some result sections in appendix should be moved to the main text as it really helps to show how the algorithm improve the prompts and how the prompts look in the end.\n\n3: I am afraid that I am not familiar with evolution algorithm literature but I personally feel the overall novel comtribution of this paper is limited as it seems all mutation operators are pretty standard, even those \"self-referential\" ones. And the backbone evolution algorithim seems very simple and out of box.   \n\n4: A minor point is that in the result tables, half of the baselines are using different LLM models, which are not directly comparable to authors' method. I strongly encourage authors to rerun the baselines with the same models if possible, or just remove them from the table."
                },
                "questions": {
                    "value": "Please see my comments in weakness sections.\n\nOverall, my main question is how such method will compare with prompt tunning method?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5283/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699599250862,
            "cdate": 1699599250862,
            "tmdate": 1699636528201,
            "mdate": 1699636528201,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ekIV97Gkzz",
                "forum": "HKkiX32Zw1",
                "replyto": "kieYYEkh3E",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5283/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5283/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you reviewer PsCD"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their detailed comments to improve our paper. We are glad to hear the reviewer agrees that Promptbreeder tackles the very important research question of whether LLMs can self-improve their \u201cthinking process\u201d via self-generating prompts. Furthermore, it is great to hear that the reviewer found the set of mutation strategies investigated in this work comprehensive, and the paper generally interesting. Below, we are addressing the reviewer\u2019s concerns.\n\n**1.& 2. Prompt Space Exploration in Evolution Algorithms:**\n\nWe acknowledge your concern regarding the adequacy of evolution algorithms for exploring the large prompt space in Large Language Models (LLMs). To address this, we have updated Figure 3 (and, as suggested, moved it from the Appendix to the main part of the paper\u2014thank you for the suggestion!) to showcase not only the highest-performing ('elite') prompts but also a variety of low-fitness prompts generated during the exploration phase. This figure shows that there is in fact considerable exploration in prompt space, with many prompts being produced that are very different to the elite prompt. Elite prompts also change in subtle ways throughout the evolutionary run and achieve much higher fitness at the end. The final prompt is \"Sentences are given, and a single word. The output should indicate whether the given word has the same sense in the two given sentences, yes or no.\" with an early elite prompt in the run being \"I'll give you two sentences and a word. Your task is to write if the meaning of the word is the same in both sentences or not.\" \n\n**1. Comparison with Gradient-Based Prompt Tuning (i.e. Soft Prompting) Methods:**\n\nWhile we agree that a comparison to soft-prompting methods could be valuable, it falls outside of the scope of this work as well as outside of the scope of prior published state-of-the-art prompt strategies such as Chain-of-Thought Prompting [1], Plan-and-Solve Prompting [2], as well as APE [3] and OPRO [4], to name just a few.\n\n[1] Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., \u2026 Zhou, D. (2023). Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. arXiv. https://doi.org/10.48550/arXiv.2201.11903\n\n[2] Wang, L., Xu, W., Lan, Y., Hu, Z., Lan, Y., Lee, R. K.-W., & Lim, E.-P. (2023). Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models. arXiv. https://doi.org/10.48550/arXiv.2305.04091\n\n[3] Zhou, Y., Muresanu, A. I., Han, Z., Paster, K., Pitis, S., Chan, H., & Ba, J. (2023). Large Language Models Are Human-Level Prompt Engineers. arXiv. https://doi.org/10.48550/arXiv.2211.01910\n\n[4] Yang, C., Wang, X., Lu, Y., Liu, H., Le, Q. V., Zhou, D., & Chen, X. (2023). Large Language Models as Optimizers. arXiv. https://doi.org/10.48550/arXiv.2309.03409\n\n**3. Simplicity and Novelty of Mutation Operators:**\n\nAlthough some of the mutation operators in Promptbreeder are standard in evolutionary algorithms, their application within the domain of LLMs is novel. Also some operators simply cannot be implemented without LLMs, e.g. the lineage based operator, or the context to prompt operator. Designing these operators for LLMs posed unique challenges, particularly in balancing diversity and relevance of the generated prompts. We demonstrate the effective use of LLMs in evolutionary algorithms, a contribution we believe is significant in this field. Furthermore, we argue that the simplicity of the individual mutation operators of Promptbreeder is in fact a strength of the approach, demonstrating that existing mutation operators can be orchestrated to create a capable self-referential self-improving system.\n\n**4. Base LLM comparison:**\n\nThank you for this comment. We have extended the baselines with PaLM-2L as suggested, and added Chain-of-Thought Prompting (COT and Manual COT) with PaLM-2L as the base model to the results. As expected, Promptbreeder outperforms these baselines by a large margin. \n\n\n**Summary:**\n\nWe hope that our clarifications will lead you to consider increasing your rating of our paper or detailing what still stands in the way of increasing your support, so that we may further improve it."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5283/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700232406186,
                "cdate": 1700232406186,
                "tmdate": 1700232406186,
                "mdate": 1700232406186,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]