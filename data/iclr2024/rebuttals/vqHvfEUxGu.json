[
    {
        "title": "A Collaborative Perspective on Exploration in Reinforcement Learning"
    },
    {
        "review": {
            "id": "QKv08xWS2h",
            "forum": "vqHvfEUxGu",
            "replyto": "vqHvfEUxGu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2065/Reviewer_6JRU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2065/Reviewer_6JRU"
            ],
            "content": {
                "summary": {
                    "value": "Built on a novel collaborative perspective on exploration, this paper proposes a simple yet powerful extension for improving exploration in reinforcement learning. Overall, this paper is well presented and shows promising results via thorough experiments. I only have few questions related to details of the algorithm design and implementation."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "(a) This paper proposes to study the exploration problem in (single-agent) RL from a collaborative perspective, which is quite novel and well motivated.\n\n(b) The algorithm design is simple and well-presented, of which the effectiveness is well supported by solid empirical results."
                },
                "weaknesses": {
                    "value": "(a) Maintaining $N$ training agents and environments at the same time may increase the computation and space complexity.\n\n(b) The related work part can be further extended."
                },
                "questions": {
                    "value": "(a) Is this the first work on collaborative exploration in (single-agent) deep RL? \n\n(b) Do the $N$ agents share parameters in their policy or value networks (I guess not, then all agents need to be trained)?\n\n(c) Could you explain why the $L_2$ norm of the action difference is a good measure for behavior diversity, rather than some other designs (like difference in $\\pi_i(\\cdot|s_t)$)?\n\n(d) It would be great if the authors can release the codes for the baseline algorithms as well. Details on reproducing each figure (or training process) in the paper should also be given in the readme file."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2065/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697922913837,
            "cdate": 1697922913837,
            "tmdate": 1699636138498,
            "mdate": 1699636138498,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "OfDNaalsmw",
                "forum": "vqHvfEUxGu",
                "replyto": "QKv08xWS2h",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2065/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2065/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the constructive review and insightful comments. Responses to the questions are below:\n\n> Q1: maintaining N agents and environments at the same time may increase the computation and space complexity.\n\nWe admit that the increasing computation and space complexity is the major limitation of our work as we explained in the common question section. In order to mitigate this issue, we implement our code in JAX with vectorized model architectures. In the experiment, using an agent number of size 4, the wall clock running time is around 2 times of training a single agent.\nAnother potential solution is to use a multi-head version of this algorithm, which is exactly the following question as we will discuss later.\n\n> Q2: The related work part can be further extended.\n\nThanks for the suggestion. We rewrote the related work section.\n\n> Q3: Is this the first work on collaborative exploration in (single-agent) deep RL?\n\nNo, we are not the first work on this topic. For example, one of the selected baselines, DiCE, also studies this problem by adding a diversity regularization to the loss function. However, our discussions of how and when to collaborate among agents and the instantiation of using the collaborative reward are novel.\n\n> Q4: Do the N agents share parameters in their policy or value networks?\n\nNo, the current agents do not share parameters. Since the main focus of this work is to study how and when to collaborate among agents, we adopt the most basic model architectures in the experiments.\nUsing a multi-head variant of the proposed method would be easy to implement where each agent shares the same torso network and uses different heads as the policy/value function.\nExperimental results of the multi-head variant are reported in Section 5.8.\n\n\n> Q5: Why is the $L_2$ norm of action difference a good measure for behavioural diversity?\n\nWe adopt the L2 norm in the pervious experiments mainly because it is intuitive and the overall results are similar to other more complex metrics. Since the purpose of Table 3 is mainly for providing some qualitative understanding of the proposed method. Therefore, we select the MSE-based metric due to its simplicity.\nWe followed the reviewers' suggestions and tried some other metrics, i.e., mutual information $I(S;A)$ and KL divergence $D_{KL}(\\pi_i,\u00a0\\pi_j)$ to approximate policy diversity in the updated paper.\n\n> Q6: It would be great if the authors can release the codes for the baseline algorithms as well\n\nWe have submitted the code of the proposed method in supplementary material at the time of our original submission. \nFollowing your suggestion, we will release more code including some baselines, the visualization scripts etc by including a Github link in the final version."
                    },
                    "title": {
                        "value": "Reply to Reviewer 6JRU"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2065/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700684655028,
                "cdate": 1700684655028,
                "tmdate": 1700690114426,
                "mdate": 1700690114426,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "eovyHd6fjQ",
            "forum": "vqHvfEUxGu",
            "replyto": "vqHvfEUxGu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2065/Reviewer_AU7q"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2065/Reviewer_AU7q"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a collaborative approach to improving exploration in reinforcement learning (RL). Rather than using a single agent, it introduces multiple agents that interact with separate environments in parallel. The key ideas are:\n\n- Multi-agent formulation: Maintain a set of N parallel agents, each with its own policy and environment. All agents share a common replay buffer.\n- Collaborative reward generator: Calculate an intrinsic reward for each agent that encourages it to visit novel states not explored by other agents. This induces collaboration and specialization between agents.\n- Collaborative data collection: Agents can also explicitly coordinate during data collection, e.g. selecting diverse actions compared to others.\n  \nThe approach is evaluated on DM Control tasks. Results show improved exploration and performance compared to single agent baselines and other intrinsic reward methods like RND and ICM. The idea of collaborative reward is general and can be integrated with different algorithms."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Intuitive idea of converting single agent RL to collaborative setting to improve exploration.\n- Flexible framework that can work with different intrinsic reward designs.\n- Showcases benefits over reasonable baselines like RND and ICM.\n- Evaluated on established DMC benchmark tasks.\n- Collaborative rewards induce specialization between agents and \n- General idea that can be integrated with many RL algorithms."
                },
                "weaknesses": {
                    "value": "Some of the obvious issues like increase in computation cost proportional to number of agents as well as training wall-clock time should be noted in the main paper itself. Overall given about 2x increase in computational cost, the minor improvements in performance don't seem _that_ significant and likely achievable with standard methods trained for as long (with appropriate hyperparameters).\n\nConnections with parameter sharing in MARL literature (for example [1, 2, 3, 4]) are completely missing. The general setup of using environment experience from 'multiple' agents to train a single policy is also used in those contexts and connections with explorations as investigated here might be relevant in those contexts as well. In general the literature review seems too biased towards just the last couple years of RL papers.\n\n[1] https://arxiv.org/abs/2005.13625\n\n[2] https://arxiv.org/abs/2102.07475\n\n[3] https://link.springer.com/chapter/10.1007/978-3-319-71682-4_5\n\n[4] https://openreview.net/forum?id=YVXaxB6L2Pl"
                },
                "questions": {
                    "value": "- How does contrastive reward work in early parts of training when initial policies are mostly random?\n- How well would it scale to different kind of environemnts like Atari? \n- Any intuitions on how to balance intrinsic vs task rewards?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2065/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698821294657,
            "cdate": 1698821294657,
            "tmdate": 1699636138431,
            "mdate": 1699636138431,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "YcR4PYPTdq",
                "forum": "vqHvfEUxGu",
                "replyto": "eovyHd6fjQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2065/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2065/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the constructive review and insightful comments. Responses to the questions are below:\n\n> Q1: increase in computation cost and minor performance improvements\n\nThanks for the suggestion. We added a description of the running time in Section 5.2.\nDue to the inherent non-determinsticness of RL, we admit that the proposed method sometimes only achieves minor performance improvements. This usually occurs on simple tasks., i.e., cheetah-run and reacher-hard in DMC, where a single agent can easily solve the task. The benefits of using collaborative information are more significant in some tasks where exploration is more difficult, i.e., hopper-hop and humanoid-run.  In these tasks, a single agent might get stuck in sub-optimal behaviors while leveraging the collaborative information is a simple and robust method to improve the performance.\nDue to the variation caused by tasks, it is better to compare the overall performance across a set of tasks. In Figure 3, we report the aggregated performance over 15 DMC tasks.\n\nFurthermore, Figure 11 in the Appendix D.6 shows the results of running a single agent for more wall-clock running time. We can observe that a single agent still performs worse when it trains for a similar wall-clock running time.\n\n\n> Q2: connections with parameter sharing in MARL literature\n\nThanks for the reminder. We added discussions of the connections and differences of our work with parameter sharing in MARL in both Section 2 and Appendix A. We also added a multihead variant of CE in Section 5.8, which uses parameter-sharing.\n\n> Q3: How does contrastive reward work in early parts of training when initial policies are mostly random?\n\nIn the early parts of training:\n1. When initial policies are mostly random, different agents usually take low-quality actions and receive near zero rewards from the environment.\n2. The introduced collaborative intrinsic reward encourages the agents to take actions that are not only novel to itself but also to the other agents. (Please refer to the reply of Q1 to R2). Actually, this is related to your other question on the balancing between intrinsic and task rewards. \n3. When some of the agents sampled good actions and received high rewards, then this information is shared to the other agents via the shared replay buffer.\n\n\n> Q4: How well would it scale to different kinds of environments like Atari?\n\nWe added extra experiments on the Atari games in Appendix D.7. We compare the proposed method to DQN on the Asterix, BeamRider, Breakout, and SpaceInvaders environments. In each environment, we train the agent for 5e6 environmental steps, and we evaluate the agent for every 2e5 steps. We run for 5 random seeds and report the mean evaluation score and standard deviation. We can observe that the proposed CE method also achieves good performances in Atari tasks.\n\n> Q5: Any intuitions on how to balance intrinsic vs task rewards?\n\n- In the current algorithm, as the quality of sampled data increases, the scale of the task will gradually outweigh the intrinsic reward.\n- A widely used practice is to linearly decay the intrinsic reward coefficient during the training.\n- Or we can adopt some more advanced method [1] to tune the intrinsic reward weight automatically.\n\n[1] Yuan, Mingqi, et al. \"Automatic Intrinsic Reward Shaping for Exploration in Deep Reinforcement Learning.\" arXiv preprint arXiv:2301.10886 (2023)."
                    },
                    "title": {
                        "value": "Reply to Reviewer AU7q"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2065/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700684549537,
                "cdate": 1700684549537,
                "tmdate": 1700690057718,
                "mdate": 1700690057718,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "dwb5kByQl5",
            "forum": "vqHvfEUxGu",
            "replyto": "vqHvfEUxGu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2065/Reviewer_JJ8w"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2065/Reviewer_JJ8w"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents collaborative exploration a framework for multi agent exploration in reinforcement learning. In this framework each agent interacts with its copy of the environment and all agents share information. The authors propose an exploration bonus that is aware of other agents to incentivize every agent to visit unexplored locations. The bonus can be implemented using a constrative reward or using RND. The proposed method is then evaluated on DMC15 tasks including pixel based tasks and is shown to perform better the baselines."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper takes on the interesting topic of exploration in reinforcement learning. While single-agent scenarios have received more attention, the multi-agent setting remains a bit of an unexplored territory, even though it holds promise for substantial improvements.\nThe paper is well-written and easy to follow. The method they suggest is put through its paces across various tasks, and it consistently shows better results.\nThe visual aids in Figures 5 and 6 are appreciated and help us get a better grasp of how the proposed algorithm works."
                },
                "weaknesses": {
                    "value": "The paper uses the term collaborative exploration when this subject has been quite studied in the past under the name \"concurrent exploration\". The paper is also missing a lot of existing work in this area [1-6]. I think it is worth adding these references in the next revision of the paper as section 3.1 could seems like the author came with concurrent exploration.\n\nIt seems like the time complexity for computing the embeddings in section 4.1 is linear in the number of agents and number of states in the replay buffer. This seems detrimental, I assume that we're interested in using a multi agent method because we care more about wall clock time than sample complexity, in that case the additional cost to compute the embedding may not be worth it. Table 10 shows that the wall clock times doubles with collaborative exploration.\n\n\n[1] Concurrent Reinforcement Learning from Customer Interactions, Silver et al. ICML 2013\n[2] Coordinated Exploration in Concurrent Reinforcement Learning, Dimakopoulou et al., ICML 2018\n[3] Scalable Coordinated Exploration in Concurrent Reinforcement Learning, Dimakopoulou et al., NeurIPS 2018\n[4] Regret Bounds of Concurrent Thompson Sampling, Chen et al. NeurIPS 2022\n[5] Efficient PAC-Optimal Exploration in Concurrent, Continuous State MDPs with Delayed Updates, Pazis\n[6] Introducing coordination in concurrent reinforcement learning, ICLR 2022 workshop"
                },
                "questions": {
                    "value": "\"Eqn.(5) encourages the agent to visit states that are not only novel from its own perspective, but also\nto respect other agents\u2019 intrinsic motivation in pursuit of novelty.\"\nIf I understand Eqn (5) correctly it is pushing agent i towards states it has not been but also towards states that have been visited by other agents?\n\n\"Simply, we can use the softmax function as the classifier\" (section 4.2) and \" In the following experiments, we always select the first agent for evaluation.\" (section 5).\nDoes it mean that only collaborative exploration is able to use the softmax action selection. This might be important as CE implicitly uses a higher capacity policy.\n\nAn ablation study would have helpful to understand the impact of each component of collaborative exploration, particularly the sofmax action selection procedure.\n\nIn section 5.3 / Figure 4, could you add RND?\n\nOn Figure 5 why do we still more trajectories going towards the bottom? Shouldn't it be optimal to split and the two goals equally?\n\n\"In the experiment, we use \u03bb = 1 for the UCB method, \u03f5 = 0.2 and M = 10 for the \u03f5-collaborative method. \u03f5-greedy method also uses \u03f5 = 0.2\" can you explain how hyperparameters were tuned?\n\nThe number of agents used for experiments (2 - 8) is quite low, could you add some results with 32, 64, 128? It would be interesting to see how the methods scale with the number of agents. It is possible that the reward becomes too noisy and lead to worse results. It also likely explains why CE4 does better than CE8 in Table 7."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2065/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699053946027,
            "cdate": 1699053946027,
            "tmdate": 1699636138340,
            "mdate": 1699636138340,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rGThUCjV0S",
                "forum": "vqHvfEUxGu",
                "replyto": "dwb5kByQl5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2065/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2065/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the constructive review and insightful comments. Responses to the questions are below:\n\n> Q1: missing related work on concurrent exploration.\n\nThanks for the suggestion. We first added a paragraph dedicated to concurrent exploration in the related work section and also explain the connection between our work and concurrent exploration in Section 3.1.\n\n\n> Q2: time complexity for computing the embeddings is linear in the number of agents and number of states in the replay buffer.\n\nYes.  Denote the memory buffer size as M, embedding dimension as d, and the agent number as N. Then, the time complexity to compute the KNN intrinsic reward is $O(KNMd)$.\nTo mitigate this issue, we use a memory buffer for each agent with a relatively small size (M = 300).\n- In practice, as discussed in the response to Q7 for R1, $N$ does not need to be a large number to strike the balance between complexity and benefits brought to exploration. \n\n> Q3: meaning of intrinsic reward\n\nFrom the definition of the intrinsic reward, we can observe that it is composed by two parts:\n\n- ego part: encourages the agent to take action to visit states that are less visited by itself.\n- mutual part: encourages the agent to take action to visit states that are less visited by the other agents.\n\nWe use a reward weight to trade-off these two parts. Overall, this intrinsic reward encourages agents to take different behaviours and visit less visited states from a holistic perspective. \n\n> Q4: only collaborative exploration is able to use the softmax action selection\n\nThe proposed collaborative data collection and collaborative evaluation are optional and only used in Section 5.6 and Section 5.7.\nIn Section 5.2/5.3/5.4, we always select the first agent for evaluation to validate the effectiveness of the proposed collaborative intrinsic reward.  We use the softmax action selection in Section 5.7 to validate the effectiveness of using collaborative information in the evaluation phase. Moreover, the softmax action selection is just a simple example to raise the point that using collaborative information in the evaluation is effective. More complex algorithmic designs can be used as well, which we leave for a future work."
                    },
                    "title": {
                        "value": "Reply to Reviewer JJ8w [Part I]"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2065/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700684359963,
                "cdate": 1700684359963,
                "tmdate": 1700690008051,
                "mdate": 1700690008051,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WGBhE9SOFz",
                "forum": "vqHvfEUxGu",
                "replyto": "dwb5kByQl5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2065/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2065/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> Q5: Ablation study for each component\n\nThanks for the suggestion. We provide the ablation study for each component in Appendix D.2. In Table 6, we can observe that combining the three components performs the best except for the fish-swim task, where using the collaborative evaluation is effective and using the collaborative data collection hurts the performance. This is because the fish-swim is a challenging goal-reaching task, and adding the collaborative data collection might distract the agent from reaching the target goal when some other agents sample sub-optimal actions. Overall, removing the collaborative training (w/o Col-Training) performs the worst, which indicates that collaborative training via the proposed collaborative reward generator is the most effective component.\n\n> Q6: In section 5.3 / Figure 4, could you add RND?\n\nThanks for the suggestion. We have added the RND in Figure 4. The proposed CE method also outperforms RND in the visual task experiments.\n\n> Q7: Why are more trajectories going to the bottom in Figure 5?\n\nIt's mainly due to the randomness inherent in the agent's data unrolling and how we sample the trajectories for visualization. When the agent is well trained, we will on average have nearly the same number of trajectories towards the upper goal and the bottom goal. \nIn the previous experiments, we have chosen to plot one trajectory for every 20 trajectories to avoid cluttered plots. However, this 1/20 ratio might be a bit too large, which can lead to sampling bias and make the figure mis-leading. \nIf we plot more frequently, i.e., one trajectory for every 10 trajectories, then trajectory distribution will be more balanced. We have updated Figure 5 in the revised paper accordingly.\n\n>  Q8: How parameters are selected\n\nMainly following some previous settings from the SUNRISE paper and did not tune much.\nBetter results are possible with more tuning.\n\n> Q9: More agent numbers 32/64/128\n\nThanks for the suggestion. We added results of using 16/32/64 agents in Table 7 in Appendix D.3. The performance of scaling to more agents actually depends on the tasks.\nIt's notable that there is a trade-off between policy diversity and training stability:\n- With more agents, we have more diverse policies and help to solve tasks where exploration is particularly difficult, i.e., the goal-reaching fish-swim task. \n- On the other hand, the off-policyness of the sampled data increases as we have more agents. Given a sampled batch, there are only on average 1/N samples that are collected by each agent. As the number N increase, the problem becomes closer to an offline RL setting, where the optimization becomes more challenging, i.e., the performance variances increase significantly for larger N.\nFor the proposed CE agents, it is also indeed possible that the reward becomes too noisy and lead to worse results. Therefore, using a medium size agent set strikes a good balance of exploration and optimization."
                    },
                    "title": {
                        "value": "Reply to Reviewer JJ8w [Part II]"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2065/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700684373607,
                "cdate": 1700684373607,
                "tmdate": 1700690165668,
                "mdate": 1700690165668,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "fW79g0kedl",
            "forum": "vqHvfEUxGu",
            "replyto": "vqHvfEUxGu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2065/Reviewer_ArFd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2065/Reviewer_ArFd"
            ],
            "content": {
                "summary": {
                    "value": "Authors present a parallelized training algorithm for a single agent in view of pooling experience samples for exploration. The proposed method is tested on DeepMind Control Suite against several baselines across multiple scenarios."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "Originality\nIt is hard to assess the originality of the submission; the proposed method is not clearly distinguished from A3C, Ape-X, or IMPALA, save a brief mention in Appendix A.\n\nQuality\nA variety of evaluation setups and baselines have been incorporated. Presented figures and tables clearly indicate a superior performance in most setups.\n\nClarity\nFigure 2 serves as an apt visual abstract for the proposed method. Reproducibility efforts have been included where applicable.\n\nSignificance\nIt is difficult to gauge whether or how the algorithm works well, given that its special design oriented towards better exploration still underperforms Replica in HalfCheetah-V3 in terms of the variance proxy and that no further explanation is provided as to how that might be the case."
                },
                "weaknesses": {
                    "value": "First of all, the \u201cmulti-agent\u201d phrasing is misleading. Nowhere in A3C, Ape-X, or IMPALA is the term used to refer to parallel instantiations of one agent. Repurposing a well-established terminology should be accompanied by far more solid evidence than a mere footnote. Related works are poorly taxonomized. For instance, if MARL research and the proposed method are indeed \u201cvery closely related\u201d, how is it that no MARL algorithm is tested against?\nImportant works on diversity, such as Diversity Is All You Need, are not discussed, and no comparison is made against information-theoretic classes of diversity-objective RL algorithms.\nDespite admitting a resemblance with distributed RL, none of the cited algorithms is set up for comparative evaluation.\nThere is no justification as to how the variance proxy may be a better measure for exploration than, say, mutual information, as in MAVEN.\nOverall, there is a mixup of neighboring lines of research, terminology, and taxonomization that make the paper exceedingly difficult to follow and its contributions hard to assess. Furthermore, agent parallelization works have long shown to be faster and scalable ways to populate state-action visitation matrices, so claiming that most existing works take a single-agent perspective is a complete disregard to several rich lines of research predating this submission."
                },
                "questions": {
                    "value": "How does CE compare against DIAYN?\nHow does CE scale with number of agent instances?\nHow does the KNN component scale with number of agent instances?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2065/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699271848936,
            "cdate": 1699271848936,
            "tmdate": 1699636138264,
            "mdate": 1699636138264,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Ei7Io6aPGq",
                "forum": "vqHvfEUxGu",
                "replyto": "fW79g0kedl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2065/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2065/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the constructive review and insightful comments. Responses to the questions are below.\n\n> Overall Question: \"There is a mixup of neighboring lines of research, terminology, and taxonomization that make the paper exceedingly difficult to follow and its contributions hard to assess.\"\n\nWe apologize for the inconvenience. This is indeed related to some confusions brought by our mis-usage of some terms etc. We read your comments carefully and categorize your concerns into two categories: (1) presentation related: Q1-Q3; (2) comparison related: Q4-Q6. \nWe have addressed each category from several aspects as detailed below and hope the reviewer can reassess the paper based on our response and the revised paper. \n\n> Q1: \"the multi-agent term is misleading ...\"\n\nWe apologize for the confusion and fully agree with you that the \"multi-agent\" term is indeed mis-leading. In our original submission, we were aware that there might be a confusion due to borrowing a term from another field, and intended to clarify it with a footnote in the original manuscript. However, it turns our that, as you commented, \"repurposing a well-established terminology\" does not seem to be a good strategy. Given the confusions it introduced, we fully agree with you on this point and have revised related terms and sections to address this.\nIn term of coming up with a proper term, as you have suggested, our work is indeed aligned with the line of parallel agent, and we further incorporated collaboration among them, therefore we have changed the term \"multi-agent\" to \"collaborative parallel agents\" to reflect both points.\n\n\n> Q2: \"the proposed method is not clearly distinguished from A3C, Ape-X, or IMPALA...\"\n\nWe have detailed the relationship of the proposed method and parallel agents work [A3C, Apex-X, IMPALA etc] in the respose to Common Question-1. \nIn short, our work is related to parallel agent in the sense it also has parallel agents. However, the key difference is that we intent to incorporate explicit collaboration among agent and further investigated the question of how and when to collaborate.\nIn more detail, our work differs from these related work in the following aspects:\n- Our work concentrates on the problem of how to collaborate between agents to improve exploration, while A3C, Ape-X and IMPALA mainly focus on improving data throughput in the parallel agent setting.\n- There is no explicit collaboration mechanism in A3C, Ape-X and IMPALA apart from the implicit way via data sharing. In our work, we attempt to answer the question of how to collaborate more effectively beyond data sharing and incorporate explicit collaboration.\nWe rewrote the related sections to distinguish our work and these parallel agent work more clearly. \n\n> Q3: \"claiming that most existing works take a single-agent perspective is a complete disregard to several rich lines of research predating this submission\"\n\nWe apologize for the confusion. We have used an imprecise term here.\nBy \"most existing works take a single-agent perspective\", we intended to mean that that many existing work do not take an explicit collaborative perspective, either because there is only one agent (where there is no structural support for collaboration), or parallel agents are used without explicit collaborations among them.\nWe have revised this sentence to make it more precise and rewrote the related sections to clarify the connections and differences of our work with previous works on parallel agents, i.e., Section 2 in the revised paper.\n\n> Q4: \"underperforms Replica in HalfCheetah-V3 in terms of the variance proxy\"\n\nWe are sorry that we did not provide further explanation in the original paper due to the space limit, and we rewrote this section in the updated manuscript.\nSimilar to the cheetah-run task in DMC, the HalfCheetah-V3 is a more like an easy task, where a single agent already achieves near-optimal performance. We added the evaluation rewards for each agent in the Table 3. We can observe that Replica agent achieves 11552.8 and CE agent achieves 11545.1, which are both very close to the optimal results in this task. Therefore, in the HalfCheetah-v3 task, the learned policies usually converge to the near-optimal policies which are less diverse. \nWe followed the reviewers' suggestions and tried some other metrics, i.e., mutual information $I(S;A)$ and KL divergence $D_{KL}(\\pi_i,\u00a0\\pi_j)$ to approximate policy diversity in the updated paper (Table 3)."
                    },
                    "title": {
                        "value": "Reply to Reviewer ArFd [Part I]"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2065/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700684163750,
                "cdate": 1700684163750,
                "tmdate": 1700731534396,
                "mdate": 1700731534396,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CxojbOYoti",
                "forum": "vqHvfEUxGu",
                "replyto": "fW79g0kedl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2065/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2065/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> Q5: \"Missing baseline for distributed RL\"\n\nThanks for the suggestion. We conducted additional experiments comparing our approach to A2C in the updated Table 1. We adopt the parameter settings from https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail/tree/master, where we run the A2C agent for 1e7 environmental steps. Notably, A2C usually underperforms other baselines, even with 10 times total environmental steps. This aligns with expectations, given that A2C-like agents (A2C/A3C/PPO) typically exhibit low sample efficiency compared with other off-policy algorithms in popular continuous control benchmarks. Our empirical results align with external evidence with on A2C and PPO, e.g.:\n- A2C is reported to have lower sample efficiency (compared to e.g. SAC) reported by StableBaselines on MuJoCo: https://github.com/DLR-RM/stable-baselines3/issues/48\n- PPO is reported to have similar performances on the DMC benchmark: https://github.com/RyanNavillus/PPO-v3\n\n> Q6: \"compare to DIAYN\"\n\nThanks for the suggestion. We are sorry that we missed the information theory-based baseline, DIAYN, in the original paper. In the previous experiments, we mainly focused on methods that do not change the model architecture of the backbone algorithm, so we overlooked this important baseline which learns a skill conditioned policy $\\pi(a \\vert s, z)$.\nNotably, the experiment setting for DIAYN is different from the other baselines, which first pre-trains a set of skills with the pseudo-reward $r_z(s, a) = \\log q_\\phi(z \\vert s) - \\log p(z)$ and then fine-tunes the best-performing skill with the task reward. It's a little bit tricky to make fair comparison for DIAYN with other baselines if we use the same number of total environment interactions. In this supplementary experiment, we ignore this constraint and simply let the DIAYN agent run 2 times of the total environmental steps. We first use 1e6 environmental steps for pre-training and use another 1e6 environmental steps for fine-tuning. We run the experiment using the official code (https://github.com/haarnoja/sac/blob/master/DIAYN.md).\nWe added the results of DIAYN to Table 1 and Figure 11. From the results, CE outperforms DIAYN in most tasks. DIAYN performs worse than CE mainly because the skills discovered in the unsupervised skill learning stage are not necessarily well aligned with the down stream tasks. \n\n\n> Q7: \"how does the method scale for more agents\"\n\nThanks for the suggestion. We added results of using 16/32/64 agents in Table 7 in Appendix D.3. The performance of scaling to more agents actually depends on the tasks.\nIt's notable that there is a trade-off between policy diversity and training stability:\n- With more agents, we have more diverse policies and help to solve tasks where exploration is particularly difficult, i.e., the goal-reaching fish-swim task. \n- On the other hand, the off-policyness of the sampled data increases as we have more agents. Given a sampled batch, there are only on average 1/N samples that are collected by each agent. As the number N increase, the problem becomes closer to an offline RL setting, where the optimization becomes more challenging, i.e., the performance variances increase significantly for larger N.\nTherefore, using a medium size agent set strikes a good balance of exploration and optimization.\n\n\n> Q8: \"scale for KNN components wrt #agents\"\n- Denote the memory buffer size as M, embedding dimension as d, and the agent number as N. \n- Then, the time complexity to compute the KNN intrinsic reward is $O(KNMd)$, which is linear w.r.t. the agent number.\n- In practice, as discussed in the response to Q7, $N$ does not need to be a large number to strike the balance between complexity and benefits brought to exploration."
                    },
                    "title": {
                        "value": "Reply to Reviewer ArFd [Part II]"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2065/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700684192376,
                "cdate": 1700684192376,
                "tmdate": 1700705410658,
                "mdate": 1700705410658,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]