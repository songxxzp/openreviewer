[
    {
        "title": "EquiAV: Single-modal Equivariance Promotes Audio-Visual Contrastive Learning"
    },
    {
        "review": {
            "id": "fSnLdGjQE7",
            "forum": "9k4Yvb75ED",
            "replyto": "9k4Yvb75ED",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5149/Reviewer_4TFL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5149/Reviewer_4TFL"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces equivariant contrastive learning for intra-model learning in audio-visual self-supervised learning (SSL). It adapts the recently proposed equivariant predictor [1] to predict representations extracted from transformed audio/video from the representations extracted from the untransformed audio/video, respectively in each of the two modalities independently. This replaces the typical invariance loss used in intra-model learning in other audio-visual SSL works e.g. [2], and, as in these previous works, is used in addition to a cross-modal loss, which in this paper enforces invariance as in previous works. Through ablations on invariance vs. equivariance, different augmentations, the changes made to the loss proposed in [1], and different types of initialization for the backbones, the authors reach an optimal training strategy and achieve competitive results on audio/visual/AV classification on AudioSet and VGGSound, and also audio-visual retrieval. \n\n\n\n[1] https://openreview.net/pdf?id=eDLwjKmtYFt\n[2] https://arxiv.org/pdf/2212.08071v2.pdf"
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The paper is well-written and easy to follow. The narrative and motivation are simple but quite clear and make sense overall. The results seem to confirm the hypotheses that are laid out in the introduction. \n\nThe methodology is well explained and does not overcomplicate the description of the losses and learning strategies used in this work/previous works. \n\nFigures 2 and 3 are well-made and highlight the important parts of the method.\n\nThe comparison with other works on classifications features a wide range of previous works, table 4 presents a very thorough and welcome ablation study, and the tables are in general clear and well-presented.\n\nThe appendix features some further ablations and a lot of details, which lead to some reasonably insightful conclusions and further validate the paper's proposed methodology."
                },
                "weaknesses": {
                    "value": "In short, I think this paper's contribution is not really sufficient, and it is difficult for me to recommend that it be accepted to such a conference in this state, even though it is technically and scientifically sound work.\n\nBasically, what the paper does is apply the intra-model equivariance loss from [1], modify it so that, in the authors' own words, it \"differs slightly\" from that original loss, and then apply it to each of the modalities (audio and video). The inter-modal loss, which is what makes AVSSL unique, is unchanged. Therefore it is solely applying the methodology of [1] to the modalities of audio and video, and then fine-tuning the hyperparameters as would be done for any uni-modal framework. It is therefore unsurprising, and a direct conclusion of [1], that this would work in some way. Therefore I just don't think there is enough novelty/creativity to this approach to call it truly novel. \n\nWe can compare this to, for example, CAV-MAE, which extends AudioMAE but clearly distinguishes itself by proposing a new framework with many new aspects that are unique to their work and exploring how to best model the interactions between audio and video in audio-visual SSL. The same can be said about MAViL, and others - their contributions are more than the direct, naive application of an existing SSL strategy.\n\nThe lack of novelty would be acceptable if the results were state-of-the-art by far, but in Table 1 they are outperformed by MAViL in some cases, and in Table 2 they are only compared with a single previous work. As a side note, I don't think it's reasonable to grey out MaVIl as concurrent work in Table 1 - the paper was released on arXiv in Dec. 2022.\n\nFinally, another paper that combines intra-modal and inter-modal losses in self-supervised audio-visual learning (although not in a contrastive way, like MAVil) is RAVEn [2]. Although it is likely not a useful comparison since they only experiment with speech, this is perhaps worth adding to the discussion, especially since their intra-model loss seems to have similar goals to yours (they don't enforce invariance - instead, they leverage a predictor).\n\nApart from this, I only found a small typo in the title of section C of the appendix: \"ADDTIONAL EXPERIMENTS\" should be \"ADDITIONAL EXPERIMENTS\"\n\n[1] https://openreview.net/pdf?id=eDLwjKmtYFt\n[2] https://arxiv.org/abs/2212.06246"
                },
                "questions": {
                    "value": "Are you planning to release 1. inference code 2. pre-trained models and 3. training code? These would be very welcome, and an important contribution to the audio-visual SSL community."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5149/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5149/Reviewer_4TFL",
                        "ICLR.cc/2024/Conference/Submission5149/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5149/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697533409222,
            "cdate": 1697533409222,
            "tmdate": 1700146853893,
            "mdate": 1700146853893,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "XM0jKVUcwq",
                "forum": "9k4Yvb75ED",
                "replyto": "fSnLdGjQE7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5149/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5149/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer 4TFL (1/2)"
                    },
                    "comment": {
                        "value": "Dear Reviewer 4TFL,\n\nThank you for your constructive and insightful comments. We respond to your questions point-by-point in the following paragraphs.\n\n&nbsp;\n\n---\n\n> it is solely applying the methodology of [1] to the modalities of audio and video, and then fine-tuning the hyperparameters as would be done for any uni-modal framework. It is therefore unsurprising, and a direct conclusion of [1], that this would work in some way.\n> \n\nAs self-supervised learning through invariance has been extended, some studies have shown that applying the concept of equivariance leads to an additional performance boost in various domains such as images and text. Specifically, in the image domain, the concept of equivariance has been applied to SimCLR through developments like E-SSL [2] and EquiMod [1]. Similarly, in the text domain, SimCSE [3] has been advanced with the application of equivariance as seen in DiffCSE [4].\n These papers show the process of adopting the idea of equivariance for representation learning in each domain. On the other hand, to our knowledge, there is no prior work that incorporates equivariance for audio modality and audio-visual representation learning.\nAlong these lines, we aim to design a framework for employing equivariant representation learning in audio-visual representation learning and demonstrate its benefits. However, extending the single-modal equivariant representation learning to multi-modal representation learning is not trivial. Our additional ablation studies (Table 3 in the revised manuscript) reveal that applying equivariance to inter-modal latent spaces is not beneficial and may even hinder the learning of audio-visual correspondences and joint representations. We found that augmentation-related information and audio-visual correspondence need to be learned in separate latent spaces to overcome this challenge. In addition, simply replacing the single-modal invariant representation learning with the equivariant one doesn\u2019t lead to performance improvement. Consequently, we have meticulously designed a framework that maximizes the benefits of equivariance, while avoiding negative impacts on learning. Each component of our framework has been verified to be effective through ablation studies, including:\n\n1) Loss function (Table 5),\n\n2) Data augmentations for effective equivariant representation learning (Table 4), and\n\n3) Projection head and augmentation encoder (Table C in Appendix).\n\nTaken together, all of these factors indicate that our contribution goes beyond simply applying existing techniques to create a new and effective framework for audio-visual representation learning.\n\n&nbsp;\n\n---\n\n> The lack of novelty would be acceptable if the results were state-of-the-art by far, but in Table 1 they are outperformed by MAViL in some cases, and in Table 2 they are only compared with a single previous work.\n> \n\nThere are a couple of key factors to consider in the performance of MAViL: \n- Firstly, MAViL utilizes 8 frames for training, in contrast to the single frame used in CAV-MAE and our work. Although using more frames improves performance, it significantly increases computational costs (MAViL uses 64 gpus vs. EquiAV uses 8 gpus). \n- Secondly, MAViL adopts a two-stage training approach. The first stage involves training the model with masked modeling and contrastive learning techniques, followed by a second stage of retraining using knowledge distillation (KD). It's worth noting that KD could potentially boost performance in other studies too, such as CAV-MAE and Audiovisual MAE, including our own.\nIn addition, we have compared our work with various other methods by reporting zero-shot retrieval results on the MSR-VTT dataset, as shown in Table A. The revised manuscript also includes further fine-tuning experiments on UCF101, HMDB51, and ESC50, which can be found in Table B.\n\n&nbsp;\n\n---\n\n> As a side note, I don't think it's reasonable to grey out MAViL as concurrent work in Table 1 - the paper was released on arXiv in Dec. 2022.\n> \n\nAs you correctly noted, MAViL was initially uploaded in December 2022. Our decision to classify it as concurrent work stems from the fact that at the time of our submission in September 2023, we were referencing its revised version from July 2023. Given that MAViL demonstrates performance comparable to our study on the same benchmarks, we included it in our main results following the guideline of covering all relevant papers. If there has been any oversight on our part in this regard, we are open to making any necessary revisions to how MAVil is mentioned in our paper."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5149/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700080047512,
                "cdate": 1700080047512,
                "tmdate": 1700153187186,
                "mdate": 1700153187186,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "31JmYwyHjY",
                "forum": "9k4Yvb75ED",
                "replyto": "fSnLdGjQE7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5149/Reviewer_4TFL"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5149/Reviewer_4TFL"
                ],
                "content": {
                    "comment": {
                        "value": "I really appreciate the detailed response.\n\nRegarding novelty, I understand your argument and am not undermining the effort it took to make this configuration work, but still firmly believe that the contribution is not sufficient for a full ICLR paper.\n\nRegarding MAViL, I understand your argument since the results have been updated in the July version but even then I think it's unreasonable to grey it out as concurrent work. I think it's a reasonable comparison.\n\nThank you for adding RAVEn as a reference, correcting the typo, and agreeing to release the code.\n\nOverall, while I appreciate the authors' detailed responses and new experiments, I don't think it's appropriate to raise the score since I still think the lack of novelty is a major issue."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5149/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700146833184,
                "cdate": 1700146833184,
                "tmdate": 1700146906410,
                "mdate": 1700146906410,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FLPbo0oeGw",
                "forum": "9k4Yvb75ED",
                "replyto": "nsji2EmJal",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5149/Reviewer_4TFL"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5149/Reviewer_4TFL"
                ],
                "content": {
                    "comment": {
                        "value": "I acknowledge, understand, and appreciate your response, but respectfully still stand by my earlier comments."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5149/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700471870993,
                "cdate": 1700471870993,
                "tmdate": 1700471870993,
                "mdate": 1700471870993,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "gnwR2pOn80",
            "forum": "9k4Yvb75ED",
            "replyto": "9k4Yvb75ED",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5149/Reviewer_A7KE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5149/Reviewer_A7KE"
            ],
            "content": {
                "summary": {
                    "value": "This work introduces EquiAV, a framework to incorporate data augmentation in audio-visual representation learning. Modality specific information is learned in a separate space from the audio-visual correspondence space. Furthermore, the modality-specific representations are learned such that they are equivariant to data augmentations. This work demonstrated the benefits of the proposed framework through downstream tasks of event classification and zero-shot retrieval. In a addition to this, an ablation study is also provided to show model robusteness to augmentation along with the proposed approach's ability to maximize the benefits of augmentation itself."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Originality :\n\nThe work is somewhat original as it combines a contrastive learning framework with an equivariance framework that was extended to a multimodal formulation from a single modality formulation. \n\nQuality :\n\nThis work creates a well structured framework to introduce data augmentations as part of a contrastive loss based learning of audio-visual representations. The proposed approach is supported by ablation studies and explorations on several downstream tasks. However, there are some questions that come up.\n\nClarity :\n\nThis work is somewhat clear although there are some parts (such as those mentioned under Questions) that could be made more clear.\n\n\nSignificance:\n\nData augmentation is one of the ways to not just improve the performance of models but also to increase their robustness. To this end, the proposed work presents a step forward in the context of audio-visual (and potentially general multimodal) representation learning."
                },
                "weaknesses": {
                    "value": "Although the work is well structured, some of the decisions/formulations are not fully explained (as mentioned in the Questions section). This work can also benefit by providing additional evidence to see of its claims through different downstream tasks and ablation studies (as indicated in the Questions section)"
                },
                "questions": {
                    "value": "If the loss is formulation (1) is contrastive in nature then the $\\mathcal{L}$ should not just output the dissimilarity as the optimization minimizes $\\mathcal{L}$ and contrastive loss is about maximizing similarity of aligned data and minimizing similarity of non-aligned data. I assume that the author's meant a formulation of contrastive loss and not just a dissimilarity measuring mechanism. Please clarify.\n\nConceptually, the augmented and non-augmented variants can be considered as two different modalities. Therefore, standard contrastive loss setup (as used in Eq 11) can be used. Is there a specific reason for using the Eq 9 formulation.  \n\nFurthermore, the reasoning behind inclusion of  'positive pair ...in the denominator ' is not clear. The appendix does seem to allude to the different weights during gradient calculations but does not elaborate on why those specific weights should necessarily be a factor. Table 5 does give results that demonstrate that it can be important but there is not an explanation as to why it is so. \n\nThere are two types of features for each modality, one from the head corresponding to the inter-modal space and the other from the intra-modal space. Perhaps I missed it, but it is not apparent which features were used in the experiments. \n\nAccording to the manuscript, Table 4 implies 'that by utilizing equivariant intra-modal representation learning, our model is able to capture augmentation-related information within each modality'. It would be good to dig deeper into this facet perhaps through heatmaps to show stronger evidence as Table 4 results show that the proposed setup is good for the given downstream tasks which is perhaps not enough evidence to strongly claim the models ability to capture augmentation related information.\n\nAs there are individual modality branches (and multiple types of modality features) available it would be beneficial to have comparisons with other (non-augmentation focused) baselines on common tasks such as unimodal action recognition (such as on UCF and HMDB) and sound classification (such as on ESC) that are often explored in works that explore audio-visual representation learning."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5149/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698512689826,
            "cdate": 1698512689826,
            "tmdate": 1699636509002,
            "mdate": 1699636509002,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "s56cIdgsHi",
                "forum": "9k4Yvb75ED",
                "replyto": "gnwR2pOn80",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5149/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5149/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer A7KE (1/2)"
                    },
                    "comment": {
                        "value": "Dear Reviewer A7KE,\n\nThank you for your valuable suggestions to improve our paper. Please see the point-by-point responses below.\n\n&nbsp;\n\n> If the loss is formulation (1) is contrastive in nature then the $\\mathcal{L}$ should not just output the dissimilarity as the optimization minimizes $\\mathcal{L}$ and contrastive loss is about maximizing similarity of aligned data and minimizing similarity of non-aligned data. I assume that the author's meant a formulation of contrastive loss and not just a dissimilarity measuring mechanism. Please clarify.\n> \n\nFirst, it's correct to consider $\\mathcal{L}$ as a contrastive loss, as you've mentioned. In our work, we aimed to establish a more general formulation for representation learning. Specifically, we formulate it as maximizing the similarity between pairs of inputs, which is equivalent to minimizing their dissimilarity. \nThe four equations described in Section 2 of our manuscript are designed to be applicable not only to contrastive learning but also to other forms of representation learning. Furthermore, these formulations are adaptable to a variety of techniques used to prevent collapse in Siamese networks, such as the use of 'stop gradient', illustrating their broad applicability in recent advancements in the field.\n\n&nbsp;\n\n---\n\n> Conceptually, the augmented and non-augmented variants can be considered as two different modalities. Therefore, standard contrastive loss setup (as used in Eq 11) can be used. Is there a specific reason for using the Eq 9 formulation.\n> \n\nAs you pointed out, they can be treated as two different modalities, which would allow the use of Eq 11. However, this method halves the number of negative pairs (from 2N-2 to N-1), which is similar to decreasing the batch size. Such a reduction in negative pairs can have a negative impact on performance, particularly in the context of contrastive learning. While Eq 11 may have its advantages when using different encoders for significantly different spaces, like in video-audio or image-text pair, we can conclude that Eq 9 is more suitable for intra-modal training.\n\n&nbsp;\n\n---\n\n> Furthermore, the reasoning behind inclusion of 'positive pair ...in the denominator ' is not clear. The appendix does seem to allude to the different weights during gradient calculations but does not elaborate on why those specific weights should necessarily be a factor. Table 5 does give results that demonstrate that it can be important but there is not an explanation as to why it is so.\n> \n\nAs explained in Appendix A of the manuscript, including the positive pair in the denominator results in a relatively larger update for hard positives. This becomes particularly advantageous when stronger augmentations lead to an increased frequency of hard positives. Training with more hard positives, alongside the application of equivariance, substantially enhances the model's capability to comprehend detailed features in single-modal contexts. The intra-modal representation quality plays a pivotal role in understanding and integrating different modalities. Consequently, the semantically rich intra-modal representation promotes effective learning of audio-visual correspondence and audio-visual joint representations. We will add a detailed analysis in Appendix A.\n\n&nbsp;\n\n---\n\n> There are two types of features for each modality, one from the head corresponding to the inter-modal space and the other from the intra-modal space. Perhaps I missed it, but it is not apparent which features were used in the experiments.\n> \n\nWe employed features projected into the inter-modal space for all downstream tasks, and Table C in Appendix C.2 demonstrates which features yield the best performance. As you mentioned, we conducted experiments using features projected into the intra-modal space, as well as features that were processed solely through the encoder without any heads. The results indicate that features projected into the inter-modal space exhibit the best performance.\n\n&nbsp;\n\n---\n\n> It would be good to dig deeper into this facet perhaps through heatmaps to show stronger evidence as Table 4 results show that the proposed setup is good for the given downstream tasks which is perhaps not enough evidence to strongly claim the models ability to capture augmentation related information.\n> \n\nIn response to your suggestion, we have further demonstrated the capability of EquiAV to capture augmentation-related information by conducting a visual sound source localization task. The qualitative results are detailed in Figure B in Appendix D of the revised manuscript. The results demonstrate that EquiAV consistently maintains audio-visual correspondence and accurately identifies the sound source, even in scenarios involving augmented audio-visual inputs. This highlights the strength of our method in not only capturing augmentation-related information within each modality but also in preserving audio-visual correspondence."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5149/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700079766121,
                "cdate": 1700079766121,
                "tmdate": 1700079766121,
                "mdate": 1700079766121,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ee6aCZpfAj",
                "forum": "9k4Yvb75ED",
                "replyto": "cT73dP1jC0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5149/Reviewer_A7KE"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5149/Reviewer_A7KE"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the author response and clarifications"
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5149/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700687096514,
                "cdate": 1700687096514,
                "tmdate": 1700687096514,
                "mdate": 1700687096514,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "txb1yfn3cr",
            "forum": "9k4Yvb75ED",
            "replyto": "9k4Yvb75ED",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5149/Reviewer_eQDc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5149/Reviewer_eQDc"
            ],
            "content": {
                "summary": {
                    "value": "The EquiAV framework proposed in this paper aims to improve audio-visual self-supervised learning by finding a strategy for utilizing data augmentation that maximizes the benefits of the model while maintaining robustness to substantial augmentation. The approach combines single-modal equivariant contrastive learning and audio-visual contrastive learning to learn audio-visual correspondence and modality-specific representations separately. The paper also compared various techniques employed in audio-visual representation learning. The EquiAV ensures that diverse augmentations applied to both audio and visual modalities benefit the model. Experimental results demonstrate that the EquiAV approach outperforms existing state-of-the-art methods in audio-visual event classification and zero-shot audio-visual retrieval tasks. Extensive ablation studies are conducted to demonstrate the effectiveness of the proposed method in learning audio-visual correspondence and enhancing representation capability."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The author uses detailed comparative experiments and ablation experiments to prove the effectiveness and advancement of the EquiAV framework using single-modal equivariant representation learning. The article is also logically clear and uses reasonable diagrams to explain the relevant content clearly.\n- In the experiment, EquiAV showed impressive results and also provided the best settings for Audio-visual data augmentation as a reference for subsequent research."
                },
                "weaknesses": {
                    "value": "- The novelty of the article is limited. The article only applies the single-modal equivariant representation learning that has been proven effective to the A-V learning task and does not try to solve particular problems in this field (for example, compared with the Text-audio, Text-vision field, what specific difficulties can this method solve?)\n- Although the author tried to compare different pre-training methods, he did not clearly explain the advantages of equivariant representation learning.\n- The author lacks design details for augmentation predictors in the article. How does it work? Why can it make the framework achieve better results? What are the specific settings? Whether to only perform linear transformations on original input embeddings and augmentation vectors\n- The author uses InvAV as the baseline but does not give a reference to this solution.\n- Tables 1 and 5 appear in the wrong chapter positions, and the layout of the article needs to be carefully revised."
                },
                "questions": {
                    "value": "- The specific details of Figure 1 are missing. What is the specific meaning of augmentation level? What specific settings were used to draw this figure?\n- In Table 1, the performance of MAViL is better than EquiAV. What is the specific reason?\n- Applying intra-modal representations equivariant to the augmentations has been proven effective in previous research. What is the novelty of this article?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5149/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698757874228,
            "cdate": 1698757874228,
            "tmdate": 1699636508898,
            "mdate": 1699636508898,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "tRnWF0O2B9",
                "forum": "9k4Yvb75ED",
                "replyto": "txb1yfn3cr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5149/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5149/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer eQDc (1/2)"
                    },
                    "comment": {
                        "value": "Dear Reviewer eQDc,\n\nWe appreciate your valuable comments. We list the responses to the concerns below.\n\n&nbsp;\n\n---\n\n> Applying intra-modal representations equivariant to the augmentations has been proven effective in previous research. What is the novelty of this article?\n> \n\nAs self-supervised learning through invariance has been extended, some studies have shown that applying the concept of equivariance leads to an additional performance boost in various domains such as images and text. Specifically, in the image domain, the concept of equivariance has been applied to SimCLR through developments like E-SSL[2] and EquiMod[1]. Similarly, in the text domain, SimCSE[3] has been advanced with the application of equivariance as seen in DiffCSE[4]. \nThese papers show the process of adopting the idea of equivariance for representation learning in each domain. On the other hand, to our knowledge, there is no prior work that incorporates equivariance for audio modality and audio-visual representation learning.\nAlong these lines, we aim to design a framework for employing equivariant representation learning in audio-visual representation learning and demonstrate its benefits. However, extending the single-modal equivariant representation learning to multi-modal representation learning is not trivial. Our additional ablation studies (Table 3 in the revised manuscript) reveal that applying equivariance to inter-modal latent spaces is not beneficial and may even hinder the learning of audio-visual correspondences and joint representations. We found that augmentation-related information and audio-visual correspondence need to be learned in separate latent spaces to overcome this challenge. In addition, simply replacing the single-modal invariant representation learning with the equivariant one doesn\u2019t lead to performance improvement. Consequently, we have meticulously designed a framework that maximizes the benefits of equivariance, while avoiding negative impacts on learning. Each component of our framework has been verified to be effective through ablation studies, including:\n\n1) Loss function (Table 5),\n\n2) Data augmentations for effective equivariant representation learning (Table 4), and\n\n3) Projection head and augmentation encoder (Table C in Appendix).\n\nTaken together, all of these factors indicate that our contribution goes beyond simply applying existing techniques to create a new and effective framework for audio-visual representation learning.\n\n&nbsp;\n\n---\n\n> The specific details of Figure 1 are missing. What is the specific meaning of augmentation level? What specific settings were used to draw this figure?\n> \n\nWe drew Figure 1 based on the ablation results in Table 4. The term \"augmentation level\" refers to the variety and strength of applied augmentations, which correspond to the augmentation settings described in each row of Table 4. For example, augmentation level 0 corresponds to the first row, 1 corresponds to the second row, and so on. We added the notion \u201cFor detailed augmentation settings, refer to Table 4\" to the caption of Figure 1. If there is a better way to describe Figure 1, we would appreciate it.\n\n&nbsp;\n\n---\n\n> In Table 1, the performance of MAViL is better than EquiAV. What is the specific reason?\n> \n\nThere are a couple of key factors to consider in the performance of MAViL: \n- Firstly, MAViL utilizes 8 frames for training, in contrast to the single frame used in CAV-MAE and our work. Although using more frames improves performance, it significantly increases computational costs (MAViL uses 64 gpus vs. EquiAV uses 8 gpus). \n- Secondly, MAViL adopts a two-stage training approach. The first stage involves training the model with masked modeling and contrastive learning techniques, followed by a second stage of retraining using knowledge distillation (KD). It's worth noting that KD could potentially boost performance in other studies too, such as CAV-MAE and Audiovisual MAE, including our own.\nPlease note that MAViL is our concurrent work and we included it as grayed out in our main results following the guideline of covering all relevant papers.\n\n&nbsp;\n\n---\n\n> Although the author tried to compare different pre-training methods, he did not clearly explain the advantages of equivariant representation learning.\n> \n\nAs demonstrated in Table 4, learning representations equivariant rather than invariant to augmentations allows for more robust learning of the semantics of inputs without compromising them. Consequently, this leads to better learning of audio-visual correspondence and joint representation. We also provide additional supporting evidence through an ablation study (Table 3 in the revised manuscript) for a more comprehensive understanding of our findings."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5149/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700079290930,
                "cdate": 1700079290930,
                "tmdate": 1700153215892,
                "mdate": 1700153215892,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "t7jNyAgsBZ",
            "forum": "9k4Yvb75ED",
            "replyto": "9k4Yvb75ED",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5149/Reviewer_3ZPL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5149/Reviewer_3ZPL"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors introduce EquiAV, a new framework that integrates single-modal equivariant contrastive learning with audio-visual contrastive learning. In the proposed framework, audio-visual correspondence and rich modality-specific representations are learned in separate latent spaces.  Extensive ablation studies verify that EquiAV outperforms the existing audio-visual self-supervised learning methods on audio-visual event classification and zero-shot audio-visual retrieval tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The authors extend single-modal equivariant representation learning to the audio-visual domain, achieving better performance than previous audio-visual self-supervised learning methods."
                },
                "weaknesses": {
                    "value": "+ Novelty. The technical contribution of the proposed method is relatively limited. It extends the existing EQUIMOD [1] method to audio and visual modalities, applying it to audio-visual self-supervised learning. I do not see any specific contributions of this work to the audio-visual learning field. The intra loss was applied to the two modalities separately, and I do not believe that this work contributes any new insights into cross-modal modality. Simply applying a state-of-the-art approach to a new application does not necessarily result in new significant contributions.\n\n+ Writing. Some statements about the key contributions are vague. For example, the authors state that \"employing augmentations in multi-modal contrastive learning requires careful consideration, as augmentations can severely distort the inter-modal correspondence.\" It would be helpful to provide some specific examples to illustrate how augmentations can distort the inter-modal correspondence. Additionally, \"the equivariance loss term in the proposed framework differs slightly from the single-modal equivariant self-supervised learning (Devillers & Lefort, 2023). The key distinction is whether or not the similarity of the positive pair is included in the denominator of the loss term.\" However, it is unclear why the positive pair is included in the denominator. Finally, the authors state that \"to model the displacement in the latent space caused by data augmentations, augmentation predictors take as input the concatenation of the original input embeddings and the augmentation vectors, and output equivariant embeddings.\" It would be helpful to describe more specifically what kinds of displacements these predictors can model and why concatenating the input embedding and augmentation vector is helpful.\n\n+ Experiment. What if we applied Equimod to audio and visual data directly without the positive term in the loss? \n\n[1] Alexandre Devillers and Mathieu Lefort. Equimod: An equivariance module to improve visual instance discrimination. ICLR, 2023."
                },
                "questions": {
                    "value": "Please address questions in Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5149/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699032223142,
            "cdate": 1699032223142,
            "tmdate": 1699636508819,
            "mdate": 1699636508819,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8MnWcSM3NN",
                "forum": "9k4Yvb75ED",
                "replyto": "t7jNyAgsBZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5149/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5149/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer 3ZPL (1/2)"
                    },
                    "comment": {
                        "value": "Dear Reviewer 3ZPL,\n\nThank you for giving us valuable comments. We present responses to the raised concerns and questions below.\n\n&nbsp;\n\n> It extends the existing EQUIMOD [1] method to audio and visual modalities, applying it to audio-visual self-supervised learning. I do not see any specific contributions of this work to the audio-visual learning field.\n> \nAs self-supervised learning through invariance has been extended, some studies have shown that applying the concept of equivariance leads to an additional performance boost in various domains such as images and text. Specifically, in the image domain, the concept of equivariance has been applied to SimCLR through developments like E-SSL [2] and EquiMod [1]. Similarly, in the text domain, SimCSE [3] has been advanced with the application of equivariance as seen in DiffCSE [4].\nThese papers show the process of adopting the idea of equivariance for representation learning in each domain. On the other hand, to our knowledge, there is no prior work that incorporates equivariance for audio modality and audio-visual representation learning.\nAlong these lines, we aim to design a framework for employing equivariant representation learning in audio-visual representation learning and demonstrate its benefits. However, extending the single-modal equivariant representation learning to multi-modal representation learning is not trivial.  Our additional ablation studies (Table 3 in the revised manuscript) reveal that applying equivariance to inter-modal latent spaces is not beneficial and may even hinder the learning of audio-visual correspondences and joint representations. We found that augmentation-related information and audio-visual correspondence need to be learned in separate latent spaces to overcome this challenge. In addition, simply replacing the single-modal invariant representation learning with the equivariant one doesn\u2019t lead to performance improvement. Consequently, we have meticulously designed a framework that maximizes the benefits of equivariance, while avoiding negative impacts on learning. Each component of our framework has been verified to be effective through ablation studies, including:\n    \n1) Loss function (Table 5),\n    \n2) Data augmentations for effective equivariant representation learning (Table 4), and\n    \n3) Projection head and augmentation encoder (Table C in Appendix).\n    \nTaken together, all of these factors indicate that our contribution goes beyond simply applying existing techniques to create a new and effective framework for audio-visual representation learning.  \n\n&nbsp;\n\n---\n> It would be helpful to provide some specific examples to illustrate how augmentations can distort the inter-modal correspondence.\n> \n\nFor a more straightforward understanding, we came up with a case where data augmentation causes distortion of the inter-modal correspondence. Let\u2019s suppose that given an audio-visual pair of a barking dog, the dog disappears due to augmentation (e.g. random cropping). When using intra-modal invariant learning, the features of the original image (i.e. the dog is barking) and the augmented one (i.e. the dog disappears) are forced to become closer in the latent space. This will give the wrong signal to the model, adversely affecting the learning of correct inter-modal correspondence (i.e. the dog and the barking sound). On the other hand, in the context of equivariant learning, the embedding of the original image is transformed by the augmentation predictor and made closer to the embeddings of the augmented image in the intra-modal latent space. Then, we utilize original inputs (i.e. dog and the barking sound) that retain the semantics of the two modalities in the inter-modal latent space while ensuring that the embeddings of the augmented image and the original audio will not become directly close. This insight allows equivariance to take advantage of the effect of strong data augmentation in audio-visual representation learning.\n\n&nbsp;\n\n---\n\n> it is unclear why the positive pair is included in the denominator.\n> \n\nAs explained in Appendix A of the manuscript, including the positive pair in the denominator results in a relatively larger update for hard positives. This becomes particularly advantageous when stronger augmentations lead to an increased frequency of hard positives. Training with more hard positives, alongside the application of equivariance, substantially enhances the model's capability to comprehend detailed features in single-modal contexts. The intra-modal representation quality plays a pivotal role in understanding and integrating different modalities. Consequently, the semantically rich intra-modal representation promotes effective learning of audio-visual correspondence and audio-visual joint representations. We will add a detailed analysis in Appendix A."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5149/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700078874620,
                "cdate": 1700078874620,
                "tmdate": 1700153225561,
                "mdate": 1700153225561,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]