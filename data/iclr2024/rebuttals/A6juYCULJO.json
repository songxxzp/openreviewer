[
    {
        "title": "Abstractive Summarization through the PRISM of Decoding Strategies"
    },
    {
        "review": {
            "id": "gdc6Ssjjer",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7055/Reviewer_BSXa"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7055/Reviewer_BSXa"
            ],
            "forum": "A6juYCULJO",
            "replyto": "A6juYCULJO",
            "content": {
                "summary": {
                    "value": "This paper explores over 2,500 combinations of three widely-used million-scale autoregressive encoder-decoder models, across six datasets and nine decoding settings. The research reveals that optimized decoding choices can significantly boost performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The authors conduct extensive experiments across various combinations and datasets.\n\n2. The paper provides visualized and in-depth analysis."
                },
                "weaknesses": {
                    "value": "1. Given that there's no universal strategy for abstractive summarization, the practical value of the experiments is questionable. For the investigated question like \"Which hyperparameter values best suit a particular AS quality attribute?\", the answer is \"Not all quality attributes are easy to temper at decoding time.\", which seems ambiguous and might not guide future work.\n\n2. Extending the method to LLM models would have made the findings more impactful in the era of LLM."
                },
                "questions": {
                    "value": "none"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7055/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7055/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7055/Reviewer_BSXa"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7055/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697219874303,
            "cdate": 1697219874303,
            "tmdate": 1699636830266,
            "mdate": 1699636830266,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "FQ0F864xmI",
                "forum": "A6juYCULJO",
                "replyto": "gdc6Ssjjer",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7055/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7055/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your review!"
                    },
                    "comment": {
                        "value": "We appreciate your constructive feedback.\n\n**(1) Questionable value of the experiments**\n> (review quote) Given that there's no universal strategy for abstractive summarization, the practical value of the experiments is questionable. For the investigated question like \"Which hyperparameter values best suit a particular AS quality attribute?\", the answer is \"Not all quality attributes are easy to temper at decoding time.\", which seems ambiguous and might not guide future work.\n- In response to your feedback, we revised and expanded our analysis, introducing per-dataset rankings (Appendix H) and new findings (Section 5). We identified prevalent performance patterns and established a set of recommendations to better assist practitioners in real-world summarization use cases. We attached a visual and easy-to-follow guideline to the conclusion (Appendix L), driving the selection of valuable decoding spaces (i.e., strategy + hyperparams) depending on the optimization target and AS type. We completely reworked the discussion of our results (Section 5), providing a more precise and in-depth dissection of quantitative and qualitative findings thanks to content re-organization, better answering each research question. We enhanced the presentation quality of the entire paper, including the Appendix, prioritizing rigor and clarity in our statements. The absence of a universal decoding strategy does not affect the validity of our analysis, the practical value of the experiments, or the potential to guide future work. To our knowledge, we are the first to (i) thoroughly examine the decoding results for AS in its various forms with respect to the input length and n-arity, (ii) present findings based on literature-supported grid search tuning for decoding hyperparameter spaces, and (iii) release a pioneering decoding-oriented dataset for AS to open new research directions. The findings unveiled by our research are several and include: (a) the shortfall of a one-size-fits-all strategy for AS (as underlined by the reviewer), (b) the huge effect of decoding strategies on final AS metric scores (as emphasized by the reviewer in the strength points), (c) the effectiveness\u2014efficiency trade-off of each decoding strategy, (d) the high sensitivity of stochastic methods to hyperparameter calibration, (e) the identification of quality attributes controllable at decoding time, (f) the ubiquitous nature of redundancy in AS, (g) the unexpected effectiveness of well-tuned stochastic methods for concise target summaries, (h) the preference of deterministic heuristics when the target is long, (i) the hard balance between fluency and recall, precision, faithfulness, (j) the highly competitive level of summaries produced by diverse strategies after optimal tuning.\n\n**(2) Lack of Large Language Models**\n> (review quote) Extending the method to LLM models would have made the findings more impactful in the era of LLM.\n- Please refer to our response to _[Reviewer Eedp]_, point (2).\n\n**(3) Limited contribution**\n> (review quote) Contribution: 2 (fair)\n- We appreciate the review and the effort put into the evaluation. However, we respectfully disagree with the assigned score, as we believe that it does not accurately reflect the depth of our contributions. We are not convinced that reported concerns adequately support a score of 2, the lowest received. We would be grateful for further clarification on specific points to address any perceived shortcomings more effectively.\n\nFurther elaboration on the introduced changes can be found in the general comment."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7055/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700703146053,
                "cdate": 1700703146053,
                "tmdate": 1700704946310,
                "mdate": 1700704946310,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "hHwvG9ZmA9",
            "forum": "A6juYCULJO",
            "replyto": "A6juYCULJO",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7055/Reviewer_rLZb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7055/Reviewer_rLZb"
            ],
            "content": {
                "summary": {
                    "value": "The paper conducts a large-scale study on the impact of decoding strategies on abstractive summarisation.\nThe authors investigate 9 decoding strategies (greedy search, contrastive search, beam search, diverse beam search, sampling, top-k sampling, top-p nucleus sampling, beam sampling, eta sampling) on 6 datasets (XSum, CNN/DM, PubMed, arXiv, Multi-News, Multi-LexSum), combined with 3 seq2seq models. In addition to human evaluation on 4 dimensions (recall, precision, faithfulness, fluence), the authors employ 10 automatic metrics (rouge, BERTScore, perplexity, coverage, density, compression, unigram n-gram ratio, normalized inverse of diversity, BARTScore, and Carburacy) and a few observations are provided."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* A large-scale study on multiple decoding strategies\n* A resource that enables further studies on abstractive summarisation regarding evaluation metrics. \n* The paper is easy to follow, except some figures are hard to interpret"
                },
                "weaknesses": {
                    "value": "It is hard to gain much insight from the results: sometimes it is unclear whether the authors are commenting on decoding strategies, datasets, or evaluation metrics."
                },
                "questions": {
                    "value": "A: can you elaborate on the human evaluation dimension of faithfulness, especially how you instruct annotators to differentiate it from precision and recall?\nB: it will be appreciated if some practical guidelines can be summarized"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7055/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698623215832,
            "cdate": 1698623215832,
            "tmdate": 1699636830091,
            "mdate": 1699636830091,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "iokDEjxYJB",
                "forum": "A6juYCULJO",
                "replyto": "hHwvG9ZmA9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7055/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7055/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your review!"
                    },
                    "comment": {
                        "value": "We appreciate your comments.\n \n**(1) Need for more insightful results**\n> (review quote) some figures are hard to interpret\n> (review quote) It is hard to gain much insight from the results: sometimes it is unclear whether the authors are commenting on decoding strategies, datasets, or evaluation metrics.\n> (review quote) it will be appreciated if some practical guidelines can be summarized\n- We acknowledge your concerns and are pleased to receive feedback on results presentation. We have invested considerable time and effort on this aspect because of the high complexity originating from the numerous analysis dimensions. Recognizing the importance of clarity and interpretability in such a comprehensive study, we have taken significant steps to address this issue. In response to your observations, we implemented the following changes: (i) we refined the graphical representation of averaged metric results in the radar plots (Figure 3), also introducing its tabulated version (Table 11); (ii) we provided a more detailed discussion of quantitative and qualitative results, enriching the answer to every research question (Section 5); (iii) we included per-dataset ranking analyzes, making inter-dataset patterns easily perceptible (Appendix H); (iv) we assembled an easy-to-follow guideline summarizing key recommendations tailored to user needs (Appendix L); (v) we elevated the overall presentation quality of the entire paper. We trust that these adjustments will address the concerns raised and provide a more insightful and accessible presentation of our findings.\n \n**(2) Annotation instructions for human evaluation**\n> (review quote) can you elaborate on the human evaluation dimension of faithfulness, especially how you instruct annotators to differentiate it from precision and recall?\n- We have added more details about the instructions furnished to annotators in Appendix F. In particular, we have incorporated a highly comprehensible example (provided within the instructions) aimed at elucidating the distinctions among the evaluation dimensions of Recall, Precision, and Faithfulness. As highlighted in Section 4.4, we note that distinguishing Faithfulness from Precision and Recall is straightforward. This is because Faithfulness involves comparing the prediction with the input document, as opposed to the gold target summary as done for Precision and Recall.\n \n**(3) Limited soundness**\n> (review quote) Soundness: 2 (fair)\n- We appreciate the effort to make the review. However, we respectfully disagree with the assigned score, as we believe that it does not accurately reflect the technical soundness of our methodological approach. We are not convinced that the concerns and weaknesses reported adequately support a score of 2. We would be grateful for further clarification on specific points to address any perceived shortcomings more effectively (if not already resolved by the novel updates in the paper).\n \nFurther elaboration on the introduced changes can be found in the general comment."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7055/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700707349972,
                "cdate": 1700707349972,
                "tmdate": 1700707528661,
                "mdate": 1700707528661,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "6Df5ppYsm1",
            "forum": "A6juYCULJO",
            "replyto": "A6juYCULJO",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7055/Reviewer_8Gmg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7055/Reviewer_8Gmg"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents an extensive study of the effect of various decoding methods on the quality of generated summaries on the task of abstractive written text summarization. The results cover three encoder-decoder transformer models with finetuned weights matched to six public abstractive summarization datasets under three categories: short text, long text, and multi-document. Explicit hyperparameter tuning was done on each (decoding strategy, dataset) pair, and the variation was evaluated by a multitude of automatic text evaluation metrics (lexical and semantic) as well as human evaluation along four quality dimensions (precision, recall, factuality, and fluency). The general findings confirm a lack of dominating decoding strategy for all tasks studied, and show that hyperparameter settings are also task/decoding strategy specific with potentially significant variation in calculated metrics between optimal and suboptimal hyperparameters."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Originality:\nOne of the biggest strength of the paper is in its scope of decoding methods (nine) and datasets (six). Although the study of effect of decoding methods on the quality of generated summaries by summarization models has been gaining momentum in the research community, a study of the scale presented in this paper has yet to be published.\n\nQuality:\nThe results presented in the paper cover a wide range of research parameters (task, dataset, hyperparameter, strategy, evaluation metrics, automatic vs human evaluation). The authors have also been quite thorough in including detailed explanation on the data selection, preprocessing, and evaluation. The power analysis in Appendix D is a welcoming addition to the paper, lending reference to robust statistical analysis on the potential reliability of the findings in the paper.\n\nClarity:\nThe paper is generally well-organized, with thorough explanation on the experimental settings for clear reproducibility test.\n\nSignificance:\nAnother major strength of the paper is the significance of the direction of the research. Evaluation metrics and decoding strategies are two main aspects in abstractive summarization that are relatively underexplored compared to research on newer summarization models or new datasets. However the decoding methods undoubtedly play an influential role in the task of summarization and text generation in general. There are arguments on the misalignment between cross-entropy based training objective and actual quality measures of interest, and the success of many reranking methods also reflects the suboptimal strategy of blindly following a single decoding strategy. Therefore, the study done in this paper can provide referential values for any related and future research in summarization that aims at improving quality of the generation."
                },
                "weaknesses": {
                    "value": "Major weakness of the paper is the presentation style of main results in Figure 3, and some inconsistency between Figure 3, the writing, and Figure 8 that may bring some of the quantitative findings into question:\n\n1. Because of the density of information and the lack of differentiability in the chosen colors for different decoding strategies, Figure 3 is very hard to follow. For example, Greedy Search and Beam Sampling are both presented with \"reddish\" color, and the reviewer finds it very difficult to identify which is which in the plot. Additional comments on the figure are presented as questions in the \"Questions\" section\n\n2. Because of the lack of interpretability of Figure 3 and lack of tabulated data for the results, it is hard for the reviewer to confirm the quantitative differences referred to in the Results section (e.g.,  +48% UNR avg. score mentioned between short and long summaries).\n \n3. Contrary to the discussion on the relative advantage of certain decoding strategies over other in section 5.1, the impression of the reviewer from Figure 8 is that \n(i) sampling, top-k, and top-p sampling clearly have larger sample variance than all other strategies, even after hyperparameter tuning, on five out of six datasets;\n(ii) the behavior of all decoding methods changes significantly on Multi-LS dataset, showing larger scale of variation in average score and much smaller sample-specific variance; \n(iii) apart from the three sampling strategies mentioned in (i), the remaining methods on all datasets except Multi-LS seem to be consistently on par with each other as evaluated by most of the evaluation metrics\n\nthese observations may contradict the quantitative findings in section 5.1, and there seems to lack specific discussion on those arguably more obvious patterns than the ones presented in Figure 3 and discussed in the text.\n\nAnother weakness is the generalizability of the findings. The reviewer agrees with the authors that the lack of a universal strategy for optimal decoding is substantively supported by the results, but apart from that the paper seems to be lacking in identifying other general trend or patterns in combination of the research parameters (decoding strategy, task, model, hyperparameters). This limits the value of the results to be observational rather than inferential. \n\nOther problems regarding writing:\n1. **Elevating Fluency undermines Recall, Precision, and Factuality.** in Section 5.2 is not a rigorous statement based on the results, what the authors have observed is a negative correlation between fluency and other quality factors as evaluated by human, but the phrasing of the finding turns such a correlation into a misleading causal statement.\n2. It is not rigorous to draw conclusion on \"factuality\", \"factual flaws\", or \"hallucination\" solely based on BARTScore (e.g., Section 5.1, **stochastic vs deterministic**), which is at best a proxy for those quality measures. Please at the very least state in the main text that the studied automatic metrics are \"proxies\" of the quality dimensions of interest.\n3. Typo: Table 4, shouldn't UNR be \"Unique N-gram Ratio\", not \"unigram n-gram ratio\"?"
                },
                "questions": {
                    "value": "1. In Figure 3, why are some of the metrics score concentrated towards low value (center of each circle, e.g., Perplexity on Multi-News)? The  reviewer may have misunderstood the presentation here, but if all scores are rescaled to [0, 1] based on min and max of each score, shouldn't there be a point on the outer perimeter for every metrics on every dataset (that is, the decoding method with the maximum score in a metric on a dataset)?\n2. Any special reason why y-axis in Figure 4 is not presented on a linear scale?\n3. Can you elaborate, maybe in the appendix, more details on the training regimen used in the human evaluation? Were the annotators given any examples or detailed explanation on each quality dimension? How did you guarantee that the source document would be viewed by the annotator? Or were the annotators already familiar with the research objectives so that training was minimal?\n4. How is Kendall's tau calculated for human evaluation results? Was it calculated between two annotators across all pair selection results, or was a ranking of all decoding strategies first determined based on all selections from one annotator, and then tau calculated between two rankings? Why are the reported values so low and even negative on some datasets? If the annotators disagree significantly in their preference, wouldn't that seriously limit the possibility of drawing any robust conclusion based on their evaluation results?\n5. Why not consider more semantic similarity metrics (e.g., NLI-based evaluation, see https://aclanthology.org/P19-1213.pdf for example) or NER based metrics? Apart from BertScore and BartScore, almost all other metrics used are lexical based (n-gram or character overlap). It's hard to say if the variation observed in those metrics truly reflect variation in qualities of the generated summaries (and based on Figure 8, it's hard to say if significant variation has been observed)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7055/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7055/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7055/Reviewer_8Gmg"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7055/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698636971541,
            "cdate": 1698636971541,
            "tmdate": 1699636829929,
            "mdate": 1699636829929,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "sogfUu34T6",
                "forum": "A6juYCULJO",
                "replyto": "6Df5ppYsm1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7055/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7055/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your review!"
                    },
                    "comment": {
                        "value": "We sincerely appreciate your constructive feedback.\n\n**(1) Need for presentation improvements**\n> (review quote) Major weakness of the paper is the presentation style of main results in Figure 3, and some inconsistency between Figure 3, the writing, and Figure 8 that may bring some of the quantitative findings into question.\n- Our decision to utilize radar plots in Figure 3 was driven by the belief that they offer a more insightful representation than tabular formats, particularly in addressing our research questions. The colored area surfaces in the radar plots facilitate quick and comprehensive visual comparisons among datasets and decoding strategies, aligning with the key evaluation aspects of our research (see RQ1 and RQ2). Given the space constraints and the high dimensionality of the presented study, which involves 9 decoding strategies, 10 metrics, and 6 datasets, our goal in the main paper was to delve into relative efficiency and effectiveness comparisons. We wanted to identify patterns and assess the impact of decoding choices, rather than focus on reporting precise metric scores. However, we acknowledge the importance of clarity and transparency. As detailed in the following comments, we significantly improved our presentation style to address your concerns.\n\n**(1.a) Lack of differentiability in the chosen colors**\n> (review quote) Because of the density of information and the lack of differentiability in the chosen colors for different decoding strategies, Figure 3 is very hard to follow. For example, Greedy Search and Beam Sampling are both presented with \"reddish\" color, and the reviewer finds it very difficult to identify which is which in the plot.\n- Thank you for bringing this concern to our attention. In response to your observation, we revised the colors related to decoding strategies, contributing to a more accessible interpretation of our findings. We updated every figure accordingly. Additionally, we employed distinct marks (circles and triangles) to visually contrast deterministic and stochastic decoding strategies.\n\n**(1.b) Lack of tabulated data**\n> (review quote) Because of the lack of interpretability of Figure 3 and lack of tabulated data for the results, it is hard for the reviewer to confirm the quantitative differences referred to in the Results section (e.g., +48\\% UNR avg. score mentioned between short and long summaries).\n- We included tabular data in Appendix G (Table 11), thus providing a detailed breakdown of Figure 3 and ensuring the reproducibility of the reported findings. Please note that the exact metric scores will be openly released in our PRISM dataset.\n\n**(1.c) Contradictions and untreated patterns**\n> (review quote) Contrary to the discussion on the relative advantage of certain decoding strategies over other in section 5.1, the impression of the reviewer from Figure 8 is that (i) sampling, top-k, and top-p sampling clearly have larger sample variance than all other strategies, even after hyperparameter tuning, on five out of six datasets; (ii) the behavior of all decoding methods changes significantly on Multi-LS dataset, showing larger scale of variation in average score and much smaller sample-specific variance; (iii) apart from the three sampling strategies mentioned in (i), the remaining methods on all datasets except Multi-LS seem to be consistently on par with each other as evaluated by most of the evaluation metrics. These observations may contradict the quantitative findings in section 5.1, and there seems to lack specific discussion on those arguably more obvious patterns than the ones presented in Figure 3 and discussed in the text.\n- We appreciate your suggestion not to overlook more evident patterns. We agree with your observations, although we found no contradictions with the quantitative findings previously reported in Section 5.1. Following your comments, we meticulously double-checked all the results outlined in the main paper and Appendix. We confirmed the content of each figure and table, except Figure 3, where we found erroneous scores. After correction, we reviewed the quantitative findings listed in Section 5, adding the suggested ones if not already present. More generally, we provided a more detailed discussion of results, enriching the answer to every research question."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7055/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700713592923,
                "cdate": 1700713592923,
                "tmdate": 1700713726630,
                "mdate": 1700713726630,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4WFovpqpcw",
                "forum": "A6juYCULJO",
                "replyto": "6Df5ppYsm1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7055/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7055/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply continuation"
                    },
                    "comment": {
                        "value": "**(2) Lack of general recommendations**\n> (review quote) Another weakness is the generalizability of the findings. The reviewer agrees with the authors that the lack of a universal strategy for optimal decoding is substantively supported by the results, but apart from that the paper seems to be lacking in identifying other general trend or patterns in combination of the research parameters (decoding strategy, task, model, hyperparameters). This limits the value of the results to be observational rather than inferential.\n- We are dedicated to providing insights that go beyond simple observations. In line with this commitment, we revised and expanded our analysis, introducing per-dataset rankings (Appendix H) and new findings (Section 5). Combined with result verification, we identified prevalent performance patterns and established a set of recommendations to better assist practitioners in real-world summarization use cases. Following your suggestions, we attached a visual and easy-to-follow guideline to the conclusion (Appendix L), driving the selection of valuable decoding spaces (i.e., strategy + hyperparams) depending on the optimization target and AS type.\n\n**(3) Writing problems**\n\n**(3.a) Not rigorous statements**\n> (review quote) Elevating Fluency undermines Recall, Precision, and Factuality. in Section 5.2 is not a rigorous statement based on the results, what the authors have observed is a negative correlation between fluency and other quality factors as evaluated by human, but the phrasing of the finding turns such a correlation into a misleading causal statement.\n- Upon careful review, we concur with your observation that our statement could be misconstrued as implying a causal relationship. We did not intend to assert a causal link and we appreciate your diligence in bringing this to our attention. We revised the finding description by using the \"negative correlation\" keyphrase.\n> (review quote) It is not rigorous to draw conclusion on \"factuality\", \"factual flaws\", or \"hallucination\" solely based on BARTScore (e.g., Section 5.1, stochastic vs deterministic), which is at best a proxy for those quality measures. Please at the very least state in the main text that the studied automatic metrics are \"proxies\" of the quality dimensions of interest.\n- We added a cautionary note at the beginning of Section 5, explicitly stating that the studied automatic metrics are \"proxies\" for the quality dimensions of interest. We underlined this concept in other parts of the paper, including Table 3 and Appendix L.\n\n**(3.b) Typos**\n> (review quote) Typo: Table 4, shouldn't UNR be \"Unique N-gram Ratio\", not \"unigram n-gram ratio\"?\n- We amended the reported typographical errors and revised the entire paper for improved fluency and clarity.\n\n**Q1: Scoring scale in Figure 3**\n> (review quote) In Figure 3, why are some of the metrics score concentrated towards low value (center of each circle, e.g., Perplexity on Multi-News)? The reviewer may have misunderstood the presentation here, but if all scores are rescaled to [0, 1] based on min and max of each score, shouldn't there be a point on the outer perimeter for every metrics on every dataset (that is, the decoding method with the maximum score in a metric on a dataset)?\n- As explained in the caption, [0, 1] rescaling is based on min-max normalization across all 2656 runs, which is key to pinpoint inter-dataset patterns. For each decoding strategy, tens or hundreds of hyperparameter configurations are tested (see Table 2). The scores depicted in Figure 3 come from the average pooling. Therefore, having maximum (1) or minimum (0) scores on average is improbable.\n\n**Q2: Clarification on Figure 4**\n> (review quote) Any special reason why y-axis in Figure 4 is not presented on a linear scale?\n- The y-axis in Figure 4 employs a non-linear scale to highlight variations among the predominant values (1-4 seconds) while ensuring clear visibility of outliers exceeding 10 seconds. In this way, we avoid squeezing distinctions within comparable time intervals, thereby facilitating the discovery of insights and enabling well-informed decision-making."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7055/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700713643732,
                "cdate": 1700713643732,
                "tmdate": 1700713923836,
                "mdate": 1700713923836,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "laejHdsAjF",
            "forum": "A6juYCULJO",
            "replyto": "A6juYCULJO",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7055/Reviewer_Eedp"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7055/Reviewer_Eedp"
            ],
            "content": {
                "summary": {
                    "value": "The paper investigates in-depth different decoding strategies for abstractive summarization, exploring more than 2500 combinations of 3 models, 6 datasets, and 9 decoding settings. The goal of the paper is to shed light on the field and demonstrate that optimized decoding choices can yield substantial improvements to the performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The experiments are very comprehensive, covering a bunch of different decoding strategies and their hyperparameters.\n\n* The data when released can be very useful, not just for further benchmarking, but also in terms of modeling and evaluation since predictions are also released.\n\n* The paper is very clearly written."
                },
                "weaknesses": {
                    "value": "* While the experiments are comprehensive, I find that the paper lacked overall recommendations that practitioners can follow in a real-world setting (which I believe is the main purpose of the paper). For example, if one has a new summarization use case, how does the paper help them decide which model and which decoding strategies (+ hyperparams) to use? The paper does provide a list of findings, however a set of recommendations as part of the conclusion would greatly improve the paper's impact.\n\n* The models are unfortunately limited to 400-500M parameters. This is significantly small if compared with LLMs, thus it is uncertain whether the results shown in this paper transfer to these large models. It could have helped to see models of varying sizes (perhaps one small-scale and one XL-scale) to show that the results to actually hold even when the model size is different."
                },
                "questions": {
                    "value": "* What is the set of recommendations that the authors think practitioners should follow?\n\n* How do these results transfer at scale (smaller or larger)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7055/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698832945698,
            "cdate": 1698832945698,
            "tmdate": 1699636829800,
            "mdate": 1699636829800,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GaaVcnQbDJ",
                "forum": "A6juYCULJO",
                "replyto": "laejHdsAjF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7055/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7055/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your review!"
                    },
                    "comment": {
                        "value": "Thank you for your review! We appreciate your constructive feedback. It is great to learn that you enjoyed reading our paper.\n\n**(1) Lack of overall recommendations**\n> (review quote) While the experiments are comprehensive, I find that the paper lacked overall recommendations that practitioners can follow in a real-world setting (which I believe is the main purpose of the paper). For example, if one has a new summarization use case, how does the paper help them decide which model and which decoding strategies (+ hyperparams) to use? The paper does provide a list of findings, however a set of recommendations as part of the conclusion would greatly improve the paper's impact.\n- We acknowledge your concerns. In response, we revised and expanded our analysis, introducing per-dataset rankings (Appendix H) and new findings (Section 5). We identified prevalent performance patterns and established a set of recommendations to better assist practitioners in real-world summarization use cases. Following your suggestions, we attached a visual and easy-to-follow guideline to the conclusion (Appendix L), driving the selection of valuable decoding spaces (i.e., strategy + hyperparams) depending on the optimization target and AS type.\n\n**(2) Different model sizes**\n> (review quote) The models are unfortunately limited to 400-500M parameters. This is significantly small if compared with LLMs, thus it is uncertain whether the results shown in this paper transfer to these large models. It could have helped to see models of varying sizes (perhaps one small-scale and one XL-scale) to show that the results to actually hold even when the model size is different.**\n- We recognize your point. We did not experiment with LLMs and tiny models. As explicitly outlined in the abstract, introduction, related works, method, and Appendix N, the focal point of our study was the evaluation of models predominantly utilized by the AS community, as opposed to those that may rise to prominence in the future. It is important to note that, at the beginning of this project, open-source LLMs were not as widespread as they are today. Our meticulous model selection process involved identifying cutting-edge solutions within each AS family (short, long, multi-document). The chosen models exhibit an encoder-decoder architecture and fall within the range of a million parameters or barely surpass 10B---a lower boundary often cited when discussing LLMs.  To further elucidate our decision-making, we refined and expanded our bibliometric analysis within the SCOPUS database.  In Appendix B, we introduced a chart depicting the intersection of AS and LLMs in the literature. The analysis unveiled that a mere 15.8\\% of conference papers from 2020 onward explicitly integrate LLMs and AS in their title, abstract, and keywords. While an extension of our study to include LLMs and tiny LMs would be commendable, focusing solely on one model for these families would be unsound, with the risk of including biased non-representative decoding-level conclusions. Consequently, it is imperative to encompass several new models. In this sense, we emphasize that the completion of our study necessitated 73 cumulative days of computation using NVIDIA 3090 RTX Turbo GPUs (Section 4.1). In particular, incorporating experiments with LLMs would entail a substantial additional commitment of time and resources. Our empirical tests revealed that transitioning from 440M models to 7B models (Bloom, Falcon, LLaMA-2) results in an average increase in decoding time per instance by a factor of x150. It becomes crucial to strike a balance between the depth of analysis and the practical constraints. As such, we believe that this extension warrants a dedicated effort and a separate study. \n\nFurther elaboration on the introduced changes can be found in the general comment."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7055/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700698633153,
                "cdate": 1700698633153,
                "tmdate": 1700710862794,
                "mdate": 1700710862794,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]