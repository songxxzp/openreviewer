[
    {
        "title": "When Scaling Meets LLM Finetuning: The Effect of Data, Model and Finetuning Method"
    },
    {
        "review": {
            "id": "TUg8BuZ9bR",
            "forum": "5HCnKDeTws",
            "replyto": "5HCnKDeTws",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8351/Reviewer_oDvP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8351/Reviewer_oDvP"
            ],
            "content": {
                "summary": {
                    "value": "This paper examines scaling laws for LLM adaptation methods (finetuning, PET like LoRA and PT), where the LLMs were trained on two different machine translation corpora (En->ZH and EN->De) and adapted to either the other translation task, or article summarization in an unseen language.\nThe paper is primarily empirical, and the goal is to fit scaling laws and identify general trends that could transfer across problem domains (e.g. vision, multimodal, etc)."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "### Overall\nThe paper is clearly written. The methodology is generally explicit, and the authors do a good job of interpreting the (many) experiments in the paper.\n\nThe figures are clear and well-organized. \n\nExcellent references to existing work in identifying scaling laws.\n\n### Experiments\nThe experimental results are somewhat expected (given enough data, use FMT, with limited data, use PT or LoRA). The authors do a good job of explaining and justifying these conclusions given experimental evidence, with the appropriate amount of uncertainty given the noisiness of the data. I think that correctly calibrating the uncertainty is a great strength of the paper. \n\nExcellent analysis of scaling different axes (pretraining data, model size, finetuning data)"
                },
                "weaknesses": {
                    "value": "### Experiments\n#### Fig 2:\nGiven the poor extrapolation to the 16B model, I'd like to understand the cause -- i.e. is this the right form of the equation, or does this multiplicative scaling break down at larger parameter counts?\nOne way to test this might be to compare the fit by including 16B in the fitting procedure, vs excluding it. \n\n#### Fig 4:\nI liked the analysis of scaling to larger PET settings (LoRA and PT in Fig 4). Since the datasets seem to support up to 1e-6 and 1e-7 training sentences, I wish the authors had compared the scaling on even larger finetuning datasets to enable direct comparison to FMT in Figure 3. \n\n#### Fig 5\nI didn't really understand Figure 5. My my understanding is that the figure analyzes the fitted power laws, and the x-y- point indicates the estimated amount of finetuning necessary to achieve the performance parity between the two approaches at different model sizes. Does the ordering here matter (e.g. FMT vs LoRA, or LoRA vs FMT) -- my understanding is that it should not."
                },
                "questions": {
                    "value": "- How does zero-shot evaluation work in Figure 7? If the source language is unseen, how is the model able to get nontrivial performance on the task?\n\n### Typos:\n- P4 footnote: is pretty week \u2192 pretty weak\n- Fig 2. center column: Propmt \u2192 Prompt"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8351/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698615869222,
            "cdate": 1698615869222,
            "tmdate": 1699637039135,
            "mdate": 1699637039135,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Rtc9gJ6stT",
                "forum": "5HCnKDeTws",
                "replyto": "TUg8BuZ9bR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8351/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8351/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer oDvP"
                    },
                    "comment": {
                        "value": "Thanks for your insightful comments! We corrected all typos in the updated version.\n\n\n**Re: is this the right form of the equation, or does this multiplicative scaling break down at larger parameter counts?**\n\nUnderstanding the reason for the bad extrapolation to 16B is valuable. We attempted different scaling formulations (both multiplicative and additive) and found that their extrapolation often fails at 16B mainly on En-Zh LLM. This inspires us that it is something else rather than the formulation causing this problem.\n\nAfter careful analysis, we discovered that the pretraining performance of the 16B En-Zh LLM can\u2019t be well predicted by single-variable scaling laws, as shown in Figure 10 in the updated version: the actual performance is worse than the expectation. We argue that such mismatch caused by pretraining instabilities gets amplified during finetuning. This issue becomes more severe for PET as PET finetuning suffers more from unstable optimization. \n\n\n**Re: The authors had compared the scaling on even larger finetuning datasets to enable direct comparison to FMT in Figure 3.**\n\nWhile finetuning LLM with PET on larger-scale datasets (e.g. up to $1e6$~$1e7$ examples) is feasible theoretically, it is non-trivial in practice. This is because the hyperparameters of PET, including the learning rate, batch size and finetuning step, are highly biased and tuned for small-scale datasets to achieve the best quality but they can hardly generalize to larger settings. For example, we set batch size to 16 for PET, with which the model converges very slowly when using $1e6$ examples and may not converge at all due to the used finetuning step constraint. Readjusting these hyperparameters are required but are expensive and it may also break the scaling trend.\n\n\n**Re: Does the ordering here matter (e.g. FMT vs LoRA, or LoRA vs FMT) -- my understanding is that it should not.**\n\nYour understanding is correct: the ordering here doesn\u2019t matter.\n\n\n**Re: How does zero-shot evaluation work in Figure 7? If the source language is unseen, how is the model able to get nontrivial performance on the task?**\n\n\nAlthough our LLMs are bilingual, they were pretrained on web-scale corpora which often unintentionally include data from other languages. In other words, the model has seen other source languages. We call it \u201czero-shot evaluation\u201d because we never finetune the model on the \u201czero-shot\u201d pairs but we expect that LLM is capable of generalizing its translation ability to them."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8351/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700511191294,
                "cdate": 1700511191294,
                "tmdate": 1700511191294,
                "mdate": 1700511191294,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VTmhRwPEXJ",
                "forum": "5HCnKDeTws",
                "replyto": "Rtc9gJ6stT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8351/Reviewer_oDvP"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8351/Reviewer_oDvP"
                ],
                "content": {
                    "title": {
                        "value": "An empirical study"
                    },
                    "comment": {
                        "value": "Thank you to the authors, for your replies to myself and the other reviewers. Thanks to the other reviewers for your comments and discussion. \n\nI think that low-resource finetuning is a very important practical technique, and one that is poorly understood. This paper is one part of building such an understanding. I think it would make a great ICLR poster.\n\nI also think that we should not focus too much on the specific scaling law formulat -- the fit seems rather poor but for now there's no alternative formula. Maybe a better one will come along, and then that hypothetical paper can compare to this one.\n- To the reviewers: I think we should not penalize the authors if the results turn out not to fit a clean formula -- other authors should get the same results, no?\n- To the authors: if the trend changes due to small changes in the hyperparameters, then I would ask how robust are these trends? Are the experiments as thorough and the findings as robust as they could be? And finally, if the trend breaks down at larger sizes, then are these really \"scaling laws\"?"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8351/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700683217403,
                "cdate": 1700683217403,
                "tmdate": 1700683217403,
                "mdate": 1700683217403,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "KrkWpcsrLG",
            "forum": "5HCnKDeTws",
            "replyto": "5HCnKDeTws",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8351/Reviewer_wU43"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8351/Reviewer_wU43"
            ],
            "content": {
                "summary": {
                    "value": "This paper describe a multiplicative joint scaling law, considering different factors including LLM model size, pretraining data size, finetuning data size, PET parameter size, on machine translation and summarization tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper introduces a multiplicative joint scaling law and offers comprehensive experiments to empirically demonstrate that this law applies to both machine translation and summarization.\n- This paper provides insightful observations regarding fine-tuning, especially parameter-efficient fine-tuning (PEFT) which I think is the most interesting part for the community now, and its relationship with parameter or data size scaling laws."
                },
                "weaknesses": {
                    "value": "Minor:\n-The paper evaluates only MT and summarization. Including more tasks or languages, particularly low-resource translations, would enhance the paper's comprehensiveness.\nMajor:\n\nMajor:\n-Regrettably, it seems that the proposed scaling law may exhibit a significant mismatch for parameter-efficient fine-tuning when the model size is 16B. This raises concerns about the law's applicability to larger models, especially those of 70B or exceeding 100B in size.\n-The paper omits some details regarding model training. Conventionally, MT models employ an encoder-decoder architecture. I believe all models in this study are decoder-only. How did the authors approach training with a decoder-only architecture for MT tasks? How might this differ from the scaling laws when using an encoder-decoder model? What prompts were utilized for MT training?"
                },
                "questions": {
                    "value": "Please see weaknesses above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8351/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698709322225,
            "cdate": 1698709322225,
            "tmdate": 1699637038990,
            "mdate": 1699637038990,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "AEGXcCgXED",
                "forum": "5HCnKDeTws",
                "replyto": "KrkWpcsrLG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8351/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8351/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer wU43"
                    },
                    "comment": {
                        "value": "Thanks for your insightful comments!\n\n\n**Re: Including more tasks or languages, particularly low-resource translations, would enhance the paper's comprehensiveness.**\n\n\nWe agree that working with different tasks and/or languages could enhance the generality and comprehensiveness of our study. That\u2019s exactly why we performed experiments on two different language pairs (En-De and En-Zh) for machine translation as well as multilingual summarization. Adding more tasks should be beneficial, which we leave to the future given the timeline and compute resource constraints. \n\n\n**Re: it seems that the proposed scaling law may exhibit a significant mismatch for parameter-efficient fine-tuning when the model size is 16B.**\n\n\nWhen extrapolating to 16B, we noticed a high mismatch particularly for LoRA and Prompt on En-Zh LLM. After careful analysis, we argue that this problem stems more from the pretraining and finetuning rather than the proposed scaling law. Specifically, we find that the pretraining performance of the 16B En-Zh LLM is not well predicted according to a single-variable scaling law as shown in Figure 10 in the updated version. The 16B model performs slightly worse than the expectation, which we argue is caused by pretraining instabilities (such as job corruptions). Such mismatch is further amplified by the finetuning as PET finetuning suffers more from unstable optimization. \n\n\n**Re: How did the authors approach training with a decoder-only architecture for MT tasks? What prompts were utilized for MT training?**\n\nWe perform finetuning for MT by converting MT examples into the LLM format and optimize the model with the conditional log-likelihood objective. More concretely, for each *source* and *target* example in MT, we concatenate them into a single sequence, i.e. *source* <sep> *target*. We feed this sequence to LLM and only compute log-likelihood on the *target*. We added this in the Appendix in the updated version.\n\n\n**Re: How might this differ from the scaling laws when using an encoder-decoder model?**\n\nBased on prior studies, the scaling of encoder-decoder models shows both similarities and differences compared to that of decoder-only models [1][2]. Exploring the scaling of finetuning encoder-decoder models is definitely an interesting research problem, which we leave to the future.\n\n>- [1] Ghorbani et al., 2021. Scaling  laws  for  neural  machine  translation.\n>- [2] Bansal et al., 2022. Data  scaling  laws  in  NMT:  The  effect  of  noise  and  architecture."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8351/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700511110955,
                "cdate": 1700511110955,
                "tmdate": 1700511110955,
                "mdate": 1700511110955,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9QX3l6jrkL",
                "forum": "5HCnKDeTws",
                "replyto": "AEGXcCgXED",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8351/Reviewer_wU43"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8351/Reviewer_wU43"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks to the authors for their reply. I would stand for my score."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8351/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700680848452,
                "cdate": 1700680848452,
                "tmdate": 1700680848452,
                "mdate": 1700680848452,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "mvZRfcbiyM",
            "forum": "5HCnKDeTws",
            "replyto": "5HCnKDeTws",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8351/Reviewer_R4DF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8351/Reviewer_R4DF"
            ],
            "content": {
                "summary": {
                    "value": "The work provides a set of simple but extensive scaling law experiments on comparing pretraining, fine-tuning, and parameter-efficient tuning LLMs on translation and summarization tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- This work provides a set of straightforward and well-motivated set of experiments.  The authors are clear where there are reasonable scaling patterns, and cases where there is no discernible pattern."
                },
                "weaknesses": {
                    "value": "- Some typographical/grammar mistakes. E.g. \"propmt\" in Figure 2, \"infers\" rather than \"implies\", \"there exits a critical point\" etc\n- It is unclear to me if the set of tasks chosen (translation, and multi-lingual summarization) are representative of broader applications of fine-tuning. However, I think the results stand on their own for this set of narrow applications at least."
                },
                "questions": {
                    "value": "- What model sizes are used in Figures 2/3/4?\n- Figure 5 is unclear to me. Does the critical point refer to when A outperforms B in \"A vs B\"?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8351/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698780527902,
            "cdate": 1698780527902,
            "tmdate": 1699637038868,
            "mdate": 1699637038868,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kRxkihKljU",
                "forum": "5HCnKDeTws",
                "replyto": "mvZRfcbiyM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8351/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8351/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer R4DF"
                    },
                    "comment": {
                        "value": "Thanks for your insightful comments! We corrected all typos and grammatical errors in the updated version.\n\n\n**Re: What model sizes are used in Figures 2/3/4?**\n\n\nIn Figure 2, we used LLMs of parameters from 1B to 16B. In Figures 3 and 4, we mainly used 1B LLM. We added this information in the updated version.\n\n\n**Re: Figure 5 is unclear to me. Does the critical point refer to when A outperforms B in \"A vs B\"?**\n\n\nThe critical point for \u201cA vs. B\u201d indicates the exact finetuning data size at which A performs equal to B given the different base model setup (x-axis) like model size or pretraining data size. We also explained this in the updated version.\n\nFor example, if we investigate how the pretraining data size of the base 1B LLM would affect the decisions on finetuning data size, we can conclude from the plot (bottom-left) that when the pretraining data size is 2e11, the critical point for FMT vs LoRA at 50K indicates that we're more likely to achieve better performance with LoRA when we have data less than 50K while we have more headroom with FMT when we have more than 50K data."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8351/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700510977132,
                "cdate": 1700510977132,
                "tmdate": 1700510977132,
                "mdate": 1700510977132,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "8sEniik4Fo",
            "forum": "5HCnKDeTws",
            "replyto": "5HCnKDeTws",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8351/Reviewer_gFLk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8351/Reviewer_gFLk"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a scaling law for language model fine-tuning. The claim is supported by experiments with fine-tuning in machine translation and summarization tasks. The other scaling factors besides fine-tuning dataset size are LLM size, pretraining data size, and PET parameter sizes for the Prompt Tuning and LoRa fine-tuning regimes. The core finding is that a multiplicative scaling model achieves better fits than a closely additive scaling model."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "It's excellent to see scaling law investigations extended to fine-tuning, where they can potentially provide a lot of practical value.\n\nThe paper is generally clear and direct, and the experimental findings are quite rich."
                },
                "weaknesses": {
                    "value": "The precise nature and strength of the findings is difficult for me to discern, and I am not sure of the value of this result for fine-tuning work. All of this leaves me concerned about the paper, but I am open-minded. I am going to express my concerns as questions and see what the answers are."
                },
                "questions": {
                    "value": "1. Table 2 shows that the multiplicative scaling is better than the additive one for WMT14 En-De. However, the multiplicative model seems strictly more expressive than the additive one, so I am not sure this is surprising. Why not add even more terms for even more expressivity? Can there be, and should there be, some controlling for the complexity of the law itself?\n\n2. Where are the counterparts of Table 2 for the other tasks and other metrics (besides perplexity)? My apologies if I am overlooking something in the paper. It seems like these other numbers would be given prominently.\n\n3. The paper does say that the BLEURT-RougeL picture \"shows high correlation with the PPL scores in general\", but eye-balling Figure 7 in the appendix doesn't support this too well, though it's easy to imagine that the quantitative picture is different. But what is the quantitative picture?\n\n4. The promise of the paper is that the scaling law will provide guidance for people seeking to fine-tune. However, the guidance seems to me that perplexity will generally go down for all methods, but that the best method and precisely ideal stopping point will be highly variable. This guidance is very familiar and doesn't need to be characterized with a \"scaling law\". Is there more specific guidance implicit in this work?\n\n__The authors gave thoughtful answers to the above questions, which helped me understand the work better, and I raised my score by 1 point in response.__"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8351/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8351/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8351/Reviewer_gFLk"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8351/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698799551933,
            "cdate": 1698799551933,
            "tmdate": 1700583523254,
            "mdate": 1700583523254,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "meDbbhl8RJ",
                "forum": "5HCnKDeTws",
                "replyto": "8sEniik4Fo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8351/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8351/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer gFLk"
                    },
                    "comment": {
                        "value": "Thanks for your insightful comments!\n\n\n**Re: The core finding is that a multiplicative scaling model achieves better fits than a closely additive scaling model.**\n\nWe appreciate your highlight of the proposed multiplicative scaling law. Note we consider it as a tool for analyzing the empirical results. Our main focus is to understand how LLM model size, pretraining data size, PET parameter size and finetuning data size affect the finetuning, which we achieve through this tool. We hope our follow-up findings could gain more attention, such as the importance of LLM model scaling, the ineffectiveness of PET scaling, and the high degree of task dependence of LLM finetuning.\n\n\n**Re: Why not add even more terms for even more expressivity? Can there be, and should there be, some controlling for the complexity of the law itself?**\n\nExpressivity is not the only consideration and we expect the law to be as simple/clean as possible.\n1) There should be a trade off between expressivity and generalization. Adding more terms increases the expressivity theoretically but also brings in risks of overfitting, i.e. the model fits well on the given data but suffers from poor extrapolation. This problem becomes more concerning in LLM finetuning as the finetuning results can be very noisy, particularly for LoRA and Prompt tuning.\n2) Keeping the formulation simple eases analysis. Note our analysis in Section 4 is mainly based on the comparison of different scaling exponents. But such direct comparison is significant only for the proposed multiplicative formulation without additional terms. In the additive formulation, for example, comparing $\\alpha$ with $\\beta$ is complicated due to the difference of $A$ and $B$.\n\n\n**Re: Where are the counterparts of Table 2 for the other tasks and other metrics (besides perplexity)?**\n\nFor the fitting errors on WMT En-Zh and MLSum, we add them in Table 6, Appendix in the updated version. We noticed that the multiplicative scaling law performs worse on MLSum compared to the additive one; but overall, our proposal generalizes slightly better over different tasks and finetuning methods.\n\n\nFor the scaling laws on metrics other than PPL, we didn\u2019t explore them. Firstly, these metrics highly correlate with PPL as demonstrated by the high Pearson\u2019s $r$ in Table 7, Appendix in the updated version. Secondly, adopting PPL for scaling law is the standard practice in the literature [1,2,3] because improvements in PPL may not be well reflected in other metrics. Take the binary classification task as an example. Improving the probability (thus PPL) of the correct answer from 0.1 to 0.2 doesn\u2019t change the classification accuracy (take 0.5 as the threshold).\n\n\n**Re: BLEURT-RougeL picture \"shows high correlation with the PPL scores in general\"**\n\nWe performed Pearson correlation analysis between PPL and BLEURT/RougeL and added Table 7 to Appendix in the updated version. Most Pearson\u2019s $r$\u2019s absolute value is larger than 0.9, and all of them are significant at $p<0.01$ except FMT for LLM model scaling on WMT En-De.\n\n\n**Re: This guidance is very familiar and doesn't need to be characterized with a \"scaling law\". Is there more specific guidance implicit in this work?**\n\nWhile the findings align with intuition and look familiar, the scaling law is still necessary as it offers a quantitative measure for the impact of different scaling factors on the finetuning. This enables the comparison between scaling factors and gives readers more convincing evidence beyond intuition and experience. We consider these findings reached through large-scale and systematic experiments and characterized by our scaling laws as significant contributions to the community.\n\n\nWe need to emphasize that our results provide rich information and the guidance is multi-dimensional, not just specific to finetuning. For example, finetuning benefits more from LLM model scaling than pretraining data scaling, which suggests a different recipe compared to the computation optimal scaling for the pretraining. In terms of finetuning, while PPL generally goes down with more data, the expected return from collecting more finetuning data differs greatly across finetuning methods as demonstrated by the scaling laws, and PET shouldn\u2019t be expected to perform well with small LLMs.\n\n\nIn the era of LLM, our study represents itself as the first kind to explore the model, data and finetuning method for the scaling of LLM finetuning, which we believe contains intriguing and substantial contents to the community. We hope the reviewer could reconsider our paper. \n\n\n>- [1] Kaplan et al., 2020. Scaling Laws for Neural Language Models\n>- [2] Ghorbani et al., 2021.  Scaling Laws for Neural Machine Translation\n>- [3] Hoffmann et al., 2022. Training Compute-Optimal Large Language Models"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8351/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700510658711,
                "cdate": 1700510658711,
                "tmdate": 1700510658711,
                "mdate": 1700510658711,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OV3Q64HiwW",
                "forum": "5HCnKDeTws",
                "replyto": "meDbbhl8RJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8351/Reviewer_gFLk"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8351/Reviewer_gFLk"
                ],
                "content": {
                    "title": {
                        "value": "\"Scaling law\" framing distracts from the experimental findings"
                    },
                    "comment": {
                        "value": "I appreciate the authors' responses and additions to the paper. I am raising my score by 1 point. Based on the discussion in this forum, I feel it's a mistake to say the paper has discovered a \"scaling law\". The scaling law looks more like a sort of guiding hypothesis that turns out to be kind of true but also pretty much wrong in many relevant scenarios. The takeaway seems to be that many aspects of fine-tuning are in fact not predictable from a simple scaling law. This is completely fine with me as a discovery about fine-tuning, and the paper offers rich experimental support for it. However, I remain concerned that the paper will be advertised as one that discovers a scaling law for fine-tuning."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8351/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700583464787,
                "cdate": 1700583464787,
                "tmdate": 1700583464787,
                "mdate": 1700583464787,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TzeUfr5cac",
                "forum": "5HCnKDeTws",
                "replyto": "VTmhRwPEXJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8351/Reviewer_gFLk"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8351/Reviewer_gFLk"
                ],
                "content": {
                    "title": {
                        "value": "I agree"
                    },
                    "comment": {
                        "value": "> To the reviewers: I think we should not penalize the authors if the results turn out not to fit a clean formula -- other authors should get the same results, no?\n\nI agree. I myself was not doing this. [My comment above](https://openreview.net/forum?id=5HCnKDeTws&noteId=OV3Q64HiwW) says, \"The takeaway seems to be that many aspects of fine-tuning are in fact not predictable from a simple scaling law. This is completely fine with me as a discovery about fine-tuning, and the paper offers rich experimental support for it. However, I remain concerned that the paper will be advertised as one that discovers a scaling law for fine-tuning.\"\n\nThe revised version of the paper takes steps to alleviate my concern about how the paper will be advertised."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8351/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700691096104,
                "cdate": 1700691096104,
                "tmdate": 1700691096104,
                "mdate": 1700691096104,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]