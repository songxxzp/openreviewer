[
    {
        "title": "GAIA: Data-driven Zero-shot Talking Avatar Generation"
    },
    {
        "review": {
            "id": "X340xyYxq6",
            "forum": "ATEawsFUj4",
            "replyto": "ATEawsFUj4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5722/Reviewer_tNS3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5722/Reviewer_tNS3"
            ],
            "content": {
                "summary": {
                    "value": "This work aims to generate talking avatars with two separate modules: 1. first disentangle motion and appearance; 2 then generate head motions in accordance with speech.  As the proposed method does not utilize 3DMMs, the proposed data-driven method is promising to generate talking avatars with better diversity and naturalness.  The reported experiments show better quantitative results and better visual quality."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This work establishes a large-scale talking avatar dataset for data-driven talking avatar generation.\n\n2. The proposed data-driven method could achieve taking avatar generation with superior performance on naturalness, diversity, lip-sync quality, and visual quality.  \n\n3. The proposed method is scalable, and the authors conduct experiments that show the larger model is employed, the better performance could be achieved.\n\n4. The authors show that the proposed method could support many related applications, such as controllable talking avatar generation and text-driven video generation."
                },
                "weaknesses": {
                    "value": "The authors did not provide a large number of generation examples. I hope it is possible to see more visual results.\nFor example, \n1. long videos for the generated talking avatars;\n\n2. different reference video/frame but the same driving video;\n\n3. different driving video but the same reference video/frame."
                },
                "questions": {
                    "value": "1. is it possible to drive cartoon characters (humanoid or non-humanoid)?\n\n2. will the proposed method and the established dataset be publically available?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "no"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5722/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698670908464,
            "cdate": 1698670908464,
            "tmdate": 1699636599026,
            "mdate": 1699636599026,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6jwZLmBWEL",
                "forum": "ATEawsFUj4",
                "replyto": "X340xyYxq6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5722/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5722/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Official Review by Reviewer tNS3"
                    },
                    "comment": {
                        "value": "Dear Reviewer tNS3, thank you for taking the time to review. We appreciate that you found our model simple and scalable with superior performance. \n\n>**Q: The authors did not provide a large number of generation examples. I hope it is possible to see more visual results.**\n\nA: Thank you for your interest. We provide more samples on the anonymous **[demo page](https://gaiavatar.github.io/gaia/)** for convenience. More specifically, 1) an example of \"long video generation\" is provided in **[this video](https://gaiavatar.github.io/gaia/video/more/long_video.mp4)**. Since we focus on demonstrating the effectiveness of our proposed method, the long video generation is not yet well explored in the original manuscript. According to your comments, we present a simple yet effective strategy to generate infinite long videos -- by fixing the first several frames of each generation step during the reverse diffusion process. In this way, we obtain a smooth transition between each sub-video. 2) more examples of \"different reference frame but the same driving video\" are provided in **[this video](https://gaiavatar.github.io/gaia/video/more/different_reference_same_driving.mp4)**. 3) more examples of \"different driving video but the same reference frame\" are provided in **[this video](https://gaiavatar.github.io/gaia/video/more/different_driving_same_reference.mp4)**.\n\nWe hope the above videos could make our results more clear. \n\n>**Q: Is it possible to drive cartoon characters (humanoid or non-humanoid)?**\n\nA: Sure. GAIA supports cartoon characters well. Examples can be found at **[this video](https://gaiavatar.github.io/gaia/video/more/cartoon1.mp4)**, **[this video](https://gaiavatar.github.io/gaia/video/more/cartoon2.mp4)** and also **[this video](https://gaiavatar.github.io/gaia/video/more/long_video.mp4)**, which demonstrates that our method generalizes well to out-of-domain data.\n\n>**Q: Will the proposed method and the established dataset be publically available?**\n\nA: We promise that the code will be released upon publication. For the dataset and pre-trained models, since privacy information is involved (i.e., real human faces), we are going through an internal compliance review and will try our best to mitigate the privacy risks and release them to facilitate future research.\n\nWe welcome further discussion and are willing to answer any further questions."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5722/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700232985030,
                "cdate": 1700232985030,
                "tmdate": 1700232985030,
                "mdate": 1700232985030,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "J2oEV7tRk4",
                "forum": "ATEawsFUj4",
                "replyto": "X340xyYxq6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5722/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5722/Authors"
                ],
                "content": {
                    "title": {
                        "value": "We hope that our response addresses your concern"
                    },
                    "comment": {
                        "value": "Dear Reviewer tNS3,\n\nWe greatly appreciate the time you've invested in reviewing our response. Having submitted our rebuttal, we are eager to know if our response has addressed your concern. As the end of the rebuttal phase is approaching, we look forward to hearing from you for any further clarification that you might require.\n\nBest,\n\nSubmission 5722 authors"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5722/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700581306720,
                "cdate": 1700581306720,
                "tmdate": 1700581306720,
                "mdate": 1700581306720,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "wUn0m5sjQC",
            "forum": "ATEawsFUj4",
            "replyto": "ATEawsFUj4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5722/Reviewer_Lkke"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5722/Reviewer_Lkke"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a data driven approach for generation of 2D avatars. The method disentangles motion and appearance and uses a diffusion model to allow motion generation conditioned on pose and speech data."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1) The method is conceptually simple and sensible.\n2) It is shown to scale well in terms of model size and as a self-supervised method can utilize readily available training data at scale.\n2) Method requires very few pretrained components.\n3) Evaluation includes user study which is always good for addressing output quality.\n4) Method is highly flexible and allows a high degree of control from pose, facial attributes and text."
                },
                "weaknesses": {
                    "value": "1) A comparison with https://arxiv.org/pdf/2012.08261.pdf for video driven is critically missing as a recently proposed SOTA method. In their paper they show improvements compared to face-vid2vid and FOMM which are used as baselines here and they provide a pretrained checkpoint."
                },
                "questions": {
                    "value": "1) will the dataset be shared as part of this submission?\n2) It would be interesting to see an ablation on training data size to assess whether there are benefits from scaling data further.\n3) How does randomness from the diffusion model affect generations?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5722/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5722/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5722/Reviewer_Lkke"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5722/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698702137079,
            "cdate": 1698702137079,
            "tmdate": 1699636598920,
            "mdate": 1699636598920,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fPebarSFUy",
                "forum": "ATEawsFUj4",
                "replyto": "wUn0m5sjQC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5722/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5722/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Official Review by Reviewer Lkke"
                    },
                    "comment": {
                        "value": "Dear Reviewer Lkke, thank you for your positive and thoughtful feedback and for looking into every detail of our work. \n\n>**Q: The comparison with HeadGAN for the video-driven generation.**\n\nA: Thanks for reminding us of this important related work! We evaluate this model with the released checkpoint on our test set. To ensure fair comparisons, in addition to the model trained on our dataset, we also train one smaller GAIA model with 80M parameters on the VoxCeleb2 dataset to keep consistent with HeadGAN. We test models on the video-driven self-reconstruction and cross-reenactment settings, and the results are shown below.\n\nTable 5. Quantitative comparisons of self-reconstruction with HeadGAN.\n\n|                       |    FID$\\downarrow$ |   LPIPS$\\downarrow$   |   PSNR$\\uparrow$ | AKD$\\downarrow$  |   MSI$\\uparrow$ |\n| --------------------- | ----- | ----- | ----- | ----- | ----- | \n| HeadGAN          | 21.499 | 0.278  | 18.555 | 2.990  | 0.835 |\n| GAIA-small w/ VoxCeleb2 | 16.099 | 0.173  | 22.896 | 1.434  | 1.083 |\n| GAIA | 15.730 | 0.167  | 23.942 | 1.442 | 0.856 |\n\nTable 6. Quantitative comparisons of cross-reenactment with HeadGAN.\n\n|                       |  FID$\\downarrow$ | AKD$\\downarrow$   |  MSI$\\uparrow$  |\n| --------------------- | ----- | ----- | ----- |\n| HeadGAN    | 90.746 | 5.964  | 0.788 |\n| GAIA-small w/ VoxCeleb2 | 27.643 | 2.968  | 1.035 |\n| GAIA | 15.200 | 2.003  | 1.102\n\nFrom the results, GAIA achieves consistent improvements over HeadGAN across different metrics and settings. We have added the comparison with HeadGAN to Table 2 and 7 in the manuscript.\n\n>**Q: Will the dataset be shared as part of this submission?**\n\nA: We promise that the code will be released upon publication. For the dataset and pre-trained models, since privacy information is involved (i.e., real human faces), we are going through an internal compliance review and will try our best to mitigate the privacy risks and release them to facilitate future research.\n\n\n>**Q: The ablation on training data size.**\n\nA: We have investigated the scaling of training data in Tables 4 \\& 5 of the manuscript, where \\#Hours indicates the total training size. It can be observed that, with the same model size (80M VAE and 180M diffusion), our method obtains better results on more training data (e.g., 17.486 vs. 18.353 FID score, 8.913 vs. 9.145 Sync-D score). It is also interesting to see what will happen on larger data size (i.e., more than 1K hours). We leave it as future work.\n\n\n>**Q: How does randomness from the diffusion model affect generations?**\n\nA: Thank you for your interest! To show the effect of randomness, we conduct three kinds of visualizations in **[this video](https://gaiavatar.github.io/gaia/video/more/randomness.mp4)**: 1) we use different random seeds (i.e., seed 42 \\& 82) for the generation, from which we observe that different seeds make slight differences for the generated videos; 2) we evaluate the models from different training epochs (i.e., epoch 1599 \\& 1799), which shows the large difference on the generated motion; 3) we make generation with different reference images, which demonstrates that the reference image affects the appearance of the generated video since the missing information should be predicted by the model. \n\nWe welcome further discussion and are willing to answer any further questions."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5722/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700232636559,
                "cdate": 1700232636559,
                "tmdate": 1700232636559,
                "mdate": 1700232636559,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PSiBRIZYbk",
                "forum": "ATEawsFUj4",
                "replyto": "wUn0m5sjQC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5722/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5722/Authors"
                ],
                "content": {
                    "title": {
                        "value": "We hope that our response addresses your concern"
                    },
                    "comment": {
                        "value": "Dear Reviewer Lkke,\n\nWe greatly appreciate the time you've invested in reviewing our response. Having submitted our rebuttal, we are eager to know if our response has addressed your concern. As the end of the rebuttal phase is approaching, we look forward to hearing from you for any further clarification that you might require.\n\nBest,\n\nSubmission 5722 authors"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5722/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700581254428,
                "cdate": 1700581254428,
                "tmdate": 1700581254428,
                "mdate": 1700581254428,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aBWkxDiqUU",
                "forum": "ATEawsFUj4",
                "replyto": "PSiBRIZYbk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5722/Reviewer_Lkke"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5722/Reviewer_Lkke"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Authors,\n\nThank you for your response to my comments. The rebuttal has addressed my concerns and I am keeping my score as it is."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5722/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700652131449,
                "cdate": 1700652131449,
                "tmdate": 1700652131449,
                "mdate": 1700652131449,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "3NP8TiZs8K",
            "forum": "ATEawsFUj4",
            "replyto": "ATEawsFUj4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5722/Reviewer_ft55"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5722/Reviewer_ft55"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes an audio-driven talking head synthesis model named GAIA that is an end-to-end trainable data-driven solution. The model has two main stages 1. disentanglement of motion and appearance with VAE 2. speech-to-motion generation based on diffusion model. Also, the paper proposes a new talking head dataset with 8.2K hours of video and 16.9K unique speakers."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The manuscript proposes a new dataset.\n\nThe writing is supported by equations and well-drawn figures that make the explanation clear.\n\nAlthough the experiments with existing models are not enough (see weaknesses), the ablation study is rich and increases the overall quality."
                },
                "weaknesses": {
                    "value": "The gap/ limitations of 3DMM-based models are found and addressed well by proposing an end-to-end trainable model. I am not sure it is novel enough as the other end-to-end trainable talking face synthesis models are not discussed enough.\n\nThe experiments are limited, especially comparison with end-to-end trainable models not provided. I suggest enriching the benchmarking with other existing models such as  PC-AVS and PD-FGC as they are also end-to-end trainable models. \n\nAlthough the writing quality is decent, it is hard to follow as it refers to other sections frequently and other issues (see Questions 1 and 2)."
                },
                "questions": {
                    "value": "1. In section 3, what does 'we collect High Definition Talking Face Dataset (HDTF) (Zhang et al., 2021) and Casual Conversation datasets v1&v2 (CC v1&v2) (Hazirbas et al., 2021; Porgali et al., 2023)' and 'we also collect a large-scale internal talking avatar dataset named AVENA' mean? Does it mean you collect those datasets you use their sample in your dataset or you use their samples in your training but they are not in your dataset? From the supplementary material, my understanding is you collect AVENA and use samples from other datasets (HDTF, CC v1, and v2) in the training of your model. Could you please elaborate and make it clear?\n\n2. Why do you have a discussion section at the end of Section 4 Model? I think it makes more sense in/after experiments. So, you can consider reorganizing the manuscript to have a better flow and complete discussion.\n\n3. 3. I am not sure the model can be named as zero-shot as it requires one shot for unseen faces. So, could you elaborate on the following '... generates a talking video of an unseen speaker with one portrait image ...'?\n\n3. Ethical consideration is left in Appendix F. However, for this study ethical consideration is important. So, you might consider putting it into the main manuscript to give necessary importance if possible."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5722/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5722/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5722/Reviewer_ft55"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5722/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698744584519,
            "cdate": 1698744584519,
            "tmdate": 1699636598809,
            "mdate": 1699636598809,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "l5teiMiWf7",
                "forum": "ATEawsFUj4",
                "replyto": "3NP8TiZs8K",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5722/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5722/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Official Review by Reviewer ft55 (1/3)"
                    },
                    "comment": {
                        "value": "Dear Reviewer ft55, thank you for taking the time to review and propose promising extensions. We appreciate that you found our rich ablation study and the good writing. We address your concerns as follows.\n\n>**Q: Comparisons with other end-to-end trainable talking face synthesis models.**\n\nA: Thanks a lot for reminding the related end-to-end trainable works! We take PC-AVS [1] and PD-FGC [2] as examples to make more discussions. PC-AVS [1] and PD-FGC [2] similarly introduced identity space and non-identity space. The identity space is obtained by leveraging the identity labels, while the non-identity space is disentangled from the inputs through random data augmentation [3]. The authors employed contrastive learning to align the non-identity space and speech content space (except pose). However, our method differs in three ways: 1) they need additional driving video to provide motion information like head pose. In contrast, we generate the full motion from the speech at the same time and also provide the option to control the head pose; 2) they use contrastive learning to align speech and visual motion, which may lead to limited diversity due to the one-to-many mapping nature between the audio and visual motion. In contrast, we leverage diffusion models to predict motion from the speech; 3) their identity information is extracted by using identity labels while our method does not need additional labels.\n\nWe further highlight our contributions as follows:\n\n* We propose a novel and sensible (as recognized by Reviewer 1S6P \\& Lkke) framework that eliminates the heuristics and generates the full motion latent at the same time. The method reveals three key insights: 1) the complete disentanglement between the motion (speech-related) and the appearance (speech-agnostic) is the key to success; 2) handling one-to-many mapping with the diffusion model and learning full motion from real data distribution result in natural and diverse generations; 3) less dependence on heuristics and labels makes the method general and scalable.\n\n* We achieve superior performance on naturalness, diversity, lip-sync quality, and visual quality (as recognized by Reviewer 1S6P \\& Lkke \\& tNS3).\n\n* We, for the first time, verify the scalability of our method in talking avatar generation (as recognized by Reviewer Lkke \\& tNS3).\n\n* The method is general and flexible to support many applications: speech-driven only, coarse-grained control (controlled by head pose), fine-grained control (fully controllable), and text-instructed avatar generation (as recognized by Reviewer 1S6P \\& Lkke \\& tNS3), which has never been explored in previous literature.\n\nWe have added the discussions in the revised manuscript and marked the revisions in blue in Section 2.\n\n[1] Pose-Controllable Talking Face Generation by Implicitly Modularized Audio-Visual Representation. CVPR 2021.\n\n[2] Progressive Disentangled Representation Learning for Fine-Grained Controllable Talking Head Synthesis. CVPR 2023.\n\n[3] Neural Head Reenactment with Latent Pose Descriptors. CVPR 2020.\n\nDue to character limitations, please refer to the next comment for other questions."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5722/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700231824937,
                "cdate": 1700231824937,
                "tmdate": 1700231824937,
                "mdate": 1700231824937,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Jokf7BoSjE",
                "forum": "ATEawsFUj4",
                "replyto": "3NP8TiZs8K",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5722/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5722/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Official Review by Reviewer ft55 (2/3)"
                    },
                    "comment": {
                        "value": ">**Q: The experiments are limited, especially comparison with end-to-end trainable models not provided. I suggest enriching the benchmarking with other existing models such as PC-AVS and PD-FGC as they are also end-to-end trainable models.**\n\nA: Thanks for your suggestion! Following your advice, we have evaluated PC-AVS [1] and PD-FGC [2] with their released models on the same test set. The results are listed in the table below. The visual comparison is provided in **[this video](https://gaiavatar.github.io/gaia/video/more/comparison_with_PC_PD.mp4)** and the full comparison is presented in **[this video](https://gaiavatar.github.io/gaia/video/more/1_speech-driven_add.mp4)**.\n\nTable 4. Quantitative comparisons of speech-driven generation with previous end-to-end trainable models.\n\n|                       |  Nat.$\\uparrow$   |   Lip.$\\uparrow$   |  Jit.$\\uparrow$  |   Vis.$\\uparrow$  |   Mot.$\\uparrow$   |  Sync-D$\\downarrow$   |  MSI$\\uparrow$   |   FID$\\downarrow$ |\n| --------------------- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- |\n| PC-AVS      |    2.797   |  3.843  |  3.546  |  3.452  |  2.091  |  8.341  |    0.677  |  59.464 |\n| PD-FGC*     |    3.283   |  3.893  |  1.905  |  3.417   | 4.512 |   8.573  |    0.478 |   58.943 |\n| GAIA        |    4.362   |  4.332  |  4.345  |  4.320  |  4.243  |  8.528   |   1.181  |  22.924 |\n\nIn this table, \\* indicates that PD-FGC depends on extra driving videos to provide pose, expression, and eye motions. We use the real (ground-truth) video as its driving video (therefore the score of motion diversity is high). We also note that the Sync-D score for real video is 8.548, which is close to ours. Overall, our method beats the previous end-to-end trainable models in terms of overall naturalness (Nat.), lip-sync quality (Lip.), motion jittering (Jit.), visual quality (Vis.), and motion diversity (Mot.). We have added the comparison in Table 8 in the revised version.\n\n[1] Pose-Controllable Talking Face Generation by Implicitly Modularized Audio-Visual Representation. CVPR 2021.\n\n[2] Progressive Disentangled Representation Learning for Fine-Grained Controllable Talking Head Synthesis. CVPR 2023.\n\n\n>**Q: Although the writing quality is decent, it is hard to follow as it refers to other sections frequently and other issues (see Questions 1 and 2).**\n\nA: Thanks for the suggestion! We have removed the unnecessary referrals to make the manuscript easy to follow. For example, we remove the referral in Section 5.3.1 (Ablation Studies on Scaling) and Section 5.5 (Discussion). In the revised version, apart from the referrals to the tables or figures, most referrals are referred to the Appendix for detailed illustrations.\n\n\n>**Q: About the collection of HDTF, CC v1\\&v2 and AVENA datasets.**\n\nA: Thanks for your careful reading of our paper and supplementary materials, and sorry for the confusion. Your understanding is correct. AVENA is an internal dataset collected from our institution, and we utilize the union of AVENA, HDTF, CC v1 and v2 as the training set of our model. We have revised the description in Section 3 (Data Collection and Filtering) and Section 5.1 (Experimental Setups) for better understanding and marked it in blue.\n\n\n>**Q: About the discussion section.**\n\nA: Thanks for the suggestion! We have reorganized the flow in the revised manuscript and marked the revisions in blue. For example, we moved the improved discussion to the end of the experiments (Section 5.5).\n\n\n>**Q: I am not sure the model can be named as zero-shot as it requires one shot for unseen faces. So, could you elaborate on the following '... generates a talking video of an unseen speaker with one portrait image ...'?**\n\nA: The term \"zero-shot\" is potentially ambiguous in literature. In this paper, we use the zero-shot term due to the following reasons:\n1) the method is \"zero-shot\" typically in the sense that no gradient updates are performed;\n2) while inference, the difference between zero-/one-/few-shot lies in the number of utilized demonstrations [1], where a demonstration usually indicates an input-output pair of data. In our case, the provided unseen face is not a kind of demonstration, making the inference procedure zero-shot;\n3) to be differentiable with the related works where a fine-tuning stage on the given reference portrait image is needed for the unseen identities [2,3,4].\n\nWe are open to further discussions on this concern.\n\n[1] Language Models are Few-Shot Learners. NeurIPS 2020.\n\n[2] Few-Shot Adversarial Learning of Realistic Neural Talking Head Models. ICCV 2019.\n\n[3] Neural Head Reenactment with Latent Pose Descriptors. CVPR 2020.\n\n[4] Tune-A-Video: One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation. ICCV 2023.\n\nDue to character limitations, please refer to the next comment for other questions."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5722/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700232145221,
                "cdate": 1700232145221,
                "tmdate": 1700232286460,
                "mdate": 1700232286460,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "i8GO1spECq",
                "forum": "ATEawsFUj4",
                "replyto": "3NP8TiZs8K",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5722/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5722/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Official Review by Reviewer ft55 (3/3)"
                    },
                    "comment": {
                        "value": ">**Q: About the location of Ethical consideration.**\n\nA: Thanks for the suggestion, and we agree that ethical considerations are crucial for the completeness of the paper. We have revised the content of the manuscript and put the ethical considerations in Section 6 (Conclusion).\n\nWe hope our rebuttal and paper revision can address your concerns. We welcome further discussion and are willing to answer any further questions."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5722/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700232250995,
                "cdate": 1700232250995,
                "tmdate": 1700232250995,
                "mdate": 1700232250995,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zvIo8LjCv0",
                "forum": "ATEawsFUj4",
                "replyto": "3NP8TiZs8K",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5722/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5722/Authors"
                ],
                "content": {
                    "title": {
                        "value": "We hope that our response addresses your concern"
                    },
                    "comment": {
                        "value": "Dear Reviewer ft55,\n\nWe greatly appreciate the time you've invested in reviewing our response. Having submitted our rebuttal, we are eager to know if our response has addressed your concern. As the end of the rebuttal phase is approaching, we look forward to hearing from you for any further clarification that you might require.\n\nBest,\n\nSubmission 5722 authors"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5722/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700581187565,
                "cdate": 1700581187565,
                "tmdate": 1700581187565,
                "mdate": 1700581187565,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xcOeRrgmYh",
                "forum": "ATEawsFUj4",
                "replyto": "3NP8TiZs8K",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5722/Reviewer_ft55"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5722/Reviewer_ft55"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Authors,\n\nThank you for the rebuttal and addressing my concerns.\n\nI have read the responses carefully. And I think, with the revisions, the overall quality has been improved which led me to reconsider my score in the final review. I tend to increase my rating."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5722/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700666368574,
                "cdate": 1700666368574,
                "tmdate": 1700666409402,
                "mdate": 1700666409402,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "nhkezJyr7A",
            "forum": "ATEawsFUj4",
            "replyto": "ATEawsFUj4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5722/Reviewer_1S6P"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5722/Reviewer_1S6P"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose to use a generative latent diffusion model to address the problem of talking head synthesis from audio and a single photo. The pipeline consists of a variational autoencoder that encodes the video frames into appearance and motion latent representations and a diffusion model that is trained to predict the pre-trained motion latent from audio and pose conditioning. The authors also propose to use a data filtering approach to remove the noisy samples from the dataset to achieve high-quality results. The experimental evaluation is quite extensive and includes audio, head pose, and text-driven examples."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Impressive quality of the results in terms of both lipsync and visual quality\n- The model's design is straightforward yet evidently effective\n- The paper is well-written and the evaluation is pretty extensive"
                },
                "weaknesses": {
                    "value": "- Missing evaluation of disentanglement between appearance and pose latent codes, i.e., cross-reenactment with the motion codes extracted from the image of a different identity.\n- Missing discussion of the related works, such as [1, 2], that explored the concept of pose-identity disentanglement for talking head synthesis before this work.\n- As far as I can tell, the proposed method and the baselines were trained on different datasets. The resulting comparison evaluates the proposed framework _and_ the dataset at the same time. A comparison should include the experiments where base methods are trained on the same dataset, and the proposed method is trained on unprocessed datasets used in previous works.\n- Comparison of the inference time between the compared methods is not provided. I would also argue that some baselines, such as SadTalker, can be substantially improved, given the computational budget of the proposed method that runs the diffusion model for every time step. Ex., with the fine-tuning of the model given the source frame.\n\n[1] Burkov et al., \"Neural Head Reenactment with Latent Pose Descriptors\", CVPR 2020\n[2] Drobyshev et al., \"MegaPortraits: One-shot Megapixel Neural Head Avatars\", ACMMM 2022"
                },
                "questions": {
                    "value": "- Please address the concerns mentioned in the weaknesses\n- Could the authors clarify if they plan to release the filtered dataset and the pre-trained models?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5722/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698826909162,
            "cdate": 1698826909162,
            "tmdate": 1699636598717,
            "mdate": 1699636598717,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BE1UUirEhj",
                "forum": "ATEawsFUj4",
                "replyto": "nhkezJyr7A",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5722/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5722/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Official Review by Reviewer 1S6P (1/2)"
                    },
                    "comment": {
                        "value": "Dear Reviewer 1S6P, we are grateful for your careful review and the valuable feedback that you provided for our paper. We appreciate that you found our paper well-written and sound with convincing results. We hope the following comments address your concerns.\n\n>**Q: Missing evaluation of disentanglement between appearance and pose latent codes, i.e., cross-reenactment.**\n\nA: We have evaluated the cross-reenactment performance of GAIA and baselines in the right half of Table 2 of the manuscript, denoted as \"Cross-Reconstruction\" in the original version, where GAIA achieves significant improvements over baselines. Sorry for the confusion about the notations we utilized, we have changed it to \"Cross-Reenactment\" in this version to keep consistent with baselines.\n\n>**Q: Missing discussion of some related works.**\n\nA: Thank you for pointing out the related works! We make discussions as follows. We have also added the discussions to Section 2 (Related Works) and the revisions are marked in blue.\n\nLatent Pose Descriptors [1] achieved pose-identity disentanglement, where the identity embedding is averaged across multiple frames and the pose embedding is obtained with augmented input. However, the model needs additional fine-tuning for the unseen identities while we can directly generalize to the unseen identities with only one reference portrait image.\n\nMegaPortraits [2] adopted warping-based transformation, which is similar to face-vid2vid [3], but uses latent descriptors to represent expression instead of keypoints like face-vid2vid [3]. However, as we have validated in the subjective comparison and the experiments of the original manuscript, the usage of the heuristics hinders direct learning from data distribution, leading to unnatural results and limited diversity.\n\nIn addition, instead of supporting video-driven avatar generation only [1,2], GAIA contains both motion \\& appearance disentanglement and speech-to-motion modules that support speech-driven, video-driven, pose-controllable, full-controllable and text-instructed avatar generation in a unified framework.\n\n[1] Burkov et al., \"Neural Head Reenactment with Latent Pose Descriptors\", CVPR 2020\n\n[2] Drobyshev et al., \"MegaPortraits: One-shot Megapixel Neural Head Avatars\", ACMMM 2022\n\n[3] Wang et al., \"One-shot free-view neural talking-head synthesis for video conferencing\", CVPR 2021\n\n\n>**Q: The proposed method and the baselines were trained on different datasets.**\n\nA: The reason we choose different datasets is that we find commonly utilized datasets such as VoxCeleb2 suffer a lot from the jittering content and thus have relatively low quality. However, we agree with the point that training on the same dataset provides fair comparisons. Therefore, we train our model on VoxCeleb2, the training dataset utilized by face-vid2vid, while keeping the test set unchanged. Due to the time limitation, we train a smaller model with 80M parameters and will train a larger one in the next. The results of video-driven self-reconstruction and cross-reenactment are listed as follows.\n\nTable 1. Quantitative comparisons of self-reconstruction when trained on VoxCeleb2 dataset.\n\n|                       |  FID$\\downarrow$  | LPIPS$\\downarrow$  | PSNR$\\uparrow$ | AKD$\\downarrow$   |  MSI$\\uparrow$  |\n| --------------------- | ----- | ----- | ----- | ----- | ----- |\n| face-vid2vid    | 18.604 | 0.184  |  23.681 | 2.195  | 0.813 |\n| GAIA-small w/ VoxCeleb2 | 16.099 | 0.173  | 22.896 | 1.434  | 1.083 |\n\n\nTable 2. Quantitative comparisons of cross-reenactment when trained on VoxCeleb2 dataset.\n\n|                       |  FID$\\downarrow$ | AKD$\\downarrow$   |  MSI$\\uparrow$  |\n| --------------------- | ----- | ----- | ----- |\n| face-vid2vid    | 28.093 | 3.630  | 0.853 |\n| GAIA-small w/ VoxCeleb2 | 27.643 | 2.968  | 1.035 |\n\nFrom the results, we can find that when trained on the same dataset, GAIA still outperforms face-vid2vid on most metrics except PSNR (though it is the smallest model), demonstrating the power of the proposed method. We have added the results in Table 7 and more discussions in Section C.1 (More Video-driven Results) of the manuscript.\n\nDue to character limitations, please refer to the next comment for other questions."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5722/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700231325033,
                "cdate": 1700231325033,
                "tmdate": 1700232712915,
                "mdate": 1700232712915,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vvwzF3tdCi",
                "forum": "ATEawsFUj4",
                "replyto": "nhkezJyr7A",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5722/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5722/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Official Review by Reviewer 1S6P (2/2)"
                    },
                    "comment": {
                        "value": ">**Q: Comparisons of the inference time are not provided.**\n\nA: Thank you for your interest! According to your suggestions, we have evaluated the inference time of the same speech (8s) for each method. Below are the results, where our diffusion step is set to 150 (the same as the one we used in all experiments).\n\nTable 3. Comparisons of the inference time of the same speech.\n\n|                       |  Inference Time |\n| --------------------- | ----- |\n| MakeItTalk                       |  12.398s |\n| Audio2Head                       |  4.878s |\n| SadTalker                         |  7.300s    |\n| PC-AVS                            | 2.202s  |\n| PD-FGC                            | 2.883s  |\n| GAIA (80M VAE + 180M Diffusion)  | 5.548s (2.639s + 2.909s) |\n| GAIA (700M VAE + 600M Diffusion)  | 20.473s (16.950s + 3.523s) |\n\nFrom the table above, we observe that our model achieves comparable inference time against the baselines (e.g., 2.639s for 80M VAE, 2.909s for 180M diffusion model). Although the larger models like 700M VAE and 600M diffusion need more inference time, it can be further reduced with advanced techniques (e.g., fast diffusion sampling, model distillation, etc.), yet is not the focus of this work. For the current studies, we have shown that our method is able to achieve substantial improvement over the state-of-the-art and illustrated the scalability and generalizability of the proposed method.\n\n\n>**Q: Could the authors clarify if they plan to release the filtered dataset and the pre-trained models?**\n\nA: We promise that the code will be released upon publication. For the dataset and pre-trained models, since privacy information is involved (i.e., real human faces), we are going through an internal compliance review and will try our best to mitigate the privacy risks and release them to facilitate future research.\n\nWe welcome further discussion and are willing to answer any further questions."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5722/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700231493112,
                "cdate": 1700231493112,
                "tmdate": 1700231493112,
                "mdate": 1700231493112,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DhC9buaA4Q",
                "forum": "ATEawsFUj4",
                "replyto": "nhkezJyr7A",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5722/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5722/Authors"
                ],
                "content": {
                    "title": {
                        "value": "We hope that our response addresses your concern"
                    },
                    "comment": {
                        "value": "Dear Reviewer 1S6P,\n\nWe greatly appreciate the time you've invested in reviewing our response. Having submitted our rebuttal, we are eager to know if our response has addressed your concern. As the end of the rebuttal phase is approaching, we look forward to hearing from you for any further clarification that you might require.\n\nBest,\n\nSubmission 5722 authors"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5722/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700581103315,
                "cdate": 1700581103315,
                "tmdate": 1700581208824,
                "mdate": 1700581208824,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "89I3yTwtu5",
                "forum": "ATEawsFUj4",
                "replyto": "DhC9buaA4Q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5722/Reviewer_1S6P"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5722/Reviewer_1S6P"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the rebuttal"
                    },
                    "comment": {
                        "value": "I thank the authors for the comprehensive rebuttal.\n\nIn the inference time section, could you please clarify what the separate timings in the parenthesis denote?\n\nAfter reading the other reviews and the rebuttal, I am quite happy with the changes the authors have provided. I tend to improve my score in the final review."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5722/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700655016382,
                "cdate": 1700655016382,
                "tmdate": 1700655016382,
                "mdate": 1700655016382,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]