[
    {
        "title": "Federated Virtual Learning on Heterogeneous Data with Local-global Distillation"
    },
    {
        "review": {
            "id": "vsvqitsm00",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission541/Reviewer_8zES"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission541/Reviewer_8zES"
            ],
            "forum": "fGXyvmWpw6",
            "replyto": "fGXyvmWpw6",
            "content": {
                "summary": {
                    "value": "1.\tThis paper proposes a federated virtual learning approach that leverages local and global dataset distillation techniques to simultaneously tackle the challenge of data heterogeneity as well as efficient training in federated learning. The authors claim that dataset distillation can exacerbate the heterogeneity among clients\u2019 local data and propose to alleviate this issue with distribution matching.\n2.\tThe problem addressed in this paper is novel and interesting. The adverse effect of dataset distillation in a federated learning setting is insightful. The proposed approach seems feasible and promising.\n3.\tIn your paper, the model on clients is split into feature extractors and classification heads. This split learning-like paradigm has been widely adopted by a series of prior works [1,2,3]. Please explain the deplorability of your approach on existing methods. More elaboration on how your proposed method relates to these works would be appreciated.\n4.\tIf I understand you correctly, FedProx is proposed by [4] rather than [5]. Do I misunderstand something?\n5.\tSome of the benchmark algorithms, such as FedProx [4], Scaffold [6], are somewhat outdated. In your experiments, you have used different open-sourced datasets as private data for clients, and this degree of data heterogeneity is apparently unfavorable for the regularization-based methods mentioned above. Would it be possible to compare your approach with some novel federated learning methods based on GANs [7], which seem to be more suitable for your scenario?\n\n[1]  \"FedICT: Federated Multi-task Distillation for Multi-access Edge Computing.\" IEEE Transactions on Parallel and Distributed Systems (2023).\n\n[2] \"Group knowledge transfer: Federated learning of large cnns at the edge.\" Advances in Neural Information Processing Systems 33 (2020): 14068-14080.\n\n[3] \"Exploring the distributed knowledge congruence in proxy-data-free federated distillation.\" arXiv preprint arXiv:2204.07028 (2022).\n\n[4] \"Federated optimization in heterogeneous networks.\" Proceedings of Machine learning and systems 2 (2020): 429-450.\n\n[5] \"On the convergence of fedavg on non-iid data.\" arXiv preprint arXiv:1907.02189 (2019).\n\n[6] \"Scaffold: Stochastic controlled averaging for federated learning.\" International conference on machine learning. PMLR, 2020.\n\n[7] \"Data-free knowledge distillation for heterogeneous federated learning.\" International conference on machine learning. PMLR, 2021."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The problem addressed in this paper is novel and interesting. The adverse effect of dataset distillation in a federated learning setting is insightful. The proposed approach seems feasible and promising."
                },
                "weaknesses": {
                    "value": "1.In your paper, the model on clients is split into feature extractors and classification heads. This split learning-like paradigm has been widely adopted by a series of prior works [1,2,3]. Please explain the deplorability of your approach on existing methods. More elaboration on how your proposed method relates to these works would be appreciated.\n\n2.If I understand you correctly, FedProx is proposed by [4] rather than [5]. Do I misunderstand something?\n\n3.Some of the benchmark algorithms, such as FedProx [4], Scaffold [6], are somewhat outdated. In your experiments, you have used different open-sourced datasets as private data for clients, and this degree of data heterogeneity is apparently unfavorable for the regularization-based methods mentioned above. Would it be possible to compare your approach with some novel federated learning methods based on GANs [7], which seem to be more suitable for your scenario?\n\n[1]  \"FedICT: Federated Multi-task Distillation for Multi-access Edge Computing.\" IEEE Transactions on Parallel and Distributed Systems (2023).\n\n[2] \"Group knowledge transfer: Federated learning of large cnns at the edge.\" Advances in Neural Information Processing Systems 33 (2020): 14068-14080.\n\n[3] \"Exploring the distributed knowledge congruence in proxy-data-free federated distillation.\" arXiv preprint arXiv:2204.07028 (2022).\n\n[4] \"Federated optimization in heterogeneous networks.\" Proceedings of Machine learning and systems 2 (2020): 429-450.\n\n[5] \"On the convergence of fedavg on non-iid data.\" arXiv preprint arXiv:1907.02189 (2019).\n\n[6] \"Scaffold: Stochastic controlled averaging for federated learning.\" International conference on machine learning. PMLR, 2020.\n\n[7] \"Data-free knowledge distillation for heterogeneous federated learning.\" International conference on machine learning. PMLR, 2021."
                },
                "questions": {
                    "value": "1.In your paper, the model on clients is split into feature extractors and classification heads. This split learning-like paradigm has been widely adopted by a series of prior works [1,2,3]. Please explain the deplorability of your approach on existing methods. More elaboration on how your proposed method relates to these works would be appreciated.\n\n2.If I understand you correctly, FedProx is proposed by [4] rather than [5]. Do I misunderstand something?\n\n3.Some of the benchmark algorithms, such as FedProx [4], Scaffold [6], are somewhat outdated. In your experiments, you have used different open-sourced datasets as private data for clients, and this degree of data heterogeneity is apparently unfavorable for the regularization-based methods mentioned above. Would it be possible to compare your approach with some novel federated learning methods based on GANs [7], which seem to be more suitable for your scenario?\n\n[1]  \"FedICT: Federated Multi-task Distillation for Multi-access Edge Computing.\" IEEE Transactions on Parallel and Distributed Systems (2023).\n\n[2] \"Group knowledge transfer: Federated learning of large cnns at the edge.\" Advances in Neural Information Processing Systems 33 (2020): 14068-14080.\n\n[3] \"Exploring the distributed knowledge congruence in proxy-data-free federated distillation.\" arXiv preprint arXiv:2204.07028 (2022).\n\n[4] \"Federated optimization in heterogeneous networks.\" Proceedings of Machine learning and systems 2 (2020): 429-450.\n\n[5] \"On the convergence of fedavg on non-iid data.\" arXiv preprint arXiv:1907.02189 (2019).\n\n[6] \"Scaffold: Stochastic controlled averaging for federated learning.\" International conference on machine learning. PMLR, 2020.\n\n[7] \"Data-free knowledge distillation for heterogeneous federated learning.\" International conference on machine learning. PMLR, 2021."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission541/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission541/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission541/Reviewer_8zES"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission541/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697290721371,
            "cdate": 1697290721371,
            "tmdate": 1700371876348,
            "mdate": 1700371876348,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2DgcMSinva",
                "forum": "fGXyvmWpw6",
                "replyto": "vsvqitsm00",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission541/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission541/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Discussion about the differences and deplorability with methods that also split the model into feature extractor and classification heads"
                    },
                    "comment": {
                        "value": "Thanks for the questions. We appreciate the opportunity to point out the differences between FedLGD and the papers you suggested to highlight our novel approach and to discuss the potential deployment of FedLGD with the methods. \n\n- **We have different setups.** Although we both formulate a classification model into a feature extractor and a classification head, FedLGD always keeps forward and backward training on both parts in local clients; however, those methods require putting the classification head on the server for inference and updates.\n- **We are orthogonal approaches.** FedLGD split the model intro feature extractor and classification head so that we can add *regularization to feature space* with *different data* (local and global distilled) on the *same local feature extractor*. In contrast, those papers are knowledge distillation methods that split the model so that they can obtain and share local features with the server. Then, they perform *regularization on logits* from the *same data* feature on *different classification heads.* \n- **Those methods make assumptions in feature sharing, but FedLGD does NOT** To enable the comparison of logits for knowledge distillation, those methods require sharing features from the local feature extractor with the server. Differently, FedLGD does not require any data feature sharing and we only share gradients following classical FL during the FL model updating stage. We consider that the sharing feature and additional knowledge may violate the intention of improving privacy in federated virtual learning.\n\nGiven the fundamental differences in the approaches and assumptions on information sharing, we believe our methods and those methods are not directly comparable. In fact, FedLGD and the model knowledge distillation-based methods can be combined as orthogonal strategies to address heterogeneity issues in FL. For example, the distilled global virtual data can serve as an ideal candidate to perform knowledge distillation as it roughly captures global data distribution. Thus, it can eliminate the requirement of sharing real data features or using public datasets. We hope our approach can shed light on leveraging virtual data in FL and inspire future work on the interesting exploration of combining our strategy with others.\n\nWe thanks the reviewer for pointing us to the interesting papers, and we have added them in our related work.\n\n[1] \"FedICT: Federated Multi-task Distillation for Multi-access Edge Computing.\" IEEE Transactions on Parallel and Distributed Systems (2023).\n\n[2] \"Group knowledge transfer: Federated learning of large cnns at the edge.\" Advances in Neural Information Processing Systems 33 (2020): 14068-14080.\n\n[3] \"Exploring the distributed knowledge congruence in proxy-data-free federated distillation.\" arXiv preprint arXiv:2204.07028 (2022)."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission541/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700353329033,
                "cdate": 1700353329033,
                "tmdate": 1700353556804,
                "mdate": 1700353556804,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xtnzdduQX3",
                "forum": "fGXyvmWpw6",
                "replyto": "vsvqitsm00",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission541/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission541/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Apologize for the wrong citation"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the careful review and for pointing out this subtle error. We incorrectly referred FedProx to [2] in our Section 4.1 and 5 due to the similarity of the reference codes used by [1] and [2] (i.e., li2020a and li2020b). It should be [1] instead. We really appreciate this comment, and we have corrected it in our revision.\n\n[1] \"Federated optimization in heterogeneous networks.\" Proceedings of Machine learning and systems 2 (2020): 429-450.\n\n[2] \"On the convergence of fedavg on non-iid data.\" arXiv preprint arXiv:1907.02189 (2019)."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission541/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700353359622,
                "cdate": 1700353359622,
                "tmdate": 1700353359622,
                "mdate": 1700353359622,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "iTiVDPKMJd",
                "forum": "fGXyvmWpw6",
                "replyto": "vsvqitsm00",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission541/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission541/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Explanation of the selection of our baseline methods"
                    },
                    "comment": {
                        "value": "We thank the reviewer for pointing out the interesting FL work, FedGen [2]. We did not include FedGen (and other knowledge distillation-based methods) in our comparison works is that we consider knowledge distillation and data distillation two orthogonal directions to solve data heterogeneity issues. \n\nIt is worth noting the training strategy of federated virtual learning is quite novel and under-explored. Thus most of the existing FL methods are not suitable for making direct comparisons. In fact, we included VHL [1], which shares most similar thoughts as FedLGD by using virtual data as regularization in local training. We show that using the virtual data generated by federated gradient matching can better handle the heterogeneous data scenario. \n\nFollowing the suggestion, we added the work to the related work in our revision and justified the orthogonality between knowledge distillation-based FL methods and FedLGD. To address the reviewer\u2019s question, we report the results on FedGen [2] on DIGITS, CIFAR10C(client ratio=1), and RETINA in the following table. We also include FedGen in the related work in our revision.\n.\n\n| Avg. Acc.       | FedGen| VHL     | FedLGD |\n|-------------------|------------|-----------|---------------|\n| DIGITS          | 66.9      | 86.5      | **86.7**    |\n| CIFAR10C     | 39.6      | 55.2      | **57.4**    |\n| RETINA         | 82.1      | 78.6      | **85.1**    |\n\nObserve that FedLGD consistently outperforms other methods. We conjecture the reason FedGen is not performing well for two reasons: It requires a larger number of training data to obtain a more reasonable global GAN. 2. It requires a larger feature space to better regularize local training with knowledge distillation (as shown in RETINA experiments, where it can use a large feature space, thus resulting in better performance). \n\n[1] Tang Z, Zhang Y, Shi S, He X, Han B, Chu X. Virtual homogeneity learning: Defending against data heterogeneity in federated learning. InInternational Conference on Machine Learning 2022 Jun 28 (pp. 21111-21132). PMLR.\n\n[2] Zhu Z, Hong J, Zhou J. Data-free knowledge distillation for heterogeneous federated learning. InInternational conference on machine learning 2021 Jul 1 (pp. 12878-12889). PMLR."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission541/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700353495081,
                "cdate": 1700353495081,
                "tmdate": 1700353495081,
                "mdate": 1700353495081,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "n2dFLVzaMf",
                "forum": "fGXyvmWpw6",
                "replyto": "2DgcMSinva",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission541/Reviewer_8zES"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission541/Reviewer_8zES"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your careful rebuttal. I am impressed by your idea of using distilled global virtual data as the candidate for knowledge distillation. Personally, I suggest clarifying the difference between knowledge distillation and dataset distillation in your paper, as the former technique combined with federated learning has accumulated rich literature and a large number of readers. Anyway, I have increased my rating in my revised official comments."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission541/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700372612835,
                "cdate": 1700372612835,
                "tmdate": 1700372612835,
                "mdate": 1700372612835,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jFTRUHmxX7",
                "forum": "fGXyvmWpw6",
                "replyto": "vsvqitsm00",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission541/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission541/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your feedback!"
                    },
                    "comment": {
                        "value": "Dear Reviewer 8zES,\n\nThank you for the recognition of our work and rebuttal. Your comments are valuable for us to refine our manuscript. We will highlight the differences between knowledge distillation and dataset distillation in our related work session in our revision.\n\nBest Regards,\n\nFedLGD authors"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission541/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700438735146,
                "cdate": 1700438735146,
                "tmdate": 1700438748993,
                "mdate": 1700438748993,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "AbtRGI20Ig",
            "forum": "fGXyvmWpw6",
            "replyto": "fGXyvmWpw6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission541/Reviewer_PXsJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission541/Reviewer_PXsJ"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a federated learning method called FedLGD that uses local and global dataset distillation to handle data heterogeneity.FedLGD uses an iterative distillation process to generate local and global virtual datasets that mitigate data heterogeneity and improve efficiency in federated learning. The local-global distillation and feature regularization are key components that help FedLGD achieve strong performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The discover of using dataset distillation can amplify the statistical distances is interesting.\n  2. Achieves state-of-the-art results on benchmark datasets with domain shifts, outperforming existing federated learning algorithms."
                },
                "weaknesses": {
                    "value": "1. t-SNE figures are not represented as vectors.\n\n2. Sharing gradients from clients to the server for a global virtual data update may pose security risks. Some attacks could potentially reconstruct raw data using gradient information, similar to the risks associated with Deep Gradient Leakage. Why sharing averaged gradients is safe?\n\n3. What is the rationale behind clients requiring local virtual data instead of training directly on their local private data?\n\n4. Could you clarify why this method has not been compared to other FL methods utilizing dataset distillation?"
                },
                "questions": {
                    "value": "See Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission541/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission541/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission541/Reviewer_PXsJ"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission541/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698772299371,
            "cdate": 1698772299371,
            "tmdate": 1699635981153,
            "mdate": 1699635981153,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "FQTWPN3aij",
                "forum": "fGXyvmWpw6",
                "replyto": "AbtRGI20Ig",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission541/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission541/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Justification and explanation of t-SNE plot"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the clarification comment. We chose to show the 2-D plot for more intuitive visualization. In our original submission, we also reported the **statistical distances** between the two example datasets from two clients to support the heterogeneity statement. Specifically, in **the second paragraph of our Introduction**  we stated \u201cQuantitatively, we found that using dataset distillation can amplify the statistical distances between the two datasets, with Wasswestein Distance and Maximum Mean Discrepancy (MMD) (Gretton et al., 2012) both increasing by around 40%.\u201d We believe that the statistical distance and t-SNE plots can provide a reasonable evidence to our finding."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission541/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700352822015,
                "cdate": 1700352822015,
                "tmdate": 1700533408260,
                "mdate": 1700533408260,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LPGIDTuYzc",
                "forum": "fGXyvmWpw6",
                "replyto": "AbtRGI20Ig",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission541/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission541/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Justification of the privacy preservation in FedLGD"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the constructive comment. Yes, privacy is indeed an important concern in federated learning. Thus, we discuss the privacy guarantee of FedLGD in the **last paragraph in Section 3** and we highlight the statements as follows: \n\n- FedLGD does not share additional client information compared to baseline methods such as FedAvg [1].\n- Notably, the gradients we share in FedLGD are w.r.t. Local virtual data distilled by Distribution Matching [2], which has been shown to defend against Membership Inference Attack and Reconstruction Attack [3]. We also show our empirical defense results against Membership Inference Attack in Appendix E.6. \n- Privacy preservation can be further improved by employing differential privacy [4] in dataset distillation, but this is beyond the main focus of our work.\n\n[1] McMahan B, Moore E, Ramage D, Hampson S, y Arcas BA. Communication-efficient learning of deep networks from decentralized data. InArtificial intelligence and statistics 2017 Apr 10 (pp. 1273-1282). PMLR.\n\n[2] Zhao B, Bilen H. Dataset condensation with distribution matching. InProceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision 2023 (pp. 6514-6523).\n\n[3] Dong T, Zhao B, Lyu L. Privacy for free: How does dataset condensation help privacy?. InInternational Conference on Machine Learning 2022 Jun 28 (pp. 5378-5396). PMLR.\n\n[4] Abadi M, Chu A, Goodfellow I, McMahan HB, Mironov I, Talwar K, Zhang L. Deep learning with differential privacy. InProceedings of the 2016 ACM SIGSAC conference on computer and communications security 2016 Oct 24 (pp. 308-318)."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission541/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700352915125,
                "cdate": 1700352915125,
                "tmdate": 1700352915125,
                "mdate": 1700352915125,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YSJLe3q335",
                "forum": "fGXyvmWpw6",
                "replyto": "AbtRGI20Ig",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission541/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission541/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rationale behind using local virtual data instead of local raw data for FL"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the clarification question. As we stated **in our abstract**, training with distilled virtual data can provide higher training efficiency and alleviate the problem of inference attacks, as demonstrated in centralized settings [1]. \n\nBuilding on this, we explore its usage in FL (named federated virtual learning - FVL), because privacy and efficiency (computation cost) are common concerns in FL, together with its obvious assistance in synchronization.\n\nSpecifically, the benefits are:\n\n- **Synchronization**: Training with a small number of representative virtual data can facilitate local training, which can improve FL synchronization.\n- **Computation Cost**: As shown in Appendix D.3 and Table 6, we empirically show that training with virtual data requires a much lighter computation cost. (We\u2019ve corrected the typo in Appendix D.3 in our revision.)\n- **Privacy**: As reported in [1], training with virtual data can protect the model from privacy attacks such as Inversion Attack [2] and Membership Inference Attack (MIA) [3]. In addition, as stated in **the last paragraph of Section 3**, \u201cwe consider averaged gradients w.r.t. local virtual data and the method potentially defends inference attacks better (Appendix E.6).\u201d\n\nThe core of this work aims to resolve the heterogeneity issue associated with distillation-based virtual data in FVL. This will make the use of virtual data more feasible in FL and lead to improved synchronization, efficiency, and privacy while approaching the utility of real data.\n\n[1] Dong T, Zhao B, Lyu L. Privacy for free: How does dataset condensation help privacy?. InInternational Conference on Machine Learning 2022 Jun 28 (pp. 5378-5396). PMLR.\n\n[2] Geiping J, Bauermeister H, Dr\u00f6ge H, Moeller M. Inverting gradients-how easy is it to break privacy in federated learning?. Advances in Neural Information Processing Systems. 2020;33:16937-47.\n\n[3] Shokri R, Stronati M, Song C, Shmatikov V. Membership inference attacks against machine learning models. In2017 IEEE symposium on security and privacy (SP) 2017 May 22 (pp. 3-18). IEEE."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission541/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700353010768,
                "cdate": 1700353010768,
                "tmdate": 1700353010768,
                "mdate": 1700353010768,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "S7Etv9Kn4T",
                "forum": "fGXyvmWpw6",
                "replyto": "AbtRGI20Ig",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission541/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission541/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Explanation of not comparing to other FL+data distillation works"
                    },
                    "comment": {
                        "value": "We thank the reviewer for raising this question. As we stated in the related work (Sec. 2.2), to our best knowledge, the current FL+Data distillation works typically share local distilled data to server [1,2,3], which we consider a non-practical and more privacy-sensitive operation. Furthermore, some methods (e.g., [1]) need to use real data as initialization to obtain good performance.\n\nOn the contrary, due to privacy concerns, we choose to train the model locally and share the gradients w.r.t. Local virtual data,  which is the default operation in classical FL, such as FedAvg. Therefore, the other FL+data distillation settings are significantly different from ours regarding the information shared and data leakage. We discussed these studies in our related work but believed it was unfair to make a direct comparison, given their special request for information sharing. \n\n[1] Xiong Y, Wang R, Cheng M, Yu F, Hsieh CJ. Feddm: Iterative distribution matching for communication-efficient federated learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition 2023 (pp. 16323-16332).\n\n[2] Goetz J, Tewari A. Federated learning via synthetic data. arXiv preprint arXiv:2008.04489. 2020 Aug 11.\n\n[3] Hu S, Goetz J, Malik K, Zhan H, Liu Z, Liu Y. FedSynth: Gradient Compression via Synthetic Data in Federated Learning. InWorkshop on Federated Learning: Recent Advances and New Challenges (in Conjunction with NeurIPS 2022) 2022 Oct 21."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission541/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700353065143,
                "cdate": 1700353065143,
                "tmdate": 1700353065143,
                "mdate": 1700353065143,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bfNwHLLFQd",
                "forum": "fGXyvmWpw6",
                "replyto": "AbtRGI20Ig",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission541/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission541/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your valuable comments and we sincerely look forward to your feedback for rebuttal"
                    },
                    "comment": {
                        "value": "Dear Reviewer PXsJ,\n\nAs the rebuttal deadline is approaching, we would like to know if our rebuttals have addressed your concerns and questions. Also, we have tried our best to reflect your brilliant feedback to our revision. We appreciate the opportunity to discussing during this stage and are delighted to address your further question if there is any. If you are satisfied with our response and revision, we would be grateful if you could kindly re-consider the rating for FedLGD.\n\nBest Regards,\n\nFedLGD authors"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission541/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700611955975,
                "cdate": 1700611955975,
                "tmdate": 1700611982080,
                "mdate": 1700611982080,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]