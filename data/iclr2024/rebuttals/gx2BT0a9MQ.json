[
    {
        "title": "ZeRO++: Extremely Efficient Collective Communication for Large Model Training"
    },
    {
        "review": {
            "id": "JWdiIZR7I6",
            "forum": "gx2BT0a9MQ",
            "replyto": "gx2BT0a9MQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2830/Reviewer_QbwT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2830/Reviewer_QbwT"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes zero++, which contains 3 communication reduction techniques (low-precision parameter all-gather, data remapping, and low-precision gradient averaging), in order to reduce the communication overhead and acceleration the training."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper proposes zero++, which contains 3 communication reduction techniques (low-precision parameter all-gather, data remapping, and low-precision gradient averaging) with sufficient details provided in the appendix.\n\n2. Additional to the system design, an optimized implementation is also proposed.\n\n3. The experiments show good performance on real-world training tasks."
                },
                "weaknesses": {
                    "value": "1. Some part of the paper is a little bit vague and confusing, please refer to the \"Questions\" below for more details.\n\n2. The main reason I'm holding back from a better score is that zero++ seems to have huge overlap with QSDP (Markov, I., Vladu, A., Guo, Q., & Alistarh, D. (2023). Quantized Distributed Training of Large Models with Convergence Guarantees. ICML 2023). I understand that QSDP was officially published on ICML not very long ago and we reviewers should not take the release date on arxiv as a reason to reject this paper. However, this is a delicate situation for me and I guess we will need the AC to judge. \nFor now, I still need the authors to give a detailed comparison between QSDP and zero++, which will help a lot for my paper review."
                },
                "questions": {
                    "value": "1. Does the gradient and parameter communication compression use the same quantizer/quantization algorithms? What are the specific quantization algorithm used in the experiments?\n\n2. In experiments, some details are missing. For example, in Section 4.3 \"Pretraining\" (or Figure 9), it is mentioned that 8-bit or 6-bit quantization is used. However, it is unknown whether is 8-bit for both parameters and gradients? or 8-bit for parameters and fp16 for gradients? or fp16 for parameters and 8-bit for gradients? I strongly recommend the authors to specify what (parameter or gradient) is quantized wherever quantization/precision configuration is mentioned.\n\n3. In the experiments,  Section 4.3 \"Fine-tuning\" (or Table 2), it is mentioned that \"with ZeRO++ using FP6, INT4, and INT2 precision for qwZ and INT-8 for qgZ ...\" I wonder why not also conduct some experiments of lower precision (int4, int2) on qgZ? Also, is there any ablation experiments on other configurations, such as int4 qwZ + fp16 qgZ?\n\nBTW, there is \"FP6\" in Section 4.3 (see the sentence I quoted above), is it a typo and the authors actually mean \"FP16\" here?\n\n4. What's the difference between Zero++ and QSDP (in system design), except for the hierarchical partitioning in Zero++ (hpZ)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2830/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698716311500,
            "cdate": 1698716311500,
            "tmdate": 1699636226691,
            "mdate": 1699636226691,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5tu8sjjTPs",
                "forum": "gx2BT0a9MQ",
                "replyto": "JWdiIZR7I6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2830/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2830/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your insightful feedback. We have carefully considered your comments and addressed them in the following response.\n\n1.\tThe main reason I'm holding back from a better score is that zero++ seems to have huge overlap with QSDP (Markov, I., Vladu, A., Guo, Q., & Alistarh, D. (2023). Quantized Distributed Training of Large Models with Convergence Guarantees. ICML 2023). I understand that QSDP was officially published on ICML not very long ago and we reviewers should not take the release date on arxiv as a reason to reject this paper. However, this is a delicate situation for me and I guess we will need the AC to judge. For now, I still need the authors to give a detailed comparison between QSDP and zero++, which will help a lot for my paper review. What's the difference between Zero++ and QSDP (in system design), except for the hierarchical partitioning in Zero++ (hpZ)?\n\nA: The common part of QSDP and ZeRO++ lies in both using block-based quantization for weights. However, there are three major differences between these two approaches.\n* i) concurrency. Although using block-based quantization will help mitigate quantization error. The cost of doing it will cancel out most of the benefit from communication reduction in terms of wall-clock time. ZeRO++ hide quantization wall-clock time by overlapping quantization with subsequent communication. More specifically, ZeRO++ put weights into chunks and then overlapping the communication of current chunk with quantization of next chunk to hide the quantization cost (as well as more optimizations discussed in Appendix C). This is directly reflected in the evaluation: for 100Gbps bandwidth, ZeRO++ can achieve 130% speedup over the baseline whereas QSDP achieves 15% speedup. We also extended our evaluation to 10Gbps scenarios where ZeRO++ achieves 300% speedup.\n* ii) gradients. We observe that performing reduce-scatter on quantized gradients, as QSDP does, will not average out the quantization error but amplify them, resulting in non-trivial convergence gap. Thus ZeRO++ proposes a novel all2all primitive to make sure communicating quantized gradients will not amplify quantization error. The convergence eval shows the gap, for 1.3B model, QSDP shows 0.34 gap in perplexity when both weights and gradients are quantized to 8 bits while ZeRO++ shows 0.005 gap when the weights are quantized to 6 bits and gradients are quantized to 4 bits. On the performance side, we also overlapped intranode all2all with internode all2all with proper synchronization to further reduce end-2-end latency, which is detailed in appendix C.1.2\n* iii) optimized placement of weights. As reviewer mentioned, ZeRO++ equips with hpZ to completely get rid of all inter-node communication in backward to further boost system performance.\n\nWe will extend our related work part to add the discussions to QSDP.\n\n2.\tDoes the gradient and parameter communication compression use the same quantizer/quantization algorithms? What are the specific quantization algorithm used in the experiments?\n\nA: We use block-based quantization for both weights and gradients. The details of it can be found in Appendix B.2. Simply put, we pre-scale the tensors by small chunks before casting them to lower precision to mitigate the difference in numerical ranges and granularities.\n\n3.\tIn experiments, some details are missing. For example, in Section 4.3 \"Pretraining\" (or Figure 9), it is mentioned that 8-bit or 6-bit quantization is used. However, it is unknown whether is 8-bit for both parameters and gradients? or 8-bit for parameters and fp16 for gradients? or fp16 for parameters and 8-bit for gradients? I strongly recommend the authors to specify what (parameter or gradient) is quantized wherever quantization/precision configuration is mentioned.\n\nA: For the experiments in the paper, we use INT4 as the quantization format for gradients. The weights are quantized into INT8 in most testing scenarios. We also include 6, 4, 2 bits for other scenarios where we state in text to showcase how even lower-bit affects the convergence. For all the evaluation, we enable all three optimization techniques (qwZ, qgZ, hpZ) so both the gradients and the weights are quantized except for the ablation study. We will update the text in our camera-ready version to clarify this.\n\n(To be continued)"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2830/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700260041661,
                "cdate": 1700260041661,
                "tmdate": 1700261947808,
                "mdate": 1700261947808,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "tdFFV2JreR",
            "forum": "gx2BT0a9MQ",
            "replyto": "gx2BT0a9MQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2830/Reviewer_ZpeK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2830/Reviewer_ZpeK"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed ZeRO++, which includes three main techniques to accelerate communication in distributed training of foundation models: i) low-precision AllGather for parameter collection, ii) hierarchy communication optimization, and ii) low-precision gradient aggregation. Empirical studies was conducted to verify the performance of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Optimizing the communication in ZeRO or more generally speaking, optimizer parallelism is an important problem to scale out / speed up foundation model training.\n\n- The proposed design and implementation are reasonable. \n\n- The evaluation is conducted on a large production-level GPU cluster to evaluate models at the state-of-the-art scales."
                },
                "weaknesses": {
                    "value": "- My main concern about the paper is lack of novelty as a research paper instead of a software report, concretely, w.r.t each technique contribution, I have the following comments:\n  - Low-precision gradient averaging: such a technique has been studied for decades in standard data parallel training; integrating them in an existing system probably cannot be viewed as a novelty in a top ML volume (i.e., ICLR).    \n  - Organizing the communication as a hierarchy topology is not a new approach; a similar implementation was introduced in PyTorch-FSDP about six months ago.\n  - Distributing the model weights with lower precision is an interesting point. However, there is a lack of concrete theoretical analysis about this approach, i.e., now the model parameter includes some quantization error; how would this influence the convergence behavior?"
                },
                "questions": {
                    "value": "Would it be possible to provide some theoretical analysis about the convergence given the quantized communication of the weights distribution?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2830/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698831561225,
            "cdate": 1698831561225,
            "tmdate": 1699636226625,
            "mdate": 1699636226625,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5mCXmiyYt7",
                "forum": "gx2BT0a9MQ",
                "replyto": "tdFFV2JreR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2830/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2830/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your insightful feedback. We have carefully considered your comments and addressed them in the following response.\n\n1.\tLow-precision gradient averaging: such a technique has been studied for decades in standard data parallel training; integrating them in an existing system probably cannot be viewed as a novelty in a top ML volume (i.e., ICLR).\n\nA: The novelty of ZeRO++ is not to use low-precision gradient but lies on using low-precision gradient without impacting the convergence and system throughput. Because even upon using the state-of-the-art quantization to mitigate the quantization error, it will still cause a significant gap in terms of convergence as well as throughput (due to the time spent on quantization). That is why we present a novel all-to-all communication primitive, and a full set of CUDA kernels, and a carefully designed system to i) maintain numeric precision even during communication in a quantized format; ii) minimize the quantization cost and iii) boost system throughput by doing intra-node and inter-node communication concurrently and hide quantization cost by doing communication and quantization concurrently.\n\n2.\tOrganizing the communication as a hierarchy topology is not a new approach; a similar implementation was introduced in PyTorch-FSDP about six months ago.\n\nA: Our approach, hpZ, is similar to FSDP/HSDP in that they both trade off memory for communication. However, hpZ is different in that it creates hierarchical partitioning only for the parameters instead of all model states, resulting in much lower memory overhead. This allows hpZ (and ZeRO++) to support larger models than FSDP can. Appendix D of our manuscript shows our comparison with MiCS, which has the same design idea as FSDP. Our evaluation comparing MiCS with hpZ shows that hpZ can support an 18 billion parameter model, whereas MiCS reports OOM at this scale (see Table 7 of our manuscript).\n\n3.\tDistributing the model weights with lower precision is an interesting point. Distributing the model weights with lower precision is an interesting point. However, there is a lack of concrete theoretical analysis about this approach, i.e., now the model parameter includes some quantization error; how would this influence the convergence behavior? Would it be possible to provide some theoretical analysis about the convergence given the quantized communication of the weights distribution?\n\nA: Regarding theoretical convergence, one notable aspect is the unique design of ZeRO. In the ZeRO paradigm, there are two kinds of weights: the temporary weights, which are all-gathered from all GPUs in order to do forward/backward on a particular model layer and then discarded; and the local weight shard, which is used to populate the temporary weights, are permanent and will be updated at each optimizer step. The ZeRO++ quantization only happens when **we communicate the temporary weights**. Thus, the local shard of the weights will be free of the quantization error. For the gradients that are computed from the quantized temporary weights, there will be minor errors, yet fruitful research such as [1, 2, 3] prove that minor disturbance to gradients will not impact the overall convergence (the convergence rate follows O(1/sqrt{T}) for T iterations), which we have also empirically shown. An empirical evaluation can be found in Table 2 where the convergence gap increases as the quantization goes from 6 bits to 2 bits. We also added the above discussion to the appendix to better discuss ZeRO++\u2019s theoretical convergence. Thank you for the constructive suggestions.\n>[1] Gorbunov, Eduard, Filip Hanzely, and Peter Richt\u00e1rik. \"A unified theory of SGD: Variance reduction, sampling, quantization and coordinate descent.\" AISTATS 2020 \n>\n>[2] Ramezani-Kebrya, Ali, et al. \"NUQSGD: Provably communication-efficient data-parallel SGD via nonuniform quantization.\" The Journal of Machine Learning Research 22.1 (2021): 5074-5116.\n>\n>[3] Alistarh, Dan, et al. \"QSGD: Communication-efficient SGD via gradient quantization and encoding.\" Advances in neural information processing systems 30 (2017).\n\nWe welcome any further questions or points of discussion."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2830/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700259934210,
                "cdate": 1700259934210,
                "tmdate": 1700259934210,
                "mdate": 1700259934210,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "PZiSSdYETJ",
            "forum": "gx2BT0a9MQ",
            "replyto": "gx2BT0a9MQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2830/Reviewer_V3tv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2830/Reviewer_V3tv"
            ],
            "content": {
                "summary": {
                    "value": "This work, ZeRO++, proposes a collective communication optimization for ZeRO3 of DeepSpeed library. Three elements are introduced: 1) quantized AllGather of weight, 2) hierarchical partition, and 3) quantized ReduceScatter of gradient with AlltoAll. When combined together, ZeRO++ reduces communication volume and network hops to mitigate the communication bottleneck of ZeRO3 training for large models."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+. Replaced Ring-ReduceScatter with AlltoAll for lower latency and less quantization/dequantization times\n\n+. Optimized intra-node and inter-node AlltoAll"
                },
                "weaknesses": {
                    "value": "-. **Limited contribution**\n> qwZ quantizes FP16 weights to lower precision right during the all-gather, and dequantizes them back to FP16 on the receiver side, and then conducts layer computation.\n\n* Why cast FP16 weight into INT4 is a novel contribution? Just use \"torch.Tensor.to()\"?\n* What if model is being trained FP8 on the modern H100? so the communication benefit will be halved?\n* Will cast FP16 into INT4 occupy 25%+ extra GPU memory (assuming zero fragmentation)?\n\n> hpZ removes cross node traffic in backward all-gather by holding secondary weight partitions in on device memory.\n\n* How is this scheme different from HSDP of PyTorch (Intra-Machine FSDP to shard weight + inter-Machine DDP to replicate weight)? HSDP was published in VLDB'2023.\n\n> qgZ is a novel quantized reduce-scatter algorithm based on all-to-all collectives that enables a 4xcommunication volume reduction of gradient reduce-scatter by replacing FP16 with INT4.\n\n* Same question as weight quantization\n\n-. **Limited Convergence Analysis**\n\n> Pretrain: We analyze the pretraining of GPT-350M and GPT-13B models on the Pile dataset\n\nHow does this work apply to different datasets for convergence behavior, considering extreme quantization?\n\nBesides Figure 9, does this work provide accurate numbers for loss diffs for pertaining?\n\n> Fine-tuning. Table 2\n\nIt seems this work always worse the perplexity compared with baseline."
                },
                "questions": {
                    "value": "*. Does this work provide open-source code?\n\n*. Is this work ever used in production for industrial models?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2830/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2830/Reviewer_V3tv",
                        "ICLR.cc/2024/Conference/Submission2830/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2830/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698919807772,
            "cdate": 1698919807772,
            "tmdate": 1700704225987,
            "mdate": 1700704225987,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "tYTl6HMIdA",
                "forum": "gx2BT0a9MQ",
                "replyto": "PZiSSdYETJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2830/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2830/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your constructive feedback. We have carefully considered your comments and addressed them in the following response.\n\n1.\tWhy cast FP16 weight/gradient into INT4 is a novel contribution? Just use \"torch.Tensor.to()\"?\n\nA: Simply casting floating-point weights/gradients to integers will cause the training to crash from the very beginning due to the significant difference in numeric granularity (e.g. the usual granularity for fp16 is $10^{-6}$ vs. $1$ in integer format). Even upon using the state-of-the-art quantization approach to mitigate the gap, it will still cause a significant gap in terms of convergence as well as throughput (due to the computation time spent on quantization and dequantization). That is why we present a novel all-to-all communication primitive, and a full set of CUDA kernels, and a carefully designed system to \n* i) maintain numeric precision even during communication in a quantized format; \n* ii) minimize the quantization cost by custom CUDA kernels;\n* iii) boost system throughput by doing intra-node and inter-node communication concurrently and hide quantization cost by doing communication and quantization concurrently.\n\n2.\tWhat if model is being trained FP8 on the modern H100? so the communication benefit will be halved?\n\nA: ZeRO++ is proposed to tackle the challenge that communication has taken a gradually larger portion of running time as scalability increases. FP8 training will reduce both the computation time and communication time thus the challenge of communication ratio remains, which makes ZeRO++ a valid solution. By hpZ, we are eliminating all backward weight intra-node communication so its reduction will not be impacted by the weight format. For qgZ and qwZ, its reduction depends on the quantization format users prefer, which is why we show an extensive evaluation with different quantization formats.\n\n3.\tWill cast FP16 into INT4 occupy 25%+ extra GPU memory (assuming zero fragmentation)?\n\nA: The system implementation of ZeRO++ makes sure quantization&communication happens by chunks. That means the memory overhead is at most two chunks (one for comm and one for quantization) which is trivial compared with the overall model size. The chunk size is adjustable so the user can even squeeze the memory overhead to almost nothing (though we believe there is hardly such need).\n\n4.\tHow is this hpZ scheme different from HSDP of PyTorch (Intra-Machine FSDP to shard weight + inter-Machine DDP to replicate weight)? HSDP was published in VLDB'2023.\n\nA: Our approach, hpZ, is similar to HSDP in that they both trade off memory for communication. However, hpZ is different in that it creates hierarchical partitioning only for the parameters instead of all model states, resulting in lower memory overhead. This allows hpZ (and ZeRO++) to support larger models than HSDP can. HSDP, by design, supports relatively medium-sized models, as noted by the authors. Appendix D of our manuscript shows our comparison with MiCS (Zhang et.al, MiCS: Near-linear Scaling for Training Gigantic Model on Public Cloud), which has the same design idea as HSDP. Our evaluation comparing MiCS with hpZ shows that hpZ can support an 18 billion parameter model, whereas MiCS reports out-of-memory (OOM) at this scale (see Table 7 of our manuscript).\n\n5.\tHow does this work apply to different datasets for convergence behavior, considering extreme quantization?\n\nA: In our evaluation, we show a wide range of datasets including Pile, Dahoas/rm-static, Dahoas/full-hh-rlhf, Dahoas/synthetic-instruct, gptj-pairwise, yitingxie/rlhf-reward-datasets. We believe these extensive convergence results are enough to demonstrate the good convergence for ZeRO++ for general applications.\n\n6.\tBesides Figure 9, does this work provide accurate numbers for loss diffs for pertaining  ?\n\nA:  We have updated our manuscript to include Table 5 and 6, providing the precise loss differences to complement Figure 9 and Table 2 for both pretraining and finetuning. This addition will ensure our results are both transparent and comprehensive. Thank you for the suggestion.\n\n(To be continued)"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2830/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700259745607,
                "cdate": 1700259745607,
                "tmdate": 1700259745607,
                "mdate": 1700259745607,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Y8bIzIjbdv",
                "forum": "gx2BT0a9MQ",
                "replyto": "bbuxbScW1B",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2830/Reviewer_V3tv"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2830/Reviewer_V3tv"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the rebuttal and clarification. The rebuttal has addressed a part of my concerns. I raised my score."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2830/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700704410113,
                "cdate": 1700704410113,
                "tmdate": 1700704410113,
                "mdate": 1700704410113,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "yFoB1KJZxE",
            "forum": "gx2BT0a9MQ",
            "replyto": "gx2BT0a9MQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2830/Reviewer_TkZG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2830/Reviewer_TkZG"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes an extension to the existing memory optimal training technique for LLMs, *ZeRO*. The proposed enhancements address the expensive communication problems of the state-of-the-art method, especially when trained on low bandwidth networks. ZeRO++ with the new quantization enabled communication strategies achieves reasonably good performance over ZeRO while keeping the performance of LLMs intact."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper is well written, detailed and easy to understand for both DL and HPC communities. (assuming readers have some basic understanding of both the worlds).\n- Has a good coverage of the existing literature and positions the paper with-in the body of work.\n- The problem is probably impactful for low-resource devices such as low band-width networks."
                },
                "weaknesses": {
                    "value": "- The paper shows the effectiveness of the proposed method on powerful DGX GPUs with infiniBand interconnects. Noway, we can consider this interconnect as low-bandwidth. \n- Intra-node weight storage sounds interesting and empirically showed to work for upto 30B model fine-tuning and 13B model pre-training. Wonder, if this can be scaled if there is any ablation on this dimension?"
                },
                "questions": {
                    "value": "- One simple experiment (even on a network of RasberryPi machines) with low-badwidth networks will obviously greatly strengthens the paper claims/contributions. This can be fine-tuning does not even have to be pre-training. \n- A feasibility study on the scalability of the method, especially because of the GPU memory tradeoff within the node, would help.\n- Upon some or all of the above concerns being addressed, would be happy to raise the score. \n- Also, wonder, if there is any open implementations for this complex custom communication MPI primitives?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2830/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2830/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2830/Reviewer_TkZG"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2830/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699740149039,
            "cdate": 1699740149039,
            "tmdate": 1699740149039,
            "mdate": 1699740149039,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "SqMzRDN1KO",
                "forum": "gx2BT0a9MQ",
                "replyto": "yFoB1KJZxE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2830/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2830/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your insightful feedback. We have carefully considered your comments and addressed them in the following response.\n\n1.\tThe paper shows the effectiveness of the proposed method on powerful DGX GPUs with infiniBand interconnects. Noway, we can consider this interconnect as low-bandwidth. One simple experiment (even on a network of RasberryPi machines) with low-badwidth networks will obviously greatly strengthens the paper claims/contributions. This can be fine-tuning does not even have to be pre-training.\n\nA: We define \"low-bandwidth\" in our context as 100Gbps to simulate the speed of Ethernet connections in typical cloud services and HPC clusters. Yet we agree with the reviewer that an evaluation for an even lower bandwidth cluster is needed. Thus, we extended our evaluation to an entry-level cluster with 10Gbps Ethernet connections and demonstrated that ZeRO++ can boost training throughput by 4x in such scenarios (see Appendix D for full results). This aligns with our previous evaluation, showing that ZeRO++'s speedup increases significantly as the inter-node bandwidth decreases.\n\n2.\tIntra-node weight storage sounds interesting and empirically showed to work for up to 30B model fine-tuning and 13B model pre-training. Wonder, if this can be scaled if there is any ablation on this dimension? A feasibility study on the scalability of the method, especially because of the GPU memory tradeoff within the node, would help.\n\nA: An analysis of the scalability of hpZ can be found in Appendix B.2 of our paper, with an example of a 100B model running on 1024 GPUs. In that section, we also provide mathematical models to calculate the memory usage for the given model size and number of GPUs.\n\n3.\tAlso, wonder, if there is any open implementations for this complex custom communication MPI primitives?\n\nA: ZeRO++ has been made publicly available to the open-source community and has seen adoption in both research and industrial applications. Unfortunately, we are unable to share the link due to double-blind policy.\n\nWe welcome any further questions or points of discussion."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2830/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700259371448,
                "cdate": 1700259371448,
                "tmdate": 1700261778046,
                "mdate": 1700261778046,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "XF18q9U6NY",
            "forum": "gx2BT0a9MQ",
            "replyto": "gx2BT0a9MQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2830/Reviewer_kJiX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2830/Reviewer_kJiX"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes ZeRO++, a communication-efficient extension of the commonly used memory-efficient training framework for LLMs, ZeRO. The proposed techniques include three specific techniques to reduce communication costs, improve the throughput and accelerate the training process."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper proposes quantized weight communication, hierarchical partitioning, and quantized gradient communication techniques to reduce the communication costs and achieves efficient implementation of overlap computing and communication. The parallelism of training is the focus.\n\n2. The experiments presented are quite convincing, including large-scale language model training and fine-tuning on a large GPU cluster. The convergence accuracy and throughput reflect great improvements.\n\n3.\tTraining on low-bandwidth clusters is an important question and this paper solves it to some degree."
                },
                "weaknesses": {
                    "value": "1.\tIn my opinion, the main insufficient of this paper is short of novelty. In fact, this paper is more like a technical report instead of an academic paper. The techniques of quantization and organizing the communication into a new topology are not new ideas. However, considering the overlap realization and trade-off between memory and communication are difficult and expensive, this paper is still significant for LLM community.\n\n2.\tAnother problem is that this paper is lack of specific introduction to used quantization techniques and corresponding theoretical analysis. For example, how the quantization error affects the convergence rate and whether the error-feedback technique works."
                },
                "questions": {
                    "value": "1.\tWhat is the specific quantization methods in qwZ and qgZ?\n2.\tHow to choose the quantization ratio during training and fine-tuning? I am interested in the inner effect of ratio on the convergence of different tasks.\n3.\tIn the real low-bandwidth environment, does the trade-off between memory and communication still work without NVLink?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2830/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2830/Reviewer_kJiX",
                        "ICLR.cc/2024/Conference/Submission2830/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2830/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699913636385,
            "cdate": 1699913636385,
            "tmdate": 1699913670770,
            "mdate": 1699913670770,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "RfbbPyoYE9",
                "forum": "gx2BT0a9MQ",
                "replyto": "XF18q9U6NY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2830/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2830/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your insightful feedback. We have carefully considered your comments and addressed them in the following response.\n\n1.\tIn my opinion, the main insufficient of this paper is short of novelty. In fact, this paper is more like a technical report instead of an academic paper. The techniques of quantization and organizing the communication into a new topology are not new ideas. However, considering the overlap realization and trade-off between memory and communication are difficult and expensive, this paper is still significant for LLM community.\n\nA: We agree that we are not the first to use quantization in communication and we appreciate the reviewer's acknowledgment of the significance of our work. In addition, we would like to note that ZeRO++'s novelty lies in its detailed application of quantization without impacting the convergence and improving system throughput. This is because, despite using state-of-the-art quantization to mitigate quantization error, a significant gap remains in terms of convergence due to numeric precision loss, and in throughput due to the computation cost involved in quantization and dequantization.   That is why we present a novel all-to-all communication primitive, a full set of CUDA kernels, and a carefully designed system to: i) maintain numeric precision even during communication in a quantized format; ii) minimize quantization costs using custom CUDA kernels; and iii) boost system throughput by concurrently performing intra-node and inter-node communication and hiding the quantization cost.\n\n2.\tAnother problem is that this paper is lack of specific introduction to used quantization techniques and corresponding theoretical analysis. For example, how the quantization error affects the convergence rate and whether the error-feedback technique works. \n\nA:   Due to page limitations, details on the block-based quantization can be found in Appendix B.1, which we will clarify in the main text.\n\nRegarding theoretical convergence, one notable aspect is the unique design of ZeRO. In the ZeRO paradigm, there are two kinds of weights: the temporary weights, which are all-gathered from all GPUs in order to do forward/backward on a particular model layer and then discarded; and the local weight shard, which is used to populate the temporary weights, are permanent and will be updated at each optimizer step. ZeRO++ quantization only occurs when **we communicate the temporary weights**. Thus, the local shard of the weights will be free of the quantization error. For the gradients that are computed from the quantized temporary weights, there will be minor errors, yet fruitful research such as [1, 2, 3] prove that minor disturbance to gradients will not impact the overall convergence (the convergence rate follows O(1/sqrt{T}) for T iterations), which we have also empirically shown. An empirical evaluation can be found in Table 2 where the convergence gap increases as the quantization goes from 6 bits to 2 bits. We put the training curves of different bits in the appendix (Table 6). We also added the above discussion to the appendix to better discuss ZeRO++\u2019s theoretical convergence.\n\n>[1] Gorbunov, Eduard, Filip Hanzely, and Peter Richt\u00e1rik. \"A unified theory of SGD: Variance reduction, sampling, quantization and coordinate descent.\" AISTATS 2020 \n>\n>[2] Ramezani-Kebrya, Ali, et al. \"NUQSGD: Provably communication-efficient data-parallel SGD via nonuniform quantization.\" The Journal of Machine Learning Research 22.1 (2021): 5074-5116.\n>\n>[3] Alistarh, Dan, et al. \"QSGD: Communication-efficient SGD via gradient quantization and encoding.\" Advances in neural information processing systems 30 (2017).\n\n\nWe view error-feedback as an orthogonal approach to ZeRO++ and we believe adding error-feedback techniques such as [4, 5] could help especially for 1-bit/2-bit training or for a smaller model, which is a part of our future work. We will have more discussion on this. Thank you for the suggestion.\n\n>[4] Tang, Hanlin, et al. \"1-bit adam: Communication efficient large-scale training with adam\u2019s convergence speed.\" International Conference on Machine Learning. PMLR, 2021.\n>\n>[5] Wu, Jiaxiang, et al. \"Error compensated quantized SGD and its applications to large-scale distributed optimization.\" International Conference on Machine Learning. PMLR, 2018.\n\n\n3.\tWhat is the specific quantization methods in qwZ and qgZ?\n\nA: We use block-based quantization for both weights and gradients. The details of it can be found in Appendix B. Simply put, we pre-scale the tensors by small chunks before casting them to lower precision to mitigate the difference in numerical ranges and granularities.\n\n(To be continued)"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2830/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700258986323,
                "cdate": 1700258986323,
                "tmdate": 1700261681940,
                "mdate": 1700261681940,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]