[
    {
        "title": "LMCC-MBC: Metric-Constrained Model-Based Clustering with Wasserstein-2 Distance of Gaussian Markov Random Fields"
    },
    {
        "review": {
            "id": "0KbdsK1p06",
            "forum": "7vzyqs8UbA",
            "replyto": "7vzyqs8UbA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6618/Reviewer_cTE9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6618/Reviewer_cTE9"
            ],
            "content": {
                "summary": {
                    "value": "Problem: This paper studies a clustering problem for data with special properties like time or spatial positions. This paper focuses on the case when there is the effect of metric autocorrelation in data, which means the variance of feature vectors is positively correlated to their temporal/spatial distances.\n\nModelling: The authors propose a metric-constrained model-based model that leverages the correlation between adjacent estimated models and their locations in metric space. They use Gaussian Markov Random Fields to model inter-observation dependency and use Wasserstein-2 distance to measure the distance between estimated model parameters. Because of the metric autocorrelation, they use a metric penalty that decreases as distance increases.\n\nKey results: From the experimental results, their algorithm appears to be computationally more efficient than other methods and has better performance in terms of ARI and NMI."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. From the experimental results, their algorithm appears to be computationally more efficient than other methods and has better performance in terms of ARI and NMI.\n2. The proposed method can deal with arbitrary dimensions of constraint space instead of just 1-D and 2-D."
                },
                "weaknesses": {
                    "value": "Major comments:\n1. In the contribution section: the authors indicate that they present a solid mathematical proof of the soundness of their generalized semivariogram. But there isn\u2019t any proof in this paper and later they say \u201cThe soundness of this generalized definition can be easily verified on real-world datasets\u201d.\n2. Same problem for the fourth contribution. It would be better to at least have a complexity analysis for your algorithm to say that your algorithm solves the problem efficiently with \u201csolid theoretical proofs\u201d.\n3. I think the authors need to justify the use of GMRF in the model since they are fitting a GMRF for each sample. It would make more sense to fit one GMRF for one cluster as different data clusters will naturally have different variable dependency structures.\n4. One of the advantages of the proposed method they can deal with arbitrary dimensions of constraint space instead of just 1-D and 2-D. Did you try it with a 3-D dataset?\n\nMinor comments:\n1. In section 5.1, the authors claim that the proposed method has only three hyperparameters to tune. What about the tuning parameter for Graphical Lasso?"
                },
                "questions": {
                    "value": "See \"Weaknesses\""
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6618/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6618/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6618/Reviewer_cTE9"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6618/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698294525308,
            "cdate": 1698294525308,
            "tmdate": 1699636755226,
            "mdate": 1699636755226,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "A1Y9y8kg9V",
                "forum": "7vzyqs8UbA",
                "replyto": "0KbdsK1p06",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6618/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6618/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Q1: solid mathematical proof of the soundness of their generalized semivariograms**\n\n**A1:** Briefly speaking, the expected Wasserstein-2 distance for a given metric distance bin is part of the computation for determining the threshold of goodness-of-fit tests, and we formally define it as Model-based semivariogram in Section 4.1 since it has similar form to the classical semivariograms. A detailed discussion and derivation can be found in Appendix 7.4. *\n\n**Q2: a complexity analysis for LMCC algorithm**\n\n**A2:** Please kindly refer to our general response Q3 as well as Appendix 7.4 for a detailed discussion and additional experiments.\n\n**Q3: justify the use of GMRF for each sample rather than learning a GMRF for one cluster**\n\n**A3:** Please see Part 2 of our general response as well as Appendix 7.4 about the main contribution of our work: a novel family of clustering objectives -- i.e., instead of optimizing the total data likelihood, we propose to minimize the number of failed goodness-of-fit tests, which compare pairs of data points and their underlying models in the clustering.\n\nFitting one model (e.g., GMRF) for one clustering is commonly used in existing model-based clustering algorithms such as TICC and STICC, which optimize the total data likelihood. They require a non-trivial  estimation-maximization (EM) procedure which leads to several drawbacks in efficiency and modeling:. \nFirstly, the EM step is very time-consuming and takes many iterations to converge. Secondly, the clustering result relies heavily on the initial assignment, and the optimization is subject to local optima. Thirdly, tuning the hyper-parameters (e.g., number of clusters) requires expensive retraining. Finally, data likelihood objectives have a lack of flexibility when modeling constraints such as spatial metric autocorrelation as a generative process. These problems can be avoided by using a bottom-up model-based clustering algorithm, i.e., data points merge into clusters according to their underlying model similarity (like DBSCAN). \n\nIn order to enable a bottom-up model-based clustering, we need to learn a GMRF for each data sample instead of learning one GMRF for one cluster. This allows us to perform the goodness-of-fit test (i.e., test whether two samples come from a statistically identical distribution) between any given two data samples in order to determine whether we can merge these two data samples into one cluster.\n\nIn short, we estimate one GMRF for one data sample instead of one cluster to enable a  bottom-up DBSCAN-like model-based clustering rather than the traditional kMeans-like practice. \n\n**Q4: Apply LMCC on 3D dataset**\n\n**A4:** This is a very good point. We have found 3-D datasets about point cloud-based scene segmentation and are working on the experiments. Due to time limits of rebuttal, we can not present the results in the rebuttal, but we will add this result to the camera-ready version.\n\n\n**Q5: Tuning parameter for Graphical Lasso**\n\n**A5:** We grid-searched the hyperparameters alpha, tolerance and maximum number of iterations for Graphical Lasso. The hyperaramters we use are alpha=0.01, tolerance=1e-4 and maximum iterartion=1000. Theoretically, as long as Graphical Lasso converges, it yields very stable covariance estimation. Thus there is no need to tune these hyperparameters on the entire LMCC-MBC algorithm. We simply find out hyperparameters that enables Graphical Lasso to converge on a subset of the dataset and fix them throughout the experiments."
                    },
                    "title": {
                        "value": "Response to Weaknesses/Questions"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6618/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700687496548,
                "cdate": 1700687496548,
                "tmdate": 1700687627948,
                "mdate": 1700687627948,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "NUqkfBeLRd",
                "forum": "7vzyqs8UbA",
                "replyto": "A1Y9y8kg9V",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6618/Reviewer_cTE9"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6618/Reviewer_cTE9"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate the authors' responses, which have addressed most of my concerns. \n\nAs for Q3, I understand your choice of GMRF for performming the goodness-of-fit test. But it is still hard for me to accept that a GMRF learned from a single sample would have sufficient statistical significance for such tests.\n\nOverall, I would like to keep my score."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6618/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700706356015,
                "cdate": 1700706356015,
                "tmdate": 1700706356015,
                "mdate": 1700706356015,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "C3T7rOYpnT",
            "forum": "7vzyqs8UbA",
            "replyto": "7vzyqs8UbA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6618/Reviewer_FyBz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6618/Reviewer_FyBz"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors focus on metric-constrained clustering (when clustering is based not only on the features of the data points but also on constraints in a metric space (time, geo data points). Within that framework, they propose a new metric-constrained model-based clustering approach, LMCC-MBC (Locally Monotonically and Continuously Constrained Model-Based Clustering) optimized to maximize intra-cluster cohesion and inter-cluster separation, working as follows:\n\n-For each data point, compute the neighboring set of points in the metric-constraint space (not using the features) and fit a Gaussian Markov Random Field model with Graphical Lasso algorithm. \n\n-For each pair of observations, compute model and metric distances (at this stage, we still don\u2019t leverage features of the data points).\n\n-Compute a semivariogram from the two distances and $\\rho$ the range of the fitted semivariogram\n\n-Compute weighted distance matrix M based on model, metric distances, semivariogram, $\\rho$\n\n-Run the some density-based clustering method (ex: DBSCAN) on M for the final clustering partition.\n\nWasserstein-2 distance is used as the model distance."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is well written and quite enjoyable to read.\n\nThe only hyperparameters of the clustering approach are the number of neighbors to consider per data point, the metric-constraint strength ($\\beta$) and a shift parameter  ($\\delta$) that was found empirically to overlap the clustering boundaries if appropriately tuned. \n\nThe authors generalize the concept of the classic semivariogram for multivariate data points.\n\nExperiments are made on 2 synthetic datasets and 7 real-world datasets equally split between the temporal and spatial use case. LMCC model with and without metric constraints show promising results by providing systematically the best ARI and NMI.\n\nA comparison is made between LMCC and TICC/STICC algorithms (the competing approaches for the metric-constrained case) regarding stability and robustness."
                },
                "weaknesses": {
                    "value": "As with many clustering algorithms, there could be a theoretical comparison analysis of the space and time complexities for LMCC-MBC and competing SOTA techniques. \n\nRegarding the following claim:  \u201cIn fact, Wasserstein-2 distance is the only feasible choice of model distance that theoretically guarantees the generalized model-based semivariogram is compatible with the classic definition. Consequently, GMRF is chosen because it is the most computationally efficient model parameterization under Wasserstein-2 distance. The following section will prove this in detail.\u201d\nIt does not sound to me that there is a proof here. The usage of Wasserstein-2 distance and GMRF is justified, yes, but I don't have the feeling that this proves that these are the only options as stated.   \n\nNo experimental study of the effects of the number of neighbors and the shift parameter on accuracy. \n\nMinor, typos:\n\n-missing space after \u201cunknown\u201d in intro p.1\n\n-requirment, p.7\n\n-natrually, p.7 before eq. 11\n\n-fittiing p.8 in Algorithm 1\n\n-hyperparamters p.16\n\n-missing space after \u201cour baselines(Kang et al, 2022) p.16\n\n-unlined instead of underlined in p.17\n\n-It seems there is a missing paragraph in the Appendix related to Execution time comparison"
                },
                "questions": {
                    "value": "Q1: Could you please provide a theoretical comparison analysis of the space and time complexities for LMCC-MBC and competing SOTA techniques?\n\nQ2: How in practice do you tune the shift parameter $\\delta$? It seems to be the determinant hyperparameter of the method but there is no study to show the influence on accuracy. Same for the number of neighbors to consider. \n\nQ3: Can you please explain how you prove that the requirement of the Wasserstein-2 distance between estimated model parameters from GMRF (model distance) to be a locally monotonic continuous function of the metric distance guarantees intra-cluster cohesion and inter-cluster separation?\n\nQ4: What is the purpose of the experiments with the synthetic datasets? They seem to be applying the same experiment setup as the real-word ones."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6618/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698708712305,
            "cdate": 1698708712305,
            "tmdate": 1699636755102,
            "mdate": 1699636755102,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "hdDO7oFY50",
                "forum": "7vzyqs8UbA",
                "replyto": "C3T7rOYpnT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6618/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6618/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Q1: Theoretical comparison analysis of the space and time complexities for LMCC-MBC and competing SOTA techniques**\n\n**A1:** Please refer to our general response Q3 as well as Appendix 7.5 for a detailed discussion and additional experiments. \n\n**Q2: No experimental study of the effects of the number of neighbors and the shift parameter on accuracy.**\n\n**A2:** Please refer to our general response Q4 and Appendix 7.6 for the added ablation studies on these two hyperparameters.\n\n**Q3: How to prove that LMCC guarantees intra-cluster cohesion and inter-cluster separation**\n\n**A3:** Thank you for this question! Please refer to Appendix 7.4, where we carefully discussed how the LMCC-MBC loss can be seen as a weighted sum of penalties that punish intra-cluster data pairs for not passing a goodness-of-fit test. Consequently, minimizing this loss equals encouraging the intra-cluster data points to be as similar as possible, i.e., intra-cluster cohesion in a model-based clustering sense. Since different clusters have different underlying models, increasing intra-cluster cohesion will automatically increase inter-cluster separation. \n\n**Q4: The proof of the usage of Wasserstein-2 distance and GMRF**\n\n**A4:** Please refer to our general response Q2 for the reason for our choice of Wasserstein-2 distance, GMRF, Graphical Lasso, and DBSCAN. We also provide the theoretical foundation of the reason why we use Wasserstein-2 distance (see our general response Q1). Essentially, in order to enable a bottom-up model-based clustering, we need to formulate the loss function (see Equation 4 ) in terms of only pairwise computations between data points. The goodness-of-fit test (i.e., test if two samples come from a statistically identical distribution) statistic perfectly matches our needs. The square of Wasserstein-2 distance (Panaretos & Zemel, 2019) satisfies our requirements since it metricizes weak convergence.\n\n**Q5: the purpose of the experiments with the synthetic datasets?**\n\n**A5:** The purpose of experiments on the synthetic dataset is to provide an understanding of whether our theory is solid, since the real-world datasets may or may not follow all the theoretical assumptions we make while we can control the data generation process of the synthetic dataset and make it follow our assumptions. It can be seen that on synthetic datasets, as all the assumptions are met, the performance of our method is extremely high (~90%), while in real-world datasets, because of violations of assumptions (e.g., data error, non-normality, etc.) the performance may drop but still significantly outperform the existing STOA clustering algorithms. \n\n**Q6: Typos.**\n**A6:** We are sorry about these typos. We have corrected the typos in our updated version, highlighted in red."
                    },
                    "title": {
                        "value": "Response to Weaknesses/Questions"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6618/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700686948789,
                "cdate": 1700686948789,
                "tmdate": 1700687644940,
                "mdate": 1700687644940,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "oOo7iNGrUX",
            "forum": "7vzyqs8UbA",
            "replyto": "7vzyqs8UbA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6618/Reviewer_FzKt"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6618/Reviewer_FzKt"
            ],
            "content": {
                "summary": {
                    "value": "This work attempts to address the metric autocorrelation problem in model-based clustering. To be specific, each data sample is modeled by a Gaussian Markov Random Field, and the distance between GMRFs are measured by Wasserstein-2 distance. The conventional clustering assumption objective is then optimized to minimize intra-cluster distances and maximize inter-cluster distances. The authors argue that the combination of Wasserstein-2 distance and GMRF is cautiously chosen and provide some theoretical analyses."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "* The paper is well-written and easy to read.\n* This work proposes to incorporate metric autocorrelation into the clustering model, which is important but overlooked by previous works.\n* Empirical results on both synthetic and real-world datasets verify the effectiveness of the proposed method."
                },
                "weaknesses": {
                    "value": "* The effectiveness of modification from Eq. (5) to (6) is empirical and lacks a theoretical guarantee.\n* The proposed method just combines several existing components. Even though the authors argue that the chosen combination is not heuristic, and provides some special theoretical properties of them, the overall technical contribution looks less significant to me.\n* The experiments are weak. Only an overview of clustering performance is provided. Ablation studies of the design choices are lacking, so these claims are not well-supported. Comparisons with strong baselines are also lacking.\n\n------\n\n## Post-rebuttal\n\nDear authors, I greatly appreciate the detailed clarification. Some of my concerns are cleared.\n\nRegarding A1 & A2, the major contribution is minimizing the number of failed goodness-of-fit tests rather than optimizing the total data likelihood according to General Response Q1, which seems novel. However, the components are still taken as-is despite being well-crafted as explained in General Response Q2, so my original comment still holds.\n\nRegarding A4, I don't work in your area so I can't suggest stronger baselines. But from a general perspective, TICC was published in 2017 hence doesn't look new. STICC was published in a geoscience journal rather than ML/DL journals/conferences, and it has been cited only 7 times as of now, that's why I had the question.\n\nOverall, I raised my rating to 5."
                },
                "questions": {
                    "value": "N/A"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6618/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6618/Reviewer_FzKt",
                        "ICLR.cc/2024/Conference/Submission6618/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6618/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698743959812,
            "cdate": 1698743959812,
            "tmdate": 1700749888574,
            "mdate": 1700749888574,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "eRisuM65Bo",
                "forum": "7vzyqs8UbA",
                "replyto": "oOo7iNGrUX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6618/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6618/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Q1: The effectiveness of modification from Eq. (5) to (6) is empirical and lacks a theoretical guarantee** \n\n**A1:** Thank you for raising this important question! We provide more detailed reasoning in the general rebuttal Q1 and Appendix 7.4. Briefly speaking, the expected Wasserstein-2 distance for a given metric distance bin is part of the computation for determining the threshold of goodness-of-fit tests, and we formally define it as Model-based semivariogram in Section 4.1 since it has similar form to the classical semivariograms. \n\n**Q2: \u200b\u200bThe proposed method just combines several existing components. The overall technical contribution looks less significant to me.** \n\n**A2:** Thank you for  raising this important question regarding our work! Please kindly refer to the general rebuttal Q2 for a discussion about our main contribution, which we briefly summarize here:\nThe most important contribution of our work  is  a novel family of clustering objectives -- i.e., instead of optimizing the total data likelihood, we propose to minimize the number of failed goodness-of-fit tests, which compare pairs of data points and their underlying models in the clustering. Compared to total data likelihood based objectives, this novel approach enjoys several benefits in efficiency and modeling (see details in Appendix 7.4).\n\nThe choice of specific implementations, i.e. Graphical Lasso and DBSCAN, is not important. Using other covariance estimation and distance-based clustering algorithms yields comparable results. We refer to Appendix 7.7.1 (Table 3) and 7.7.2 (Table 4) for the ablation studies on this. That demonstrates we are not mere heuristic combinations of existing algorithms.\n\n**Q3: Ablation studies of the design choices are lacking.**\n\n**A3:** Thanks for your suggestion. We updated our paper and added an ablation study on the influence of hyperparameters. Please kindly refer to the general rebuttal Q4 and Appendix 7.6. In general, the conclusion is that both the number-of-neighbor hyperparameter and the shift hyperparameter have nearly convex responses. That means we can easily tune them by simple hierarchical grid search.\n\nAdditionally, Table 1 in our paper does not only report the overall clustering performance, but also compares the effectiveness of introducing spatial information (i.e., we compared performances w/wo spatial distance). We can see from the results that introducing metric distance into the clustering objective significantly improves the clustering accuracy.\nWe also added ablations studies on the choice of covariance matrix estimation methods (*Graphical Lasso* v.s. *Minimum Covariance Determinant* v.s. *Shrunk Covariance*) and the choice of distance-based clustering (*DBSCAN* v.s. *HDBSCAN* v.s. *OPTICS*). Please refer to our general response Q2.3, Appendix 7.7.1 and Appendix 7.7.2.\n\n**Q4: Comparisons with strong baselines is lacking.**\n**A4:** To the best of our knowledge, in temporal and spatial clustering, TICC and STICC are the strongest baselines for metric constrained clustering. If  you can point us to other baselines we are happy to do more experimental comparisons."
                    },
                    "title": {
                        "value": "Response to Weaknesses/Questions"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6618/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700686060576,
                "cdate": 1700686060576,
                "tmdate": 1700687652897,
                "mdate": 1700687652897,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]