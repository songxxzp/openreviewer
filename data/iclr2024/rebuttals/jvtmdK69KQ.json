[
    {
        "title": "Statistical Perspective of Top-K Sparse Softmax Gating Mixture of Experts"
    },
    {
        "review": {
            "id": "4H3CnPdmwh",
            "forum": "jvtmdK69KQ",
            "replyto": "jvtmdK69KQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6919/Reviewer_dwg3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6919/Reviewer_dwg3"
            ],
            "content": {
                "summary": {
                    "value": "Based on the Gaussian mixture expert model, the influence of top-K sparse softmax gating function on density estimation and parameter estimation was analyzed. Novel loss functions and Voronoi metrics were used to characterize the behavior of different regions in the input space."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "S1. The writing is clear and concise, making it easy to understand theory.\n\nS2. From the author's introduction, this paper is the first to perform convergence analysis for maximum likelihood estimation (MLE) under top-K sparse softmax gated Gaussian MoE. The results of this paper are innovative and helpful in inspiring new expert mixture system designs."
                },
                "weaknesses": {
                    "value": "W1. The theoretical results of this paper lack experimental verification.\n\nW2. It only considers Gaussian mixture expert models and does not consider other types of models."
                },
                "questions": {
                    "value": "Considering W2, I would like to know the difficulties in generalizing the results of this article to MoE methods without Gaussian assumption."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "Nan."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6919/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698667538076,
            "cdate": 1698667538076,
            "tmdate": 1699636805585,
            "mdate": 1699636805585,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9uGW1naMX2",
                "forum": "jvtmdK69KQ",
                "replyto": "4H3CnPdmwh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6919/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6919/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer dwg3 (Part 1)"
                    },
                    "comment": {
                        "value": "Dear Reviewer dwg3,\n\nThanks for providing your valuable reviews, and giving **good (3)** grades to the **soundness, presentation** and **contribution** of our paper. We hope that we address your concerns regarding our paper with the responses below.\n\n**Q1: The theoretical results of this paper lack experimental verification.**\n\nThanks for raising your concern. Actually, we already conducted numerical experiments to empirically verify our theoretical results in Appendix D. In that appendix, we show that the empirical convergence rates of parameter estimation totally match the theoretical rates under both the exact-specified and over-specified settings. Therefore, we would like to refer the reviewer to Appendix D for further details."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6919/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700075153043,
                "cdate": 1700075153043,
                "tmdate": 1700075153043,
                "mdate": 1700075153043,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gIGlewCpnU",
                "forum": "jvtmdK69KQ",
                "replyto": "4H3CnPdmwh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6919/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6919/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer dwg3 (Part 2)"
                    },
                    "comment": {
                        "value": "**Q2: The paper only considers Gaussian mixture-of-experts models. What are the difficulties in generalizing the results to MoE methods without Gaussian assumption?**\n\nThanks for your question. We would like to clarify that the proof technique in the paper can be extended to other types of models. The reason that we consider only top-K sparse softmax Gaussian MoE is because it is the most popular one in practice. Per your suggestion, we have generalized the results of the paper to the other settings of MoE when $f$ belongs to the family of location-scale distributions $\\mathcal{F}:=\\{f(Y|h_1(X,a,b),\\sigma):(a,b,\\sigma)\\in\\Theta\\}$, where the expert function $h_1(X,a,b):=a^{\\top}X+b$ stands for the location, $\\sigma$ denotes the scale and $\\Theta$ is a compact subset of $\\mathbb{R}^d\\times\\mathbb{R}\\times\\mathbb{R}_+$. Due to the space limit, we will summarize our findings here, and leave further details, including rigorous proofs, in Appendix E of the current manuscript.\n\nFirstly, we characterize the family $\\mathcal{F}$ based on the following notion of strong identifiability:\n\n**Definition 1 (Strong Identifiability).** We say that the family $\\mathcal{F}$ is strongly identifiable if $f(Y|h_1(X,a,b),\\sigma)$ is twice differentiable w.r.t its parameters and the following assumption holds true: \n\nFor any $k\\geq 1$ and $k$ pairwise different tuples $(a_1,b_1,\\sigma_1),\\ldots,(a_k,b_k,\\sigma_k)\\in\\Theta$, if there exist real coefficients \n$\\alpha^{(i)}_{\\ell_1,\\ell_2}$, for $i\\in[k*]$ and $0\\leq \\ell_1+\\ell_2\\leq 2$, such that \n\n$$\n\\sum_{i=1}^{k}\\sum_{\\ell_1+\\ell_2=0}^{2} \\alpha^{(i)}_{\\ell_1,\\ell_2}\\cdot\\dfrac{\\partial^{\\ell_1+\\ell_2}f}{\\partial h_1^{\\ell_1}\\partial \\sigma^{\\ell_2}}(Y|(a_i)^{\\top}X+b_i,\\sigma_i)=0,\n$$\n\nfor almost surely $(X,Y)$, then we obtain that $\\alpha^{(i)}_{\\ell_1,\\ell_2}=0$ for any $i\\in[k*]$ and $0\\leq \\ell_1+\\ell_2\\leq 2$.\n\n**Example.** The families of Student's t-distributions and Laplace distributions are strongly identifiable, while the family of location-scale Gaussian distributions is not. Therefore, it suggests that the location-scale Gaussian distribution is weakly identifiable.\n\nSince the convergence rates of maximum likelihood estimation when $\\mathcal{F}$ is a family of Gaussian distributions, which is not strongly identifiable, have already been studied, we focus on the scenario when $\\mathcal{F}$ is strongly identifiable. Under that assumption, the density $f(Y|h_1,\\sigma)$ is twice differentiable w.r.t its parameters, thus, it is also Lipschitz continuous. As a consequence, the density estimation rates under both the exact-specified and over-specified in Theorem 1 and Theorem 3 still hold true. Therefore, we consider only the parameter estimation problem in the sequel. \n\n**Parameter estimation under the strongly identifiable MoE:** In high level, we need to establish the Total Variation lower bound \n\n$$E_X[V(g_{G}(\\cdot|X),g_{G*}(\\cdot|X))]\\gtrsim \\mathcal{D}(G,G*),$$ \n\nfor any $G\\in\\mathcal{O}_k(\\Omega)$. \n\nThen, this bound together with the density estimation rate in Theorem 1 (resp. Theorem 3) leads to the parameter estimation rates in Theorem 2 (resp. Theorem 4). Here, the key step is to decompose the difference between the density $g_{\\widehat{G}_n}(Y|X)$ \n\nand the true density $g_{G*}(Y|X)$ into a combination of linearly independent terms using Taylor expansions. Therefore, we have to involve the above notion of strong identifiability, and separate our convergence analysis based on that notion.  \n\n**Under the exact-specified settings**, we demonstrate in Theorem 5 that the rates for estimating true parameters \n\n$\\exp(\\beta^{\\ast}_{0i})$, \n\n$\\beta^{\\ast}_{1i}$ , $a^{\\ast}_i$, $b^{\\ast}_i$, $\\sigma^{\\ast}_i$ are of order $O(n^{-1/2})$, which match those in Theorem 2. \n\n**Under the over-specified settings**, we show in Theorem 6 that \nthe rates for estimating true parameters $\\beta^{\\ast}_{1i},a^{\\ast}_i,b^{\\ast}_i,\\sigma^{\\ast}_i$, which are fitted by more than one component, are of order $\\widetilde{\\mathcal{O}}(n^{-1/4})$. Notably, these rates are no longer determined by the solvability of the system of polynomial equations in Eq. (7). Thus, they are significantly faster than those in Theorem 4. The main reason for this improvement is when $\\mathcal{F}$ is strongly identifiable, the interaction among expert parameters via the second partial differential equation in Eq. (6) does not occur. \n\nFinally, we would like to emphasize that we consider only Gaussian MoE models in our original manuscript since Gaussian is the most challenging distributions. In particular, as the family of location-scale Gaussian distributions does not satisfy the strong identifiability conditions, we encounter an interaction among parameters via the PDE in Eq. (6), which subsequently leads to the complex system of polynomial equations in Eq. (7). As a result, the convergence analysis for the Gaussian MoE models are much more challenging than that for the strongly identifiable MoE models."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6919/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700075239869,
                "cdate": 1700075239869,
                "tmdate": 1700075239869,
                "mdate": 1700075239869,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "vF4PpHqh8g",
            "forum": "jvtmdK69KQ",
            "replyto": "jvtmdK69KQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6919/Reviewer_X5io"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6919/Reviewer_X5io"
            ],
            "content": {
                "summary": {
                    "value": "This paper analyzes Top-K sparse softmax gating mixture of experts. In particular, they focus on Gaussian mixture of experts and present theoretical results on the effect of MoE on density and parameter estimations."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* Analysis of MoE is a highly relevant and impactful direction of research.\n* The analysis seems thorough and rigorous at a high level and the presented results are interesting."
                },
                "weaknesses": {
                    "value": "* The paper is quite dense, and it would have been helpful to outline high level ideas before delving into notation-heavy math.\n* Chen et al. [1] also analyze MoE, but in the deep learning setting. How does the analysis presented here differ from this work?\n\n[1] Towards Understanding Mixture of Experts in Deep Learning https://arxiv.org/pdf/2208.02813.pdf"
                },
                "questions": {
                    "value": "1. What are the practical implications of the work, if any, for the use of Top-K sparse MoE as a means of conditional computation?\n2. How does this work compare to that of [1]?\n\n[1] Towards Understanding Mixture of Experts in Deep Learning https://arxiv.org/pdf/2208.02813.pdf"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6919/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698865771816,
            "cdate": 1698865771816,
            "tmdate": 1699636805401,
            "mdate": 1699636805401,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "pdUikkNwxM",
                "forum": "jvtmdK69KQ",
                "replyto": "vF4PpHqh8g",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6919/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6919/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer X5io"
                    },
                    "comment": {
                        "value": "Dear Reviewer X5io\n\nWe would like to thank you for the wealth of your comments, and giving **good (3)** grades to the **soundness, presentation** and **contribution** of our paper. Below are our responses to your questions. We hope that we address your concerns regarding our paper.\n\n**Q1: Chen et al. [1] also analyze MoE, but in the deep learning setting. How does the analysis presented here differ from this work?**\n\nThanks for your question. We would like to highlight the differences between Chen et al. [1] and our work in terms of goals, settings and results as follows:\n\n**1. Goals.**\n\n- Chen et. al. [1] studied the mechanism of the mixture of experts (MoE) layers for deep learning from the perspective of a classification problem. Firstly, they would like to know whether using the mixture of experts yields a higher test accuracy than using a single expert. Secondly, they considered the top-K sparse softmax gating MoE model, where $K=1$. They aimed to explore how the router learns to dispatch the data to the right expert.\n\n- In our work, we aim to understand the effects of the top-K sparse softmax gating function, for arbitrary $K\\geq 1$, on the MoE models via the density and parameter estimation problems. Then, we compare the performance of the top-K sparse softmax gating MoE to that of the MoE with dense softmax gating function. Finally, we provide insight into finding the best choices of $K$ based on the theoretical results.\n\n**2. Settings.** \n\n- Chen et. al. [1] sampled the data from a distribution with multiple clusters, and generated their label from the set {$1,-1$} uniformly. Then, they trained a top-1 sparse softmax gating MoE layer based on the data using the gradient descent method to minimize an empirical logistic loss. Here, they used a linear gating network and formulated each expert as a two-layer convolutional neural network. \n\n- In our work, we assume that the data are generated from the ground-truth top-K sparse softmax gating Gaussian MoE model, where the mean of each Gaussian distribution is a linear expert network (an extension to general expert network is possible but will require more complex calculations and we leave that for the future work). Then, we estimate the true density function and true parameters of that model by using the maximum likelihood method. Note that, parameter estimation is important to understand as they directly lead to the estimation of experts and softmax weights.   \n\n**3. Results.**\n\n- Chen et. al. [1] demonstrated that the top-K sparse softmax gating mixture of non-linear experts can achieve nearly 100\\% test accuracy, while a single expert can only reach a test accuracy of no more than 87.5\\% on the data distribution. Moreover, they proved that each expert will be specialized to a specific portion of the data, which is determined by the initialization of the weights. Finally, they figured out that the router can learn the cluster-center features and route the input data to the right experts.\n\n- In our work, we establish the convergence rates of density estimation and parameter estimation under the top-K sparse softmax gating Gaussian MoE model when the true number of experts is known and unknown. From our theoretical results, we realize that using the top-K sparse softmax gating function not only helps scale up the model capacity but also keeps the model performance comparable to that when using the dense softmax gating function. Additionally, we also find out that using the top-K sparse softmax gating function, for $K=1$, leads to faster convergence rates of parameter estimation than other values of $K$. For further details, please refer to our response to your Question 2. \n\n**Q2: What are the practical implications of the work, if any, for the use of Top-K sparse MoE as a means of conditional computation?**\n\nThanks for your question. We would like to refer the reviewer to our response to the Common Question 1 in the General Response (part 2) for further details. \n\n**Q3: It would have been helpful to outline high level ideas before delving into notation-heavy math.**\n\nThanks for your suggestion. We would like to refer the reviewer to the Common Question 2 in the General Response (part 2) for further details.\n\n**References**\n\n[1] Z. Chen, Y. Deng, Y. Wu, Q. Gu, Y. Li. Towards understanding mixture of experts in deep learning. In NeurIPS, 2022."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6919/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700074682724,
                "cdate": 1700074682724,
                "tmdate": 1700110159484,
                "mdate": 1700110159484,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "gKfSIAwBML",
            "forum": "jvtmdK69KQ",
            "replyto": "jvtmdK69KQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6919/Reviewer_qHcA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6919/Reviewer_qHcA"
            ],
            "content": {
                "summary": {
                    "value": "The paper provides theoretical analysis on the convergence properties of sparse softmax gating mixtures of Gaussian experts. The key contributions are proving convergence rates in two cases - when the number of experts is exactly-specified, and when there is an overspecified number of experts. \n\nFor the exact-specified case, the authors show a convergence rate of $O(n^{-1/2})$ to the true parameters under maximum likelihood estimation. This is an important theoretical result as it quantifies the sample complexity.\n\nFor the overspecified case, the convergence rate depends on the cardinality of the Voronoi cells induced by the gate activations"
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The theoretical analysis of convergence rates for MoE models is novel and useful. Understanding sample complexity of different MoE architectures is valuable.\n- The paper is technically strong, with detailed proofs of the main results. The analysis for the overspecified case considering Voronoi cells is creative.\n- The assumptions are clearly laid out, making the results easy to interpret and apply."
                },
                "weaknesses": {
                    "value": "- As noted, the paper is quite dense with heavy notation. More intuition and examples earlier on could make it more accessible.\n- The writing in the universal assumptions section is unclear, and should be revised for readability.\n- The implications of the theory for practitioners could be expanded on more in the discussion. Guidance on model design is lacking."
                },
                "questions": {
                    "value": "- How do these convergence results compare to prior analysis on softmax vs sparse gating? Does this theory suggest one gating approach over the other?\n- Due to the instability of the EM algorithm, there is no error bar provided in Figure 3b. Can we still consider this experiment to be reliable to to justify the theoretical results?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6919/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6919/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6919/Reviewer_qHcA"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6919/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699339565156,
            "cdate": 1699339565156,
            "tmdate": 1700108544057,
            "mdate": 1700108544057,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "sxyvSSi78r",
                "forum": "jvtmdK69KQ",
                "replyto": "gKfSIAwBML",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6919/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6919/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer qHcA"
                    },
                    "comment": {
                        "value": "Dear Reviewer qHcA,\n\nWe would like to thank you for providing your constructive feedback and giving **good (3)** grade to the **contribution** of our paper. We hope that we can address your concerns with the responses below, and eventually convince you to raise your final score.\n\n**Q1: The writing in the universal assumptions section is unclear, and should be revised for readability.**\n\nThanks for your suggestions. We have revised the universal assumptions in the current manuscript (in blue color) as follows: \n\n**(U.1) Convergence of MLE:** To ensure the convergence of parameter estimation, we assume that the parameter space $\\Omega$ is compact subset of $\\mathbb{R}\\times\\mathbb{R}^d\\times\\mathbb{R}^d\\times\\mathbb{R}\\times\\mathbb{R}_+$, while the input space $\\mathcal{X}$ is bounded.  \n\n**(U.2) Identifiability:** Next, we assume that $\\beta^*_{1k_*}=0_d$ and $\\beta^*_{0k_*}=0$ so that the top-K sparse softmax gating Gaussian mixture of experts is identifiable. Under that assumption, we show in Appendix C that if $G$ and $G'$ are two mixing measures such that $g_{G}(Y|X)=g_{G'}(Y|X)$ for almost surely $(X,Y)$, then it follows that that $G\\equiv G'$. Without that assumption, the result that $G\\equiv G'$ does not hold, which leads to unncessarily complicated loss functions (see Proposition 1 in [1]).\n\n**(U.3) Distinct Experts:** To guarantee that experts  in the mixture [1] are different from each other, we assume that parameters $(a^{\\ast}_i,b^{\\ast}_i,\\sigma^{\\ast}_i)$, for $i\\in[k*]$, are pairwise distinct.\n\n**(U.4) Input-dependent Gating Functions:** To make sure that the gating functions depend on the input $X$, we assume that at least one among parameters $\\beta^*_{11},\\ldots,\\beta^*_{1k_*}$ is different from zero. Otherwise, the gating functions would be independent of the input $X$, which simplifies the problem significantly. In particular, the model (1) would reduce to an input-free gating Gaussian mixture of experts, which was already studied in [2].\n\n**Q2: The implications of the theory for practitioners could be expanded on more in the discussion.**\n\nThanks for your suggestion. We would like to refer the reviewer to our response to the Common Question 1 in the General Response (Part 2) for further details. \n\n**Q3: How do these convergence results compare to prior analysis on softmax vs sparse gating?**\n\nThanks for your question. It is worth emphasizing that our paper is the very first work establishing the theoretical guarantee for the effects of top-K sparse gating function on the mixture of experts via the parameter estimation problem. Additionally, the most related work to ours is [1], which established the convergence rates of parameter estimation under the dense softmax gating Gaussian mixture of experts. For the comparison of parameter estimation rates under these two models, please refer to the paragraph \"No trade-off between model capacity and model performance\" in your Question 2.\n\n**Q4: Guidance on model design is lacking. Does this theory suggest one gating approach over the other?**\n\nThanks for your question. We would like to refer the reviewer to our response to the Common Question 1 in the General Response (part 2) for further details. \n\n**Q5: Due to the instability of the EM algorithm, there is no error bar provided in Figure 3b. Can we still consider this experiment to be reliable to to justify the theoretical results?**\n\nThanks for your comment. Indeed, as we mentioned in the Appendix, the EM iterates are unstable, which leads to high error bars. That is why we did not include that in the previous version of our manuscript. In the new version of our manuscript, per your suggestion, we have included error bars for the EM iterates in Figure 3b.  \n\n**Q6: More intuition and examples earlier on could make it more accessible.**\n\nThanks for your suggestion. We would like to refer the reviewer to the Common Question 2 in part 2 of the General Response section for further details.\n\n**References**\n\n[1] H. Nguyen, TT. Nguyen, and N. Ho. Demystifying softmax gating in Gaussian mixture of experts. Advances in Neural Information Processing Systems, 2023a.\n\n[2] N. Ho, C. Y. Yang, and M. I. Jordan. Convergence rates for Gaussian mixtures of experts. Journal of Machine Learning Research, 23(323):1\u201381, 2022."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6919/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700074510721,
                "cdate": 1700074510721,
                "tmdate": 1700074510721,
                "mdate": 1700074510721,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xewHJ3mLCU",
                "forum": "jvtmdK69KQ",
                "replyto": "gKfSIAwBML",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6919/Reviewer_qHcA"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6919/Reviewer_qHcA"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the thorough response. I have increased the score to 6."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6919/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700108688988,
                "cdate": 1700108688988,
                "tmdate": 1700108730958,
                "mdate": 1700108730958,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]