[
    {
        "title": "Single-Trajectory Distributionally Robust Reinforcement Learning"
    },
    {
        "review": {
            "id": "hjQDSTCUXb",
            "forum": "xuKVVYxU5D",
            "replyto": "xuKVVYxU5D",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1968/Reviewer_7vQe"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1968/Reviewer_7vQe"
            ],
            "content": {
                "summary": {
                    "value": "This work develops a three time-scale algorithm, aiming to solve the robust RL problem with a single trajectory with an online fashion. The work is important considering the current works in the area."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The idea is interesting. To solve the unbiased estimation issue, another time scale is introduced. I believe this idea is interesting and novel.\n2. The experiment results are promising."
                },
                "weaknesses": {
                    "value": "1. The presentation of the results is infusing and can be improved. E.g., there are too many assumptions made to imply the results. If this work is focusing on the three time-scale stochastic approximation framework itself, it is OK to make these assumptions; But this works is for a concrete problem, i.e., DR-RL, I believe it should not be reasonable to make all these assumptions. \n2. As mentioned, with so many assumptions made, I doubt the soundness of the convergence results. E.g., how can I know that under the DR-RL problem, the associated ODE has a unique global asymptotically stable equilibrium?"
                },
                "questions": {
                    "value": "1. The work use gradient descent to solve the support function. Why does this DRO problem can be solved by GD? Why is this problem smooth? Why does the gradient exist? I didn't see this gradient descent approach much used in previous DRO works.    \n2. How do you justify the assumptions you made?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1968/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697565098153,
            "cdate": 1697565098153,
            "tmdate": 1699636128269,
            "mdate": 1699636128269,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "iCMaJVypoU",
                "forum": "xuKVVYxU5D",
                "replyto": "hjQDSTCUXb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1968/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1968/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate the reviewer's comments and efforts sincerely. We believe the following point-to-point response can address all the concerns and misunderstandings.\n\n---\n\n **Q.** Confusing presentation and too many assumptions. How to justify them.\n\n**A.**\nWe apologize for the confusing presentation of the results, particularly the organization of the assumptions.\n\nFor the main theoretical result (Theorem 3.3), which focuses on the asymptotic convergence of the DRRL algorithm, **we only require ONE assumption about the learning rate (Assumption C.10)**. This assumption can be easily satisfied by some common choices, such as $\\zeta_1(n) = \\frac{1}{1+n^{0.6}}$, $\\zeta_2(n) = \\frac{1}{1+n^{0.8}}$, and $\\zeta_3(n) = \\frac{1}{1+n}$.\n\nThe remaining assumptions are not directly related to the specific application of our proposed algorithm but serve to support our general three-timescale stochastic approximation framework. This framework is a side-product of our paper and aims to ensure its generality and independent usage value. **Regarding the soundness of the convergence results**, we verify that our proposed algorithm (Algorithm 1) meets all the necessary assumptions on Pages 23-24 in the proof of convergence. Consequently, its associated ODE has a unique asymptotically stable equilibrium.\n\nThese assumptions are standard and essential in the classical two-timescale stochastic approximation framework (refer to Section 6 in Borkar 2009), and we extend them to establish a three-timescale counterpart.\n\n---\n\n**Q.** The reason why this DRO problem can be solved by the GD?\n\n**A.**  Although smoothness can contribute to faster convergence speeds when using GD, our DRRL problem is convex and Lipschitz with respect to the dual variable $\\eta$, which is sufficient to establish asymptotic convergence.\n\nIn our DRO problem (Equation 4), the gradient is almost defined on the entire real line, except when $\\eta = \\max_{a'}Q(s',a')$. In such cases, we can utilize the subgradient, and our algorithm will still function effectively. In fact, Duchi and Hongseok (2021) also consider the same DRO problem with Cressie-Read divergence of the $f$-family.\nOur DRRL problem is included in their problem if the $Q$ function is fixed.\nThey apply gradient descent with backtracking Armijo line-searches to address the DRO problem for large datasets.\n\n GD-based approaches are prevalent in past DRO studies, such as those by Namkoong Hongseok and John C. Duchi (2017), Jin, Jikai et al (2021), Qi, Qi et al (2021), Sinha, Aman et al (2017), and Blanchet, Jose et al (2022). Our work is closely related to Duchi and Hongseok (2021), focusing on the Cressie-Read divergence of the $f$-family. Our DRRL problem is included by their DRO problem when the $Q$ function is fixed. They use gradient descent with backtracking Armijo line-searches (Boyd, Stephen P. 2004) for large datasets (Duchi and Hongseok, 2021). Namkoong and John (2016) reformulate the DRO problem as a two-player game and develop a stochastic gradient-based solution combined with bandit learning. In the DRRL community, Wenhao Yang et al. (2023) propose a model-free DRRL algorithm using gradient descent. We believe GD-based DRRL solutions could be promising for efficient problem-solving, especially for online update requirements in RL applications.\n\n---\n\n[1] Duchi, John C., and Hongseok Namkoong. \"Learning models with uniform performance via distributionally robust optimization.\" The Annals of Statistics 49.3 (2021): 1378-1406.\n\n[2] Borkar, Vivek S. Stochastic approximation: a dynamical systems viewpoint. Vol. 48. Springer, 2009.\n\n[3] Namkoong, Hongseok, and John C. Duchi. \"Stochastic gradient methods for distributionally robust optimization with f-divergences.\" Advances in neural information processing systems 29 (2016).\n\n[4] Namkoong, Hongseok, and John C. Duchi. \"Variance-based regularization with convex objectives.\" Advances in neural information processing systems 30 (2017).\n\n[5] Yang, Wenhao, et al. \"Avoiding model estimation in robust markov decision processes with a generative model.\" arXiv preprint arXiv:2302.01248 (2023).\n\n[6] Jin, Jikai, et al. \"Non-convex distributionally robust optimization: Non-asymptotic analysis.\" Advances in Neural Information Processing Systems 34 (2021): 2771-2782.\n\n[7] Qi, Qi, et al. \"An online method for a class of distributionally robust optimization with non-convex objectives.\" Advances in Neural Information Processing Systems 34 (2021): 10067-10080.\n\n[8] Sinha, Aman, et al. \"Certifying some distributional robustness with principled adversarial training.\" arXiv preprint arXiv:1710.10571 (2017).\n\n[9] Boyd, Stephen P., and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004.\n\n[10] Blanchet, Jose, Karthyek Murthy, and Fan Zhang. \"Optimal transport-based distributionally robust optimization: Structural properties and iterative schemes.\" Mathematics of Operations Research 47.2 (2022): 1500-1529."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1968/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700494164617,
                "cdate": 1700494164617,
                "tmdate": 1700494164617,
                "mdate": 1700494164617,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Q36Rxii7ox",
            "forum": "xuKVVYxU5D",
            "replyto": "xuKVVYxU5D",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1968/Reviewer_JJMJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1968/Reviewer_JJMJ"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors proposed a distributionally robust variant of Q-learning aiming to solve distributionally robust MDP in an online fashion. Their algorithm utilizes a three-timescale stochastic approximation framework and possesses an almost surely convergence guarantee. They conduct thorough experiment illustrating the performance of their algorithm."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "They extend the classical two-timescale stochastic approximation framework into the DRO problem, and design an online algorithm for DRRL. Comprehensive experiments are conducted to illustrate the performance."
                },
                "weaknesses": {
                    "value": "The writing is problematic. I find the paper hard to follow. There are many notations/terms lack of definitions and several critical statements lack of discussion and explanation. Moreover, there are typos and factual errors in this paper, which make the results not credible. All in all, it's hard to verify the validity of this paper."
                },
                "questions": {
                    "value": "1. What does the misspecified MDP in the paper mean?\n2. Badrinath & Kalathil (2021) and Roy et al. (2017) didn't use R-contamination model. Their uncertainty set is a general one and covers yours. Both of them study the online setting and present asymptotic results. Why don't you compare with their works in experiment and compare their theoretical results with yours.\n3. Why the first equation on page 5 is true? \n4. On page 6, why keeping the learning speeds of $\\eta$ and $Q$ different can stabilize the training process?\n5. The three-timescale regime used in algorithm 1 is complicated and provided with limited explanations. Why and how it works? \n6. In the proof, there are more than 10 assumptions and most of them don't have any explanation. Why do they hold in your case? Are they necessary? Are they reasonable in practical problems?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1968/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1968/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1968/Reviewer_JJMJ"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1968/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698597489234,
            "cdate": 1698597489234,
            "tmdate": 1699636128164,
            "mdate": 1699636128164,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BknQDR1TgC",
                "forum": "xuKVVYxU5D",
                "replyto": "Q36Rxii7ox",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1968/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1968/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate the reviewer's comments and efforts sincerely. We believe the following point-to-point response can address all the concerns and misunderstandings.\n\n---- \n\n**Q.** Could you please clarify the meaning of the misspecified MDP in the paper?\n\n**A.**  We apologize for not providing a formal definition of the misspecified MDP in the paper. In the context of distributionally robust reinforcement learning (DRRL), the main objective is to learn an optimal policy that is robust to unknown environmental changes. This becomes important when the transition model P and reward function r used during the training data collection differ from those in the test environment. In such cases, **we refer to the MDP environment (consisting of the transition model P and reward function r) used for training data collection as the misspecified MDP**. A detailed and rigorous definition can be found in Section 2.3. We appreciate your understanding and will ensure to provide clearer explanations in future work.\n\n-----------\n\n**Q.** Comparison with Roy et al. (2017) and Badrinath & Kalathil (2021).\n\n**A.** First, we would like to clarify that the uncertainty sets considered in Roy et al. (2017) and Badrinath & Kalathil (2021) **do not cover the ones in our work**, as they **do not require the transition model to be a valid probability transition** in the test environment. In fact, **their uncertainty set is equivalent to R-contamination set**. This leads to over-conservatism and significantly simplifies the theoretical difficulty and algorithmic design.\n\nIn their works, Roy et al. (2017) and Badrinath & Kalathil (2021) use the following ambiguity set:\n\n$$\nP_i^a := \\\\{x+ p_i^a \\lvert x\\in U_{i}^a \\\\},\n$$\n\nwhere $p_i^a$ is the unknown state transition probability vector from the state $i\\in \\mathcal{X}$ to every other state in $\\mathcal{X}$. The confidence region $U_i^a$ must be constrained to ensure $x+p_{i}^a$ lies within the probability simplex. However, they drop the requirement and consider a proxy confidence region. \nRecall the definition of the R-contamination model,\n\n$$\nP_s^a=\\\\{(1-R) p_s^a+R q \\mid q \\in \\Delta_{|S|}\\\\}, s \\in S, a \\in A, \\text { for some } 0 \\leq R \\leq 1.\n$$\n\nNote that  $(1-R)p_s^a+R q$ may also lie outside the probability simplex. Thus under the relaxation, the ambiguity set used in Roy et al. (2017) and Badrinath & Kalathil (2021) are equivalent to the R-contamination model.\nIn contrast, our approach requires every element in the ambiguity set to lie within the probability simplex. \n\nThis relaxation simplifies the algorithmic design and the convergence establishment. In particular, the $Q$-learning algorithm can be reformulated (see Equation (32) in Roy et al. (2017)) in terms of the operator $H$ as \n$$\nQ_t(i,a) = (1-\\gamma_t) Q_{t-1}(i,a) + \\gamma_t (HQ_t(i,a) + \\eta_t(i,a)),\n$$\nwhere $\\eta_t(i,a)$ is some martingale noise in the non-robust $Q$-learning. \n\nIn the robust Q-learning approaches of Roy et al. (2017) and Badrinath & Kalathil (2021), they also benefit from the relaxation mentioned earlier, as the proxy uncertainty set in Equation 10 in Roy et al. (2017) **does not depend on the incoming sample $j$**. Despite having sufficient samples, their algorithm may still penalize some cases that will never occur, as their ambiguity set lacks distribution-awareness. Thus they mainly focus on ellipsoid and parallelepiped as concrete examples.\n\nIn contrast, our approach maintains the worst-case distribution as a valid transition probability (referred to as distributionally aware) to prevent over-conservatism. As a result, $\\eta_t(i,a)$ is not mean zero in our paper, as we require the transition to always fall within the probability complex. To address this challenge, our three-timescale algorithmic design, particularly the innermost loop and the second loop, along with the extended stochastic approximation framework, has been proposed.\n\n---\n\n\n**Q.** Why the first equation on Page 5 is true?\n\n**A.** Recall that the definition of $c_k(\\rho)$ defined in Lemma 3.1 as  $c_k(\\rho) = (1+k(k-1)\\rho)^{1/k}\\ge 1$. Define \n\n$$\nf(\\eta) = \\eta-c_k(\\rho)(\\eta - \\max_{a'}Q(s',a'))^+.\n$$ \n\nThen the first order $f'(\\eta) = 1 - c_k(\\rho) \\mathbb{1}\\\\{\\eta \\ge \\max_{a'}Q(s',a')\\\\}$. \n\nWe know $f'(\\eta) = 1>0$ when $\\eta <\\max_{a'}Q(s',a')$ and $f'(\\eta) = 0$ when $\\eta \\ge \\max_{a'}Q(s',a')$. \n\nThus $\\max_{\\eta\\in \\mathbb{R}}f(\\eta) = f(\\max_{a'}Q(s',a')) = \\max_{a'}Q(s',a')$.\n\nPlug it into Equation 5 we have \n$$\n\\begin{aligned}\nr(s,a)+\\gamma \\sup_{\\eta\\in \\mathbb{R}}\\\\{\\eta-c_k(\\rho)(\\eta - \\max_{a'}Q(s',a'))\\\\}\n= r(s,a)+\\gamma \\{\\max_{a'}Q(s',a')-c_k(\\rho) \\cdot 0\\} = r(s,a) + \\gamma \\max_{a'}Q(s', a').\n\\end{aligned}\n$$"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1968/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700493845065,
                "cdate": 1700493845065,
                "tmdate": 1700495452675,
                "mdate": 1700495452675,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "EnZgdmmCjN",
            "forum": "xuKVVYxU5D",
            "replyto": "xuKVVYxU5D",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1968/Reviewer_DkEx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1968/Reviewer_DkEx"
            ],
            "content": {
                "summary": {
                    "value": "This paper considers a distributionally robust RL problem. A model-free online TD Q-learning type method is developed to find the robust Q values without relying on a simulator. Specifically, a three-timescale framework is introduced to approximate the robust Bellman equation and asymptotic analysis is provided. The main algorithm is validated through a tabular RL example; a more practical DQN type algorithm is introduced to handle more complicated examples."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- This paper considers distributionally robust MDP using f-divergence as the uncertainty set, which is novel.\n- The motivations of not using SAA to approximate robust bellman equation are clear.\n- The reasons of not using multilevel Monte-Carlo method are clear."
                },
                "weaknesses": {
                    "value": "- I guess the paper was written in parallel with the Panaganti 2022 (Robust offline) paper. However, as the Panaganti 2022 paper is published for over 6 months, it is better to compare and discuss the difference choices of uncertainty sets and problem settings. The authors claim that this paper is the first model-free DR RL paper in the literature, which is not true as the Panaganti 2022 is also model-free. To some extent, their paper considers a harder offline problem while this paper considers an online version. \n- I don't think this is an issue when evaluating the contribution of this paper. However, it is better to state the contribution according to the latest literature (indeed, RFQI was mentioned in the experiment) and explicitly comment on the differences. \n- The proposed algorithm 1 only works in the tabular setting. This is fine. The authors introduce a more practical algorithm using DQN type of learning scheme, which is provided in algorithm 4 in the Appendix. However, algorithm 4 looks almost the same as algorithm 1. Is this paper an older version? If yes, please provide the full algorithm 4 description in the future. More importantly, I believe algorithm 4 could potentially have a greater impact on DRRL problems; its connection and modifications with algorithm 1 could be further explained in details in the main paper."
                },
                "questions": {
                    "value": "- In my opinion, the combination of online RL with DRO is a bit weird. It makes more sense to study offline RL with DRO. The question is that why would you prefer to learn a robust policy in the testing environment, i.e., this robust policy is not optimal in the testing environment. Is it because there are some technical challenges when applying f-diverngence to offline dataset, e.g., when no e-greedy policy is allowed? \n- Same to the weakness, please compare with Panaganti 2022 in details and explicitly."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1968/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1968/Reviewer_DkEx",
                        "ICLR.cc/2024/Conference/Submission1968/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1968/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698610244741,
            "cdate": 1698610244741,
            "tmdate": 1700585625866,
            "mdate": 1700585625866,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "RTDsgLxIea",
                "forum": "xuKVVYxU5D",
                "replyto": "EnZgdmmCjN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1968/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1968/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank gratefully the reviewer for various valuable suggestions and the praise of our interesting findings and clear writing! The points you raised are explained in the following.\n\n---\n\n**Q.** Comparison with Panaganti et al. 2022.\n\n**A.**  We appreciate your feedback and agree that our initial claim in the abstract may have been overstated.  We will make the necessary corrections to our claim.\n\nWe acknowledge the valuable contribution of the RFQI algorithm proposed by Panaganti et al. in 2022 and their novel work in addressing the DR offline RL problem with both theoretical guarantees and practical applications. Our primary focus is on the single trajectory scheme, which presents unique challenges compared to other papers, including Panaganti et al. 2022.\n\nAt the time of completion of our paper, the existing literature on model-free DRRL was limited, with the closest works being Panaganti et al. 2022 and Liu et al. 2022. We have already provided a thorough comparison of our algorithm with Liu et al. 2022 in the main text.\n\n We would like to kindly point out the primary distinctions between our proposed algorithm and Panaganti's approach:\n\n\n1. Ambiguity Set: RFQI concentrates on the **total variation distance**, while we consider the Creese-Read family of $f$-divergence which can cover several commonly used divergence.\n2. Technical Assumptions: RFQI heavily relies on the **fail-state** assumption, which assumes the existence of a state that yields a 0 reward regardless of the chosen action and must return to itself once entered. Such assumption aids to bypass the nonlinearity in the dual problem of the DRO problem with TV distance, i.e., the $\\inf_{s''}V(s'')$ part in Equation 4 in Panaganti et al. 2022, and significantly simplify the estimation. This assumption may not be applicable in certain cases, such as infinite-horizon problems. Instead, the dual problem of the Creese-Read family of $f$-divergence is nonliear with respect the expectation part and we don't impose assumption to remove the nonlinearity. Instead we propose our multi-timescale algorithmic design to address it directly.\n4. Data Collection Requirement: While RFQI aims to learn an optimal DR RL policy using a pre-collected offline dataset, our algorithm offers greater flexibility by allowing data to be fed in a single trajectory manner, which includes the pre-collected dataset setting.\n5. Algorithmic Design: RFQI is based on the batch data scheme and develops their algorithm from value iteration with function approximation, alternating between updating the dual variable and the $Q$ value to ensure satisfactory optimization before updating the other variable. Conversely, our algorithm is grounded in the $Q$-learning approach and incrementally updates both the dual variable and the $Q$ value function simultaneously upon each data arrival, albeit with distinct learning rates.\n\n\n   Our algorithm employs the Cressie-Read family of $f$-divergence and can accommodate several common divergences, including KL and $\\chi^2$ divergence. The dual problem under this family is highly nonlinear with respect to the expectation, which is addressed by our multiple-timescale algorithmic design. Additionally, we do not impose extra assumptions for our chosen ambiguity set. \n\n|  | RFQI | DRQ (Ours) |\n| --- | --- | --- |\n| Ambiguity Set | Total Variation Distance | Cressie-Read family of f-divergence |\n| Assumption for Ambiguity Set | Fail-State | No assumptions specific for ambiguity set |\n| Data Collection | A batch of pre-collected data | Allows even single-trajectory data |\n| Algorithmic Design | Value Iteration with alternative update between dual variable and Q | Q-learning with incremental and simultaneous update for dual variable and Q |"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1968/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700492096070,
                "cdate": 1700492096070,
                "tmdate": 1700492096070,
                "mdate": 1700492096070,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aFoOeKBH6g",
                "forum": "xuKVVYxU5D",
                "replyto": "EnZgdmmCjN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1968/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1968/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Q.** Clarification about Algorithm 4.\n\n**A.** We are sincerely sorry that mistakenly attaching the Algorithm 4 to the appendix, which is the same as Algorithm 1. **The more practical algorithm using DQN type of learning scheme is summarized into Algorithm 2**.\n\nIn particular, in Algorithm 2, we utilize the Deep Q-Network (DQN) architecture and neural networks as functional approximators for the dual variable and Q function in place of the tabular function in Algorithm 1. To improve training stability, we introduce target networks that are updated at a slower rate. We adopt a two-timescale update approach, which is employed to minimize the Bellman error for the Q network and maximize the DR Q value for the dual variable network. Even though this may introduce bias in the dual variable's convergence, resulting in a lower target value for the Q network, this approach serves as a robust update strategy for our DRRL problem, with further discussion in Appendix B.3. Moreover, Algorithm 1 updates on each data arrival, while the practical implementation uses a batch scheme, training the neural networks on a resampled subset of samples with a replay buffer retaining all previous samples.\nWe will move its connection and modifications with algorithm 1 to the main text.\n \n\n\n-------\n\n\n**Q.** The motivation of the combination of online RL with DRO.\n\n **A.** We appreciate the reviewer's suggestion that combining DRO with offline RL would have more direct application value. One potential application of our online DRRL algorithm is in situations where the test environment's transition or reward functions change incrementally over time. In such cases, a promising algorithm should continuously learn a robust policy to counter potential upcoming perturbations.\n\nOur paper aligns with the model-free Robust RL literature, such as works by Dong, Jing, et al. (2022), Badrinath, et al. (2021), and Wang, Yue, and Shaofeng Zou (2021). These works present model-free algorithms capable of learning optimal robust policies without the need for a simulator or a pre-collected dataset. Our novel analysis of the Cressie-Read family of $f$-divergence and the multi-timescale algorithmic design are generic and can be easily integrated with other non-robust offline RL algorithms, such as the CQL algorithm (Kumar, Aviral, et al., 2020). We chose to present our method using online learning to demonstrate its value under the most stringent update requirements.\n\n---\n\n[1] Dong, Jing, et al. \"Online policy optimization for robust MDP.\" arXiv preprint arXiv:2209.13841 (2022).\n\n[2] Badrinath, Kishan Panaganti, and Dileep Kalathil. \"Robust reinforcement learning using least squares policy iteration with provable performance guarantees.\" International Conference on Machine Learning. PMLR, 2021.\n\n[3] Wang, Yue, and Shaofeng Zou. \"Online robust reinforcement learning with model uncertainty.\" Advances in Neural Information Processing Systems 34 (2021): 7193-7206.\n\n[4] Kumar, Aviral, et al. \"Conservative q-learning for offline reinforcement learning.\" Advances in Neural Information Processing Systems 33 (2020): 1179-1191."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1968/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700492201087,
                "cdate": 1700492201087,
                "tmdate": 1700495287300,
                "mdate": 1700495287300,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XXOt6TnygG",
                "forum": "xuKVVYxU5D",
                "replyto": "RTDsgLxIea",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1968/Reviewer_DkEx"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1968/Reviewer_DkEx"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks. I guess RFQI might only work under TV distance as they want to use least square to find the Q function in the offline setting. If KL is used, as you said, it is nonlinear w.r.t the expectation and LS does not work. \nDo you think it is possible to apply the type of method you developed to the offline setting?"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1968/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700584175757,
                "cdate": 1700584175757,
                "tmdate": 1700584175757,
                "mdate": 1700584175757,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "IAE2QfXSYL",
                "forum": "xuKVVYxU5D",
                "replyto": "hPm0WYGSfu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1968/Reviewer_DkEx"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1968/Reviewer_DkEx"
                ],
                "content": {
                    "comment": {
                        "value": "I think the authors address my concerns. I raised my score to 6.\n\nSome of the strengths and improvements compared to previous papers are due to the online setting (continuous explorations, two-timescale stochastic approximation). Besides, I am not convinced by the promising applications of applying DRO to online RL problems."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1968/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700585588346,
                "cdate": 1700585588346,
                "tmdate": 1700585588346,
                "mdate": 1700585588346,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ZTmBB0ScsW",
            "forum": "xuKVVYxU5D",
            "replyto": "xuKVVYxU5D",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1968/Reviewer_ogVe"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1968/Reviewer_ogVe"
            ],
            "content": {
                "summary": {
                    "value": "This work designs model-free algorithms for distributionally robust RL problems in a sample-efficient manner. It proposed a three-time scale algorithm that solves a class of robust RL problems using the uncertainty set constructed by the Cressie-Read family of f-divergence. A theoretical asymptotic guarantee has been provided. Moreover, this work conducted experiments to evaluate the performance and sample efficiency of this work."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. It targets an interesting problem: design a model-free algorithm for distributionally robust RL problems.\n2. A three-timescale algorithm has been proposed that enjoys an asymptotic guarantee and practical sample efficiency.\n3. The introduction of the algorithm is clear and easy to follow."
                },
                "weaknesses": {
                    "value": "1. For the experiments in Figure 5. It seems the proposed algorithm DDQR has a very similar performance compared to the existing robust algorithm SR-DQN. It will be helpful to add more discussion about this.\n2. As mentioned in the algorithm, the three-timescale serves as a key role in the algorithm to ensure convergence. So it will be better to introduce what is the three learning rates that the practical algorithm uses. And ablation study using different learning rates will give a message about whether the algorithm is sensitive to the learning rate."
                },
                "questions": {
                    "value": "1. As the update of $\\eta$ is independent for each $(s,a)$ pair, will the proposed algorithm works for $s$-rectangular cases?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1968/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1968/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1968/Reviewer_ogVe"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1968/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698710758825,
            "cdate": 1698710758825,
            "tmdate": 1700182479947,
            "mdate": 1700182479947,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8SjSDyA1gV",
                "forum": "xuKVVYxU5D",
                "replyto": "ZTmBB0ScsW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1968/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1968/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your detailed reading and valuable comments. Please find our response to the comments below:\n\n--- \n\n**Q.** Disscussion about the similar performance of the proposed DDQR algorithm with the existing robust algorithm SR-DQN.\n\n**A.** The SR-DQN algorithm demonstrates comparable performance to our proposed DDQR method, primarily when environmental parameters are perturbed (e.g., FMP perturbation in the Cartpole environment and EPP perturbation in the LunarLander environment). In these perturbations, even if the force magnitude (FMP) or engine power (EPP) is altered in the test environment, the transition remains almost deterministic, meaning only one state transitions when an action is taken under the current state. Consequently, these perturbations maintain a similar level of randomness as the original environment, which might not substantially degrade the learned policy. As a result, the soft-robustness principle employed in SR-DQN may be sufficient to manage the perturbation, leading to similar performance as our proposed algorithm.\n\nHowever, when it comes to action perturbations in both the Cartpole and LunarLander environments, multiple states can be transitioned as different actions are deployed in the environment due to the perturbation. This increases the randomness level, and SR-DQN is notably outperformed by our DDQR algorithm, as it does not provide adequate robustness.\n\nIt is worth mentioning that SR-DQN does perform well in certain situations, as demonstrated in the Cartpole experiment by Panaganti et al. (2022).\n\n---\n\n**Q.** The potential of the proposed algorithm to work in s-rectangular setting.\n\n**A.** We agree with the reviewer's insight that our proposed algorithm can work in s-rectangular setting by tracking the optimal dual variable for each state, rather then for each state-action pair in the current sa-rectangular setting.\n\n---\n\n[1] Panaganti, Kishan, et al. \"Robust reinforcement learning using offline data.\" Advances in neural information processing systems 35 (2022): 32211-32224."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1968/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700491912728,
                "cdate": 1700491912728,
                "tmdate": 1700491912728,
                "mdate": 1700491912728,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Yc9kEDgVDO",
                "forum": "xuKVVYxU5D",
                "replyto": "YhmJloji9s",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1968/Reviewer_ogVe"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1968/Reviewer_ogVe"
                ],
                "content": {
                    "title": {
                        "value": "Response to the author"
                    },
                    "comment": {
                        "value": "Thanks for providing the additional ablation study for the learning rate. The answers address the reviewer's concerns and the reviewer would like to keep the positive scores for this work."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1968/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700631282644,
                "cdate": 1700631282644,
                "tmdate": 1700631282644,
                "mdate": 1700631282644,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ufnfop19MW",
            "forum": "xuKVVYxU5D",
            "replyto": "xuKVVYxU5D",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1968/Reviewer_UFU7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1968/Reviewer_UFU7"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a novel distributionally robust reinforcement learning (DRRL) algorithm which is model-free and uses single trajectories to update Q function estimates and compute the robust policy. The approach consists of a multi-scale approximation scheme that utilizes existing stochastic approximation results. Asymptotic convergence is proven. Moreover, the method is evaluated experimentally on two tabular environments and a DQN-based implementation is tested on larger (and widely used) control tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Distributionally robust reinforcement learning is a relevant and active area of research. While a few methods have been already proposed, the paper is novel in that it considers a model-free setting, without assuming access to a simulator but only to single trajectory data. I believe this is significantly more practical and makes a good contribution.\n\n- A reasonable amount of experiments illustrate the features of the proposed approach, compared to existing DRRL baselines.\n\n- The paper is nicely written and the results look sound, although i could not verify their proofs."
                },
                "weaknesses": {
                    "value": "- Only asymptotic convergence is proven, and no sample-complexity guarantees. Do the authors have a guess on how these may compare with previous work, e.g. (Panaganti et al. 2022)?\n\n- In Section 4.2, a practical implementation of DQR is utilized for the experiments. How is this implemented? I would be nice to discuss such implementation in the main text."
                },
                "questions": {
                    "value": "See weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1968/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698856183670,
            "cdate": 1698856183670,
            "tmdate": 1699636127890,
            "mdate": 1699636127890,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Vvkk87ZCDA",
                "forum": "xuKVVYxU5D",
                "replyto": "ufnfop19MW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1968/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1968/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your review and positive comments on our paper. Please find our response to the comments below:\n\n------\n\n**Q. Comparison of the sample-complexity of the proposed algorithm with Panaganti et al. 2022.**\n\n**A.**  Based on the current literature, a reasonable guess of the sample complexity of our proposed algorithm might be $O(n^{-1/3})$, which may not be as efficient as the $O(n^{-1/2})$ in Panaganti et al. 2022. Our algorithm is based on the multi-timescale nonlinear stochastic approximation method, which, to our knowledge, has not been considered for finite sample efficiency in existing literature. While previous works have mostly focused on the linear case for the two-timescale scenario, there is limited information available for the nonlinear counterpart.\n\nBroadly speaking, for linear stochastic approximation, a notable result in Dalal et al. 2018 suggests that the sample efficiency can be at most $O(n^{-1/3})$, where $n$ represents the sample size. Subsequently, Kaledin Maxim et al. 2020 and Dalal Gal et al. 2020 improved this rate to $O(n^{-1/2})$ by further exploiting the linear structure. In the case of nonlinear two-timescale stochastic approximation, Doan Thinh T has established a $O(n^{-1/3})$ sample efficiency guarantee. Given the current progress in understanding two-timescale nonlinear stochastic approximation, we anticipate that the optimal sample efficiency we can achieve is $O(n^{-1/3})$.\n\n-------\n\n**Q: Discussion about the practical implementation of DQR algorithm.**\n\n**A:** We have deferred the discussion of the specific implementation to Appendix B.3 and summarized into Algorithm 2. In this answer, we restate the implementation details, particularly the differences from the DRQ algorithm (Algorithm 1), and will move the main part of the discussion to the main text.\n\nInstead of the tabular function in Algorithm 1, we adopt the Deep Q-Network (DQN) architecture (Mnih et al., 2015) and choose neural networks as functional approximators for the dual variable and the Q function. To enhance training stability, we introduce another set of neural networks as the corresponding target networks, which are updated at a slower rate. Due to the existence of approximation error, we adopt a two-timescale update approach: our Q network aims to minimize the Bellman error, while the dual variable $\\eta$ network strives to maximize the DR Q value. The two-timescale update approach could introduce bias in the convergence of the dual variable, and thus the dual variable \u03b7 may not be the optimal dual variable for the primal problem. Given the primal-dual structure of this DR problem, this could render an even lower target value for the Q network to learn. This approach can be understood as a robust update strategy for our original DRRL problem. More discussion about the validity of this update strategy can be found in Appendix B.3.\n\nAnother significant difference is in the update scheme. While in Algorithm 1, we update on each data arrival, in the practical implementation, the neural networks are updated in a batch scheme, i.e., trained on a resampled subset of samples with a replay buffer keeping all the previous samples.\n\n------\n\n[1] Mnih, Volodymyr, et al. \"Human-level control through deep reinforcement learning.\" nature 518.7540 (2015): 529-533.\n\n[2] Dalal, Gal, et al. \"Finite sample analysis of two-timescale stochastic approximation with applications to reinforcement learning.\" Conference On Learning Theory. PMLR, 2018.\n\n[3] Panaganti, Kishan, et al. \"Robust reinforcement learning using offline data.\" Advances in neural information processing systems 35 (2022): 32211-32224.\n\n[4] Kaledin, Maxim, et al. \"Finite time analysis of linear two-timescale stochastic approximation with Markovian noise.\" Conference on Learning Theory. PMLR, 2020.\n\n[5] Dalal, Gal, Balazs Szorenyi, and Gugan Thoppe. \"A tale of two-timescale reinforcement learning with the tightest finite-time bound.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 34. No. 04. 2020.\n\n[6] Doan, Thinh T. \"Nonlinear two-time-scale stochastic approximation convergence and finite-time performance.\" IEEE Transactions on Automatic Control (2022)."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1968/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700490919948,
                "cdate": 1700490919948,
                "tmdate": 1700544931620,
                "mdate": 1700544931620,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]