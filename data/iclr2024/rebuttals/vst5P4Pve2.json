[
    {
        "title": "Towards Global Interaction Efficiency of Graph Networks"
    },
    {
        "review": {
            "id": "BOji8ddp25",
            "forum": "vst5P4Pve2",
            "replyto": "vst5P4Pve2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7293/Reviewer_uvYG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7293/Reviewer_uvYG"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors aim to address the limitations of existing GNNs in representing interactions. They propose to study interactions between node pairs in a graph from a global perspective. They also propose a metric called interaction efficiency for assessing GNN performance. In the analysis of interaction efficiency,  two aspects - interaction sensitivity and interaction expressiveness - are discussed. Finally, a new GNN model, called Universal Interaction Graph Convolution (UIGC), is presented. The authors claim that this proposed GNN model has superior interaction efficiency."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Studying interactions among nodes in a graph is of significance to both theoretical foundations and practical applications of the GNN community. Thus, the paper is tackling an important topic. The intention of characterising interactions from the lens of efficiency, sensitivity, and expressiveness also has some novelty. However, the quality of the paper is a concern (see comments in the section \"Weaknesses\")."
                },
                "weaknesses": {
                    "value": "(W1)  The key concepts of the paper (such as global/universal interaction, interaction sensitivity, and interaction expressiveness) are not well-defined.\n\n- The definition of global interaction is defined in terms of the channels of inputs and outputs. How does this relate to interaction patterns, e.g., the five synthetic interaction patterns in Section 5.1? Also, how does this relate to node/pair symmetry? \n\n- For \"how a model can universally express any desired interaction within a given graph\", what does \"universally\" mean? The paper also mentions universal interaction but no formal definition is provided. \n\n- Interaction sensitivity is defined to measure the sensitivity of model outputs to perturbations in input node features. However,  perturbations in input node features are not the same as interactions between node pairs - so how are they related? Also, why is  interaction expressiveness considered as one aspect of interaction efficiency?\n\n(W2) The formulation and notations are not well presented.\n\n- Figure 1: What does \"a variable capable of taking K values (K=4 there)\" mean? How do you decide such a K value?\n\n- What does the notation $S_{i,j,:}$ refer to? What is $\\mathbb{R}^*$?\n\n- Equation 6 is not well defined. What do local structures mean precisely? Also the domain of $\\varphi^{LE}$ is vague - this needs to be formulated and clearly defined.\n\n- For the statement \"the discrimination ability of non-symmetry pairs of $\\varphi^{LE}$ is a partial order with the injectivity to be the most discriminative one\", it needs a clarification.\n\n- What is the definition of $S/\\sim$? Since $S\\subseteq \\mathbb{R}^{n\\times n}$, why is $S/\\sim=\\mathbb{R}^{\\eta}$?\n\n- In Proposition 1, is $f_{\\mathcal{W}b}$ a typo? What is $\\mathbf{x}_a$?\n\n(W3) Some explanations are needed to improve the clarity of the paper.\n\n- The paper mainly focuses on graph convolution networks as stated in Equation 1. But this is not clarified in the abstract and introduction which seem to consider graph neural networks in general.  \n\n- It is unclear how the issues under-reaching and over-squashing mentioned in the abstract and introduction can be addressed by the proposed UIGC layer defined in Equation 7.  \n\n- For the statement \"UIGC infers the interaction of each pair directly through their local encodings, which will not be affected by the connectivity of graphs\", I don't understand this. Why is it not affected by the connectivity of graphs?\n\n(W4) The setup of experiments may cause some confusions. For example,\n\n- How many graphs are randomly selected for the experiments on learning interactions on synthetic data, only one graph? How many graphs are considered in the result presented in Table 1?\n\n- For the experimental results on the five distinct interaction patterns shown in Figure 3, what are these interaction patterns? The current description in Section 5.1 is vague. Also, why are such interaction patterns selected? \n\n- The paper claims that the proposed UIGC can address the issues such as under-reaching and over-squashing of existing works. But there is no experiment provided to compare with existing works on how the proposed UIGC performs for solving such issues.\n\n- How is K selected? Why are only K=3 and K=8 considered?"
                },
                "questions": {
                    "value": "See the questions in W1-W4."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7293/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7293/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7293/Reviewer_uvYG"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7293/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698412909136,
            "cdate": 1698412909136,
            "tmdate": 1699636871658,
            "mdate": 1699636871658,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "nU1NUpQUzb",
                "forum": "vst5P4Pve2",
                "replyto": "BOji8ddp25",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7293/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7293/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We sincerely thank you for your time and valuable comments.\n\n**(W1) The key concepts of the paper (such as global/universal interaction, interaction sensitivity, and interaction expressiveness) are not well-defined.**\n\n- **The definition of global interaction is defined in terms of the channels of inputs and outputs. How does this relate to interaction patterns, e.g., the five synthetic interaction patterns in Section 5.1? Also, how does this relate to node/pair symmetry?**\n\n  Global interaction as in Eq. 2 defines a $n\\times n$ matrix, where each entry corresponds to a node pair. An interaction pattern refers to one such matrix with specific entry values. However, we do not need to deal with all $n\\times n$ entries as the symmetric pairs always have the same entry values, which we call **symmetry bias** of GNNs (Please refer to the modification Section 3.2 in the updated PDF). So, let $\\eta$ denote the number of symmetric partitions, an interaction pattern is an assignment of $\\eta$ partitions. The 5 synthetic interaction patterns in Section 5.1 are 5 different assignments on the $\\eta$ partitions.\n\n- **For \"how a model can universally express any desired interaction within a given graph\", what does \"universally\" mean? The paper also mentions universal interaction but no formal definition is provided.**\n\n  We refine the statement to \"how a model can universally approximate any interaction\", as universal approximation is a more well-adopted concept [1]. Then \"universal interaction\" means a model's capability to approximate any interaction with proper neural parameter assignments. This concept aligns with the idea of universal filters, as explored in various graph signal filtering studies [2, 3, 4, 5]. Please refer to the middle part of Page 2 in the updated PDF for further details.\n\n- **Interaction sensitivity is defined to measure the sensitivity of model outputs to perturbations in input node features. However, perturbations in input node features are not the same as interactions between node pairs - so how are they related? Also, why is interaction expressiveness considered as one aspect of interaction efficiency?**\n\n  As in Eq. 2, the global interaction $\\mathrm J_{\\theta,W}\\in\\mathbb R^{n\\times n}$ includes interactions among all node pairs ($\\mathrm J_{i,j}\\in\\mathbb R$ indicates the interaction between nodes $i$ and $j$), and it can be formalized as the Jacobian of all node features before and after GNN operations. That is why \"perturbations in input node features\" and \"interactions between node pairs\" are related. The determinant of the Jacoabian is commonly used in sensitivity measurement, so we use it to study interaction sensitivity, i.e. the rate of change of outputs with respect to the perturbation of inputs. Briefly, GNNs can learn different interactions $\\mathrm J_{\\theta,W}$ for the underlying graph, some interactions with small $|\\mathrm J_{\\theta,W}|$ refer to the change of model output less sensitive to the perturbations of model input.\n  We employ the term 'interaction efficiency' to convey the ability to effectively model diverse and complex interactions. Analogous to filter expressiveness [2, 3, 4, 5], interaction expressiveness gauges the extent to which a model can express/approximate interactions. It serves as the fundamental criterion for interaction efficiency."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7293/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700311169928,
                "cdate": 1700311169928,
                "tmdate": 1700311169928,
                "mdate": 1700311169928,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rS2rmFursE",
                "forum": "vst5P4Pve2",
                "replyto": "kMh6ThSMVZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7293/Reviewer_uvYG"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7293/Reviewer_uvYG"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for the detailed responses. I don't have further questions. Nonetheless, some of my major concerns remain. For example, I'm not convinced that the concepts of global/universal interaction and interaction expressiveness are well-defined. In the revised version, the authors mentioned that \"Interaction expressiveness shares a similar concern with graph filter expressiveness (Balcilar et al., 2021)\". However, the paper by Balcilar et al., 2021 mainly provides some perspective to bridge spectral GNNs and spatial GNNs. There is no specific notion/framework for graph filter expressiveness."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7293/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700699020577,
                "cdate": 1700699020577,
                "tmdate": 1700699020577,
                "mdate": 1700699020577,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Em3Hm7cT2U",
            "forum": "vst5P4Pve2",
            "replyto": "vst5P4Pve2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7293/Reviewer_vtTz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7293/Reviewer_vtTz"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the expressiveness of graph neural networks. In particular, it focuses on the \"global efficiency\" of GNNs, and the analytical tool this paper proposed is to study the Jacobian matrix of the graph convolution layer."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "S1. This paper is of good presentation.\n\nS2. The analytic tool proposed by this paper, Jacobian matrix of the graph convolution layer and its determinant, is reasonable.\n\nS3. Experiments of this paper include broad baseline methods and datasets.\n\nS4. This paper is open-sourced to ensure its reproducibility."
                },
                "weaknesses": {
                    "value": "W1. A part of the theory proposed by this paper is problematic.\n\nW2. Some existing contributions are not clearly mentioned, which somehow undermines the contribution of this paper.\n\nW3. Writing in Section 4 can be improved to make the proposed method more clear.\n\nW4. A minor weakness is that the baselines can be selected to only include the most SOTA baselines, and try to ensure most baselines have results on all the datasets (but not only report the results from the original papers)"
                },
                "questions": {
                    "value": "Q1. Seems Proposition 5 conflicts with the core idea of this paper. If I understand correctly, the core idea of this paper is \"to ensure all symmetric pairs, i.e., within the same pair partitions, learn the same interaction\" (from the bottom of Page 5). However, one of the assumptions of Proposition 5 says that $\\varphi$ is an injective function, which conflicts with the idea that \"symmetric\" but different node pairs can learn the same interaction.\n\nQ2.The core design of the proposed UIGC is the $\\varphi^{LE}$. However, it is not clearly mentioned what the specific $\\varphi^{LE}$ is used in this paper. Table 4 and Figure 4 only show the $\\varphi^{LE}$ of existing methods.\n\nQ3. The study on global interaction efficiency is not new. For example, this paper cites [1], which studies \u201ctotal resistance\u201d $R_{tot}$, a quantitative measure of global interaction efficiency. Thus, the authors should revise the statement of their first contribution. Furthermore, it does not seem necessary to distinguish \u201cglobal\u201d and \u201clocal\u201d interaction efficiency because what the authors mean by \u201cglobal\u201d is just a function (i.e., determinant, in Section 3.1) of the \u201clocal\u201d interaction efficiency matrix ($\\mathbf{J}_{\\theta,W}$ in Eq. (2)).\n\nQ4. Their UIGC might not be scalable because UIGC needs to compute graph automorphism, which seems to require at least $O(mn^2)$ time. A more efficient approach to addressing the long-distance interaction issue might be increasing the GNN depth together with techniques to alleviate oversquashing (e.g., [2,3] can train GNNs with 7~1000 layers.), or as simple as adding a supernode. Authors should compare with such methods in terms of both efficiency and performance in their experiments.\n\n**Two comments/suggestions:**\n\nC1. The applicable scope of graph automorphism might be limited. A classic result (see, e.g., theorem 2 in [4]) shows that almost all graphs do not have non-trivial automorphism, i.e., in almost all graphs, all but at most one pair of nodes are non-equivalent. In practice, only very special graphs like locally symmetric molecules have non-trivial automorphism. This limitation is also supported by their experiments because their UIGC has at most marginal gain on IMDB and RDT (social network datasets).\n\nC2. The discussion at the end of Section 4 looks vague. It might be more meaningful and more interesting if the authors can extend the discussions to rigorous theoretical analysis.\n\n[1] Black, Mitchell, Zhengchao Wan, Amir Nayyeri, and Yusu Wang. \"Understanding oversquashing in gnns through the lens of effective resistance.\" ICML 2023.\n\n[2] Li et al. DeepGCNs: Can GNNs go as deep as CNNs? ICCV 2019.\n\n[3] Li et al. Training graph neural networks with 1000 layers. ICML 2021.\n\n[4] Erd \u030bos & R \u0301enyi. Assymetric graphs. Acta Math. Acad. Sci. Hungar., 14:295\u2013315, 1963."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7293/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7293/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7293/Reviewer_vtTz"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7293/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698791625646,
            "cdate": 1698791625646,
            "tmdate": 1699636871537,
            "mdate": 1699636871537,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "LcYFgs9otT",
                "forum": "vst5P4Pve2",
                "replyto": "Em3Hm7cT2U",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7293/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7293/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We sincerely thank you for your time and valuable comments.\n\n\n**Q1: Clarifications on Proposition 5**\n\nProposition 5 serves as a general and self-consistent conclusion, with no need to specify symmetric or non-symmetric cases. Specifically, for the symmetric case, although injective $\\varphi$ will generate different outputs for symmetric pairs, the MLP $f_{\\Theta}$ which takes the outputs of $\\varphi$ as inputs can learn the same interaction for them. So Proposition 5 always holds, whether considering the symmetry or not. Then, to ensure symmetric pairs always learn the same interactions, we relax the injectivity of $\\varphi$ by ensuring that symmetric pairs always share the same output as in Equation 6.\n\nWe note that the related statement may result in such confusion, and we update the related statements in the updated PDF to make it clearer.\n\n\n**Q2: What is the specific $\\varphi^{\\textrm{LE}}$ is used in this paper?**\n\nApart from existing design of $\\varphi^{\\textrm{LE}}$, we propose the new one $[(\\tilde D^{\\epsilon}\\tilde A\\tilde D^{-1-\\epsilon})^k]\\_{k\\in[K],\\epsilon\\in[-1,0]}$. It preserves the benefits of two popular designs while avoiding their respective drawbacks: Compared with $[(\\tilde D^{-1}\\tilde A)^k]\\_{k\\in[K]}$ in [1], it provides more diverse pair encodings, and compared with $[(\\tilde D^{\\epsilon}\\tilde A\\tilde D^{\\epsilon})^k]\\_{k\\in[K],\\epsilon\\in[-1,0]}$ in [2], it has the bounded spectrum thus can be easier to handle numerical instability when applying a larger $K$.\n\nGiven the flexible nature of the $\\varphi^{\\textrm{LE}}$ design, our primary focus lies in understanding its general properties, such as the partial order among implementations and the presence of symmetry bias. As a result, we include our implementation of $\\varphi^{\\textrm{LE}}$ in the appendix rather than the main body of the updated PDF.\n\n\n\n\n**Q3: Comparisons with \u201ctotal resistance\u201d [3] & Possible misunderstandings on global interactions**\n\nOur interaction efficiency analysis involves interaction sensitivity and interaction expressiveness respectively. The former one may be comparable to \"total resistance\" $R_{tot}$ [3] as they all measure the property of the entire graph with a scalar. But the considered metrics are different. We use the determinant of the Jacobian of all nodes. It is a commonly used metric in stability and sensitivity analysis, and in our settings, it captures the rate of change of all output nodes to changes in all input nodes. The total resistance study extends local pairwise effective resistance to the global scenario by summing of the effective resistance between all pairs of nodes.\n\n\"Furthermore, it does not seem necessary to distinguish 'global' and 'local' interaction efficiency because ...\" seems to be a misunderstanding. Here is the clarification:\n\nOur work aims to extend existing individual pair interaction studies, e.g. long-range interaction, to the global scenario. To this end, we use $\\mathrm J_{\\theta,W}\\in\\mathbb R^{n\\times n}$ in Eq. 2 to model global interactions. Based on $\\mathrm J_{\\theta,W}$, we analyze\n\n- interaction sensitivity, measured by its determinant $|\\mathrm J_{\\theta,W}|$ in Section 3.1, and\n- interaction expressiveness, measured by the space $\\{\\mathrm J_{\\theta,W}\\big|\\theta\\in\\mathbb R^k,W\\in\\mathbb R^{\\star}\\}$ in Section 3.2.\n\nWe added discussions on the difference with [3] in Section 1 in the updated PDF.\n\n\n\n**Q4: The scalability of UIGC**\n\nOur UIGC does not require the computation of graph automorphisms. Notably, learning the same interactions for symmetric pairs (where symmetry is induced by graph automorphisms) is inherently satisfied by all GNNs, including ours. This is facilitated by the **symmetry bias** inherent in $\\varphi^{\\textrm{LE}}$, as discussed in Section 4. The mention of graph automorphisms in our submission is solely for the purpose of interaction expressiveness analysis and does not introduce any additional computations."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7293/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700311010967,
                "cdate": 1700311010967,
                "tmdate": 1700311010967,
                "mdate": 1700311010967,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "YHmeIOxarb",
            "forum": "vst5P4Pve2",
            "replyto": "vst5P4Pve2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7293/Reviewer_6p3F"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7293/Reviewer_6p3F"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors aim to investigate the global interactions in graphs. In particular, they propose a metric named global efficiency for accessing GNNs' performance. It moves beyond the traditional approach of focusing on local interactions between individual node pairs and examines GNN interactions from a global perspective. Furthermore, inspired by the insights from these investigations, they propose Universal Interaction Graph Convolution (UIGC) with very superior interaction efficiency."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "*Originality*: The paper is original in its approach to a fundamental challenge in the field of GNNs. It introduces the novel concept of universal interaction expression within graphs, surpassing conventional limitations. The proposed solution to utilize the Jacobian matrix for quantifying interaction efficiency, distinguishing between interaction sensitivity and expressiveness, is original and useful.\n\n\n*Significance*: The paper has the potential to enhance the capabilities of GNNs from a new perspective."
                },
                "weaknesses": {
                    "value": "*Motivation*: It would be helpful if the authors could provide better motivations for some of their arguments and designs. For instance, throughout the paper, the authors are discussing about pair interactions. However, it is not very clearly demonstrated why they are so important. \n\n*Experiments*: The results in Table 2 and Table 3 show that the proposed method achieves strong performance on some of the datasets while falling short on some others. It would be helpful if the authors could provide more explanations on why this is the case. Some investigations into the data properties and their connections to global interactions would be especially helpful."
                },
                "questions": {
                    "value": "Please address the questions in the Section of weakness. In addition, there are a few other questions as follows.\n\n1. In Figure 1, why do we specifically care about pairs of nodes and their interactions? It would be better if the authors could provide more details and better descriptions. \n\n2. It might be helpful if the authors could provide some examples on how the proposed method is more expressive in terms of capturing patterns such as \"benzene rings\" (mentioned in the introduction). \n\n3. The setup for Section 5.1 is not very clear. It would be helpful if the authors could provide more descriptions of the synthetic interaction patterns. In particular, what is the value in the y-axis in Figure 3?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7293/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698986016716,
            "cdate": 1698986016716,
            "tmdate": 1699636871431,
            "mdate": 1699636871431,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "LR5WzvNtwz",
                "forum": "vst5P4Pve2",
                "replyto": "YHmeIOxarb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7293/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7293/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We sincerely thank you for your time and valuable comments.\n\n**W1, Q1, Q2: Why do we specifically care about pairs of nodes and their interactions? & Why do we study global interactions?**\n\n- Why do we specifically care about pairs of nodes and their interactions?\n\n  In a graph, one node can influence another node through direct or indirect connections. Pair interactions serve as a fundamental component to model this mutual influence. For instance, existing studies on long-range interactions concentrate on scenarios where a pair of nodes is distant from each other, demonstrating that their pair interaction decays exponentially with respect to the distance [1, 2].\n\n- Why we should study global interactions which consider all pair interactions simultaneously?\n\n  The influence among distinct pairs of nodes is not independent. Certain information can only be effectively captured when examined in a 'simultaneous' rather than an 'individual' manner. To explain this, we replace the \"benzene rings\" example with a more intuitive one: considering the scenario where A, B, and C co-author a paper together, where the relationship cannot be accurately represented by breaking it down into three separate relations like 'A and B co-author a paper together,' 'B and C co-author a paper together,' and 'A and C co-author a paper together.' The reason is that these individual relations also encompass cases where A&B, B&C, and A&C co-author in three distinct papers, respectively. To address this, we adopt a global interactions perspective, where we consider all node pairs and their interactions simultaneously, as illustrated in Figure 1.\n\nInteraction expressiveness shares a similar concern with graph filter expressiveness [3]. For a graph with $n$ nodes, The corresponding Laplacian involves $n$ graph Fourier bases, each associated with a filtering coefficient. The concurrent consideration of filtering across different bases results in a filter space denoted as $\\mathbb R^n$ [4, 5, 6, 7]. Recent studies emphasize the necessity to enhance filter expressiveness to effectively span and cover this space. In a parallel analogy, within the framework of global interactions, each node pair is assigned an interaction coefficient. To view all pair interactions simultaneously, the dimensions of the interaction space is related to the number of pairs.\n\nWe have refined the description of our motivations in Section 1 in the updated PDF.\n\n\n**W2: Experiments: The results in Table 2 and Table 3 show that the proposed method achieves strong performance on some of the datasets while falling short on some others.**\n\nIn UIGC, we utilize the MLP $f_{\\Theta}$ to model interactions. The classification improvement on small-scale datasets shows the alleviation of the overfitting issue. However, datasets like IMDB-B have no classification gains, indicating that label-related interaction patterns on these graphs may be inherently simple and can be easily captured by basic models.\n\n\n**Q3: The setup for Section 5.1**\n\nWe refined the description of the setup in Section 5.1 in the updated PDF. In Section 5.1, we test five interaction patterns with different approximation difficulties, including $h(k)=\\alpha$,  $h(k)=\\alpha k$, $h(k)=k^{\\alpha}$, $h(k)=\\sin(\\alpha k)$ and $h(k)=\\alpha\\lceil k/40\\rceil$, where $k$ is the pair partition index, $h(k)$ is the synthetic interaction, $\\alpha$ is a scalar used to control the output. In Figure 3, x-axis is the partition index $k$ and y-axis is the interaction $h(k)$.\n\n[1] Topping, Jake, et al. \"Understanding over-squashing and bottlenecks on graphs via curvature.\" International Conference on Learning Representations. 2021.\n\n[2] Liu, Juncheng, et al. \"Mgnni: Multiscale graph neural networks with implicit layers.\" Advances in Neural Information Processing Systems 35 (2022): 21358-21370.\n\n[3] Balcilar, Muhammet, et al. \"Analyzing the expressive power of graph neural networks in a spectral perspective.\" Proceedings of the International Conference on Learning Representations (ICLR). 2021.\n\n[4] He, M., Wei, Z., & Xu, H. (2021). Bernnet: Learning arbitrary graph spectral filters via bernstein approximation. *Advances in Neural Information Processing Systems*, *34*, 14239-14251.\n\n[5] Yang, Mingqi, et al. \"A new perspective on the effects of spectrum in graph neural networks.\" *International Conference on Machine Learning*. PMLR, 2022.\n\n[6] Wang, Xiyuan, and Muhan Zhang. \"How powerful are spectral graph neural networks.\" *International Conference on Machine Learning*. PMLR, 2022.\n\n[7] Bo, Deyu, et al. \"Specformer: Spectral Graph Neural Networks Meet Transformers.\" *The Eleventh International Conference on Learning Representations*. 2022."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7293/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700310869359,
                "cdate": 1700310869359,
                "tmdate": 1700310869359,
                "mdate": 1700310869359,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]