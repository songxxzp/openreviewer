[
    {
        "title": "Improving Language Models via Plug-and-Play Retrieval Feedback"
    },
    {
        "review": {
            "id": "otKGk4nOMi",
            "forum": "EyfOZKXpcN",
            "replyto": "EyfOZKXpcN",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission407/Reviewer_fuhX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission407/Reviewer_fuhX"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel pipeline, REFEED, to provide LLMs with automatic retrieval feedback in a plug-and-play manner, without the need of expensive fine-tuning.\nREFEED includes advanced modules to improve the proposed pipeline, specifically diversifying the initial generation outputs and ensembling initial and post-feedback outputs.\nThat is, REFEED first generates initial outputs, then utilizes a retrieval model to acquire relevant information from large document collections. \nThen, the retrieved information is incorporated into the in-context demonstration to refine the initial outputs, which is more efficient and cost-effective than human feedback or fine-tuning."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* REFEED is simple architecture that improves large language model in a plug-and-play framework. \n* This architecture and a retrieval method allows REFEED a practical and efficient solution without the need for expensive fine-tuning.\n* To produce more reliable and accurate answers and mitigate the risk of misleading retrieval feedback, REFEED has equipped with two newly introduced modules, diverse answer generation and an ensemble approach."
                },
                "weaknesses": {
                    "value": "* While REFEED looks promising approach to improve large language model in a plug-and-play framework, \nits goals are not specifically clarified.\nBecause of the wide range of goals, it is necessary to compare and discuss with other approaches, such Prompt tuning, Chain-of-Thought (CoT), Tree of Thoughts (ToT), and, Retrieval Augmented Generation (RAG) have not been described.\nFigure 1, 2, and 3 can be interpreted as a kind of RAG.\n* Backbone language models are limited, and it is difficult to determine whether the effect is due to the approches or the emergent nature of the language model.\n* Reproducibility, limitations, and lack of qualitative evaluation do not confirm its validity. Table 1 is inadequate because the settings are not clear."
                },
                "questions": {
                    "value": "* Pease explian why you chose text-davinci-003 and Code-Davinci-002 (Codex) as the backbone language models?\n* Can REFEED avoid generating incorrect or hallucinated information?\n* See Weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission407/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698486690579,
            "cdate": 1698486690579,
            "tmdate": 1699635967395,
            "mdate": 1699635967395,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GUl56lYa0x",
                "forum": "EyfOZKXpcN",
                "replyto": "otKGk4nOMi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission407/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission407/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer fuhX [1/2]"
                    },
                    "comment": {
                        "value": "Dear Reviewer fuhX,\n\nThanks very much for your review! Here are the responses to your questions/weakness.\n\n- Q1: Pease explian why you chose text-davinci-003 and Code-Davinci-002 (Codex) as the backbone language models? Backbone language models are limited, and it is difficult to determine whether the effect is due to the approches or the emergent nature of the language model.\n\n\nThank you for highlighting the importance of backbone model selection. Our initial decision to use Davinci-003 and Codex as backbone models was made to ensure a fair comparison with baseline methods (e.g., GenRead, RePLUG) that we have discussed in the paper. \n\nBesides, the ChatGPT and GPT-4 system might involve many user data for instruction tuning, which could have contamination with many academic benchmark dataset, such as NQ and TriviaQA. \n\nLastly, in OpenAI\u2019s announcement that both ChatGPT and GPT-4 will be subject to ongoing updates in their model parameters. These continual modifications would lead to non-reproducible experiments, potentially compromising the reliability of our research outcomes.\n\nIn response to your concern, we have conducted additional experiments using ChatGPT (gpt-3.5 turbo) and GPT-4, which reinforce the adaptability and effectiveness of our approach across different model architectures. These results are now detailed in the revised version of our paper, providing a comprehensive view of our method's applicability.\n\n\n| Experiment on ChatGPT (gpt-3.5-turbo)                  | NQ          | TriviaQA    | HotpotQA    |\n|--------------------------------------------------------|-------------|-------------|-------------|\n| Closebook question answering (Question -> Answer)      | 32.3 / 39.9 | 65.6 / 69.5 | 23.5 / 24.0 |\n| Retrieve-then-Read (Question -> Retrieval -> Answer)   | 34.3 / 41.5 | 58.7 / 63.7 | 31.7 / 33.6 |\n| ReFeed (Question -> Answer -> Retrieval -> Refinement) | 37.5 / 48.1 | 66.3 / 71.1 | 34.1 / 36.0 |\n\n\n| Experiment on GPT4 (gpt-4)                             | NQ          | TriviaQA    | HotpotQA    |\n|--------------------------------------------------------|-------------|-------------|-------------|\n| Closebook question answering (Question -> Answer)      | 34.8 / 49.6 | 64.6 / 72.8 | 30.8 / 33.7 |\n| Retrieve-then-Read (Question -> Retrieval -> Answer)   | 32.5 / 46.5 | 59.9 / 67.1 | 31.6 / 37.5 |\n| ReFeed (Question -> Answer -> Retrieval -> Refinement) | 36.8 / 54.4 | 66.3 / 74.0 | 36.9 / 42.6 |\n\nFrom the above table, we can obserbe:\n\n(1) The retrieval-feedback consistently improves the system, compared to both standard closebook QA method, and retrieve-then-read pipeline.\n\n(2) The traditional retrieve-then-read system even sometimes hurt by noisy retrieval. On the contrary, our ReFeed could avoid misled issue if the first-round generation is good. \n\n\n- Q2: Can REFEED avoid generating incorrect or hallucinated information?\n\nYes, this is the main contribution of ReFeed, avoiding hallucination via using retrieval feedback to validate the correctness of generated output. The evaluations are mainly based on open-domain factual QA, the improvement reveals the ReFeed could greatly reduce the hallucination / improve factuality. Figure 5 in the paper show some cased scenarios. \n\n- Q3: Reproducibility, limitations, and lack of qualitative evaluation do not confirm its validity. Table 1 is inadequate because the settings are not clear.\n\nThank you for your suggestions. We acknowledge these aspects are crucial for confirming the validity of our research.\n\nRegarding reproducibility, while this was not included in the main paper, comprehensive details have been provided in Appendices A.1 and A.2. Additionally, to further ensure reproducibility, we will make all codes and data publicly available for sure.\n\nRegarding the limitations of our study, we have addressed this by adding a dedicated section on page 9 of our paper. This section aims to provide a transparent overview of the potential constraints and boundaries of our research.\n\nRegarding qualitative evaluation, all methods were evaluated in a few-shot QA/generation setting. Although the domains vary, with focusing on commonsense QA [1] and on open-domain QA [2], methods share the objective of using feedback to improve the performance of large language models like GPT-3. Table 1 mainly reveals the main differences in method design and application domain, without mentioning their implementation details.\n\n[1] Rethinking with retrieval: Faithful large language model inference. \n\n[2] Check your facts and try again: Improving large language models with external knowledge and automated feedback."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission407/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700713517480,
                "cdate": 1700713517480,
                "tmdate": 1700713517480,
                "mdate": 1700713517480,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9h9alAJAb5",
                "forum": "EyfOZKXpcN",
                "replyto": "otKGk4nOMi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission407/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission407/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer fuhX [2/2]"
                    },
                    "comment": {
                        "value": "- Q4: Related work discussion on Prompt tuning, Chain-of-Thought (CoT), Tree of Thoughts (ToT), and, Retrieval Augmented Generation (RAG).\n\n\nThank you for incorporating the related work discussion into your paper. Here's a polished summary of the differences between your work and existing approaches:\n\nFirst, in comparison to Retrieval-Augmented Generation (RAG), our \\textsc{ReFeed} method also leverages retrieval as a crucial component of our pipeline. However, unlike conventional methods that directly use retrieval to enhance model performance in complex reasoning and factual accuracy, our research introduces a novel application. We demonstrate that retrieved documents can be effectively employed as feedback to refine language model outputs, significantly improving factual accuracy.\n\nSecond, our approach diverges from recent advancements such as Chain-of-Thought, Tree-of-Thought reasoning, and Self-Refinement. These methods do not utilize external knowledge to augment language model reasoning and factual accuracy. Instead, they concentrate on improving the language model's reasoning abilities through diverse prompt designs."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission407/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700713530135,
                "cdate": 1700713530135,
                "tmdate": 1700713530135,
                "mdate": 1700713530135,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wcmvpqvXwc",
                "forum": "EyfOZKXpcN",
                "replyto": "9h9alAJAb5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission407/Reviewer_fuhX"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission407/Reviewer_fuhX"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Authors"
                    },
                    "comment": {
                        "value": "Thank you for your answer. \n\nYou have helped me to better understand."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission407/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700729396323,
                "cdate": 1700729396323,
                "tmdate": 1700729396323,
                "mdate": 1700729396323,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "EofcCskdg4",
            "forum": "EyfOZKXpcN",
            "replyto": "EyfOZKXpcN",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission407/Reviewer_Jo78"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission407/Reviewer_Jo78"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents ReFeed, a retrieve-then-read pipeline for knowledge-intensive tasks. This pipeline involves three processes: generate the initial output only given the question, retrieve supporting documents using the question and the initial output, and finally refine the previous output. The authors further propose two enhancements: diversifying retrieval feedbacks and ensembling initial and post-feedback outputs."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The proposed method is straightforward and effective."
                },
                "weaknesses": {
                    "value": "- The paper's novelty is in question, as the process of using an initial output for retrieval and then refining it does not appear to be a novel approach, and the absence of citation to related work, such as [1], raises concerns. Additionally, the paper lacks in-depth insights into the understanding of retrieve-then-read pipelines. The paper altogether seems more like a system report that shows the effectiveness of each trick, not an academic research.\n- Typos\n    - In Section 4.3.1, the second paragraph contains several sentences that are missing the subject.\n    - In Section 4.3.2, the paragraph labeled \"Module-1 \u2026\" mentions \"as shown in 4\", which should be corrected to \"as shown in Figure 4\u201d.\n\n[1] Jiang et al. Active Retrieval Augmented Generation. 2023."
                },
                "questions": {
                    "value": "How are the two enhanced modules integrated together?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission407/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698757305814,
            "cdate": 1698757305814,
            "tmdate": 1699635967314,
            "mdate": 1699635967314,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "FpwWHRaJgL",
                "forum": "EyfOZKXpcN",
                "replyto": "EofcCskdg4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission407/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission407/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Jo78"
                    },
                    "comment": {
                        "value": "Dear Reviewer Jo78,\n\nThanks very much for your review! Here are the responses to your questions/weakness.\n\n- Q1: The paper's novelty is in question, as the process of using an initial output for retrieval and then refining it does not appear to be a novel approach, and the absence of citation to related work, such as [1], raises concerns. Additionally, the paper lacks in-depth insights into the understanding of retrieve-then-read pipelines. The paper altogether seems more like a system report that shows the effectiveness of each trick, not an academic research.\n\nWe acknowledge the concerns regarding the recent work by Jiang et al. on Active Retrieval Augmented Generation (2023). **It's important to note that our paper was published within 10 days on a third-party open-access repository. Due to the anonymity policy, we cannot specify the exact repository and date, but we emphasize that our work was indeed novel at the time of its publication and is contemporary with Jiang et al.'s research.**\n\nDespite the similarities in the pipeline (generate-retrieve-regenerate) between Active RAG and our ReFeed approach, we offer distinct contributions. Firstly, one-pass decoding in Active RAG may limit potential matches to diverse relevant information. Our research demonstrates that multi-pass decoding, or diverse generation, can facilitate more comprehensive retrieval feedback based on varying outputs.\n\nSecondly, we address a significant issue in the pipeline where retrieved documents might mislead the language model, leading to a correct output being inaccurately revised. We propose an ensemble strategy that allows for the re-evaluation of answer trustworthiness, thereby enhancing the reliability of the output. This aspect of our research presents a novel approach to managing the complexities inherent in retrieve-then-read pipelines.\n\n- Q2: How are the two enhanced modules integrated together?\n\nThe two enhanced modules are built in two different steps. First, the diverse generation is built in the first step when generating initial answer, instead of generating only one answer, the system generates multiple diverse answer via sampling. Then, we use each generated initial answer to retrieve a set of documents and remove the duplicated documents. Lastly, the ensemble method is built in the second step when producing the final answer. The final output will be based on all initial answer likelihood and refined answer likelihood. \n\n\n- Q3: Typos.\n\nThank you for pointing out the typos. We have revised them and updated the paper accordingly."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission407/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700713444145,
                "cdate": 1700713444145,
                "tmdate": 1700713444145,
                "mdate": 1700713444145,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "c5GypHEFH2",
            "forum": "EyfOZKXpcN",
            "replyto": "EyfOZKXpcN",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission407/Reviewer_KK6a"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission407/Reviewer_KK6a"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses factualness in QA systems. The idea of REFEED is as follow: 1) generate an output, 2) use retrieve relevant information from large document collections. 3) Integrate the latter into the prompt and refine the answer. The pipeline is plug-and-play and does not require any fine-tuning. The overall approach is more efficient and cost-effective than human feedback.\n\nMore in depths, REFEED generates multiple diverse outputs. For each, REFEED uses retrieval conditioned on the input and output instead of the input only, which makes it different than standard RAG approaches. The retriever (e.g., BM25) identifies the top-k relevant documents and remove duplicates. During refinement, the retrieved documents are integrated into the prompt along the outputs. However, there is a likelihood that the retrieved documents are misaligned with the output. The authors circumvent this problem by taking the average language modeling probability to rank the generated outputs before and after incorporating the retrieved documents. Then by comparing the different, the authors pick the first or refined output.\n\nThe experiments are based on single-hop QA, TriviaQA, multi-hop QA, and WoW. The models used are davinci-002 and davinci-003. The baselines are fair. The performance in the zero/few-shot experiments are convincing. The ablation study highlights the necessity of the multiple-output generation and the ensemble proposed to identify whether a refined output is better. Finally, the authors show how their approach can be integrated into chain-of-prompt and even improved the results.\n\nOverall, this is a good paper, well written and structured. The idea, while simple, is novel. My only concern would be whether the proposed approach would work for other models than davinci-002/3 (see also related question regarding the calibration). I would ask the authors to experiment with one or two others LLMs (e.g., T5)."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Simple but effective method\n- Strong results"
                },
                "weaknesses": {
                    "value": "- It is unclear whether the proposed approach would work with another backbone than davinci\n- I'm skeptical that taking average probabilities of the output before and after the refinement would work in all models"
                },
                "questions": {
                    "value": "- How would verify that the LLM is well-calibrated in order to decide whether a refinement is more plausible or not? If the model is not well-calibrated, how would you proceed?\n- How would the approach generalize for other models than davinci?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission407/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698770599006,
            "cdate": 1698770599006,
            "tmdate": 1699635967232,
            "mdate": 1699635967232,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wLPYJvtz6x",
                "forum": "EyfOZKXpcN",
                "replyto": "c5GypHEFH2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission407/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission407/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer KK6a"
                    },
                    "comment": {
                        "value": "Dear Reviewer KK6a,\n\nThanks very much for your review! Here are the responses to your questions/weakness.\n\n- Q1: It is unclear whether the proposed approach would work with another backbone than davinci. How would the approach generalize for other models than davinci?\n\n\n\nThank you for highlighting the importance of backbone model selection. Our initial decision to use Davinci-003 and Codex as backbone models was made to ensure a fair comparison with baseline methods (e.g., GenRead, RePLUG) that we have discussed in the paper. \n\nBesides, the ChatGPT and GPT-4 system might involve many user data for instruction tuning, which could have contamination with many academic benchmark dataset, such as NQ and TriviaQA. \n\nLastly, in OpenAI\u2019s announcement that both ChatGPT and GPT-4 will be subject to ongoing updates in their model parameters. These continual modifications would lead to non-reproducible experiments, potentially compromising the reliability of our research outcomes.\n\nIn response to your concern, we have conducted additional experiments using ChatGPT (gpt-3.5 turbo) and GPT-4, which reinforce the adaptability and effectiveness of our approach across different model architectures. These results are now detailed in the revised version of our paper, providing a comprehensive view of our method's applicability.\n\n\n| Experiment on ChatGPT (gpt-3.5-turbo)                  | NQ          | TriviaQA    | HotpotQA    |\n|--------------------------------------------------------|-------------|-------------|-------------|\n| Closebook question answering (Question -> Answer)      | 32.3 / 39.9 | 65.6 / 69.5 | 23.5 / 24.0 |\n| Retrieve-then-Read (Question -> Retrieval -> Answer)   | 34.3 / 41.5 | 58.7 / 63.7 | 31.7 / 33.6 |\n| ReFeed (Question -> Answer -> Retrieval -> Refinement) | 37.5 / 48.1 | 66.3 / 71.1 | 34.1 / 36.0 |\n\n\n| Experiment on GPT4 (gpt-4)                             | NQ          | TriviaQA    | HotpotQA    |\n|--------------------------------------------------------|-------------|-------------|-------------|\n| Closebook question answering (Question -> Answer)      | 34.8 / 49.6 | 64.6 / 72.8 | 30.8 / 33.7 |\n| Retrieve-then-Read (Question -> Retrieval -> Answer)   | 32.5 / 46.5 | 59.9 / 67.1 | 31.6 / 37.5 |\n| ReFeed (Question -> Answer -> Retrieval -> Refinement) | 36.8 / 54.4 | 66.3 / 74.0 | 36.9 / 42.6 |\n\nFrom the above table, we can obserbe:\n\n(1) The retrieval-feedback consistently improves the system, compared to both standard closebook QA method, and retrieve-then-read pipeline.\n\n(2) The traditional retrieve-then-read system even sometimes hurt by noisy retrieval. On the contrary, our ReFeed could avoid misled issue if the first-round generation is good. \n\n\n- Q2: I'm skeptical that taking average probabilities of the output before and after the refinement would work in all models. How would verify that the LLM is well-calibrated in order to decide whether a refinement is more plausible or not? If the model is not well-calibrated, how would you proceed?\n\nThanks for your question! Indeed, there is a fundamental limitation in ensuring that the language model is appropriately calibrated. And we admit our method should be based on a relatively well-calibrated language model. \n\nIf the model is not well-calibrated, one way is to better measure uncertainty using semantic-based metrics [1], instead of using probability only. \n\nWe add a limitation discussion in the paper to point out the issue and remind our ensemble strategy could only be applied on well-calibrated model. Besides that, our main retrieval feedback framework is not relying on model calibration."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission407/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700713352171,
                "cdate": 1700713352171,
                "tmdate": 1700713352171,
                "mdate": 1700713352171,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "8SaU9jjnYF",
            "forum": "EyfOZKXpcN",
            "replyto": "EyfOZKXpcN",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission407/Reviewer_N6CA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission407/Reviewer_N6CA"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors propose to leverage the outputs of LLMs (i.e., the generated initial answers) to retrieve relevant documents for refining LLM outputs and quality. They also present two modules to enhance the performance by diversifying retrieval feedback and ensembling initial and post-feedback outputs. Experiments are conducted on several conventional QA benchmark datasets. The experimental results demonstrate the proposed method can improve the performance of different LLMs in both close- and open-book settings. The authors also conducted some ablation studies to show the effectiveness of each component."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* S1: The proposed framework is an easy-to-use and plug-and-play blackbox retrieval-augment approach.\n* S2: The improvements over baseline methods are significant across different datasets and settings.\n* S3: Each proposed component is validated through the ablation study"
                },
                "weaknesses": {
                    "value": "* W1: Some important details are missing, e.g., how to conduct de-duplication; \n* W2: Not applied to the state-of-the-art LLMs (e.g., GPT-4).\n* W3:Lack of discussions and analysis on how hyper-parameters affect the performance."
                },
                "questions": {
                    "value": "* Q1: Following W1, I would encourage the authors to describe the methods with details and motivation, especially when each component is shown effective individually. For instance, I wonder how the de-duplication is done to ensure diversity; and why BM25 is chosen instead of other retrieval methods (of course not just answering \"other methods also just used it\").\n\n*Q2: Following W2, I would really like to know how the proposed method can be applied to state-of-the-art models like GPT-4. Although it might not be reproducible for a certain metric number, it can still give some signs about the performance upper bound. Similarly, I wonder if the method can be applied to a conventional smaller neural language model (e.g., BART and T5) since the proposed is a plug-and-play method.\n\n*Q3: Following W3, there are still several hyper-parameters (e.g., k in the module-1) in the proposed method. I wonder how they affect the performance (instead of \"because other papers use this number\")."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission407/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699081299971,
            "cdate": 1699081299971,
            "tmdate": 1699635967168,
            "mdate": 1699635967168,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "PivATxw2Rk",
                "forum": "EyfOZKXpcN",
                "replyto": "8SaU9jjnYF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission407/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission407/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer N6CA [1/2]"
                    },
                    "comment": {
                        "value": "Dear Reviewer N6CA,\n\nThanks very much for your review! Here are the responses to your questions/weakness.\n\n- Q1: Following W1, I would encourage the authors to describe the methods with details and motivation, especially when each component is shown effective individually. For instance, I wonder how the de-duplication is done to ensure diversity; and why BM25 is chosen instead of other retrieval methods (of course not just answering \"other methods also just used it\").\n\nWe appreciate your emphasis on the need for detailed methodological descriptions. Regarding the de-duplication process, we first use each generated initial answer to retrieve a set of documents. For each document retrieved, we calculate a corresponding similarity score. Subsequently, we merge all retrieved documents and rank them based on their similarity scores, keeping only the top-k documents. This approach ensures that we maintain diversity in the information we gather and use.\n\nThere are two main reasons for choosing BM25 as our retrieval method. First, BM25 demonstrates excellent adaptability and generalizability across a variety of contexts. Second, dense retrieval methods, such as DPR, are often trained with QA datasets or synthetic query-document pairs. Therefore, evaluating such dense retrieval methods might disproportionately reflect their ability to provide retrieval feedback, as the retrieval more accurate than BM25. However, demonstrating that BM25 performs well within our retrieval feedback framework suggests that there is potential for significant improvement with more advanced retrieval methods.\n\n\nLastly, we want to summarize out motivation of each component:\n\n(1)\tGeneral Retrieval Feedback Pipeline: Language models often generate information that is either incorrect or hallucinated. The retrieved information allows the language model to reassess the initial outputs and, if necessary, refine them to produce new answers.\n\n(2)\tThe diverse retrieval strategy enhances the diversity of the outputs and enables more comprehensive retrieval feedback based on these varied outputs.\n\n(3)\tEnsemble method is motivated that there are times when the retrieved documents may mislead the language model, leading to a correct output being revised into an incorrect one. So, the ensemble technique takes into account both the initial and refined outputs, ultimately enhancing the overall performance.\n\n- Q2: Following W2, I would really like to know how the proposed method can be applied to state-of-the-art models like GPT-4. Although it might not be reproducible for a certain metric number, it can still give some signs about the performance upper bound. Similarly, I wonder if the method can be applied to a conventional smaller neural language model (e.g., BART and T5) since the proposed is a plug-and-play method.\n\n\nThank you for highlighting the importance of backbone model selection. Our initial decision to use Davinci-003 and Codex as backbone models was made to ensure a fair comparison with baseline methods (e.g., GenRead, RePLUG) that we have discussed in the paper. \n\nBesides, the ChatGPT and GPT-4 system might involve many user data for instruction tuning, which could have contamination with many academic benchmark dataset, such as NQ and TriviaQA. \n\nLastly, in OpenAI\u2019s announcement that both ChatGPT and GPT-4 will be subject to ongoing updates in their model parameters. These continual modifications would lead to non-reproducible experiments, potentially compromising the reliability of our research outcomes.\n\nIn response to your concern, we have conducted additional experiments using ChatGPT (gpt-3.5 turbo) and GPT-4, which reinforce the adaptability and effectiveness of our approach across different model architectures. These results are now detailed in the revised version of our paper, providing a comprehensive view of our method's applicability.\n\n\n| Experiment on ChatGPT (gpt-3.5-turbo)                  | NQ          | TriviaQA    | HotpotQA    |\n|--------------------------------------------------------|-------------|-------------|-------------|\n| Closebook question answering (Question -> Answer)      | 32.3 / 39.9 | 65.6 / 69.5 | 23.5 / 24.0 |\n| Retrieve-then-Read (Question -> Retrieval -> Answer)   | 34.3 / 41.5 | 58.7 / 63.7 | 31.7 / 33.6 |\n| ReFeed (Question -> Answer -> Retrieval -> Refinement) | 37.5 / 48.1 | 66.3 / 71.1 | 34.1 / 36.0 |\n\n\n| Experiment on GPT4 (gpt-4)                             | NQ          | TriviaQA    | HotpotQA    |\n|--------------------------------------------------------|-------------|-------------|-------------|\n| Closebook question answering (Question -> Answer)      | 34.8 / 49.6 | 64.6 / 72.8 | 30.8 / 33.7 |\n| Retrieve-then-Read (Question -> Retrieval -> Answer)   | 32.5 / 46.5 | 59.9 / 67.1 | 31.6 / 37.5 |\n| ReFeed (Question -> Answer -> Retrieval -> Refinement) | 36.8 / 54.4 | 66.3 / 74.0 | 36.9 / 42.6 |"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission407/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700713227881,
                "cdate": 1700713227881,
                "tmdate": 1700713274666,
                "mdate": 1700713274666,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3lDftdrKu8",
                "forum": "EyfOZKXpcN",
                "replyto": "8SaU9jjnYF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission407/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission407/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer N6CA [2/2]"
                    },
                    "comment": {
                        "value": "From the above table, we can obserbe:\n\n(1) The retrieval-feedback consistently improves the system, compared to both standard closebook QA method, and retrieve-then-read pipeline.\n\n(2) The traditional retrieve-then-read system even sometimes hurt by noisy retrieval. On the contrary, our ReFeed could avoid misled issue if the first-round generation is good. \n\n\n\n- Q3: Following W3, there are still several hyper-parameters (e.g., k in the module-1) in the proposed method. I wonder how they affect the performance (instead of \"because other papers use this number\").\n\nPrevious research, as mentioned in papers [1] and [2], indicates that increasing the value of 'k' beyond 10 leads to only marginal improvements while simultaneously contributing to higher complexity. Additionally, as noted in [3], expanding the context length excessively for LLMs can result in \u201clost in the middle issue\u201d, i.e., LLM ignores the middle part of a long text input.\n\nTo further investigate the effect of varying 'k', we conducted experiments using ChatGPT (gpt-3.5-turbo) on datasets such as NQ, TriviaQA, and HotpotQA. The results are shown in the following tables (metric: left EM, right accuracy). The experimental results are consistent with findings in [1] [2]. As shown in the Table when \u2018k\u2019 is set as 10 performs significantly better than when \u2018k\u2019 is set as 1 or 5, however, when \u2018k\u2019 is set as 20, the improvement is marginal over when \u2018k\u2019 is set as 10. \n\nWe also observe that when 'k' is set as a small number, such as 1, the language model could be misled because it tends to trust the only one given documents. \n\n| K=1 with ChatGPT (gpt-3.5-turbo)                       | NQ          | TriviaQA    | HotpotQA    |\n|--------------------------------------------------------|-------------|-------------|-------------|\n| Closebook question answering (Question -> Answer)      | 32.3 / 39.9 | 65.6 / 69.5 | 23.5 / 24.0 |\n| Retrieve-then-Read (Question -> Retrieval -> Answer)   | 23.6 / 30.0 | 40.5 / 44.8 | 23.2 / 24.0 |\n| ReFeed (Question -> Answer -> Retrieval -> Refinement) | 34.0 / 41.9 | 65.9 / 68.7 | 27.1 / 28.3 |\n\n| K=5 with ChatGPT (gpt-3.5-turbo)                       | NQ          | TriviaQA    | HotpotQA    |\n|--------------------------------------------------------|-------------|-------------|-------------|\n| Closebook question answering (Question -> Answer)      | 32.3 / 39.9 | 65.6 / 69.5 | 23.5 / 24.0 |\n| Retrieve-then-Read (Question -> Retrieval -> Answer)   | 30.5 / 37.6 | 50.4 / 55.4 | 28.5 / 29.0 |\n| ReFeed (Question -> Answer -> Retrieval -> Refinement) | 34.3 / 42.1 | 67.5 / 70.8 | 30.0 / 30.4 |\n\n| K=10 with ChatGPT (gpt-3.5-turbo)                      | NQ          | TriviaQA    | HotpotQA    |\n|--------------------------------------------------------|-------------|-------------|-------------|\n| Closebook question answering (Question -> Answer)      | 32.3 / 39.9 | 65.6 / 69.5 | 23.5 / 24.0 |\n| Retrieve-then-Read (Question -> Retrieval -> Answer)   | 34.3 / 41.5 | 58.7 / 63.7 | 31.7 / 33.6 |\n| ReFeed (Question -> Answer -> Retrieval -> Refinement) | 37.5 / 48.1 | 66.3 / 71.1 | 34.1 / 36.0 |\n\n| K=20 with ChatGPT (gpt-3.5-turbo)                      | NQ          | TriviaQA    | HotpotQA    |\n|--------------------------------------------------------|-------------|-------------|-------------|\n| Closebook question answering (Question -> Answer)      | 32.3 / 39.9 | 65.6 / 69.5 | 23.5 / 24.0 |\n| Retrieve-then-Read (Question -> Retrieval -> Answer)   | 34.6 / 42.0 | 59.5 / 64.5 | 31.8 / 33.8 |\n| ReFeed (Question -> Answer -> Retrieval -> Refinement) | 37.8 / 48.2 | 66.5 / 71.2 | 34.4 / 36.5 |\n\n\n[1] Prompting GPT-3 To Be Reliable. ICLR 2023\n\n[2] REPLUG: Retrieval-Augmented Black-Box Language Models. EMNLP 2023.\n\n[3] Lost in the Middle: How Language Models Use Long Contexts. TACL 2023."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission407/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700713240121,
                "cdate": 1700713240121,
                "tmdate": 1700713288466,
                "mdate": 1700713288466,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aAXm1OWACa",
                "forum": "EyfOZKXpcN",
                "replyto": "8SaU9jjnYF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission407/Reviewer_N6CA"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission407/Reviewer_N6CA"
                ],
                "content": {
                    "comment": {
                        "value": "I acknowledge that I have read all of the author responses."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission407/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700726018914,
                "cdate": 1700726018914,
                "tmdate": 1700726018914,
                "mdate": 1700726018914,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]