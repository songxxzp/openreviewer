[
    {
        "title": "ViFu: Visible Part Fusion for Multiple Scene Radiance Fields"
    },
    {
        "review": {
            "id": "XXmymhb4KX",
            "forum": "C3msSjudA7",
            "replyto": "C3msSjudA7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2692/Reviewer_Monn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2692/Reviewer_Monn"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a method to segment and recover a static, clean background and 360-degree objects from multiple scene observations. The idea is that by observing the same set of objects in various arrangements, parts that are invisible in one scene may become visible in others. By fusing the visible parts from each scene, occlusion-free rendering of both background scene and foreground objects can be achieved. The proposed method first performs objects/background segmentation and alignment based on the point cloud-based methods. It then performs radiance fields fusion, where a visibility field is introduced to quantify the visible information of radiance fields. Last, a visibility-aware rendering is used to obtain clean background and 360-degree object rendering. Experiments were conducted on synthetic and real datasets, and the results demonstrate the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper studies the under-modeled invisible parts of NeRF and introduces a new setting of complementing the invisible parts by fusing multiple scene information.\n\nThis paper introduces a visibility field, a volumetric representation to quantify the visibility of scenes, and proposes novel visibility-aware rendering, which leverages the visibility field to achieve the fusion of visible parts of multiple scenes.\n\nSynthetic and real datasets are created to validate the proposed idea, and the experimental results show the effectiveness of the proposed method."
                },
                "weaknesses": {
                    "value": "One major concern pertains to the practicality of the proposed setup. Reconstructing multiple radiance fields for different variants of a scene, assuming diverse object positions and poses, may be challenging in real-world applications. The implicit assumption of fixed illumination for different radiance fields can limit the practicality of the approach. If we can control the scene, we can reconstruct each object independently rather than using this setup. It would be beneficial to address this concern and discuss potential solutions or scenarios where this setup could be more applicable.\n\nThe omission of modeling scene illumination and shadows is a notable limitation. Figure 4 shows that some reconstructed objects have darker or shadowed parts, which raises questions about the fidelity of the reconstructions. I think considering illumination and shadows in the method is critical to enhance the paper's completeness.\n\nWhile the paper introduces a new setup, the technical novelty of some aspects, such as the segmentation and visibility parts, is limited. Traditional point cloud registration methods and straightforward visibility calculations may not sufficiently push the boundaries of the field.\n\nThe quality of results in real scenes appears to be a concern, with reconstructed objects being described as blurry. It is important to provide further analysis or insights into why this issue occurs and potential strategies for improving the quality of real scene reconstructions."
                },
                "questions": {
                    "value": "- Please kindly justify the proposed setup is practical in real-world applications.\n- The limitation of not modeling illumination and shadows.\n- Please justify the technical novelty of the method. \n- Results in the real-world scene is not good.\nPlease refer to the Weaknesses part for details."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2692/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2692/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2692/Reviewer_Monn"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2692/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697975365567,
            "cdate": 1697975365567,
            "tmdate": 1699668552175,
            "mdate": 1699668552175,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "B5qNvtwVlt",
                "forum": "C3msSjudA7",
                "replyto": "XXmymhb4KX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2692/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2692/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate the reviewer's summary of our work! For each question raised by the reviewer, we provide responses below:\n\n> Please kindly justify the proposed setup is practical in real-world applications.\n\nWe have outlined some potential application scenarios for our settings in the **Real-world applications** section in the common responses.\n\n> The limitation of not modeling illumination and shadows.\n\nWe acknowledge that modeling illumination is beneficial for complex scenes. However, the core focus of this paper is the fusion strategy for given multiple NeRFs, and modeling illumination is a specific improvement for each NeRF reconstruction. Additionally, as our method is a general blending algorithm for neural fields, replacing NeRF with more advanced methods that consider lighting decomposition is a direct solution for more complex scenes. However, some failed experimental results also indicate the importance of lighting conditions for the final results. Therefore, we believe that eliminating the influence of lighting is a valuable future direction. Please also refer to the **Lighting conditions** section in common responses.\n\n> Please justify the technical novelty of the method.\n\nWe believe the novelty of our method lies in the core of the proposed approach: the solution and insights into the fusion of multiple neural fields. In recent developments in the 3D field, neural fields have gradually become a mainstream representation alongside point clouds or meshes. When given many 3D assets, fusing them is crucial for practical applications, such as creating new scenes. For point clouds or meshes, their fusion is straightforward (e.g., combining each point cloud). However, for neural fields, due to their implicit nature, how to fuse them is non-trivial. We recognized this fundamental yet unexplored area and proposed a practical method to address it. Therefore, we believe that, despite the simplicity of our approach, it fills a gap in the important direction of neural field fusion, enhances its potential to become a better universal 3D representation, and provides valuable insights for researchers working with neural fields.\n\n> Results in the real-world scene is not good. \n\nWe speculate that the occurrence of blurriness in real scenes may have two main reasons. Firstly, inaccuracies in camera pose prediction for real scenes may lead to inaccurate parts in the reconstructed NeRF, causing errors in the object alignment method based on extracted point clouds, resulting in a blurry fusion result. As discussed in the **Robustness** section of common responses, using alternative alignment methods might improve this aspect. Secondly, for real-world scenes, we model the entire surrounding environment using NeRF, not just the foreground objects and the table but also extending to the distant background. This results in the foreground objects having a relatively small proportion in the overall NeRF model. In our implementation, we use Instant-NGP to encode the entire scene, which could result in insufficient resolution to capture fine details of foreground objects during modeling, leading to blurry outcomes. For this aspect, we believe that improving the overall model resolution, or focusing on modeling the space near the foreground object, may lead to improvements."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2692/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700324229758,
                "cdate": 1700324229758,
                "tmdate": 1700324229758,
                "mdate": 1700324229758,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2tZizlg1Rc",
                "forum": "C3msSjudA7",
                "replyto": "B5qNvtwVlt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2692/Reviewer_Monn"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2692/Reviewer_Monn"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks to the authors for addressing my comment. My primary concerns are \"the practicality of the proposed setup\" and the \"limitation of not modeling illumination and shadows\". Other reviewers also raise similar concerns. However, the authors' responses do not resolve my major concerns. I would keep my initial rating."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2692/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700613939156,
                "cdate": 1700613939156,
                "tmdate": 1700613939156,
                "mdate": 1700613939156,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "sxsMMt2amB",
            "forum": "C3msSjudA7",
            "replyto": "C3msSjudA7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2692/Reviewer_hrRk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2692/Reviewer_hrRk"
            ],
            "content": {
                "summary": {
                    "value": "The paper tackles the problem of reconstructing and rendering a scene in a compositional manner (both background and multiple objects) given multiple videos of the same scene with rearranged objects. It reconstructs each video individually with NeRF, then uses classic methods (point could alignment, DBSCAN) to register and segment each component. The innovation is a visibility-based compositing method, which composes neural fields based on their visibility (how well a point can be reached by rays from all cameras) in each video.\n\nIt shows good qualitative results on a synthetic dataset and a real one."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The writing is clear and easy to follow. The assumptions are discussed thoroughly.\n- Multi-scene registration and reconstruction is a practical problem to solve. \n- The visibility-based fusion method is sound ."
                },
                "weaknesses": {
                    "value": "**Method**\n- Scalability. The paper suggests training a per-video representation and using visibility to compose them. This might not scale well in terms of speed and compute cost if there are many videos (needs MxN NeRFs). Since the geometry/appearance of each NeRF is assumed to be consistent across videos, it seems to make more sense to learn a single representation across all videos for each component (needs M NeRFs).\n\n\n**Experiments**\n- There is no quantitative evaluation, comparison with the closest baseline, or ablation study. I think those are important for a paper.\n  - Evaluation: how well can the background and objects be segmented (2D IoU) and reconstructed (PSNR / 3D Chamfer distance)? \n  - Baseline: I think the shared NeRFs alternative I mentioned earlier is a valid baseline.\n  - Ablation: How well does the method work without the proposed visibility composition? How does the selection strategy affect the result (max selection vs weighted)?\n\n**Setup**\n- I like the simple setup the paper used to verify the idea. However, it seems too ideal to be generalized to the other cases, for example,\n  - Registration failure (e.g., partial scans, nonrigid objects) as mentioned in the limtation\n  - The videos have different illumination conditions as mentioned in the limitation. \n  - The appearance of the background changes over time (e.g., adding a table cloth)"
                },
                "questions": {
                    "value": "Minor comments\n- The term \"scene\" can be confusing. The underlying scene is the same across all videos, but the paper refers to them as different scenes. This makes the definition of visibility confusing. I think naming them video 1...N would be clearer.\n- Fig 3 bottom right subfigure, should the points near the end of the ray be red, as they are not visible from the camera?\n- In Eq 4, it might be worth showing the exact equation that controls the peakedness (I assume it is similar to temperature-scaled softmax?)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2692/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698590179761,
            "cdate": 1698590179761,
            "tmdate": 1699636210704,
            "mdate": 1699636210704,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5cvgEXqyEE",
                "forum": "C3msSjudA7",
                "replyto": "sxsMMt2amB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2692/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2692/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate the reviewer's recognition of our work and the detailed suggestions from various perspectives! Below are our responses:\n\nScalability. Since our method involves NeRF reconstruction for N captures, we indeed need to reconstruct N NeRFs, rather than MxN. For further discussion on the number of scans and required NeRFs, please refer to the **Baseline methods** section in the common responses.\n\nWe agree with the reviewer that quantitative evaluation is crucial.\n- **Evaluation**: As mentioned in the paper, finding a quantitative metric that accurately assesses the rendering results of the reconstructed objects is challenging since we aim for the color of the 360-degree objects to be close to the ambient color rather than the color under specific lighting conditions. However, regarding the accuracy of the reconstructed geometry, we agree with the reviewer that the 3D Chamfer distance is a reasonable metric.\n- **Baseline**: As discussed in the **Baseline methods** section in the common responses, in our early experiments, we attempted to separate foreground objects using a segmentation model and individually reconstruct them. However, this process required a significant amount of manual labor for refinement to achieve satisfactory reconstruction results. Nevertheless, we acknowledge that although labor-intensive, using a straightforward method as a baseline is appropriate, and we plan to include a comparison with this approach if possible.\n- **Ablation**: We appreciate the reviewer's suggestions. We will incorporate experiments comparing our proposed visibility-based fusion and naive fusion approaches, such as taking the average.\n\nRegarding setup, please refer to the **Robustness** and **Lightning conditions** sections in the common responses. For the issue of background changing over time, our current method indeed cannot address this, as the current segmentation algorithm assumes that the background remains unchanged across scenes to achieve foreground/background segmentation. However, we agree that this is an important point for practical applications, and improving the generalization of the segmentation step is a feasible direction.\n\n---\nMinor comments:\n- We appreciate the reviewer's suggestions. We agree that changing \"scene\" to \"video\" or \"object configurations\" might be clearer.\n- Yes, they should be in red. We will correct that.\n- Eq.4 is the specific formula for calculating the blending weight based on the computed visibility. What do you mean by the \"exact equation\"? Yes, it is similar to a temperature-scaled softmax."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2692/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700323939984,
                "cdate": 1700323939984,
                "tmdate": 1700323939984,
                "mdate": 1700323939984,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cVlpzp5jTp",
                "forum": "C3msSjudA7",
                "replyto": "sxsMMt2amB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2692/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2692/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We have incorporated new experiments on illumination conditions based on your feedback. We created scenes with highly reflective foreground objects/background and validated our method under both unchanged and varying lighting conditions. Please kindly refer to Fig. 4 and the light condition section of the ablation study in our updated paper for details."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2692/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700739888998,
                "cdate": 1700739888998,
                "tmdate": 1700739888998,
                "mdate": 1700739888998,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "QGrEjRSA77",
            "forum": "C3msSjudA7",
            "replyto": "C3msSjudA7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2692/Reviewer_etRq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2692/Reviewer_etRq"
            ],
            "content": {
                "summary": {
                    "value": "This paper study an interesting problem in novel view synthesis or compositional scene modeling where the some part of the scene is not visible due to occlusion. This paper propose to capture multiple scenes of the same background and same set of foreground objects with different configurations (poses, lightings) and train different NeRF of each scene configuration. Then they propose to align the scene with the point cloud extracted from a trained NeRF by assuming that background have enough overlap and different scene configuration can be aligned based on the extracted point cloud. With the alignment and the proposed visibility field, they can rendering clean background and 360-degree objects."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper study an interesting problem in novel view synthesis or compositional scene modeling where the some part of the scene is not visible due to occlusion which is largely ignored in previous work.\n2. The paper proposed to capture/scan the scene with different configurations (same background and same foreground objects but with different poses). Each capture is used to train a NeRF to extract point clouds which are used for alignment between different scenes configurations. I think this is reasonable as the invisible part will never be reconstructed unless it becomes visible. \n3. The fusion of different nerf based on visibility field is also interesting.\n4. Experiments results show the effectiveness of the proposed method."
                },
                "weaknesses": {
                    "value": "1. Two assumptions seems reasonable but far from reality? For example, if a car on top of the table is never\n2. The alignment relies on the extracted point cloud from NeRF. In the simple scenarios shown in the paper, NeRF can do a very good job to extract point cloud, but what if NeRF's point cloud is noisy when there is reflection or textless region?\n3. The object segmentation and clustering is heuristic and looks like not very robust. The segmentation of object relies on the clustering algorithm based only point cloud? While I think this simple method works just fine in the simple scenarios studied in the paper, I don't think it will still work in a more complex scene."
                },
                "questions": {
                    "value": "1. Why not use some semantic segmentation method to separate the background and foreground and then to foreground matching of each objects? This seems even easier and straightforward. Or maybe just use the object feature to do matching?\n2. If you have control the capturing process, why not just pre capture the scene with only background? And similarly capture the object one by one. I think capturing the scene like this dose not take very much work compared to capturing multiple configurations proposed in the paper.\n3. In object clustering, do you need to predefined the number of objects.\n4. How to obtain a clean background is not very clear to me? Do you mean if a part is less occlusion, then it belongs to the background?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2692/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2692/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2692/Reviewer_etRq"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2692/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698680002784,
            "cdate": 1698680002784,
            "tmdate": 1699636210623,
            "mdate": 1699636210623,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KcnpnoIkpU",
                "forum": "C3msSjudA7",
                "replyto": "QGrEjRSA77",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2692/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2692/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We sincerely appreciate the reviewer for finding our work interesting! Thank you for providing valuable suggestions and questions. Below are our responses: \n- Regarding assumptions, please refer to the sections on **Experimental setup** and **Real-world applications** in the common responses.\n- Regarding the robustness of point cloud-based computations, please refer to the **Robustness** section in the common responses.\n- Regarding alternative methods based on segmentation, please refer to the discussion on **Baseline methods** in the common responses.\n\n> In object clustering, do you need to predefined the number of objects\n\nYes, for robustness, our current method requires specifying the number of objects M. After obtaining the set of foreground point clouds through distance thresholding, we use DBSCAN to obtain multiple point cloud clusters. However, these clusters may include those consisting entirely of noise formed by a small number of points. Therefore, we select the top-M clusters based on the number of points contained in each cluster to obtain the point clouds of each foreground object.\n\n> How to obtain a clean background\n\nIn this work, we define the term \u201cbackground\u201d as the part that remains unchanged across different captures.. We assume that each part of the background (e.g., a part of the table surface) can be observed in at least one capture. For example, an area with no object occlusion in the first capture (no objects above it, no occlusion), but is occluded by objects in subsequent captures (occlusion), our algorithm selects its scene from the first capture as the background. For all areas, we determine the clean background by comparing visibilities, selecting the one with higher visibility."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2692/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700323290889,
                "cdate": 1700323290889,
                "tmdate": 1700323290889,
                "mdate": 1700323290889,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "zXxSt1tQ06",
            "forum": "C3msSjudA7",
            "replyto": "C3msSjudA7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2692/Reviewer_bZoV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2692/Reviewer_bZoV"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a method to improve the reconstruction of NeRF models by fusing the visible parts of an objects captured under different scene setting. In order to achieve the goal, it introduces a visibility field which records the visible information of radiance field, and use it to guide the fusion of input images from different scene. The proposed method is evaluated on synthetic and real world captured objects. An ablation study is included to evaluate the sensitivity of proposed method under different lighting, parameter setting, number of input scene, and variation of object placement."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The proposed method is simple and easy to follow. It also demonstrated some good results to reconstruct objects with 360 degree visibility."
                },
                "weaknesses": {
                    "value": "The proposed method are only evaluated using \"toy\" examples. First, the scene captured are all under very simple lighting condition, without any directional light sources, and/or object shadows/reflections. The reconstruction examples are also very simple in its appearance which the observations of the objects do not have any view dependent variations, e.g. specular highlight and/or glossy surface. The different scene setting are also very simple, which each objects can be separated easily. I would consider all the scene setting and examples presented in the paper are well controlled and artificial. Second, in term of technical contribution, once the input images of an object are segmented and are grouped together, we can directly reconstruct the object NeRF model using self calibration method to obtain the relative camera position of each input images. What are the benefits to include the additional visibility field? Besides, I also feel that the proposed method still require very dense observations in order to achieve the high quality reconstruction. Third, the ablation study, except for the evaluation about lighting, I do not think the others are necessary. Instead, I would hope to see deeper analyses on how the proposed method can achieve self-calibration on lighting to resolve that concerns mentioned above about \"toy\" examples. Note that, although I agree the ablation study on lighting is necessary, but I also do not agree that the current ablation study on variation of lighting is good enough. It is also too simple and does not reflect the real world scenarios as discussed above. Lastly, another limitation is that the proposed method cannot handle dynamic objects. If all the objects are static, there are really not much technical challenges, and static objects can be easily registered."
                },
                "questions": {
                    "value": "Please try to address my comments above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2692/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698977456108,
            "cdate": 1698977456108,
            "tmdate": 1699636210548,
            "mdate": 1699636210548,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wzDtwvubV4",
                "forum": "C3msSjudA7",
                "replyto": "zXxSt1tQ06",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2692/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2692/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate the reviewer's feedback on our work and the many insightful questions! Below, we provide responses to each of them:\n- Lighting conditions. We utilized directional light in both synthetic dataset and real captures, introducing some level of view-dependent variation. However, we agree with the reviewer's perspective on the importance of validating objects with significant appearance changes across different scenes. We plan to augment our experiments to include more challenging scenarios, such as extreme lighting conditions or glossy objects. Please also refer to the discussion on **Lighting conditions** in the common responses.\n- Regarding the discussion on why we do not use segmented images for reconstruction, please refer to the **Baseline method** section in the common responses.\n- Dense observations. For real captures, as mentioned in Sec. 4.1, we capture a video by circling around the scene, and from that, we extract 60-80 frames for reconstruction. This aligns with the typical setup for 3D reconstruction.\n- As mentioned above, we will add a discussion on lighting conditions in the ablation study.\n- We acknowledge that the current method cannot directly handle dynamic objects. However, considering that ViFu is a general fusion method for neural fields, combining it with some ideas from dynamic scene modeling, such as a deformation field (e.g., introduced in D-NeRF, https://arxiv.org/abs/2011.13961), could be a feasible direction."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2692/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700322980297,
                "cdate": 1700322980297,
                "tmdate": 1700322980297,
                "mdate": 1700322980297,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WdKCTIUwW0",
                "forum": "C3msSjudA7",
                "replyto": "wzDtwvubV4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2692/Reviewer_bZoV"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2692/Reviewer_bZoV"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks the authors for the feedback. Although the rebuttal addressed my comments to some extent, I am not fully convinced, since my major criticism is the \"toy\" examples presented in the papers. I was expecting to see some new examples of challenging scenes with more severe lighting changes, and more thoughtful comparisons with the \"baseline\" methods. I cannot raise my score based on the current rebuttal and submitted materials."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2692/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700603715511,
                "cdate": 1700603715511,
                "tmdate": 1700603715511,
                "mdate": 1700603715511,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "vdJYUy4iRq",
            "forum": "C3msSjudA7",
            "replyto": "C3msSjudA7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2692/Reviewer_p7zS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2692/Reviewer_p7zS"
            ],
            "content": {
                "summary": {
                    "value": "This paper aims to perform a \"piecewise\" fusion of NeRFs so that a static background and multiple objects can be separately rendered from arbitrary viewing directions. A scalar volumetric visibility field R^3 -> [0, 1] is proposed, to facilitate the visible part fusion. It assumes all surfaces of all objects are visible somewhere and that accurate pose alignment can be found for all objects. Qualitative results are shown on a synthetic dataset of objects rendered on a flat tabletop surface, as well as a real dataset in a similar setting."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Well motivated. Although the assumptions seem too strong for in-the-wild data in its current form, I can see further development happening, to jointly optimize pose, perform completion, and be able to handle different lighting conditions."
                },
                "weaknesses": {
                    "value": "No quantitative experiment. Some analysis of the number of images or viewpoints needed would have been interesting. The paper says the method consistently produced satisfactory results, but in some sense, that is guaranteed to happen given satisfactory input. It would have been better if there was more we could learn from the paper experimentally, including failure cases. My interpretation is that the paper has two take-home messages: 1. visible part fusion is a problem that needs more attention. 2. we can define volumetric density functions based on visibility to serve as a blending weight function, assuming we have everything else needed. On second thought, while the presentation is great, I wish I could see more."
                },
                "questions": {
                    "value": "Perhaps consider citing FiG-NeRF: \"separating foreground objects from their varying backgrounds\" (instead of the same static background).\n\nWhat if the background isn't flat?\n\nHow robust is this to noise in point-cloud registration?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2692/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699030200875,
            "cdate": 1699030200875,
            "tmdate": 1699636210444,
            "mdate": 1699636210444,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Vh2F0g8INQ",
                "forum": "C3msSjudA7",
                "replyto": "vdJYUy4iRq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2692/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2692/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for summarizing our work! The two take-home messages mentioned by the reviewer indeed are our main focuses and contributions. We provide specific responses below:\n\nFailure cases. We acknowledge the importance of failure cases. In our method, failure cases primarily arise from two aspects: (1) failure in object pose registration and (2) significant appearance discrepancies between different captures. We have added discussions on these two aspects in the section of **Robustness** and **Lightning conditions** in our common responses.\n\n> Consider citing FiG-NeRF\n\nWe appreciate the reviewer's suggestions. We find its motivation similar to ours, and we will add it in the citation.\n\n> What if the background isn't flat?\n\nThere shouldn't be any issues. In our assumption, if a part remains unchanged in all scenes, we consider it as part of the background. For example, if a keyboard on a desk doesn't move in each capture, it is considered part of the background, and in this case the separated background will include both the desk and the keyboard. A flat background is not our assumption. (If time permits, we would like to append experimental results under non-flat background conditions.)\n\n> How robust is this to noise in point-cloud registration?\n\nPlease refer to our discussion on **Robustness** in the common responses."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2692/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700322804781,
                "cdate": 1700322804781,
                "tmdate": 1700322804781,
                "mdate": 1700322804781,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YvQBeCTrdZ",
                "forum": "C3msSjudA7",
                "replyto": "vdJYUy4iRq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2692/Reviewer_p7zS"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2692/Reviewer_p7zS"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate the authors' response. The major concern from other reviewers was the simplicity of the examples, which I did not find problematic. However, the lack of quantitative evaluation, as noted in the weaknesses section, remains unaddressed. I would have preferred to see some indication of potential experimental validation results in the future, but the response was not convincing. Despite the paper's clean presentation and strong motivation, I choose to maintain my initial rating. I am inclined towards borderline, which unfortunately, I cannot select as an option."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2692/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700738678798,
                "cdate": 1700738678798,
                "tmdate": 1700738748298,
                "mdate": 1700738748298,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]