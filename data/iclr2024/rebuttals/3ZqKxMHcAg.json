[
    {
        "title": "Evaluating Language Models Through Negotiations"
    },
    {
        "review": {
            "id": "8voptv2u9n",
            "forum": "3ZqKxMHcAg",
            "replyto": "3ZqKxMHcAg",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7151/Reviewer_ZZpj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7151/Reviewer_ZZpj"
            ],
            "content": {
                "summary": {
                    "value": "This paper considers using negotiation games to evaluate the intelligence of LLMs. The authors designed a specific structured negotiation protocols, where the agents need to compose a private mental note as well as a public message to the other party each negotiation round. The authors found that GPT-4 is generally more skillful in these negotiation games."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The idea of using negotiation game to assess the intelligence behaviors are promising."
                },
                "weaknesses": {
                    "value": "The presentation is not clear. The evaluation studies are limited."
                },
                "questions": {
                    "value": "I think the idea of using negotiation games to evaluate LLM is great. However, I have concerns about the approaches and evaluations in this paper. Also some of the concepts are not explained well. Specifically:\n\n1. About the negotiation protocol & the way it uses LLM to compose a strategy. Where do $q_{n/m}, \\beta_i$ and context $c$ come from? Are they fixed or sampled from some distribution during each negotiation instance? Why the negotiation strategy has to be constructed in such way, and how does it compare with other approaches? E.g., just directly input previous negotiation rounds results and output a text message.\n\n2. Can the author further clarify what is distributive v.s. compatible negotiation?\n\n3. For the cross-play results, are the scores in Table 5 and 6 averaged across every possible opponents? From Table 13, 14 it appears there certain strategic structure (such rock-paper-scissors cycle). What will be the mean conclusion from there then?\n\n4. In you opinions, why different LLMs behaviors qualitively different?\n\n5. There have been several previous works that evaluate LLMs using negotiation games [1, 2]. Can the authors compare your work with theirs.\n\n[1] \"Improving Language Model Negotiation with Self-Play and In-Context Learning from AI Feedback\" Fu et. al.\n[2] \"Evaluating LLMs with Interactive Multi-Agent Negotiation Games\", Abdelnabi et. al."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7151/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7151/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7151/Reviewer_ZZpj"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7151/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698178025527,
            "cdate": 1698178025527,
            "tmdate": 1699636846759,
            "mdate": 1699636846759,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GE1D2vQaDN",
                "forum": "3ZqKxMHcAg",
                "replyto": "8voptv2u9n",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7151/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7151/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Question 1"
                    },
                    "comment": {
                        "value": "Thank you for taking the time to review our work and the thoughtful questions and suggestions. We hope that our answers below can clarify some of the perceived shortcomings in our presentation. We would like to point out that the presented evaluations comprise thousands of multi-turn negotiations, covering different game settings and models from all leading LM providers. Due to space constraints and to not overwhelm the reader, we did our best to compress our findings as concisely as possible. We will add additional analyses to the Appendix.\n\n> 1. About the negotiation protocol & the way it uses LLM to compose a strategy. Where do q_n/m and beta_i  and context c come from? Are they fixed or sampled from some distribution during each negotiation instance? Why the negotiation strategy has to be constructed in such way, and how does it compare with other approaches? E.g., just directly input previous negotiation rounds results and output a text message.\n\nThe context, c, consists of the protocol, game, and issue descriptions. The protocol descriptions, consisting of the message and note query prompts, q_m and q_n, the negotiation rules, and termination conditions are fixed across negotiations. The rules used in our experiments are strongly inspired by those of the famous \u2018Rio Copa\u2019 negotiation game [1], a negotiating protocol commonly used to simulate negotiations for didactic purposes at leading business schools.\n \nThe queries q_m and q_n are designed to elicit structured outputs and promote measurements of metrics relevant to alignment while remaining as concise as possible. These prompts are the results of many repeated experiments on held-out data across models to minimize promoting unintended behaviors. As noted in Section 5, the need for \u2018prompt engineering\u2019 is a limitation inherent to current language models. It is thus likely that a set of prompts exists that is even more effective than the ones used in our work. However, as all models use the same prompts, the only point of concern would be if the prompts would provide an unfair example to one of the models. Between the minimality of the prompts used and the extensive tests conducted, we did not observe any evidence that this is the case. In Appendix E.1 we listed the fixed rules and prompts used for our experiments.\n \nA game and issue descriptions and corresponding preference weights beta_i are fixed as well, based on the evaluator\u2019s objectives. This basic formulation allows for a wide range of possible negotiations. The game and issues used in our experiments are described in Appendix E.2. For the non-integrative setting, issue preference weights beta_i are all equal, i.e. 1 / #-issues. For the integrative setting, the main requirement for the choice of preference weights is that clear collaborative bargaining opportunities become available, i.e., at least one issue is more important than the other issues. In our experiments, we therefore opted for a fixed {1/3,  2/3} v. {2/3, 1/3} split to provide clear trade-offs between issues. We will update the Appendix to include these experimental setup details.\n \nGenerally, there are two ways a negotiation can be formatted using LMs:  \n1. **Direct dialogue between two LMs**: Each LM-instance takes the \u2018Human role\u2019 to query the other LM-instance, responding in the \u2018AI role\u2019. However, a third, \u2018System\u2019 role is necessary to initialize the negotiation rules and provide per-round instructions.\n2. **Represent negotiations as a transcript**: Each LM-instance receives messages from the other LM-instance as a transcript presented by a \u2018Human\u2019 or \u2018System\u2019 role. This same Human/System role can be used to initialize and mediate the negotiations, thus only requiring two distinct API roles.\n \nAs listed in Appendix B, not all LM-providers currently support three distinct roles (AI, Human, and System). Following preliminary experiments described in Appendix A.2 to measure the difference between the two formats, we opted to use the \u2018transcript\u2019 format to include models from all popular LM-providers.\n \nAs described in equations 2, 3, and 4, when formulating its next note/message, the LM-agent indeed observes a sequence of the previous negotiation rounds (note the t superscripts). As agents are initialized using the context c, these were omitted in these equations. However, we recognize that this notation is not clear enough and will update the revised text accordingly. Thank you for pointing this out and we apologize for the confusion.\n\n[1] Bontempo, R., & Iyengar, S. (2008). Rio Copa: A negotiation simulation. Columbia Caseworks."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7151/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700072120934,
                "cdate": 1700072120934,
                "tmdate": 1700072120934,
                "mdate": 1700072120934,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "L2JRmBY4vS",
            "forum": "3ZqKxMHcAg",
            "replyto": "3ZqKxMHcAg",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7151/Reviewer_pM8H"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7151/Reviewer_pM8H"
            ],
            "content": {
                "summary": {
                    "value": "- Joint framework to evaluate performance and alignment of LLMs using structured negotiation tasks.\n\n- Creates a negotiation task benchmark, which involves evaluating the success of LLMs negotiating toward goals in self-play and cross-play.\n\n- Incorporates LMs into the evaluation benchmark so that the benchmark \"co-evolves\" with the models they are designed to test.\n\nOverall, I am currently giving this paper a 3 (reject) before discussions, considering the weaknesses outlined below related to empirical design and the lack of formalism. However, I am giving my rating a confidence of 2 since I am unfamiliar with related work."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- timely and relevant subject\n\n- interesting idea on evaluating both alignment and performance, notably given the uncertainty around the orthogonality hypothesis.\n\n- interesting cross-play results to compare LLMs to each other."
                },
                "weaknesses": {
                    "value": "# Big weaknesses\n\n- empirical design with insurmountable reproducibility issues and cost issues impacting statistical validity. Unless the experiments were performed all simultaneously, it is not obvious that they are valid since these models undergo continuous improvement, meaning you might've been comparing different models across different experiments, even if API access was the same and there would be no way to know, right? For this same reason, the experiments are not necessarily reproducible. It might be better in the future to use open-source LLMs for which the models can be held frozen by selecting a checkpoint.\n\n- no definition of agency despite it being a central concept to the paper\n\n- the metrics in table 1 seem to require a lot of human checking, which makes it difficult to scale this benchmark. Are you also using LLMs to compute these benchmark values(e.g. internal faithfulness)?\n\n- there is a whole range of issues between opposing and aligned interests, e.g. mixed cooperative-competitive settings or variable-sum games. It would be interesting to establish benchmarks on these types of settings as well.\n\n- having each agent play both sides and starting positions and averaging does not control for bias, since different LLMs might be more or less able to take advantage of these asymmetries depending on the game design. You would probably also need to ensure sufficient diversity in the game contexts to help control for bias (i.e. not just rent negotiation games)\n\n- it is not obvious that allowing multiple turns to take place would provide more information into understanding which persona is active especially if the persona mixture depends on previous context and can evolve through a conversation, nor that this persona activation would be consistent across different runs with different random seeds. However, I am not very familiar with this literature.\n\n===\n\n# Small weaknesses\n\n- bad reference formatting: \"Jacob Andreas. Language models as agent models, 2022.\"\n\n- fix typos (\" the challenge is to figure out agent interests are aligned. command, ...\")\n\n- no reference provided for \"Theory of Mind\"\n\n- a cooperative game in game theory is one in which players can negotiate binding contracts, which can be confusing given that we are discussing games in game theory formalism, though with a different meaning for \"cooperative\".\n\n- not obvious that issues necessarily have linearly weighted preferences. A related subject is scalarization in multi-objective optimization.\n\n- \"the possible effects and opportunities of stories, traits, rules, and prompts have been discussed in the previous subsections\" stories were not discussed \n\n- \"providing too much capacity might lead to hallucinations\" citation needed"
                },
                "questions": {
                    "value": "- ToM strategy is not introduced formally\n\n- Why is the utility 0 if there is no agreement on all issues?\n\n- How are the prompts designed to test the LLM's negotiation capacities? For any given game, do you test multiple prompt variations with similar semantic meanings? How do you know the elicited personas will be the same across differences in the input prompts?\n\n- Why is the goal to measure if there is a significant difference in performance between the average, expert and novice initialization? I thought the goal was to evaluate LLMs in general, and it's far from obvious that such an initialization will transfer the same way across different LLMs\n\n- Does the co-evolution of the benchmark in terms of cross-play rely on having sufficient diversity among language models? How do you see the benchmark holding up in the future?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Potentially harmful insights, methodologies and applications"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "The author discusses the ethical considerations of their work in relation to malicious actors. AI governance is outside my area of expertise, therefore I am flagging for Ethics Review such that an ethics reviewer may review the author's claims in Section 5, \"Ethical Considerations\" paragraph, though I do not believe that this framework constitutes a potentially harmful methodology on its own, it is aimed at evaluating AI capabilities and alignment."
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7151/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698709488434,
            "cdate": 1698709488434,
            "tmdate": 1699636846657,
            "mdate": 1699636846657,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "HZlD1gBdqC",
                "forum": "3ZqKxMHcAg",
                "replyto": "L2JRmBY4vS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7151/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7151/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Weaknesses 1-3"
                    },
                    "comment": {
                        "value": "Thank you for your detailed review and for voicing your concerns. We hope that our answers to the perceived weaknesses you highlighted and the questions you posed will allow you to reassess our work. In addition to the replies below, we also included a general summary placing our efforts more squarely in the currently accepted approach to language model evaluation benchmarking.\n> 1. empirical design with insurmountable reproducibility issues and cost issues impacting statistical validity. Unless the experiments were performed all simultaneously, it is not obvious that they are valid since these models undergo continuous improvement, meaning you might've been comparing different models across different experiments, even if API access was the same and there would be no way to know, right? For this same reason, the experiments are not necessarily reproducible. It might be better in the future to use open-source LLMs for which the models can be held frozen by selecting a checkpoint.\n\n> a. [Experimental results might not be valid and are not necessarily reproducible since closed-source models are continuously improving]\n\nLanguage modeling (LM) technology has matured to a point where commercial applications are viable, leading to closed-source implementations to protect large R&D investments. The reviewer\u2019s argument appears to be against evaluating closed-source models in general. We would argue that this strategy is dangerous. Especially now that LMs are rapidly entering the public sphere and are being used by hundreds of millions, more effort than ever should be dedicated to evaluating performance and alignment.\n\nAny evaluation benchmark is limited by how often evaluations are performed. For example, the current industry-leading LM evaluation benchmark, \u2018HELM\u2019 [1], has the same limitation. Furthermore, closed-source models often come with external checkpoints. More importantly, they surely come with internal checkpoints. Legislative pressure could force LM providers to share evaluation metrics on several benchmarks before public releases, similar to how other models deployed in critical infrastructure are treated, e.g., risk models for consumer lending at retail banks.\n\n[1] Liang, Percy, et al. \"Holistic evaluation of language models.\" Transactions on Machine Learning Research (2023).\n\n> b. Cost issues [impact] statistical validity\u2019\n\nMachine learning models have dramatically increased in size over the past 15 years with the rise of deep learning. This increase has come at a price of reproducibility, e.g., not many labs have access to hundreds of GPUs to host a single model. The recent growth of APIs for deep learning models has increased the possibility for evaluations by lowering the thresholds of accessibility. Yet, we recognize that performing evaluations on a large suite of models comes at a hefty price (The experimental costs for this work were ~USD 10,000-). As noted in the previous point, the cost of running such evaluations between models from large providers could be imposed on the large model providers themselves before public releases. Indeed, one of our core contributions to the community is the dataset of thousands of negotiation transcripts to serve as a starting point for further research.\n\nHowever, as outlined in Section 5, \u2018Limitations and Ethical Considerations\u2019, a viable alternative exists for smaller labs or individual researchers interested in benchmarking their models against third-party options. By establishing a latent-ability framework like the ELO rating system used in chess, a model could be compared to cheaper third-party models, after which results can be extrapolated.\n\n\n> c.  It might be better in the future to use open-source LLMs for which the models can be held frozen by selecting a checkpoint\n\nAs described in Section 3.2 \u2018Qualifiers\u2019 and at the start of Section 4, we indeed attempted to benchmark the LLaMA 2 model family, widely considered the state-of-the-art in open-source LLMs at the time of research. However, we found that these models were unable to pass our Qualifier round, i.e., to successfully and consistently complete a structured negotiation.\n\n> 2. no definition of agency despite it being a central concept to the paper\n\nIn the first paragraph of our introduction, we describe LM-agents as: \u2018capable of completing tasks that require interactive reasoning [... and] acting over multi-step horizons.\u2019 We will emphasize and sharpen this definition in the revised text.\n\n> 3. the metrics in table 1 seem to require a lot of human checking, which makes it difficult to scale this benchmark. Are you also using LLMs to compute these benchmark values(e.g. internal faithfulness)?\n\nThe metrics displayed in all tables are fully automated, as described in Appendix A.4. We utilize a mixed strategy of regexes and an LM equipped with a concise set of hand-crafted examples. We further performed a large number of sampled spot-checks to ensure the extracted offers were reasonable and accurate."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7151/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700077113344,
                "cdate": 1700077113344,
                "tmdate": 1700077113344,
                "mdate": 1700077113344,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "x2ExLNMuoQ",
            "forum": "3ZqKxMHcAg",
            "replyto": "3ZqKxMHcAg",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7151/Reviewer_yVDW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7151/Reviewer_yVDW"
            ],
            "content": {
                "summary": {
                    "value": "The authors are proposing a technique to evaluate large language models using a scenario where they are required to participate in a multi-issue negotiation, for instance a rental agreement. The overall claim of the paper is that investigating such a negotiation might lead to a more accurate evaluation of the performance and alignment of the language model compared to other approaches. The authors had tested a number of currently available language models through their APIs."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The authors are making a good case that the proposed evaluation method is a useful aspect of the behaviors of the large language models.\n* The paper proposes a methodology that carefully considers the variety of biases that can be introduced by the measuring process, and takes credible steps to avoid them. \n* Extensive evaluation over six-seven LLMs, including self-play and cross-play."
                },
                "weaknesses": {
                    "value": "* Many of the current language models are not trained to sustain a negotiation type conversation. For instance, they don't have a framework to keep track of the issues agreement had been reached upon, or the current alternatives that are under discussion. Thus, the proposed metric measures an aspect on which the models had not been trained, and indeed their performance on it is more a side effect of some artifacts in the training data."
                },
                "questions": {
                    "value": "Clearly, the performance of the LLMs in this task can be improved relatively easily, as the underlying mathematical negotiation problem is much simpler than LLM's language abilities. How would one rank a model that would have minimal language abilities, but use a specialized algorithmic plugin?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7151/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698790368247,
            "cdate": 1698790368247,
            "tmdate": 1699636846484,
            "mdate": 1699636846484,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "eFI6YHLVmt",
                "forum": "3ZqKxMHcAg",
                "replyto": "x2ExLNMuoQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7151/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7151/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your positive and constructive review. We are happy to see our effort to de-bias the presented metrics and the extensive evaluations conducted being recognized. We hope the responses below can alleviate the remaining perceived weakness and answer your question.\n \n> Many of the current language models are not trained to sustain a negotiation type conversation. For instance, they don't have a framework to keep track of the issues agreement had been reached upon, or the current alternatives that are under discussion. Thus, the proposed metric measures an aspect on which the models had not been trained, and indeed their performance on it is more a side effect of some artifacts in the training data.\n \nThank you for highlighting this important point. Current language models (LMs) are indeed not explicitly optimized for multi-step negotiations. Yet, LMs are broadly marketed as \u201cAGI\u201d, which encourages users to use them as agents for tasks not seen during training (see for example the recent \u201cpersonal GPTs\u201d release by OpenAI [1]). We\u2019re testing to what extent such (encouraged) \u201coff-label use\u201d is warranted. \n\nAs many of the challenges integral to successful negotiations show up in various real-world tasks of interest, negotiation performance can serve as an insightful proxy for expected behavior. Additionally, metrics like \u2018internal/external faithfulness\u2019 and instruction-following provide insight into the consistency of an LM-agent\u2019s state of mind and the ability to stay within user-defined boundaries.\n \n[1] https://openai.com/blog/introducing-gpts\n\n> Clearly, the performance of the LLMs in this task can be improved relatively easily, as the underlying mathematical negotiation problem is much simpler than LLM's language abilities. How would one rank a model that would have minimal language abilities, but use a specialized algorithmic plugin?\n \nThat is a great question. It is indeed possible that a new generation of specialized negotiation-algorithms will emerge, where LMs will be used as \u2018sensing\u2019 and \u2018acting\u2019 modules. Such a module would be responsible for decoding offer indications in natural language to a more restricted format for a specialized algorithmic solver, then encode back proposed solutions into natural language to communicate externally. We highlight examples of these types of \u2018hybrid-LM-agents\u2019 in our related work. Yet, even for such hybrid-models, the bar for \u2018minimal language abilities\u2019 may be quite high to accurately capture all subtleties contained in natural language.\n \nWhile it is clear a specialized solver would be much more capable of finding \u2018optimal\u2019 solutions to the underlying mathematical problem, it is less clear how much this would help in finding the language required to successfully convince the opposing agent toward an acceptable agreement. Additionally, strong language understanding would be required to \u2018guess\u2019 the other agent\u2019s payoff matrix, i.e., perform \u201cTheory of Mind\u201d inference.\n\nHowever, the purpose of our work is not necessarily to provide a benchmark for the best \u2018negotiating\u2019 agent, but rather measure the innate, un-optimized ability of LMs to behave as agents. If it becomes the case that LM providers would find negotiating performance important enough to design specialized training/finetuning and inference routines, such advantages should quickly disappear in cross-play performance."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7151/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700076027728,
                "cdate": 1700076027728,
                "tmdate": 1700076027728,
                "mdate": 1700076027728,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "s0TmX5kv4G",
                "forum": "3ZqKxMHcAg",
                "replyto": "eFI6YHLVmt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7151/Reviewer_yVDW"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7151/Reviewer_yVDW"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the additional explanations. I will retain my rankings."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7151/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700711208661,
                "cdate": 1700711208661,
                "tmdate": 1700711208661,
                "mdate": 1700711208661,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]