[
    {
        "title": "Encoding Expert Knowledge into Federated Learning using Weak Supervision"
    },
    {
        "review": {
            "id": "XmIKvS8p9C",
            "forum": "OqLrv5oH6r",
            "replyto": "OqLrv5oH6r",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission264/Reviewer_qXVo"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission264/Reviewer_qXVo"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a novel paradigm, WSHFL,  for integrating programmatic weak supervision with on-device federated learning. The key ideas are composed of 2 components: automatic labeling function (heuristics) generations and federated learning with WeaSEL (Weakly Supervised End-to-end Learning). The authors conduct experiments to show that both components work across three benchmark tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "To the best of the reviewer\u2019s knowledge, the idea of incorporating federated learning with programmatic weak supervision proposed in this paper is novel. This work is clearly motivated and it obviously fills the need for distributed programmatic weak supervision, especially with privacy concerns. I also appreciate the authors\u2019 consideration of data beyond the text (i.e. ECG)."
                },
                "weaknesses": {
                    "value": "Overall, despite this paper(WSHFL)\u2019s obvious merits, the method construction itself is incremental. The ideas of automated labeling functions generations have been a relatively well studied topic. If possible an open-source code base for federated learning with WeaSEL or other non end-to-end programmatic weak supervision would be very beneficial to the community."
                },
                "questions": {
                    "value": "From the automatic labeling functions generation scheme, for example, the unigram proposal mechanism still can pose private data leakage risks. I would very much appreciate it if the authors can offer some ideas about initial solutions around this issue."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission264/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission264/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission264/Reviewer_qXVo"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission264/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698799386684,
            "cdate": 1698799386684,
            "tmdate": 1699635952059,
            "mdate": 1699635952059,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "TiHHOMGOtp",
                "forum": "OqLrv5oH6r",
                "replyto": "XmIKvS8p9C",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission264/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission264/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for reviewing! Our rebuttal"
                    },
                    "comment": {
                        "value": "Dear Reviewer qXVo, \n\nWe thank you for your time and effort in reviewing our paper. \n\nWe are glad you find our work well motivated and novel, and that you recognize our efforts in performing experiments with ECG data. We have commented on the concerns about privacy raised by the reviewers in our *general response*. In that same response, we touch upon the concern that some aspects of the proposed method (automated LF generation) are already well studied in centralized settings. \n\nRegarding the suggestion to include an **open-source code base**, we plan to open-source our code when we release our paper. In the meantime, we have made the code anonymously available at https://anonymous.4open.science/r/wshfl_pipeline-A13C/README.md. We have included a link to this resource in the manuscript.\n\nPlease let us know if there are any further areas we can improve on."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission264/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700172849915,
                "cdate": 1700172849915,
                "tmdate": 1700172861246,
                "mdate": 1700172861246,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "XRW4XAspLu",
            "forum": "OqLrv5oH6r",
            "replyto": "OqLrv5oH6r",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission264/Reviewer_W4Yv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission264/Reviewer_W4Yv"
            ],
            "content": {
                "summary": {
                    "value": "The title is clear and gives a good indication of the paper's main focus. The abstract provides a concise overview of the problem, the proposed solution, and the results. The introduction sets the context well, highlighting the importance of learning from on-device data and the challenges associated with it. The motivating example of arrhythmia detection provides a real-world context, making it relatable for readers. This paper introduces a new method, WSHFL, that bridges the gap between federated learning and programmatic weak supervision, addressing a challenge in the field. Addressing the issue of on-device data annotation is timely and relevant, given the increasing importance of privacy and on-device computations."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper introduces the concept of Programmatic Weak Supervision (PWS) into the federated setting, and the topic of on-device data annotation is timely and important."
                },
                "weaknesses": {
                    "value": "1.\tThe novelty seems limited as PWS has already been explored in centralized settings. The paper's main contribution appears to be the adaptation of an existing technique to a federated scenario rather than introducing a fundamentally new approach.\n\n2.\tFigure 1 aims to visualize the strategy for generating LFs in the WSHFL method. However, the figure appears to be a high-level representation without detailed annotations or explanations. The flow between the different stages (a to d) and the significance of the values associated with the \"IF nice\" statements are not immediately clear. A more detailed caption or accompanying text might help in understanding the figure's content.\n\n3.\tThe paper mentions comparisons with fully supervised baselines, but there seems to be a lack of comprehensive comparison with state-of-the-art methods in FSSL. Such a comparison would be crucial to establish the proposed method's effectiveness and relevance in the current research landscape."
                },
                "questions": {
                    "value": "see weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission264/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698834656752,
            "cdate": 1698834656752,
            "tmdate": 1699635951992,
            "mdate": 1699635951992,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "12oKlApfM3",
                "forum": "OqLrv5oH6r",
                "replyto": "XRW4XAspLu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission264/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission264/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for reviewing! Our rebuttal"
                    },
                    "comment": {
                        "value": "Dear Reviewer W4Yv, \n\nWe thank you for your time and effort in reviewing our paper. We are happy that you like our motivating example. As you point out, on-device data annotation is a timely and relevant challenge. \n\nWe have commented on the concerns on presentation and novelty raised by the reviewers in our general response. Regarding your concern on the *lack of comprehensive comparison with state-of-the-art methods in FSSL*, we want to note that most of these techniques have been proposed for image data and rely on some form of weak or strong data augmentation [1, 3, 5, 7, 8, 9, 10, 14, 15]. Thus, it is not trivial to naively extend them to text and time-series data. As such FSSL for new data modalities is an exciting research direction in itself. Having said that, we are **actively working on modifying some of these techniques with the aim of addressing your concern in time**. \n\nPlease let us know if there are any further areas we can improve on."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission264/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700172777351,
                "cdate": 1700172777351,
                "tmdate": 1700172777351,
                "mdate": 1700172777351,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "LqlDDCA7Z5",
            "forum": "OqLrv5oH6r",
            "replyto": "OqLrv5oH6r",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission264/Reviewer_mKax"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission264/Reviewer_mKax"
            ],
            "content": {
                "summary": {
                    "value": "The paper discusses the challenges of leveraging on-device data for training intelligent mobile applications. The data is distributed on client devices and it is sensitive data, making it difficult to obtain expert annotations for traditional supervised machine learning. Current federated learning techniques typically use unsupervised approaches and cannot capture expert knowledge through data annotations. To address this issue, the paper introduces a method called Weak Supervision Heuristics for Federated Learning (WSHFL). WSHFL utilizes labeling functions, which are heuristic rules, to annotate on-device data in cross-device federated settings. The paper presents experimental results across two data modalities, text and time-series, demonstrating that WSHFL achieves competitive performance compared to fully supervised methods without the need for direct data annotations."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper addresses the problem of obtaining labels for the data distributed across devices and training a model using the labeling signal obtained. I like the motivation and introduction of the problem. \n\n\n2. The authors provide a solution based on programmatic weak supervision (PWS) in which the expert feedback is incorporated using a set of heuristic rules, which is more efficient than asking for annotations of each data point. The authors apply it in the federated learning setting since the data cannot leave the client device i.e. not sharable. The proposed solution generates a set of candidate labeling functions on each client and then uses expert feedback to select accurate labeling functions that are used to train the model using WeaSEL model (Cachay et al. 2021). \n\n\n3. Experiments on natural language and time series domains are provided that demonstrate the feasibility and effectiveness of the proposed approach WSHFL."
                },
                "weaknesses": {
                    "value": "1. I find the method section 3 dense and confusing.  I have following confusions/questions. How are the candidate LFs generated on client? What is exactly happening at the server, in particular in ExpertQuery function? This function is providing $u_t$ and the TrainClient function is giving estimates $\\hat{u}_t$. What is the neural network trained on and for what purpose? $u_j$ is a variable denoting whether $\\lambda_j$ is selected or not, but statements like \u201c where we sequentially inspect the candidates in or der to discover members of our desired class ($u_j=1)$ make it look like it is one of the classes in $\\mathcal{Y}$. The presentation of section 3 can be improved greatly to make it more lucid. There is very little horizontal space between the algorithm block. \n\n2. The solution is aimed at federated learning setting, so privacy should be ensured. It is not clear to me whether LFs can leak personal information and does the overall solution ensures privacy."
                },
                "questions": {
                    "value": "1. How does the method ensure privacy? The data is not shared with the experts/serve but I think there is a potential risk of information leak via labeling functions.\n2. How many times are each method run in experiments? Does running more times reduce variance?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission264/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission264/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission264/Reviewer_mKax"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission264/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699133604754,
            "cdate": 1699133604754,
            "tmdate": 1700714827165,
            "mdate": 1700714827165,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Bgfkau06Ht",
                "forum": "OqLrv5oH6r",
                "replyto": "LqlDDCA7Z5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission264/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission264/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for reviewing! Our rebuttal"
                    },
                    "comment": {
                        "value": "Dear Reviewer mKax, \n\nWe thank you for your time and effort in reviewing our paper. \n\nWe are glad that you found our work to be well motivated, feasible and effective. We are actively working on improving the readability of Section 3, and have commented on the privacy concerns raised by the reviewers in our general response. Below are answers to your specific questions:\n\n**How are the candidate LFs generated on the client?** \n\nSection 4 describes how LFs are generated in each client. Figures 2 and 3 show examples of text and time-series LFs respectively.\n\n1. To generate text LFs, each client first identifies a set of unigrams (i.e. words) within a certain document frequency range. For example, say the client identifies 2 unigrams in their vocabulary: [nice, bad]. To create labeling functions, each client takes the cross product of these unigrams and the set of possible labels. If the set of possible labels is [negative sentiment, positive sentiment], then the client generates 4 LFs: [nice \u2192 positive sentiment, nice \u2192 negative sentiment, bad \u2192 positive sentiment, bad \u2192 negative sentiment]. To find keywords, we use the CountVectorizer implementation in scikit-learn for the same. \n2. To generate time-series labeling functions, we first cluster time-series into $k$ clusters. We refer to the cluster representatives (the mean time-series) of these clusters as representative templates. To construct labeling functions, we take the cross product of these representative templates and the set of possible labels. The time-series labeling function is shown in Fig. 3. Intuitively, the closer a time-series is to a representative template, the more likely it is to be assigned to the class corresponding to the representative template.  \n\nWe have now added these toy examples to the appendix.\n\n**What is exactly happening at the server, in particular in `ExpertQuery` function?** \n\n`ExpertQuery` happens at the server, and is the process by which a human domain expert assigns $u_j=1$ (LF is useful/accurate) or $u_j=0$ (LF is not useful/accurate) to the given LF. In our work, the expert is positioned at the server.\n\n**What is the neural network trained on and for what purpose?** \n\nFrom the context of your question, we assume you are referring to the neural network $h_k$. This network is trained on the LFs that have already been inspected by the expert $h_k: \\lambda_j \\to u_j$ and, intuitively, predicts the probability that a new LF will be considered useful by the expert. We use this network to later on to select which LF we will actually show to the expert for inspection.\nHow many times are each method run in experiments? Does running more times reduce variance? We mention in the \"Methods and Models\" section that \"Unless mentioned otherwise, we repeat each experiment five times with different random seeds and report the mean and standard deviation.\" In preliminary analyses, we ran some experiments with 10 repetitions, but the behavior of the error bars did not qualitatively change. \n\nPlease let us know if there are any further areas we can improve on. Based on your questions, we have already made some changes to improve the readability of our work."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission264/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700172703864,
                "cdate": 1700172703864,
                "tmdate": 1700172703864,
                "mdate": 1700172703864,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uoRXO5PjXK",
                "forum": "OqLrv5oH6r",
                "replyto": "Bgfkau06Ht",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission264/Reviewer_mKax"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission264/Reviewer_mKax"
                ],
                "content": {
                    "comment": {
                        "value": "Appreciate your response to my questions. Most of my concerns are addressed. I would still encourage the authors to enhance the clarity of the paper in particular I like the conceptual contribution of the paper and I think the process of LF generation, role of human/expert in LF selection should be super clear early in the paper, if possible have a few more examples/illustrations."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission264/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700714791006,
                "cdate": 1700714791006,
                "tmdate": 1700714791006,
                "mdate": 1700714791006,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]