[
    {
        "title": "Pre-training with Synthetic Data Helps Offline Reinforcement Learning"
    },
    {
        "review": {
            "id": "fdVhCcV58X",
            "forum": "PcxQgtHGj2",
            "replyto": "PcxQgtHGj2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1152/Reviewer_jxkz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1152/Reviewer_jxkz"
            ],
            "content": {
                "summary": {
                    "value": "Reid et al. (2022) demonstrated that pre-training a Decision Transformer (DT) on a Wikipedia corpus can substantially improve its performance on downstream Deep Reinforcement Learning (DRL) tasks. This paper explores whether a synthetic pre-training corpus can act as a substitute for the Wikipedia corpus. The main finding is that synthetic data generated using a one-step Markov Chain with a state space of 100 states and 75% fewer updates outperforms Wikipedia for pre-training. Additionally, the performance is relatively unaffected by the order of the Markov Chain or the size of the state space. A softmax temperature of 1.0 yields the best results. IID data performs marginally worse than Markov Chain samples, but still outperforms both no pre-training and pre-training with Wikipedia. For conservative Q-learning (CQL), synthetic pre-training data generated using a Markov Decision Process (MDP) leads to significant improvements over no pre-training. Similar to Decision Transformer, the best performance for CQL is achieved using a state space with a size of 1000, a softmax temperature of 1, and 100k updates. IID data performs slightly worse than MDP data, but still outperforms no pre-training."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* Presents empirical evidence that contradicts the prevailing belief that language data is essential for pre-training models for offline Deep Reinforcement Learning.\n\n* Demonstrates the benefits of synthetic pre-training data for both transformer and Q-learning based approaches to offline DRL.\n\n* Reports ablation studies to investigate the influence of various parameters such as the size of the state space, temperature, and order of the Markov Chain."
                },
                "weaknesses": {
                    "value": "* The paper does not investigate the impact of the number of updates during fine-tuning, which is kept constant at 100k for DT and 1M for CQL. It would be useful to understand the relationship between the parameters of the synthetic data and the number of updates in fine-tuning.  This has the practical implication that fine-tuning data is typically task-specific and its availability may be severely limited. Alternatively, computational constraints may limit the fine-tuning budget.\n* The paper does not report the performance of the baseline (DT/CQL) if it is run for x more updates (where x is 80K for Wikipedia and 20k for the proposed synthetic data)."
                },
                "questions": {
                    "value": "* How does the optimal configuration of synthetic data vary as a function of the number of fine-tuning updates?\n* What is the performance of the baseline (DT) if it is run for an additional 80k updates on Wikipedia or 20k updates on the proposed synthetic data? This would provide us a side-by-side comparison of the 3 models when run for the same number of updates."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1152/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1152/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1152/Reviewer_jxkz"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1152/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698187276355,
            "cdate": 1698187276355,
            "tmdate": 1699636041461,
            "mdate": 1699636041461,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "YMHeYkC7TB",
                "forum": "PcxQgtHGj2",
                "replyto": "fdVhCcV58X",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1152/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1152/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer jxkz"
                    },
                    "comment": {
                        "value": "Thank you for your detailed comments and constructive suggestions. We have added a number of experiments to address your concerns and we are excited to share the new results. \n\n> - The paper does not investigate the impact of the number of updates during fine-tuning, which is kept constant at 100k for DT and 1M for CQL. It would be useful to understand the relationship between the parameters of the synthetic data and the number of updates in fine-tuning. This has the practical implication that fine-tuning data is typically task-specific and its availability may be severely limited. Alternatively, computational constraints may limit the fine-tuning budget.\n> - How does the optimal configuration of synthetic data vary as a function of the number of fine-tuning updates?\n\nOn page 31 of the paper, we provide learning curves that show the performance of using different synthetic data settings with different numbers of fine-tuning updates. In summary, these new results show:\n1. Having a state space of 1000 gives the best performance for different finetuning updates.\n2. Having a MC step size of 1 gives the best performance for different finetuning updates.\n3. A temperature of 1 gives the best performance for different finetuning updates.\n4. It is interesting that DT+Wiki actually has slightly worse performance than the DT baseline when the number of fine-tuning updates is small. However, DT+Synthetic consistently outperforms the DT baseline for all numbers of fine-tuning updates.\n\nWe additionally provide ablations on how different amounts of fine-tuning data affect performance. In summary, these results show:\n1. Having a state space of 100 gives the best performance for all amounts of fine-tuning data. \n2. Having a MC step size of 1 gives the best performance for all amounts of fine-tuning data. \n3. A temperature of 1 gives the best performance for all amounts of fine-tuning data.  \n4. The DT+Synthetic variants consistently outperform the DT baseline and DT+Wiki for all amounts of fine-tuning data.\n\n> - The paper does not report the performance of the baseline (DT/CQL) if it is run for x more updates (where x is 80K for Wikipedia and 20k for the proposed synthetic data).\n> - What is the performance of the baseline (DT) if it is run for an additional 80k updates on Wikipedia or 20k updates on the proposed synthetic data? This would provide us a side-by-side comparison of the 3 models when run for the same number of updates.\n\nThank you for this great suggestion. We have conducted new experiments that allow the DT baseline to train for more updates. The following table summarizes the results (performance is averaged over all 12 environment datasets).\n\n|            |           DT | DT+20K more|DT+80K more|DT+Wiki|DT+Synthetic|\n| ----------- | ----------- | ---| ---| ---| ---|\n| Final performance  | 67.2 \u00b1 4.0| 69.1 \u00b1 5.1| 70.7 \u00b1 4.6| 69.9 \u00b1 5.6| **73.6** \u00b1 4.9|\n\nThese results show that:\n1. When trained for more updates, the DT baseline can achieve stronger performance.\n2. Recall DT with 80K more updates has the same total number of updates as DT+Wiki (180K total updates). They achieve a similar performance. This result further supports our finding that Wiki pre-training does not bring a special benefit.\n3. DT with 20K more updates has the same total number of updates as DT+Synthetic (120K total updates). However, DT+Synthetic achieves stronger performance (73.6 > 69.1). Even when DT trains for 80K more updates instead of 20K, DT+Synthetic is still stronger (73.6 > 70.7). This shows synthetic pre-training brings a performance boost that cannot be achieved by simply taking more finetuning updates.\n\nMore details can be found on page 28 of the paper. We also present the learning curves for DT, DT+Wiki and DT+Synthetic all trained to 180K total updates; it shows DT+Synthetic consistently achieves the best performance for different total numbers of updates.\n\nWe would like to thank the reviewer again for the great suggestions, and we are happy to discuss any further questions or comments."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1152/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700734690846,
                "cdate": 1700734690846,
                "tmdate": 1700734738650,
                "mdate": 1700734738650,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "YdIKAupbpj",
            "forum": "PcxQgtHGj2",
            "replyto": "PcxQgtHGj2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1152/Reviewer_WUkY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1152/Reviewer_WUkY"
            ],
            "content": {
                "summary": {
                    "value": "This paper explored pre-training Transformer (DT) with synthetic data. They found that pre-training with synthetic IID data can match the performance gains from pre-training on large-scale language data. \n\nThe authors also apply the pre-training methods into the conservative Q-learning (CQL) framework. Experimental results show that pre-training with IID data and Markov decision process data can improve the performance of CQL."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper proposes a simple yet effective pre-training method with synthetic data for Decision Transformer.\n\n2. Results demonstrate that the proposed pre-training method with CQL can achieve significant improvements."
                },
                "weaknesses": {
                    "value": "1. The experiments lack comparison for some pre-trained DT models, such as Future-conditioned Unsupervised Pretraining for Decision Transformer(https://proceedings.mlr.press/v202/xie23b/xie23b.pdf).\n\n2. It is more convincing to evaluate the proposed methods on more tasks."
                },
                "questions": {
                    "value": "How much do different synthetic data construction methods affect the results?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1152/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698752996209,
            "cdate": 1698752996209,
            "tmdate": 1699636041374,
            "mdate": 1699636041374,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8ralH6pWvs",
                "forum": "PcxQgtHGj2",
                "replyto": "YdIKAupbpj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1152/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1152/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer WUkY"
                    },
                    "comment": {
                        "value": "Thank you for your helpful questions, suggestions and for pointing out this interesting paper. Below is our response:\n\n> How much do different synthetic data construction methods affect the results?\n> It is more convincing to evaluate the proposed methods on more tasks.\n\nAs discussed in the main paper, when constructing the synthetic dataset, different parameters such as the number of steps in Markov Chain (MC) data, state space size, and state transition dynamics temperature can all affect performance. Here we summarize the best- and worst-performing settings in our ablations for DT (results are averaged across all 12 datasets):\n- MC number of steps: 1-MC (73.6), 5-MC (71.8)\n- State space size: S1000 (74.0), S10 (71.9)\n- Temperature: tau = 1 (73.6), IID (71.3)\n\nWe also tried two new synthetic data schemes, Identity Operation and Case Mapping, which are studied in synthetic pre-training papers for NLP (with details presented in page 30 in the Appendix):\n- DT+Identity (70.0)\n- DT+Mapping (70.4)\n\nNote that for all these synthetic pre-training variants, we are able to achieve similar or better performance compared to DT+Wiki (69.9) and DT baseline (67.7). These results show that simple synthetic pre-training across different settings can consistently outperform the performance from Wiki pre-training, and support our claim that Wiki pre-training does not provide a special benefit for offline RL. \n\nWe have also added other new experiments and analyses to improve the extensiveness of our empirical results and make our conclusions more convincing, and we will continue to conduct more experiments. Here is a summary:\n\n(1) On page 31 of the paper, we added new figures showing how different synthetic data configurations affect performance when given a different number of finetuning updates.\n\n(2) On the same page, we also present new results showing how their performances are affected by different finetuning dataset ratios.\n\nThe results from all these new experiments show that in all experiment settings, the proposed synthetic pre-training scheme achieves similar or better performance compared to DT+Wiki, showing that synthetic pre-training is quite robust in improving offline RL performance.\n\n(3) In order to further understand whether the improved performance of pre-training schemes can be achieved by simply training the DT baseline for a larger number of finetuning updates, we conducted the following experiments (More details can be found on page 28 of the paper):\n\n|            |           DT | DT+20K more|DT+80K more|DT+Wiki|DT+Synthetic|\n| ----------- | ----------- | ---| ---| ---| ---|\n| Final performance  | 67.2 \u00b1 4.0| 69.1 \u00b1 5.1| 70.7 \u00b1 4.6| 69.9 \u00b1 5.6| **73.6** \u00b1 4.9|\n\nThese results show that:\n1. Recall DT with 80K more updates has the same total number of updates as DT+Wiki (180K total updates). They achieve a similar performance. This result further supports our finding that Wiki pre-training does not bring a special benefit.\n2. DT with 20K more updates has the same total number of updates as DT+Synthetic (120K total updates). However, DT+Synthetic achieves stronger performance (73.6 > 69.1). Even when DT trains for 80K more updates instead of 20K, DT+Synthetic is still stronger (73.6 > 70.7). This shows synthetic pre-training brings a performance boost that cannot be achieved by simply taking more finetuning updates.\n\nWe also present the learning curves for DT, DT+Wiki and DT+Synthetic all trained to 180K total updates; they show DT+Synthetic consistently achieves the best performance for different total numbers of updates.\n\n\n> The experiments lack comparison for some pre-trained DT models, such as Future-conditioned Unsupervised Pre-training for Decision Transformer(https://proceedings.mlr.press/v202/xie23b/xie23b.pdf).\n\nThank you for pointing us to this interesting paper. We have added a discussion of it in our related work section. The paper focuses on the problem of offline pre-training to online finetuning. Our work is different from this paper in that (1) we study the offline RL setting, where further online interactions are not allowed; and (2) we focus on investigating whether Wiki pre-training provides a special benefit for offline RL.\n\nAs reviewer jxkz pointed out, a major strength of our work is it \"Presents empirical evidence that contradicts the prevailing belief that language data is essential for pre-training models for offline Deep Reinforcement Learning.\" And we deliberately choose very simple synthetic pre-training schemes to show that even these simple schemes can provide a similar or better performance boost, and thus showing Wiki pre-training does not provide a special benefit.\n\nWe would like to thank the reviewer again for the helpful questions and suggestions, and we are happy to discuss any further questions or comments."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1152/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700734487725,
                "cdate": 1700734487725,
                "tmdate": 1700734487725,
                "mdate": 1700734487725,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "dB9dt84aHw",
            "forum": "PcxQgtHGj2",
            "replyto": "PcxQgtHGj2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1152/Reviewer_dSfS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1152/Reviewer_dSfS"
            ],
            "content": {
                "summary": {
                    "value": "The manuscript presents a unique approach to pre-training for offline reinforcement learning, utilizing synthetic data generated by a Markov Chain in lieu of traditional real-world language resources. The core premise is that this synthetic data can achieve comparable results to real-world data in downstream task performance, which is a significant assertion in the field of offline RL."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Clarity of Presentation: The paper is well-structured, making it accessible even to those who may not be deeply versed in the domain. The significance of the research question is conveyed effectively, which facilitates a quick grasp of the paper's importance.\n\n2. Innovation in Data Construction: The methodology employed for the generation of synthetic data is both novel and straightforward, potentially offering a simpler alternative to more complex data generation strategies."
                },
                "weaknesses": {
                    "value": "1. Methodological Justification: The rationale behind the adoption of a Markov Chain for synthetic data generation requires further elaboration. While the introduction suggests that understanding the underlying question is crucial for enhancing pre-training in deep reinforcement learning (DRL), the link between this understanding and the proposed method is not convincingly established.\n\n2. Need More Deep Analysis: The paper primarily demonstrates the efficacy of the proposed method without a robust analysis. It is advisable that the authors consider incorporating analysis akin to those found in the literature regarding Synthetic Data utilization in Transformer models (see Synthetic Pre-Training Tasks for Neural Machine Translation (ACL 2023) and its related works). This could potentially refine the proposed method and offer deeper insights through a more comprehensive analysis."
                },
                "questions": {
                    "value": "1. Could you provide a more detailed justification for the methodological choices, specifically the use of a Markov Chain for data synthesis?\n\n2. Are there illustrative examples of synthetic data that could be shared to better understand its characteristics and how it compares to real-world data?\n\n**After Rebuttal**\nThank you for the response. I have raised my score from 5 to 6 and my confidence is 2."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1152/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1152/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1152/Reviewer_dSfS"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1152/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699277257448,
            "cdate": 1699277257448,
            "tmdate": 1700819916999,
            "mdate": 1700819916999,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "lqiQWgHKD5",
                "forum": "PcxQgtHGj2",
                "replyto": "dB9dt84aHw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1152/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1152/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer dSfS"
                    },
                    "comment": {
                        "value": "Thank you for your detailed review and insightful questions. We have carefully read the paper (Pre-Training Tasks for Neural Machine Translation) that you mentioned. We find it insightful, and we have made sure to cite the paper and provide additional experiments inspired by the synthetic data strategies in the paper. We would like to first answer your question on an example of the synthetic data, and then address your other comments and concerns:\n\n> Are there illustrative examples of synthetic data that could be shared to better understand its characteristics and how it compares to real-world data?\n\nHere is a concrete example of a synthetic MC dataset: assume we have a state space of 3, and we use \u201cA\u201d, \u201cB\u201d, and \u201cC\u201d to refer to these states respectively. Assume we use 1-MC, so the next state only depends on the previous state.\n\nWe generate a transition probability table, which can be:\n\n|   | A | B   | C   |\n|---|---|-----|-----|\n| A | 0 | 0.5 | 0.5 |\n| B | 0 | 0   | 1   |\n| C | 1 | 0   | 0   |\n\nThis table means when in state A, we have 50% chance of transitioning to B and 50% chance of transitioning to C each; and when in state B we will always transition to C; when in C we always transition to A.\n\nTo generate one trajectory, we first randomly sample a state, for example, A, and then follow the transition probability table to generate rest of the trajectory. Assume the trajectory has a length of 3. Then for example, the generated trajectories can be:\n\nA B C\n\nC A C\n\nB C A\n\nA C A\n\n. . .\n\nEssentially, the transition probability table is like a frequency table for N-grams of text in NLP. And in our case, 1-MC is essentially 1-gram.\n\nLanguage data from the real world can be seen as generated from a much larger state space (the state space here is essentially the vocabulary), and with longer-term dependencies. In some sense, a 1-MC synthetic dataset with a state space of 100 can be seen as generated from an unknown language with a vocabulary size of 100, and each word in this language only depends on the previous word.\n\n\n> Methodological Justification: The rationale behind the adoption of a Markov Chain for synthetic data generation requires further elaboration. While the introduction suggests that understanding the underlying question is crucial for enhancing pre-training in deep reinforcement learning (DRL), the link between this understanding and the proposed method is not convincingly established.\n\n> Could you provide a more detailed justification for the methodological choices, specifically the use of a Markov Chain for data synthesis?\n\nThank you for this excellent question, we chose a Markov Chain (MC) for data synthesis for the following reasons:\n\n1. Simplicity: We deliberately choose a simple way to generate the synthetic data. This is important because, as reviewer jxkz pointed out, a major contribution of our paper is that it \u201cpresents empirical evidence that contradicts the prevailing belief that language data is essential for pre-training models for offline Deep Reinforcement Learning\u201d. In our paper, we show that even synthetic pre-training with IID data can outperform DT+Wiki. Furthermore, the two simple schemes from the paper you suggested do as well as DT+Wiki. This indicates that Wiki pre-training does not provide a special benefit for offline RL.\n\n2. Connection to language data: As mentioned above, a synthetic MC dataset can be seen as generated from an unknown language. Compared to the human language, this unknown language may have different vocabulary sizes (the state space size), shorter- or longer-term dependencies (controlled by the number of MC steps), and different grammar (controlled by the transition probabilities). And a human language dataset can be seen as generated from a special MC, with long-term dependency, large vocabulary size, and a specific transition probability table.\n\nSo essentially DT+Synthetic can be seen as trained with a language dataset that has short-term dependency, a small vocabulary, and overall less structure. Given this setup, the fact that DT+Synthetic can achieve significantly better performance than DT+Wiki shows that Wiki data (along with its unique language properties) does not provide a special benefit for offline RL."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1152/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700734120978,
                "cdate": 1700734120978,
                "tmdate": 1700734120978,
                "mdate": 1700734120978,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "iYUvM4Y7Op",
                "forum": "PcxQgtHGj2",
                "replyto": "dB9dt84aHw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1152/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1152/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer dSfS (continued)"
                    },
                    "comment": {
                        "value": "> Need More Deep Analysis: The paper primarily demonstrates the efficacy of the proposed method without a robust analysis. It is advisable that the authors consider incorporating analysis akin to those found in the literature regarding Synthetic Data utilization in Transformer models (see Synthetic Pre-Training Tasks for Neural Machine Translation (ACL 2023) and its related works). This could potentially refine the proposed method and offer deeper insights through a more comprehensive analysis.\n\nThank you for the great suggestion and for bringing this paper to our attention. We have added a discussion to our related work section, and have tested two alternative synthetic data generation methods from this paper: Identity Operation and Case Mapping. The results are summarized below, with more details provided in Appendix F (page 30).\n\n| Average Last Four      | DT         | DT+Wiki    | DT+Synthetic | DT+Identity | DT+Mapping |\n|------------------------|------------|------------|--------------|-------------|------------|\n| Average (All Settings) | 67.7 \u00b1 5.4 | 69.9 \u00b1 5.6 | 73.6 \u00b1 4.9   | 70.0 \u00b1 5.5  | 70.4 \u00b1 5.7 |\n\nThe results show these alternative data generation methods can also match the performance of DT+Wiki, while DT+Synthetic is still the best. This result further supports our claim that Wiki pre-training does not provide a special benefit for offline RL.\n\nFollowing your suggestion, we also conducted more analysis experiments to gain a deeper insight into the effect of different pre-training schemes. The results are summarized in the table below, with more details and a figure presented in Appendix H (page 32).\n\nHere for each dataset, we look at the weights and features of the trained network at different training stages, and compare their similarity. Here RI stands for random initialization, PT stands for pre-train, and FT stands for fine-tune.\n\n| Average (All Settings) | DT | DT+Wiki | DT+Synthetic | DT+IID | DT+Identity | DT+Mapping |\n|------------------------|----|--------|--------------|-------|-------------|------------|\n| RI vs. FT Feature Sim. | **0.64** | 2.9E-03 | 5.2E-05     | 3.6E-03 | 3.9E-03     | -2.6E-03   |\n| PT vs. FT Feature Sim. | -  | 0.33   | 0.66         | **0.73** | 0.56       | 0.62       |\n|----|----|----|----|----|----|----|\n| RI vs. FT Weight Sim.  | **0.72** | 0.59   | 0.58        | 0.65   | **0.70**    | **0.69**   |\n| PT vs. FT Weight Sim.  | -  | **0.84** | 0.76        | 0.77   | **0.81**    | **0.82**   |\n\nWe find that the cosine-similarity between the initial weights and the weights after fine-tuning (RI vs. FT) for all the pre-training schemes are lower compared to that of the DT baseline (RI vs. fine-tuning without pre-training at all). This suggests that pre-training together with fine-tuning alters the angle of the weights more than when doing fine-tuning alone. This phenomenon suggests that pre-training is able to move the weight vector to a new subspace which is more beneficial for downstream RL tasks.\n\nWe observe that the weight similarities for the pre-training schemes are inversely proportional to their final performance (DT+Synthetic has the best performance while being the least similar, while DT+Wiki has the worst performance while being the most similar). This suggests that, during the fine-tuning stage, encouraging a bigger movement in weights is more beneficial, and that our synthetic pre-training scheme allows for such a movement.\n\nSimilar to the weight comparison, we also find that the cosine-similarities between the features from randomly initialized models and those after fine-tuning (RI vs. FT) for all the pre-training schemes are much lower compared to that of the DT baseline (by three orders of magnitude), suggesting a bigger change of the features from pre-trained and then fine-tuned models than that when doing fine-tuning alone. Such a movement of the feature vectors might indicate better learning of the feature representations.\n\nIn addition, the feature similarity for DT+Wiki before and after fine-tuning is lower (0.33) than that of the synthetic pre-training schemes, suggesting that the features need to be altered more due to the domain gap between language and RL, potentially hindering its performance.\n\nWe would like to thank the reviewer again for the insightful questions and suggestions, and we are happy to discuss any further questions or comments."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1152/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700734220491,
                "cdate": 1700734220491,
                "tmdate": 1700740773693,
                "mdate": 1700740773693,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]