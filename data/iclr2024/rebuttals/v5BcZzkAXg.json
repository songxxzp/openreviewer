[
    {
        "title": "Multi-label Learning with Random Circular Vectors"
    },
    {
        "review": {
            "id": "Io04q3BBGY",
            "forum": "v5BcZzkAXg",
            "replyto": "v5BcZzkAXg",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2445/Reviewer_Cc3g"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2445/Reviewer_Cc3g"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new strategy to mitigate the large computational and resource expenses of deep neural networks in the context of the extreme multi-label classification task. The authors advocate using random circular vectors as the prediction of the final layer of classifiers, where each vector component is represented as a complex amplitude. The authors claim that the proposed method helps to decrease the scale of output layers and improve performance. The provided experiments confirm this assertion."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The authors proposed an approach to improve the ability to represent data instances that belong to many classes.\n\n - The authors provide extensive experiments to prove that the proposed approach outperforms its counterpart.\n\n - The proposed approach (CHRR) involves double output nodes in comparison with HRR, however, experiments show that by halving CHRR nodes into two groups, CHRR-Half is able to maintain similar performance while mitigating the scaling problem."
                },
                "weaknesses": {
                    "value": "- The variances of the proposed model (CHRR-sin, CHRR-tanh) show minimal empirical improvement in the provided experiments. The motivation of it is also ambiguous. The authors are recommended to provide more details to explain or reconstruct this part.\n\n - Specifically, the absolute value of CHRR appears to be comparable between Figure 5(a)/6(a) and Figure 5(b)/6(b), but this consistency is not reflected in Figure 5(c)/6(c), even though they exhibit very similar trends. It is advised that the authors thoroughly review their figures and tables to eliminate any potential errors or misuses. If this is not the case, the authors are encouraged to provide a clear explanation of the notable performance improvement.\n\n-  The authors give less-than-convincing explanation on the problem of `Wiki10-31K P@1` performance. The increase `Wiki10-31K P@5` and `Wiki10-31K P@10`  would not be as dramatic as it is if the explanation is valid. The authors are recommended to provide a convincing explanation of this problem.\n\n- The authors underscore that the proposed methods reduce the size of the output layer, but few details are provided. On what criteria do the authors conclude that the output size is reduced by 59%~97%? What is the figure for each experiment? \n\n- The details of network FC are not provided. The authors claim that they adopt the code provided by Ganesan et al 2021 but no further information is available.  The authors should provide a clear setting for it."
                },
                "questions": {
                    "value": "- Can authors provide any insight on the performance degradation in Fig 7(a, b) other than section5.4?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2445/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698077454498,
            "cdate": 1698077454498,
            "tmdate": 1699636180468,
            "mdate": 1699636180468,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zvyPaxvOb9",
                "forum": "v5BcZzkAXg",
                "replyto": "Io04q3BBGY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2445/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2445/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Explanations for your concerns about our experiments"
                    },
                    "comment": {
                        "value": "Reviewer Cc3g: We would like to express our sincere appreciation for your careful reading and valuable feedback on our paper. We greatly appreciate your insightful comments and questions.\n\n### Weakness & Questions\n\n#### W1. The variances of the proposed model (CHRR-sin, CHRR-tanh) show minimal empirical improvement in the provided experiments. The motivation of it is also ambiguous. The authors are recommended to provide more details to explain or reconstruct this part.\n\n- A1. As you pointed out, the motivation for the experiment was not clearly stated. One of the main contribution is that the proposal of an effective method to integrate CHRR into a deep learning model. Therefore, we conducted the experiment to demonstrate the advantage of the proposed architecture (CHRR) against naive implementation (CHRR-sin, CHRR-tanh). We added the mention for the motivation into S4.2 to clarify it.\n\n#### W2. Specifically, the absolute value of CHRR appears to be comparable between Figure 5(a)/6(a) and Figure 5(b)/6(b), but this consistency is not reflected in Figure 5( c )/6( c ), even though they exhibit very similar trends. It is advised that the authors thoroughly review their figures and tables to eliminate any potential errors or misuses. If this is not the case, the authors are encouraged to provide a clear explanation of the notable performance improvement.\n\n- A2.\n- First, we are sorry for putting an incorrect figure in Figure 6( c ). In the current draft paper, we replaced all figures with correct ones.\n- For a detailed analysis, we conducted more experiments by varying the dimension size of hidden layers of FC, HRR and CHRR from 768 to 2,048, respectively. We displayed the results in Figure 5 and in Table 3 of our new draft paper. We think that the results clearly show the advantage of CHRR over HRR. If the problem still remains, it would be helpful if you could point it out again.\n\n#### W3. The authors give less-than-convincing explanation on the problem of Wiki10-31K P@1 performance. The increase Wiki10-31K P@5 and Wiki10-31K P@10 would not be as dramatic as it is if the explanation is valid. The authors are recommended to provide a convincing explanation of this problem.\n\n- A3. Since P@5,10,20 are more important than P@1 for XMC, Figure 5 in our current draft paper shows the results for P@5 instead of P@1.\n\n#### W4. The authors underscore that the proposed methods reduce the size of the output layer, but few details are provided. On what criteria do the authors conclude that the output size is reduced by 59%~97%? What is the figure for each experiment?\n- A4.\n- In our current draft paper, we reported the compression rate ((1-d/L) * 100%) (d: dimension size of output layer, L: the number of labels) and did not include the size \"d * L\" of non-learnable label matrix of CHRR/HRR. We will consider to show the model size (h * L for FC and h * d + d * L for CHRR/HRR) in the final version of our paper. We would like to note that when h=2,048 for FC, the model size of CHRR/HRR (h=768) is smaller than that of FC. \n\n\n#### W5. The details of network FC are not provided. The authors claim that they adopt the code provided by Ganesan et al 2021 but no further information is available. The authors should provide a clear setting for it.\n\n- A5. We are sorry for omitting the details. In S5.3 of our current draft paper, we described the information about the FC network architectures used in our experiments.\n\n#### Q1. Can authors provide any insight on the performance degradation in Fig 7(a, b) other than section5.4?\n\n- A6. Data samples belonging to many class labels represent a variety of concepts and it may be difficult to make label predictions for such data samples.\n\n---"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2445/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700043196281,
                "cdate": 1700043196281,
                "tmdate": 1700043196281,
                "mdate": 1700043196281,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "IygyKgxg3y",
                "forum": "v5BcZzkAXg",
                "replyto": "zvyPaxvOb9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2445/Reviewer_Cc3g"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2445/Reviewer_Cc3g"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate the authors for their comprehensive responses. There still remain some problems:\n\n1. There seems to be a red line missing in Figure 5(d)\n2. I agree with the author that the model size, including the non-learnable label matrix should be indicated in Table3\n3. I share the concerns with other reviewers on the backbone. The authors should provide a comparison on different models other than FC.\n\nThe authors have not provided their final paper yet. Thus, I keep my score."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2445/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700639464043,
                "cdate": 1700639464043,
                "tmdate": 1700639464043,
                "mdate": 1700639464043,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "WdTZqOFGSr",
            "forum": "v5BcZzkAXg",
            "replyto": "v5BcZzkAXg",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2445/Reviewer_7H1H"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2445/Reviewer_7H1H"
            ],
            "content": {
                "summary": {
                    "value": "Existing DNN methods for XMC problem often consist of a large output label matrix where each column corresponds to a trainable label vector. This paper proposes the use of random circular vectors as non-learnable label vectors, which significantly reduce the trainable model parameters. The author proposed Circular-HRR (CHRR), which represents random circular vectors in complex domain, and designs a model architecture that predicts a low-dimensional circular vector. On moderate-size XMC datasets, the proposed CHRR method performs better than the fully-connected baseline as well as the previous method HRR."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The overall presentation of the paper is clear and easy to follow\n\n2. Using circular vectors with complex amplitude is technically sound"
                },
                "weaknesses": {
                    "value": "1. CHRR do not reduce model parameters at the inference stage, compared to the FC baseline.\n\n2. The inference time complexity of CHRR is as high as `O(L)`  while FC and HRR can be `O(log(L))`. \n\n3. The experiment results are not very comprehensive. see detailed questions below."
                },
                "questions": {
                    "value": "1. The proposed method (CHRR) reduces the \"trainable\" model parameters by using a non-learnable label matrix at the training stage. However, CHRR do not reduce the model parameters at inference stage, because it still need to store the non-learnable label matrix for finding top-k most relevant labels, given the model-predicted output random circular vector. What's the model parameters used at inference time, compared to HRR and FC?\n\n2. At the inference stage, to compute similarity between the label matrix and the model predicted random circular vector, CHRR seems to involve more complex computations (Table 1) that is non-standard euclidean distance metric. This suggests CHRR can not enjoy the advantage of fast/approximate nearest neighbor search methods that reduces the inference time complexity from `O(L)` to `O(log(L))` where `L` is the number of labels. On the other hand, HRR can actually leverage ANN search methods at the inference stage. Any analysis on the inference time complexity and the actually inference latency? \n\n3. The experiment results are not very comprehensive:\n(1) There should be a table comparing the model size at training/inference stage, and the detail hyper-parameters such as `h` and `d`.\n(2) Compared to the FC baseline, It is not clear whether the performance gain of CHRR is from larger model size (including the non-learnable label matrix). \n(3) The proposed method did not compare with more advanced XMC methods, such as Transformer-based encoders."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2445/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698602146859,
            "cdate": 1698602146859,
            "tmdate": 1699636180391,
            "mdate": 1699636180391,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "cKTSnksBqf",
                "forum": "v5BcZzkAXg",
                "replyto": "WdTZqOFGSr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2445/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2445/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Explanations for your concerns about our experiments and model efficiency"
                    },
                    "comment": {
                        "value": "Reviewer 7H1H: We would like to express our gratitude for your valuable feedback on our paper. We greatly appreciate your insightful comments and questions.\n\n### Weakness & Questions\n\n#### W1. CHRR do not reduce model parameters at the inference stage, compared to the FC baseline.\n#### Q1. What's the model parameters used at inference time, compared to HRR and FC?\n#### W2. The inference time complexity of CHRR is as high as O(L) while FC and HRR can be O(log(L)).\n#### Q2. Any analysis on the inference time complexity and the actually inference latency?\n\n- A1.\n- First, we could improve the inference efficiencies of both HRR and CHRR using an Approximate Nearest Neighbor (ANN) method for the Maximum Inner Product Search (MIPS). The reason why we can apply MIPS to circular vectors is that the CHRR\u2019s similarity operation between two circular vectors \u03b8 and \u03c6 is actually equivalent to the inner product between two real-valued vectors, which are obtained via the inverse FFTs of  \u03b8 and \u03c6, respectively (The fact is simply based on the Parseval\u2019s theorem).\n- As you pointed out, HRR and CHRR require d * L parameters for keeping non-learnable label matrix in the inference stage.\n- Speedups in training time for CHRR against FC were almost the same as those for HRR reported in Table 3 of [Ganesan et. al. 2021], while CHRR provides a clean and better label encoding/retrieval framework for XMC than HRR. While CHRR does not require any FFT/iFFT in the inference stage, its similarity computation contains the calculation of the cos function, which makes the inference slow. From our experimental results, the computational cost for CHRR is almost the same as that of HRR in practice.\u3000 \n\n#### W3. The experiment results are not very comprehensive. see detailed questions below.\n\n#### Q3.(1) There should be a table comparing the model size at training/inference stage, and the detail hyper-parameters such as h and d.\n- A21. \n- Please see Table 3 of our new draft paper.\n- For a detailed analysis, we conducted more experiments by varying the dimension size of hidden layers of FC, HRR and CHRR from 768 to 2,048, respectively. Especially, the performance of FC was improved when h=2,048. We displayed the new results in Figure 5 and Table 3 of our new draft paper. In Table 3, we described the compression rate ((1-d/L) * 100%), but as you pointed out, we will consider to show the model size (h * L for FC and h * d + d * L for CHRR) in the final version of our paper.\n\n#### Q3.(2) Compared to the FC baseline, it is not clear whether the performance gain of CHRR is from larger model size (including the non-learnable label matrix).\n- A22.\n- According to A21, we confirmed that the performance gain of CHRR is not necessarily caused by a larger model size. For example, to see Figure 5 and Table 3, CHRR (h=768) with d>=400 on Wiki10-31K outperforms FC in all metrics while the model size of CHRR (including the non-learnable label matrix) is smaller than FC. \n- We conducted additional experiments with a larger dimension size (h=2,048) of hidden layers in FC, where the number of model parameters was larger than that of CHRR/HRR. However, CHRR/HRR still outperformed FC for several datasets. Though the reason is unclear, we suspect that FC requires a larger dimension size for hidden layers due to an extremely large output space. We would like to further investigate this point in the final version (since additional experiments require a lot of machine resources.).\n\n#### Q3.(3) The proposed method did not compare with more advanced XMC methods, such as Transformer-based encoders.\n- A23.\n- For Wiki10-31k, we conducted additional experiments by using input features generated by XLNet [Chang et. al. 2020] instead of BoW features to investigate the potential of transformer-based models. The results were comparable with recent models.\n    -   | FC w/XLNet |h=768|h=1024|h=2048|h=4096| \n        | ----       | --- | ---  | ---  | ---  |\n        | P@1        |81.5 |83.3  |84.0  |85.1  |\n        | P@5        |54.2 |56.6  |58.9  |61.7  | \n        | P@10       |39.2 |41.6  |43.6  |46.1  | \n        | P@20       |26.2 |27.9  |29.5  |31.2  | \n  \n    -   | CHRR w/XLNet |h=768,d=800|h=768,d=1500 | \n        | ---- | --- | --- | \n        | P@1  |85.5     |86.1     | \n        | P@5  |61.5     |62.7     | \n        | P@10 |43.7     |45.8     | \n        | P@20 |25.5     |28.8     | \n- In the final version of this paper, we will show more experimental results using fine-tuned LMs for other datasets including AmazonCat-13K and Wiki-500K. Moreover, we will try a combination of XLNet and TF-IDF features for further performance improvements as X-Transformer [Chang et. al. 2020] has done.\n\n---"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2445/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700045119505,
                "cdate": 1700045119505,
                "tmdate": 1700118072311,
                "mdate": 1700118072311,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "nYhIvmF5gF",
            "forum": "v5BcZzkAXg",
            "replyto": "v5BcZzkAXg",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2445/Reviewer_vWH8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2445/Reviewer_vWH8"
            ],
            "content": {
                "summary": {
                    "value": "This paper is about improving the representative power of embeddings in Extreme Multi-label Learning (XML) with ideas from Holographic Reduced Representations (HRR). Typical XML approaches, which leverage a large linear classifier layer to map an input to a label set, have limitations: the linearity constraint restricts the modelling power while the enormous number of classifiers blow-up the time and space complexities. Several approaches have been proposed to mitigate the complexity issues, e.g. tree-search based and negative label mining based approaches. This paper alternatively proposes to use HRRs which learn classifiers in the Fourier-transformed space instead of the original linear embedding space, thus resulting in more powerful, non-linear classifier learning. The proposed approach also brings down the training complexity by leveraging loss functions that depend only on the positive labels (which are typically sparse).\n\nThe paper largely exploits the key ideas from earlier papers [HRR in Plate '95, HRR for XML in Ganesan et.al. '21]. In addition, it generalizes  [Ganesan et.al.'21] through Complex-valued HRR representations. Experiments demonstrate that, keeping other factors constant, the accuracy with CHRR > HRR > naive fully connected XML classifier layer."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "* The paper introduces the innovative idea of complex-valued holographic reduced representations (CHRR) for XML tasks which can significantly improve the XML prediction accuracy over and above that achieved by real-valued HRR\n\n* Experiments demonstrate the efficacy of proposed approach on several moderately large-scaled XML datasets in terms of P and PSP gains"
                },
                "weaknesses": {
                    "value": "* The contributions of this paper are rather limited. The key ideas behind adapting HRR to XML, such as unitary normalization and HRR XML loss, have been borrowed from [Ganesan et.al. '21]. The main novelty lies in generalizing real to complex HRR. While this is useful, its efficiency-accuracy trade-offs relative to original HRR have not been well established.\n\n* The experimental validation of proposed approach is weak. \n- Datasets involve moderate scale XML datasets and none from >1 million scale\n- Datasets considered are bag-of-words based whereas contemporary XML literature has shifted focus to transformer-learnt representations\n- Model architecture considered is based on fully-connected layers whereas contemporary XML literature has shifted focus to transformers\n- No comparison is provided with existing schemes to reduce training complexity such as tree, hashing or negative label sampling based approaches\n- Even though the main claim of this paper is to reduce training complexity, no training time comparisons have been reported\n- Due to all these factors, the real utility and impact towards XML field due to this paper is hard to evaluate. A substantial amount of additional work is needed in this direction\n\n* The proposed approach only improves training time and does not appear to reduce prediction time or memory requirements which are also important requirements in XML"
                },
                "questions": {
                    "value": "* What are the cost-accuracy trade-offs of CHRR vs HRR with cost measured in wallclock time and ram requirements?\n* How does CHRR fare relative to FC and HRR on BERT based architectures and on datasets with much larger quantity of labels?\n* What are the relative advantages and disadvantages of CHRR versus tree, hashing or negative label sampling based approaches to reduce XML training and prediction complexities ?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2445/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698656449138,
            "cdate": 1698656449138,
            "tmdate": 1699636180305,
            "mdate": 1699636180305,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7VqwgAa2fv",
                "forum": "v5BcZzkAXg",
                "replyto": "nYhIvmF5gF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2445/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2445/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Novelty of our work (Weakness)"
                    },
                    "comment": {
                        "value": "Reviewer vWH8: We would like to express our gratitude for your valuable feedback on our paper. We greatly appreciate your insightful comments and questions.\n\n### Weakness\n\n#### W1. The contributions of this paper are rather limited.\n\n- A1. We would like to highlight that our main contributions are; (1) We revealed the issue on the unitary normalization method of [Ganesan et.al. '21] in S3 and (2) We proposed an effective method to integrate CHRR into a Deep Learning (DL) framework as presented in S4. In the point (1), we raised a new perspective rather than borrowing ideas from [Ganesan et.al. '21]. In the second point (2), it was not obvious how to integrate CHRR into a DL framework as we discussed in S4.2 and S5.5. We believe that these two points are our concrete contributions.\n\n#### W2. The experimental validation of proposed approach is weak.\n\n#### W3. Datasets involve moderate scale XML datasets and none from >1 million scale\n\n#### W4. Datasets considered are bag-of-words based whereas contemporary XML literature has shifted focus to transformer-learnt representations\n\n- A2.\n- For Wiki10-31k, we conducted additional experiments by using input features generated by XLNet [Chang et. al. 2020] instead of BoW features to investigate the potential of transformer-based models. The results were comparable with those of recent models.\n    -   | FC w/XLNet |h=768|h=1024|h=2048|h=4096| \n        | ----       | --- | ---  | ---  | ---  |\n        | P@1        |81.5 |83.3  |84.0  |85.1  |\n        | P@5        |54.2 |56.6  |58.9  |61.7  | \n        | P@10       |39.2 |41.6  |43.6  |46.1  | \n        | P@20       |26.2 |27.9  |29.5  |31.2  | \n  \n    -   | CHRR w/XLNet |h=768,d=800|h=768,d=1500 | \n        | ---- | --- | --- | \n        | P@1  |85.5     |86.1     | \n        | P@5  |61.5     |62.7     | \n        | P@10 |43.7     |45.8     | \n        | P@20 |25.5     |28.8     |\n- In the final version of this paper, we will show more experimental results using fine-tuned LMs for other datasets including AmazonCat-13K and Wiki-500K. Moreover, we will try a combination of XLNet and TF-IDF features for further performance improvements as X-Transformer [Chang et. al. 2020] has done.\n- For a detailed analysis, we also conducted more experiments by varying the dimension size of hidden layers of FC, HRR and CHRR from 768 to 2,048, respectively. We displayed the new results in Figure 5 and Table 3 of our new draft paper. Especially, for Delicious-200k, CHRR outperformed HRR more clearly, compared to the older version. We here used P@5 instead of P@1 for clear explanations (We think that P@k (k>=5) is much important than P@1 for XMC).\n\n#### W5. Model architecture considered is based on fully-connected layers whereas contemporary XML literature has shifted focus to transformers\n- A3.\n- Transformer-based models for XMC tasks (e.g. [Chang et. al. 2020]) employ some tricks such as label clustering to reduce the learning/prediction cost of the large language models. Such tricks make a fair comparison of the label representations of CHRR and HRR difficult. Therefore, we decided to use a fully-connected layer as our fundamental architecture.\n- On the other hand, we agree with your point. However, it is not easy to find a method to apply CHRR to complicated transformer-based models. Instead, we conducted additional experiments to investigate the potential of transformer-based models, as we mentioned in A2. In the future, we would like to investigate a better way to combine CHRR with transformer-based models.\n\n#### W6. No comparison is provided with existing schemes to reduce training complexity such as tree, hashing or negative label sampling based approaches\n\n- A4. The vector symbolic architectures like HRR are actually similar to the concept of Bloom Filter as shown in the following theoretical paper. We also think that comparisons with other methods like tree and hashing are important. In a furture work, we would like to investigate how to use the Autoscaling Bloom Filter (a hashing method) for training XMC models more efficiently. \n    - Kleyko et. al., \"Autoscaling Bloom Filter: Controlling Trade-off Between True and False Positives\", Neural Computing and Applications, 2019.\n\n#### W7. Even though the main claim of this paper is to reduce training complexity, no training time comparisons have been reported\n\n- A5. Speedups in training time for CHRR against FC were almost the same as those for HRR reported in Table 3 of [Ganesan et. al. 2021], while CHRR provides a clear and better label enconding/retrieval framework for XMC than HRR."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2445/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700042190127,
                "cdate": 1700042190127,
                "tmdate": 1700118854929,
                "mdate": 1700118854929,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "sHCS5K1XIJ",
                "forum": "v5BcZzkAXg",
                "replyto": "nYhIvmF5gF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2445/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2445/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Novelty of our work (Questions)"
                    },
                    "comment": {
                        "value": "(Continuing the Response to Reviewer vWH8)\n\n#### W8. The proposed approach only improves training time and does not appear to reduce prediction time or memory requirements which are also important requirements in XML\n\n- A6. We could improve the inference efficiencies of both HRR and CHRR using an Approximate Nearest Neighbor (ANN) method for the Maximum Inner Product Search (MIPS). The reason why we can apply MIPS to circular vectors is that the CHRR\u2019s similarity operation between two circular vectors \u03b8 and \u03c6 is actually equivalent to the inner product between two real-valued vectors, which are obtained via the inverse FFTs of \u03b8 and \u03c6, respectively (The fact is simply based on the Parseval\u2019s theorem).\n### Questions\n\n#### Q1. What are the cost-accuracy trade-offs of CHRR vs HRR with cost measured in wallclock time and ram requirements?\n\n- A7.\n- Regarding to the memory cost, CHRR has no clear disadvantage against HRR: CHRR has twice the label encoding capacity and can retrieve labels more accurately than HRR as we shown in S3.\n- However, while CHRR does not require FFTs and inverse FFTs, the similarity operation of CHRR requires the cos function, which makes the inference slow in practice. However, as we mentioned in A6, since the similarity operation of CHRR can be equivalently transformed into the inner product in the real-valued vector space, ANN methods could be used to reduce the computational complexity.\n- As we mentioned in A5, training time of CHRR was almost the same as HRR for all datasets.\n\n#### Q2. How does CHRR fare relative to FC and HRR on BERT based architectures and on datasets with much larger quantity of labels?\n\n- A8.\n- Each sample in Delicious-200k has several tens of labels on average, and CHRR outperformed HRR clearly on the dataset, as we shown in Figure 5 (g)-(l) of our new draft paper. \n- While there is no fine-tuned pretrained language model for Delicious-200k, FC, HRR and CHRR with a fine-tuned XLNet for Wiki10-31k achieved the following performances, which are comparable with recent Transformer-based models.\n    -   | FC w/XLNet |h=768|h=1024|h=2048|h=4096| \n        | ----       | --- | ---  | ---  | ---  |\n        | P@1        |81.5 |83.3  |84.0  |85.1  |\n        | P@5        |54.2 |56.6  |58.9  |61.7  | \n        | P@10       |39.2 |41.6  |43.6  |46.1  | \n        | P@20       |26.2 |27.9  |29.5  |31.2  | \n  \n    -   | CHRR w/XLNet |h=768,d=800|h=768,d=1500 | \n        | ---- | --- | --- | \n        | P@1  |85.5     |86.1     | \n        | P@5  |61.5     |62.7     | \n        | P@10 |43.7     |45.8     | \n        | P@20 |25.5     |28.8     | \n\n#### Q3. What are the relative advantages and disadvantages of CHRR versus tree, hashing or negative label sampling based approaches to reduce XML training and prediction complexities ?\n\n- A9.\n- As we mentioned in A4, in our future work, we would like to compare CHRR with a hashing method based on Autoscaling Bloom Filter.\n- As we mentioned in A5, CHRR can improve the training efficiency of FC networks, while providing a better label encoding/retrieval framework for XMC than HRR."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2445/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700042252234,
                "cdate": 1700042252234,
                "tmdate": 1700118913605,
                "mdate": 1700118913605,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "q3EGm0s8UY",
                "forum": "v5BcZzkAXg",
                "replyto": "sHCS5K1XIJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2445/Reviewer_vWH8"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2445/Reviewer_vWH8"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for addressing the concerns of novelty and for additional experiments with XLNet. However, I feel that, while the ideas presented in this paper are novel and potentially useful for XML, there is significant scope for improving the experimental validation to realize this potential. I would encourage the authors to revise their paper by improving the motivation, empirical results and discussion aspects of the paper and resubmit. The most important question for me is - Do these proposed ideas change the way XML is approached at present in any significant sense?\n\nAs of now, I keep my score."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2445/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700632706294,
                "cdate": 1700632706294,
                "tmdate": 1700632706294,
                "mdate": 1700632706294,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "G5JEmcvUSv",
                "forum": "v5BcZzkAXg",
                "replyto": "nYhIvmF5gF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2445/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2445/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Appreciating your response"
                    },
                    "comment": {
                        "value": "We would like to thank you for your devoted time in reading our responses carefully.\nWe insist again that as demonstrated in Ganesan21 (Table 3), HRR-based approach for XMC is clearly effective in practice.\nAs you mentioned, moreover, CHRR has several clear advantages over HRR.\nIn this sense, we believe that CHRR is novel and potentially useful for XMC."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2445/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700636739230,
                "cdate": 1700636739230,
                "tmdate": 1700642907130,
                "mdate": 1700642907130,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "RkgsTTTsVP",
            "forum": "v5BcZzkAXg",
            "replyto": "v5BcZzkAXg",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2445/Reviewer_7cd9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2445/Reviewer_7cd9"
            ],
            "content": {
                "summary": {
                    "value": "This paper solves the issue by exploring the use of random circular vectors, where each vector component is represented as a complex amplitude. Specifically, the paper developed an output layer and loss function of DNNs for XMC by representing the final output layer as a fully connected layer that directly predicts a low-dimensional circular vector encoding a set of labels for a data instance.  Extensive experiments on synthetic datasets to verify the effectiveness of circular vectors."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The motivation is clear and the algorithm is sensible.\n2. The proposed method is tested on several benchmarks."
                },
                "weaknesses": {
                    "value": "The paper is in general easy to follow and well-structured. There are some interesting theoretical guarantees, which seem simple and effective. Nevertheless, I have the following concerns:\n\n1. Not enough empirical evaluations. it necessary to evaluate other state-of-the-art benchmarks.\n2. What is the computational cost of method? [addressed by rebuttal]\n3.  Will the code be shared? [addressed by rebuttal]."
                },
                "questions": {
                    "value": "I am very impressed by the ideas and the writing of this paper. The method is simple and well-motivated. The evaluations address many aspects of the method."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2445/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2445/Reviewer_7cd9",
                        "ICLR.cc/2024/Conference/Submission2445/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2445/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698926657034,
            "cdate": 1698926657034,
            "tmdate": 1699954953496,
            "mdate": 1699954953496,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "K8TgJ6sqM0",
                "forum": "v5BcZzkAXg",
                "replyto": "RkgsTTTsVP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2445/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2445/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Appreciating your attention: Explanations for your concerns about our experiments"
                    },
                    "comment": {
                        "value": "Reviewer 7cd9: We greatly appreciate your encouraging comments and insightful questions.\n\n### Weakness\n\n#### W1. Not enough empirical evaluations. it necessary to evaluate other state-of-the-art benchmarks.\n- A1.\n- For Wiki10-31k, we conducted additional experiments by using input features generated by XLNet [Chang et. al. 2020] instead of BoW features to investigate the potential of transformer-based models. The results were comparable with recent models as follows.\n    -   | FC w/XLNet |h=768|h=1024|h=2048|h=4096| \n        | ----       | --- | ---  | ---  | ---  |\n        | P@1        |81.5 |83.3  |84.0  |85.1  |\n        | P@5        |54.2 |56.6  |58.9  |61.7  | \n        | P@10       |39.2 |41.6  |43.6  |46.1  | \n        | P@20       |26.2 |27.9  |29.5  |31.2  | \n  \n    -   | CHRR w/XLNet |h=768,d=800|h=768,d=1500 | \n        | ---- | --- | --- | \n        | P@1  |85.5     |86.1     | \n        | P@5  |61.5     |62.7     | \n        | P@10 |43.7     |45.8     | \n        | P@20 |25.5     |28.8     | \n- In the final version of this paper, we will show more experimental results using fine-tuned LMs for other datasets including AmazonCat-13K and Wiki-500K. Moreover, we will try a combination of XLNet and TF-IDF features for further improvements as X-Transformer [Chang et. al. 2020] has done.\n- For a detailed analysis, we conducted more experiments by varying the dimension size of hidden layers of FC, HRR and CHRR from 768 to 2,048, respectively. We displayed the results in Figure 5 and in Table 3 of our new draft paper. Especially, for Delicious-200k, CHRR outperformed HRR more clearly, and we think that the results support the main claim of this paper.\n\n#### W2. What is the computational cost of method? [addressed by rebuttal]\n- A2.\n- Speedups in training time for CHRR against FC were almost the same as those for HRR reported in Table 3 of [Ganesan et. al. 2021], while CHRR provides a clean and better label encoding/retrieval framework for XMC than HRR.\n- We could improve the CHRR\u2019s inference for finding top-k most relevant labels using an Approximate Nearest Neighbor  (ANN) method for the Maximum Inner Product Search because the CHRR\u2019s similarity operation between two circular vectors \u03b8 and \u03c6 is actually equivalent to the inner product between two real-valued vectors, which are obtained via the inverse FFTs of \u03b8 and \u03c6, respectively (The fact is simply based on the Parseval\u2019s theorem).\n\n#### W3. Will the code be shared? [addressed by rebuttal]\n- A3. Yes, we will make our codes publicly available from the first author\u2019s github page.\n\n---"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2445/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700041889379,
                "cdate": 1700041889379,
                "tmdate": 1700117916606,
                "mdate": 1700117916606,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WECEs7gNC3",
                "forum": "v5BcZzkAXg",
                "replyto": "K8TgJ6sqM0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2445/Reviewer_7cd9"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2445/Reviewer_7cd9"
                ],
                "content": {
                    "comment": {
                        "value": "I have read the feedback from the authors and I still have concerns about insufficient experiments as well as presentation issues. I would encourage the authors to revise their paper by improving the discussion aspects of the paper. Thus, I maintain my review score."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2445/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700634184773,
                "cdate": 1700634184773,
                "tmdate": 1700634184773,
                "mdate": 1700634184773,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]