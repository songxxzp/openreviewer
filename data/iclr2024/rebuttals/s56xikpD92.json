[
    {
        "title": "BaDExpert: Extracting Backdoor Functionality for Accurate Backdoor Input Detection"
    },
    {
        "review": {
            "id": "fLVYI0mJqb",
            "forum": "s56xikpD92",
            "replyto": "s56xikpD92",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3874/Reviewer_79zr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3874/Reviewer_79zr"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a backdoor defence method called BaDExpert. The main motivation is that normal data can be quickly forgotten with finetuning on inconsistent labels, while backdoor data does not. Based on this characteristic, the proposed method can extract a backdoor functionality of the model, which can only correctly classify backdoor data. Experiments show that the proposed method can effectively detect and defend against backdoor attacks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The proposed method is simple, straightforward and effective. Extracting the backdoor functionality is a new idea in backdoor attack research. \n- The empirical evaluations are very comprehensive, including different attacks, datasets, model architectures, and adaptive attacks. Results show it is effective against existing attacks and also demonstrated its limits under adaptive cases."
                },
                "weaknesses": {
                    "value": "- The proposed method relies on several procedures, each with different hyperparameters. However, the authors provide an ablation study in each component, with no overall insights/guides for applying such a method in real-world applications. In real applications, the defender does not know for sure that backdoor attacks exist in their data, so it might not be easy to find these suitable hyperparameters. \n- The comprehensive experimental results are appreciated. It could be more comprehensive to compare with recent defence methods such as ABL [1], and detection methods [2,3]. \n- The method mainly focuses on the detection of backdoor samples (end of section 3). It could help to further clarify what happens after. Results in Table 1 focus on the CA/ASR. It is not clear what happens to the detected samples in order to obtain these results. \n\n[1] Li, Yige, et al. \"Anti-backdoor learning: Training clean models on poisoned data.\" Advances in Neural Information Processing Systems (2021).\\\n[2] Pan, Minzhou, et al. \"ASSET: Robust Backdoor Data Detection Across a Multiplicity of Deep Learning Paradigms.\" USENIX Security Symposium (2023).\\\n[3] Huang, Hanxun, et al. \"Distilling Cognitive Backdoor Patterns within an Image.\" The Eleventh International Conference on Learning Representations (2023).\\"
                },
                "questions": {
                    "value": "No further questions, please address the weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3874/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3874/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3874/Reviewer_79zr"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3874/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697756286277,
            "cdate": 1697756286277,
            "tmdate": 1700716081741,
            "mdate": 1700716081741,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "hiXBg58Hn3",
                "forum": "s56xikpD92",
                "replyto": "fLVYI0mJqb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3874/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3874/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 79zr (Part 1)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for acknowledging our proposed method and experimental evaluations. We have incorporated the reviewer's suggestions, and hope our following clarifications could further resolve the reviewer's remaining concerns:\n\n---\n\n### **Q1: Hyperparameter selection.**\n\nWe totally agree with the reviewer that it's important to provide a general insight/guidance to real-world practitioners, in order to help them select an appropriate set of hyperparameters. In our submitted draft, we have briefly discussed how to select key hyperparameters in Appendix A.1 and A.2. To futher clarify, we make the following arguments to show that probing for an appropriate set of hyperparameters for BaDExpert is not difficult in practice.\n* **Fine-tuning rate $\\eta\u2019$.** In Appendix A.1.1, we have provided a brief ***rule of thumb*** to guide practitioners to select the fine-tuning learning rate $\\eta'$, by observing the clean accuracy (CA) trend of the model during fine-tuning. Specifically, we observe that clean finetuning can best diminish the backdoor functionality with a large learning rate, where the finetuned model\u2019s CA drops to \u223c $0$% in the first one or two epochs, and then recovers to a significant value in the following epochs.\n* **Unlearning rate $\\eta$**. As studied in Appendix C.4, we have shown that our method is consistently effective across a wide range of $\\eta$ (from $5\\cdot 10^{-5}$ to $5\\cdot 10^{-3}$). Nevertheless, we understand the reviewer's concern, that practitioners in the real world might need certain guidance to identify this coarse-grained range of $\\eta$.\n    \n    Similar to $\\eta'$, a simple ***rule of thumb for $\\eta$*** could be: starting from an $\\eta$ that is orders of magnitude smaller than the $\\eta'$ used for clean finetuning, keep tuning $\\eta$ until observing the model's clean accuracy (CA) diminishes by a certain extent (but does not completely vanish, i.e., worse than random guess). This rule of thumb helps practitioners idenfity an approximate range of $\\eta$ according to the model's CA drop magnitude, which can be derived from Fig 8 -- we can see the backdoor functionality is usually preserved when $\\eta$ is conservatively small, in the sense that CA has not degraded to the random-guess level."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3874/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700289414632,
                "cdate": 1700289414632,
                "tmdate": 1700289414632,
                "mdate": 1700289414632,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gui5XRkg8J",
                "forum": "s56xikpD92",
                "replyto": "fLVYI0mJqb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3874/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3874/Authors"
                ],
                "content": {
                    "title": {
                        "value": "A Gentle Reminder of the Final Feedback"
                    },
                    "comment": {
                        "value": "Dear Reviewer 79zr,\n\nWe would like to thank the reviewer for the helpful suggestions. We hope our previous response has adequately resolved your concerns regarding *applying our method in the real world* and comparing with *additional baselines*. As the deadline of ICLR rebuttal period is approaching, we look forward to hearing your feedback on our response, and would be happy to clarify any additional questions.\n\nBest,\n\nAuthors"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3874/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700601990848,
                "cdate": 1700601990848,
                "tmdate": 1700601990848,
                "mdate": 1700601990848,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "w0GAmu7qek",
                "forum": "s56xikpD92",
                "replyto": "hiXBg58Hn3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3874/Reviewer_79zr"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3874/Reviewer_79zr"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the clarifications. They addressed concerns Q2/Q3. The reviewer would suggest the authors add these results to the paper and provide additional details/analysis regarding these new experiments. \n\nRegarding Q1, the reviewer is not convinced by the clarification. In practice, the defender should not know what is the current CA. This is also mentioned by reviewer 59E7."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3874/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700635576077,
                "cdate": 1700635576077,
                "tmdate": 1700635576077,
                "mdate": 1700635576077,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "acmP4EDBhO",
                "forum": "s56xikpD92",
                "replyto": "NzmHIxGMt4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3874/Reviewer_79zr"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3874/Reviewer_79zr"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the clarification. Reserving a portion of the data for evaluating CA is reasonable and practical. The reviewer suggests the authors add experiments to verify this setting in the next revision, e.g. reserve a portion of the training set on CIFAR. \n\nAlthough the reviewer still has some concerns about the practicality of the proposed method, considering the interesting idea of extracting the backdoor functionality and the rebuttal discussion, the reviewer believes the merit outweighs the weaknesses and will increase the rating to 6."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3874/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700716062818,
                "cdate": 1700716062818,
                "tmdate": 1700716062818,
                "mdate": 1700716062818,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "4z8wdjKT5b",
            "forum": "s56xikpD92",
            "replyto": "s56xikpD92",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3874/Reviewer_xRnn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3874/Reviewer_xRnn"
            ],
            "content": {
                "summary": {
                    "value": "A novel backdoor defense BaDExpert is proposed in this paper. BaDExpert is designed to distinguish test instances with the backdoor trigger from benign test instances. The key idea is to fine-tune a \"backdoor-expert\" model with only the backdoor functionality so that benign test instances will be poorly recognized. Thus, test instances that are differently classified by the backdoored model and the backdoor-expert model are deemed benign; otherwise, a test instance is deemed to contain the trigger."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1) The method is well-motivated and the idea is novel.\n\n2) The experiments are thorough, involving many attack settings and covering many SOTA baselines.\n\n3) The presentation is excellent."
                },
                "weaknesses": {
                    "value": "More comparisons with existing works regarding the methodology can be included."
                },
                "questions": {
                    "value": "1. Is the design philosophy of BaDExpert related to [1]? In [1], adversarial examples are detected by encouraging the model to carry malicious behaviors such as a backdoor.\n\n[1] Shan et al, Gotta Catch 'Em All: Using Honeypots to Catch Adversarial Attacks on Neural Networks, 2019.\n\n2. Can BaDExpert outperform [2] which also detects malicious inputs?\n\n[2] Li et al, Test-time detection of backdoor triggers for poisoned deep neural networks, 2022."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3874/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698346107761,
            "cdate": 1698346107761,
            "tmdate": 1699636345853,
            "mdate": 1699636345853,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "tXq9DbzERt",
                "forum": "s56xikpD92",
                "replyto": "4z8wdjKT5b",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3874/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3874/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer xRnn"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the high appreciation of our work. We hope our following supplementary results and clarifications could further resolve the reviewer's remaining concerns.\n\n---\n\n### **Q1: Additional comparisons with recent baselines.**\n\nDuring the rebuttal periord, we have supplemented results comparing BaDExpert with several additional baselines [1-4]. As shown in Rebuttal-Table 1, BaDExpert outperforms all these baseline defenses, achieving higher CA and lower ASR. These results are also updated in our paper revision (Appendix C.8).\n\nRegarding the reviewer's suggested baseline [5], we have included it as a related work in our revision (see updated Sec 5). But due to time and resource limitations, we were yet not able to incorporate the experimental results of [5]; we will try to include the results in the future revisions.\n\n**Rebuttal-Table 1: Comparing BaDExpert with additional baselines.** (CA for Clean Accuracy, ASR for Attack Success Rate) For fair comparison: we follow the suggested hyperparameters and only tune them to approach the officially reported results; when reporting RNP results, we stop pruning once ASR drops to below $20$%; when reporting CD results, we follow their suggested and default threshold selection strategy. As shown, **BaDExpert outperforms all these baselines w.r.t. both CA and ASR**.\n\n\n| Attack | | No Defense | ABL[1] | AWM[2] | RNP[3] | CD[4] | BaDExpert |\n| -------- | -------- | -------- | -------- | -------- | -------- | -------- | -------- |\n| **BadNet** | CA  | 94.1 | 92.5 | 92.7 | 70.0 | 78.6 | **93.1** | \n|             | ASR | 100.0 | 11.7 | 4.8 | 18.9 | 2.9 | **0.0** |\n| **Blend**  | CA  | 94.1 | 91.9 | 88.9 | 77.9 | 79.0 | **93.1** |\n|             | ASR | 90.6 | 11.8 | 29.7 | 19.6 | 72.8 | **11.4** |\n\n---\n\n### **Q2: Relationship to Shan et al.[6].**\n\nWe thank the reviewer for the pointer to [6], where the authors propose an adversarial example detection method using \"honeypots\" --- a trapdoor that would enforce malicious adversarial inputs to manifest a certain neural network activation-pattern signature. We also found that their key design philosophy to be subtly connected to ours, in the sense that [6] detect potential adversarial examples via \"activation signature similarity measurement\", and we detect backdoor examples via \"model prediction/confidence agreement measurement\". However, their motivation, problem, and method are still largely different from ours, thus we did not discuss this work in our previous submitted version. To make sure we cover related work comprehensively, we have added a separate Appendix section (B.6) to discuss the connection between our design to [6] in our revision.\n\n---\n\n[1] Li, Yige, Xixiang Lyu, Nodens Koren, Lingjuan Lyu, Bo Li, and Xingjun Ma. \"Anti-backdoor learning: Training clean models on poisoned data.\" Advances in Neural Information Processing Systems 34 (2021): 14900-14912.\n\n[2] Chai, Shuwen, and Jinghui Chen. \"One-shot neural backdoor erasing via adversarial weight masking.\" Advances in Neural Information Processing Systems 35 (2022): 22285-22299.\n\n[3] Li, Yige, Xixiang Lyu, Xingjun Ma, Nodens Koren, Lingjuan Lyu, Bo Li, and Yu-Gang Jiang. \"Reconstructive Neuron Pruning for Backdoor Defense.\" arXiv preprint arXiv:2305.14876 (2023).\n\n[4] Huang, Hanxun, Xingjun Ma, Sarah Erfani, and James Bailey. \"Distilling Cognitive Backdoor Patterns within an Image.\" arXiv preprint arXiv:2301.10908 (2023).\n\n[5] Li, Xi, Zhen Xiang, David J. Miller, and George Kesidis. \"Test-time detection of backdoor triggers for poisoned deep neural networks.\" In ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 3333-3337. IEEE, 2022.\n\n[6] Shan, Shawn, Emily Wenger, Bolun Wang, Bo Li, Haitao Zheng, and Ben Y. Zhao. \"Gotta catch'em all: Using honeypots to catch adversarial attacks on neural networks.\" In Proceedings of the 2020 ACM SIGSAC Conference on Computer and Communications Security, pp. 67-83. 2020."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3874/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700289325038,
                "cdate": 1700289325038,
                "tmdate": 1700289339004,
                "mdate": 1700289339004,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "GT10ZducAn",
            "forum": "s56xikpD92",
            "replyto": "s56xikpD92",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3874/Reviewer_59E7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3874/Reviewer_59E7"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel backoor detection approach: It firstly finetune the backdoor model over a small subset of mislabeled samples to remove the benign functionality while preserving the backdoor-related functionality. Then, badexpert detects the backdoor examples based on the agreement between the backdoor expert and backdoored model. The effectiveness of the propsed method is tested on both small  (CIFAR-10 and GTSRB) and large dataset (ImageNet), CNN and ViT."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1 This paper is easy to follow.\n\n2 This paper is well written.\n\n3 Appreciate the solid experiments shown in the experimental section.  The authors demonstrate the novel performance of Badexpert in multiple datasets and architectures."
                },
                "weaknesses": {
                    "value": "1 Admitting the effectiveness of the proposed method, I think the practicality of Badexpert is limited. As shown in Appendix A, the optimal learning rate could vary across datasets: ($\\eta=10^{-4}$ for CIFAR-10 and $\\eta=2.5\\cdot10^{-5}$ for GTSRB). Even for the same dataset,  the optimal $\\eta$ could be different across different architectures:   $\\eta=10^{-4}$ for ResNet18 and  $\\eta=10^{-6}$ for pretrained vit_b_16. Considering the tremendous hyperparameter required for Badexpert, I think the overall guidlines for how to choose hyperparameter are needed to help Badexpert better defend against  potential risks.\n\n2 BadExpert depends on the stong mapping from the trigger to the pre-defined behaviour. I firmly believe, as long as the backdoor attack is weak enough (decreasing the poison rate/ the size of trigger/ the blend rate), will potentially leads to the Badexpert unsuccessful.  Sincerely, I hope to further discuss with the authors when encountering the above situations. Table 10 shows parts of the results, but not enough from my view. In addition, I think ASR and CA is a more appropriate metric to indicate the performance of BaDExpert instead of AUROC: Combing Table 1 and Table 2, BaDExpert obtains 11.4% ASR against blend attack. However, the AUROC is 99.2% which is quite close to 100%.\n\n3 The chosen of hyperparemter $\\eta'$ could also be unpractical in real world. In reality, only the small subset of clean images is available for defenders. Therefore, they have little knowledge about which $\\eta'$will meets the requirement of Badexpert: the CA of the finetuned model's CA first drops to ~ $0\\%$ and recovers to a significant value in the following epochs. The defender dooesn't exactly know what the CA of current model is. Therefore, the requirement of Badexpert may be too ideal.\n\n4 Some of the baselines are missing. For example, AWM [1] and ABL [2]."
                },
                "questions": {
                    "value": "1 Table 1 shows that BadExpert is relatively weak to defend against Blend or Dynamic attack. Can you explain the reason behind this phenomenon?\n\nFor other questions, please refer to the weakness section.\n\n[1] One-shot Neural Backdoor Erasing via Adversarial Weight Masking\n\n[2] Anti-Backdoor Learning: Training Clean Models on Poisoned Data"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Other reasons (please specify below)"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "Is the **ethics statement section** considered as the part of main text for the submitted paper? The ethics statement appears in the page 10. However, according to ICLR's rule, the main text of the submission is limited to 9 pages with unlimited additional pages for citations or appendixes."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3874/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3874/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3874/Reviewer_59E7"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3874/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698659803420,
            "cdate": 1698659803420,
            "tmdate": 1699636345749,
            "mdate": 1699636345749,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fph6FwXjVA",
                "forum": "s56xikpD92",
                "replyto": "GT10ZducAn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3874/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3874/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 59E7 (Part 1)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for acknowledging our proposed method, experiments, and paper writing. We have incorporated the reviewer's suggestions, and hope our following clarifications could further resolve the reviewer's remaining concerns:\n\n---\n\n### **Q1: Hyperparameter selection.**\n\nWe totally understand the reviewer's concern regarding our method's practicality of hyperparameter choosing. We hereby clarify our practical guidances for hyperparameters selection in respond to the reviewer's concern, showing that probing for an appropriate set of hyperparameters for BaDExpert is not difficult in practice.\n* **Fine-tuning rate $\\eta\u2019$.** In Appendix A.1.1, we provide a brief rule of thumb to select the fine-tuning learning rate $\\eta'$ by observing the clean accuracy (CA) trend of the model. Specifically, we observe that clean finetuning can best diminish the backdoor functionality with a large learning rate, where the finetuned model\u2019s CA drops to \u223c$0$% in the first one or two epochs, and then recovers to a significant value in the following epochs.\n\n    Indeed, we implicitly assume the model deployer (defender) has certain knowledge of the model $\\mathcal M$'s clean utility (or CA), since he/she is going to deploy the model for his/her own downstream task (and must make sure the model has such capability, i.e., high CA). We believe this is not a \"too ideal\" assumption, given that nowadays a standard test set is often available for diverse CV tasks, and that the clean utility of the model to be deployed is rather crucial to the model deployer.\n* **Unlearning rate $\\eta$**. To clarify, we would not claim our selection of $\\eta$ is \"optimal\" in each setting -- we only adjust the $\\eta$ in a coarse-grained way such that the unlearning process (Alg 1) does not collapse when the model architecture is different. In fact, as studied in Appendix C.4, we have shown that our method is consistently and similarly effective across a wide range of $\\eta$ (from $5\\cdot 10^{-5}$ to $5\\cdot 10^{-3}$). Nevertheless, we understand the reviewer's concern, that practitioners in the real world might need certain guidance to identify this coarse-grained range of $\\eta$.\n        \n    Similar to $\\eta'$, a simple **rule of thumb for $\\eta$** could be: starting from an $\\eta$ that is orders of magnitude smaller than the $\\eta'$ used for clean finetuning, keep tuning $\\eta$ until observing the model's clean accuracy (CA) diminishes by a certain extent (but does not completely vanish, i.e., worse than random guess). This rule of thumb helps practitioners idenfity an approximate range of $\\eta$ according to the model's CA drop magnitude, which can be derived from Fig 8 -- we can see the backdoor functionality is usually preserved when $\\eta$ is conservatively small, in the sense that CA has not degraded to the random-guess level."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3874/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700289131774,
                "cdate": 1700289131774,
                "tmdate": 1700289131774,
                "mdate": 1700289131774,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pdTF9JeKEN",
                "forum": "s56xikpD92",
                "replyto": "GT10ZducAn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3874/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3874/Authors"
                ],
                "content": {
                    "title": {
                        "value": "A Gentle Reminder of the Final Feedback"
                    },
                    "comment": {
                        "value": "Dear Reviewer 59E7,\n\nWe would like to thank the reviewer for the helpful suggestions. We hope our previous response has adequately resolved your concerns regarding our method's *practicality*, our defense against *weak backdoor attacks*, and comparing with *additional baselines*. As the deadline of ICLR rebuttal period is approaching, we look forward to hearing your feedback on our response, and would be happy to clarify any additional questions.\n\nBest,\n\nAuthors"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3874/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700601898197,
                "cdate": 1700601898197,
                "tmdate": 1700602003314,
                "mdate": 1700602003314,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vr93mYmExu",
                "forum": "s56xikpD92",
                "replyto": "pdTF9JeKEN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3874/Reviewer_59E7"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3874/Reviewer_59E7"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your rebuttal"
                    },
                    "comment": {
                        "value": "Dear authors, \n\nThank you for your retailed rebuttal. I still have concerns about the hyperparameter settings of BadExpert. In addition, I notice that the hyperparameter of the baseline defense, e.g. NAD, are consistent cross different attacks. I worry about the fairness of comparison in Table 1.\n\nBest,\n\nReviewer 59E7"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3874/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700634320075,
                "cdate": 1700634320075,
                "tmdate": 1700634320075,
                "mdate": 1700634320075,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QeEFjkbaSN",
                "forum": "s56xikpD92",
                "replyto": "GT10ZducAn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3874/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3874/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer 59E7,\n\nAs it's the last few hours to the end of ICLR rebuttal period, we look forward to hearing from you regarding whether our followup response resolves your additional concern. Again, we are grateful for your engagement during the review and rebuttal periods!\n\nBest,\n\nAuthors"
                    },
                    "title": {
                        "value": "Looking forward to hear from you regarding your additional concern"
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3874/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700726019732,
                "cdate": 1700726019732,
                "tmdate": 1700726056795,
                "mdate": 1700726056795,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "wm4KPA3wwp",
            "forum": "s56xikpD92",
            "replyto": "s56xikpD92",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3874/Reviewer_Xe68"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3874/Reviewer_Xe68"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces BaDExpert, an innovative defense mechanism against backdoor attacks targeting deep neural networks (DNNs). The defense is built upon the concept of extracting the backdoor functionality from a backdoored model to create a backdoor expert model. The backdoor expert model is then used to detect backdoor inputs during model inference. BaDExpert's efficacy is showcased across various datasets and model architectures, highlighting its impressive performance in terms of AUROC, a significant reduction in Attack Success Rate (ASR), and a minimal decline in clean accuracy (CA)."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper introduces a novel approach for defending against backdoor attacks by extracting the backdoor functionality from a backdoored model.\n- The paper provides a well-structured explanation of the methodology.\n- The paper presents extensive experimental results on multiple datasets and model architectures, demonstrating the effectiveness of BaDExpert."
                },
                "weaknesses": {
                    "value": "- The paper lacks in-depth theoretical analysis to support the proposed method.\n\n- The technique may not perform optimally when applied to models that haven't been backdoored.\n\n- The experimental section seems to omit comparisons with certain recent relevant works."
                },
                "questions": {
                    "value": "(1) A core tenet of the proposed method is that fine-tuning on a small set of mislabeled clean data can isolate the backdoor functionality. While this paper attempts to validate the idea through experimentation, providing a rigorous theoretical analysis would bolster the method's credibility.\n\n(2)  In real-world scenarios, after acquiring a model online, it's often uncertain whether it has been backdoored. If the model is a benign one, there would be a disagreement between the outputs of model \\mathcal{M} and \\mathcal{B} (on the left side of Figure 2). This divergence could potentially hinder BaDExpert's performance.\n\n(3) Could this paper elucidate the time complexity of the proposed method and compare it with methods like I-BAU? Given that this technique necessitates model fine-tuning, there are concerns about its efficiency.\n\n(4) It seems that some recent published related works are missing to be compared in the paper. For example,  [1] presents defense results that are on par with those in this paper, reporting an ASR of 5.03 and a CA of 92.18 for the CIFAR10 dataset.\n\n(5) Publicly releasing the code would facilitate better reproducibility and peer verification, enhancing the paper's value.\n\nIf the authors could solve some concerns mentioned above, the reviewer would reconsider the rating.\n\n[1] Li, Y., Lyu, X., Ma, X., Koren, N., Lyu, L., Li, B., and Jiang, Y.G., 2023. *Reconstructive Neuron Pruning for Backdoor Defense*. arXiv preprint arXiv:2305.14876."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3874/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3874/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3874/Reviewer_Xe68"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3874/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698801047829,
            "cdate": 1698801047829,
            "tmdate": 1699636345680,
            "mdate": 1699636345680,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Ziv6SgZVy8",
                "forum": "s56xikpD92",
                "replyto": "wm4KPA3wwp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3874/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3874/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Xe68 (Part 1)"
                    },
                    "comment": {
                        "value": "We thank the reviewer's acknowledgement of our work, together with the constructive suggestions and questions. We have incorporated the reviewer's suggestions, and hope our following clarifications may help resolve the raised concerns & questions:\n\n---\n\n### **Q1: Lack of theoretical analysis.**\n\nWe sincerely appreciate the reviewer's suggestion on theoretically analyzing our observation that \"fine-tuning on a small set of mislabeled clean data can isolate the backdoor functionality\". In our work, we have provided theoretical insight into the *decision rule* (Neyman-Pearson Lemma in Remark 1, Sec 4.2). Mathematically analyzing the effect of *learning (fine-tuning) dynamics* of neural networks, on the other hand, is rather technically difficult --- researchers are often simplifying DNNs into overparameterized linear models to do this. These simplified analysis can only provide more empirical intuitions behind the methods, but not rigorous guarantees. \n    \nIn our work, in order to provide sturdy support for our method's \"credibility\", we tried our best to conduct various experiments, as many as possible. Regarding the observation of \"*fine-tuning on a small set of mislabeled clean data can isolate the backdoor functionality*\", we have shown its pervasiveness across a big arrary of attacks (Fig 8). Regarding the overall effectiveness of our method, BaDExpert, we have validated its effectiveness across different attacks, datasets, architectures, and accompanied with various ablation studies. Moreover, we have studied its \"worst-case\" effectiveness against a tailored adaptive attack. All these empirical efforts jointly confirm our method's credibility.\n\n---\n\n### **Q2: BaDExpert's performance on clean (benign) models**.\n\nIn Table 1 \"No Attack\" row, we have shown the case when the model $\\mathcal M$ is benign (CA = $93.1$%). In case of potential misunderstanding, we clarify the occasion when $\\mathcal M$ is not backdoored more specifically:\n\n1. *Remember that \"backdoor input\" is only meaningful when the model is embedded with a corresponding backdoor.* So when the model $\\mathcal M$ is benign, there would be no concept as \"backdoor input\" or \"backdoor functionality\"; every input can simply be considered as a \"clean input\". As a result, Fig 2 (left) has no meaning in this context -- there is no attacker, and the \"patched inputs\" shown in Fig 2 (left) can just be considered as benign inputs with watermarks. \n3. The extracted \"backdoor expert model\" $\\mathcal B$ is just a DNN that \"forgets its benign task\" -- making incorrect predictions and disagreeing with $\\mathcal M$ (in most cases). Therefore, by looking at Fig 2 (right), we can come to the conclusion: BaDExpert would accept most benign images (i.e., low false positive rates).\n\nIn summary, we argue that BaDExpert should be similarly effective, no matter the model $\\mathcal M$ is backdoored or benign.\n    \nP.S.: In Sec 4.4 and Appendix C.8.1, we have also considered the **Natural** backdoor attack (which is not a traditional backdoor attack setting, more similar to the universal adversarial attack setting; our arguments above do not include this) against *benign models*. We found that BaDExpert is still effective at detecting these Natural backdoor inputs (AUROC $>92$%), and our key finding that \"*when finetuning on mislabeled clean data, the backdoor functionality would remain, while normal functionality does not*\" still holds in general."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3874/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700288721442,
                "cdate": 1700288721442,
                "tmdate": 1700288721442,
                "mdate": 1700288721442,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OZmgN36ZM0",
                "forum": "s56xikpD92",
                "replyto": "wm4KPA3wwp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3874/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3874/Authors"
                ],
                "content": {
                    "title": {
                        "value": "A Gentle Reminder of the Final Feedback"
                    },
                    "comment": {
                        "value": "Dear Reviewer Xe68,\n\nWe would like to thank the reviewer for the helpful suggestions. We hope our previous response has adequately resolved your concerns regarding our method's performance on *clean / benign models*, comparing with *additional baselines*, and *efficiency (time complexity)*. As the deadline of ICLR rebuttal period is approaching, we look forward to hearing your feedback on our response, and would be happy to clarify any additional questions.\n\nBest,\n\nAuthors"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3874/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700602129918,
                "cdate": 1700602129918,
                "tmdate": 1700602129918,
                "mdate": 1700602129918,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "voDEfTjC9d",
                "forum": "s56xikpD92",
                "replyto": "wm4KPA3wwp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3874/Reviewer_Xe68"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3874/Reviewer_Xe68"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the response"
                    },
                    "comment": {
                        "value": "Dear Authors, \n\nThanks a lot for the response. It solved some of my concerns such as the code and new baselines. However, I still have some concerns regarding the experiment settings and the model settings.\n\nFirst, for new baselines, it would be better if the paper can include more details, such as the experiment settings (i.e., which dataset, hyper-parameters), and evaluation on more datasets. If the results are obtained from CIFAR dataset, the ASR of the original model seems to be low for me (I am unsure, since it can vary for different settings, that is why I feel I need some clarifications on the settings).\n\nSecond, based on my understanding, I still feel the method might not be robust enough. it seems the paper requires some knowledge about the CA/model estimation before the defenses, which is not available in most of the cases (similar as pointed out by reviewer 79zr). That is also why I wonder if the proposed method can work perfectly on different clean models. Even though estimation could be possible, it is hard to really implement it. For example, how to sample the reserved test set, what size, and how often for the testing are difficult to decide, and the numbers vary for different attacks as well as datasets."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3874/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700662938346,
                "cdate": 1700662938346,
                "tmdate": 1700662962313,
                "mdate": 1700662962313,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "S2b38tQeFt",
                "forum": "s56xikpD92",
                "replyto": "wm4KPA3wwp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3874/Reviewer_Xe68"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3874/Reviewer_Xe68"
                ],
                "content": {
                    "title": {
                        "value": "Response to the rebuttal"
                    },
                    "comment": {
                        "value": "Dear authors,\n\nThanks for the quick response. The rebuttal addressed most of my questions, but I still have some minor concerns about the details of the experiment, including the setup for each experiment, as well as the specific parameter choices, sensitivities, and assumptions about the defenders. Hence, I will keep my score for now."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3874/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700716308868,
                "cdate": 1700716308868,
                "tmdate": 1700716422560,
                "mdate": 1700716422560,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]