[
    {
        "title": "Matryoshka Diffusion Models"
    },
    {
        "review": {
            "id": "PxTfS1G9nA",
            "forum": "tOzCcDdH9O",
            "replyto": "tOzCcDdH9O",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3843/Reviewer_cN8u"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3843/Reviewer_cN8u"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes Matryoshka Diffusion Models for high-resolution image and video synthesis. Unlike conventional approaches that use either cascade models or latent diffusion models with an additional autoencoder, Matryoshka Diffusion Models uses a diffusion process that denoises the multi-resolution input jointly, where such a process can be trained progressively and improves the optimization efficiency significantly. The paper shows the effectiveness of the method on popular image generation and video generation tasks and verifies the training efficiency of the method compared with existing diffusion model variants."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper is generally well-written and easy to follow. \n- The paper is well-motivated.\n- The paper conducts experiments with various datasets, including ImageNet, MSCOCO, and WebVid-10M.\n- The high-resolution image generation results are quite impressive."
                },
                "weaknesses": {
                    "value": "- The paper lacks an analysis on \"comparison with literature\". The paper simply states the result is comparable to other baselines, but the results show a clear gap (e.g., FID 3.60 (LDM) while 6.62 (MDM) on ImageNet 256x256). In this respect, the authors should provide an extensive analysis and reasons why the performance is worse than the baselines, not just saying the proposed method shows comparable performance. \n- To verify the \"faster convergence\", I think x-axis in Figure 4 should be wall-clock time rather than training iterations. Otherwise, I think the authors should provide time/iteration for each baseline used for the evaluation. \n- Some important implementation details are missing: learning rate, batch size, model configurations, etc. \n- Missing quantitative evaluation on text-to-video generation compared with existing baselines.\n- No video files included for illustrating text-to-video generation results."
                },
                "questions": {
                    "value": "- Why some points in Figure 4 (e.g., after 200K of Latent DM in Figure 4(a)) are missing?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3843/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698634051023,
            "cdate": 1698634051023,
            "tmdate": 1699636342288,
            "mdate": 1699636342288,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "yElMkKx2Ed",
                "forum": "tOzCcDdH9O",
                "replyto": "PxTfS1G9nA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3843/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3843/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for acknowledging the motivation and the results of our paper. We also appreciate your constructive feedbacks. We answer your questions below, and we will provide more details with the updated draft.\n\n\n> The paper lacks an analysis on \"comparison with literature\"\n\nOur design for the ImageNet experiments are mostly focused on the control experiments, and we have directly borrowed the same architecture and hyper parameters from our text2image experiments. Also, due to computational demands, we did not train our models with a lot of iterations. This means that we have not optimized our FID results on ImageNet, and Table 1 is a rather a pessimistic estimate of our model\u2019s performance. We are working on improving the results, and will update Table 1. \n\n\n> To verify the \"faster convergence\", I think x-axis in Figure 4 should be wall-clock time rather than training iterations. Otherwise, I think the authors should provide time/iteration for each baseline used for the evaluation.\n\n\nThanks for the suggestion, we will provide the training cost details in the updated draft. Roughly speaking, the cost per iteration is near identical between single diffusion, MDM and CDM for the high resolution training, all are more costly that LDM.\n\n\n> Some important implementation details are missing: learning rate, batch size, model configurations, etc.\n\n\nWe will update the draft for the implementation details. Our design for all the experiments have largely been using the same set of hyper parameters regarding lr, architecture etc, and we only tune the batch size to fit the GPU memory. \n\n\n> Missing quantitative evaluation on text-to-video generation compared with existing baselines.\n\nDue to resource constraints, the experimental results on the text-to-video tasks are preliminary, and our goal is to demonstrate qualitatively that the same idea behind MDM also applies to video generation. There is great interest to us to further improve the video generation\u2019s quality and compare it with state of the art methods, which we leave as future work.\n\n\n> No video files included for illustrating text-to-video generation results.\n\nWe will include video results in the supplementary material.\n\n\n> Why some points in Figure 4 (e.g., after 200K of Latent DM in Figure 4(a)) are missing?\n\n\nThe LDM experiments were not finished at the point of submission, but we will upload a complete comparison for LDM in Figure 4."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3843/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700595298945,
                "cdate": 1700595298945,
                "tmdate": 1700595298945,
                "mdate": 1700595298945,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0RMxThDDK4",
                "forum": "tOzCcDdH9O",
                "replyto": "yElMkKx2Ed",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3843/Reviewer_cN8u"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3843/Reviewer_cN8u"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Thanks for the detailed response. Since the draft is not updated until now and many evaluations had not been completed in the submission, it is difficult for me to raise the score. I will retain my score."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3843/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700692949873,
                "cdate": 1700692949873,
                "tmdate": 1700692949873,
                "mdate": 1700692949873,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "yL4kAwbD1o",
            "forum": "tOzCcDdH9O",
            "replyto": "tOzCcDdH9O",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3843/Reviewer_Z34F"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3843/Reviewer_Z34F"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a multi-stage image/video generation model based on f-DM. The model is structured as a NestedUNet: a UNet with multiple inputs/outputs of increasing/decreasing resolutions. Compared to f-DM, it incorporates progressive growing and benchmarks the approach on multiple large-scale text-to-image and text-to-video datasets. The works positions itself as a new paradigm for high-resolution diffusion models and rivals latent DMs and cascaded DMs. It features faster training convergence in terms of the amount of iterations compared to the existing paradigms. The obtained results visually look quite good to me."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Visually, the results look very good. And it is especially remarkable given the little compute used to train the models.\n- A good advantage of the given method is that, compared to CDMs, it does not need require the previous stage to be well-trained to have meaningful training of the current stage with reliable scores. For CDMs, while it's possible to train all the stages simultaneously, one cannot generate images from scratch in the middle since the base stage has not been fully trained yet. Somehow, this does not happen for the given model.\n- The ablations and evaluation in general is quite solid.\n- The exposition is good, and the paper is written well."
                },
                "weaknesses": {
                    "value": "- The method is not end-to-end (or at least does not perform well when trained in the end-to-end manner), despite what the paper claims. If I am not mistaken, at the end it still trains stage-by-stage similarly to CDMs \u2014 and without such stage-by-stage training it produces considerably worse results.\n- The paper does not report training costs rigorously for all the experiments, and it's impossible to compare between methods without knowing their training cost.\n- The comparison to CDMs does not seem fair, since the paper compares to under-trained CDMs. If one trains CDMs withing a limited computational budget, then more focus should be put on the base stage, since the final stages converge much faster.\n- FID scores on ImageNet are ~3x times higher than the current SotA (e.g., MDT).\n- The paper makes a claim about good results on a small text-to-image dataset (CC12M), but does not compare to existing large-scale text-to-image generators. This makes it impossible to evaluate this claim \u2014 e.g., Figure 8c should contain the results of existing text-to-image generators to make the existing model comparable. Otherwise, such a claim is not grounded. Judging by the maximum CLIPScore on a CLIP/FID trade-off chart is wrong since in the Imagen's paper, one can notice that even their bad models can attain very high CLIP scores under a strong enough guidance."
                },
                "questions": {
                    "value": "- What are the computational budgets of all the experiments? (I can only see the amount of GPUs being used for the experiments \u2014 without a notice on for how long). I believe that the smaller amount of training could also justify inferior FID results on ImageNet compared to SotA.\n- Are there any other differences compared to f-DM [1] apart from the progressive training idea and larger-scale experiments? It seems that f-DM uses the same NestedUNet idea, but the f-DM authors just do not call it a \"NestedUNet\". Do you use the same noise schedule as f-DM?\n- To be honest, I do not quite understand why the FID on ImageNet is so high. Samples in Figure 5 looks very good to me (given that they are random samples). What CFG weight was used generate them?\n- Please, include the comparison with existing text-to-image generators. Judging by the maximum attainable CLIP score is misleading.\n- I find the results on video generation to be quite good. For how long has the model been trained and was there joint image/video training (or image pretraining) used?\n- Why do you think your model does not suffer from the \"train/test gap\" problem of CDMs and its late stages can generate meaningful images even when the low-resolution stage has not been trained yet?\n- I have a suspicion that the video generator can struggle in generating videos with moving scenes because the base low-resolution generator produces just a single frame. Could you please provide the video results for moving scenes? And in general include some mp4/gif videos in the submission (i have not found any in the supplementary).\n\nSome typos:\n- \"crtical\" => \"criticial\" (page 2)\n- \"under performs\" => \"underperforms\" (page 6)\n- \"eg\" => \"e.g.\" (page 7)\n- page 8 \u2014 no space before the bracket \"(\" in multiple places.\n\n[1] Gu et al \"f-DM: A Multi-stage Diffusion Model via Progressive Signal Transformation\""
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3843/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3843/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3843/Reviewer_Z34F"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3843/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698780303642,
            "cdate": 1698780303642,
            "tmdate": 1700695170992,
            "mdate": 1700695170992,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BBoCCzfFxy",
                "forum": "tOzCcDdH9O",
                "replyto": "yL4kAwbD1o",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3843/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3843/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for acknowledging our contributions and results, and for the comprehensive review and constructive questions. We answer your questions below, and we will provide more details with the updated draft.\n\n\n> The method is not end-to-end\n\nFirst, we apologize for misunderstandings on the end-to-end claim in our presentation. We agree that our progressive training mode does not exactly fit into the common perception of end-to-end training, and we are happy to remove such claims in the paper. However, we\u2019d like to highlight that even without progressive training, MDM still offers significant advantage over simple diffusion, as well as our own implementations of CDMs and LDM (see Figure 4). In addition, the inference mode of MDM follows a single inference pass in the same latent space, with or without progressive training, which also differentiates itself from multi-stage approaches like CDM, LDM and fDM. \n\n\n> The paper does not report training costs rigorously for all the experiments, and it's impossible to compare between methods without knowing their training cost.\n\n\nWe agree, thanks for pointing it out. Roughly speaking, the cost per iteration is near identical between single diffusion, MDM and CDM for the high resolution training, all are more costly that LDM. We will provide the actual training cost in the updated draft.\n\n\n> The comparison to CDMs does not seem fair, since the paper compares to under-trained CDMs. If one trains CDMs withing a limited computational budget, then more focus should be put on the base stage, since the final stages converge much faster.\n\nWe agree that our own CDM baseline is not optimally tuned, however we do believe that it\u2019s still a fair comparison and reflects the advantage of MDM over CDM. As correctly stated by the reviewer, CDM is sensitive to quality of the low res model, which requires one to both train the low resolution model to near optimal as well as carefully tune the noise augmentation level for the low resolution inputs. The comparison in Figure 4 shows that, starting from the same low resolution model, MDM clearly shows an advantage which demonstrates the robustness of our progressive training scheme over cascaded diffusion.\n\n\n> FID scores on ImageNet are ~3x times higher than the current SotA\n\n\nOur design for the ImageNet experiments are mostly focused on the control experiments, and we have directly borrowed the same architecture and hyper parameters from our text2image experiments. Also, due to computational demands, we did not train our models with a lot of iterations. This means that we have not optimized our FID results on ImageNet, and Table 1 is a rather a pessimistic estimate of our model\u2019s performance. We are working on improving the results, and will update Table 1. \n\n\n> The paper makes a claim about good results on a small text-to-image dataset (CC12M), but does not compare to existing large-scale text-to-image generators\n\n\nWe are working on updating the draft to include a comparison with other text-to-image baselines. However, we respectfully disagree that high CLIP score is not indicative of a model\u2019s perceptual quality \u2014 in our experience, CLIP score correlates much better with the sample\u2019s perceptual quality than FID. The reviewer refers to the Imagen paper\u2019s results and indicate that bad models are capable of achieving high CLIP scores, however we could not find such evidence (eg, in Figure 4(a) ). Can you elaborate which Figure/Table supports this claim? (the closest thing we could find is Figure A.11, however these plots are from the same model size and the FID differences are also small)."
                    },
                    "title": {
                        "value": "Response 1/N"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3843/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700594892190,
                "cdate": 1700594892190,
                "tmdate": 1700594923520,
                "mdate": 1700594923520,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PkWeIfVeRf",
                "forum": "tOzCcDdH9O",
                "replyto": "yL4kAwbD1o",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3843/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3843/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response 2/N"
                    },
                    "comment": {
                        "value": "> computational budgets of all the experiments\n\n\nWe will update the draft with the exact computational budgets for the experiments. But roughly speaking, our ImageNet experiments are all using 8xA100 GPUs, and Text2Image experiments are using 32xA100 GPUs. All experiments run between 1-2 weeks, which correspond to <500K training iterations. \n\n\n> Are there any other differences compared to f-DM \n\nMDM is related to f-DM in terms of parameter sharing across different resolutions, however with one fundamental difference. In f-DM, different resolutions are chained up sequentially, whereas the network handles one resolution at a time. This means that for f-DM the network architecture is identical to that of a standard diffusion model when denoising the highest resolution. In MDM, all resolutions are processed in parallel by the NestedUNet, both during training and inference. This sequential vs parallel difference is subtle, but makes a big difference wrt their empirical performance. \n\n\n> Figure 5 CFG weight\n\nFigure 5 is generated with  a CFG weight of 2. We have inspected results from different weights within the range [1.1, 5], they mostly visually appealing which indicates that CFG is applicable to MDM similarly to a standard diffusion model.\n\n\n> For how long has the (video) model been trained and was there joint image/video training (or image pretraining) used?\n\n\nThe video model was trained following the same protocol as the image domain, whereas we first train an image generator for ~400K iterations. Then the video model is trained with the (temporal) NestedUnet, whereas there is a video loss and image loss (similar to image losses in different resolutions). The training time of the video model is less than two weeks on 32xA100 GPUs. \n\n\n> Why do you think your model does not suffer from the \"train/test gap\" problem of CDMs and its late stages can generate meaningful images even when the low-resolution stage has not been trained yet?\n\n\nThe key difference between progressively training MDM and CDM is that for MDM, all parameters are jointly optimized, whereas CDM freezes the low resolution generator. In other words, the role of progressive training in MDM can be viewed as a better initialization of the weights. During inference time, MDM generates low and high resolution images in parallel, where we rely on a good inference schedule to deal with the training/test gap issue.\n\n\n> I have a suspicion that the video generator can struggle in generating videos with moving scenes because the base low-resolution generator produces just a single frame\n\n\nThe reviewers hypothesis is correct and we will provide additional examples in the supplementary material."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3843/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700594999154,
                "cdate": 1700594999154,
                "tmdate": 1700594999154,
                "mdate": 1700594999154,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SnVjFLpwPT",
                "forum": "tOzCcDdH9O",
                "replyto": "PkWeIfVeRf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3843/Reviewer_Z34F"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3843/Reviewer_Z34F"
                ],
                "content": {
                    "title": {
                        "value": "Following up on the discussion"
                    },
                    "comment": {
                        "value": "I am thankful to the authors for providing their response, it helped me to understand their work better.\n\n> About the end-to-end nature.\nI believe this claim either should be reformulated with more caution, or more experiments and results should be provided with end-to-end training (e.g., most of the results should become about the end-to-end trained model).\n\n> About FID scores on ImageNet\nFrankly, it is difficult to complain for me about FID on ImageNet (since, from my personal experience, one needs to evolve the entire project around ImageNet to get good FID on it), but such scores just make it difficult to position the method among the existing ones. How can I conclude from your exposition that it's just not enough training time instead of some inherent pathological property of the method that leads to high FID scores (e.g., what if the model stops improving below this FID)? Would it be possible to fine-tune MDM vs CDM from some base well-performing pixel-space diffusion (e.g., EDM)?\n\n> About FID/CLIPScore and comparison to existing text-to-image models.\nAfter reading your response, I withdraw my claim that bad models can achieve high CLIPscore: I checked Imagen (and a couple of other papers) once again and couldn't find enough evidence to support it (the only plot that could support this is indeed A.11 in Imagen, but the majority of their plots show quite the opposite, i.e. that here you are right and I am wrong). I apologize for bringing this up. However, another part of my concern of reporting CLIPScores for existing generators still seems valid.\n\n> About MDM's vs CDM's train/test gap\nCould you please provide the details about your \"inference schedule to deal with the train/test gap\" or point out to a paper's section where it is described? I checked Section 3.1 and \"Implementation details\" in Section 4.1 and couldn't find any details on it.\n\nSeveral my concerns have been resolved, and I've updated my score accordingly.\nHowever, some of my concerns still remain:\n- The claim about the end-to-end nature is not entirely justified\n- Fig 4 feels confusing and does not really distinguish the convergence of different methods, since various methods use various pre-trained components (MDM/CDM/LDM), which can have different influence on the convergence of their subsequent stage. In this way, I cannot confidently arrive to the conclusion about the better convergence of MDMs in general (which is implied from the work). What feels more possible is that MDMs, within some fixed computational budget, would achieve better performance than CDMs/LDMs.\n- Training costs (in terms of GPU days/years) are reported too vaguely and it's unclear whether the calculations include the pre-training stage or describe only the main training stage.\n- The situation with too high FID scores on ImageNet is somewhat unclear to me (but at the same time I do not think that it's reasonable from my side as a reviewer to require the authors to tune their method exclusively on ImageNet). It's unclear to me what conclusion should I make from the provided experiments on ImageNet.\n- Lack of video results\n- Positioning the developed text-to-image/text-to-video model in terms of the scores within the existing methods\nI am looking forward to the manuscript update to further revise my score."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3843/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700695154510,
                "cdate": 1700695154510,
                "tmdate": 1700695154510,
                "mdate": 1700695154510,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "yZ7pBh0BVu",
            "forum": "tOzCcDdH9O",
            "replyto": "tOzCcDdH9O",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3843/Reviewer_iXvr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3843/Reviewer_iXvr"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a new framework for high-resolution image synthesis using diffusion models. Due to computational limitations, diffusion models are often limited to cascaded approaches in pixel-space or operating in latent space. The proposed framework, Matryoshka Diffusion Models (MDMs), denoises images at various resolutions simultaneously. MDM is trained using the standard diffusion objective jointly at multiple resolutions with a progressive schedule where the higher resolutions are added into the objective later in training. The authors demonstrate that MDM has greater efficiency with comparable performance on image and video generation."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The proposed framework is straightforward and easy to understand, drawing inspiration from existing GAN literature to address limitations of current diffusion models for high-resolution synthesis.\n- The authors evaluate MDM on multiple synthesis tasks and demonstrate comparable results. The qualitative results look impressive especially given the relatively small scale of training data.\n- Ablation studies quantify the effects of progressive training and the number of nested levels on the quality and alignment of the outputs."
                },
                "weaknesses": {
                    "value": "- The variables in the provided pseudocode for the NestedUNet architecture are not clear. A quick description to clarify the inputs to the function would be beneficial.\n- In Table 1 we see that there is a noticeable gap between MDM and the baselines. It would be helpful if the authors provided insight as to why they think this may be the case.\n- The authors highlight video generation as a contribution of MDM, but there is no discussion of the experiments or results (aside from implementation details and a few subsampled frames). It is difficult to get a sense of MDM's performance for this task.\n- There are several typos (especially with spacing before citation parentheses on page 8)."
                },
                "questions": {
                    "value": "- Generally the experiments section would benefit from including more insights on the results, especially for the scenarios where MDM is outperformed by the baselines.\n- While there is some differentiation already denoted through the colors, it would be helpful to explicitly label the novel pathways introduced by the NestedUNet architecture in Figure 3 to clearly distinguish from skip connections from the original UNet.\n- It is helpful to understand how the proposed multi-resolution prediction affects sampling speed."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3843/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3843/Reviewer_iXvr",
                        "ICLR.cc/2024/Conference/Submission3843/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3843/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698810285836,
            "cdate": 1698810285836,
            "tmdate": 1700825442377,
            "mdate": 1700825442377,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "AyubJjGn5g",
                "forum": "tOzCcDdH9O",
                "replyto": "yZ7pBh0BVu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3843/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3843/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We\u2019d like to thank the reviewer for acknowledging our contributions and results. We answer your questions below, and we will provide more details with the updated draft.\n\n\n> The variables in the provided pseudocode for the NestedUNet architecture are not clear\n\n\nWe will update to pseudocode to clarify this.\n\n\n> In Table 1 we see that there is a noticeable gap between MDM and the baselines.\n\n\nOur design for the ImageNet experiments are mostly focused on the control experiments, and we have directly borrowed the same architecture and hyper parameters from our text2image experiments. Also, due to computational demands, we did not train our models with a lot of iterations. This means that we have not optimized our FID results on ImageNet, and Table 1 is a rather a pessimistic estimate of our model\u2019s performance. We are working on improving the results, and will update Table 1. \n\n\n> Video results\n\nDue to resource constraints, the experimental results on the text-to-video tasks are preliminary, and our goal is to demonstrate qualitatively that the same idea behind MDM also applies to video generation. There is great interest to us to further improve the video generation\u2019s quality and compare it with state of the art methods, which we leave as future work.\n\n\n> Sampling speed\n\nMDM\u2019s sampling speed is comparable to that of a standard UNet, and we will provide measurements in the updated draft."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3843/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700593486993,
                "cdate": 1700593486993,
                "tmdate": 1700593486993,
                "mdate": 1700593486993,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UoNcGjoUDd",
                "forum": "tOzCcDdH9O",
                "replyto": "AyubJjGn5g",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3843/Reviewer_iXvr"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3843/Reviewer_iXvr"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you to the authors for addressing the reviewers' concerns. While I still believe this work is very interesting, the other reviewers have pointed out some incompleteness in the evaluations that I initially missed and think are valuable points to more concretely address than in the initial replies by the authors in order to maintain a positive score. I am interested in seeing these concerns addressed in the revised draft before updating my final score."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3843/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700687967398,
                "cdate": 1700687967398,
                "tmdate": 1700687967398,
                "mdate": 1700687967398,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "XjQJS4TICK",
            "forum": "tOzCcDdH9O",
            "replyto": "tOzCcDdH9O",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3843/Reviewer_7XqF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3843/Reviewer_7XqF"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a diffusion model capable of denoising multiple resolutions simultaneously. To enhance computational efficiency, they use a nested UNet structure. While it is possible to train the model all at once, the results show that progressively training it led to better convergence."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The proposed model converges faster compared to traditional cascaded diffusion models. In the case of MS-COCO, the model demonstrates superior performance."
                },
                "weaknesses": {
                    "value": "It is anticipated that there will be an increase in computational load. And the performance does not seem to surpass that of LDM, which employs classifier-free guidance. Also, since multi-ratio and resolution training is already being conducted in stable-diffusion XL, the proposed method with multi-resolution training is not much novel."
                },
                "questions": {
                    "value": "It is curious about the computational load increases. Also, it would be nice if the performance difference with usual UNet and NestedUNet is provided."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3843/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3843/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3843/Reviewer_7XqF"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3843/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698837346779,
            "cdate": 1698837346779,
            "tmdate": 1699636342029,
            "mdate": 1699636342029,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "RyG8IR09DZ",
                "forum": "tOzCcDdH9O",
                "replyto": "XjQJS4TICK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3843/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3843/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the constructive feedbacks. We answer some of your questions below, and we will provide more details with the updated draft.\n\n\n> It is anticipated that there will be an increase in computational load\n\n\nNote that the computational cost of Nested Unet is almost identical to a standard Unet, as shown in Figure 3. \n\n\n> And the performance does not seem to surpass that of LDM, which employs classifier-free guidance\n\nIn Figure 4, we show the comparison of MDM to our own implementation of LDM. The LDM baseline uses the same Unet architecture as our 64x64 Unet, and we see that MDM outperforms LDM in this controlled setting. Note also that we didn\u2019t apply CFG to either MDM or LDM, and we show in Figure 8 that CFG is applicable to MDM, similar to that shown in LDM.\n\n\n> Also, since multi-ratio and resolution training is already being conducted in stable-diffusion XL, the proposed method with multi-resolution training is not much novel.\n\nWe agree that multi resolution training in SDXL is related to MDM, but we respectfully point out that they work in fundamentally different ways. In SDXL, multi-resolution training can be considered as form a scheduled data augmentation, where the same model is applied to data of different resolutions, and during inference, the model is only capable of generating images of the highest resolution. MDM on the other hand, consumes images of multiple resolutions as part of the model architecture as well as the inference pipeline. It allows one to train images of multiple resolutions at the same time, and also generate images of multiple resolutions in a single inference run (see Figure 1 (a) top left corner as an example). \n\n\n> Also, it would be nice if the performance difference with usual UNet and NestedUNet is provided\n\nWe will provide additional experiments training a single NestedUnet and standard Unet in the updated draft \u2014 our observation is that they have near identical performance as two stand alone architectures."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3843/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700593022843,
                "cdate": 1700593022843,
                "tmdate": 1700593022843,
                "mdate": 1700593022843,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]