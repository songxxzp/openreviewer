[
    {
        "title": "Small Variance, Big Fairness: A Path to Harmless Fairness without Demographics"
    },
    {
        "review": {
            "id": "RipZsvwAHi",
            "forum": "VhQUwxIHER",
            "replyto": "VhQUwxIHER",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5096/Reviewer_DML4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5096/Reviewer_DML4"
            ],
            "content": {
                "summary": {
                    "value": "The authors consider the problem of training a fair model with comparable accuracy on all protected groups. The catch is that the model doesn't know which observations belong to which protected groups during the training process. The main idea of the paper is to achieve low loss for each observation during training. This means minimizing the expected loss across the observations *and* the variance of the loss across the observations. The authors formulate this update as a Lagrangian and describe some heuristics for updating in a \"harmless\" way where the variance can decrease without impacting the expectation too much. They run experiments on four different data sets and show that their method achieves comparable accuracy to other approaches but can achieve lower variance and fairness. They also perform some ablation experiments to show their heuristic updates improve performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The idea of minimizing the variance of losses is interesting.\n\n* I like Proposition 2 that losses have to be independent of protected groups. I would like to see it in the main body and the conclusion that, since protected groups aren't known, a very natural thing to do is minimize the variance in all losses.\n\n* The way of choosing the harmless update shows substantial thought but I would've liked more explanation.\n\n* The experiments are comprehensive: several data sets, baselines, and metrics."
                },
                "weaknesses": {
                    "value": "* I found the motivation for why they want to minimize the variance of loss weak a little weak in the main body. Moving Proposition 2 to the main body and adding more explanation could help.\n\n* I found Proposition 1 unpersuasive because the squareroot of the variance is a *very* loose upper bound on the maximum accuracy difference loss. At one point in the proof, they blow up one side by a factor of n. I would only find a result like this persuasive if there was a (somewhat) tight upper and lower bound.\n\n* Honestly, the results in Table 1 aren't impressive to me. Most of the metrics are quite similar except for variance which, to be fair, only their algorithm has been optimized for."
                },
                "questions": {
                    "value": "Is my assessment of Proposition 1 as loose correct?\n\nIs my assessment of the algorithms having comparable performance in the experiments correct?\n\nCan the problem be sold in the context of federated learning? It seems like a natural fit there where you're training on a private distribution and you want your model to perform well on other distributions that you don't have access to.\n\nDo you have results for the time it took to train each model (sorry if I missed this)? It seems like your training method uses more information which might be an unfair advantage over other methods if it takes significantly longer."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5096/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5096/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5096/Reviewer_DML4"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5096/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697569993410,
            "cdate": 1697569993410,
            "tmdate": 1699636501053,
            "mdate": 1699636501053,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rTgitQnexc",
                "forum": "VhQUwxIHER",
                "replyto": "RipZsvwAHi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5096/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5096/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer,\n\nWe appreciate your thoughtful review of our paper. Your feedback is instrumental in refining our work, and we appreciate the time and consideration you have given to our manuscript. Here are our responses to your comments and questions:\n\n1. **The position of Proposition 2**\n\n+ Regarding the motivation of minimizing the variance of losses, we suggest readers also refer to the **Idea**\u200b paragraph in the Introduction section, where variance is also viewed as the second moment of loss distribution. We thought it is straightforward and useful to character accuracy disparities when demographics are unknown. \n+ Note that Proposition 2 is derived by using MAD metric, and it also expects that loss should be (approximately) independent of input, which is used to help understand the problem only. Thus, we left it to the appendix as a supplementary.\n\n2. **Concern about Proposition 1**\n\nThank you for your detailed reading.\n\n+ In Appendix B, we presented that Eq. (10) formulated as a pairwise loss which is tighter than variance without blowing up one side by a factor of $n$. However, shown as in Table 3 on Page 13, the pairwise loss has not achieved better results because the global information (e.g., mean loss) is hard to be incorporated for a minibatch update in practical implementation. \n+ Although loose, experimental results in Table 3 demonstrated that variance minimizing is useful to characterize accuracy disparity without demographics. We understand tight bound is a pursuit in most cases, while it is not the only criterion in this scenario.\n\n3. **Experimental Results**\n\nWe have conducted numerous detailed new experiments, please refer to the global comments for more information.\n\n4. **Relationship with Federated Learning**\n\nWe understand that learning on data without demographics is closely related to the data privacy community. However, please note that our problem setting does not introduce any prior about demographics, which indicates the solution should adapt to any demographic distribution. Thus, there is no need to set distributed groups that are based on federated learning. If each local group is exactly a group divided by interested attributes, then the central model at least knows how many local groups there exist (when it executes gradient aggregation), which can be some side information to demographics and thus beyond our problem setting.\n\n5. **Training time of each method**\n\nEach method shares similar training steps. While explicit details of the training steps are not provided, a comprehensive account of the experimental configurations is presented in Appendix F, specifically within the section titled \"Harmless Implementation of Baselines.\" In this section, we meticulously elucidate the selection of steps within a harmless fairness context. To expound further, each method is characterized by an empirical loss denoted as $\\hat{\\mu}$ in our methodology and as the learner loss (in comparison to adversarial loss) in the ARL approach. This empirical loss serves as the criterion for step selection, whereby we opt for the harmless step that manifests the loss value most proximate to that of a well-trained ERM model. This methodological choice ensures a judicious alignment with established benchmarks, fostering a fair evaluation of the experimental outcomes.\n\nWe hope these planned revisions and clarifications will address your concerns. Please let us know if you have any further questions. We are inclined to address them to improve the quality of this work."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5096/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700317515604,
                "cdate": 1700317515604,
                "tmdate": 1700319513244,
                "mdate": 1700319513244,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "541JFWuHh5",
                "forum": "VhQUwxIHER",
                "replyto": "rTgitQnexc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5096/Reviewer_DML4"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5096/Reviewer_DML4"
                ],
                "content": {
                    "comment": {
                        "value": "1. **The position of Proposition 2**\n\nTotally your purview about where the proof goes. Personally, I found the proof to be elucidating and reading it was when I started to get excited about your work.\n\n2. **Concern about Proposition 1**\n\nHard to discern from your response but it sounds like you agree that Proposition 1 is quite loose in particular the step in Equation 10. I would *strongly* encourage you to make this clear when you state Proposition 1 in the body. Otherwise, it is quite misleading.\n\n4. **Relationship with Federated Learning**\n\nMy comment was only a suggestion about a potential application of your work that could make it even stronger.\n\n5. **Training time of each method**\n\nFrom your response, it sounds like you use approximately the same number of training steps? That's not necessarily the same thing as time because maybe each one of your training steps takes longer. I would really like to see the total training time for your model in comparison to the other ones to address this concern.\n\nBased on your current responses, I will keep my evaluation."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5096/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700685016817,
                "cdate": 1700685016817,
                "tmdate": 1700685016817,
                "mdate": 1700685016817,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "STIpuYSyEs",
            "forum": "VhQUwxIHER",
            "replyto": "VhQUwxIHER",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5096/Reviewer_6MVk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5096/Reviewer_6MVk"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies the problem of deploying a fair machine learning model without using user demographic information. This problem emerges when demographic information is sensitive or private and, therefore, cannot be collected and stored by the model deployer. The paper proposes minimizing the variance of losses during training while roughly maintaining the average loss as a solution to improve fairness without demographics. Experiments are performed to demonstrate that this approach (variance minimization) improves fairness without demographics."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "\u2022\tThe paper is well written, and the related work is introduced in a very reader-friendly manner.   \n\n\u2022\tThe idea of minimizing the loss variance as a surrogate for improving fairness is exciting. Specifically, it makes practitioners avoid learning group information, increasing privacy. I would appreciate it if the paper gave more intuition about why we should expect that variance minimization improves fairness.   \n\n\u2022\tThis reviewer appreciates the detailed discussion and intuition about how to solve the proposed optimization problem. It was very clearly explained.  \n\n\u2022\tThe results in the paper (Table 1) indicate that the simple proposed approach of decreasing variance may outperform existing methods \u2013 see weaknesses for comments."
                },
                "weaknesses": {
                    "value": "\u2022\tThe paper only considers accuracy disparities as fairness metrics. At the beginning of the paper, the authors mention fairness metrics such as equalized odds; however, such metrics are not considered. The authors focus on accuracy disparities and do not provide results for other fairness metrics of major interest in the literature. This reviewer strongly suggests the authors replace two of the metrics in Table 1 with other fairness metrics (e.g., equal odds and statistical parity).  \n\n\u2022\tTable 1 provides results for the performance of different fairness improvement techniques. However, the reported values are very close. Therefore, it is hard to get any conclusion out of the reported values. Can the authors please add confidence intervals?"
                },
                "questions": {
                    "value": "\u2022\tHow does this paper relate with works in machine learning that argue that a flat loss landscape implies better generalization [1]? Does variance minimization imply a flat minimum in the loss landscape?  \n\n\u2022\tThe problem of minimizing some model performance constrained to the loss being small enough has been studied in the Rashomon effect literature. The paper may benefit from related work in the field, such as [2]. Moreover, the authors argue that if $\\delta$ (Eq. 1) is small enough, then the \"fair\" model and the ERM are equivalent. A recent paper showed how small $\\delta$ needs to be to ensure that the models are equivalent [3].  \n\n\n[1] Hao Li et al. Visualizing the Loss Landscape of Neural Nets. 2018.  \n\n[2] Amanda Coston et al. Characterizing Fairness Over the Set of Good Models Under Selective Labels. 2021.  \n\n[3] Lucas Monteiro Paes. On the Inevitability of the Rashomon Effect. 2023."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5096/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5096/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5096/Reviewer_6MVk"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5096/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698709627769,
            "cdate": 1698709627769,
            "tmdate": 1699636500957,
            "mdate": 1699636500957,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "nwC6GJghgQ",
                "forum": "VhQUwxIHER",
                "replyto": "STIpuYSyEs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5096/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5096/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer,\n\nWe appreciate your thorough review of our paper and your insightful comments. Your feedback is invaluable in refining and strengthening our work. Here are our responses to your comments and questions:\n\n1. **Other Fairness metrics**\n\n+ As we claimed in the Introduction, utility disparity across different groups is our interest, which also aligns with the recent studies based on Rawlsian fairness where predictive accuracy of the worst-off group is concerned and they do not include equal odds and statistical parity either.\n+ Note that equal odds and statistical parity measure fairness over predicted y, which requires the latent representation or final prediction to be somehow invariant to groups. On the one hand, these metrics are not compatible with the concept of model utility we advocated. On the other hand, to achieve such fairness we need to reformulate the objective because our original goal is to let loss value (determined by both predicted y and ground truth y) be approximately invariant to groups.\n\n2. **Experimental Results**\n\nWe have conducted numerous detailed new experiments, please refer to the global comments for more information.\n\n3. **Discussion of related works**\n\nThanks for suggesting the related works. We will incorporate relevant discussions and references in the related work section to provide a more comprehensive background for our work.\n\n+ We understand variance regularization on ERM aims at improving the model reliability but may not necessarily imply a flat minimum in the loss landscape [1]. A flat minimum like sharpness-aware minimization [4] expects that average loss should not change by much if a model is perturbed within a small region. However mathematically, variance minimization cannot ensure such a property.\n+ Based on the concept of Rashomon [3], research [2] also proposed a method that achieves fairness from good-utility models, which is quite similar to the branch of \u201cHarmless Fairness\u201d in Related Work. Our work differs from theirs by two key differences. (1) We further focused on \u201cHarmless fairness without demographic\u201d and thus paid much attention to how to describe fairness under such a restrictive scenario. (2) Research [2] solves a constrained optimization problem, which means we need a proper upper bound for the average loss. One may obtain it by running an ERM model beforehand and then setting it, which is not needed as we treat it as a bi-objective problem and the constraint is satisfied by adjusting the weight factor $\\lambda$.\n\nWe hope these planned revisions and clarifications will address your concerns. Please let us know if you have any further questions. We are inclined to address them to improve the quality of this work. \n\n[1] Hao Li et al. Visualizing the Loss Landscape of Neural Nets. 2018.\n\n[2] Amanda Coston et al. Characterizing Fairness Over the Set of Good Models Under Selective Labels. 2021.\n\n[3] Lucas Monteiro Paes. On the Inevitability of the Rashomon Effect. 2023.\n\n[4] Sharpness-Aware Minimization for Efficiently Improving Generalization, ICLR 2021."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5096/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700317397870,
                "cdate": 1700317397870,
                "tmdate": 1700319412426,
                "mdate": 1700319412426,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HjhU6fQq3k",
                "forum": "VhQUwxIHER",
                "replyto": "nwC6GJghgQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5096/Reviewer_6MVk"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5096/Reviewer_6MVk"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you, authors, for your response. \n\nI will maintain my score. \nHowever, I want to highlight the importance of improving the paper based on the comments of reviewers 6VBP and NHS2."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5096/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700677851769,
                "cdate": 1700677851769,
                "tmdate": 1700677851769,
                "mdate": 1700677851769,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Qn9HcgmNWt",
            "forum": "VhQUwxIHER",
            "replyto": "VhQUwxIHER",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5096/Reviewer_NHS2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5096/Reviewer_NHS2"
            ],
            "content": {
                "summary": {
                    "value": "To enhance fairness in situations where sensitive attributes are unavailable, this paper proposes the minimization of the variance of the loss function, specifically minimizing loss distribution\u2019s second moment while not increasing its first moment, dubbed as VFair in this paper. \n\nTo enhance fairness without compromising the model's utility, this paper builds upon dynamic barrier gradient descent [1] and introduces further improvements. During the training process, the neural network's parameters are updated harmlessly, thereby ensuring the model's utility is preserved."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "\u2022\tThis paper proposes a simple yet effective method to improve fairness without demographic information, namely by minimizing the variance of the training loss. \n\n\u2022\tFurthermore, to ensure the enhancement of fairness without compromising the model's utility, the harmless update method is used to update the model's parameters. This method ensures that gradient updates align with the primary objective's gradient without causing conflicts.\n\n\u2022\tPoint out the relationship between MAD and the variance of the loss function."
                },
                "weaknesses": {
                    "value": "\u2022\tAlthough this is the first time introducing the idea of reducing the variance of the loss function into the fairness domain, similar concepts have been explored in earlier works [2, 3, 4]. It may be advisable to include a more comprehensive discussion in the related work section.\n\n\u2022\tIt is advisable to supplement additional experimental details. For example, a more comprehensive elaboration on the configuration of the ERM model for various datasets, particularly those not discussed in the DRO, would be beneficial. Additionally, it is better to specify the optimizer and the learning rate decay strategy, as these factors may have a impact on the stability and convergence of the loss function. While the paper briefly mentions the setting of the hyperparameter $\\epsilon$, it may appear somewhat ambiguous. It is also necessary to clarify the robustness of the hyperparameter $\\epsilon$.\n\n\u2022\tThe explanation of the optimization objective in Appendix B.2 seems to contradict the main body of the paper. The $\\hat{\\sigma}$ in the main body refers to a different object than the $\\hat{\\sigma}$ mentioned in the appendix.\n\n\u2022\tWhile the gains over baselines are moderate to low, it is advisable to provide standard deviations to better illustrate the stability of the outcomes.\n\n[1] Gong, Chengyue, and Xingchao Liu. \"Bi-objective trade-off with dynamic barrier gradient descent.\" NeurIPS 2021 (2021).\n\n[2] Balaban, Valeriu, Hoda Bidkhori, and Paul Bogdan. \"Improving Robustness: When and How to Minimize or Maximize the Loss Variance.\" 2022 21st IEEE International Conference on Machine Learning and Applications (ICMLA). IEEE, 2022.\n\n[3] Li, Tian, et al. \"Tilted empirical risk minimization.\" arXiv preprint arXiv:2007.01162 (2020).\n\n[4] Lin, Yexiong, et al. \"Do We Need to Penalize Variance of Losses for Learning with Label Noise?.\" arXiv preprint arXiv:2201.12739 (2022)."
                },
                "questions": {
                    "value": "\u2022\tIs the \"the sample will be filtered out from updating in this iteration.\" in the second-to-last paragraph on page five meaning not using this portion of the sample for training?\n\n\u2022\tHow is it derived from equation (6) that $\\lambda = 0$ in certain cases?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5096/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698737384965,
            "cdate": 1698737384965,
            "tmdate": 1699636500834,
            "mdate": 1699636500834,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "yNfLfSZDnG",
                "forum": "VhQUwxIHER",
                "replyto": "Qn9HcgmNWt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5096/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5096/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer,\n\nWe appreciate your detailed and insightful feedback on our manuscript, and we would like to express our gratitude for taking the time to thoroughly review our work. We have carefully considered each of your points and suggestions, and we are committed to addressing them to enhance the quality and clarity of our contribution.\n\n1. **Discussion about related works**\n\nThank you for suggesting the related studies. As we discussed in Section 5, we agreed that penalizing the variance of losses can be motivated by different applications, and we would add its advantage of robustness [1]. Particularly, in the submitted manuscript, we have pointed out the relation between variance regularization and instance-reweight learning inspired by the work [2], which is not a fairness study though. TERM [3] reformulates the standard ERM which improves the worst-performance fairness and thus would be included as a worst-case fairness method.\n\n2. **Additional experimental details**\n\nYour suggestion regarding additional experimental details is well-received. We apologize for the omission of the model structure and will include the necessary details in the paper.\n\n+ All the models, excluding FairRF, which operates within a distinct problem setting, conform to a shared neural network framework. Specifically, for binary classification tasks, the core neural network architecture consists of an embedding layer followed by two hidden layers, with 64 and 32 neurons, respectively. In the ARL model, an additional adversarial component is integrated, detailed in its respective paper, featuring one hidden layer with 32 neurons. For multi-classification tasks, the primary neural network transforms into resnet18, and the embedding layer transitions to a Conv2d-based frontend. Throughout these experiments, the Adagrad optimizer was employed. FairRF, utilizing its officially published code implementation, maintains the same backbone network with nuanced variations in specific details. \n+ To address the ambiguity in the explanation of the hyperparameter epsilon, we will provide a more detailed clarification. When the angle of the gradient reaches an orthogonal angle, Formula 4 can be simplified as $\\lambda \\ge \\epsilon$, indicating that in the absence of conflicts in gradient updating, to what extent do we still want the primary objective to be updated? **We designate this as a hyperparameter because we have observed variations in the quality of data across different datasets.** For instance, on the COMPAS dataset, known for its numerous noisy samples [4], a larger epsilon is needed to control the model to maintain utility. In this case, the regularization of the second moment of the distribution is constrained, preventing the model from focusing more on outlier samples. For reproducibility, it is recommended to set the value of epsilon as 0, 0.5, 0.5, and 3 for Law School, UCI Adult, CelebA, and COMPAS, respectively.\n\n3. **The ambiguity of symbols**\n\nWe apologize for the ambiguity of objectives. To be clearer, we used $\\pi, \\hat{\\sigma}, \\hat{\\sigma}^2$ to denote the 3 objectives derived in B.1 and Table 3: \n\n+ $\\pi=\\sum_{i=1}^{N-1}|\\ell_i - \\ell_{i+1}|$\n+  $\\hat{\\sigma} = \\frac{1}{\\sqrt{N}}\\sqrt{\\sum_{i=1}^N(\\ell_i - \\hat{\\mu})^2}$\n+ $\\hat{\\sigma}^2 = \\frac{1}{N}\\sum_{i=1}^N(\\ell_i - \\hat{\\mu})^2$\n\n4. **Experimental Results**\n\nWe have conducted numerous detailed new experiments, please refer to the global comments for more information.\n\n5. **Details about the filtered samples**\n\nYour interpretation is accurate. This subset of samples will not be utilized for training during the current iteration. Nevertheless, in the subsequent iteration, we will reevaluate the selection of samples\n\n6. **The derivation of $\\lambda=0$**\n\nWe appreciate your inquiry and apologize for the confusion we may have caused. Actually, the statement is not derived from Equation 6. Here, we aim to provide an intuitive explanation of the update strategy. If the primary objective is satisfactory, attention should shift to the secondary objective. In this case, the lambda should be relatively small, potentially becoming zero as it is non-negative. The design of $\\lambda_2$, as outlined in Equation 6, aligns with this strategy. When the primary objective is satisfactory, the value of \\mu tends to be close to the minimum loss of the selected samples, resulting in a value close to zero.\n\nWe hope these planned revisions and clarifications will address your concerns. Please let us know if you have any further questions. We are inclined to address them to improve the quality of this work."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5096/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700315515321,
                "cdate": 1700315515321,
                "tmdate": 1700319429085,
                "mdate": 1700319429085,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "v38Cd94ySC",
                "forum": "VhQUwxIHER",
                "replyto": "4HN80cbkIZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5096/Reviewer_NHS2"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5096/Reviewer_NHS2"
                ],
                "content": {
                    "title": {
                        "value": "Response to the author"
                    },
                    "comment": {
                        "value": "Dear authors,\n\nThanks for your reply. I agree with 6VBP and there is still some to deal with. So I stand my score."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5096/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700705673929,
                "cdate": 1700705673929,
                "tmdate": 1700705673929,
                "mdate": 1700705673929,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "pUPPrVXSl7",
            "forum": "VhQUwxIHER",
            "replyto": "VhQUwxIHER",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5096/Reviewer_6VBP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5096/Reviewer_6VBP"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes minimizing the standard deviation of classification loss among training samples as a way to promote fairness without access to sensitive attributes (by promoting worst-off group accuracy)."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "- Ensuring worst-off group fairness for unknown group partitions has strong real-world significance.\n- Using variance as a proxy for worst-off group fairness looks novel.\n- The paper is well written."
                },
                "weaknesses": {
                    "value": "- Experimental results look underwhelming and not accompanied by confidence intervals or any form of statistical significance tests.\n  - Differences between different methods or to unconstrained ERM do not seem large; it's difficult to attribute the proper importance to the very small metric differences without this context.\n  - Plus, all results correspond to averages over 10 runs, so these details should be easy to compute.\n\n- Code or experimental artifacts are not shared.\n  - Properly reviewing the paper would require access to code and empirical results, since the main claims are quite empirical in nature (e.g., lower loss variance equates to higher fairness).\n\n- It would be important to have a comparison with methods from the constrained optimization literature (standard variance minimization subject to constraint on the empirical risk) or from the standard fairness literature (e.g., with access to sensitive attributes).\n  - Of course it is not expected for VFair to surpass methods that have access to sensitive attributes, but it would be useful to properly contextualize VFair's results.\n\n- Some important details are missing (see `Questions`)."
                },
                "questions": {
                    "value": "- Was a standard constrained optimization approach to the problem attempted before constructing the proposed algorithm?\n  - Although the intuition behind VFair is well explained, it seems that to motivate a new algorithm it would make sense to compare to standard CO algorithms or justify in some other way why they are not used.\n  - Why is the standard Lagrangian dual ascent formulation not used? e.g., see [A], and the [TensorFlow CO framework](https://github.com/google-research/tensorflow_constrained_optimization/blob/master/README.md)\n\n\n- > \"Lagrangian function, which however employs a fixed and finite multiplier and thus may not fully satisfy the constraint\"\n  - The Lagrangian function does not employ a fixed multiplier, the $\\lambda$ would have to be optimized jointly with the $\\theta$ until a stable saddle point is reached.\n\n- What ML algorithm is actually used to fit the data? The paper is written w.r.t. generic model parameters $\\theta$, which is fine, but what actual model was used in the experiments? A neural network?\n- Possibly I missed it, but I can't seem to find an actual definition for the loss function $\\ell$ used throughout the paper. Is it BCE?\n  - Multiple plots show the distribution of sample-wise loss for different models; are all algorithms evaluated on the same loss function?\n\n- Is there any explanation for why some (greyed out) rows of Table 2 always result in a constant/uniform classifier over the 10 runs?\n  - Perhaps plotting $\\lambda_1$ and $\\lambda_2$ as well as loss as training progresses could shed some light.\n\n---\n[A]: Cotter, Andrew, Heinrich Jiang, and Karthik Sridharan. \"Two-player games for efficient non-convex constrained optimization.\" Algorithmic Learning Theory. PMLR, 2019."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5096/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698754048637,
            "cdate": 1698754048637,
            "tmdate": 1699636500735,
            "mdate": 1699636500735,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "X3TuYhDpjx",
                "forum": "VhQUwxIHER",
                "replyto": "pUPPrVXSl7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5096/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5096/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer,\n\nWe would like to express our gratitude for taking the time to thoroughly review our work. We have carefully considered each of your comments, and we are committed to addressing them to enhance the overall quality.\n\n1. **Experimental Results**\n\nWe have conducted numerous detailed new experiments, please refer to the global comments for more information.\n\n2. **Code and Experimental Artifacts**\n\nFollowing your suggestion, we have included our code in the supplementary materials, which showcases a simple yet effective approach to achieving fairness. We will release the full code once this paper is accepted.\n\n3. **Comparison with Constrained Optimization Approaches**\n\n+ We reminded that utilizing constrained optimization essentially requires a proper $\\delta$ (an upper bound of the minimum of loss in Eq. 1&2) which is not accessible unless minimizing the average losses on the entire training set beforehand. \n+ We clarified that the targeted problem is treated as a bi-objective problem in this paper, with two objectives having different priorities. In this sense, the proposed algorithm does not have to handle $\\delta$ separately, while \u201cthe constraint\u201d is implicitly satisfied. While the suggested [1] and TFCO focus on solving constrained optimization problems. To avoid confusion, we will cite these works and offer a reminder in the main body of the paper. \n+ Using a fixed \u03bb, which is simply set to 1 in Table 2, we linearly combined two objectives, which serve as an ablation study in our experiments. The results showed that the model performance may significantly drop, e.g., on COMPAS. \n+ We apologize for the description of \"a fixed multiplier\". Our intended meaning is that the advanced Lagrange method aims at the optimal multiplier while it is dynamic along with the optimization process in our method.\n\n4. **Details on the model structure**\n\nFollowing your advice, we have included the necessary description in the paper.\n\nAll the models, excluding FairRF, which operates within a distinct problem setting, conform to a shared neural network framework. Specifically, for binary classification tasks, the core neural network architecture consists of an embedding layer followed by two hidden layers, with 64 and 32 neurons, respectively. In the ARL model, an additional adversarial component is integrated, detailed in its respective paper, featuring one hidden layer with 32 neurons. For multi-classification tasks, the primary neural network transforms into resnet18, and the embedding layer transitions to a Conv2d-based frontend. Throughout these experiments, the Adagrad optimizer was employed. FairRF, utilizing its officially published code implementation, maintains the same backbone network with nuanced variations in specific details. \n\n5. **Details about Loss Function**\n\nThank you once again for your detailed reading. Throughout the experiments, we implemented binary cross-entropy (BCE) for binary classification and categorical cross-entropy (CE) for multi-class classification. However, we would like to underscore that our method is general and is compatible with other forms of loss as well. We will incorporate this information into our main paper.\n\n6. **Grayed Out Rows in Table 2**\n\nTable 2 shows that applying a fixed $\\lambda$ or using $\\lambda_1$ only might result in uniform classifiers. This is because without adjustment of $\\lambda_2$, the model is at risk of providing very uncertain predictions to training samples, that is, most of the data lies near the decision boundary. Fig. 3 observes the same phenomenon by monitoring the distance between samples to the decision boundary, which demonstrates the necessity of $\\lambda_2$. Mathematically, $\\lambda_2$ lifts up the importance of the average loss from the \u201closs perspective\u201d even if the gradients of two objectives do not conflict with each other by much. Following your advice, we added the plot of $\\lambda$ in Appendix G.         \n\nWe hope these planned revisions and clarifications will address your concerns. Please let us know if you have any further questions. We are inclined to address them to improve the quality of this work. \n\n[1]: Cotter, Andrew, Heinrich Jiang, and Karthik Sridharan. \"Two-player games for efficient non-convex constrained optimization.\" Algorithmic Learning Theory. PMLR, 2019.\n\n[2] Gong, Chengyue, and Xingchao Liu. \"Bi-objective trade-off with dynamic barrier gradient descent.\" NeurIPS 2021 (2021)."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5096/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700315012979,
                "cdate": 1700315012979,
                "tmdate": 1700319616139,
                "mdate": 1700319616139,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "EVD9oWhpkd",
                "forum": "VhQUwxIHER",
                "replyto": "X3TuYhDpjx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5096/Reviewer_6VBP"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5096/Reviewer_6VBP"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the rebuttal clarifications and the paper additions. I think the added details have definitely improved readability of the manuscript.\n\nRegarding the added standard deviation details for Table 1 (which are still missing for Table 2): are the standard deviation results in percentage (as are the values shown), or are the results in absolute terms? I find it hard to believe that a standard deviation of 0.007% can be achieved over 10 runs of a stochastic method on such a small dataset as COMPAS. If these values are not in percentage, please correct to use the same unit throughout the table. Additionally, if they are in absolute terms, it means most metrics for most models are actually pair-wise indistinguishable, except for the variance for which only VFair was optimized. This fact seems to have been pointed out by several reviewers."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5096/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700664993251,
                "cdate": 1700664993251,
                "tmdate": 1700664993251,
                "mdate": 1700664993251,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]