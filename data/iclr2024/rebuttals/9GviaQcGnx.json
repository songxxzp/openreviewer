[
    {
        "title": "Constrained Parameter Regularization"
    },
    {
        "review": {
            "id": "eT4rryQs4g",
            "forum": "9GviaQcGnx",
            "replyto": "9GviaQcGnx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8203/Reviewer_EQkA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8203/Reviewer_EQkA"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a constrained parameter regularization (CPR) technique that dynamically adjusts its regularization strength on parameters based on the violation of the constraints. The authors address the optimization of CPR by an adaptation of the augmented Lagrangian method and provide a simple mechanism to adapt the upper bounds during the optimization. The experimental results speak in favor of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper studies a very important field as the performance of modern NNs are often bottlenecked by the underlying optimization and many tricks boil down to easing the optimization difficulty. I found the topic significant and interesting. \n- The design of adjusting the regularization strength dynamically is very interesting and novel.\n- The performance of the proposed method looks good in terms of both the average and the best performance. The method is also robust against different design choices of $\\kappa$."
                },
                "weaknesses": {
                    "value": "- Although it is a very interesting paper, I feel like the analysis is not good enough. There are many interesting aspects that could be further explored IMHO. For example:\n1. If $\\lambda$ is used to accumulate constraint violations, shouldn't it be $\\lambda_{t+1}=\\lambda_t + \\mu(R(\\theta_t) - \\kappa)^+$? \n2. Have the authors considered adjusting $\\lambda$ in a non-linear way? For example $\\lambda_{t+1}=\\lambda_t + \\mu f((R(\\theta_t) - \\kappa))^+$, where $f$ is a non-linear function. Maybe it leads to faster convergence if tuned properly?\n3. Should $\\lambda$ have a self decay factor? Similar to a diverging behavior with large learning rates, when a feasible point $\\theta_{t}$ is found at a very large $\\lambda_t$. In the next iteration, $\\theta_{t+1}$ may overshoot in the opposite direction and becomes infeasible again because $\\lambda_t$ has not yet become small enough.\n\n- The experimental setting is not very clear to me. What is the percentage of correct labels in the Object Detection experiment? Is the \"Object Detection\" actually classification on CIFAR100? As \"Object Detection\" is a well-established field itself, I found the terms very confusing. \n\n- How significant is the performance difference between CPR and weight decay? As the absolute performance difference is not huge, it would be great if the standard deviation of different runs of the same setting can be provided. \n\n- Maybe I missed it, but how about a baseline of weight decay factor dependent on the magnitude of the parameters, e.g. larger weights leads to a larger decay coefficient (linear or non-linear dependence)? I wonder how well this could achieve the goal of the proposed method.\n\n\nMinor:\n- Equation 6 should have $\\kappa^j$ rather than $\\kappa$ \n- The authors claim that the method is not sensitive to $\\mu$, but it would still be great to see the analysis on it.\n- Better to use consistent notations. E.g. in Figure E1, fix -> Kappa-K, dependent -> Kappa_kI0, warm started ->Kappa_Is\n- Larger scale evaluation, e.g. on image-net, would be strongly advised."
                },
                "questions": {
                    "value": "See weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8203/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698620003230,
            "cdate": 1698620003230,
            "tmdate": 1699637017907,
            "mdate": 1699637017907,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "TPQwCpfKms",
                "forum": "9GviaQcGnx",
                "replyto": "eT4rryQs4g",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8203/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8203/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer EQkA 1/2"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the helpful feedback and hope our response and answers can convince the reviewer to increase their score. We address the concerns and questions as follows:\n\n**W1:** *If $lambda$  is used to accumulate constraint violations, shouldn't it be $\\lambda_{t+1} = \\lambda_t + \\mu (R(\\theta_t )-\\kappa)^+ $?*\n\nUnfortunately, this wording was imprecise. $\\lambda$ accumulates constraint function values (not constraint violations, otherwise $\\lambda$ would never decrease). We adjusted the wording in the manuscript (Section 4.1).\n\n\n**W2:** *Have the authors considered adjusting $\\lambda$  in a non-linear way? For example $\\lambda_{t+1} = \\lambda_t + \\mu f ((R(\\theta_t )-\\kappa))^+ $, where $f$ is a non-linear function. Maybe it leads to faster convergence if tuned properly?*\n\nWe have not tried a non-linear adjustment. The update rule is derived as the closed form solution for the maximization in $\\hat{F}$, see Equation (2). Note that this can also be seen as a (projected) gradient ascent step on $f(x) + \\lambda \\cdot c(x)$ (see right side of Equation (1)) with respect to $\\lambda$ and a learning rate $\\mu$. Additionally, for a different choice of $\\hat{F}$, a different update rule is obtained. \nNevertheless, it should be noted that we use the method as a means to implement a computationally cheap adjustment procedure for the individual $\\lambda_i$. It is not immediately clear if a faster convergence of the $\\lambda_i$ would lead to more favorable training dynamics and thus an overall better training result. \n\n\n**W3:** *Should $\\lambda$ have a self decay factor? Similar to a diverging behavior with large learning rates, when a feasible point $\\theta_t$  is found at a very large $\\lambda_t$. In the next iteration,  $\\theta_{t+1}$  may overshoot in the opposite direction and becomes infeasible again because $\\lambda_t$ has not yet become small enough.*\n\nIf the constraint violations are growing moderately, and since we initialize $\\lambda=0$, $\\lambda$ should also not be growing too quickly. Additionally, the procedure seeks to find a stationary point where $\\eta  \\cdot \\nabla L = - \\lambda \\cdot \\nabla c$ (in case of gradient descent). Instabilities may arise, for example, if one of the balancing forces, in this case, the update direction for $L$ may drastically change. However, this should be unlikely when training with momentum on $L$. \nWe thus assume that only a very unstable training progress could potentially cause such issues, and we never observed them in any of our experiments. \n\n\n**W4:** *The experimental setting is not very clear to me. What is the percentage of correct labels in the Object Detection experiment? Is the \"Object Detection\" actually classification on CIFAR100? As \"Object Detection\" is a well-established field itself, I found the terms very confusing.*\n\nWe apologize for this misnaming. You are right, we mistakenly named the task \u201cobject detection\u201d instead of \u201cimage classification\u201d. We changed the wording in the paper accordingly to  \u201cimage classification\u201d. The reported metric is simply accuracy. \n\n**W5:** *How significant is the performance difference between CPR and weight decay? As the absolute performance difference is not huge, it would be great if the standard deviation of different runs of the same setting can be provided.*\n\nWe performed additional experiments for three random seeds in the language modeling experiment (similar to the image classification experiments) and added the standard deviation for experiments on image classification and language modeling. We added the standard deviation for the LLM experiments in the plot and in Tables 6 and 7 in Appendix G and for the image classification experiments in the heat map plots in Appendix F. \nTo assess the significance of our results, we also performed a Welch\u2019s t-test on the GPT2s experiments with the best AdamW and best AdamCPR configuration, showing a significant difference with a p-value of 0.018. \n\n\n\n**W6:** *Maybe I missed it, but how about a baseline of weight decay factor dependent on the magnitude of the parameters, e.g. larger weights leads to a larger decay coefficient (linear or non-linear dependence)? I wonder how well this could achieve the goal of the proposed method.*\n\nWe are unsure if we understand this idea of a baseline correctly. Since the decoupled weight decay update already contains the parameter in the update step: $\\theta_i = \\theta_{i-1} - \\eta \\cdot \\operatorname{Opt}(L)  - \\eta \\cdot \\gamma \\cdot \\theta_{i-1}$. Should the weight decay value $\\gamma$ additionally be a function of $\\theta_0$ or  $\\theta_{i-1}$? \n\n1/2"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8203/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700000694634,
                "cdate": 1700000694634,
                "tmdate": 1700000694634,
                "mdate": 1700000694634,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UWpG1uKMWU",
                "forum": "9GviaQcGnx",
                "replyto": "eT4rryQs4g",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8203/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8203/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer EQkA 2/2"
                    },
                    "comment": {
                        "value": "**WM1:** *Equation 6 should have $\\kappa^j$  rather than $\\kappa$*\n\nThanks, yes indeed, we have fixed this missing index.\n\n**WM2:** *The authors claim that the method is not sensitive to $\\mu$  but it would still be great to see the analysis on it.*\n\nWe agree that the paper missed so far to show experiments on the update rate $\\mu$. We added a new section in the Appendix (Appendix D) with two plots. One on the modular addition task and one on the image classification task which compares different $\\mu$ values and demonstrates the low sensitivity of the update rate. \n\n\n**WM3:** *Better to use consistent notations. E.g. in Figure E1, fix -> Kappa-K, dependent -> Kappa_kI0, warm started ->Kappa_Is*\n\nThanks for pointing us to this inconsistency, we corrected the figures accordingly. \n\n**WM4:** *Larger scale evaluation, e.g. on image-net, would be strongly advised.*\n\nWe expected the GPT2m experiment to be a large-scale experiment since the GPT2m model has 354M parameters (in contrast to 124M in GPT2s). As far as we know, a ResNet152, which is often used for imagenet training, only has ~64M parameters. \n\nIf we have addressed (some of) your concerns, we would be very thankful if you considered raising your score. We would be very happy to answer any follow-up questions and concerns. \n\n2/2"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8203/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700000767711,
                "cdate": 1700000767711,
                "tmdate": 1700000767711,
                "mdate": 1700000767711,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6ioadhtE5N",
                "forum": "9GviaQcGnx",
                "replyto": "eT4rryQs4g",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8203/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8203/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer EQkA,\n\nWe received a wide range of scores in our paper's reviews and have carefully addressed the feedback provided. We kindly request that you reconsider your evaluation in light of our responses. We have made specific revisions by correcting issues in our manuscript, adding a section about the update rule, and adding the standard deviation to provide significance. We believe these changes address your initial concerns. \n\nWe welcome any further questions or the opportunity to discuss any remaining issues. Thank you for your time and consideration."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8203/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700388758743,
                "cdate": 1700388758743,
                "tmdate": 1700388758743,
                "mdate": 1700388758743,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "X0q3tAGyYO",
                "forum": "9GviaQcGnx",
                "replyto": "eT4rryQs4g",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8203/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8203/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer EQkA, \n\nWe submitted our rebuttal eight days ago and sent a reminder 3 days ago, and we would very much appreciate a reply during the reviewer-author discussion, which ends in 14 hours. We would gladly reply promptly if you still have any concerns or questions or replies. We\u2018re looking forward to your reply. Thank you for your service to ICLR!"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8203/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700689662081,
                "cdate": 1700689662081,
                "tmdate": 1700689810892,
                "mdate": 1700689810892,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "HUxHK1PdWD",
            "forum": "9GviaQcGnx",
            "replyto": "9GviaQcGnx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8203/Reviewer_HpN9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8203/Reviewer_HpN9"
            ],
            "content": {
                "summary": {
                    "value": "Summary\uff1a The paper proposes an alternative to traditional weight decay, which does not uniformly apply a constant penalty to all parameters. Instead, it enforces an upper bound on statistical measures for parameter groups and addresses this issue through adjustments to an enhanced Lagrangian method. It also allows for different regularization strengths for each parameter group, without the need to explicitly set penalty coefficients for regularization terms."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "Strengths:\n1\uff09 The authors introduce a constrained parameter regularization that does not uniformly \napply a constant penalty to all parameters.\n2\uff09 The authors provide an open-source implementation of CPR, which can be easily adjusted \nby replacing the optimizer class."
                },
                "weaknesses": {
                    "value": "1\uff09 The authors should provide a clearer exposition of the problem statement and research \nmotivation in the abstract.\n2\uff09 In the related work section, the authors should provide a more comprehensive review of \nprior research.\n3\uff09 The experimental results conducted by the authors did not show a significant \nimprovement in optimization performance, and they did not provide additional \nexperimental results or comparisons related to existing work.\n4\uff09 While applying different regularization strengths to each parameter group may seem \nintriguing, the authors have not discussed the necessity and potential advantages of this \napproach compared to traditional methods."
                },
                "questions": {
                    "value": "N/A"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8203/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698797886644,
            "cdate": 1698797886644,
            "tmdate": 1699637017788,
            "mdate": 1699637017788,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "tlkYPvYo1V",
                "forum": "9GviaQcGnx",
                "replyto": "HUxHK1PdWD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8203/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8203/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer HpN9"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the helpful feedback and hope our response and answers can convince the reviewer to increase their score. We address the concerns and questions as follows:\n\n**W1:** *The authors should provide a clearer exposition of the problem statement and research motivation in the abstract.*\n\nWe thank the reviewer for the constructive comments. We adjusted the abstract accordingly\nand tried to clearly emphasize the research question. We moved the motivation to the beginning of the abstract and reformulated it to have a clearer and more trenchant abstract.\n\n**W2:** *In the related work section, the authors should provide a more comprehensive review of prior research.*\n\nWe added an additional related work on adaptive weight decay [Nakamura & Hong (2019)] and compared it experimentally. We also added baseline experiments on weight decay schedule and rescaling. Please find the experimental results in the appendix (Figures E2, E3, E4). To the best of our knowledge, we mentioned all relevant related work. Does the reviewer know any additional related work we missed so far? We would gladly add it.\n\n\n**W3:** *The experimental results conducted by the authors did not show a significant improvement in optimization performance, and they did not provide additional experimental results or comparisons related to existing work.*\n\nWe performed additional experiments for three random seeds in the language modeling experiment and added the standard deviation for experiments on image classification and language modeling. We added the standard deviation for the LLM experiments in the plot and in Table G.6 and G.7 in Appendix G and for the image classification experiments in all heat map plots in Appendix F. \nTo assess the significance of our results, we also performed a Welch\u2019s t-test on the GPT2s experiments with the best AdamW and best AdamCPR configuration, showing a significant difference with a p-value of 0.018. \nFurthermore, we provide additional experiments for comparison to existing work (see comment on the previous point). Does this address the concerns of the reviewer regarding the significance and comparison to existing work? \n\n\n**W4:** *While applying different regularization strengths to each parameter group may seem intriguing, the authors have not discussed the necessity and potential advantages of this approach compared to traditional methods.*\n\nChoosing individual regularization strength is a more general case of choosing one strength for all parameter groups. Our hypothesis was that this additional flexibility leads to better performance. E.g., we do not regularize parameters that do not need regularization. We clarify this in the modified abstract too. In the introduction, we already write: \"However, not all parameters in a neural network have the same role or importance and different weights could benefit from different regularizations. Similarly, it is unclear if a single weight decay value is optimal for the entire duration of optimization, especially for large-scale training.\u201d Does this address your concern? \n\nIf we have addressed (some of) your concerns, we would be very thankful if you considered raising your score. We would be very happy to answer any follow-up questions and concerns."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8203/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700000475603,
                "cdate": 1700000475603,
                "tmdate": 1700000475603,
                "mdate": 1700000475603,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YLcin3t8fs",
                "forum": "9GviaQcGnx",
                "replyto": "HUxHK1PdWD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8203/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8203/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer HpN9, \n\nwe received a wide range of scores in our paper's reviews and have carefully addressed the feedback provided. We kindly request that you consider our responses and would love to engage with you. We have made specific revisions by updating the abstract, adding the standard deviation to provide significance, and providing more related work and baselines. We believe these changes address your initial concerns. \n\nWe welcome any further questions or the opportunity to discuss any remaining issues. Thank you for your time and consideration."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8203/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700388622721,
                "cdate": 1700388622721,
                "tmdate": 1700388622721,
                "mdate": 1700388622721,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fgaCpKUHFr",
                "forum": "9GviaQcGnx",
                "replyto": "HUxHK1PdWD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8203/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8203/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer HpN9, \n\nWe submitted our rebuttal eight days ago and sent a reminder 3 days ago, and we would very much appreciate a reply during the reviewer-author discussion, which ends in 14 hours. We would gladly reply promptly if you still have any concerns or questions or replies. We\u2018re looking forward to your reply. Thank you for your service to ICLR!"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8203/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700689641147,
                "cdate": 1700689641147,
                "tmdate": 1700689832240,
                "mdate": 1700689832240,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "FARHLmhHho",
            "forum": "9GviaQcGnx",
            "replyto": "9GviaQcGnx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8203/Reviewer_VW8f"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8203/Reviewer_VW8f"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposed constrained parameter regularization as an alternative to L2 / weight decay regularization. The paper provided a computationally efficient method, explored different constraints initialization, and delivered experiments on various tasks."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1) The methods allow different constraints for different parameter groups which is not possible to achieve with a traditional approach.\n2) No computational overhead and the code is open-source.\n3) The approach seems to be effective on different empirical tasks e.g. grokking, object detection, and medical image tasks."
                },
                "weaknesses": {
                    "value": "1) It\u2019s difficult to find a good value of the upper bound for each group of the parameters.\n2) The accuracy gain for the language model is quite small, so it\u2019s not clear to say that AdaCPR is better than AdamW on this task.\n3) See questions"
                },
                "questions": {
                    "value": "1) \u201cWe can also close the gap between training and validation performance by increasing the weight decay regularization but this comes at the price of unstable training\u201d . Shouldn\u2019t we compare the AdamAdaCPR with AdamW with a bigger regularization parameter? Since our concern here is the grokking effect. \n\n2) There are large performance drops in Table 3 when the number of warm-start steps increases from 3k to 4k. This increment is not large compared to the total number of training steps is 25k. Does this imply that CPR with Kappa-I_s is highly sensitive to the warm-start parameter?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8203/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8203/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8203/Reviewer_VW8f"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8203/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698802426939,
            "cdate": 1698802426939,
            "tmdate": 1699637017667,
            "mdate": 1699637017667,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "LAbKsft3Yr",
                "forum": "9GviaQcGnx",
                "replyto": "FARHLmhHho",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8203/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8203/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer VW8f"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the high score and warm words. We address the concerns and questions as follows:\n\n**W1:** *It\u2019s difficult to find a good value of the upper bound for each group of the parameters.*\n\nIndeed, there is still no hyperparameter-free regularization technique. Similar to the weight decay value, one needs to choose e.g. the warm start steps. These could be more  interpretable or transferable than the weight decay value, but we agree that so far this is a weakness and should be addressed in future works. \n\n**W2:** *The accuracy gain for the language model is quite small, so it\u2019s not clear to say that AdaCPR is better than AdamW on this task.*\n\nWe now performed three random seeds on the GPT2 experiments and added the standard deviation in the plot and in Table G.6 and G.7 in Appendix G. We also performed a Welch\u2019s t-test on the GPT2s experiments with the best AdamW and best AdamCPR configuration, showing a significant difference with a p-value of 0.018. \n\n**Q1:** *\u201cWe can also close the gap between training and validation performance by increasing the weight decay regularization but this comes at the price of unstable training\u201d . Shouldn\u2019t we compare the AdamAdaCPR with AdamW with a bigger regularization parameter? Since our concern here is the grokking effect.*\n\nWe agree, we can also compare it to an AdamW training with a higher weight decay, therefore we added a new plot (E.7) in Appendix E. We see the ability of weight decay to close the gap as well. We also see the instability in comparison to a more stable behavior with AdamCPR and AdamAdaCPR. \n\n**Q2:** *There are large performance drops in Table 3 when the number of warm-start steps increases from 3k to 4k. This increment is not large compared to the total number of training steps is 25k. Does this imply that CPR with Kappa-I_s is highly sensitive to the warm-start parameter?*\n\nThank you for mentioning this issue. The problem arises because the datasets are very small. Consequently, unregulated optimization tends to overfit quickly which leads to NaNs. When CPR regularization starts too late, as was the case after 4000 steps in this instance, the model is already overfitted, resulting in some broken folds. These are counted as zero in the mean, which explains the low number in the table.\n\n We would be very happy to answer any follow-up questions and concerns."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8203/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700000346258,
                "cdate": 1700000346258,
                "tmdate": 1700000346258,
                "mdate": 1700000346258,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]