[
    {
        "title": "FABRIC: Personalizing Diffusion Models with Iterative Feedback"
    },
    {
        "review": {
            "id": "j955v9Me7D",
            "forum": "zsfrzYWoOP",
            "replyto": "zsfrzYWoOP",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1988/Reviewer_bHFH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1988/Reviewer_bHFH"
            ],
            "content": {
                "summary": {
                    "value": "In the growing field of machine learning-driven visual content generation, integrating human feedback can greatly enhance user experience and image quality. This study introduces FABRIC, a method that uses the self-attention layer in popular diffusion-based text-to-image models to condition the generative process on feedback images without additional training. Through a thorough evaluation methodology, the study demonstrates that iterative human feedback significantly improves generation results, paving the way for personalized content creation and customization."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Iterative Workflow: The research emphasizes an iterative process, allowing for continuous refinement and improvement of generated images based on previous feedback.\n2. Dual Feedback System: By utilizing both positive and negative feedback images from previous generations, the method provides a balanced approach to influence future image results.\n3. Reference Image-Conditioning: This approach manipulates future results by conditioning on feedback images, offering a dynamic way to steer the generative process.\n4. Enhanced User Experience: By integrating human feedback into the generative models, the research ensures a more tailored and enhanced user experience in visual content generation.\n5. Potential in Personalized Content Creation: The findings have significant implications for creating personalized visual content based on individual user preferences and feedback.\n\nOverall, the paper introduces a robust and flexible method for refining machine-generated visual content through iterative human feedback, ensuring better alignment with user preferences."
                },
                "weaknesses": {
                    "value": "1. Limited Expansion of Distribution: The method struggles to widen the distribution beyond the initial text-conditioned one provided by the model.\n2. Feedback Loop Limitation: Since the feedback originates from the model's output, it creates a cyclical limitation where the model might only reinforce its existing biases.\n3. Diversity Collapse: As the strength of the feedback and the number of feedback images increase, the diversity of the generated images tends to diminish. The images tend to converge towards a single mode that closely resembles the feedback images.\n4. Binary Feedback System: The current feedback collection method only allows users to provide binary preferences (like/dislike) for the images. This limitation prevents users from providing nuanced feedback about specific aspects of an image.\n5. Lack of Detailed Feedback: Users cannot specify which particular aspects of an image they appreciate or dislike. This restricts the model's ability to fine-tune its output based on detailed user preferences."
                },
                "questions": {
                    "value": "See above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1988/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698107053090,
            "cdate": 1698107053090,
            "tmdate": 1699636130553,
            "mdate": 1699636130553,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "aPGGb4GQTe",
                "forum": "zsfrzYWoOP",
                "replyto": "j955v9Me7D",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1988/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1988/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the accurate summary of our method and for the insightful and constructive comments. In the following, we would like to offer our perspective on some of the points raised.\n\n- **Limited Expansion of Distribution**\n  \n  It is true that FABRIC does not necessarily expand the conditional distribution of generated images, especially when the feedback images are sampled from the same model, but this is generally the case for interventions that improve quality. Indeed, in order to improve quality it is necessary to constrain the distribution by emphasizing desirable and cutting out undesirable parts. Still, it is possible to reintroduce some diversity. For example, by adding feedback images from an external corpus in addition to the ones from the current generation, one can add more diverse images, guiding the generative process in new directions. This can even be automated by retrieving images from the external corpus that are similar to the prompt or existing feedback images (e.g. using CLIP similarity). Additionally, by adding random alphanumeric characters to the prompt it is possible to artificially increase the diversity of the generations [1]. We have added clarifying comments about this and the following two points to Section 6 of the manuscript.\n- **Feedback Loop Limitation**\n  \n  Similar to the previous point, this is more a limitation of our experimental setup rather than a fundamental limitation of FABRIC. Indeed, it is possible to move the balance of the exploration-exploitation trade-off in the direction of the former with very simple extensions to FABRIC such as the addition of a retrieval corpus. More sophisticated methods might also take the prompt into consideration, by rephrasing the prompt for increased diversity or by iterating on the prompt and taking prior feedback into account, but we leave exploration of this issue to future work.\n- **Diversity Collapse**\n  \n  We agree with the reviewer that in our experimental setup FABRIC clearly suffers from diversity collapse. However, we would like to point out that this is not necessarily the case in a general setup. In order to automate the process of evaluation, we were only giving images from the ones generated during previous rounds as feedback images. There, naturally (and hopefully), the process has to converge to a result very similar to the target image. In practice, the user can select feedback based on a variety of criteria, hence producing more diverse feedback images and (anecdotally) more diverse results. Additionally, we would like to emphasize that of all the methods we investigated, FABRIC was able to elicit the highest increase in quality metrics for the lowest decrease in diversity (see Section 4.3 on the quality-diversity trade-off), comparing favorably to both finetuning and LoRA training.\n- **Binary Feedback System**\n  \n  The binary nature of feedback is to a large degree imposed by the binary nature of CFG. Still, scalar ratings can be approximated with minor modifications to FABRIC: One could choose different feedback strengths based on the rating, with the strongest rating receiving the highest feedback strength and the weakest rating having a feedback strength of 0. While we did not evaluate this extension, it is easy to implement on the application side. The main reason why this was excluded from the evaluation is because it goes against the original motivation of considering sparse binary feedback, in addition to making the experiment design more complicated and comparison to baselines more challenging. We added a comment to Section 6 in order to clarify this.\n- **Lack of Detailed Feedback**\n  \n  Even though we did not evaluate it experimentally, it is possible to provide a textual description in order to steer the feature extraction process. Namely, we use the null prompt for extracting attention features from the feedback images in our experiments. However, one may use an arbitrary prompt for each feedback image, describing for instance the specific aspects they like/dislike. This would significantly complicate the experiment design, as one would have to find a way to automatically generate textual descriptions of the feedback. In addition, it again goes against the aim of reducing the amount of prompt engineering required to achieve desired results by just shifting the prompting to a different stage of the process. We also note that attention masking is another simple extension that could be used to constrain the feedback to certain parts of the image (similar to inpainting with Stable Diffusion). We have added a clarifying comment about this to Section 6.\n\n---\n\n**References**\n\n[1] Deckers et al. 2023. [Manipulating Embeddings of Stable Diffusion Prompts](https://downloads.webis.de/publications/papers/deckers_2023b.pdf)"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1988/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700241287619,
                "cdate": 1700241287619,
                "tmdate": 1700241287619,
                "mdate": 1700241287619,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "J9uAQOZhMf",
            "forum": "zsfrzYWoOP",
            "replyto": "zsfrzYWoOP",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1988/Reviewer_vmZN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1988/Reviewer_vmZN"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a training-free method for text-to-image generation with iterative feedback, which is a novel and useful tool. The FABRIC framework is proposed and experiments are well-designed, showing the effectiveness of the method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper proposes a very interesting and practically meaningful topic.\n2. The method design is reasonable, which utilizes the power of self-attention in Stable Diffusion.\n3. Despite this is the first training-free iterative-feedback generation work, it designs interesting and sound experiments.\n4. The proposed method has great potential to optimize a lot of tasks based on Stable Diffusion."
                },
                "weaknesses": {
                    "value": "The weakness of the paper mainly lies in writing. It is better to incorporate more method descriptions, including model design and formulations in the main script instead of the appendix."
                },
                "questions": {
                    "value": "I'd like to accept this paper if the writing problem is addressed."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1988/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1988/Reviewer_vmZN",
                        "ICLR.cc/2024/Conference/Submission1988/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1988/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698496613426,
            "cdate": 1698496613426,
            "tmdate": 1700805724894,
            "mdate": 1700805724894,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "MJvNYhxInU",
                "forum": "zsfrzYWoOP",
                "replyto": "J9uAQOZhMf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1988/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1988/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the excellent summary of our contributions and for the concise, constructive feedback. We agree that certain sections of the writing could be improved and have uploaded a revised version of the paper. In particular, the methods section has been majorly overhauled, hopefully addressing your concerns. We refer to the general comment for a more detailed description of the changes that have been made."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1988/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700241021342,
                "cdate": 1700241021342,
                "tmdate": 1700241021342,
                "mdate": 1700241021342,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zwSLptQz75",
                "forum": "zsfrzYWoOP",
                "replyto": "MJvNYhxInU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1988/Reviewer_vmZN"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1988/Reviewer_vmZN"
                ],
                "content": {
                    "title": {
                        "value": "Response to the revision"
                    },
                    "comment": {
                        "value": "The authors provide a better method description in the revised version and addressed my concerns. I keep my original rating towards this paper."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1988/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700630871068,
                "cdate": 1700630871068,
                "tmdate": 1700630871068,
                "mdate": 1700630871068,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "qVfDeKlstk",
            "forum": "zsfrzYWoOP",
            "replyto": "zsfrzYWoOP",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1988/Reviewer_dJNm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1988/Reviewer_dJNm"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a novel method to control diffusion models to generate user-preferred images through iterative feedback. This method is based on augmenting the attention module. The proposed method is training-free and model-agnostic (as long as attention plays a core role in the image generation model), and can generate images based on any user preferences by having them provide positive and negative labels of their preference on images."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The proposed technique is model-free and training-agnostic, and is easily applicable to most attention-based image generation methods.\n\n- The proposed technique surpasses baselines and enable existing models to follow preferences reasonably\n\n- Extensive exploration of important parts of the proposed technique: the trade-off between diversity and quality, and the effects of adjusting feedback strength on PickScore."
                },
                "weaknesses": {
                    "value": "- **Limited technical novelty**: While the proposed method is effective in incorporating user feedback, the extension to enabling 'iterative feedback' is rather naive, and the feedback is constrained to binary labels (which the author(s) have acknowledged as a limitation). It would be more interesting to explore more advanced way of users' feedback across multiple rounds, and incorporating other modalities, such as text explanations beyond binary preferences.\n\n- **Lack of human rating in a paper focused on iterative human feedback**: While the author(s) have used reasonable proxy to evaluate the effectiveness of the model in following human preferences, it would strengthen the paper if the author(s) can include some form of user study, given this papers' focus is in incorporating human feedback in the image generation process.\n\n- **Missing discussion to some prior work**: I believe the proposed method has some technical similarity to prompt-based image editing methods, such as instruct-pix2pix [1] and prompt2prompt. [2] While the proposed method is different in the types of feedback and preference investigated, it would be great if the author(s) can systematically compare and survey related techniques that use attention map for feedback and/or image editing. I also have some doubts about whether it is reasonable to claim that the method \"outperformed\" supervised-learning baselines (HPS), see question below.\n\n*References:*\n\n[1] InstructPix2Pix: Learning to Follow Image Editing Instructions. Tim Brooks*, Aleksander Holynski*, Alexei A. Efros. CVPR 2023\n\n[2] Prompt-to-Prompt Image Editing with Cross Attention Control. Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, Daniel Cohen-Or. ICLR 2023."
                },
                "questions": {
                    "value": "- While the paper claims to outperform a supervised-learning baseline (HPS LoRA), it is unclear to me how does HPS relate to PickScore, as they both appear to measure human preference. Would the author(s) please clarify how might they relate to each other? As the models are evaluated on PickScore but LoRA-tuned on HPS.\n\n- How does the method relate to/differ from prompt2prompt and instruct-pix2pix? As stated above, it would be helpful to systematically compare them (and other related prior work) in a table."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1988/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1988/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1988/Reviewer_dJNm"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1988/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698897933202,
            "cdate": 1698897933202,
            "tmdate": 1700673378645,
            "mdate": 1700673378645,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0vfWUrki7t",
                "forum": "zsfrzYWoOP",
                "replyto": "qVfDeKlstk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1988/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1988/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the valuable feedback and suggestions to improve our work. In the following, we would like to address your concerns to the best of our ability and answer any open questions.\n- **Limited technical novelty**\n  \n  While we agree that the core conditioning mechanism isn\u2019t novel and has been proposed by the authors of ControlNet [1], the combination with weighted attention and CFG makes for a very versatile and flexible algorithm even beyond the scenarios that were evaluated experimentally. For example, real-valued ratings can be incorporated by varying the feedback strength according to the magnitude of the rating and using it as positive/negative feedback depending on the sign. Textual descriptions as well as masking of certain parts of the image can also be supported by varying the prompt used for extracting attention features or by masking out certain keys and values.\n  \n  However, both of these extensions go against the core motivation, which is to alleviate the need for prompt engineering by making the best possible use of sparse binary feedback. In addition, it would have further complicated experiment design, making the results harder to reproduce and more difficult for future work to compare against. This was somewhat unclear from the writing and we have added clarifying comments to Section 6.\n- **Lack of human rating in a paper focused on iterative human feedback**\n  \n  We agree with the reviewer that studying human interaction with the system would certainly be insightful. Doing a user study was considered, but we ultimately decided against it due to the challenges involved with the design and execution of such a study. To illustrate: A naive study design would simply let the user try out the system and subsequently have them rate their satisfaction and the perceived quality of generated images. This, however, leaves many variables uncontrolled. To name a few: How much of the perceived quality comes from the base model as opposed to the improvements made by FABRIC? How much does generation time (which is noticeably higher for FABRIC) impact user satisfaction and perceived quality? How much does the user interface influence the results? How big is the learning effect from using the system and how much skill is involved in achieving desirable results?\n  \n  As a consequence, if not executed properly, the results of such a study would amount to little more than anecdotal evidence (which we now have added to Section 4, Experiments) and might prove difficult to reproduce due to the number of variables that can impact the final rating. Considering all of this, rather than drawing conclusions from potentially weak evidence, we believe that it is appropriate to leave the empirical analysis of human interaction with FABRIC for future work.\n- **Missing discussion of prior work**\n  \n  These two papers, prompt2prompt and instruct-pix2pix, are indeed related to our work, as they use similar techniques but have different goals. We thank the reviewer for pointing this out and have added a paragraph on image editing to Section 5 (Related Work).\n- **Comparing HPS and PickScore**\n  \n  The reviewer is correct that HPS and PickScore are very similar and solve the same task, which is human preference estimation. In fact, in the early phases of the project, we were using HPS as the main evaluation metric but decided to replace it with PickScore when that was published since it demonstrated superior accuracy thanks to a larger training set (see the table below), which makes for a more accurate human proxy. We continue using the HPS LoRA as a baseline model explicitly trained to maximize human preference, which is ultimately also what PickScore measures. We note that FABRIC outperforms HPS LoRA whether measured by HPS or PickScore (early experiments used HPS).\n  | Method | #prompts | #choices | LoRA of SD available |\n  |---|---|---|---|\n  | HPS | 25k | 25k | yes |\n  | PickScore | 38k | 584k | no |\n- **Systematic comparison of related work**\n  \n  We like the idea of adding a systematic comparison of methods which incorporate human feedback in the generation process of diffusion models, even beyond just the techniques using attention-injection, but unfortunately we couldn\u2019t find space to include it in the paper. Instead, we\u2019ll just attach it here and possibly add it to the Appendix if it is considered a valuable addition. The table is provided in the next comment due to the character limit.\n\nFinally, we would like to ask for clarification why the reviewer, despite rating the soundness and presentation of the work as good and the contribution as excellent, ultimately deems the paper to be below the acceptance threshold. Are there significant flaws that prevent acceptance of the paper in its current state but that could be addressed or is there a more fundamental issue?\n\n---\n\n**References**\n\n[1] https://github.com/Mikubill/sd-webui-controlnet/discussions/1236"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1988/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700240918808,
                "cdate": 1700240918808,
                "tmdate": 1700240918808,
                "mdate": 1700240918808,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SPni5Do58d",
                "forum": "zsfrzYWoOP",
                "replyto": "0vfWUrki7t",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1988/Reviewer_dJNm"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1988/Reviewer_dJNm"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for responding to my review. I apologize for making an error in the Contribution rating, which is my fundamental issue with this paper (that relates to the overall limitation of technical novelty). I have modified my contribution score accordingly and my overall rating score stays the same."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1988/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700673357473,
                "cdate": 1700673357473,
                "tmdate": 1700673357473,
                "mdate": 1700673357473,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]