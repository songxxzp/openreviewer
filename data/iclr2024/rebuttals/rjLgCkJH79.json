[
    {
        "title": "A SIMILARITY-AGNOSTIC REINFORCEMENT LEARNING APPROACH FOR LEAD OPTIMIZATION"
    },
    {
        "review": {
            "id": "l7sN3lQiA6",
            "forum": "rjLgCkJH79",
            "replyto": "rjLgCkJH79",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8186/Reviewer_NNdZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8186/Reviewer_NNdZ"
            ],
            "content": {
                "summary": {
                    "value": "The paper tries to tackle a challenging problem in drug discovery, where it is common to optimize a lead compound to remove deficiencies and maintaining the favorable properties. \nThey highlight the challenges of using reinforcement learning based on similarity metrics to define certain constrains on the optimized compound, which potentially can introduce a bias in the generative process. \nTherefore, the authors propose a so call similarity agonistic reinforcement learning approach and remove the dependency on the similarity metric as additional constrain for optimization. This is achieved by goal-conditioned reinforcement learning."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "In my opinion the paper has the following strengths:\n\n-\tTo the best of my knowledge the idea of using complete molecules as goal (for goal conditioned reinforcement learning), as the authors propose it, is novel.\n-\tIn general, the method section is well explained with minor exceptions.\n-\tUsing reaction rules partially circumvents a general problem in generative models for drug discovery, namely a significant part of generated molecules are difficult to synthesize in the lab hindering a fast-pace early stage drug discovery program. The use of reaction rules conditions the generative model to generate more chemically plausible molecules with a direct synthesize path. \n-\tThe method seems to improve upon their baseline on all experiments."
                },
                "weaknesses": {
                    "value": "Lead optimization in drug discovery is an important and difficult task. I have difficulty accepting the method as an invention or improvement for lead optimization. For my understanding lead optimization is a much more complicated process than purely looking on QED score or a similarity score, which the authors didn\u2019t investigated. \n\nIn general, the paper would gain strength if the authors would compare their method against more recent methods in generative design and more properties other than QED. Especially, the baseline seems to be quite weak with all the efforts recently put into improving generative methods. \nFor example, the author could have a look at a standardized benchmark, e.g. [1]. \nThis would strength their method and would help to better showcase the potential\nimprovement compared to other methods. \nThe authors might also consider comparing their methods against other methods in the domain of scaffold hopping, e.g. [3].\n\nThe second contribution of their paper as stated on page 2, says:\n\u201cwe propose a search strategy\u2026\u201d\nCould the authors elaborate more on the search strategy? In case it is just generating thousands of molecules and sorting them based on a score, this seems to me not like a novel strategy.\n\nI very much like the idea of using reaction rules, although not completely novel, e.g. [2]. I think a more detailed description how exactly they mine the reaction rules and a better description of the reaction dataset in general would help the reader to better understand the topic. It doesn\u2019t have to be in the main text. \n\nI had trouble understanding the last paragraph of section 4.6., \u201cwe found that under the condition G(a_t leads to g)\u2026\u201d. Could the authors elaborate a little bit more on the issue they observed?\n\nTo summarize, although certain ideas are interesting and in some sense novel, I am hesitant to accept the paper mainly because of in my opinion a weak experiment section. The paper doesn\u2019t showcase a technique for lead optimization, which is much more complicated than what is investigated in the paper. Also, claims like: \n\u201cThough we do not explore comparison with multi-property optimization works in the scope of this work, the results shown induce confidence in our model to be able to generate lead candidates that satisfy multiple properties.\u201d Sec. 6, \nseem to be too strong for the experiments considered. \n\n\n[1] Wenhao Gao, Tianfan Fu, Jimeng Sun, and Connor W. Coley. Sample Efficiency Matters: A\nBenchmark for Practical Molecular Optimization, October 2022.\n\n[2] Tianfan Fu, Wenhao Gao, Connor W. Coley, Jimeng Sun, Reinforced Genetic Algorithm for Structure-based Drug Design, 2022. \n\n[3] Krzysztof Maziarz et al. LEARNING TO EXTEND MOLECULAR SCAFFOLDS WITH STRUCTURAL MOTIFS, 2022."
                },
                "questions": {
                    "value": "-\tDid I understand it correctly that the offline dataset just contains molecules randomly put together using the reaction rules, so potentially not chemically plausible at all? \n-\tMy understanding of actor-critic reinforcement learning is to use the output of the critic for the loss of the actor. From eq. (1) and (2) this seems not the be the case, could the authors elaborate a little bit?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8186/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698392709253,
            "cdate": 1698392709253,
            "tmdate": 1699637015320,
            "mdate": 1699637015320,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "hCZ8Tdk5zs",
                "forum": "rjLgCkJH79",
                "replyto": "l7sN3lQiA6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8186/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8186/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the review. To address common concerns by the reviewers, we have added a Common Response (https://openreview.net/forum?id=rjLgCkJH79&noteId=AeabWUzjwD). We respond to the reviewer's comments below along with reference to the Common Response where needed.\n\n\n> \u201cLead optimization in drug discovery is an important and difficult task. I have difficulty accepting the method as an invention or improvement for lead optimization. For my understanding lead optimization is a much more complicated process than purely looking on QED score or a similarity score, which the authors didn\u2019t investigated.\u201d\n\nWe refer the reviewer to Common Response (1).\n\nAs mentioned, our method is a generalized approach for generating alternate candidates to the lead molecule. Therefore, we have not explored the subsequent step of docking or binding affinity in the context of our work.\n\n  \n\n> \u201cIn general, the paper would gain strength if the authors would compare their method against more recent methods in generative design and more properties other than QED.\u201c\n\nWe refer the reviewer to Common Responses (2) and (3). Results for the added baseline and metrics can be found in the Results Table in the Common Response.\n\n  \n\n> \u201cThe second contribution of their paper as stated on page 2, says: \u201cwe propose a search strategy\u2026\u201d Could the authors elaborate more on the search strategy? In case it is just generating thousands of molecules and sorting them based on a score, this seems to me not like a novel strategy.\u201d\n\nThe reviewer is referring to the following statement in the paper:\n\n\u201cWe propose a search strategy that separates the property optimization from the training and offloads it as a post-curation process, thereby simplifying the task of learning.\u201d\n\n\nWe acknowledge the comment made by the reviewer. The novelty comes from the idea of using the search strategy to offload the property optimization from the training procedure due to the problems associated with multi-objective optimization.\n\nTo explain further, we take the example of the reward function used in MolDQN [1] for multi-objective optimization:\n\n$r_t = w \\times SIM(m_t, m_0) + (1-\\omega) \\times QED(m_t)$\n\nThe optimal value of \u2018w\u2019 in this multi-objective case is hard to determine quantitatively during hyperparameter tuning. In fact, the notion of optimality may be task-dependent and may very likely not be well understood for most tasks. If the user is not satisfied with the generated molecules, a new model would need to be trained with a different \u2018w\u2019. As opposed to this, in our work, the model would not need to be re-trained. Only the much cheaper search strategy needs to be employed to generate a large number of molecules that can be filtered to retain desired properties. Other multi-objective optimization challenges, such as Pareto dominance, conflicting objectives, etc are also avoided in our method.\n\n  \n\n>\u201cI very much like the idea of using reaction rules, although not completely novel, e.g. [2]. I think a more detailed description how exactly they mine the reaction rules and a better description of the reaction dataset in general would help the reader to better understand the topic. It doesn\u2019t have to be in the main text.\u201d\n\nWe thank the reviewer for the acknowledgment and suggestion. We have added the details to the supplementary material.\n\n  \n\n>\u201cI had trouble understanding the last paragraph of section 4.6., \u201cwe found that under the condition G(a_t leads to g)\u2026\u201d. Could the authors elaborate a little bit more on the issue they observed?\u201d\n\nThank you for the question. We had noticed that based on different values of the negative rewards that we chose, the policy would either converge or diverge.\n\n  \n\nTo recap,\n\n-   In our method, we select a fixed number L of negative return trajectories per high return trajectory.\n    \n-   The policy gradient loss moves the function away from low-return areas and closer to high-return areas.\n    \n\nTherefore, if the magnitude of the negative reward were high, the cumulative magnitude of gradient from the negative samples in a batch would overwhelm the magnitude of positive samples, causing it to diverge. The condition we mentioned works as a normalizing factor for the negative component of the loss and was essential for reward design in our case to ensure convergence."
                    },
                    "title": {
                        "value": "Response to Reviewer NNdZ [1/2]"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8186/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700378504957,
                "cdate": 1700378504957,
                "tmdate": 1700570446162,
                "mdate": 1700570446162,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mrhJH4VcbZ",
                "forum": "rjLgCkJH79",
                "replyto": "l7sN3lQiA6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8186/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8186/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Questions:**\n\n>Q1. Did I understand it correctly that the offline dataset just contains molecules randomly put together using the reaction rules, so potentially not chemically plausible at all?\n\nWe are unsure if we understood the question correctly. The trajectories in the offline dataset are indeed put together using a random policy **given the reaction rules**. This still makes them chemically feasible though.\n\nWe elaborate for further clarification: Say we are given a start molecule. To generate a trajectory for the offline dataset, first, a reaction rule will be selected randomly among those that are applicable to it. This will result in a product which is chemically feasible. even though it is the result of a random sampling. By repeating this step multiple times, we generate a trajectory for our dataset using a random policy where each molecule is chemically feasible.\n\n  \n\n>Q2: My understanding of actor-critic reinforcement learning is to use the output of the critic for the loss of the actor. From eq. (1) and (2) this seems not the be the case, could the authors elaborate a little bit?\n\nThe reviewer has noticed correctly. Due to our unique setup, we were able to use calculated returns to update the actor, thus not requiring the higher variance estimation by the critic.\n\n  \n\nThe critic is still a crucial part of our method. Firstly, it provides additional gradients to update the embedding module, resulting in higher-quality representations. Secondly, it is used in the generation procedure to sort the actions suggested by the policy."
                    },
                    "title": {
                        "value": "Response to Reviewer NNdZ [2/2]"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8186/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700378550651,
                "cdate": 1700378550651,
                "tmdate": 1700378768158,
                "mdate": 1700378768158,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cKTcMH0GBo",
                "forum": "rjLgCkJH79",
                "replyto": "mrhJH4VcbZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8186/Reviewer_NNdZ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8186/Reviewer_NNdZ"
                ],
                "content": {
                    "comment": {
                        "value": "I very much appreciate the effort the authors put into the rebuttal and thank you for answering my questions. \n\nAlthough, I think certain parts of the approach are very interesting, for example, the large action space, the experimental section is still not convincing to me. This is among other things connected to the metrics considered. Almost all methods considered have a validity, uniqueness, and novelty score of 1. The QED and similarity score alone are not convincing. \n\nI strongly recommend the authors to consider other benchmark systems. As a suggestion maybe they can apply their method to scaffold hopping or test on standard benchmark systems like a Dopamine Type 2 Receptor Case study (c.f. [1]).\nTherefore, I will leave my score as is.  \n\n[1] \"Augmented Memory: Capitalizing on Experience Replay to Accelerate De Novo Molecular Design\", Jeff Guo and Philippe Schwaller, (2023)."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8186/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700644106052,
                "cdate": 1700644106052,
                "tmdate": 1700644106052,
                "mdate": 1700644106052,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "gqylBC5dci",
            "forum": "rjLgCkJH79",
            "replyto": "rjLgCkJH79",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8186/Reviewer_PmbC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8186/Reviewer_PmbC"
            ],
            "content": {
                "summary": {
                    "value": "This work presents LOGRL, a unique approach to lead optimization using a goal-conditioned reinforcement learning framework. Given an expert dataset, this work trains a goal-conditioned policy with binary reward shaping, treating reaction rules as actions. Then, LOGRL compares Tanimoto similarity and QED of generated molecules with two baselines using an online RL method, which is PPO."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "- The paper is well-written and presents clearly.\n- The paper demonstrates comprehensive related work."
                },
                "weaknesses": {
                    "value": "The experimental comparison in this paper raises some concerns regarding fairness and appropriateness. The authors compare their proposed off-line Reinforcement Learning (RL) policy with on-line RL baselines. This comparison between on-line and off-line RL algorithms seems somewhat unconventional. Moreover, it's unclear whether the on-line RL baselines, such as the S model and Q+S model, employ an expert dataset similar to LOGRL. If they do not utilize expert data, this could introduce an unfair advantage to LOGRL, as it relies on additional expert data. It would be beneficial to see how LOGRL performs when compared to baselines that also use the same expert dataset.\n\nAdditionally, I suggest exploring the possibility of supervised learning in this context. The authors assume access to a substantial amount of expert dataset containing high-reward samples. In such a scenario, imitation learning often outperforms offline RL. It would be valuable to understand why the authors chose offline RL over supervised learning, given the abundance of expert data.\n\nThe paper employs policy gradient, which typically assumes that the training policy and the behavior policy are aligned, making it an on-policy approach. The suitability of using a policy gradient in an offline RL setup is a point of concern. It would be helpful to see more discussion and justification regarding the use of an on-policy algorithm like policy gradient in this context.\n\nFinally, it would be interesting to know if the proposed method is capable of generating diverse outputs. One potential concern is whether the method might collapse and generate a single output, as there doesn't appear to be a regularizer that can control all possible outputs directed toward the target molecule. Exploring the diversity of outputs and addressing this potential issue would strengthen the paper.\nOverall, while the paper presents a promising approach, addressing these concerns and providing more clarity would enhance the quality of the work and its relevance in the field of machine learning and RL.\n\n---\n\nminor\n\nTypo in Section 4.5 line3: in the training batch, batchwe"
                },
                "questions": {
                    "value": "See Weakness section"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8186/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8186/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8186/Reviewer_PmbC"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8186/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698731135586,
            "cdate": 1698731135586,
            "tmdate": 1699637015188,
            "mdate": 1699637015188,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "w75tKT1634",
                "forum": "rjLgCkJH79",
                "replyto": "gqylBC5dci",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8186/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8186/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the review. To address common concerns by the reviewers, we have added a Common Response (https://openreview.net/forum?id=rjLgCkJH79&noteId=AeabWUzjwD). We respond to the reviewer's comments below along with reference to the Common Response where needed.\n\n> \u201cThe authors compare their proposed off-line Reinforcement Learning (RL) policy with on-line RL baselines. This comparison between on-line and off-line RL algorithms seems somewhat unconventional.\u201d\n\nWe agree with the reviewer that the comparison of offline RL with online RL baseline might be unconventional. However, we would like to clarify that our intended comparison is not between online and offline RL methods but instead between our binary reward method and prior distance-based reward methods (that use molecular similarity as rewards). Due to binary rewards leading to severe sparsity in our problem setup, we have adopted the use of binary rewards in an offline setting.\n\nWe have added another baseline for Random Search and refer the reviewer to the Results Table in our Common Response.\n\n>\u201cMoreover, it's unclear whether the on-line RL baselines, such as the S model and Q+S model, employ an expert dataset similar to LOGRL. If they do not utilize expert data, this could introduce an unfair advantage to LOGRL, as it relies on additional expert data.\u201d\n\nThe baselines are completely online and, hence do not employ any expert data. Since the offline dataset was not collected using an expert policy or a human, we disagree with the reviewer to call it \u201cexpert data\u201d. The data available to the offline algorithm is a result of running a random policy in the same underlying environment as the online RL baseline and relabelling the rewards. The procedure is very trivial and should not provide any extra information to the offline RL algorithm.\n\n>\u201cAdditionally, I suggest exploring the possibility of supervised learning in this context. The authors assume access to a substantial amount of expert dataset containing high-reward samples. In such a scenario, imitation learning often outperforms offline RL.\u201d\n\nWe thank the reviewer for the suggestion. We had explored behavior cloning(BC) during our early experiments and found offline RL to significantly outperform BC. Results from those experiments are given below. We evaluated BC vs our model on trajectories of different lengths (steps) - 1, 2 and 5 and calculated the top-k accuracy of the model. Top-k accuracy here indicates the percentage of test samples for which the model ranked the positive action within the top k of its predictions. The top k predictions are the actions suggested in a single step of the generation procedure described in Algorithm 2 in the manuscript. The results for those experiments are given below.\n\n\n| Steps | Model | top-10 | top-5 | top-1 |\n|:-----:|:-----:|:------:|:-----:|:-----:|\n|   1   |   BC  |  96.05 | 89.48 | 73.13 |\n|       | LOGRL |   100  |  100  |  100  |\n|       |       |        |       |       |\n|   2   |   BC  |  35.51 | 27.45 |   13  |\n|       | LOGRL |  96.1  | 95.62 | 95.22 |\n|       |       |        |       |       |\n|   5   |   BC  |  32.42 | 23.92 |  9.37 |\n|       | LOGRL |  84.13 | 84.07 | 84.02 |\n\n> \u201cFinally, it would be interesting to know if the proposed method is capable of generating diverse outputs. One potential concern is whether the method might collapse and generate a single output, as there doesn't appear to be a regularizer that can control all possible outputs directed toward the target molecule. Exploring the diversity of outputs and addressing this potential issue would strengthen the paper.\u201c\n\nWe thank the reviewer for pointing it out. We have added an evaluation of the uniqueness of generation along with metrics for validity and novelty. Uniqueness is a measure of how unique are the molecules generated; 1 indicates all molecules are unique (or perfect diversity) and 0 indices all the generated molecules are the same (no diversity). All models, including our baselines, achieve near-perfect uniqueness scores. However, a major contribution to this metric would be a result of our design of the problem; due to the incredibly large search space and the generation procedure promoting uniqueness. That said, there do exist multiple paths for generating the same molecule, therefore the models are also effective in not reproducing the same paths repeatedly. We refer the reviewer to the Results Table in the Common Response for the metric scores.\n\n  \n\n> \u201cTypo in Section 4.5 line3: in the training batch, batchwe\u201d\n\nThank you for pointing it out. We have corrected it."
                    },
                    "title": {
                        "value": "Response to Reviewer PmbC"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8186/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700376456405,
                "cdate": 1700376456405,
                "tmdate": 1700378681047,
                "mdate": 1700378681047,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "JgKHlKCR15",
            "forum": "rjLgCkJH79",
            "replyto": "rjLgCkJH79",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8186/Reviewer_TrZv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8186/Reviewer_TrZv"
            ],
            "content": {
                "summary": {
                    "value": "In this study, a new lead optimization method, LOGRL, is proposed. This method uses offline reinforcement learning to train the model how to optimize molecular structures to get closer to the target structures (goal-conditioned reinforcement learning). Furthermore, a set of reactions is used to ensure the synthetic accessibility of the generated structures. The beam search algorithm is used, which helps in obtaining a diverse set of modified structures that meet desired properties. LOGRL is compared against two RL baselines and achieves promising results in optimizing molecules towards the target structures, both in terms of similarity and drug-likeness defined by QED."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The method is presented in a very clear way. The background section provides all the basics that are required to understand the method.\n- Offline reinforcement learning is used to avoid sparse rewards when navigating the vast chemical space.\n- The goal-conditioned reinforcement learning is used to guide the generative process, which in my opinion is the main novelty of the paper. This way, similarity measures are no longer needed to train the model.\n- Reaction rules extracted from the USPTO-MIT dataset are used to ensure the synthesizability of the generated molecules, which is important for proposing high-quality results."
                },
                "weaknesses": {
                    "value": "- The significance of the work is not clear. The method is trained to optimize molecules towards the target structures, but I am unsure if I understand how this model could be used in practice. Usually, the goal of lead optimization is to improve a set of molecular properties without impacting binding affinity. In the presented setup, the optimization changes the structure of lead candidates to make them more similar to known drugs, which oftentimes is undesired because only novel structures can remain outside the patented chemical space.\n- The experimental section seems very preliminary. Only two RL baselines were trained, and there is no comparison with other state-of-the-art methods in molecular optimization. The evaluation metrics used in the experiments are very simple and do not show if the proposed method can optimize any molecular properties or at least retain high binding affinity. The Authors claim that their search strategy separates property optimization from training, but the results of the optimization are not presented. Additionally, all methods were run only once (if I understand correctly), and the results can be hugely impacted by random initialization, especially for online RL methods like the baselines. I would strongly suggest running these methods multiple times and providing confidence intervals for the evaluation metrics.\n- (minor) I think the Authors could consider comparing their approach to the simpler, yet conceptually similar, Molpher model [1]. In Molpher, a trajectory between two molecules is found by an extensive search (not RL-based) of possible reaction-based structure mutations. The motivation of that paper is also different, Molpher was proposed for effective chemical space exploration.\n\n[1] Hoksza, David, et al. \"Molpher: a software framework for systematic chemical space exploration.\" Journal of cheminformatics 6.1 (2014): 1-13."
                },
                "questions": {
                    "value": "1. What is the success rate of molecular optimization using LOGRL? Can you find many well-optimized molecules in the post-training filtering step, or do you think additional RL objectives could improve these properties significantly?\n2. What are the real-life applications of this optimization algorithm? Can it be used for other optimization problems besides lead optimization (see the problems I mentioned in the \"Weaknesses\" section)?\n3. In Section 3.2, two GCRL methods are described. Did you try the other method and if so, could you provide the comparison results?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8186/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698842484808,
            "cdate": 1698842484808,
            "tmdate": 1699637015067,
            "mdate": 1699637015067,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "a0EFegYKqm",
                "forum": "rjLgCkJH79",
                "replyto": "JgKHlKCR15",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8186/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8186/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the review. To address common concerns by the reviewers, we have added a Common Response (https://openreview.net/forum?id=rjLgCkJH79&noteId=AeabWUzjwD). We respond to the reviewer's comments below along with reference to the Common Response where needed.\n\n> \"The significance of the work is not clear.  The method is trained to optimize molecules towards the target structures, but I am unsure if I understand how this model could be used in practice. Usually, the goal of lead optimization is to improve a set of molecular properties without impacting binding affinity.\"\n\nWe refer the reviewer to Common Response (1).  \nHaving not considered a target receptor, we did not discuss binding affinity. We thank the reviewer for pointing it out. We acknowledge that it would be an important step after our method for further curation.\n\n> \"Only two RL baselines were trained, and there is no comparison with other state-of-the-art methods in molecular optimization.\"\n\nWe refer the reviewer to Common Response (2).\n\n> \"The evaluation metrics used in the experiments are very simple.\"\n\nWe refer the reviewer to Common Response (3).\n\nA table with additional baseline and metrics is present in the \"Results Table\" section of Common Response.\n\n**Questions:**\n\n> Q1 (A): What is the success rate of molecular optimization using LOGRL? \n\nWithout docking, the notion of success rate depends on the user. For instance, for our chosen examples of trypsin inhibitors, among 10000 molecules generated by LOGRL, 73 had QED > 0.7. \n\n> Q1 (B): Can you find many well-optimized molecules in the post-training filtering step?\n\nIt is possible to find many optimized molecules in the post-training filtering step. Towards this effect, we can simply increase the breadth or depth of the search, which results in a larger number of molecules. \n\n> Q1 (C): Do you think additional RL objectives could improve these properties significantly?\n\nAdditional RL objectives would surely improve the properties of generated molecules. Unfortunately, this has several drawbacks, some are mentioned in our paper. Firstly, for each combination of properties, there would need to be a separate model trained. Secondly, problems related to multi-objective optimization: additional cost of training due to extra hyperparameters, Pareto dominance, conflicting objectives, etc. \n\nOur method avoids these problems by separating molecular search and property search into two different steps. \n\n> Q2: What are the real-life applications of this optimization algorithm? Can it be used for other optimization problems besides lead optimization (see the problems I mentioned in the \"Weaknesses\" section)?\n\nThe proposed algorithm can be used in offline RL settings with a large discrete action space, such as recommender systems and language generation.\n\n> Q3: In Section 3.2, two GCRL methods are described. Did you try the other method and if so, could you provide the comparison results?\n\nThe two methods described are binary rewards and distance-based rewards. Our proposed method uses binary rewards, while the online RL baselines use distance-based rewards. To our understanding, the comparison requested is present in the paper.\n\nWe would also like to clarify that similarity-based rewards are a type of distance-based reward."
                    },
                    "title": {
                        "value": "Response to Reviewer TrZv"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8186/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700375364883,
                "cdate": 1700375364883,
                "tmdate": 1700378656236,
                "mdate": 1700378656236,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]