[
    {
        "title": "ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving"
    },
    {
        "review": {
            "id": "7VHE2WFeeP",
            "forum": "Ep0TtjVoap",
            "replyto": "Ep0TtjVoap",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission263/Reviewer_Gq26"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission263/Reviewer_Gq26"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents ToRA, a method of utilising interleaved reasoning and tool-use traces to improve language models' capability in mathematics. The authors used GPT-4 to get a collection of interactive tool-use trajectories for mathematical problem solving, and fine-tuned open-sourced language models on this collection to improve their performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The performance gains on the downstream mathematical reasoning benchmarks is impressive.\n- The ablation studies convincingly proved the necessity of both the rationale and the program parts of the ToRA pipeline."
                },
                "weaknesses": {
                    "value": "- Adopting complicated mathematical fonts when unnecessary only takes from the readability of the paper. There is no need to use them when the default fonts suffice."
                },
                "questions": {
                    "value": "How should we improve upon ToRA? Since the pipeline relies on a proprietary model to generate the dataset, and the final performance of the strongest ToRA is still inferior to GPT-4, how can we further improve?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission263/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697908727211,
            "cdate": 1697908727211,
            "tmdate": 1699635951971,
            "mdate": 1699635951971,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7B8BBQMoS6",
                "forum": "Ep0TtjVoap",
                "replyto": "7VHE2WFeeP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission263/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission263/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Official Review by Reviewer Gq26"
                    },
                    "comment": {
                        "value": "Dear reviewer Gq26,\n\nWe appreciate your thorough review and insightful comments on our paper. We are pleased that the you recognizes our efforts and the promising results of the ToRA pipeline. We will address the concerns raised and answer the questions posed to improve our work further.\n\n### **Enhancing Readability with Appropriate Mathematical Fonts**\n\n> Adopting complicated mathematical fonts when unnecessary only takes from the readability of the paper. There is no need to use them when the default fonts suffice.\n> \n\nWe appreciate your feedback! We understand the reviewer's concern about the complexity of mathematical fonts making the paper less readable. Our intention was to maintain the mathematical accuracy and precision, e.g., our choice to use $\\wp$ for \"prompt\" and $p$ for \"program\" was driven by a desire to avoid confusion, but we understand that this might have inadvertently complicated the presentation. To enhance readability, we will revise these symbols to more commonly used and easily comprehensible ones without losing the distinction between the two terms.\n\n### **Suggestions for Further Improvements on Math Reasoning**\n\n> How should we improve upon ToRA? Since the pipeline relies on a proprietary model to generate the dataset, and the final performance of the strongest ToRA is still inferior to GPT-4\n> \n\nThank you for posing an insightful question about improving upon ToRA! One point that needs clarification is that the core method of ToRA is tool-integrated reasoning learning, and the data annotation in this process is **model-independent** for collecting tool-integrated reasoning trajectories. It can also be obtained through other open-source models or even manually annotated for higher-quality data.\n\nMoreover, our experiments substantiated the effectiveness of this tool-integrated reasoning format on GPT-4, demonstrating its potential to further augment GPT-4's performance. As illustrated in Fig. 4 (right), the implementation of tool-integrated reasoning can yield a 9.8% improvement compared to PAL.\n\nLooking forward, we see several main directions to further enhance mathematical reasoning based on ToRA:\n\n- **Continue Pre-training**: We found in subsequent experiments that applying ToRA to models continue pre-trained on math and code data brings a more dramatic improvement! This suggests that pre-training and data engineering might deserve more attention for the research community!\n- **SFT Mixture Scale-up**: We could explore amplifying the quality and coverage of fine-tuning data in tool-integrated reasoning format, which might further improve generalization and robustness.\n- **Reward Modeling**: We could explore methods to enhance tool-integrated reasoning by incorporating reward modeling methods like [1], to further improve accuracy, intention alignment, interpretability, and to make it more user-friendly.\n\nWe extend our gratitude for your constructive feedback and the opportunity to clarify the nuances of our work. We believe that the proposed changes will significantly improve our paper's readability and impact.\n\n[1] Lightman, Hunter, et al. \"Let's Verify Step by Step.\" arXiv preprint arXiv:2305.20050 (2023)."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission263/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700213994073,
                "cdate": 1700213994073,
                "tmdate": 1700213994073,
                "mdate": 1700213994073,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nr55Z38lAb",
                "forum": "Ep0TtjVoap",
                "replyto": "7B8BBQMoS6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission263/Reviewer_Gq26"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission263/Reviewer_Gq26"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your reply! The suggested improvements are very promising. I look forward to seeing them.\n\nI shall retain my rating."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission263/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700392317476,
                "cdate": 1700392317476,
                "tmdate": 1700392317476,
                "mdate": 1700392317476,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "dNfGQirWBM",
            "forum": "Ep0TtjVoap",
            "replyto": "Ep0TtjVoap",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission263/Reviewer_U6Sz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission263/Reviewer_U6Sz"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces TORA,\n a series of Tool-integrated Reasoning Agents designed to enhance mathematical problem-solving by combining natural language reasoning with external tools. TORA models are trained using interactive tool-use trajectories, employing imitation learning and output space shaping techniques. Experimental results demonstrate that TORA outperforms open-source models on various mathematical reasoning datasets."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper is easy to follow\n- TORA achieves good performance on math datasets"
                },
                "weaknesses": {
                    "value": "- **Limited of technical novelty**: \n  - Using imitation learning to improve the mathematical reasoning ability of open-source models has been proposed in many recent works, e.g.,\n    - Scaling relationship on learning mathematical reasoning with large language models, https://arxiv.org/abs/2308.01825\n    - WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct, https://arxiv.org/abs/2308.09583\n    - MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models, https://arxiv.org/abs/2309.12284\n    - MAmmoTH: Building Math Generalist Models through Hybrid Instruction Tuning, https://arxiv.org/abs/2309.05653\n  - At the Output Space Shaping step, the authors use nucleus sampling to generate more reasoning paths and pick the corrected paths. This is a technique widely used in the works listed above. The only difference is that this paper fixes some of the preceding portions of wrong trajectories, while existing works resample the whole trajectory. However, validating which portions of trajectories are correct is very challenging, and the authors need to **enumerate** possible preceding portions of wrong trajectories, which is time-consuming. In my opinion, the existing method (i.e., re-sampling and picking the correct paths) is simpler and may be more effective.\n\n- **Concerns on reproducibility**: As training data are not provided, reviewers and readers can't check the reproducibility of this paper. Note that existing works like WizardMath, MetaMath, and MAmmoTH have released their training data for the community to reproduce their results. Moreover, checkpoints of TORA are not provided in the appendix for checking reproducibility.\n\n\n\n- \"TORA outperforms WizardMath by around 45% in Algebra and Number Theory, **which is attributed to stimulating and shaping tool-use behavior**.\" From Table 3, we cannot conclude that the better performance of TORA is due to stimulating and shaping tool-use behavior, as \nWizardMath uses augmented data **from LLaMA**, while TORA uses data generated **from GPT-4**. Note that GPT-4 is much more powerful than LLaMA. \n\n- In section 2.2, greedy decoding is used for generating trajectories from GPT-4. Thus, only one path per question can be obtained in TORA-CORPUS dataset. In my opinion, the accuracy of LLaMA trained on TORA-CORPUS has yet to saturate (e.g., plot the accuracy w.r.t. #samples of TORA-CORPUS).  To generate more trajectories, a simple approach (which is widely used in the above works) is to use temperature sampling (rather than greedy decoding)  and pick the correct ones for training.\n\n- As temperature sampling can generate more samples from GPT-4, TORA-CORPUS can be more diverse (compared with greedy decoding). To verify the effectiveness of Output Space Shaping in Section 3.5.2, it is better to **augment more data from GPT-4 and let the accuracy of LLaMA trained on the TORA-CORPUS saturates first**. Otherwise, it is difficult to say whether the improvement of LLaMA is from more training data or the proposed Output Space Shaping.\n\n- Ablation study of hyperparameter $n$ (maximum rounds). in experiments, $n=3$ is used. Question: is the performance of TORA sensitive to $n$?\n\n- writing:\n  - \"forward and backward reasoning, as well as result verification\": references for forward/backward reasoning, verification\n  - \"Addressing these challenges requires complex symbolic reasoning over algebraic expressions\": references for symbolic reasoning \n  - where is the definition of $\\theta$ in (4)?"
                },
                "questions": {
                    "value": "- In the Conclusion section, the authors mention that \"our systematic analysis ... paving the way for the development of more advanced and versatile reasoning agents\". **How and why does this analysis pave the way?** \n- What is the difference between Tool-Integrated Reasoning (Algorithm 1) and existing methods (e.g., PAL (Gao et al., 2022), PoT (Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks, https://arxiv.org/abs/2211.12588))?\n- \"For numerical values, we perform **rounding**, while for expressions, we employ **sympy** for parsing.\" Are these two techniques used in the baseline methods? \n\n- Are there any empirical results can support the ``Output Space Shaping improves **diversity**''. Furthermore, the diversity measure is not defined in the paper."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission263/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission263/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission263/Reviewer_U6Sz"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission263/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698643695695,
            "cdate": 1698643695695,
            "tmdate": 1699692615585,
            "mdate": 1699692615585,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "HthS6zaOUr",
                "forum": "Ep0TtjVoap",
                "replyto": "dNfGQirWBM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission263/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission263/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Official Review by Reviewer U6Sz (1/3)"
                    },
                    "comment": {
                        "value": "Dear Reviewer U6Sz,\n\nThank you for your time and thoughtful feedback on ToRA! We have addressed each of your points below.\n\n### **Technical Novelty**\n\n> Using imitation learning to improve the mathematical reasoning ability of open-source models has been proposed in many recent works, e.g.,\n> \n> - Scaling relationship on learning mathematical reasoning with large language models,\u00a0https://arxiv.org/abs/2308.01825\n> - WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct,\u00a0https://arxiv.org/abs/2308.09583\n> - MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models,\u00a0https://arxiv.org/abs/2309.12284\n> - MAmmoTH: Building Math Generalist Models through Hybrid Instruction Tuning,\u00a0https://arxiv.org/abs/2309.05653\n\nWe appreciate your reference to concurrent works. While both ToRA and these studies employ imitation learning, the Tool-integrated Reasoning format proposed to train ToRA fundamentally differs from the CoT or PoT formats used in these works, yielding significantly better results.\n\nSpecifically, references [1-3] primarily adopt natural language rationale (CoT), focusing on augmenting CoT solutions [1] and questions [2, 3] for improvements. On the other hand, reference [4] concentrates on hybrid training using CoT and PoT.\n\nIn contrast, ToRA introduces the Tool-integrated reasoning format, which brings clear advantages. For instance, even the smallest ToRA-Code-7B outperforms MetaMath-70B [3] (44.6% vs. 26.0%) and MAmmoTH-Coder-34B [4] (44.6% vs. 43.6%).\n\n### **Output Space Shaping**\n\n> At the Output Space Shaping step, the authors use nucleus sampling to generate more reasoning paths and pick the corrected paths. This is a technique widely used in the works listed above. The only difference is that this paper fixes some of the preceding portions of wrong trajectories, while existing works resample the whole trajectory. However, validating which portions of trajectories are correct is very challenging, and the authors need to\u00a0enumerate\u00a0possible preceding portions of wrong trajectories, which is time-consuming.\n> \n\nWe proposed the method of **learning from small model errors corrected by a teacher model**, which we call **teacher correction**, and we combined it with sampling for output space shaping. \n\nThe step-level enumeration for teacher correction is a one-time cost for constructing training data. In our experiments, compared with using sampling alone for output space shaping, introducting teacher correction does not increase training cost because we maintain the same volume of training data.\n\n> In my opinion, the existing method (i.e., re-sampling and picking the correct paths) is simpler and may be more effective.\n> \n\nOur research shows that integrating teacher correction contributes to higher efficacy than merely using sampling. As illustrated in Figure 5, employing the sampling strategy alone resulted in a 2.7% performance enhancement. Conversely, the incorporation of the correction strategy led to a performance improvement ranging from 3.4% to 4.0% across two datasets. This improvement was achieved while maintaining the same volume of training data as with sampling alone, thereby substantiating the effectiveness of our output space shaping strategies.\n\n### **Reproducibility**\n\n> Concerns on reproducibility: As training data are not provided, reviewers and readers can't check the reproducibility of this paper. Note that existing works like WizardMath, MetaMath, and MAmmoTH have released their training data for the community to reproduce their results. Moreover, checkpoints of TORA are not provided in the appendix for checking reproducibility.\n> \n\nWe understand your concerns about reproducibility. We're currently undergoing an internal review to open-source the ToRA-Corpus and will add the open-source links to code, model checkpoints, etc., after the anonymous review period."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission263/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700243122191,
                "cdate": 1700243122191,
                "tmdate": 1700243583021,
                "mdate": 1700243583021,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "L8LHpN3P2q",
                "forum": "Ep0TtjVoap",
                "replyto": "dNfGQirWBM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission263/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission263/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Official Review by Reviewer U6Sz (3/3)"
                    },
                    "comment": {
                        "value": "### **Setting Maximum Rounds of Interaction**\n\n> Ablation study of hyperparameter $n$ (maximum rounds). in experiments, $n=3$ is used. Question: is the performance of TORA sensitive to $n$?\n> \n\nOur preliminary experiments found that on mathematical reasoning benchmarks like GSM8k and MATH, large language models nearly always solve problems with n \u2264 3 interactions, so we set it as the default hyperparameter. Setting a higher number of interactions does not bring significant gains.\n\n### **Writing**\n\n> - \"forward and backward reasoning, as well as result verification\": references for forward/backward reasoning, verification\"\n> - Addressing these challenges requires complex symbolic reasoning over algebraic expressions\": references for symbolic reasoning\n> - where is the definition of $\\theta$ in (4)?\n\nWe appreciate your meticulous attention to details and pointing out writing typos. We will add all the references you mentioned and the definition of $\\theta$ in equation (4) in future updates.\n\n### **Questions**\n\n> In the Conclusion section, the authors mention that \"our systematic analysis ... paving the way for the development of more advanced and versatile reasoning agents\". How and why does this analysis pave the way?\n> \n\nWe appreciate your question! As an initial exploration of constructing math reasoning agents based on tool-integrated reasoning, we demonstrated the effectiveness of combining natural language with programming language to solve challenging reasoning problems. We believe this approach has potential and is worth further exploration. Additionally, in Section 3.6, we have conducted an in-depth analysis of the areas that still require enhancement and merit additional attention. We believe that this systematic analysis and identification of areas for improvement lays the groundwork for the development of more advanced and versatile reasoning agents.\n\n> What is the difference between Tool-Integrated Reasoning (Algorithm 1) and existing methods (e.g., PAL, PoT)?\n> \n\nAs shown in Fig. 2, PAL and PoT are Program-based methods to solve tasks with program synthesis, while our proposed Tool-integrated Reasoning format integrates natural language reasoning with program-based tool use, in order to combine the benefits of both worlds.\n\nAs shown in Fig 4, the proposed Tool-Integrated Reasoning consistently surpasses Rationale-only and Program-only approaches. Remarkably, Using LLaMA-2, Tool-Integrated Reasoning achieves substantial improvements of 29.0% and 6.7% over Rationale-only and Program-only, respectively. With the closed-source GPT-4, the improvements are 19.1% and 9.8%, respectively. This emphasizes the effectiveness of integrating natural language rationales with programs.\n\n> \"For numerical values, we perform rounding, while for expressions, we employ sympy for parsing.\" Are these two techniques used in the baseline methods?\n> \n\nSure! For a fair comparison, we adopted the same evaluation for all methods.\n\n> Are there any empirical results can support the ``Output Space Shaping improves diversity''. Furthermore, the diversity measure is not defined in the paper.\n> \n\nThanks for your question! We constructed many trajectories for each problem through sampling and teacher correction, where the reasoning processes are different but the answers are correct. In other words, improving diversity means increasing the number of different solution paths for the same problem.\n\n### **References**\n\n[1] Yuan, Zheng, et al. \"Scaling relationship on learning mathematical reasoning with large language models.\" arXiv preprint arXiv:2308.01825 (2023).\n\n[2] Luo, Haipeng, et al. \"Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct.\" arXiv preprint arXiv:2308.09583 (2023).\n\n[3] Yu, Longhui, et al. \"Metamath: Bootstrap your own mathematical questions for large language models.\" arXiv preprint arXiv:2309.12284 (2023).\n\n[4] Yue, Xiang, et al. \"Mammoth: Building math generalist models through hybrid instruction tuning.\" arXiv preprint arXiv:2309.05653 (2023).\n\n[5] An, Shengnan, et al. \"Learning From Mistakes Makes LLM Better Reasoner.\" arXiv preprint arXiv:2310.20689 (2023)."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission263/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700243452598,
                "cdate": 1700243452598,
                "tmdate": 1700243768376,
                "mdate": 1700243768376,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BoNzVXc2rK",
                "forum": "Ep0TtjVoap",
                "replyto": "dNfGQirWBM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission263/Reviewer_U6Sz"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission263/Reviewer_U6Sz"
                ],
                "content": {
                    "title": {
                        "value": "Reply to rebuttal"
                    },
                    "comment": {
                        "value": "Thanks for the response.\nSome of my concerns were addressed, but some remain.\n\n- limited novelty: Existing works have shown the effectiveness of imitation learning in improving the performance of open-source models, while this work just shows that PoT or CoT can be replaced with more powerful PoT+CoT (which is also used in MAmmoTH) to boost performance. Moreover, what are the clear advantages in the reply \"In contrast, ToRA introduces the Tool-integrated reasoning format, which brings clear advantages\"? \n- the accuracy of LLaMA trained on TORA-CORPUS has yet to saturate (e.g., plot the accuracy w.r.t. #samples of TORA-CORPUS). A follow-up question is: Are data generated from Output Space Shaping more effective than data generated by GPT-4? If not, why not just enlarge the latter? I understand GPT-4 is costly, but how do we maintain a good trade-off? \n- \"Our systematic analysis ... paving the way for the development of more advanced and versatile reasoning agents\" in the conclusion section might be overclaimed. Just a minor comment.\n- MAmmoTH and MetaMath are **missing** the main table (i.e., Table 2). \n\n> For a fair comparison, we adopted the same evaluation for all methods\n\nIt seems that some of the results in the main table are copied from their publications, \nI checked some of them but failed to find these techniques (round, `sympy`), e.g., WizardMath (https://arxiv.org/pdf/2308.09583.pdf), RFT (https://arxiv.org/abs/2308.01825)\n\n> Improving diversity means increasing the number of different solution paths for the same problem\n\nNote that different solution paths do not mean they are diverse. \n\nbtw, is the current paper the latest? (updating the paper is allowed in ICLR)"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission263/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700485850850,
                "cdate": 1700485850850,
                "tmdate": 1700487972722,
                "mdate": 1700487972722,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "NPEeseEgJx",
            "forum": "Ep0TtjVoap",
            "replyto": "Ep0TtjVoap",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission263/Reviewer_V1TN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission263/Reviewer_V1TN"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces TORA (Tool-integrated Reasoning Agents), which seamlessly integrates natural language reasoning with external tools to solve complex mathematical problems. By combining language models' analytical capabilities with computational efficiency tools, TORA significantly outperforms open-source models on 10 mathematical reasoning datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.This paper proposes a two-stage training framework that utilizes training data alternating between natural language and code language to enhance the reasoning ability of language models in mathematical reasoning tasks. The experimental results demonstrate the significant improvement of this approach across 10 datasets.\n\n2.The paper is generally well-written and the figures and tables presented are clear and easy to understand."
                },
                "weaknesses": {
                    "value": "1.From Figure 5, it can be observed that the performance of the model does not significantly decrease when output space shaping is removed. More experiments are needed to demonstrate whether the performance improvement in this stage is due to this training strategy rather than additional data and more training epochs.\n\n2.Regarding the TORA-corpus proposed in this paper, more detailed information is needed regarding the data construction process, quality evaluation, and dataset statistics."
                },
                "questions": {
                    "value": "1.It appears that the method proposed in this paper shows more significant improvements on smaller models, as the performance of the 7B, 13B, and 70B models shown in Figure 1 does not appear to differ significantly. How to explain this phenomenon?\n\n2.The alternating reasoning approach between natural language and tool usage proposed in this paper is fundamentally similar to the plan paradigm of Thought, Action, and Observation alternation in REACT [1]. Do you think this paradigm will become the dominant paradigm for agents to solve complex reasoning problems?\n[1] ReAct: Synergizing Reasoning and Acting in Language Models, ICLR 2023"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission263/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698680906836,
            "cdate": 1698680906836,
            "tmdate": 1699635951803,
            "mdate": 1699635951803,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Yif8e5fez0",
                "forum": "Ep0TtjVoap",
                "replyto": "NPEeseEgJx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission263/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission263/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Official Review by Reviewer V1TN (1/2)"
                    },
                    "comment": {
                        "value": "Dear reviewer V2TN, \n\nThank you for your comprehensive and meticulous review. We appreciate your recognition of the excellent performance of ToRA, as well as the approval of our presentation.\n\n### **On Output Space Shaping Results in Table 5**\n\n> From Figure 5, it can be observed that the performance of the model does not significantly decrease when output space shaping is removed. More experiments are needed to demonstrate whether the performance improvement in this stage is due to this training strategy rather than additional data and more training epochs.\n> \n\nWe appreciate your keen observation regarding Figure 5. There was a mistake in Figure 5 in the paper where the \u201csampling\u201d strategy was Inappropriately referred to as \u201cshaping\u201d, and we will correct this in future updates. Here\u2019s the clarification:\n\n1. Output Space Shaping, in fact, brought a significant improvement. As shown in Table 3, this method improved the performance of the 7B, 13B, and 34B models by 4.4%, 3.5%, and 3.4% respectively. For instance, ToRA-34B increased from 47.4% to 50.8%. Furthermore, the improvement was particularly noticeable in subtopics like Precalculus, Geometry, and Algebra, with an increase of up to 5.9% ~ 6.3%.\n2. Output space shaping includes two strategies: sampling and correction. In Figure 5, we conducted three experiments for comparison: pure imitation learning, using only the sampling strategy for shaping, and a combination of sampling + correction strategies for shaping. It's worth noting that all models were trained for the same number of epochs and both shaping experiments used the\u00a0**same amount of training data**. The results showed that shaping using only sampling led to a 2.7% average improvement, while combining sampling + correction strategies led to an average improvement of 3.4% ~ 4.0% on two datasets under the same data amount. These results demonstrate the effectiveness of the sampling and correction strategies in our proposed output space shaping.\n\n### **On Including More Detailed Information of ToRA-Corpus**\n\n> Regarding the TORA-corpus proposed in this paper, more detailed information is needed regarding the data construction process, quality evaluation, and dataset statistics.\n> \n\nThank you for your suggestion! Based on your suggestion, we will add a section in the appendix to provide a more detailed introduction to the data construction process, quality control, and report more data statistical information, beyond Sec. 2.2. Specifically:\n\n- **Data format and quality control**: In our preliminary experiments, we found that the tool-integrated reasoning trajectory format generated by zero-shot prompting was somewhat chaotic. Therefore, we designed a few-shot prompting to control the reasoning format, which effectively improved data quality. On the other hand, we increased the annotation success rate by sampling, ensuring more comprehensive coverage of the training query.\n- **Data filtering process:** For the data constructed, we filtered out paths that produced incorrect answers by matching them with standard answers. To prevent the model from learning incorrect intermediate reasoning processes, we further filtered out data samples with intermediate program execution errors.\n- **Dataset statistics**: We compared the annotation accuracy (i.e., sample coverage) of the training set on GSM8k, MATH, and MATH subtopics of ToRA-Corpus-Greedy (Sec. 2.2) using only the greedy trajectories, and ToRA-Corpus-16k combined with sampled trajectories. Furthermore, we reported the statistical data of ToRA-Corpus-16k, such as the number of samples, average question length, average, minimum, and maximum trajectory length, as shown in the following tables.\n\n*Table 1: Accuracy of ToRA-Corpus-16k on GSM8k and MATH.*\n\n|  | GSM8k | MATH | MATH | MATH | MATH | MATH | MATH | MATH | MATH |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n|  | Overall | Overall | Intermediate Precalculus | Algebra | Geometry | Number Theory | Counting & Probability | Prealgebra | Algebra |\n| ToRA-Corpus-Greedy | 94.4 | 64.3 | 51.0 | 51.5 | 70.0 | 77.4 | 72.2 | 89.8 | 85.1 |\n| ToRA-Corpus-16k | 98.2 | 83.1 | 72.9 | 70.0 | 58.9 | 91.6 | 81.7 | 95.5 | 96.3 |\n\n*Table 2: Statistics of ToRA-Corpus-16k*\n\n|  | GSM8k | MATH | Total |\n| --- | --- | --- | --- |\n| # Train Samples | 7,657 | 7,881 | 15,538 |\n| Avg Question Length | 236 | 189 | 211 |\n| Avg Trajectory Length | 678 | 704 | 691 |\n| Min Trajectory Length | 218 | 119 | 119 |\n| Max Trajectory Length | 1,713 | 2,486 | 2,486 |"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission263/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700151164335,
                "cdate": 1700151164335,
                "tmdate": 1700151164335,
                "mdate": 1700151164335,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Rf6Necir80",
                "forum": "Ep0TtjVoap",
                "replyto": "NPEeseEgJx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission263/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission263/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Official Review by Reviewer V1TN (2/2)"
                    },
                    "comment": {
                        "value": "### ****On More Significant Improvements on Smaller Models****\n\n> It appears that the method proposed in this paper shows more significant improvements on smaller models, as the performance of the 7B, 13B, and 70B models shown in Figure 1 does not appear to differ significantly. How to explain this phenomenon?\n> \n\nThank you for your detailed observation! According to Table 2, ToRA-Code's performance on GSM8k increased from 72.6 to 75.8 and 80.7 as the model size increased from 7B to 13B and 34B, respectively. In contrast, the performance on the MATH dataset saturated as the model size increased, with performances of 44.6, 48.1, and 50.8, respectively.\n\nTo further understand the bottleneck encountered by ToRA on the MATH dataset at around 50%, we collected the accuracy of different models on MATH problems of different difficulty levels, as shown in the following table:\n\n|  | Level 1 | Level 2 | Level 3 | Level 4 | Level 5 |\n| --- | --- | --- | --- | --- | --- |\n| # Test Samples | 437 | 894 | 1,131 | 1,214 | 1,324 |\n| Avg Question Length | 123.8 | 150.9 | 169.1 | 203.0 | 248.4 |\n| ToRA-Code-7B | 77.3 | 62.3 | 50.0 | 40.9 | 22.3 |\n| ToRA-Code-13B | 78.5 | 64.2 | 54.3 | 45.3 | 26.4 |\n| ToRA-Code-34B | 82.4 | 66.9 | 59.5 | 46.8 | 27.3 |\n| $\\Delta$ 13B $\\rightarrow$ 34B | +3.9 | +2.7 | +5.2 | +1.5 | +0.9 |\n| GPT-4 PAL | 80.1 | 65.4 | 58.4 | 45.8 | 30.0 |\n| GPT-4 Tool-integrated Reasoning | 89.5 | 77.7 | 71.0 | 55.6 | 39.0 |\n| Training query coverage | 97.7 | 91.6 | 86.5 | 81.3 | 68.0 |\n\nFrom the table, it is clear that the saturation of performance with increasing model size is primarily evident in difficult problems, i.e., levels 4 and 5. This phenomenon can be attributed to the following reasons:\n\n1. Firstly, ToRA greatly improved the learning efficiency of smaller models, and raised the starting point of the scaling curve. As shown in Fig. 4, the format of Tool-integrated reasoning is more conducive to model learning under the same amount of training data, allowing smaller models to reach a very high level.\n2. Secondly, the performance of the largest ToRA model is already close to that of the GPT-4 used for data annotation, and the end point of the scaling curve is mainly limited by the bottleneck of data annotation. The data constructed using GPT-4 has a relatively low coverage of difficult samples (81.3 and 68.0% for Level 4 and Level 5 samples, respectively). Increasing the size of the model increases the model's capacity, but the bottleneck is likely to be the quality and quantity of annotated samples.\n\nThis could encourage further research to focus on challenging math problems and more effective data engineering.\n\n### **On Discussing Alternating Reasoning Paradigm for Agent Reasoning**\n\n> The alternating reasoning approach between natural language and tool usage proposed in this paper is fundamentally similar to the plan paradigm of Thought, Action, and Observation alternation in REACT [1]. Do you think this paradigm will become the dominant paradigm for agents to solve complex reasoning problems?\n> \n\nThank you for your question! We agree with your view. Since language is the interface for LLMs to interact with tools and environments, the solving process for most LLM-Agent related tasks can be seen as interleaved rationale and interaction trajectory. Therefore, we believe alternating reasoning is a fundamental task completion paradigm based on the language interface, and we look forward to future work further promoting this method to solve more complex reasoning tasks with LLM!"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission263/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700151305049,
                "cdate": 1700151305049,
                "tmdate": 1700151305049,
                "mdate": 1700151305049,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WjCDsg913I",
                "forum": "Ep0TtjVoap",
                "replyto": "Rf6Necir80",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission263/Reviewer_V1TN"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission263/Reviewer_V1TN"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your reply, I have no further questions. I will remain my score."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission263/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700628847372,
                "cdate": 1700628847372,
                "tmdate": 1700628847372,
                "mdate": 1700628847372,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "0BJfx4ruLc",
            "forum": "Ep0TtjVoap",
            "replyto": "Ep0TtjVoap",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission263/Reviewer_mvjp"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission263/Reviewer_mvjp"
            ],
            "content": {
                "summary": {
                    "value": "The authors present a framework for improving mathematical reasoning by combining natural language descriptions with program synthesis and execution. A hybrid tool-integrated reasoning context is used to sample candidate reasoning trajectories for the GSM8K and MATH datasets. Candidate trajectories are then verified and those that are successful are added to the ToRA corpus. This corpus is used to fine-tune intermediate models. Such models are then employed as in the initial setting to sample candidate trajectories with feedback from teacher correction to aid the completion of partial trajectories. This final set of valid trajectories is used for further fine-tuning and finally producing the ToRA fleet of models, extended from the LLaMA-2 and CodeLLaMA base families. The resulting models show considerable performance boosts on a range of diverse mathematical reasoning datasets."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "The idea is clear, well-constructed, and well-explained. The figures are excellent and the algorithm is clearly laid out. The resulting models show considerable performance increases under a range of evaluation settings confirming the efficacy of the strategy."
                },
                "weaknesses": {
                    "value": "While the authors have presented what worked well, there is a considerable amount to be gleaned from the failure modes. The authors loosely allude to failure cases including geometric problems and program timeouts, and provide single examples in the appendix, but there are surely more interesting patterns. It would be wonderful if the authors could provide more specific examples and comment on more systematic classes of errors beyond these simple categorizations. For example, are there certain patterns in the natural language specification of the original problem or rationale construction that fail to formalize well as programs? Were the patterns informative in terms of which problems were amenable to imitation learning vs which required output space shaping in order to produce initial valid reasoning trajectories?"
                },
                "questions": {
                    "value": "As noted above, additional discussion of failure modes would be beneficial."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission263/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698700275678,
            "cdate": 1698700275678,
            "tmdate": 1699635951734,
            "mdate": 1699635951734,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wTib1QEK89",
                "forum": "Ep0TtjVoap",
                "replyto": "0BJfx4ruLc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission263/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission263/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Official Review by Reviewer mvjp (1/2)"
                    },
                    "comment": {
                        "value": "Dear Reviewer mvjp,\n\nThank you for your thoughtful review and the recognition of our work's clarity and effectiveness. We are pleased to provide more in-depth insights into the failure modes and the impact of output space shaping, as per your suggestions.\n\n### **Adding Comprehensive Failure Mode Analysis**\n\n> While the authors have presented what worked well, there is a considerable amount to be gleaned from the failure modes. The authors loosely allude to failure cases including geometric problems and program timeouts, and provide single examples in the appendix, but there are surely more interesting patterns. It would be wonderful if the authors could provide more specific examples and comment on more systematic classes of errors beyond these simple categorizations. Are there certain patterns in the natural language specification of the original problem or rationale construction that fail to formalize well as programs?\n\nWe appreciate your suggestion for a more comprehensive analysis of failure modes. To this end, we manually annotated 100 randomly selected trajectories from the MATH test set, identifying and categorizing their failure modes. Here are the results:\n\n| **Error Type** | **Definition** | **Percentage** |\n| --- | --- | --- |\n| Reasoning Error | Mistakes due to incorrect reasoning steps or missing conditions | 38% |\n| Hallucination | Fabrication of numbers or answers | 5% |\n| Diagram Understanding Error | Misinterpretation of the input diagram | 21% |\n| Inappropriate Tool Usage | Incorrect use of external tools, especially when the problem can't be solved directly with libraries | 10% |\n| Syntax Error | Persistent syntax errors despite multiple correction attempts | 9% |\n| Runtime Error | Errors during program execution, unresolved by retrying | 9% |\n| Rationale-only Error | Cannot be formalized into a program, and the subsequent rationale is also erroneous. | 3% |\n| False Negative | Correct answers that don't fully match the ground truth | 5% |\n\nWe observed that:\n\n1. Incorrect reasoning steps constitute the primary source of errors for ToRA on complex math reasoning tasks (38%), with hallucination issues also evident during problem interpretation and answer finalization (5%).\n2. Misinterpretation of input diagrams is the second largest category of errors (21%), particularly prevalent in Geometry, Precalculus, and Intermediate Algebra. This may be due to the fact that diagrams in the MATH dataset are often specified in text with the Asymptote language [1], which poses challenges to ToRA when understanding diagrams through text alone.\n3. Issues with tool usage include Inappropriate Tool Usage (10%), Syntax Error (9%), and Runtime Error (9%). These issues often manifest as ToRA being unable to correctly use tools after multiple rounds of modification and attempts. There are certain inputs that fail to formalize well as programs (3%), which require abstract reasoning rather than computation.\n4. We also found that there are false negatives when using automatic indicators, i.e., correct predictions that are misjudged as wrong, but the proportion is relatively small (5%).\n\nWe will include examples for each failure mode in the Appendix of the updated paper."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission263/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700150460630,
                "cdate": 1700150460630,
                "tmdate": 1700153707438,
                "mdate": 1700153707438,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "F9LFUTSKoV",
                "forum": "Ep0TtjVoap",
                "replyto": "0BJfx4ruLc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission263/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission263/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Official Review by Reviewer mvjp (2/2)"
                    },
                    "comment": {
                        "value": "### **Understanding the Impact of Output Space Shaping in Relation to Question Difficulty**\n\n> Were the patterns informative in terms of which problems were amenable to imitation learning vs which required output space shaping in order to produce initial valid reasoning trajectories?\n> \n\nYour question about the patterns that determine which problems are amenable to imitation learning versus those that require output space shaping is very interesting!\n\nInitially, we compared the results before and after output space shaping for different subtopics of MATH in Table 3 of the paper. We found that shaping has a more pronounced effect on Precalculus, Geometry, and Algebra, with improvements ranging from 5.9% to 6.3%.\n\nIn response to your question, we further compared the effects of output space shaping on MATH problems of different difficulty levels (from level 1 to level 5):\n\n|                     | Level 1  | Level 2  | Level 3  | Level 4  | Level 5  |\n|---------------------|----------|----------|----------|----------|----------|\n| # Test Samples      | 437      | 894      | 1131     | 1214     | 1324     |\n| Avg Question Length | 123.8    | 150.9    | 169.1    | 203.0    | 248.4    |\n| Avg Answer Length   | 503.1    | 655.8    | 751.2    | 881.6    | 1083.8   |\n| ToRA-Code-7B        | 74.1     | 57.5     | 46.9     | 35.2     | 19.4     |\n| + Shaping           | 77.3     | 62.3     | 50.0     | 40.9     | 22.3     |\n| $\\Delta$            | +3.2     | **+4.8** | +3.1     | **+5.7** | +2.9     |\n| ToRA-Code-13B       | 78.7     | 63.4     | 48.7     | 39.6     | 21.0     |\n| + Shaping           | 78.5     | 64.2     | 54.3     | 45.3     | 26.4     |\n| $\\Delta$            | -0.2     | +0.8     | **+5.6** | **+5.7** | **+5.4** |\n| ToRA-Code-34B       | 79.6     | 65.8     | 54.4     | 43.6     | 24.4     |\n| + Shaping           | **82.4** | **66.9** | **59.5** | **46.8** | 27.3     |\n| $\\Delta$            | +2.8     | +1.1     | **+5.1** | **+3.2** | +2.9     |\n| GPT-4 PAL           | 80.1     | 65.4     | 58.4     | 45.8     | **30.0** |\n\nOur findings indicate:\n\n- Across these different difficulty levels, output space shaping generally brings a significant improvement of 4.0% on average across different model sizes.\n- Comparing the growth rates for different difficulty levels, we find that output space shaping brings significant improvements for difficult, long problems. For example, with ToRA-Code-13B, shaping does not significantly improve level 1 to level 2 problems, but it brings a substantial improvement of 5.4% to 5.7% for level 3 to level 5 problems.\n- After using shaping, ToRA-34B outperforms GPT-4 PAL on problems from Level 1 to Level 4, but there is still a gap at Level 5 (27.3% vs. 30.0%). These problems are usually longer (average about 248.4 characters), require more reasoning steps (>1,000 characters) to solve, and more often include diagram inputs (about 20%). These findings may guide future work to focus more on solving these more difficult problems.\n\nWe will include these analyses in the updated version of the paper. Once again, thank you for your excellent suggestions!\n\n### References\n\n[1] Hendrycks, Dan, et al. \"Measuring mathematical problem solving with the math dataset.\" arXiv preprint arXiv:2103.03874 (2021)."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission263/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700150764021,
                "cdate": 1700150764021,
                "tmdate": 1700150764021,
                "mdate": 1700150764021,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "En3fgqgCmO",
                "forum": "Ep0TtjVoap",
                "replyto": "F9LFUTSKoV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission263/Reviewer_mvjp"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission263/Reviewer_mvjp"
                ],
                "content": {
                    "title": {
                        "value": "Reviewer Response to Author Comments"
                    },
                    "comment": {
                        "value": "Thank you for your reply. These additional findings are indeed useful and informative, highlighting opportunities for future contribution.\n\nI recommend acceptance. Great job!"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission263/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700415644766,
                "cdate": 1700415644766,
                "tmdate": 1700415644766,
                "mdate": 1700415644766,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]