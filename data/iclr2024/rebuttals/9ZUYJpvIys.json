[
    {
        "title": "TOSS: High-quality Text-guided Novel View Synthesis from a Single Image"
    },
    {
        "review": {
            "id": "zfkInVhKgv",
            "forum": "9ZUYJpvIys",
            "replyto": "9ZUYJpvIys",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3544/Reviewer_LBZY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3544/Reviewer_LBZY"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes TOSS, which is an image-to-image diffusion model. Comparing to pioneering work like Zero123, TOSS is different because it's not only conditioned on input image and relative camera pose, but also an optional text prompt. \n\nSince single image to other views are naturally an ill-pose problem, the authors claim that text prompt can provide more detailed guidance, which allows for increasing plausibility and controllability, and consequently better quality. In TOSS authors also analyze the drawbacks of previous conditioning mechanism and propose a dense attention mechanism for improved performance. \n\nAuthors provide a valid pipeline for automatic captioning and train the TOSS model on the large-scale Objaverse dataset. The extensive experiments prove the validness of each components and the superior performance of the proposed TOSS model."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "In general, this paper is motivated, well-organized and provide valuable results. In detail I find paper's strength in several aspects:\n\n1. The design of dense attention mechanism and the necessity of text as input are well motivated, with clear reason and proof.\n\n2. The experiment part is sufficient. Extensive experiments have been done on not only for the comparison and main results, but also ablation on a wide range of components.\n\n3. The authors also propose some valuable practices about image-to-image diffusion model, for example the expert denoiser design."
                },
                "weaknesses": {
                    "value": "### 1. Over-claim multiview-consistency ###\n\nI think the fundamental weakness for this paper is over-claim. In Sec.3.2.1, authors list two advantages of introducing text-prompt: plausibility and controllability, and I agree these two points. However, in the other parts of the paper, for example the Fig.1's caption, authors list *multiview-consistency* as another improvement, which I totally disagree.\n\nFundamentally, TOSS is like the Zero123 but with more input condition, it doesn't adopt some explicit design to guarantee the multiview-consistency like in SyncDreamer[1], so naturally it shouldn't be multi-view consistent. And although authors claim this in a lot of places in paper, I didn't find explanation towards this point.\n\nAlso, in Fig.13, while author titled it as \"Generated Multiview-consistent images\", it's actually not. For example in the provided microphone samples, I can clearly find it's not consistent across different views. So I think authors should revise the manuscript and avoid over-claim on being multivew-consistent.\n\n### 2. Emphasize the controllability ###\n\nAnother minor weakness is I hope authors can emphasize more on the TOSS's controllability. I would like to see the authors provide more visualization results where from the same input image, different text can generate different novel views, currently the results are mainly in supplementary and I think it's better to emphasize this ability and move the part to the main paper.\n\n[1] Liu, Yuan, et al. \"SyncDreamer: Generating Multiview-consistent Images from a Single-view Image.\" arXiv preprint arXiv:2309.03453 (2023)."
                },
                "questions": {
                    "value": "In Sec4.5, what is the detail meaning of \"replace the SD v1.4 model\". Does it mean you replace the VAE encoder and the decoder? Does it replaced only in inference or in the training and re-train the model?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3544/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3544/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3544/Reviewer_LBZY"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3544/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698104378304,
            "cdate": 1698104378304,
            "tmdate": 1699636308510,
            "mdate": 1699636308510,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "UcpNBGKzA7",
                "forum": "9ZUYJpvIys",
                "replyto": "zfkInVhKgv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3544/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3544/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to R4"
                    },
                    "comment": {
                        "value": "Thank you very much for the insightful feedbacks. We\u2019ve provided more results according to your comments in the new revision.\n1. **Over-claim multiview-consistency**: We agree that we do not introduce explicit modules to encourage multiview-consistency like in SyncDreamer. **However, we claim that text prompts improve the multiview-consistency with semantic constraints.** We support the conclusion with both qualitative and quantitative comparisons. **For qualitative comparison**, we refer to the minion in Figure 16 as an example. As we provide a front view to be the input condition image, Zero123 generates severe multi-view inconsistent results (minion with/without backpack), even under similar camera poses. However, our method significantly improves multi-view consistency with semantic constraint from text prompt \"minion without backpack\". **For quantitative comparison**, we evaluate 3D consistency scores in Table 2, which shows that our method indeed improves the multiview-consistency. In comparison to SyncDreamer, our improvement on multi-view consistency is not limited by azimuth or elevation angles. We also agree that it's extremely challenging to achieve 100% consistency with single-view novel view synthesis as eventually, this problem is severely under-constrained. We will revise the manuscript to avoid over-calim on being multivew-consistent but rather with improvements on multivew-consistency. We'll also cite more works on explicit multiview-consistency improvement for readers' reference.\n2. **Emphasize the controllability**: Thanks for pointing this out. We now provide more results on controllability in Figure 15. \n3. **Replace SD model for better performance**: We replace the initilazation weights of UNet and finetune it with higher resolution images. The other parameters such VAE encoder and decoder remain frozen."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3544/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700479041147,
                "cdate": 1700479041147,
                "tmdate": 1700479041147,
                "mdate": 1700479041147,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9TwD2bZYq3",
                "forum": "9ZUYJpvIys",
                "replyto": "UcpNBGKzA7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3544/Reviewer_LBZY"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3544/Reviewer_LBZY"
                ],
                "content": {
                    "comment": {
                        "value": "I have read all the reviews and author's response. I think overall this is a good paper and the minor problems can be fixed in revision. Therefore I'll keep my rating the same."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3544/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700631762755,
                "cdate": 1700631762755,
                "tmdate": 1700631762755,
                "mdate": 1700631762755,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "emimCS8fOR",
            "forum": "9ZUYJpvIys",
            "replyto": "9ZUYJpvIys",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3544/Reviewer_Givb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3544/Reviewer_Givb"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a TOSS that is text to the task of novel view synthesis (NVS) from a single image. Compared to a previous method (Zero123), TOSS utilizes a text as high-level semantic information to constrain the NVS solution space. It is based on the text-to-image Stable Diffusion pre-trained on the large-scale dataset. The proposed method achieves plausible results and those are controllable. The effectiveness of the proposed method is validated on the dataset."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "[Novelty] \n\n- This paper utilizes diffusion prior and text embedding features with cross attention to perform NVS using a single image.\n\n- This improves reconstruction performance for unseen areas and allows for consistent image generation.\n\n[Quality] \n\n- Overall, the paper is easy to follow and well written.\n\n- This paper reports the impact of each module of the proposed method in terms of performance as appropriate.\n\n[Clarity] \n\n- The paper is clearly written.\n\n- Motivation and explanation of each proposed module are reasonable.\n\n[Significance] \n\n- As a study that improves upon existing research, I think there are many elements that can be utilized in follow-up studies."
                },
                "weaknesses": {
                    "value": "- Compared to just using cross attention (CLIP embeddings), it seems that it needs to analyze various perspectives such as computational cost or memory.\n\n- Also, the analysis of the effectiveness of Expert Denoioser seems to be somewhat lacking.\n\n- There are experiments that show the quantitative effectiveness of each of the proposed modules, but the qualitative, in-depth analysis is somewhat lacking.\n\n- This paper is somewhat limited by the fact that comparative experiments were conducted with Zero123 only. I think it would be a better paper to compare NVS with a single image, or a diffusion-based model adapted to a given task, even if it is not exactly the same task.\n\t\t \t \t \t\t\n- There is no section 3.2.2, so there is also no need for section 3.2.1. It is better to use just section 3.2."
                },
                "questions": {
                    "value": "- In Table 1, there are no experiments for \"inference w/o text\" and no experiments for \"w/ expert denoisers\" for 160M?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3544/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698394520939,
            "cdate": 1698394520939,
            "tmdate": 1699636308411,
            "mdate": 1699636308411,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "l2c3ZLpO3H",
                "forum": "9ZUYJpvIys",
                "replyto": "emimCS8fOR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3544/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3544/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to R3"
                    },
                    "comment": {
                        "value": "Thank you very much for the insightful questions and suggestions. We\u2019ve provided more results according to your comments in the new revision.\n1. **Time/Memory cost**: As suggested, we analyze time/memory cost per module. (1) **Token-level attention** maintains the same time complexity but requires slightly more memory consumption for the VAE embeddings of the condition image. (2) **Text prompt attention** raises the attention token number from CLIP embedding's 1 to 77. In experiments, we evaluate time/memory cost on Nvidia A100 (80G) GPU with batchsize 128. During mixed-precision training, our method costs 2.13s and 52G per batch, while Zero123 costs 1.61s and 47G. **It's worth noting that our method is sufficiently stable, allowing us to employ half-precision training without any performance degradation, while Zero123 does not exhibit the same stability.** Therefore time/memory cost of out method could further decrease to 1.21s and 45G per batch with half-precision training, which is more efficient than Zero123.\n2. **Effectiveness of expert denoisers**: For completeness, we follow the reviewer's suggestion and supplement the results of expert denoisers under the 160M setting in Table 1, showing quantitatively better results of expert denoises across different training budgets. \n3. **More comparative experiments**: Thank you for the suggestion, we provide the results of DietNerf and Image Variation on GSO datasets for more comparative experiments in Table 1.\n4. **Section 3.2.1**: Thanks a lot for pointing this out. We have removed the title of section 3.2.1 in the revision.\n5. **\"inference w/o text\" and \"w/ expert denoisers\" experiments for 160M setting**: We've now included results under the 160M setting in Table 1."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3544/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700478929220,
                "cdate": 1700478929220,
                "tmdate": 1700478929220,
                "mdate": 1700478929220,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RVRFRGtkKr",
                "forum": "9ZUYJpvIys",
                "replyto": "l2c3ZLpO3H",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3544/Reviewer_Givb"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3544/Reviewer_Givb"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the feedback."
                    },
                    "comment": {
                        "value": "Considering the opinions of other reviewers and author's, I keep my original positive score."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3544/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700655228096,
                "cdate": 1700655228096,
                "tmdate": 1700655228096,
                "mdate": 1700655228096,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "T7BVDAYc7g",
            "forum": "9ZUYJpvIys",
            "replyto": "9ZUYJpvIys",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3544/Reviewer_wyYi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3544/Reviewer_wyYi"
            ],
            "content": {
                "summary": {
                    "value": "Novel view synthesis from a single RGB image is an under-constrained problem, and Zero123 solves this problem by training a view-conditioned latent diffusion model. However, Zero123 treats it as a pure image-to-image translation problem and thus suffers from pixel-level inconsistency problems. In this paper, TOSS adds text as high-level semantic information and, more importantly, proposes the cross-attention mechanism for better 3D consistency. Through exhaustive experiments, TOSS outperforms baseline Zero123 with more plausible and multiview-consistent NVS results, thus leading to substantial improvement in 3D reconstruction."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The proposed dense cross-attention mechanism is quite reasonable, and the explanation, especially in Figure 3, is persuasive. In the experiment section, both quantitative and qualitative results of novel view synthesis demonstrate the effectiveness of this module. More importantly, it significantly improves the 3D consistency, reflected in better 3D reconstruction results.\n2. The additional text conditions increase the controllability of the TOSS model. Hence, text prompts lead to diverse NVS results as presented in the paper.\n3. This paper is well-written and presented with a clear architecture. For the contributions summarized in the introduction, the corresponding experiment support can almost be found in the experiments."
                },
                "weaknesses": {
                    "value": "1. The authors claim that the text prompts increase the plausibility of novel view synthesis results. However, only the corresponding quantitative results (especially in the ablation study) can be found in the paper. Apart from the illustration in Figure 1, more qualitative results should be presented to support this claim.\n2. From the comparisons in Tables 1&2, TOSS only brings minor improvements over baseline method Zero123. Actually, prior work One-2-3-45 presents an interesting heatmap in Figure 4, showing the PSNR values are significantly affected by the relative azimuth and elevation angles. Hence, the authors should elaborate on how the results in Tables 1&2 were obtained to avoid unfair comparison.\n3. For the 3D reconstruction, the authors should also compare with relevant methods such as Magic123 and Consistent123."
                },
                "questions": {
                    "value": "1. During the cross-attention process, have you tried other orders or combinations of text, image, and camera pose?\n2. Expect more detailed elaborations on expert denoisers in the paper, in particular the motivation."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3544/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3544/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3544/Reviewer_wyYi"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3544/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698514194746,
            "cdate": 1698514194746,
            "tmdate": 1699636308336,
            "mdate": 1699636308336,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7Hv6YYUd0j",
                "forum": "9ZUYJpvIys",
                "replyto": "T7BVDAYc7g",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3544/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3544/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to R2"
                    },
                    "comment": {
                        "value": "Thank you very much for the insightful questions and suggestions. We\u2019ve provided more results according to your comments in the new revision.\n1. **Plausibility results**: In Figure 14 we've provided more qualitative results emphasizing plausibility improvements via text prompts. \n2. **Camera pose details for fair comparison**: For all the NVS results, we **randomly sample** the camera pose similar to how Zero123 renders the Objaverse objects for training. That means **camera poses are not strictly limited in certain relative azimuth or elevation angles**, which is quite different from Sycdreamer[1]. Furthermore, we use the same sets of camera poses for all methods for fair comparison.\n[1]Liu, Yuan, et al. \"SyncDreamer: Generating Multiview-consistent Images from a Single-view Image.\" arXiv preprint arXiv:2309.03453 (2023).\n3. **More 3D reconstruction baselines**: We've further included the comparison to Magic123 in Table 3. Unfortunately, the code of consistent123 is not open-sourced yet, we plan to add its result and comparisons to more methods upon availability. As Magic123 utilizes Stable Diffusion and Zero123 separately without joint training on image and prompts, the quality of its results is highly reliant on the quality of text prompts. Therefore we use the same Blip2-generated captions across all methods for fair comparison.\n6. **Ablation on cross attention process**: We supplement results of three ablation experimets on cross attention process below: (1) reverse the order of token-level attention and text prompt attention; (2) inject the camera pose information to text prompt attention instead of to token-level attention; (3) replace the token-level attention with gated attention and train it as an adapter with other parameters freezed. We can conclude from results of (1)(2) that different order of attention modules or different camera pose injecting choices lead to decrease in performance. Besides, experiment (3) reveals that **finetuning significantly enhances model performance compared to adapter training**.\n\n\n    | Method | PSNR | SSIM |\n    | -------- | -------- | -------- |\n    | (1) Reverse the order of  attention     | 16.63     | 0.8328     |\n    | (2) Inject the camera pose with text prompts     | 16.84     | 0.8275     |\n    | (3) Gated attention | 13.51 | 0.7105 |\n    | TOSS | 16.95 | 0.8369 |\n\n5. **More detailed elaborations on expert denoisers**: We discover that during sampling, novel view pose is dedicated at large timesteps and details are further refined at small timesteps. The observation is similar to the observation in diffusion-based 2D image generation that structure forms during early sampling steps and later sampling steps are responsible for image details. Therefore, we draw inspiration from ensemble of expert denoisers [1] to train two expert denoisers that are specialized for denoising at different timestep intervals.\\\n[1] Balaji, Yogesh, et al. \u201cediffi: Text-to-image diffusion models with an ensemble of expert denoisers\u201d."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3544/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700478777987,
                "cdate": 1700478777987,
                "tmdate": 1700478777987,
                "mdate": 1700478777987,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jOgjYJkRpg",
                "forum": "9ZUYJpvIys",
                "replyto": "7Hv6YYUd0j",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3544/Reviewer_wyYi"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3544/Reviewer_wyYi"
                ],
                "content": {
                    "title": {
                        "value": "Response to the authors"
                    },
                    "comment": {
                        "value": "I have read the authors' response, which settles most of my concerns. I suggest to include these additional results in the revision.  Considering the opinions of other reviewers, I decide to keep my original score and am leaning to accept this paper."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3544/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700731070675,
                "cdate": 1700731070675,
                "tmdate": 1700731070675,
                "mdate": 1700731070675,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "cE8dqJpLCi",
            "forum": "9ZUYJpvIys",
            "replyto": "9ZUYJpvIys",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3544/Reviewer_gFkU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3544/Reviewer_gFkU"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes to use text as high-level semantic information to constrain the NVS solution space. With this text, the proposed method can generate the multi-view consistency images. Meanwhile, it proposes dense cross-attention to align the reference and target image."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The whole idea is Intuitive and effective.  Using the text as semantic guidance to preserve multi-view consistency is reasonable since the ref image can not provide enough information. Meanwhile, since the proposed model uses the text prompt, it can finetune the SD directly, which preserves the ability of generation.\n2. The attention strategy is reasonable, which will help these two images align better."
                },
                "weaknesses": {
                    "value": "1. My main concern is Whether the texts bring enough improvement.  As shown in Table 4, I find \"Token-level attention\"  improves the model the most, while the \"Text prompt\" is not the best.  Meanwhile, the author should show these cases visually: 1. Zero123 + Token-level attention 2. Zero123 + Text prompt.\n2. I'm curious about the effect of this model on the human portrait since the Zero123 performance is bad in human portraits (Dataset limitations). The author can show two cases visually: 1. Taylor Swift (you can select an image of Taylor Swift as input) 2. Young male (you can randomly select a young male image).\n\nIf the authors can address my question properly, I will be glad to improve my grade."
                },
                "questions": {
                    "value": "see the weakness"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3544/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698936537028,
            "cdate": 1698936537028,
            "tmdate": 1699636308274,
            "mdate": 1699636308274,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "v0D9Jf939i",
                "forum": "9ZUYJpvIys",
                "replyto": "cE8dqJpLCi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3544/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3544/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to R1"
                    },
                    "comment": {
                        "value": "Thank you very much for the insightful questions. We\u2019ve provided more results according to your comments in the new revision.\n1. **Contribution of text prompt**: As requested, we've provided additional visual results showing contributions of the introduced modules in Figure 13. We can see that even though \"Token-level attention\" indeed improves the model the most in terms of metrics, the \"Text prompt\" **still significantly enhances the quality of the generated novel views** visually, including more plausible and intricate details (for example the lights on the top of the police car). We would like to point out that **the contribution of text prompts exceeds what the metrics demonstrate**. The reviewer's intriguing question precisely aligns with the observation in 3DiM[1]: **The standard metrics (PSNR/SSIM) are not sufficient to effectively evaluate geometry-free models for view synthesis**. Since NVS from a single image is an ill-posed problem (e.g. novel view generation of the backview given the front view), sometimes a plausible generation  which is inconsistent with the ground truth may result in a decrease in PSNR/SSIM. \\\n[1]Watson, Daniel, et al. Novel View Synthesis with Diffusion Models. Oct. 2022.\n2. **Effect on Human Portrait**: Thank you very much for the intriguing question. We provide additional 3D reconstruction results on Taylor Swift, young male and more in Figure 20. We acknowledge that there remains a quality gap between our method and the human-centric approaches, but **our results are of higher quality than zero123**. We agree with the reviewer that the quality concern in human portraits comes from the pretrain datasets (Objaverse), which lack sufficient human data. Therefore we believe the issue can be alleviated by finetuning on human-centric datasets which remains for future work."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3544/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700478668677,
                "cdate": 1700478668677,
                "tmdate": 1700478668677,
                "mdate": 1700478668677,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "sTDjjCyfIr",
                "forum": "9ZUYJpvIys",
                "replyto": "v0D9Jf939i",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3544/Reviewer_gFkU"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3544/Reviewer_gFkU"
                ],
                "content": {
                    "title": {
                        "value": "Reponse to Authors"
                    },
                    "comment": {
                        "value": "I think the authors address my first concern well, but it does not show good performance in human portraits due to the dataset limitation. So, I keep my original rating, I think this work can be accepted, but not to a high degree(i.e., oral or spotlight)."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3544/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700660870857,
                "cdate": 1700660870857,
                "tmdate": 1700660870857,
                "mdate": 1700660870857,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]