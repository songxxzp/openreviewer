[
    {
        "title": "Fast-DetectGPT: Efficient Zero-Shot Detection of Machine-Generated Text via Conditional Probability Curvature"
    },
    {
        "review": {
            "id": "T6nPXPWIXH",
            "forum": "Bpcgcr8E8Z",
            "replyto": "Bpcgcr8E8Z",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission862/Reviewer_nx1F"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission862/Reviewer_nx1F"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces an extension of DetectGPT called Fast-DetectGPT, which modifies the curvature criterion to operate per token, using the difference between the observed and average log probabilities. This requires only a single parallel forward pass from each model."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The proposed idea is intuitive. The conditional probability function is naturally parallelized by autoregressive models and this value should naturally be close to a local maximum for a model that generated a given text. The connection to likelihood and entropy was also interesting.\n\nThe experimental results are strong and comprehensive. Fast-DetectGPT is faster than DetectGPT by over two orders of magnitude due to its parallelization and also shows performance gains across six datasets. Even in the black-box (surrogate) evaluation setting, DetectGPT achieves impressively high recall at low false positive rates. It also shows qualitatively better behavior than DetectGPT on longer passages, where quirks of T5 masking cause DetectGPT to start underperforming as sequence length increases beyond a point."
                },
                "weaknesses": {
                    "value": "The end of section 2 shows that the criterion for Fast-DetectGPT can be seen as closely related to likelihood and entropy. While this connection is nice, I think the paper could be stronger if it analyzed each term in (7) in isolation to see what is most contributing to increased performance and why. Both likelihood and entropy are points of comparison in the result tables, but they do not perform as well; does their sum perform well? If not, and the denominator in (7) plays a key role, what probabilistic interpretation does that have, and what does that imply about the log_p surfaces of LLMs?\n\nNot really a weakness and perhaps out of scope for this submission, but I'd be interested in knowing how Fast-DetectGPT would work for very long passages, given that it scales favorably with passage length."
                },
                "questions": {
                    "value": "Please see sections for strengths and weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission862/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698462474163,
            "cdate": 1698462474163,
            "tmdate": 1699636013010,
            "mdate": 1699636013010,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "LtYKOzBkJR",
                "forum": "Bpcgcr8E8Z",
                "replyto": "T6nPXPWIXH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission862/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission862/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your constructive feedback.\n\n**Q1**: Both likelihood and entropy are points of comparison in the result tables, but they do not perform as well; does their sum perform well? What does that imply about the log_p surfaces of LLMs?\n\n**A1**: We have revised the paper to include a segment titled \u201cEntropy Ablation\u201d in Appendix C on page 18 to discuss it in detail. Briefly speaking, among the total 75% relative improvement, 10% is attributed to normalization, while the remaining 65% stems from the contribution of the numerator (log-likelihood + entropy). The entropy plays a crucial role in achieving high detection accuracy in Fast-DetectGPT. \n\nThe significance of entropy lies in stabilizing detection statistics by reducing variance in token-level log-likelihood across diverse contexts. The subtraction of $\\log p_{\\theta}(x_j|x_{<j})$ and $\\tilde{\\mu}_j$ results in a more stable statistic, robust to token or context fluctuations. In an experiment with ChatGPT generations for XSum, the average standard deviation reduces from 2.1893 (log-likelihood) to 1.6342 (log-likelihood + entropy), underscoring the efficacy of entropy in enhancing stability and reliability in the detection process.\n\n\n**Q2**: I'd be interested in knowing how Fast-DetectGPT would work for very long passages, given that it scales favorably with passage length.\n\n**A2**: Theoretically, the longer the passage is, the higher the detection accuracy will be, due to the statistical nature of the conditional probability curvature. However, the accuracy may not increase unlimited close to 1.0 given that the amount of increase in detection accuracy becomes marginal for longer passages. For example, when we increase the length from 120 to 150 and 180 tokens, the detection accuracy increases from 0.8864 to 0.9038 and 0.9061. The amount of increase between 0.9038 and 0.9061 is marginal compared to the increase between 0.8864 and 0.9038."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission862/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700202473227,
                "cdate": 1700202473227,
                "tmdate": 1700202473227,
                "mdate": 1700202473227,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Tc1FwbzGXt",
            "forum": "Bpcgcr8E8Z",
            "replyto": "Bpcgcr8E8Z",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission862/Reviewer_ubhC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission862/Reviewer_ubhC"
            ],
            "content": {
                "summary": {
                    "value": "This paper provides a state-of-the-art approach to zero-shot detection of LLM-generated text based on the difference of text likelihood and entropy. The paper provides extensive experiments, outperforming DetectGPT and a number of statistical baselines, as well as a supervised RoBERTa-based approach. The approach performs especially well when the scoring and target LLMs differ, e.g., when using GPT-J to detect whether an article was written by ChatGPT or GPT-4, which is a known failure mode of the existing DetectGPT approach. The paper also includes a number of experiments on different decoding strategies, reports performance across document lengths, and experiments with paraphrasing attacks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The main strength of this work is the performance of the proposed method, which is better than much more computationally intensive zero-shot detectors such as DetectGPT. The set of ablation experiments (across languages, domains, decoding strategies, and paraphrase attacks) is also reasonably thorough, and the proposed method shows state-of-the-art performance across almost all tested conditions and datasets."
                },
                "weaknesses": {
                    "value": "I find the framing of this paper and its comparison to be somewhat misleading. In particular, while the proposed method is described as a more efficient alternative to DetectGPT, its approach of computing the difference between the conditional probabilities of words and their alternatives is more similar to likelihood-based (Solaiman et al. 2019) or rank-based (GLTR; Gehrmann et al. 2019) approaches. Framing the method as a 340x speedup over DetectGPT therefore does not seem appropriate, although the method does seem to outperform existing zero-shot approaches. The sampling step in Fast-DetectGPT is also not clearly motivated and straightforwardly approximates an expected difference, so IMO the derivation could just immediately be replaced by the analytical solution. \n\nThe paper also includes supervised RoBERTa baselines from OpenAI; however, these are not state-of-the-art for supervised detection. I believe the paper would be strengthened by comparison to state-of-the-art supervised methods, such as Ghostbuster (Verma et al. 2023) or GPTZero (commercial model), especially given the claims in Section 5 that supervised methods have limited generalization capabilities in LLM-generated text detection. Because the primary purpose of the paper is to evaluate and compare zero-shot methods, however, this does not affect my score or recommendation for the paper.\n\nMinor notes:\n- The paper mentions both Rank and LogRank baselines in Section 3.1 but only provides LogRank in tables"
                },
                "questions": {
                    "value": "- Did you experiment with computing the difference between the probability of the top-ranked word according to an LM scorer and the observed word? I expect this should be closely correlated with the metric proposed in this paper, and is also a slightly more informative alternative to the Rank model."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission862/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698629924218,
            "cdate": 1698629924218,
            "tmdate": 1699636012935,
            "mdate": 1699636012935,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6imWmQjfGK",
                "forum": "Bpcgcr8E8Z",
                "replyto": "Tc1FwbzGXt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission862/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission862/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your constructive feedback.\n\n**Q1**: I believe the paper would be strengthened by comparison to state-of-the-art supervised methods, such as Ghostbuster (Verma et al. 2023) or GPTZero (commercial model).\n\n**A1**: We have revised the paper to include GPTZero as a baseline as Table 3 on page 7 and Table 7 on page 17 show. In summary, Fast-DetectGPT obtains higher average AUROCs than GPTZero. Specifically, on GPT-3 generations, GPTZero demonstrates poor detection accuracy on all three datasets, suggesting that GPTZero may not be trained on GPT-3 generations. On Chat-GPT and GPT-4 generations, GPTZero performs better than Fast-DetectGPT on news (XSum) but worse on stories (Writing) and technical articles (PubMed), which suggests that GPTZero may mainly be trained on news generations.\n\n**Q2**: Did you experiment with computing the difference between the probability of the top-ranked word according to an LM scorer and the observed word?\n\n**A2**: That is an interesting idea. We just experimented with it on GPT-4 generations and obtained an average AUROC of 0.8542, which is better than the likelihood (0.8212) and the log-rank (0.8088) but worse than Fast-Detect (0.9061). The results suggest that the probability difference is a better criterion than previous simple baselines.\n\n**Q3**: The sampling step in Fast-DetectGPT is also not clearly motivated and straightforwardly approximates an expected difference, so IMO the derivation could just immediately be replaced by the analytical solution.\n\n**A3**: We have revised the paper to introduce an individual segment titled \u201cConditional Independent Sampling\u201d to discuss it in detail. \n\nThe sampling process plays a pivotal role in guiding us toward the solution. To discern whether a token within a given context is machine-generated or human-authored, it is essential to compare it against a range of alternative tokens in the same context. By sampling a substantial number of alternatives (say 10,000), we can effectively map out the distribution of their $\\log p_{\\theta}(\\tilde{x}_ j|x_{<j})$ values. Placing the $\\log p_{\\theta}(x_ j|x_{<j})$ value of the passage token within this distribution provides a clear view of its relative position, enabling us to ascertain whether it is an outlier or a more typical selection. This fundamental insight forms the core rationale behind the development of Fast-DetectGPT.\n\nThe analytical solution is computationally more efficient but intuitively harder to understand, which we did not anticipate at the early stage of the research. Furthermore, when we use a single model for both sampling and scoring, the analytical solution shows a close connection to the Likelihood and Entropy baselines as we described on page 5. This connection seems so simple as the numerator in Eq. 7 is just the sum of Likelihood and Entropy. However, the sampling intuition plays a key role in finding this connection, where the Likelihood and Entropy have been used since year 2008.\n\n**Q4**: The paper mentions both Rank and LogRank baselines in Section 3.1 but only provides LogRank in tables.\n\n**A4**: We have revised the paper to remove the Rank baseline. Since Rank generally performs worse than LogRank, we remove it from the table to save space."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission862/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700203448002,
                "cdate": 1700203448002,
                "tmdate": 1700203448002,
                "mdate": 1700203448002,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "wBCpS5aSFI",
            "forum": "Bpcgcr8E8Z",
            "replyto": "Bpcgcr8E8Z",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission862/Reviewer_aeWx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission862/Reviewer_aeWx"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new method for detecting LLM generated text that offers not only substantial performance benefits over DetectGPT but is also much less compute intensive. This is underpinned by a hypothesis that context matters in determining the differences between human and machine generated output. Their method accordingly uses a new criteria, the conditional probability curvature, which they find is more positive for LLM output than human. They perform experiments on a variety of datasets, and analyze robustness with respect to multiple text attributes."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "- The proposed method is well motivated and described, and follows naturally from existing work\n- The results are strong both from a performance and efficiency standpoint, compared to DetectGPT\n- There is meaningful analysis with respect to attributes like passage length, paraphrasing, decoding strategies, etc."
                },
                "weaknesses": {
                    "value": "- The discussion of prior work with respect to alternate detection strategies such as watermarking is shallow. The Kirchenbauer et al. 2023 paper is for example not cited. While this paper takes an orthogonal approach, it would be good to see some motivation or discussion around the tradeoffs of those strategies.\n- The discussion of ethical considerations and broader impacts is lacking. Liang et al. 2023 has shown that LLM detection systems tend to exhibit higher false positive rates for non-native speakers. While this doesn\u2019t invalidate the usefulness of this work, at the least it is worth engaging with that literature and acknowledging the potential problems at play with this task. At best there could be experiments on the relative performance of this system on text written by different demographics as compared to prior work. Granted there is some analysis of performance on languages besides English but this is also relatively shallow."
                },
                "questions": {
                    "value": "Have you investigated the effects of varying the temperature setting or the value of k for Top-k?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission862/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission862/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission862/Reviewer_aeWx"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission862/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698817877641,
            "cdate": 1698817877641,
            "tmdate": 1699636012855,
            "mdate": 1699636012855,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Ym73en36xB",
                "forum": "Bpcgcr8E8Z",
                "replyto": "wBCpS5aSFI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission862/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission862/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your constructive feedback.\n\n**Q1**: Have you investigated the effects of varying the temperature setting or the value of k for Top-k?\n\n**A1**: We have revised the paper to include our experiments on a temperature of 0.6, top-k of 30, and top-p of 0.90 in Table 9 on page 19 and a detailed description in Appendix D.1 on page 20. Briefly speaking, reducing the values of T, k, and p enhances the determinism of generated samples, facilitating easier detection and resulting in higher AUROCs. \n\n\n**Q2**: The discussion of prior work with respect to alternate detection strategies such as watermarking is shallow. The Kirchenbauer et al. 2023 paper is for example not cited. It would be good to see some motivation or discussion around the tradeoffs of those strategies.\n\n**A2**: We have revised the paper to add a deeper discussion about watermarking in Appendix E on page 21, where the content is as follows.\n\nAnother line of detection methodology is watermarking that deliberately embeds information within machine-generated text to trace its origin \\citep{jalil2009review, kamaruddin2018review, abdelnabi2021adversarial, gu2022watermarking, kirchenbauer2023watermark}. In comparison, Fast-DetectGPT relies on the innate distinction between the texts generated by humans and by machines, which may further be strengthened by explicit watermarks as additional features. \n\nIn practice, these two strategies could potentially be combined to provide a more reliable detection solution. On the one hand, watermarking can be used to authorize the content generated by a specific service. On the other hand, when the service is out of our control and we cannot enforce the watermarking or a potential attacker has a strong LLM to remove the watermarks, the watermarking approach fails in these situations but the general detector like Fast-DetectGPT can still provide a valid solution.\n\n**Q3**: The discussion of ethical considerations and broader impacts is lacking. \n\n**A3**: We have revised the paper to add a segment \u201cEthical Considerations and Broader Impact\u201d in Appendix E on page 21 with the following content.\n\nFast-DetectGPT, serving as a highly efficient detector for machine-generated text, holds promise in enhancing the integrity of AI systems by combating issues like fake news, disinformation, and academic plagiarism. However, akin to other methods reliant on Large Language Models (LLMs), it is susceptible to inheriting biases present in the training data. Notably, as emphasized by \\cite{liang2023gpt}, LLM-based detection systems may exhibit an elevated false-positive rate when confronted with text from non-native English speakers. Given the widespread and diverse utilization of such technologies, this presents a notable concern.\n\nAn immediate suggestion is to substitute the underlying LLMs in Fast-DetectGPT with alternative models trained on more varied and representative corpora. Additionally, we advocate for community involvement in the ongoing efforts to develop more inclusive LLMs, a development that would benefit not only Fast-DetectGPT but also similar systems at large."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission862/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700203716433,
                "cdate": 1700203716433,
                "tmdate": 1700203716433,
                "mdate": 1700203716433,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "kqdK4NMcEb",
            "forum": "Bpcgcr8E8Z",
            "replyto": "Bpcgcr8E8Z",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission862/Reviewer_7WUs"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission862/Reviewer_7WUs"
            ],
            "content": {
                "summary": {
                    "value": "This paper present an extension for DetectGPT, improving its efficiency and effectiveness. Relying on LLM's output probability, the model can threshold and perform zero-shot detection. Given a sentence, the model will first autoregressively predict x' from the input, and then use the original input x as input to a LLM but calculate the probability to predict x'. The modification is simple, and effective, which intuitively makes sense."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Improved results over DetectGPT for 3 points, with also faster speed.\n\n2. The paper also showed results on detect GPT-4 results.\n\n3. Analytical solution presented to avoid sampling approximation.\n\n4. Ablation study on different lengths, decoding strategies, paraphrasing has been shown."
                },
                "weaknesses": {
                    "value": "1. Presentation should be made clear. In the intro, paragraph 4 talked about the algorithm, yet it is unclear what does \\tilt mean, what does <j means, also, the insight on why conditional probability is better is missing here, especially given that this is an extension of DetectGPT.\n\n2. Is there results for speed comparison?"
                },
                "questions": {
                    "value": "Can you elaborate how \\tilt {x} is generated? The reviewer is still confused.\n\nWhere does the acceleration come from? DetectGPT samples 100 pertrubations, how could this method accelerate 340 times? How many sampling does this needs?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission862/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698865751817,
            "cdate": 1698865751817,
            "tmdate": 1699636012743,
            "mdate": 1699636012743,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "hAjJkQMlmD",
                "forum": "Bpcgcr8E8Z",
                "replyto": "kqdK4NMcEb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission862/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission862/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your constructive feedback.\n\n**Q1**: In the intro, paragraph 4 talked about the algorithm, yet it is unclear what \\tilde mean, what does <j means. The insight on why conditional probability is better is missing here. Can you elaborate on how \\tilde{x} is generated? How many sampling does this needs? Where does the acceleration come from? \n\n**A1**: We have made revisions to enhance the clarity of the paper. Please refer to the improved paragraph 4 in the introduction and the newly added section titled \"Conditional Independent Sampling\" on page 4 for detailed explanations. \n\nBriefly speaking, $\\tilde{x}$ is generated by sampling each token $\\tilde{x}_ j$ independently from the conditional probability $p(\\tilde{x}_ j|x_{<j})$, where the passage $x$ is fixed. The independence of $\\tilde{x}_j$ given the passage $x$ is the key to the high efficiency of Fast-DetectGPT. Specifically, Fast-DetectGPT requires only a single forward pass to sample and evaluate 10,000 samples (our default setting), in contrast to DetectGPT, which typically necessitates 100 forward passes for a mere 100 perturbations. This results in a significant reduction in computation time and resources.\n\n**Q2**: Is there results for speed comparison?\n\n**A2**: We compared the speedup in Table 1 on page 1 and we have revised the paper to include a new segment titled \u201cInference Speedup\u201d on page 7, providing a detailed description. \n\nBriefly speaking, for the detection of XSum generations produced by the 5-model. DetectGPT total costs (14598s+13819s+14383s+15941s+20372s) = 79113s, which is about 22 hours. In contrast, Fast-DetectGPT total costs (34s+31s+34s+49s+85s) = 233s, which is about 4 minutes. The speedup is calculated by 79113s / 233s, which is around 340x."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission862/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700203996373,
                "cdate": 1700203996373,
                "tmdate": 1700204317482,
                "mdate": 1700204317482,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "5wuQGPtwiC",
            "forum": "Bpcgcr8E8Z",
            "replyto": "Bpcgcr8E8Z",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission862/Reviewer_RgLj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission862/Reviewer_RgLj"
            ],
            "content": {
                "summary": {
                    "value": "This paper improves the previous zero-shot method for detecting machine-generated text, DetectGPT, by replacing the perturbations as sampling using the same source model. Through the conditional probability curve, the author proves the effectiveness of this method. However, some experimental details are missing. More importantly, it did not mention another zero-shot work [1] released 5 months ago, which is the first to propose using a conditional probability curve for detection. Considering the similarity with the previous work [1], I would like to question the novelty of this paper since the long 5-month period clearly shows they are not concurrent work."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "Strength:\nThe experiments over diverse datasets and models validate its effectiveness.\nThe author considers both open-sourced and closed-source models for the detection. Thus, the results can be easily reproduced.\nThe ablation study is enough to support its claim regarding parameter sensitivity, attacks, etc.\nThe paper is well-written and easy to follow. The tables and figures are arranged properly.\n\nMissing reference: \nThe following zero-shot method is missing either in the related work or in the baselines.\n[1] Yang X, Cheng W, Petzold L, Wang WY, Chen H. DNA-GPT: Divergent N-Gram Analysis for Training-Free Detection of GPT-Generated Text. arXiv preprint arXiv:2305.17359. 2023 May 27."
                },
                "weaknesses": {
                    "value": "Weakness: \n1. The novelty is limited. The conditional probability curve has already been used by another zero-shot detector released 5 months ago [1]. However, the author neither cites this previous work nor discusses its differences. Considering the reference [1] work was released 5 months ago, I will not consider them as concurrent work.\n2. It is not clear how the sampling process works. Give a passage x, how do you sample the alternative x\u2019 ? Throughout the paper, I did not find any explanation for this.\n3. How would the number of resampled instances influence the result? I did not find any result for this.\n4. What is your default setting for the number of resampled instances for all the experiments? There is no clarification at all. \n5. How do you compare the speedup of your result over DetectGPT? Since the setting of your number of samples is unclear, I am not sure how did you compare it.\n\n\nAfter rebuttal: Thanks for the clarification. The authors addressed most of my concerns. I would like to raise my score."
                },
                "questions": {
                    "value": "Questions: \nThe number of relative improvements is confusing. For example, in Table 1, why is the relative improvement 74.7%? In my understanding, (0.9887\u22120.9554)/0.9554*100%=3.48%. I do not understand why you report 74.7%. \nSee more in Weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "n/a"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission862/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission862/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission862/Reviewer_RgLj"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission862/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699137941416,
            "cdate": 1699137941416,
            "tmdate": 1700616410007,
            "mdate": 1700616410007,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "897so3hNQE",
                "forum": "Bpcgcr8E8Z",
                "replyto": "5wuQGPtwiC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission862/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission862/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response (Part1)"
                    },
                    "comment": {
                        "value": "Thank you for your feedback.\n\n**Q1**: Missing reference: The following zero-shot method is missing either in the related work or in the baselines. \n\n**A1**: We have revised the paper to incorporate DNA-GPT as a baseline. Please refer to Tables 2, 3, 5, 7, and the relevant sections on related work for details.\n\n**Q2**: The novelty is limited. The conditional probability curve has already been used by another zero-shot detector released 5 months ago [1]. \n\n**A2**: We **strongly disagree** with this statement given the following evidence.\n\nFirst, **neither the term \u201cconditional probability curvature\u201c nor terms with similar meanings are found in the paper** including both arXiv versions published on 5/27/2023 (https://arxiv.org/abs/2305.17359v1) and on 10/4/2023 (https://arxiv.org/abs/2305.17359v2), where the second version is even after our submission to ICLR 2024 on 9/28/2023. \n\nSecond, **the basic idea of DNA-GPT is fundamentally different from that of Fast-DetectGPT**. DNA-GPT generates various completions given a truncated prefix of a candidate passage, where the completions are generated autoregressively (requiring a decoding process for each completion). In contrast, Fast-DetectGPT does token-level conditionally independent sampling, where all samplings are done in the predictive distribution (requiring only one forward pass instead of any decoding process). \n\nThird, **DNA-GPT costs 80x more inference time than Fast-DetectGPT**. Experiments on XSum generations produced by the 5-model show that DNA-GPT takes 19289s (about 5 and half hours) to run across the five models, while Fast-DetectGPT only takes 233s (about 4 minutes). DNA-GPT costs 82.8x the inference time of Fast-DetectGPT.\n\nLast, **DNA-GPT has much worse detection accuracy than Fast-DetectGPT** on both 5-model generations and ChatGPT/GPT-4 generations.  As Tables 2, 3, 5, and 7 in the revised version show, DNA-GPT performs even worse than DetectGPT on 5-model generations. Although its detection accuracy on ChatGPT and GPT-4 generations is higher than DetectGPT but is still significantly lower than Fast-DetectGPT (0.8836 and 0.7648 for DNA-GPT vs. 0.9615 and 0.9061 for Fast-DetectGPT on ChatGPT and GPT-4, respectively). \n\n**Q3**: It is not clear how the sampling process works. Give a passage x, how do you sample the alternative x\u2019?\n\n**A3**: We mentioned it in various places and in various forms, for example, \u201cthe conditional probabilities of alternative tokens p(\\tilde{x}_j|x_{<j})\u201d and \u201cour approach begins by **sampling alternative word choices at each token**\u201d on page 2, the definition of conditional probability function \u201cp(\\tilde{x}|x)\u201d and \u201cconditional sampling\u201d part of Algorithm 1 on page 4, and the analytical expression of the sample mean in Eq. 5 on page 5.\n\nThe sampling of alternative \\tilde{x} is conditionally independent given a passage x that each token \\tilde{x}_j is sampled from the conditional distribution of p(\\tilde{x}_j|x_{<j}) independently. We have revised the paper to introduce an additional segment titled \u201cConditionally Independent Sampling\u201d to illustrate the sampling process in detail.\n\n**Q4**: How would the number of resampled instances influence the result?\n\n**A4**: As we mentioned in \u201cthe analytical solution achieves a detection accuracy **almost identical** to the sampling approximation with 10,000 samples\u201d on page 5, sampling with the number of resampled instances from 10,000 to 50,000 produces NO obvious differences in the AUROC, and also NO obvious difference in the time cost. \n\n**Q5**: What is your default setting for the number of resampled instances for all the experiments?\n\n**A5**: We by default use 10,000 samples for the experiments as we mentioned in \u201cthe analytical solution achieves a detection accuracy almost identical to the sampling approximation with **10,000 samples**\u201d on page 5.\n\n**Q6**: How do you compare the speedup of your result over DetectGPT? \n\n**A6**: We mentioned it in the caption of Table 1 \u201cSpeedup assessments were conducted **using the XSum** news dataset, with computations **on a Tesla A100 GPU**\u201d, and we further revised the paper to include a new segment \u201cInference Speedup\u201d on page 7 to discuss it in detail.\n\nSpecifically, we use 10,000 samples for Fast-DetectGPT and the default setting of 100 perturbations for DetectGPT. The speedup is evaluated on XSum generations from the 5 models. DetectGPT exhibited substantial computational demands, with total processing times across five runs summing up to 79,113 seconds (14598s+13819s+14383s+15941s+20372s, approximately 22 hours). Conversely, Fast-DetectGPT demonstrated exceptional efficiency with total times of only 233 seconds (34s+31s+34s+49s+85s, about 4 minutes). The resulting speedup factor is approximately 340x, calculated by dividing the total time for DetectGPT by that for Fast-DetectGPT (79,113s / 233s \u2248 340)."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission862/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700205385636,
                "cdate": 1700205385636,
                "tmdate": 1700205385636,
                "mdate": 1700205385636,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pX2ufSblu8",
                "forum": "Bpcgcr8E8Z",
                "replyto": "5wuQGPtwiC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission862/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission862/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response (Part 2)"
                    },
                    "comment": {
                        "value": "**Q7**: The number of relative improvements is confusing.\n\n**A7**: We have revised the paper to clarify it as Table 1 on page 1 shows. The relative improvements are calculated by (new \u2013 old) / (1.0 \u2013 old), where the denominator (1.0 \u2013 old) represents the maximum possible improvement from the old AUROC. The metric shows how much improvement has been made relative to the maximum possible improvement, which is especially useful when the values of new and old are already high (close to 1). For example, intuitively, an improvement from 0.9554 to 0.9887 looks so different from an improvement from 0.7225 to 0.9338. However, when we compare their relative improvements, we can see that the former 74.7% is close to the latter 76.1%, which reveals the inner consistency between the two experiments."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission862/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700205501344,
                "cdate": 1700205501344,
                "tmdate": 1700205547713,
                "mdate": 1700205547713,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gnnsqvvDRz",
                "forum": "Bpcgcr8E8Z",
                "replyto": "897so3hNQE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission862/Reviewer_RgLj"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission862/Reviewer_RgLj"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the clarification"
                    },
                    "comment": {
                        "value": "Thanks for the clarification. I have modified my score."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission862/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700616483151,
                "cdate": 1700616483151,
                "tmdate": 1700616483151,
                "mdate": 1700616483151,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]