[
    {
        "title": "CLIP-Guided Reinforcement Learning for Open-Vocabulary Tasks"
    },
    {
        "review": {
            "id": "U6kl4ISokp",
            "forum": "Wx97sznZwB",
            "replyto": "Wx97sznZwB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3362/Reviewer_VdeC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3362/Reviewer_VdeC"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a new reward function and task specification strategy for RL on Minecraft. First, ChatGPT is used to extract a target object from a given language instruction. Then, a modified version of MineCLIP is used to convert the target object and current observation image into a segmentation map that highlights the location of the target object in the image. The authors propose a shaped reward function that incentivizes both increasing the area of the target object in the segmentation map and centering the target object in the frame. Instead of conditioning the policy on the language instruction, the policy is conditioned on the segmentation map. PPO is used to optimize the policy against the proposed shaped reward (plus the original sparse reward). Experiments show that this method outperforms prior methods (STEVE-1, Cai et al) and ablations on language-conditioned Minecraft tasks. Additionally, the experiments show that the learned policy can generalize zero-shot to new instructions."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper addresses reinforcement learning of open-world, open-vocabulary instruction following which is a problem of significant interest to the community. Building on prior work, the authors use Minecraft as a test-bed for their method. Minecraft is becoming a standard benchmark for these types of methods and so this choice will allow for easier comparison to prior work. The proposed method is a novel modification of existing work (MineCLIP). The motivation and explanation of the method is clear. The experiments ablate the different components of the method and compare to prior work."
                },
                "weaknesses": {
                    "value": "My main concern is that this method makes more assumptions than the prior work it is compared to. Specifically, this method assumes the task involves navigating to a target object that is specified in the instruction. An example of an instruction where this method would not work as well is \"build a tower\" (since there is no target object to move toward). Notably, MineCLIP, STEVE-1, and Cai et al do not make this assumption.\n\nAdditionally, the comparison to imitation learning methods like STEVE-1 and Cai et al should be justified since the proposed method does online RL. Do the imitation learning methods see more or less relevant data for the evaluations tasks? Imitation learning and online RL are different classes of methods so some explanation is needed here.\n\nSmaller comments:\n- Throughout the paper it seems that \"open-vocabulary\" is used to mean \"unseen instructions\" For instance, the \"open-vocabulary\" section in the experiments describes testing generalization to unseen instructions. While a method must be open-vocabulary to accept unseen instructions, it would be more clear to specifically state that the capability these experiments test is zero-shot generalization to unseen instructions.\n- In Figures 7 and 8 it would be good to indicate in the plots which tasks involve unseen instructions and which just involve an unseen biome (since it seems both are tested). \n- In Figure 8, it seems like the success rate plots (b and c) could be combined (with some indication of which tasks involve unseen instructions/biomes) like in Figure 7."
                },
                "questions": {
                    "value": "- Why wasn't EmbCLIP evaluated on unseen instructions in the hunt domain?\n- Why is \"shear a sheep\" considered an unseen instruction when it's part of the instructions seen during training? (\"open vocabulary generalization\" section, second paragraph)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3362/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3362/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3362/Reviewer_VdeC"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3362/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698680694120,
            "cdate": 1698680694120,
            "tmdate": 1699636286546,
            "mdate": 1699636286546,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jPNLbBFdpp",
                "forum": "Wx97sznZwB",
                "replyto": "U6kl4ISokp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3362/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3362/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your efforts and valuable comments. We would like to address the weaknesses and questions you raised in the review.\n\n> Why wasn't EmbCLIP evaluated on unseen instructions in the hunt domain?\n\nWe do not evaluate EmbCLIP on unseen instructions in the hunt domain due to its limited performance on the training instructions. Since the model does not perform well on tasks it is trained for, we determine that it would not provide meaningful insights to test it on unseen instructions.\n\n> Why is \"shear a sheep\" considered an unseen instruction when it's part of the instructions seen during training? \n\nWe would like to clarify that 'shear a sheep' is not considered as an unseen instruction here. As shown in Figure 8, we do not highlight 'wool' in green. **The inclusion of 'shear a sheep' in the generalization test is used to calculate the precision**. As defined in the same paragraph, precision is calculated as the number of times correctly harvesting the specified target divided by the number of times harvesting any target declared in the group\u2019s instructions. Therefore, 'shear a sheep' is grouped with 'shear a mushroom cow' since both tasks require the tool 'shears'. This grouping allows us to assess the precision of the model in correctly identifying the unseen target object 'mushroom cow', ensuring that the agent does not simply shear any object indiscriminately. Similarly, another training instruction 'milk a cow' is also included in a group along with an unseen instruction 'harvest water' for the same reason.\n\n> Comparison to imitation learning methods.\n\nIn our study, the comparison between COPL and the imitation learning methods, STEVE-1 and Cai et al., is due to the lack of alternative Minecraft foundation models with goal- or instruction-conditioned policies trained via reinforcement learning. We select STEVE-1 and Cai et al. for their capabilities in mastering multiple basic skills in Minecraft, similar to the objectives of our open-vocabulary evaluation.\n\nWe acknowledge that comparing an online RL method with imitation learning methods is not a perfect match since the training data is quite different. Therefore, our intention is not to determine which method performs best in learning basic skills in Minecraft. Instead, we just use STEVE-1 and Cai et al. as a reference to understand how COPL performs in this specific domain.\n\n> Smaller comments.\n\nThank you for your detailed suggestions. We will make improvements based on them in the revision.\n\n----\n\nIf you have any additional questions or comments, please do not hesitate to let us know. We are more than happy to provide any further clarifications or information that may be helpful. If you find our responses satisfactory, we would be grateful if you could consider raising the score."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3362/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700038662106,
                "cdate": 1700038662106,
                "tmdate": 1700038662106,
                "mdate": 1700038662106,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Rl15Kr0oHl",
                "forum": "Wx97sznZwB",
                "replyto": "jPNLbBFdpp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3362/Reviewer_VdeC"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3362/Reviewer_VdeC"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for answering my questions. \n\nHowever, I remain concerned that this method makes more assumptions about the task than the prior work it is compared to. As the other reviewers have pointed out, this proposed reward is only valid for tasks that involve moving towards a target object while the prior methods work for more general Minecraft tasks (MineClip, STEVE-1, Cai et. al). This assumption is clear in the choice of tasks for the experiments (only hunting and harvesting tasks). Additionally, the proposed method only outperforms much simpler baselines (one-hot and CLIP embedding conditioning) in the hunting domain. \n\nWhile it's fine to propose a method with more assumptions and more limited applicability, the paper should make those assumptions clear. The paper currently claims that the proposed method solves open-vocabulary tasks, implying the method is applicable to all tasks that can be specified with language."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3362/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700450518872,
                "cdate": 1700450518872,
                "tmdate": 1700450518872,
                "mdate": 1700450518872,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dPp5BdmJIp",
                "forum": "Wx97sznZwB",
                "replyto": "U6kl4ISokp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3362/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3362/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer, we are happy to receive your response and appreciate the opportunity to further discuss and address your concern.\n\n> While it's fine to propose a method with more assumptions and more limited applicability, the paper should make those assumptions clear. The paper currently claims that the proposed method solves open-vocabulary tasks, implying the method is applicable to all tasks that can be specified with language.\n\nIn the original version of our paper, we claimed \"By open-vocabulary task, we mean that the agent is instructed to interact with diverse objects beyond the training scope.\" at the beginning of Section 2, by which we limit the scope of tasks that our method is designed to solve. We acknowledge, however, that this description may not be precise enough. Therefore, based on the comments from reviewers, we append an additional statement to provide a more restrictive claim on our assumption and application: **\"More specifically, we focus on object-centric tasks and the open-vocabulary ability over target objects.\"**, in the latest revision. Thank you for bringing this to our attention, and we hope this clarification aligns with the paper's objectives more accurately.\n\n> Additionally, the proposed method only outperforms much simpler baselines (one-hot and CLIP embedding conditioning) in the hunting domain.\n\nFirstly, we would like to highlight that our proposed method outperforms baselines not only in the hunt domain but also in the harvest domain. While the learning curves of the three models on **training instructions** appear comparable, as shown in Figure 8(a), the gap between our method and the baseline is significant on **unseen instructions** during test, as illustrated in Figure 8(c). This result demonstrates our method's superior ability to generalize to unseen instructions than EmbCLIP, suggesting that our proposed approach of taking 2D confidence map as a policy conditioning representation offers better open-vocabulary ability over target objects compared to relying on language descriptions.\n\nSecondly, we think that using a one-hot vector as the task indicator (One-Hot) and conditioning policy on language (EmbCLIP) are not simple, but are essential instead. COPL, EmbCLIP, and One-Hot share the same network architecture and training procedures, thus enabling a clear and direct comparison of generalization ability between policy conditioning representations from different modalities (vision, language, and vector), excluding the influence of other factors.\n\n> ... this proposed reward is only valid for tasks that involve moving towards a target object while the prior methods work for more general Minecraft tasks (MineClip, STEVE-1, Cai et. al).\n\nWe acknowledge the limitation of COPL in the aspect of generality, as we newly added description in Section 2. At last, we would like to offer a comprehensive perspective that combines both theoretical and empirical considerations here.\n\n**MineCLIP.** Although theoretically, MineCLIP can align video clips with a wide range of language descriptions, besides object-centric tasks, previous work [1,2] and our experiments demonstrate its practical limitations. We find that in practice, MineCLIP still tends to align entities in video clips with those in language descriptions like the original CLIP, instead of utilizing temporal information (otherwise the MineCLIP reward would effectively guide the agent to approach targets in object-centric tasks).\n\n**Imitation learning.** STEVE-1 struggles with accurately differentiating targets, as evidenced by its low performance in the hunt domain. Cai et al. (2023) show limited generalization ability over target objects, as illustrated in Figure 8(b), due to its condition on language input. This aligns with our observations on EmbCLIP. Theoretically, we can attribute these deficiencies to the limitation of the imitation learning dataset, instead of the methods themselves. However, from another perspective, reinforcement learning shows advantages here of being independent of the quality and distribution of the demonstration dataset.\n\nIn summary, no method can perfectly handle all tasks and perform zero-shot generalization. Our method could be regarded as a practical compromise that **sacrifices some of MineCLIP's theoretical capabilities of generality but enhances the practical performance for object-centric tasks**, which compose a large portion of tasks in Minecraft.\n\n[1] Cai et al., 2023, Open-World Multi-Task Control Through Goal-Aware Representation Learning and Adaptive Horizon Prediction.\n\n[2] Yuan et al., 2023, Plan4MC: Skill Reinforcement Learning and Planning for Open-World Minecraft Tasks."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3362/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700537205726,
                "cdate": 1700537205726,
                "tmdate": 1700537354903,
                "mdate": 1700537354903,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "JE1e8eKh9f",
            "forum": "Wx97sznZwB",
            "replyto": "Wx97sznZwB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3362/Reviewer_NgQ4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3362/Reviewer_NgQ4"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a method for improving CLIP-guided rewards for Reinforcement Learning for completing open-vocabulary tasks within the game of Minecraft.\n\nThe approach builds on top of MineCLIP, a VLM trained on internet-scale Minecraft videos, used as an auxiliary reward model for training PPO to solve Minecraft tasks expressed in the form of natural language. Instead of using the original MineCLIP image embedding and computing its individual cosine distance with the MineCLIP language encoding of the task to obtain a reward, the authors propose leveraging recent techniques for open-vocabulary segmentation with CLIP to produce a \u201c2d confidence map\u201d for the open-vocabulary task target over the image visual field. This 2d confidence map is computed by obtaining CLIP embeddings for \u201cpatches\u201d of the image, and obtaining the normalized cosine distance of each patch with the embedding for the target text (with a subsequent \u201cdenoising\u201d step making use of several negative prompts). This 2d map is used as an additional input to the policy MLP. Moreover, to then obtain a reward, one must integrate over this 2d confidence map, weighting the entries with a gaussian kernel (obtaining a \u201cfocal\u201d reward). This reward is then multiplied by a constant and summed to the vanilla environment reward.\n\nThe authors conduct several experiments on tasks belonging to the \u201chunting\u201d domain, comparing their baseline (COPL) with other techniques for auxiliary rewards, such as the original MineCLIP. They also test the generalization of their method on tasks belonging to the \u201chunting\u201d and \u201charvesting\u201d domains, generalizing the target of the task to unseen objects in an open-domain fashion. Over these experiments, COPL reliably performs better than alternatives."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The proposed method addresses a specific shortcoming of MineCLIP, a previous approach for auxiliary semantic rewards for Minecraft tasks. Essentially, the problem with MineCLIP is that it serves as a very noisy and not well shaped reward signal for language based tasks. A well-shaped RL reward for several Minecraft tasks should involve distance to a target object, and COPL fixes this problem with its 2d confidence map technique.\n\nThe experiments seem to show a clear improvement over baselines for the chosen hunting tasks, showing that COPL indeed works as a better-shaped CLIP reward for such tasks."
                },
                "weaknesses": {
                    "value": "Overall, the main problem with the approach is that it does not seem to be very \u201cgeneral\u201d. This would not be a problem per se (not all ICLR papers should aim at \u201cgeneral\u201d solutions to problems), if not for the fact that the work builds directly on top of MineCLIP, which was aimed at producing a multi-task \u201cgeneral\u201d agent for open-ended Minecraft tasks.\n\nTo be more specific: the original MineDojo paper involved attempting to solve all kinds of Minecraft tasks based on their language descriptions (\u201cmilk a cow\u201d, \u201chunt a sheep\u201d, \u201ccombat a zombie\u201d, \u201cfind a nether portal\u201d, \u201cdig a hole\u201d, \u201clay a carpet\u201d). For this reason, they proposed a \u201cgeneral\u201d method making use of a MineCLIP encoder, encoding image sequences and language commands, which is not limited to a specific \u201ctask domain\u201d. The MineCLIP encoder can in principle encode image sequences for every Minecraft task, be it hunting, combat, pure world exploration, or tasks that do not involve focusing on a game \u201centity\u201d, such as simply digging a hole. (Whether it achieved satisfactory results is another matter)\n\nIn this paper, the MineCLIP encoder is instead taken as a building block, to then do image segmentation based on individual \u201centity\u201d labels such as \u201ccow\u201d, \u201cpig\u201d and \u201csheep\u201d. This means that essentially, in order to improve performance on some specific task domains such as hunting, the method\u2019s generality was reduced to be only suitable for tasks involving focusing on and getting closer to specific game entities (it is no longer possible to conceivably use this technique to learn the task \u201cdig a hole\u201d, or \u201clay a carpet\u201d). Essentially, the \u201cCOPL\u201d technique consists of a method for turning a suitable open-vocabulary image segmentation model into a 2d confidence map that helps both as a better task-conditioned input for the policy MLP, and as a more well-shaped reward model, biased towards looking at entities and getting close to them. \n\nIn the experiment section, most results that paint the COPL method in a clearly positive light belong to the \u201chunt\u201d domain. Essentially, what needs to be learnt within this domain is to identify an entity in the world based on a word, keep it within the center of the screen, attack it and pursue it while it flees. It is apparent why the specific biases of the focal COPL reward function would help in this case, to the point it could be considered as \u201coverfitted\u201d to tasks similar to this. For the \u201charvesting\u201d domain (which still involves in practice finding a specific \u201centity\u201d in the world, getting close to it, and collecting it), the benefits of the technique already appear smaller or not present. No other open-ended tasks have been tried, and it\u2019s doubtful that the COPL technique would even be applicable for them (how to do so for \u201cdig a hole\u201d?).\n\nWhat I\u2019m getting at is, if the specific domains and settings for this method to be useful have to be so restricted, what stops us from directly using traditional reward shaping in the state space of the Minecraft world (not general purpose, but strong)? In any case, we no longer support free-form text prompts and are overfitted to hunting tasks. I would appreciate further elaboration on this point."
                },
                "questions": {
                    "value": "I have the following questions:\n* Could you elaborate on the experiment design for the experiment in Figure 8? Why are learning curves so similar for all methods in panel (a), but not in the panels (c) and (d), where MineCLIP seems to perform worse than COPL? Why is there no \u201cone hot\u201d in panels (b), (c) and (d)?\n* From a cursory look, it seems that the MineCLIP baseline agent for tasks such as \u201chunt a cow\u201d seems to severely underperform relative to the one from the original MineCLIP paper. Can you comment on this?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3362/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3362/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3362/Reviewer_NgQ4"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3362/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698765946910,
            "cdate": 1698765946910,
            "tmdate": 1699636286401,
            "mdate": 1699636286401,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "sIdJd0gi6A",
                "forum": "Wx97sznZwB",
                "replyto": "JE1e8eKh9f",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3362/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3362/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your efforts and valuable comments. We appreciate such a detailed and insightful discussion on our method. We would like to address the weaknesses and questions you raised in the review.\n\n> Why are learning curves so similar for all methods in panel (a), but not in the panels (c) and (d), where MineCLIP seems to perform worse than COPL? \n\nThere might be a typo in the question, as 'MineCLIP' is not mentioned in Figure 8; we assume 'EmbCLIP' is intended.\n\nIn Figure 8(a), the learning curves show the success rates of three models on **training instructions** throughout the training process. The similarity in these curves indicates that all methods have comparable performance on the training tasks. The results demonstrated in Figure 8(c) and (d) include training instructions and **unseen instructions** (highlighted in green). The gap between COPL and EmbCLIP on training instructions ('milk a cow' and 'shear a sheep') is narrow and approximately aligns with the gap observed at the end of the training in Figure 8(a). However, the gap widens on unseen instructions ('harvest water', 'shear a mushroom cow', and 'harvest sand'), suggesting COPL's superior generalization ability.\n\nSpecifically, for EmbCLIP, while the text encoder can produce a language embedding for the novel target object, this embedding is essentially out-of-distribution for the policy network. This is because the language embedding space is huge but the policy network is only trained on a limited set, *i.e.*, embeddings of four training target objects. In contrast, COPL's policy network takes a 2D confidence map as input. After our modified MineCLIP converts the unseen target name into a confidence map, the policy network can process it without a generalization gap. \n\nWe would like to highlight this novel use of a 2D confidence map as a policy conditioning representation is another contribution of our work, besides the intrinsic reward. Multi-task RL with the original MineCLIP model cannot acquire such a unified representation but only takes as input the language instruction, as implemented in the original MineCLIP paper.\n\nThe elaborated experiment settings are available in Appendix B.3.3.\n\n> Why is there no 'one hot' in panels (b), (c) and (d)?\n\nNo One-Hot in Figure 8(b): Figure 8(b) focuses on training instructions. Since COPL, EmbCLIP, and One-Hot exhibit similar performance on these training instructions, we omit EmbCLIP and One-Hot here for brevity. We provide success rates of these methods on each training instruction in Figure 17.\n\nNo One-Hot in Figure 8(c) and 8(d): Figure 8(c) and 8(d) are intended to present generalization performance on unseen instructions. However, One-Hot is only a multi-task baseline and cannot perform meaningful generalization test. In detail, it uses a one-hot vector as the task indicator, which is set to the same length as the number of training tasks. This design means that One-Hot lacks the flexibility to handle unseen instruction, as there is no appropriate task indicator.\n\n> From a cursory look, it seems that the MineCLIP baseline agent for tasks such as \u201chunt a cow\u201d seems to severely underperform relative to the one from the original MineCLIP paper. Can you comment on this?\n\nWe appreciate your observation of this noticeable phenomenon that we did not mention in the paper but is worthy of analysis.\n\nIn the original MineCLIP paper, four tasks, 'milk a cow', 'hunt a cow', 'shear a sheep', and 'hunt a sheep', are trained together under a multi-task reinforcement learning framework. Among these four tasks, 'milk a cow' and 'shear a sheep' are relatively easier to learn than the others. Therefore, the MineCLIP reward likely guides the agent to successfully learn these easier tasks first, and then a behavior bias, moving towards a cow or sheep, is introduced into the policy network, facilitating the learning of more challenging hunt tasks. However, in our hunt task learning experiments, there are no such easy tasks to introduce behavior bias, resulting in the underperformance of MineCLIP reward. Other minor reasons include the fact that we do not use action smoothing and self-imitation learning and we employ a different PPO codebase.\n\nAnother noticeable point is that, as shown in its Table 1, multi-task RL in the original MineCLIP paper seems to overfit to cow-related tasks, possibly due to the use of language instructions as input. This challenge of correctly grounding language into visual targets is also observed in our experiment (EmbCLIP in Figure 7(a)). This demonstrates the advantage of our method using a unified 2D confidence map as the policy condition.\n\nAdditionally, Figure 13 demonstrates that our implementation of the MineCLIP reward successfully guides the agent to master harvest tasks, including 'milk a cow' and 'shear a sheep', confirming the correctness of our implementation."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3362/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700038523912,
                "cdate": 1700038523912,
                "tmdate": 1700416872544,
                "mdate": 1700416872544,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wsYIzUVSyp",
                "forum": "Wx97sznZwB",
                "replyto": "JE1e8eKh9f",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3362/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3362/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> For the \u201charvesting\u201d domain, the benefits of the technique already appear smaller or not present.\n\nA detailed explanation of the open-vocabulary evaluation in the harvest domain is in the answer to the first question.\n\n> ... what stops us from directly using traditional reward shaping in the state space of the Minecraft world (not general purpose, but strong)?\n\nIf the goal is only to learn tasks in Minecraft, we agree that using traditional reward shaping in the state space of the Minecraft world, such as the 'manual reward' designed in the original MineCLIP paper, is a direct approach. But from our perspective, we wish our proposed methodology applicable in diverse environments, rather than only in Minecraft, for the problem defined in Section 2. As you succinctly summarized, \"Essentially, the 'COPL' technique consists of a method for ... getting close to them\", the whole framework is not tailored for Minecraft and can also be applied to other similar first-person view and object-centric environments such as robotic navigation. We use Minecraft as a convenient and suitable testbed to validate our method. Therefore, we avoid the use of environment-specific information to guide reinforcement learning.\n\n> Comparison with MineCLIP in terms of generality.\n\nAs mentioned above, our method could be viewed as a framework applicable to diverse environments, not limited to the specific use case of Minecraft. In Minecraft, we use MineCLIP to realize the first component of our method, \"turning a suitable open-vocabulary image segmentation model into a 2d confidence map\". We do not intend to build a new fundamental model or method that theoretically surpasses MineCLIP in terms of multimodal ability and generality in Minecraft. Instead, it just serves as a key module specific to Minecraft within our framework to solve the problem we defined in Section 2. In other first-person view and object-centric environments, our framework can adapt by replacing the modified MineCLIP with general models like Grounding DINO + SAM or domain-specific models.\n\n----\n\nIf you have any additional questions or comments, please do not hesitate to let us know. We are more than happy to provide any further clarifications or information that may be helpful. If you find our responses satisfactory, we would be grateful if you could raise the score."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3362/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700038563087,
                "cdate": 1700038563087,
                "tmdate": 1700038563087,
                "mdate": 1700038563087,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PlI4BJBnz0",
                "forum": "Wx97sznZwB",
                "replyto": "wsYIzUVSyp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3362/Reviewer_NgQ4"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3362/Reviewer_NgQ4"
                ],
                "content": {
                    "title": {
                        "value": "Response to the Authors"
                    },
                    "comment": {
                        "value": "Dear Authors,\n\nI thank you for your detailed reply. I still have concerns, namely, citing your reply:\n\n> Specifically, for EmbCLIP, while the text encoder can produce a language embedding for the novel target object, this embedding is essentially out-of-distribution for the policy network. This is because the language embedding space is huge but the policy network is only trained on a limited set, i.e., embeddings of four training target objects. In contrast, COPL's policy network takes a 2D confidence map as input. After our modified MineCLIP converts the unseen target name into a confidence map, the policy network can process it without a generalization gap.\n\nThis is clear, however this implies that all information contained in the language embedding is essentially lost, except for a 2d scalar map, which is only useful insofar as the language task description mentions an object that the agent needs to get close to. If we expect this embedding to be useful to condition policies for any other task (such as \"place a carpet\"), we have lost all information. This is what I meant when I said that the proposed method reduces the generality of MineCLIP.\n\n> In the original MineCLIP paper, four tasks, 'milk a cow', 'hunt a cow', 'shear a sheep', and 'hunt a sheep', are trained together under a multi-task reinforcement learning framework. Among these four tasks, 'milk a cow' and 'shear a sheep' are relatively easier to learn than the others. Therefore, the MineCLIP reward likely guides the agent to successfully learn these easier tasks first, and then a behavior bias, moving towards a cow or sheep, is introduced into the policy network, facilitating the learning of more challenging hunt tasks. However, in our hunt task learning experiments, there are no such easy tasks to introduce behavior bias, resulting in the underperformance of MineCLIP reward. Other minor reasons include the fact that we do not use action smoothing and self-imitation learning and we employ a different PPO codebase.\n\nYour reply here leads me to believe that your approach should fit in seamlessly in the multitask RL setting from the original paper, and thus better comparisons would have been obtained if starting from such a multitask baseline, adding your focal reward technique on top.\n\nI read and understood your further points on how you were not planning on introducing \"a new fundamental model or method that theoretically surpasses MineCLIP in terms of multimodal ability and generality in Minecraft\". Still, even understanding this, the paper's contribution seem underwhelming, as they boil down to the use of a focal reward function based on a 2d object detection map."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3362/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700584292783,
                "cdate": 1700584292783,
                "tmdate": 1700584292783,
                "mdate": 1700584292783,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "cXgXDMszff",
            "forum": "Wx97sznZwB",
            "replyto": "Wx97sznZwB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3362/Reviewer_9Xm8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3362/Reviewer_9Xm8"
            ],
            "content": {
                "summary": {
                    "value": "COPL (Clip-guided Open vocabulary Policy Learning), the proposed algorithm in this paper, attempts to solve the problem of open vocabulary language instructed reinforcement learning (RL) -- the task of accomplishing a goal described in natural language without having any constraints (ideally) on the words used to specify the instruction. The paper works in a setting where the behaviors are still fixed, for instance, the agent still has to perform a similar sort of sequence of actions like hunting, but the object of hunting is chosen from objects that did not occur during training and are rather chosen from other available objects that can be hunted. COPL works as follows: first, the object that is to be acted on is segmented out using a modified version of MineCLIP. This gives a confidence map. This confidence map, combined with a focal objective that tries to get the desired object in the center of the frame and nearer to the agent, forms what is called as focal reward. The agent looks at the current observation from the environment in addition to the confidence map derived and is asked to take action. These actions are then optimized using standard RL algorithms such as PPO on a combination of focal reward with reward coming from the environment. \n\nThe experimental analysis involves (i) showing how the proposed (selective segmentation + focus)  works on a single task, e.g., only hunting a pig, (ii) working of the algorithm on multi-task settings, e.g., hunting a pig and hunting a sheep (plus, cow and chicken), and (iii) open-vocabulary testing of agent's capabilities. The approach is compared with different reward combinations and ablations of the proposed reward."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Writing and presentation quality is very high. The paper is dense with content and I enjoyed going through the paper multiple times. The architecture description is neat, although I had to assume few things about the implementation while evaluating the correctness. \n\nThe sections are introduced in a logical order, and every choice behind the focal reward is well-motivated. As I understand it, the focal objective is very similar to how a human would accomplish the task of, say, 'hunting a pig', by being confident about the animal that is to be approached and getting a better focus. This description might seem to overfit the case of hunting, but other tasks that require inferring the intended object correctly and approaching it efficiently are also covered. This also stems from other problems, which I discuss in the weakness section.\n\nThe approach described in the paper is compared with relevant baselines while comparing single-task, multi-task, and open vocabulary capabilities.\n\nMore importantly, I find this work important from the point of view of starting a discussion on how open-vocabulary instructions can be used in conjugation with RL. The overall methodology and evaluation framework outlined is quite systematic and serves as a guide for future research in this area."
                },
                "weaknesses": {
                    "value": "**Limits of confident and focused seeking of object**: From my limited knowledge of Minecraft as a game, it is an open-ended environment where the agent can build by gathering resources and surviving. It is open-ended in the sense that one gets to express complex ideas, which involves using resources through intents. I am not sure all this open-endedness is captured in being confident about the desired object and focusedly approaching it. Put another way, the approach might be overfitting only a part of the actual space of possible Minecraft behaviors. \n\n**Issue with negative words**: For segmenting out the object in the intent, the method uses negative words. While this approach would work in domains such as Minecraft, where the entities are finite and known, I am unsure whether it will hold when applied to real-world cases where entities could be unknown and infinite.\n\n**Comment on novelty**: I am not fairly acquainted with Minecraft research. From the related works pointed out in the paper, it does seem that the approach presented is novel in its entirety. But, from the computer vision perspective, both prompt-based local segmentation and focal vision are pretty standard. The use of CLIP for Minecraft is the prior work that COPL builds on. So, I find the novelty of COPL in applying everything in a functional manner to test the open-vocabulary capabilities of the assembled system."
                },
                "questions": {
                    "value": "I have the following questions for the authors:\n1. By limiting the CLIP model to a set of pre-determined negative words, isn't the paper limiting the scope and moving away from the actual aim of being open vocabulary? \n2. To extend the previous question, is it possible to perform a similar analysis, but instead of negative words, use the entire vocabulary?\n3. Would it make sense to keep the objects fixed and change the intended behavior to a similar but nuanced variant of the behavior? Again, I have limited knowledge of Minecraft as a game and have limited knowledge about possible behaviors. However, it seems very logical to me to test open vocabulary capabilities where the agent might 'catch' an animal rather than 'kill' an animal where catching is out of distribution. These behaviors are not very different from training behavior as 'exploring the world' is to it."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3362/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3362/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3362/Reviewer_9Xm8"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3362/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698773183662,
            "cdate": 1698773183662,
            "tmdate": 1699636286330,
            "mdate": 1699636286330,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "e5lL2joOhO",
                "forum": "Wx97sznZwB",
                "replyto": "cXgXDMszff",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3362/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3362/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your efforts and valuable comments. We appreciate such a detailed and insightful discussion on our method. We would like to address the weaknesses and questions you raised in the review.\n\n> By limiting the CLIP model to a set of pre-determined negative words, isn't the paper limiting the scope and moving away from the actual aim of being open vocabulary?\n\nThese pre-determined negative words are used to instruct the model on what to ignore, thereby enhancing the accuracy of the 2D confidence map by reducing noise. Concretely, we compute similarities between these negative words and image patches, in addition to similarities between the target object name and image patches. By applying a softmax function to these similarities on each patch, we suppress the activation of the target object on non-target patches, especially where objects from the negative word list are likely present. This results in a cleaner, low-noise confidence map. \n\nGiven the principle mentioned above, we construct the negative word list containing objects that frequently appear in Minecraft for effective denoising. Note that while we limit the negative words, we do not restrict the range of target objects. So in principle, our modified MineCLIP can generate the confidence map for any target object and does not move away from the aim of being open-vocabulary.\n\n> To extend the previous question, is it possible to perform a similar analysis, but instead of negative words, use the entire vocabulary?\n\nIf 'the entire vocabulary' refers to all object names in Minecraft, we believe it will work as well because our negative is equivalent to the most frequent part of these objects and is proven to be effective. If 'the entire vocabulary' means the general vocabulary that CLIP can understand, we suppose that it would be less effective. There are multiple reasons. For example, synonyms for the target object will suppress its activation on the confidence map because the synonyms also have high similarities on the patches where the target object presents. Additionally, words with different levels of granularity will complicate the segmentation results. For instance, a human might be identified as 'human' as a whole but also as 'head', 'arms', etc., separately.\n\nTherefore, a pre-determined, domain-specific word list is essential for open-vocabulary tasks. This is also evidenced in open-vocabulary learning studies, such as open-vocabulary segmentation [1,2,3], where benchmark datasets would provide a pre-determined class set for segmentation evaluation. We hope this could also address your concerns about negative words raised in Weaknesses.\n\n[1] Ding et al., 2022, Open-Vocabulary Panoptic Segmentation with MaskCLIP.\n\n[2] Zhou et al., 2022, Extract Free Dense Labels from CLIP.\n\n[3] Liang et al., 2023, Open-Vocabulary Semantic Segmentation with Mask-adapted CLIP.\n\n> Would it make sense to keep the objects fixed and change the intended behavior to a similar but nuanced variant of the behavior? \n\nWe appreciate this insightful suggestion and conduct relevant tests to explore our model's ability to adapt to similar but nuanced behavior patterns. We test the hunt domain model on the 'milk a cow' task, where the object 'cow' is familiar, but the required behavior pattern is slightly different. In detail, hunt tasks require 'attack' action to interact but 'milk a cow' requires 'use' action. The hunt model achieves a 43(\u00b116)% success rate, lower than that of the harvest model trained on this task but significantly higher than zero, suggesting that COPL can partially generalize to a nuanced variant of the training behavior pattern.\n\nAnother relevant example is the test task 'harvest sand' in the harvest domain, where both object and behavior are out of the training distribution. As we introduced at the end of Appendix B.3.3, 'harvest sand' requires a slightly different behavior from the training task, and at the same time, 'sand' is a novel target object. As demonstrated in Figure 8(c), COPL outperforms the baseline EmbCLIP on this task.\n\nHowever, it is important to note that this generalization may stem from the stochastic or noisy nature of the policy output and does not represent that COPL has a true open-vocabulary ability over verbs.\n\n----\n\nIf you have any additional questions or comments, please do not hesitate to let us know. We are more than happy to provide any further clarifications or information that may be helpful."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3362/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700038422932,
                "cdate": 1700038422932,
                "tmdate": 1700038422932,
                "mdate": 1700038422932,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "012BdAWn3Q",
                "forum": "Wx97sznZwB",
                "replyto": "e5lL2joOhO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3362/Reviewer_9Xm8"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3362/Reviewer_9Xm8"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your detailed response. My questions are answered to a good extent. I do not have anything more to add."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3362/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700603981481,
                "cdate": 1700603981481,
                "tmdate": 1700603981481,
                "mdate": 1700603981481,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "aiipJrUST7",
            "forum": "Wx97sznZwB",
            "replyto": "Wx97sznZwB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3362/Reviewer_6CKq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3362/Reviewer_6CKq"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a new intrinsic reward for open-vocabulary tasks in minecraft. The proposed technique first applies existing dense CLIP methods to the MineCLIP model and discovers similar open-vocabulary segmentation property. The authors then proposes an intrinsics reward that motivates the agent to approach the segmented object. With this intrinsics reward, the paper allows the minecraft agent to learn to perform certain open-vocabulary tasks. The authors demonstrate the effectiveness of the method in terms of single tasks, multi-tasks, and open-vocabulary setting."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The writing of the paper is clear. Method and motivations are discussed thoroughly.\n- The paper introduces spatial priors for its intrinsics reward which was missing in vanilla way of using MineCLIP\n- Evaluation is thorough despite limitations I will mention in the weakness section."
                },
                "weaknesses": {
                    "value": "While I acknowledge the soundness of the approach and the presentation, I don't find this paper's contribution significant enough for acceptance. This is the main reason of my rating. \n\nThe core contribution of the paper is an intrinsics reward for minecraft with a lot of limitations. 1. the reward seems to be specifically tailored for minecraft's first-person-view setting, and specifically towards tasks that involves approaching an object 2. the tasks have to be object-centric, and the generalization is mostly object level. \n\nWhile the promise of this paper / minecraft itself is about open-vocabulary, the presented method is limited to approaching objects in fpv setting. This is also reflected in the evaluation, where the task covered are no way near open-world. This also has been mentioned by multiple other reviewers. The authors should tune down their claim about open-world.\n\nThe proposed open-vocab segmentation seems to be very similar to previous methods like [1]. Even if I disregard this fact, it seems to me that if this reward is already tailored for minecraft (first-person view + task is mainly approach object), one may well use some open-vocabulary detection model tailored for the few tasks the paper benchmarked in. Once one detects the objects from text, the reward the authors propose seems an obvious thing to do. The evaluation, as a result of the limitations of such reward, are also constrained to be very object centric ones, which breaks the purpose of MineCLIP. I would not claim the proposed technique to be effective for general open-vocabulary tasks.\n\nAfter the rebuttal, I decide to lift my score from 3 to 5 for added result and also raise my confidence from 4 to 5. This is because I had personal experience trying almost every single component the paper used, and have tried some them on the figures the authors provided during rebuttal period. I believe the current approach have its merit, but would belong to a more system/experiment heavy paper where such a reward only plays part of the role. At its current state, I reiterate my belief that such a reward alone, under the broken promise of open-worldness, doesn't constitute the technical contribution a full ICLR paper needs.\n\n[1] https://arxiv.org/pdf/2112.01071.pdf"
                },
                "questions": {
                    "value": "1. I am not exactly sure whether the authors claim the architecture in figure 2 to be a main contribution. If so, the authors should probably discuss previous approaches and how is your approach different, either when you mention MaskCLIP for the first time or in related work.\n\n2. From my understanding, for this reward to work properly, the object has to be already in FOV, correct?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3362/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3362/Reviewer_6CKq",
                        "ICLR.cc/2024/Conference/Submission3362/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3362/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698822659872,
            "cdate": 1698822659872,
            "tmdate": 1700716864509,
            "mdate": 1700716864509,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "b2PAZl5ONG",
                "forum": "Wx97sznZwB",
                "replyto": "aiipJrUST7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3362/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3362/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your efforts and valuable comments. We would like to address the weaknesses and questions you raised in the review.\n\n> I am not exactly sure whether the authors claim the architecture in figure 2 to be a main contribution.\n\nThe architecture illustrated in Figure 2 is not the main contribution of our study but rather a key module within our overall framework. As introduced in Section 3.1, it is adapted from MaskCLIP. The differences between our modified MineCLIP and MaskCLIP are twofold: firstly, MineCLIP includes an additional temporal transformer in the vision pathway; secondly, our modified MineCLIP outputs a confidence map for the target object instead of a semantic segmentation result. \n\n> From my understanding, for this reward to work properly, the object has to be already in FOV, correct?\n\nGetting a non-zero focal reward indeed requires the target object to be in the agent's field of view. When the target object is not present in the FOV, resulting in a confidence map with no high-probability patches, the focal reward is zero. This characteristic would prompt the agent to look around immediately after spawning in the Minecraft world to find the target object. In our experiments, target objects often appear in the agent's observable range, which means that the agent can see the target object when facing the direction of the target object.\n\nIf we consider a more challenging setting where the target object is located out of the agent's observable range at the spawning point, an additional exploration algorithm should be integrated to encourage the agent to move around. For example, incorporating novelty-induced exploration.\n\n> The core contribution of the paper is an intrinsics reward for minecraft with a lot of limitations ...\n\nWe acknowledge that our method is primarily designed for first-person view and object-centric tasks. But we want to clarify that this setting is not exclusive to Minecraft, but contains a wide range of realistic and general applications. For example, embodied AI research domains, such as autonomous driving, navigation, and robotic manipulation, often employ first-person views and/or focus on object-centric interactions. This setting better mirrors the complexity of real-world challenges compared to canonical RL environments, where our method may not be suitable.\n\nAdditionally, we would like to emphasize that our contribution also includes a novel policy conditioning representation, namely the 2D confidence map. Using a 2D confidence map to replace language input substantially improves the generalization ability of learned policy, as demonstrated in Section 4.2. Although currently restricted to object-centric tasks, it offers some insights about grounding language into vision to facilitate multimodal policy learning.\n\nWe hope this elaboration clarifies the generality and contributions of our work, while also acknowledging and appreciating the limitations you have pointed out.\n\n> ... which breaks the purpose of MineCLIP.\n\nAs mentioned above, our method is primarily designed for first-person view and object-centric tasks, which are not restricted in Minecraft. We use Minecraft as a convenient and suitable testbed to validate our method and adapt MineCLIP to provide the segmentation functionality that our method requires. Therefore, we do not intend to develop a new fundamental model or method that follows the purpose of MineCLIP and theoretically surpasses it in terms of multimodal ability and generality in Minecraft. Instead, it just serves as a key module specific to Minecraft within our framework to solve the problem we defined in Section 2. In other first-person view and object-centric environments, our framework can adapt by replacing the modified MineCLIP with general models like Grounding DINO + SAM or domain-specific models.\n\n---\n\nIf you have any additional questions or comments, please do not hesitate to let us know. We are more than happy to provide any further clarifications or information that may be helpful. If you find our responses satisfactory, we would be grateful if you could raise the score."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3362/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700038295073,
                "cdate": 1700038295073,
                "tmdate": 1700038295073,
                "mdate": 1700038295073,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nhFuFPVv4C",
                "forum": "Wx97sznZwB",
                "replyto": "b2PAZl5ONG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3362/Reviewer_6CKq"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3362/Reviewer_6CKq"
                ],
                "content": {
                    "comment": {
                        "value": "Dear authors, thank you for your quick response. Before I make my comprehensive response to your rebuttal, I'd really appreciate it if you can give a convincing response to my point about open-vocabulary detection model under the same adoption to minecraft(or fpv games). I deem a clear explanation and supporting experiments to be essential to address that point."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3362/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700367667269,
                "cdate": 1700367667269,
                "tmdate": 1700367667269,
                "mdate": 1700367667269,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "c2JCsFJVXT",
                "forum": "Wx97sznZwB",
                "replyto": "aiipJrUST7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3362/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3362/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer, we appreciate the opportunity to further discuss and respond to your point about the use of open-vocabulary detection models in Minecraft or other first-person-view (FPV) games. \n\n**Minecraft**. Firstly, we would like to clarify that currently, there is not an existing open-vocabulary detection model tailored for Minecraft. An alternative approach is to use some off-the-shelf open-vocabulary object detection models directly. However, as detailed in our newly appended Appendix A.4, due to the significant domain gap between visuals in Minecraft and the real world, direct application of these models to Minecraft results in inaccurate detection. In other words, they require adaptation to the Minecraft domain, implying additional costs associated with labeled data collection and fine-tuning. In contrast, our modification on MineCLIP does not demand further fine-tuning, allowing for immediate and effective implementation.\n\n**Other FPV games.** Similar to Minecraft, other FPV games also suffer from the domain gap between their visual styles and the real world more or less. As mentioned previously, if we want to adapt an off-the-shelf open-vocabulary detection model to a specific game domain, a labeled game dataset is required. Annotating object bounding boxes and names demands extensive human labor. However, the wealth of game videos on the Internet, such as those on YouTube, presents an alternative approach. These videos often come with subtitles, providing natural labels that can be utilized for training. Therefore, compared to adapting an object detection model, training a CLIP-like vision-language model (VLM), such as CLIP [1] and CLIP4Clip [2], on this massive data would be much more labor-saving and automated. Most importantly, MineCLIP has already demonstrated this route is feasible. Once a domain-specific CLIP-like model is obtained for a game, our method could be then implemented based on this VLM to guide the agent to learn basic skills in this game.\n\nIn summary, compared to tailored open-vocabulary detection models, our method could be nested in a general pipeline with VLM learning, which requires less labor and is more automated, for FPV games. We hope the discussion above addresses your concern. If you still have any additional questions or comments, please do not hesitate to let us know.\n\n[1] Radford et al., 2021, Learning Transferable Visual Models From Natural Language Supervision.\n\n[2] Luo et al., 2021, CLIP4Clip: An Empirical Study of CLIP for End to End Video Clip Retrieval."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3362/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700418465541,
                "cdate": 1700418465541,
                "tmdate": 1700419314776,
                "mdate": 1700419314776,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jwu5cG95qu",
                "forum": "Wx97sznZwB",
                "replyto": "aiipJrUST7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3362/Reviewer_6CKq"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3362/Reviewer_6CKq"
                ],
                "content": {
                    "comment": {
                        "value": "Dear authors, \n\n> However, as detailed in our newly appended Appendix A.4, due to the significant domain gap between visuals in Minecraft and the real world, direct application of these models to Minecraft results in inaccurate detection.\n\nThis is not True, because open-vocabulary detection easily generalizes in many video games just as well as CLIP (in fact many of them use CLIP). A while ago I tried [grounded-SAM](https://github.com/IDEA-Research/Grounded-Segment-Anything) on minecraft. Its open-vocabulary detection worked perfectly. I just tried a few images from you paper's Figure 3, and it worked perfectly with a fast speed. The model successfully detected sheep, flower, cow, even the sword. \n\nIn addition, I personally tried every single figure in your Appendix figure 9, following one negative word of \"grass\" which you also did in method. Grounded DINO was able to detect the object 6/6. In your added figure 11, grounded dino got 4/6 correct from your result, but the original prompt I tried is \"minecraft sheep\" for (d) and it succeeded, so I'd consider open-vocabulary detection to work in 11/12 cases. I believe you can potentially get better results with higher resolution screenshots. I think such result proves they are strong replacements even without minecraft specific thing.\n\nTo addresses my concerns, I believe results with grounded SAM for example, or results in other FPV domains are necessary, which can be use to defend the merit of your method."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3362/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700430236918,
                "cdate": 1700430236918,
                "tmdate": 1700545624824,
                "mdate": 1700545624824,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aa0zEPaIBg",
                "forum": "Wx97sznZwB",
                "replyto": "iUcwCPnKmq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3362/Reviewer_6CKq"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3362/Reviewer_6CKq"
                ],
                "content": {
                    "comment": {
                        "value": "Hi authors,\n\nIn case you don't have time to finish my requested additional experiments, there is another simpler thing that will help understanding a lot. Since we have contradicting experience with open-vocabulary detectors, it would be helpful to do some unbiased test about it, which will also enhance the evaluation of the paper.\n\nYou can conduct such an experiment with your method vs Grounded SAM detection and list the result side by side.\nFor data, take object from [pig, cow, sheep, mushrooom cow, tree, flower, horse, torch], google search \"minecraft [object name] screenshot\" in image tab, and take the first two images that includes the item and the has item fully in field of view.\nThis boils down to a total of 16 test images (although ideally you paper should have even more diverse examples)\n\nRun Grounded-SAM's huggingface demo with just one negative class \"grass\".\nYou can run your method to get bounding box with whatever negative classes you want, but the set of negative words have to remain the same across all images.  \n\nI believe such a test would be a fairer quantitative comparison."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3362/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700595251327,
                "cdate": 1700595251327,
                "tmdate": 1700595251327,
                "mdate": 1700595251327,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xKBWWyOOCn",
                "forum": "Wx97sznZwB",
                "replyto": "1jyLY3bGNQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3362/Reviewer_6CKq"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3362/Reviewer_6CKq"
                ],
                "content": {
                    "comment": {
                        "value": "Dear author, \n\nThis is good but could you follow my exact instruction for grounding-DINO? aka using only [cow, grass] for an image with cow, instead of a whole list of objects. I think this is a fairer comparison as many previous method already use LLM to derive object names to detect from instruction, and such as [this paper from 2022](https://nlmap-saycan.github.io/)\n\nI will consider raising my score after this is done as well as reading other reviewer's updates."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3362/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700664520285,
                "cdate": 1700664520285,
                "tmdate": 1700664520285,
                "mdate": 1700664520285,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "h8bG1lbGnm",
                "forum": "Wx97sznZwB",
                "replyto": "aiipJrUST7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3362/Reviewer_6CKq"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3362/Reviewer_6CKq"
                ],
                "content": {
                    "comment": {
                        "value": "Here is my final review after taking a look at other reviews' comments. I've edited response, score and confidence to reflect my final opinion.\n\nI appreciate the authors response to my questions about open-vocabulary detector. The additional ablations contribute to my score increase. However, as all other reviewers pointed out, the paper's method has a lot of limitation that contradicts open-world promise of the claim. \n\nWhile the promise of this paper / minecraft itself is about open-vocabulary, the presented method is limited to approaching objects in fpv setting. This is also reflected in the evaluation, where the task covered are no way near open-world. This also has been mentioned by multiple other reviewers. The authors should tune down their claim about open-world.\n\nAfter the rebuttal, I decide to lift my score from 3 to 5 for added result and also raise my confidence from 4 to 5. This confidence score is due to the fact that I had personal experience trying many components the paper used before ICLR (Dense CLIP, MineCLIP, inverse bbox size for distance, LLM extract objects from task, minecraft, open-vocab detection on minecraft), and have tried one of them on the figures the authors provided during rebuttal period. Such confidence score raise has no conflict of interest, as I don't have any in-submission or on-going paper that's close enough to the author's.\n\nI acknowledge current approach's merit, but would believe it has to belong to a more system/experiment heavy paper where such a reward only plays part of the role. Many many more diverse tasks, or a whole end-to-end decision making system would make this paper much stronger. At its current state, I reiterate my belief that such a reward design alone, under the broken promise of open-worldness, doesn't constitute the technical contribution a full ICLR paper needs. On the other hand, I think the paper is above a score of 3 once the authors would incorporate all our feedbacks. If my confidence score between 4 and 5 is an absolute tie breaker for the AC, I recommend accept this paper."
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3362/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700717508043,
                "cdate": 1700717508043,
                "tmdate": 1700718113393,
                "mdate": 1700718113393,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "PFqeNGXUb9",
            "forum": "Wx97sznZwB",
            "replyto": "Wx97sznZwB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3362/Reviewer_Q4te"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3362/Reviewer_Q4te"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a method for training an open-vocabulary policy on Minecraft tasks via RL guided by CLIP. Intrinsic rewards for the RL policy are computed based on the patch-wise CLIP similarity between the mentioned target object in a given language instruction and the image of the enviornment. Confidence maps based on the patch-wise similarity are passed into the policy in addition to visual information. The choices for the intrinsic rewards and inputs to the policy allow for some invariance in behavior to be learned across target objects, permitting some generalization when performing a seen task on an unseen object."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Overall, the paper is clear and easy to read.\n- The paper has some interesting insights about using CLIP guidance in the Minecraft context -- including the focal reward, the subtraction the probabilities of negative classes as a denoising procedure, and the way in which the patch similarities/probabilities are determined from MineCLIP.\n- The paper includes ample architecture and implementation details in the Appendix which promotes reproducibility."
                },
                "weaknesses": {
                    "value": "- The broad idea of using VLMs to allow for open-vocabulary manipulation with objects has been explored previously (e.g. MOO, Stone et al. 2023), though the paper does have some interesting insights about applying VLM guidance that are particular to the Minecraft setting, as mentioned above.\n- As is common with many reward shaping approaches, the hyperparameter $\\lambda$ must be tuned to determine the weighting on the focal reward. According to Figure 11 in the Appendix, the choice of this hyperparameter can have a significant effect on the results. Is the same value of $\\lambda$ optimal across multiple task families?\n- The approach does have some hand-crafted components that are specific to this domain. For example, the Gaussian kernel in the focal reward relies on the fact that interaction in Minecraft occurs \"when the cursor in the center of the agent view aligns with the target\" and also guides the agent to focus on a single target rather than multiple. How was this kernel chosen and how sensitive is the performance of the method to the specific choice of kernel? Another example is the negative word list; while effective for denoising, it is determined in a domain-specific fashion, so it is unclear if the benefit of this procedure would be helpful for other domains.\n- The choice of adding the unified 2D confidence map to the policy input is an interesting way to get some invariance across objects. But removing the natural language input constrains the policies to be single task instead of multi-task policies. What is the advantage of removing natural language? One rationale might be that unseen objects which are OOD for the policy do not have to be encoded by the text encoder--but these unseen objects are already being encoded by the visual encoder, so it is unclear if this is the reason. Was the choice of not including text as a policy input ablated?"
                },
                "questions": {
                    "value": "- How sensitive is $\\lambda$ across multiple task families?\n- How was the Gaussian kernel constructed?\n- Why were natural language instructions not included as an input to the policy?\n- Given that the language instructions are fairly simple, what is the rationale for using an LLM to find the target object?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3362/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3362/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3362/Reviewer_Q4te"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3362/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698830878749,
            "cdate": 1698830878749,
            "tmdate": 1699636286116,
            "mdate": 1699636286116,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "P8Ksk2bx9J",
                "forum": "Wx97sznZwB",
                "replyto": "PFqeNGXUb9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3362/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3362/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your efforts and valuable comments. We would like to address the weaknesses and questions you raised in the review.\n\n> Another example is the negative word list; while effective for denoising, it is determined in a domain-specific fashion, so it is unclear if the benefit of this procedure would be helpful for other domains.\n\nWe would like to clarify that constructing a negative word list is inherently domain-specific. For instance, in another domain, indoor navigation tasks, the list should be constructed to contain indoor objects rather than outdoor ones. Although the approach may vary with each domain, the concept of building a task-relevant negative word list to denoise and enhance segmentation effectiveness is universally applicable.\n\n> How sensitive is $\\lambda$ across multiple task families? Is the same value of $\\lambda$ optimal across multiple task families?\n\nIn our experiments, we observe that the average focal reward per step consistently falls into a range of 0.1 to 0.3 across all tasks due to the similar state spaces (all tasks are evaluated in Minecraft). The focal rewards' uniformity in order of magnitude across task families indicates the robustness of our $\\lambda$ selection, although it is derived from 'hunt a cow' task and 'hunt a sheep' task. To validate this, we conduct additional experiments on two tasks from the harvest task family, 'milk a cow' and 'shear a sheep', and add the results to Appendix B.4. As shown in Figure 20(a) and (b), $\\lambda=5$ still outperforms other values on these two tasks.\n\nWhile we acknowledge that $\\lambda=5$ may not be the exact optimal value for every individual task, it is a reasonably near-optimal choice (*e.g.* on the same order of magnitude as the optimal value), serving as a broadly effective parameter for our multi-task training.\n\n> How was the Gaussian kernel constructed? How sensitive is the performance of the method to the specific choice of kernel?\n\nThe Gaussian kernel in our method is constructed based on the simple principle that values should be higher near the center and decrease with distance. $\\sigma=(H/3,W/3)$ is an intuitive choice, aiming to maintain high contrast between the center and the edge, as well as between the edge and areas outside the field of view.\n\nTo quantitatively evaluate this intuition and answer the second question, we conduct additional experiments to test the sensitivity of our methods to Gaussian kernels with different variances and add the results to Appendix B.4. The experimental settings are the same as those in Figure 6(c) and (d). As illustrated in Figure 21, $\\sigma=(H/3,W/3)$ outperforms the other two choices. We suppose that the Gaussian kernel with $\\sigma=(H/2,W/2)$ is too wide to provide sufficient contrast between the center and the edge, while the Gaussian kernel with $\\sigma=(H/5,W/5)$ is too narrow to provide sufficient contrast between the edge and areas outside the field of view.\n\n> Why were natural language instructions not included as an input to the policy?\n\nWe do not include the natural language instruction as input to the policy primarily because the instruction cannot offer additional useful information beyond what is already provided by the 2D confidence map in our Minecraft experiments. The information in the language instruction, besides the target object, often consists of verbs like 'hunt', 'harvest', or 'collect'. However, such actionable information does not significantly aid in task completion. On the one hand, the action patterns required to complete tasks in Minecraft are relatively simple, *i.e.*, move towards the target object and take 'attack' or 'use' action. On the other hand, the mapping between verbs and action patterns is not one-to-one. For example, an instruction with the verb 'collect' may require the agent to either 'attack' or 'use' depending on the type of the target object. Therefore, we believe that language instruction cannot provide additional useful information for the agent's policy. \n\nWe acknowledge that in other domains like robotic manipulation, language instructions often contain essential information, such as verbs 'pick' and 'push' indicating totally different behavior patterns. Under this circumstance, taking as input the language instruction will be necessary."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3362/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700037595216,
                "cdate": 1700037595216,
                "tmdate": 1700586301526,
                "mdate": 1700586301526,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zmNdpmlFAp",
                "forum": "Wx97sznZwB",
                "replyto": "PFqeNGXUb9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3362/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3362/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> Given that the language instructions are fairly simple, what is the rationale for using an LLM to find the target object?\n\nThe use of an LLM is intended to address the challenge where the name of the target object is not explicitly mentioned in the language instruction. For example, consider the instruction 'obtain wool with shears'. Here, 'wool' is the target item to acquire, but the actual target object for the agent to approach and interact with is 'sheep', which does not appear in the instruction. It is hard to design a programmatical method to extract such an implicit target object from the instruction. Consequently, we resort to LLMs, which understand the underlying relationship between the target item (wool) and the target object (sheep). More examples are available in Appendix A.1.\n\n----------\n\nIf you have any additional questions or comments, please do not hesitate to let us know. We are more than happy to provide any further clarifications or information that may be helpful. If you find our responses satisfactory, we would be grateful if you could consider raising the score."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3362/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700037624470,
                "cdate": 1700037624470,
                "tmdate": 1700037702092,
                "mdate": 1700037702092,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ePJBTuUeUS",
                "forum": "Wx97sznZwB",
                "replyto": "zmNdpmlFAp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3362/Reviewer_Q4te"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3362/Reviewer_Q4te"
                ],
                "content": {
                    "title": {
                        "value": "Response to author"
                    },
                    "comment": {
                        "value": "Thank you for the response and the clarifications. The approach has some hand-crafted components that are specific to this domain, and further, is restricted to tasks that involve approaching an object, and is a bit sensitive to tuning. As other reviewers have mentioned, the contribution is somewhat limited in scope. I maintain my score of weak accept and will continue to monitor the discussions."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3362/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700598865796,
                "cdate": 1700598865796,
                "tmdate": 1700598865796,
                "mdate": 1700598865796,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]