[
    {
        "title": "Defending Against Transfer Attacks From Public Models"
    },
    {
        "review": {
            "id": "j5IWP9FjBS",
            "forum": "Tvwf4Vsi5F",
            "replyto": "Tvwf4Vsi5F",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7997/Reviewer_CVr6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7997/Reviewer_CVr6"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a pragmatic defence against a newly proposed practical threat model - namely adversaries utilising transfer attacks via publicly available surrogate models (with no ability to fine-tune models or perform query-based attacks). The defence is motivated by a game-theoretic perspective, and evaluated against a range of public models, attack algorithms and datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "A very well written and presented paper. The game-theoretic motivation is clear and provides a reasoned foundation for the approach taken - though that approach does not fully solve the game-theory problem as posed, the framework at least justifies the concept of the approach as more than just arbitrary/ad hoc.\n\nThorough experimental results including extensive ablation studies. The investigations, for example, of the effects of removing various groups of models on the accuracy was interesting. Further, investigating the cosine similarity of perturbations between attacks was suggestive of further structure to be explored in future work.\n\nThe candidate defence outperforms SOTA by a reasonable margin, and provides a practical mechanism in contrast to SOTA adversarial training. I think because of this practical performance requirement for their approach with still SOTA or better results, it could be a useful technique to use in practise or to consider developing further."
                },
                "weaknesses": {
                    "value": "The \"slogan\" is not very catchy! :-)"
                },
                "questions": {
                    "value": "In section 7.3, PUBDEF's vulnerability to white-box and surrogate attacks is mentioned. It would have been good to see some results on this if possible in order to give an idea of the degradation of performance that such weakening of the threat model would provide, i.e. effectively an ablation study on the threat model assumptions.\n\nIs there some value to considering a case in which the set of publicly available models visible to the attacker and defender are different. Maybe there are communities not visible to the defender, or perhaps it could be because of time-based issues (e.g. defender model generated and frozen but subsequent public models appear which are available to the attacker)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7997/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697846024976,
            "cdate": 1697846024976,
            "tmdate": 1699636985310,
            "mdate": 1699636985310,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1oDiTfJ6TL",
                "forum": "Tvwf4Vsi5F",
                "replyto": "j5IWP9FjBS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7997/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7997/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response to Reviewer CVr6"
                    },
                    "comment": {
                        "value": "Thank you for taking the time to review our paper. We really appreciate your comments and constructive feedback. Please see the answers and clarifications to your questions below.\n\n### White-box attacks\n\n> In section 7.3, PUBDEF's vulnerability to white-box and surrogate attacks is mentioned. It would have been good to see some results on this if possible in order to give an idea of the degradation of performance that such weakening of the threat model would provide, i.e. effectively an ablation study on the threat model assumptions.\n\nWe appreciate the suggestion. Against white-box attacks, PubDef is no better than an undefended model (~0% accuracy against a 100-step white-box PGD attack). We will add this statement in our revision.\n\nWe would like to also take the chance to re-emphasize that a core argument of our paper is that it is unnecessary in practice to make every model robust against white-box attacks. Instead, the defender should identify the most likely threats to their system and focus on those. If the model has to be completely public, then robustness against white-box attacks is necessary. Otherwise, using white-box adversarial training would be overkill, like \u201cusing a sledgehammer to crack a nut.\u201d \n\n### Unseen public models\n\n> Is there some value to considering a case in which the set of publicly available models visible to the attacker and defender are different. Maybe there are communities not visible to the defender, or perhaps it could be because of time-based issues (e.g. defender model generated and frozen but subsequent public models appear which are available to the attacker)?\n\nThe paper shows that not much robustness is lost if a defender is unaware of some public models that the attacker is aware of. Please see the discussion of unseen source models in Sections 6.2, 7.2, Table 2, Figure 4. To summarize, the accuracy under attack drops by less than 2 percentage points on CIFAR-10/100 and by 8 points on ImageNet under unseen attacks (unseen attack algorithm and unseen source models). For any other table or figure, we always report the accuracy against the strongest attack including both seen and unseen. In practice, we believe it is likely that defenders can identify most or all publicly available models.\n\nThat said, we have conducted an ablation study where **the defender only sees one random public model from each group.** See Table 9 in Appendix A.4.1. Here, PubDef is still significantly better than all the baseline models; on CIFAR-10, it has 10 and 9 percentage points higher clean and adversarial accuracy (7 and 11 pp. for CIFAR-100) than the best white-box adversarially trained models. We note that choosing randomly is completely unrealistic and is the worst possible strategy for the defender. There is no reason to believe that the defender would just randomly choose the source models to train against, so we expect that in practice the defense will perform significantly better than this, even if some models are not known to the defender.\n\nThank you for reading our rebuttal. Please don\u2019t hesitate to let us know if there are other questions or concerns we have not addressed."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7997/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700205824932,
                "cdate": 1700205824932,
                "tmdate": 1700205824932,
                "mdate": 1700205824932,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aJbjvHCURr",
                "forum": "Tvwf4Vsi5F",
                "replyto": "1oDiTfJ6TL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7997/Reviewer_CVr6"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7997/Reviewer_CVr6"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you so much for your responses. I retain my original overall review score."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7997/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700429960415,
                "cdate": 1700429960415,
                "tmdate": 1700429960415,
                "mdate": 1700429960415,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "NMDP97IDJD",
            "forum": "Tvwf4Vsi5F",
            "replyto": "Tvwf4Vsi5F",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7997/Reviewer_nPyj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7997/Reviewer_nPyj"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a defense against transfer-based evasion attacks. The defense uses publicly-available pretrained models to create adversarial attacks and incorporates these attacks in the training process of the defended model. The process is similar to adversarial training, however it uses adversarial examples from (multiple) other robust models rather than from the trained model itself."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ introduction is well written\n+ formulation is clear"
                },
                "weaknesses": {
                    "value": "- evaluation is not convincing\n- no analysis on how expensive the method is, compared, e.g., to adversarial training"
                },
                "questions": {
                    "value": "**Evaluation is not convincing, missing: white-box evaluation, query-based evaluation, other norms, targeted attacks, strong ensemble transfer attacks**\n\nThe defense should be tested fully, also against white-box evasion attacks. This is motivated by the fact that the authors are trying to make the model robust, hence they should demonstrate the robustness against worst-case adversaries. Otherwise, there is a high risk that the defense might be broken by stronger attacks. As the approach is similar to AT, the authors should demonstrate that the decision boundary of the model is in fact becoming more robust to unseen adversarial attacks and perturbations, thus they should also test gradient-based attacks against the model itself. Additionally, the authors should test against an attack that is generated ensembling the gradients of multiple models, which should improve transferability and also test effectively the robustness claim that the authors are making.\nThe model should also be tested with variations of the attacks, e.g., with PGD with logit loss, or against APGD with DLR loss. The authors should demonstrate that the defense generalizes against other unseen attacks and variations, to really claim robustness. The defense should be robust regardless of the method used to find the adversarial attacks. \nThe model should also be tested against query-based attacks to properly validate the fact that it is not suffering from gradient masking problems.\nFinally, all the parameters of the attacks should be specified to ensure they are conducted properly and they are not suffering from optimization issues.\n\n**No analysis on how expensive the method is, compared, e.g., to adversarial training**\n\nThe authors should discuss how expensive is to train a model with this technique, compared, e.g., with AT. Additionally, they should specify how costly is to add further heuristics.\nFor example, the authors use a heuristic to choose the models to use for creating the transfer attacks. However, this heuristic seems expensive, as it requires to compute the adversarial accuracy (in transfer) against all available models. Does this require to launch a full evaluation against all models?\n\n\n**Clarifications needed**\n\n- the authors state that they propose \"using all available system-level defenses\", however the presented method is clearly a ML-level defense. The authors should clarify this aspect, as it is confusing to read and there is no discussion in the rest of the paper on which system-level defenses are mounted on the model, or if they are mounted at all.\n\n- the authors state that the drawback of AT is that it degrades clean accuracy, however this does not seem to be the case https://robustbench.github.io. The authors should clarify this aspect by detailing this statement.\n\n- The method assumes that the defended is aware of the same set of public models as the attacker. However, with the proliferation of public models (also supported in the introduction of the paper), this might not be realistic to assume that the defender is aware of all possible models available. Furthermore, assuming that the attacker has $s \\dot a$ attack strategies might bring an excessive number of combinations. The authors should clarify these statements and propose solutions for these limitations. In fact, when the authors claim that the proposed approach \"achieves over 90% accuracy against all four transfer attacks\", they should also disclose that this holds only when the set of target models is the same as the source models $T = S$."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7997/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7997/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7997/Reviewer_nPyj"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7997/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697961250825,
            "cdate": 1697961250825,
            "tmdate": 1699636985195,
            "mdate": 1699636985195,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "r3rO5lcQ3x",
                "forum": "Tvwf4Vsi5F",
                "replyto": "NMDP97IDJD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7997/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7997/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response to Reviewer nPyj (1/3)"
                    },
                    "comment": {
                        "value": "Thank you for taking the time to review our paper. We really appreciate your comments and constructive feedback. Please see the answers and clarifications to your questions below.\n\n### Unseen attacks and public models\n\n> The authors should demonstrate that the defense generalizes against other unseen attacks and variations, to really claim robustness. \n\n> The method assumes that the defended is aware of the same set of public models as the attacker. \n\n**The paper shows that not much robustness is lost if a defender is unaware of some public models that the attacker is aware of.** This surprising effectiveness or generalization is a result of our technique to choose one source model from each group. Please see the discussion of unseen source models in Sections 6.2, 7.2, Table 2, Figure 4. To summarize, the accuracy under attack drops by less than 2 percentage points on CIFAR-10/100 and by 8 points on ImageNet under unseen attacks (unseen attack algorithm and unseen source models). For any other table or figure, we always report the accuracy against the strongest attack including both seen and unseen. In practice, we believe it is likely that defenders can identify most or all publicly available models.\n\nThat said, we have conducted an ablation study where **the defender only sees one random public model from each group.** See Table 9 in Appendix A.4.1. Here, PubDef is still significantly better than all the baseline models; on CIFAR-10, it has 10 and 9 percentage points higher clean and adversarial accuracy (7 and 11 pp. for CIFAR-100) than the best white-box adversarially trained models. We note that choosing randomly is completely unrealistic and is the worst possible strategy for the defender. There is no reason to believe that the defender would just randomly choose the source models to train against, so we expect that in practice the defense will perform significantly better than this, even if some models are not known to the defender.\n\n### White-box evasion attacks\n\n> The defense should be tested fully, also against white-box evasion attacks. This is motivated by the fact that the authors are trying to make the model robust, hence they should demonstrate the robustness against worst-case adversaries.\n\nThere appears to be a misunderstanding. The paper makes no claims about the robustness of PubDef against white-box attacks nor about the decision boundary. Against white-box attacks, PubDef is no better than that of an undefended model (0% accuracy against white-box attacks).  PubDef is not designed to provide security against white-box attacks and is not appropriate in scenarios where white-box attacks are possible, so such attacks are beyond the scope of this paper. We do not claim \"robustness\" in general; rather, we claim robustness against TAPM attacks. Achieving robustness against all white-box attacks, without significant loss of clean accuracy, is an open problem that has yet to be achieved despite concerted effort from the community.\n\nWe would like to re-emphasize that a core argument of our paper is that **it is not necessary in practice to make every model robust against white-box attacks.** Instead, the defender should identify the most likely threats to their system and focus on those. If the model has to be completely public, then robustness against white-box attacks is necessary. Otherwise, using white-box adversarial training is overkill, like \u201cusing a sledgehammer to crack a nut.\u201d \n\nWe believe that a more common situation is that the model does not need to be public. In that case, we can use systems-level defenses to make white-box attacks, query-based attacks, and transfer attacks from non-public models unattractive to an attacker, so that the top priority is to provide robustness against TAPM attacks; and we show that we are able to achieve a surprising level of robustness against such attacks, with low impact on clean accuracy. This is, as far as we are aware, a new insight, and it has significant implications for how to defend models in practice.\n\n### Variations of attacks\n\n> The model should also be tested with variations of the attacks, e.g., with PGD with logit loss, or against APGD with DLR loss. \n\nThere is already extensive research literature on variations of transfer attacks, and we experimented with 11 state-of-the-art schemes in the literature (which also includes APGD with DLR loss). We ensure that they are diverse by choosing ones that employ different mechanisms for improving transferability. While it is always possible to try even more variations, it is not our intention to innovate in transfer attacks, nor do we believe it to be necessary in our case. We also found that our defense generalizes well to \u201cunseen\u201d transfer attack algorithms that it wasn't trained against. See Sections 6.2 and A.5.2 (supplementary material)."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7997/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700205583338,
                "cdate": 1700205583338,
                "tmdate": 1700205731717,
                "mdate": 1700205731717,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TVbqwCyrdB",
                "forum": "Tvwf4Vsi5F",
                "replyto": "0NGe1LreCE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7997/Reviewer_nPyj"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7997/Reviewer_nPyj"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Reviewer nPyj"
                    },
                    "comment": {
                        "value": "I acknowledge the authors' responses, and I thank them for their attention in clarifying their work. \n\nMy points on the choice of public models, cost of training, clarification on the type of defenses, and clean accuracy of AT have been fully addressed.\n\nHowever, my crucial point on \"it is not necessary in practice to make every model robust against white-box attacks\" still stands and the fact that this defense does not improve even by a little the robustness against white box attacks is concerning. Why would one choose this method over AT, when AT provides robustness against black-box **and** white-box attacks? Furthermore, I expect the boundary to be more robust even with transfer attacks, but this is not the case. Thus, what is this model learning exactly? \n\nI also want to clarify that the ensemble attack that I requested is not the one that the authors show in their paper (where they take the best optimization result out of the pool of adversarial perturbations). The attack I wanted to see is one that uses for the optimization the mean gradient gathered from a set of models instead of separate runs of attacks. I apologize if that was not clear from my initial review.\n\nFor these observations, I retain my original score."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7997/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700467240920,
                "cdate": 1700467240920,
                "tmdate": 1700467240920,
                "mdate": 1700467240920,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "50fycG5ovF",
            "forum": "Tvwf4Vsi5F",
            "replyto": "Tvwf4Vsi5F",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7997/Reviewer_DtJG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7997/Reviewer_DtJG"
            ],
            "content": {
                "summary": {
                    "value": "The authors approach transfer attacks from the practical perspective and propose training procedure that allows to achieve robustness with a small drop in clean accuracy. They empirically show that their rather simple approach to selecting models allows good performance with respect to SOTA white-box defences. They do this by performing extensive evaluations on a wide range of public models"
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "-\tThe paper considers a real-life scenario of defending specifically against black-box attacks.\n-\tEvaluation is very extensive (considering different 264 combinations of source models and attack mechanisms)\n-\tSolid results with potentially high relevance for real-life industrial applications"
                },
                "weaknesses": {
                    "value": "The authors state the weaknesses and limitations of their approach quite fully in Section 7.3. I could add that although proposed heuristics and empirical results could be significant in practical applications, the methodological contribution of this work is rather incremental."
                },
                "questions": {
                    "value": "The authors state that PubDef achieves much better results than SOTA models from RobustBench. Where in the main paper can we find the clean and robust accuracy of these SOTA models to compare? I found the presentation of the results a bit confusing."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7997/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7997/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7997/Reviewer_DtJG"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7997/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698786694975,
            "cdate": 1698786694975,
            "tmdate": 1699636985051,
            "mdate": 1699636985051,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ytuBZXLRxd",
                "forum": "Tvwf4Vsi5F",
                "replyto": "50fycG5ovF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7997/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7997/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response to Reviewer DtJG"
                    },
                    "comment": {
                        "value": "Thank you for taking the time to review our paper. We really appreciate your comments and constructive feedback. Please see the answers and clarifications to your questions below.\n\n### Contributions\n\n> The authors state the weaknesses and limitations of their approach quite fully in Section 7.3. I could add that although proposed heuristics and empirical results could be significant in practical applications, the methodological contribution of this work is rather incremental.\n\nWe would like to add that the practical contribution is enabled by a scientific discovery on the generalization ability of the defense against unseen attacks. We did not emphasize this in the paper because it is not our main contribution, but we believe that this is a surprising result that has not been documented before. We attempt to explain this phenomenon in Section 7.2 through some empirical measurements. We also give our intuition in Appendix A.5.3. \n\n### SOTA models from RobustBench\n\n> The authors state that PubDef achieves much better results than SOTA models from RobustBench. Where in the main paper can we find the clean and robust accuracy of these SOTA models to compare? I found the presentation of the results a bit confusing.\n\nFor SOTA models from RobustBench, please see Table 1 (\"Best white-box adv. train\") for the clean and robust accuracy of these models. We compare our defense to the most robust models on [RobustBench](https://robustbench.github.io/) (at the time of submission) with the same architecture as our PubDef models.\n\nFor completeness, the white-box adversarially trained models we use (under \u201cBest white-box adv. train\u201d in all tables) are:\n- CIFAR-10, WideResNet-34-10: Addepalli et al., Scaling Adversarial Training to Large Perturbation Bounds, ECCV 2022.\n- CIFAR-100, WideResNet-34-10: Addepalli et al., Efficient and Effective Augmentation Strategy for Adversarial Training, NeurIPS 2022.\n- ImageNet, ResNet-50: Salman et al., Do Adversarially Robust ImageNet Models Transfer Better?, NeurIPS 2020.\n\nThank you for reading our rebuttal. Please don\u2019t hesitate to let us know if there are other questions or concerns we have not addressed."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7997/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700205399830,
                "cdate": 1700205399830,
                "tmdate": 1700205399830,
                "mdate": 1700205399830,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mCCkuRt4aU",
                "forum": "Tvwf4Vsi5F",
                "replyto": "ytuBZXLRxd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7997/Reviewer_DtJG"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7997/Reviewer_DtJG"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Thank you for your response. I am retaining my score."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7997/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700596257643,
                "cdate": 1700596257643,
                "tmdate": 1700596257643,
                "mdate": 1700596257643,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "TNbI4EXSj4",
            "forum": "Tvwf4Vsi5F",
            "replyto": "Tvwf4Vsi5F",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7997/Reviewer_nHrU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7997/Reviewer_nHrU"
            ],
            "content": {
                "summary": {
                    "value": "The paper argues that the common modality for attack on ML models (image classifiers are used in experiments) is going to be via transfer attacks on public models since companies are likely to keep their model weights protected. The paper then recommends defenders to adversarially train against a reasonable subset of  public models and find that works against L-inf norm attacks that are transferred from public models. They call their technique PUBDEF.\n\nThe authors present the work in a game-theoretic perspective where the attacker only has access to public models.\n\nThe authors acknowledge some of the limitations of the work. In my judgement, they are quite significant. They mention that if an attacker somehow is able to infer model weights (say by training a surrogate model), then they can bypass the defense. For typical classifiers, I think this is a significant concern. One limitation that they do not appear to consider is the possibility of black-box attacks directly on the model (no transfer attack needed) and do not evaluate their defense against a black-box attacker. Thus, the setting is somewhat limited in which the attacker can only do transfer attacks on the protected model and nothing else. \n\n\nOverall, the motivation for the paper, the defense approach used, and evaluation all need to be better."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The threat model of transfer attacks is well known. The authors assume that this is the main likely modality in practice. That assumption is likely a reasonable assumption only in very complex and large models. In most other cases, including datasets and models that the authors consider, there are other practical attack strategies, including blackbox attacks and model stealing attacks. To authors' credit, they acknowledge some of these limitations (especially model stealing attack via training a surrogate), but that doesn't make the assumption more realistic.\n\nGiven their assumption, their experimental results appear reasonable. The main contribution is that the defender can choose a small subset of public models against which to adversarially train their model. They require that public models be trained on the same task.  In practice, they find that such a model is often robust against a broad range of adversarial attacks that are restricted to using the public models."
                },
                "weaknesses": {
                    "value": "Weaknesses are unfortunately significant. \n\n-- The assumption underlying the paper is likely unrealistic in that most blackbox models will also permit querying. Thus, transfer attacks are not the only option for an adversary. Blackbox attacks are also a possibility and, in fact, may be the primary attack strategy. The proposed defenses were not evaluated against Square attack and other blackbox attack strategies.\n\n\n[Rebuttal]\n\nI considered the rebuttal provided so far.  The main concern its that the approach is susceptible to blackbox attacks (model stealing attacks are a potential issue as well, but I think the primary attack vector is likely to be blackbox attacks). An attacker will choose the best attack strategy available among many options, including transfer attacks, blackbox attacks, and model stealing attacks. The authors present preliminary results that showed that blackbox attacks would succeed and they would thus either need stateful defenses to work (which were unfortunately recently broken by Feng et al. 2023) or they need to combine their scheme with a noise-based blackbox defense where inputs are combined with noise before doing an inference. The latter combination showed some promise, exceeding the performance of a simple white box defense, but significantly lowering the reported natural accuracy and adversarial accuracy of PubDef by about 10% each in Table 1. \n\nI am raising my score, assuming that the authors are willing to include a deeper analysis of their work against blackbox attacks on CIFAR-10 and CIFAR-100 in the final paper and include a clear acknowledgment something along the following lines in bullet 3:\n\nBlackbox attacks are a potential concern against PubDef. When we started this work, a stateful defense was available, which would have thwarted blackbox attacks and could be combined with our scheme  with no or little change to reported results. Unfortunately, very recently,  Feng et al. broke current stateful defenses, but did not rule out potential new stateful defenses. If new stateful were to be developed, they should be combined with your scheme. In the absence of stateful defenses being available, we present preliminary findings  against a blackbox attacker (see Appendix, section X). They suggest that PubDef, unmodified, would be vulnerable to blackbox attacks and not provide an adequate defense. However, PubDef in combination with a noise-based defense shows some promise (see Appendix, Section X). For example, on the CIFAR-10 dataset, with a sigma of 0.02, PubDef  has an adversarial accuracy of 79.8 against the best attacker, with a natural accuracy of 88.9. This is a drop of adversarial accuracy  and natural accuracy of about 10% each  from the  results in Table 1 that only factored in transfer attacks, but still superior to simply using best white-box adversarial training in both clean and natural accuracy. [And then report additional results on other datasets and include them in the paper.]"
                },
                "questions": {
                    "value": "I recommend authors to consider a different setting for their work where transfer attacks are more common due to the complexity of stealing models or doing direct blackbox attacks. The setting would be attacks on generative AI models such as ChatGPT. Recent work shows that transfer attacks are possible on such models (see a 2023 paper by Zico Kolter, Matt Fredrikson and others that  was also mentioned in a recent NYTimes article) where the attacker is able to append a suffix to a prompt to cause the model to output something that can be considered harmful in some way. Unfortunately, though, adversarial training is not the defense approach that is used in such settings (at least until now). So, the approach presented by the authors in this paper is unlikely to work and may require significant changes. But, the setting is likely more realistic for the assumptions they rely on."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7997/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7997/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7997/Reviewer_nHrU"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7997/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699225971683,
            "cdate": 1699225971683,
            "tmdate": 1700755402500,
            "mdate": 1700755402500,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BbqMppZXhv",
                "forum": "Tvwf4Vsi5F",
                "replyto": "TNbI4EXSj4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7997/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7997/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response to Reviewer nHrU (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for taking the time to review our paper. We really appreciate your comments and constructive feedback. Please see the answers and clarifications to your questions below.\n\n### Black-box attacks\n\n> One limitation that they do not appear to consider is the possibility of black-box attacks directly on the model (no transfer attack needed) and do not evaluate their defense against a black-box attacker.\n\nWe do consider black-box attacks. There are two types of black-box attacks: transfer attacks and query-based attacks. We propose using \u201cthe right tool for the job\u201d: we propose defending against query-based attacks with system-level defenses, and defending against transfer attacks with PubDef's training method (an ML-based defense). As mentioned in the paper, there is an orthogonal line of research that develops system-level defenses against query-based attacks, such as stateful detection [1] and noise addition [2]. We do not attempt to innovate on such system-level defenses and recommend practitioners deploy such defenses in concert with PubDef.\n\nCloud API (e.g., Clarifai, Google Vision AI, or Azure AI Vision) is an application where PubDef can be very helpful. First, the model weights are private by default; this already stops white-box attacks. Then, system-level defenses like [1] or [2] can be deployed to stop the query-based attacks. Finally, when combined with PubDef, we have a system that addresses all the attack threat models effectively and minimally sacrifices utility.\n\nAlternatively, in settings where the cost of a failed attack is extremely high, PubDef can be deployed without any system-level defenses against query-based attacks. This means the adversary wants to come up with the best possible attack before submitting it to the victim system. This rules out query-based attacks as they require submitting lots of adversarial inputs most of which are failed attacks. For instance, with supervised biometric authentication, a few failed attempts at fooling a biometric authentication may lead to the adversary being arrested, as the attack is immediately detected and the adversary must be present in person. Another example is social media bots. Creating a fake account is very costly; it often requires some degree of identification (e.g., phone number or email address) and connection to real users. A failed attack would lead to these accounts being permanently banned.\n\n- [1] Stateful Detection of Black-Box Adversarial Attacks, https://arxiv.org/abs/1907.05587\n- [2] Random Noise Defense Against Query-Based Black-Box Attacks, https://arxiv.org/abs/2104.11470\n\n### Surrogate models\n\nWe agree it is likely possible to collect a large dataset, label those samples manually, train a surrogate model, and use it in a transfer attack (see Section 7.3, item 2). However, such an attack would likely be very expensive in practice, due to the labeling and the training cost. It will additionally require expertise in machine learning as well as in the attacked domain. In security, greatly increasing the cost of attacks is often sufficient to deter attackers or render attacks unprofitable. Often, if an attacker has sufficient funding and motivation, it may not be possible to stop them."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7997/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700205198113,
                "cdate": 1700205198113,
                "tmdate": 1700205318523,
                "mdate": 1700205318523,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "MSmcfWP5mD",
                "forum": "Tvwf4Vsi5F",
                "replyto": "TNbI4EXSj4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7997/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7997/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response to Reviewer nHrU (2/2)"
                    },
                    "comment": {
                        "value": "### Simple defense strategy (white-box adversarial training)\n\n> Another limitation of the work is why the defender only has to use public models for adversarial training. It appears that the defender should be able to adversarially train against white box attack model (since the defender knows all the weights), even if the attacks are less powerful and are transfer attacks. A standard defense should have been evaluated.\n\n> No evaluation against a simpler defense strategy of simply adversarial training the model using the standard techniques (e.g., PGD), assuming a white box attacker and then evaluating that defense against transfer attacks.\n\nWe appreciate the suggestion. We already evaluated it; the state-of-the-art white-box adversarially trained models lose to our method in both the clean accuracy and robustness against the transfer attacks. See Table 1 (\u201cBest white-box adv. train\u201d). We emphasize that we compare our defense to the most robust models (at the time of submission) on [RobustBench](https://robustbench.github.io/) that do not use extra or synthetic training data and have the same architecture as our PubDef models.\n\nFurthermore, deploying white-box adversarial training in practice is completely unrealistic; it incurs an unacceptably large penalty for clean accuracy. For instance, for ImageNet, this simple defense degrades clean accuracy from 80% to 63% (Salman et al., 2020). Our scheme completely avoids this problem.\n\nFor completeness, the white-box adversarially trained models we use (under \u201cBest white-box adv. train\u201d in all tables) are:\n- CIFAR-10, WideResNet-34-10: Addepalli et al., Scaling Adversarial Training to Large Perturbation Bounds, ECCV 2022.\n- CIFAR-100, WideResNet-34-10: Addepalli et al., Efficient and Effective Augmentation Strategy for Adversarial Training, NeurIPS 2022.\n- ImageNet, ResNet-50: Salman et al., Do Adversarially Robust ImageNet Models Transfer Better?, NeurIPS 2020.\n\n### GenAI\n\n> I recommend authors to consider a different setting for their work where transfer attacks are more common due to the complexity of stealing models or doing direct blackbox attacks. The setting would be attacks on generative AI models such as ChatGPT.\n\nThis would be an exciting application of our methods, but unfortunately, LLMs are far beyond our computational resources to evaluate. Training an LLM with adversarial training is significantly more expensive than training an LLM from scratch. As far as we are aware, no group has yet reported successful adversarial training of a non-trivial LLM. Using PubDef on an LLM directly would also require generating transfer attacks on the training and the test sets. The only transfer attack that reliably works against LLMs (Zou et al., 2023) takes about an hour to generate an attack for a single sample on an Nvidia A100 GPU, so applying adversarial training is infeasible with current methods.\n\n> Unfortunately, though, adversarial training is not the defense approach that is used in such settings (at least until now). So, the approach presented by the authors in this paper is unlikely to work and may require significant changes. But, the setting is likely more realistic for the assumptions they rely on.\n\nWe believe that the fact that white-box adversarial training does not work makes our approach even more attractive. As mentioned in Section 1, the two reasons white-box adversarial training is not used in practice are (1) the training cost and (2) the accuracy drop. Our method exactly deals with both of these issues; unlike adversarial training, it is more efficient to train, and it has a much better robust-accuracy trade-off.\n\nThank you for reading our rebuttal. Please don\u2019t hesitate to let us know if there are other questions or concerns we have not addressed."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7997/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700205255696,
                "cdate": 1700205255696,
                "tmdate": 1700205354116,
                "mdate": 1700205354116,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xAgpTQSfhI",
                "forum": "Tvwf4Vsi5F",
                "replyto": "TNbI4EXSj4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7997/Reviewer_nHrU"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7997/Reviewer_nHrU"
                ],
                "content": {
                    "title": {
                        "value": "Feedback to the authors on their response"
                    },
                    "comment": {
                        "value": "Current stateful defenses are now unfortunately broken. See the following paper:\n\nhttps://arxiv.org/abs/2303.06280\n\nI think you cite this work already.  I think there is a reasonable doubt as a result of this work  if one can rely on system-based defenses against blackbox attacks. An adaptive attacker, that factors in the noise injection, may also be able to overcome the noise-based method you mention -- at least, there is good reason to not entirely rely on such defenses. They also will degrade accuracy in the clean case. \n\nSo, I suggest disclosing the results against blackbox attacks as well, even if they make your defense look bad. That sets a potential bound on the effectiveness of the defense against a motivated adversary.  You can point out how the results would change if noise were to be injected in the queries and whether you can achieve close to reported adversarial accuracy with a reaonsable level of noise injection and not hurting clean accuracy significantly in that process. \n\n(My point in the above is that the issue of defending against a blackbox adversary is not entirely orthogonal. It impacts achievable clean accuracy and adversarial accuracy, given your assumptions.)\n\nThanks for clarifying that you considered white box defenses in your comparison in Table 1. The evaluation there seems reasonable."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7997/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700539663304,
                "cdate": 1700539663304,
                "tmdate": 1700540760397,
                "mdate": 1700540760397,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lhVycaXLMk",
                "forum": "Tvwf4Vsi5F",
                "replyto": "3bR7a5F5hg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7997/Reviewer_nHrU"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7997/Reviewer_nHrU"
                ],
                "content": {
                    "comment": {
                        "value": "Any results on noise-based blackbox defenses? How well do they combine with your defense?"
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7997/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700702369060,
                "cdate": 1700702369060,
                "tmdate": 1700702369060,
                "mdate": 1700702369060,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]