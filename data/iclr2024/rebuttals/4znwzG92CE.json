[
    {
        "title": "Habitat 3.0: A Co-Habitat for Humans, Avatars, and Robots"
    },
    {
        "review": {
            "id": "mNKQF5WEP4",
            "forum": "4znwzG92CE",
            "replyto": "4znwzG92CE",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2766/Reviewer_HoUA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2766/Reviewer_HoUA"
            ],
            "content": {
                "summary": {
                    "value": "This paper provides a simulator supporting humanoid avatars and robots for the study of collaborative human-robot tasks in home environments. Diverse and realistic human models are constructed, addressing the challenges of accurate modeling of the human body as well as the human-like appearance and motion diversity of the models. Besides, The platform supports interaction between a real person and a simulated robot via a mouse and keyboard inputs or VR interface, enabling human-in-the-loop simulation. This paper also investigates two collaborative human-robot interaction tasks, social navigation, and social reorganization, and provides insights into learned and heuristic baselines for both tasks in the simulator."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "- This work presents a Co-Habitat for Humans, Avatars, and Robots, offering a simulation environment for humanoids and robots within a wide range of indoor settings, which can promote the development of human-robot tasks in the Embodied AI field.\n- Habitat3 provides realistic human and robot simulation. In the process of realistic human modeling, this work addresses the challenges posed by efficiency realism and diversity in terms of appearance and movement.\n- This work develops a Human-in-the-Loop evaluation platform within the simulator, allowing the control of humanoid robots using a mouse, keyboard, or VR devices. It provides a method for interacting and evaluating with real humans. Furthermore, it supports data collection and reproducibility during the interaction, offering a convenient tool for further research.\n- This paper introduces two benchmark tasks for human-robot interaction, along with baselines for each task. This paper leverages end-to-end RL to study collaborative behaviors and examines the performance of various learning strategies. The Human-in-the-Loop evaluation in the social rearrangement task reveals potential avenues for improving social embodied agents.\n- This simulator can be used for end-to-end reinforcement learning for robot agents, significantly reducing the time required for reinforcement learning. It also provides a validation environment for a broader range of robot agents, thus reducing potential risks to the environment and humans."
                },
                "weaknesses": {
                    "value": "- Current robots are equipped with only a depth camera, human detector, and GPS, providing a relatively limited amount of information. It is worth considering whether additional sensor types, such as LiDAR and sound sensors, can be integrated in the future to enhance obstacle avoidance and navigation capabilities.\n- It has been noted that in human simulation, fixed hand movements can lead to a decrease in visual accuracy. It may be considered to address the deformability of the skin during the simulation process and create hand motion animations during activities like grasping and walking.\n- This simulator has a wide range of potential applications and can be further explored to implement other embedded artificial intelligence tasks, such as visual-language navigation."
                },
                "questions": {
                    "value": "See Weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2766/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698734072265,
            "cdate": 1698734072265,
            "tmdate": 1699636219488,
            "mdate": 1699636219488,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "QXQBpYAfUD",
                "forum": "4znwzG92CE",
                "replyto": "mNKQF5WEP4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2766/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2766/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer HoUA"
                    },
                    "comment": {
                        "value": "We thank reviewer *HoUA* for the insightful feedback. The questions and concerns are addressed below. \n\n**It is worth considering whether additional sensor types, such as LiDAR and sound sensors, can be integrated in the future to enhance obstacle avoidance and navigation capabilities.**\n\nThank you for your suggestion. We agree that incorporating these sensors is useful, and it is on our roadmap. Since our work builds upon the Habitat platform, it already provides support for some of these sensors (for example, sound in [1]) and it would not be difficult to integrate them into future  tasks. \n\n**Fixed hand movements can lead to a decrease in visual accuracy. It may be considered to address the deformability of the skin during the simulation process and create hand motion animations during activities like grasping and walking.**\n\nThank you for your comment. While we did not model hand movements in our work, we use the SMPL-X body format to represent the humanoids, which allows us to specify hand and finger positions. More realistic hand movement could be addressed by recording hand keypoints in motion capture data or by using generative models for hand poses [2].\n\n\n**This simulator has a wide range of potential applications and can be further explored to implement other embedded artificial intelligence tasks, such as visual-language navigation.**\n\nThank you for highlighting the potential of the simulator for a wide range of applications. Given that Habitat 3.0 is based on the Habitat platform, it automatically supports well-studied Embodied AI tasks [3,4,5]. Furthermore, it opens the opportunity to update many of the existing tasks to include humanoids (e.g., \u201cwalk to the human as soon as they sit on the chair\u201d, for visual-language navigation) or to evaluate them using our human-in-the-loop tool. \n\n\n**References.**\n\n[1]  Chen, C., Schissler, C., Garg, S., Kobernik, P., Clegg, A., Calamia, P., ... & Grauman, K. (2022). Soundspaces 2.0: A simulation platform for visual-acoustic learning. Advances in Neural Information Processing Systems (NeurIPS)\n\n[2] Romero, J., Tzionas, D., & Black, M. J. (2017). Embodied hands. ACM Transactions on Graphics, 36(6).\n\n[3] Krantz, J., Lee, S., Malik, J., Batra, D., & Chaplot, D. S. (2022). Instance-Specific Image Goal Navigation: Training Embodied Agents to Find Object Instances. arXiv preprint arXiv:2211.15876.\n\n[4] Szot, A., Clegg, A., Undersander, E., Wijmans, E., Zhao, Y., Turner, J., ... & Batra, D. (2021). Habitat 2.0: Training home assistants to rearrange their habitat. Advances in Neural Information Processing Systems (NeurIPS).\n\n[5] Chaplot, D. S., Gandhi, D. P., Gupta, A., & Salakhutdinov, R. R. (2020). Object goal navigation using goal-oriented semantic exploration. Advances in Neural Information Processing Systems (NeurIPS)."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2766/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700099318427,
                "cdate": 1700099318427,
                "tmdate": 1700099318427,
                "mdate": 1700099318427,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "AP0VMSQyzK",
            "forum": "4znwzG92CE",
            "replyto": "4znwzG92CE",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2766/Reviewer_ndxG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2766/Reviewer_ndxG"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents Habitat 3.0, an Embodied AI simulator designed to facilitate research in human-robot interaction and collaboration within complex indoor environments. The proposed platform aims to address the need for efficient simulation tools to study AI agents' capabilities in realistic and diverse human-robot interaction scenarios. The main contributions of the paper are as follows:\n\nDiverse Humanoid Simulation: Habitat 3.0 offers a framework for creating and simulating diverse humanoid avatars. These avatars encompass various appearances and motions, enhancing the realism of agent interactions within the simulated environments. By employing techniques like skeletal models and linear blend skinning, the simulator achieves a balance between efficiency and visual fidelity.\n\nHuman-in-the-Loop (HITL) Evaluation Tool: The paper introduces a HITL interface that allows real human operators to control humanoid avatars within the simulated environment. This tool enables online human-robot interaction evaluations and data collection, providing a unique platform for studying how AI agents collaborate with humans.\n\nSocial Navigation and Social Rearrangement Tasks: The paper explores two collaborative tasks\u2014social navigation and social rearrangement\u2014to assess AI agents' ability to interact with human or humanoid partners. These tasks require the agents to find and follow humans at a safe distance or to collaborate with a humanoid in rearranging objects within the environment.\n\nEvaluation of AI Agents: The paper compares multiple AI agent baselines in both automated and HITL evaluation settings. These agents include heuristic experts and end-to-end reinforcement learning policies. The evaluations highlight the agents' adaptability in working with different partners to some extent and provide insights into their efficiency and success rates.\n\nRobust HITL Assessments: By conducting HITL evaluations involving real human participants, the paper evaluates AI agents' coordination abilities in scenarios involving diverse partners. These assessments help in understanding how baseline AI agents impact human efficiency and reveal insights into the dynamics of human-robot interactions."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Quality:\nThe paper demonstrates a fair level of quality in its methodology and execution. The development of Habitat 3.0 is well-detailed and addresses a clear need in the field of HRI research. The simulator provides an effective platform for investigating human-robot interaction. The evaluation of AI agents in both automated and HITL settings enhances the paper's quality.\n\nClarity:\nThe paper is generally well-written and clear, with detailed explanations of the simulator framework and the tasks studied. \n\nOriginality:\nHabitat 3.0 introduces valuable original contributions to the field. The combination of diverse humanoid simulation, HITL control, and the study of social navigation and rearrangement tasks provide some initial steps and ideas in this domain. Furthermore, the paper explores HITL evaluations involving human participants, which adds to the originality of the work. While the components themselves are not entirely novel, their integration and application within a single framework is original and significant.\n\nSignificance:\nHabitat 3.0, with its focus on embodied AI and human-robot interaction, addresses a critical aspect of AI development. The simulator has the potential to open new avenues for research, including collaborative AI, human-robot teamwork, and social embodied agents. The HITL evaluations are particularly significant, offering insights into how AI agents impact human performance and behavior. The paper's findings and methodology are likely to influence future research in these domains.\n\nOverall, its strengths can be listed as:\n- focus on human-robot interaction compared to previous platforms offering single-agent or multi-homogeneous-agent training.\n- efficient simulation implementation that enables faster progress in training/evaluating developed algorithms.\n- human-in-the-loop evaluation tool that can open up interesting use-cases and approaches to improve and analyze HRI methods.\n- a fair amount of evaluations."
                },
                "weaknesses": {
                    "value": "The critical weaknesses are:\n- currently, the focus of simulations seem to be more on the visual realism, which is a valid concern. However, the movement of the agents lacks physical realism, which hinders the extend of how human-robot interaction can be evaluated accurately. \n\n- the focus of this work is not proposing novel learning algorithms but still the results indicate that none of the baselines achieve useful following rate (F) nor feasible collision rate (CR) in the social navigation task. Similarly, for the social rearrangement task, none of the methods seem to generalize and let the robots assist their partners effectively (checking the relative efficiency (RE) metric). Even for HITL evaluations, which would be simpler since humans adjust to robots on the fly, the results are not encouraging. This, then, makes it harder to evaluate and take some insights from these evaluations, which is a major component of the paper.\n\n- humanoid shape diversity has been considered, however, robotic agent diversity was not addressed. \n\n- similar to the previous point, the platform, as it is now, lacks task diversity as well.\n\n- it feels like (looking at the efficiency improvements (RE metric) when collaborated vs. solo cases) maybe the tasks do not offer enough opportunities for collaboration. \n\n- personally, I find the HITL evaluations more interesting, however, the paper does not cover detailed evaluation and analysis of these experiments."
                },
                "questions": {
                    "value": "- what are possible solutions/integrations to alleviate the unrealistic humanoid locomotion problem, i.e., the agent first rigidly rotates to face the next target waypoint and then follows a straight path. The autonomous agents trained against such human movement models will not be directly transferable to real-world settings, nor the analysis would not be informative.\n\n- it is unclear how easy and flexible to import motion capture data. Can you elaborate on that?\n\n- it is also unclear how trivial it is to use the AMASS dataset along with VPoser to compute humanoid poses and then to import them into the simulator. Trying to use such external tools that the benchmark providers do not support/maintain themselves frequently becomes a huge hassle and ease-of-use of such external tools is critical, so can you also provide some clarification on their integration and/or usability? \n\n- About reliance on VPoser: Depending on the complexity of the task, simple interpolation between poses might not be sufficient, what would be possible solutions?\n\n- is it possible to incorporate physical collaboration scenarios, i.e., partners acting on the same object? would it require additional steps than what was explained on the paper?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2766/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2766/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2766/Reviewer_ndxG"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2766/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698766910839,
            "cdate": 1698766910839,
            "tmdate": 1699636219382,
            "mdate": 1699636219382,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "stIdEfLmTS",
                "forum": "4znwzG92CE",
                "replyto": "AP0VMSQyzK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2766/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2766/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer ndxG (Part 1)"
                    },
                    "comment": {
                        "value": "We thank reviewer *ndxG* for the insightful feedback. The questions and concerns are addressed below. The references are in the last part of the response.\n\n**The movement of the agents lacks physical realism, which hinders the extent of how human-robot interaction can be evaluated accurately.**\n\nHabitat 3.0 supports a wide range of humanoid motion, with motion coming from learning-based motion generation models, MoCap data, or by simulating the humanoid dynamically using the Bullet physics backend, such that it responds to forces, gravity, obstacles or momentum. In this work, we choose to simulate the humanoid with some assumptions for efficient simulation, which still achieve a higher level of realism compared to the current Embodied AI simulators with humanoid support, such as VirtualHome [9] or Co-Gail [10]. \n\n\nHowever, there is another deeper question in the reviewer\u2019s comment \u2013 how accurate does the simulator need to be? No simulation is perfect, and it is an open question how much physical realism is needed for sim2real transfer. Prior work [11] has shown that dynamic simulation and high fidelity physics might not necessarily lead to better sim2real transfer in navigation. We believe that an analogous systematic sim2real evaluation is necessary for Social Navigation and Social Rearrangement in the context of humanoid simulation to judge the level of realism needed. This is on our roadmap for future work.  \n\n\n**The results indicate that none of the baselines achieve a useful following rate (F) nor feasible collision rate (CR) in the social navigation task.**\n\nWe believe that only focusing on F alone does not provide an accurate picture of our results. First, note that our RL baseline can almost perfectly locate a moving human in an unseen house, with a success rate of 97%. The Heuristic Expert-designed baseline (which has access to a privileged map of the environment and a path planner), obtains a follow rate of 0.51 and collision rate of 0.52, showing that this is a challenging task, deserving further study. In comparison, the RL baseline achieves a following rate of 0.44, and collision rate of 0.51, without any privileged information. Finally, the following metric (F) is highly influenced by the time it takes to find the human (p). This shows that, while there is room for improvement, our baseline is able to fairly accurately follow the human around cluttered environments. Our experiments establish that these are indeed tasks where state-of-the-art approaches suffer at achieving competitive performance, further enhancing the significance of our work, and opening venues for future research.\n    \n\n**For the social rearrangement task, none of the methods seem to generalize and let the robots assist their partners effectively (checking the relative efficiency (RE) metric). Even for HITL evaluations, which would be simpler since humans adjust to robots on the fly, the results are not encouraging.**\n\nWhile there is room for improvement in our current results, our baselines actually show improvement over performing the task alone. Two agents can, at most, have a relative efficiency of 200%, which would occur if both perfectly split the tasks and did not need to avoid each other (quite unlikely in a typical scenario). Therefore, in both our automated and HITL results, where the relative efficiency is above 100%, our agents effectively assist humans (a maximum of 159.2% and 109.75% for seen and unseen partners, respectively, and 133.80% in the HITL setting). While we agree that human adaptation should further enhance task efficiency in the HITL results, humans also take time to infer their partner's actions, resulting in idle steps that reduce the overall efficiency. The drop in RE for unseen partners further underlines the challenging nature of our tasks, where state-of-the-art approaches have room for improvement.\n\n\n**It feels like (looking at the efficiency improvements (RE metric) when collaborated vs. solo cases maybe the tasks do not offer enough opportunities for collaboration.**\n\nRE is not necessarily a measure of how collaborative a task is since it also measures the ability of a particular approach at improving efficiency over solo execution. In fact, some of our baselines such as Learn-Single (where the agent is trained with a single humanoid policy) offer non-trivial efficiency improvement (160%) when paired with seen partners. This shows that the current social rearrangement task does offer opportunities for collaboration, and merits further study in future work, especially when paired against unseen partners."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2766/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700098801739,
                "cdate": 1700098801739,
                "tmdate": 1700149022763,
                "mdate": 1700149022763,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "f0sbQpsQKz",
                "forum": "4znwzG92CE",
                "replyto": "AP0VMSQyzK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2766/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2766/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer ndxG (Part 2)"
                    },
                    "comment": {
                        "value": "**Robotic agent diversity was not addressed.**\n\nHabitat 3.0 already supports a range of robots (Stretch, Spot, Franka, Fetch) and has the ability to include new robot models (via URDF files) since it is based on the Habitat platform.\n\n\n**Task diversity.**\n\nSince our work builds upon the Habitat platform, it automatically supports social variants of a wide range of Embodied AI tasks, including Image [1], Object [2] or Language [3] based navigation and rearrangement [4], or learning scene representations with embodied agents [5]. In this work, we propose two example tasks that build on navigation and rearrangement, highlighting challenges in human-robot collaboration. However, Habitat 3.0 opens the possibility to define a wide range of tasks, or even update currently studied tasks with humanoid simulation and human-in-the loop interaction. Some ways in which the tasks could be adapted include adding humanoids with complex motions (such as the ones shown in the supplementary video), asking agents to answer questions about the humanoid behavior, anticipating when a humanoid would fall, and performing manipulation tasks in close proximity or jointly with the simulated humanoids such as performing object handovers, or jointly carrying large objects. \n\n\n\n**I find the HITL evaluations more interesting, however, the paper does not cover detailed evaluation and analysis of these experiments.**\n\nAppendix G provides more analysis of the HITL experiments. Specifically, we provide post-hoc pairwise comparisons and significance results between solo execution and the two methods that we evaluate in HITL experiments. \n\n\n**The agent first rigidly rotates to face the next target waypoint and then follows a straight path. What are possible solutions/integrations to alleviate this?**\n\nTwo possible solutions to alleviate the simplifying assumption made in our work would be: (1) turning the humanoid before reaching the waypoint in combination with a forward motion, resulting in a smoother curve around the waypoint or (2) training a motion generation model conditioned on waypoints using motion captured data, such as [8]. \n\n\n**It is unclear how easy and flexible it is to import motion capture data. Can you elaborate on that?**\n\nHabitat 3.0 represents human motion as a sequence of joint rotations, using the same joints as in the SMPL-X body format. We provide a script to process a SMPL-X pose to be played in Habitat 3.0. As such, any motion capture data can be imported into Habitat by converting it into the SMPL-X format (using open-source tools such as MoSh++ [12]). The script to convert from SMPL-X to the Habitat format can be found in the link below, and will be open sourced.\nhttps://anonymous.4open.science/r/habitat-lab-C5B4/habitat-lab/habitat/utils/convert_smplx_to_habitat.py \n\n**It is also unclear how trivial it is to use the AMASS dataset along with VPoser to compute humanoid poses and then to import them into the simulator.**\n\nAs mentioned above, we provide a script to convert SMPL-X poses into the Habitat 3.0 format. Since VPoser outputs poses in the SMPL-X format, these can directly be played by running the given script, and since AMASS has been processed in the SMPL-X format, it can be directly played as well."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2766/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700099009701,
                "cdate": 1700099009701,
                "tmdate": 1700100236039,
                "mdate": 1700100236039,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tuM9cHxtzk",
                "forum": "4znwzG92CE",
                "replyto": "aaRQsQULtE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2766/Reviewer_ndxG"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2766/Reviewer_ndxG"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your explanations. I still think the analyzed task diversity along with some highly simplifying assumptions on the human movement and physical interactions limit the potential of the benchmark in its current form (some of which were also highlighted by other reviewers). Having said that, the study offers a new environment that can pave the way for future improvements. I will re-consider my evaluation based on your clarifications."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2766/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700649764507,
                "cdate": 1700649764507,
                "tmdate": 1700649764507,
                "mdate": 1700649764507,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "wJ8ITufGCr",
            "forum": "4znwzG92CE",
            "replyto": "4znwzG92CE",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2766/Reviewer_FYWe"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2766/Reviewer_FYWe"
            ],
            "content": {
                "summary": {
                    "value": "- This work introduces Habitat 3.0, a simulation platform for studying human-robot collaboration in home environments.\n- The environment includes accurate humanoid simulation and a human-in-the-loop infrastructure for real-time interaction and provides a way to evaluate different robot policies with real human collaborators.\n- It also allows the exploration of collaborative tasks: Social Navigation and Social Rearrangement. \n- Finally, the authors demonstrate that learned robot policies can effectively collaborate with unseen humanoid agents and human partners. Shows emergent behaviors during task execution, such as yielding space to humanoid agents."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This work complements existing works (e.g. Habitat, VirtualHome, etc.) in a human-centric way. It allows more flexible human models, human-object interactions, human-robot interactions, and human-in-the-loop evaluation, which are often ignored in previous works.\n2. This work is in general well-written, providing a good survey for the field of embodied AI environments.\n3. This work designs two social/collaborative tasks, navigation and rearrangement, and shows promising results."
                },
                "weaknesses": {
                    "value": "The main limitation of this work lies in universality. While I believe this work is interesting and helpful to the field, I am wondering if it could be scaled to incorporate more elements, supporting more tasks, so that the progress wouldn't stop here at the two example tasks. While I understand that these aspects might be beyond the scope of a single work, it would be beneficial to demonstrate, or at least discuss, how future works can develop upon Habitat 3.0. For example,\n- physics simulation \n- fine-grained object interactions\n- sim2real deployment"
                },
                "questions": {
                    "value": "1. For social arrangement, what is the motivation for using population-based approaches? More discussions would be helpful to understand the setting.\n2. Discuss relevant previous works. What is the relationship between the proposed social navigation task and visual tracking (e.g. [a])? Seems quite similar and more discussions are needed. Besides, [a] also contains humanoid simulation and end-to-end RL with Conv+LSTM.\n3. For object interaction, Sec. 3.1 explains that \"Once the hand reaches the object position, we kinematically attach or detach the object to or from the hand.\" Are all the objects simplified as a point particle? Are all the objects in the environment interactive? Is it possible to add more properties to them (e.g. geometry - shape, physics - weight, material, etc.)? It would be great to explore/discuss how to incorporate more general and complex object interactions, from pre-defined motion primitives (e.g. lie, sit) to freeform actions (e.g. grasp).\n4. Currently this work focuses restricted set of motions while more motions can be potentially added with the SMPL-X representation. In demo video 2:56, it also discusses complex motions (e.g. wiping, knocking). It would be beneficial to discuss how future works can incorporate more motions with interactive objects and form meaningful tasks. \n5. Another question is, what kind of tasks can Habitat 3.0 support in addition to navigation and arrangement? Again, I understand that these two tasks are already great for this work, but more discussions on the potential of Habitat 3.0 would make this work more general and influential.\n6. I wonder if the HITL tools would also be standardized and open-source.\n\n[a] End-to-end Active Object Tracking via Reinforcement Learning, ICML 2018."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2766/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2766/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2766/Reviewer_FYWe"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2766/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698899192158,
            "cdate": 1698899192158,
            "tmdate": 1699636219320,
            "mdate": 1699636219320,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "LXaAla0lUf",
                "forum": "4znwzG92CE",
                "replyto": "wJ8ITufGCr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2766/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2766/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer FYWe (Part 1)"
                    },
                    "comment": {
                        "value": "We thank reviewer *FYWe* for the insightful feedback. The questions and concerns are addressed below. The references are in the last part of the response.\n\n**Universality in terms of (1) physics simulation, (2) sim2real deployment, (3) fine-grained object interaction.**\n\nNote that Habitat 3.0 is built on top of the Habitat platform, providing support for physics simulation, a wide range of tasks and examples of sim2real transfer. We provide more details below:\n\n- Physics simulation: Habitat 3.0 supports physics simulation via bullet, allowing to specify physical properties for every object or agent and to simulate rigid-body dynamics (forces, collisions, etc). Our current work primarily uses a \u2018kinematic\u2019 simulation, which does not model dynamics. Our future work will include more sophisticated physical simulation for human-robot interaction, such as joint object manipulation and handovers.\n\n- Sim2Real deployment: Previous works leveraging the Habitat platform have established a track record of sim2real deployments in the context of navigation [1] and mobile manipulation [2]. Since Habitat 3.0 uses the same physics and rendering engine, and all policies in our submission use depth-sensors - which have shown to have low sim2real gap [1,3] - we believe that there is potential for sim2real deployment in the tasks we study. We leave this as a venue for future work.  \n\n- Fine Grained Object Interaction: In this work, our focus was primarily on high-level interactions such as task division and reducing space interference with the other agent. In our future work, we will consider more complex interactions such as lying and sitting for our tasks. However, the inclusion of features such as dexterous manipulation remains outside the current scope of Habitat 3.0. We believe other simulators and physics engines (e.g., IsaacSim) are particularly well-suited and designed for these tasks.\n\n\n\n**Universality in terms of tasks.**\n \nWe propose social variants of two commonly studied tasks in Embodied AI simulators (i.e. navigation and rearrangement), adapting them to human-robot collaboration scenarios. Similarly, we can create social variants of other embodied AI tasks, such as embodied question answering [10], object [7], image [8] or language navigation/rearrangement [9] by incorporating the humanoids. Furthermore, the simulator can be potentially used for other human-centered tasks, such as long-term scene-dependent motion generation with language input, anticipation of human actions, and human intent inference since it enables generating motions for humanoids and it includes the human-in-the-loop infrastructure to evaluate the tasks with real people. \n\n\n**For social rearrangement, what is the motivation for using population-based approaches?**\n\nThe goal in social rearrangement is to build robot agents that can rearrange objects together with humans, who can perform this task in different ways. An effective robot agent needs to adapt to its partner\u2019s preferences to effectively assist them. We frame this as a zero-shot coordination problem and refer to [6], which has demonstrated that training agents with a diverse population enables this type of adaptation. Note that we also present a baseline (Learn-Single) which does not use a population, and instead learns to coordinate with a single partner. Our experiments show that this baseline performs worse than population-based approaches at zero-shot coordination with unseen partners (in both automated and human-in-the-loop settings).  \n\n**Relationship between Active Object Tracking and Social Navigation.**\n\nThank you for the reference; we have updated the paper with the relationship of our task with Luo et al. [5]. We note two important differences between [5] and the SocialNav task: First, in [5] there is no robot to control, and thus, does not consider important human-robot interaction aspects that we observe in our work, such as being aware of the robot\u2019s physical extent, avoiding human-robot collisions by maintaining a safe distance or backing up when necessary to unblock the human. Second, [5] assumes that the human to track is mostly in the field of view of the robot, whereas in our case, the robot starts far away from the human and thus has to first explore the environment to find the human (Find phase) before following and tracking it around the environment (Follow phase)."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2766/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700097374202,
                "cdate": 1700097374202,
                "tmdate": 1700194660426,
                "mdate": 1700194660426,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Mm2xdiY0pR",
                "forum": "4znwzG92CE",
                "replyto": "wJ8ITufGCr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2766/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2766/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer FYWe (Part 2)"
                    },
                    "comment": {
                        "value": "**Clarification for Section 3.1. Are all the objects simplified as a point particle? Are all the objects in the environment interactive? Adding properties to objects.**\n\nObjects are *not* represented as point particles; instead, they have volumetric collision meshes and physical properties, which can be edited by researchers. Some objects can also be articulated allowing agents to open doors or cabinets. For the experiments in this work, we do not simulate dynamics (forces, momentum, friction, etc.) and kinematically attach the object volume to the agent arm, but the simulator allows for dynamic simulation. We have clarified this in the updated revision.  \n\n\n**It would be beneficial to discuss how future works can incorporate more motions with interactive objects and form meaningful tasks.**\n\nOur work provides an interface to represent human poses or motions via the SMPL-X format (i.e. as a sequence of rotations for each human joint). As such, any model or system that can generate these poses, could also be used in Habitat 3.0. Researchers could for example use [4], to generate human motion from language descriptions, allowing for tasks that require robots to reason about these motions (for instance, wait for a person to sit in the chair before bringing them a glass of water). \n\n\n\n**I wonder if the HITL tools would also be standardized and open-source.**\n\nYes, we will open-source the HITL tool, together with the simulator and experiment code. The source code for the HITL tool can be found in the following anonymous link: https://anonymous.4open.science/r/habitat-lab-C5B4/README.md.\n\n\n**References.**\n\n[1] Truong, J., Chernova, S., & Batra, D. (2021). Bi-directional domain adaptation for sim2real transfer of embodied navigation agents. IEEE Robotics and Automation Letters, 6(2), 2634-2641.\n\n[2] Yokoyama, N., Clegg, A. W., Undersander, E., Ha, S., Batra, D., & Rai, A. (2023). Adaptive Skill Coordination for Robotic Mobile Manipulation. arXiv e-prints.\n\n[3] Truong, J., Rudolph, M., Yokoyama, N. H., Chernova, S., Batra, D., & Rai, A. (2023). Rethinking sim2real: Lower fidelity simulation leads to higher sim2real transfer in navigation. In Conference on Robot Learning (CoRL).\n\n[4] Tevet, G., Raab, S., Gordon, B., Shafir, Y., Cohen-or, D., & Bermano, A. H. (2022). Human Motion Diffusion Model. In International Conference on Learning Representations (ICLR).\n\n[5] Luo, W., Sun, P., Zhong, F., Liu, W., Zhang, T., & Wang, Y. (2018). End-to-end active object tracking via reinforcement learning. In International Conference on Machine Learning (ICML).\n\n[6] Szot, A., Jain, U., Batra, D., Kira, Z., Desai, R., & Rai, A. (2023). Adaptive Coordination in Social Embodied Rearrangement. In International Conference on Machine Learning (ICML).\n\n[7] Chaplot, D. S., Gandhi, D. P., Gupta, A., & Salakhutdinov, R. R. (2020). Object goal navigation using goal-oriented semantic exploration. Advances in Neural Information Processing Systems (NeurIPS).\n\n[8] Krantz, J., Lee, S., Malik, J., Batra, D., & Chaplot, D. S. (2022). Instance-Specific Image Goal Navigation: Training Embodied Agents to Find Object Instances. arXiv preprint arXiv:2211.15876.\n\n[9] Krantz, J., Wijmans, E., Majumdar, A., Batra, D., & Lee, S. (2020). Beyond the nav-graph: Vision-and-language navigation in continuous environments. In European Conference on Computer Vision (ECCV).\n\n[10] Das, A., Datta, S., Gkioxari, G., Lee, S., Parikh, D., & Batra, D. (2018). Embodied question answering. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR)."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2766/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700098150499,
                "cdate": 1700098150499,
                "tmdate": 1700099611284,
                "mdate": 1700099611284,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]