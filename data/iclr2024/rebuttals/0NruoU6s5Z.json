[
    {
        "title": "CompoDiff: Versatile Composed Image Retrieval With Latent Diffusion"
    },
    {
        "review": {
            "id": "il7wWeDi6P",
            "forum": "0NruoU6s5Z",
            "replyto": "0NruoU6s5Z",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1626/Reviewer_e76K"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1626/Reviewer_e76K"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel diffusion-based model, named CompoDiff, which could merge the multimodal conditional information, for solving composed image retrieval (CIR) task. It also proposes a newly created synthetic dataset, named SynthTriplet18M, of 18 million training triplets (reference image, conditions, and target image). The proposed model and dataset address the poor generalizability of existing CIR methods, due to the small training dataset scale and limited types of conditions. The experimental results show the proposed method achieves better results on four public CIR benchmarks."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The idea of leveraging the synthetic data for training of CIR task is pertinent, since the triplet-labeled training data required for CIR task is laborious to collect. \n2. Borrowing the idea from the diffusion of generative task, the authors also explore the possibility of adopting the diffusion mechanism for latent feature extraction in discrimination task, and prove it has the potential to achieve good retrieval accuracy. \n3. It is interesting that the adoption of diffusion enables negative text for CIR task."
                },
                "weaknesses": {
                    "value": "1. The section 3.1 needs to be written more clearly, it is preferable to annotate the letters and variables that appear in Eq. (1), (2), (3) in Figure 3, for example, $z_{i,masked}$; what is the relationship between $e_{t}$ and $z_{i}^{t+1}$? It is difficult to understand from Figure 3 and Eq.(1), (2), and (3).\n2. The diffusion part for feature extraction is very opaque. In Figure 3, what is the intuition of forward diffusion (adding noise T times) and denoise diffusion? In the generation task, the output of the diffusion process consists of pixel values, which have a clear and explicit meaning, while in the retrieval task, the output of the diffusion process is latent variables that do not have a clear and explicit meaning.\n3. I think the authors should consider the time and resource consumption carefully. The training stage is very complex, since the framework involves two stages that both require massive data, and the stage 2 requires some tricks such as alternative strategy, which is unstable, and resource-consuming. \nFor the inference stage, it requires 5 diffusion processes for each query sample while each diffusion process still needs multiple steps, which is very time-consuming since the diffusion process is very slow. It is necessary to compare the inference time with previous methods. In section 4, when collecting the synthesized caption, fine-tuning the OPT-6.7B model is very time-consuming and resource-consuming. \n4. I think there is some mistake on the left side of Eq.(4). Besides, in section 4, x_c is used in the fourth and sixth rows, while x_{c_T} is used in the fifth row."
                },
                "questions": {
                    "value": "In keyword-based diverse caption generation of Figure 5, according to my knowledge, it is not quite reliable to collect the alternative keywords using the CLIP feature similarity. Firstly, some alternative keywords may share a similar concept with the target keyword, but the synthesized caption may not be reasonable. For example, \u201cplants\u201d, \u201cflora\u201d share similar concept with \u201cstrawberry\u201d, but \u201cplants tart\u201d and \u201cflora tart\u201d is ridiculous. Even if frequency filtering is used and restricting the CLIP similarity within 0.5~0.7, this phenomenon still exists. Moreover, keywords such as \u201cportrait, figure, image\u201d have consistently high similarity with most keywords, but they do not have specific meanings; keywords such as \u201cpainting, drawing, walk, hiking\u201d may have different parts of speech (verb and noun), and some keywords such as \u201clight, chair, season\u201d may involve different meanings in different context. Note that what I'm referring to is not limited to the examples mentioned above, but it's a general issue, and all these problems can lead to the generation of very strange modified captions. I am curious how these problems are considered and solved."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1626/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698671129617,
            "cdate": 1698671129617,
            "tmdate": 1699636091501,
            "mdate": 1699636091501,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ecSM9Zjqwj",
                "forum": "0NruoU6s5Z",
                "replyto": "il7wWeDi6P",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1626/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1626/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their positive feedback and constructive comments. We will address all the raised concerns by the reviewer, and will revise our paper as soon as possible (Hopefully before Tuesday).\n\n**[W1] Meaning of diffusion for feature extraction**\n\nWe would like to emphasize that the diffusion process itself is invariant to the input domain, whatever the input domain is, a diffusion model. Diffusion model just learns an underlying distribution of the given data into a Gaussian distribution. From this point of view, CompoDiff learns an underlying distribution of image latent embeddings with proper guidance. Note that our work is not the only method to learn a diffusion model on a latent space. For example, LatentDiffusion (or StableDiffusion) learns a diffusion model on 64 x 64 dimensional latent space of the VQ-GAN encoder. Dalle-2 prior model learns a diffusion model on CLIP latent feature space, as ours. If the reviewer wonders the difference between CompoDiff and these methods, please check our response for Reviewer AJF4, **\u201cCompoDiff vs. StableDiffusion\u201d**.\n\n**[W2] [W4] Clarity and typos in section 3.1 and Eq (4)**\n\nThanks for your comment! We will revise our paper as soon as possible.\n\n**[Q1] Reliability of keyword-based diverse caption generation**\n\nThanks for the question. Our process can handle the cases raised by the reviewer based on CLIP-based filtering. In page 14, we describe the details of our filtering process:\n\n> We apply a filtering process following Brooks et al. (2022) to remove the low-quality \u27e8xiR , xc, xi\u27e9. \nWe filter the generated images for an image-image CLIP threshold of 0.70 to ensure that the images \nare not too different, an image-caption CLIP threshold of 0.2 to ensure that the images correspond \nto their captions, and a directional CLIP similarity of 0.2 to ensure that the change in before/after \ncaptions correspond with the change in before/after images. Additionally, for keyword-based data \ngeneration, we filter out for a keyword-image CLIP threshold of 0.20 to ensure that images contain \nthe context of the keyword, and for instruction-based data generation, we filter out for an instruction modified image CLIP threshold of 0.20 to ensure consistency with the given instructions.\n\nNow, we explain how the examples by the reviewer can be handled by our filtering process. First, if the caption itself is really ridiculous, then StableDiffusion and Prompt2Prompt will not be able to generate proper images. For example, it could make a noisy pixels, just white images, or \u201cbroken\u201d images for a weird caption. These images will be filtered out by CLIP similarity because we measure the similarity between the generated images is sufficiently high, and the broken images will have low similarities with clean images.\n\nSecond, we define keywords as \u201cnoun\u201d; hence, different POS rather than noun will not be extracted. Similar to the first case, if the given caption is really weird and ridiculous to make an image, then the generated model will make a broken image. It will be filtered out."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1626/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700488788534,
                "cdate": 1700488788534,
                "tmdate": 1700666034599,
                "mdate": 1700666034599,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BVI5OXQDy1",
                "forum": "0NruoU6s5Z",
                "replyto": "il7wWeDi6P",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1626/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1626/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**[W3] Resource consumption**\n\nThanks for your constructive feedback. We agree that this paper would be better to have a more careful discussion of resource consumption. We will include each item in the revised paper.\n\n**[W3-1] Complex training stage**\n\nWe would like to emphasize that the two-staged training procedure is not too much complex strategy. For example, SEARLE employs two-staged training, where the first stage is for learning pseudo token embeddings, and the second stage is for learning the projection module by distillation using the learned pseudo token embeddings. Combiner also employs two-staged training, where the first stage is a pre-training stage for the text encoder and the second stage is for training the combiner module using the pre-trained model. Our two-staged training strategy is not a mandatory procedure, but we employ two-staged training for better performance. Conceptually, stage 1 is for a pre-training by training text-to-image generation diffusion model with pairwise relationships. The stage 2 is for a fine-tuning process using triplet relationships. Lastly, the alternative optimization strategy for stage 2 is for helping optimization, not for resolving the instability. As shown in Table 3, CompoDiff can be trained only with Eq (3), but adding more objective functions makes the final performances stronger. In terms of diffusion model fine-tuning, we argue that our method is not specifically complex compared to other fine-tuning methods, such as ControlNet.\n\nWe partially agree with the reviewer, that both stages are trained with massive data points. However, as shown in Table 4, the massive data points are not the necessary condition for training CompoDiff. In fact, Table 4 shows that CompoDiff follows scale law; namely, CompoDiff performances are consistently improved by scaling up the data points. As far as we know, this is the first study that shows the impact of the dataset scale on the zero-shot CIR performances.\n\n**[W3-2] Inference stage**\n\nWe would like to emphasize that CompoDiff retrieval does not take a very long inference time, as the reviewer's concern. Below, we add the comparison table for the comparisons of the inference speed of comparison methods. The table will be added to the revised paper soon.\n\n| Method            | Inference time for a single batch (secs) |\n|-------------------|----------------------|\n| Pic2Word (ViT-L)  | 0.02                 |\n| SEARLE (ViT-L)    | 0.02                 |\n| Compodiff (ViT-L) | ~~0.23~~ 0.12                 |\n| ARTEMIS (RN50)    | 0.005                |\n| Combiner (RN50)   | 0.006                |\n\nIn the table, we can confirm that CompoDiff is practically useful with high throughput (about ~~230ms~~ 120ms). ~~Note that these numbers highly depend on the hardware. For example, in Table 5 in our main paper, we report the average inference time as 120ms with a better machine. In the above table, we measured the numbers with a bit worse machine than the machine used for Table 5.~~ *The previous numbers are based on batch size 32, while the other numbers are based on batch size 1. We fixed the table to avoid confusion*\n\nOne of the advantages of our method is that we can control the trade-off between retrieval performance and inference time. Note that it is impossible for the other methods. If we need a faster inference time, even with a worse retrieval performance, we can reduce the number of diffusion steps. More detailed experiments for the trade-off are shown in Table 5.\n\nAlso, we would like to emphasize that one of our main contributions is a novel and efficient conditioning for the diffusion model. Instead of using a concatenated vector of all conditions and inputs as the input of the diffusion model, we use the cross-attention mechanism for conditions and leave the input size as the same as the original size. As shown in Table C.5., our design choice is three times faster than the naive implementation.\n\n**[W3-3] OPT-6.7B model fine-tuning**\n\nWe fine-tune the OPT model using LoRA fine-tuning (Low-Rank Adaptation of Large Language Models) with 8-bit quantization. We would like to emphasize that the quantized LoRA fine-tuning is very lightweight, only a single GPU can handle the model and the fine-tuning usually takes less than one day. Note that InstructPix2Pix uses the GPT-3.5 turbo API, which needs additional API cost, but our LoRA fine-tuning is easier, cheaper, and faster."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1626/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700488967261,
                "cdate": 1700488967261,
                "tmdate": 1700666459632,
                "mdate": 1700666459632,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jHgtHazdhP",
                "forum": "0NruoU6s5Z",
                "replyto": "il7wWeDi6P",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1626/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1626/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer e76K,\n\nThanks for your constructive and valuable comments on our paper. We would like to notify the reviewer that the revised paper has been uploaded. The revised contents are highlighted in magenta. We have revised our paper to address the reviewer's concerns.\n\n- **[W1] Meaning of diffusion for feature extraction**: We add the background section of diffusion models in Section 2.1. Due to the page limitation, the detailed discussion is in Appendix A.2, including our response to the reviewer's question.\n- **[W2] [W4] Clarity and typos in section 3.1 and Eq (4)**: We fixed the typo. We also omit the time embedding in Figure 3, where the time embedding is too fine detail compared to other concepts. Instead, we specify that the time embedding is a common choice for the existing diffusion models in Appedix A.2 \n- **[Q1] Reliability of keyword-based diverse caption generation**: We clarify that there exists an additional filtering process in Section 4, and split previous B.3 (\"Triplet generation from caption triplets\") to B.3 (\"Triplet generation from caption triplets\") and B.4 (\"CLIP-based filtering\") for emphasizing the filtering process. We also added our response to the reviewer's concern in Section B.4.\n- **[W3-1] Complex training stage**: We clarify the training complexity of our method is not specifically complex compared the others in Section C.1. Also, to avoid confusion, we revise the sentence \"Due to the training stability, CompoDiff uses a two-stage training strategy\" to \"CompoDiff uses a two-stage training strategy\" and clarify the meaning of each stage instead. Note that it is not for hiding our weakness, but it is to avoid confusion such as, \"our method shows unstable training (hence, not converged) without two-stage training\" which is not true.\n- **[W3-2] Inference stage**: We add Appendix D.1 Inference time comparison. Note that our first response has an error: CompoDiff takes 0.12s for forwarding a single image, while 0.23s is for batch size 32.\n- **[W3-3] OPT-6.7B model fine-tuning**: We clarify that the LoRA fine-tuning is lightweight in Section 4. We also would like to emphasize that OPT is only for generating the synthetic training dataset, not for the retrieval model.\n\nPlease feel free to ask anything to us if the reviewers think the revised paper is insufficient. We are open to discussion and will address all the concerns of the reviewers."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1626/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700666391344,
                "cdate": 1700666391344,
                "tmdate": 1700667329413,
                "mdate": 1700667329413,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "2G9rXvMSCb",
            "forum": "0NruoU6s5Z",
            "replyto": "0NruoU6s5Z",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1626/Reviewer_wkZo"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1626/Reviewer_wkZo"
            ],
            "content": {
                "summary": {
                    "value": "The authors introduces CompoDiff, a novel diffusion-based model, for the task of Composed Image Retrieval (CIR) with latent diffusion. It also proposes a new dataset named SynthTriplets18M. Importantly, it supports diverse conditions like negative text and image masks, offers control over query importance, and allows trade-offs between inference speed and performance, improving the overall CIR process."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The concept introduced here is really interesting, although the components used here carry less novelty.\n\n- The writing of introduction, and the overall paper is quite fluid and easy to understand.\n\n- The synopsis of every topic is provided in a self-contained manner.\n\n- Qualitative Figures are well portrayed."
                },
                "weaknesses": {
                    "value": "- Although the experiments are extensive, little reasoning is provided as to why the methods perform (low/high) in the way they do. More analytical reasoning would be encouraged.\n\n- The paper could've been written in a more self-contained manner. A basic background of unCLIP, segCLIP and other components could have been provided instead of simply citing the paper, even 2-3 lines would enhance the readability of the paper.\n\n- The training paradigm seems a bit convoluted. Rephrasing of certain sentences could bring about clarity in the understanding, for instance, discussing a small background on diffusion models first, then bringing in text-image composite part.\n\n- Although not intuitive in this respect it makes me wonder what would be the effect if a learnable text prompt is used in the CLIP-text branch?\n\n- Despite having a few competitors, it would have been better to provide a few baselines focussing on variations of design components used for the proposed method."
                },
                "questions": {
                    "value": "- Does this retrieval include images containing multiple target objects for retrieval as well?\n- Although not intuitive in this respect it makes me wonder what would be the effect if a learnable text prompt is used in the CLIP-text branch?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1626/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1626/Reviewer_wkZo",
                        "ICLR.cc/2024/Conference/Submission1626/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1626/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698756100281,
            "cdate": 1698756100281,
            "tmdate": 1700884007643,
            "mdate": 1700884007643,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "JCMvpZJt1Z",
                "forum": "0NruoU6s5Z",
                "replyto": "2G9rXvMSCb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1626/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1626/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their positive feedback and constructive comments. We will address all the raised concerns by the reviewer, and will revise our paper as soon as possible (Hopefully before Tuesday).\n\n**[W1] More analytical reasoning of the CIR benchmark performance gaps is encouraged**\n\nThanks for the comment! First, we would like to emphasize that our experimental results are \u201czero-shot\u201d, which means the models are not trained on the target dataset. It can cause a significant domain gap between the target dataset domain and the training dataset. CIR datasets, such as FashionIQ and CIRR datasets, have specific domain characteristics that cannot be solved without training on the datasets. For example, the real-world queries that we examine are mainly focused on small editing, addition, deletion, or replacement. However, because the datasets are constructed by letting human annotators write a modification caption for a given two images, the text conditions are somewhat different from the real-world CIR queries. For example, the CIRR dev set has some text conditions like: \u201cshow three bottles of soft drink\u201d (different from the common real-world CIR text conditions), \u201csame environment different species\u201d (ambiguous condition), \u201cChange the type of dog and have it walking to the left in dirt with a leash.\u201d (multiple conditions at the same time). *These types of text conditions are extremely difficult to be solved in a zero-shot manner, but we need access to the CIRR training dataset.*\n\nWhen we perform a qualitative study on LAION-2B image index, we observe that the retrieval quality becomes better by increasing the dataset scale from 1M to 18.8M. We could not build a quantitative evaluation benchmark on the LAION-2B index set, because as our previous comment, the existing datasets need human verification, and it is impossible to perform on billion-scale images. The example retrieval results from LAION-2B are shown in Fig 6 and C.1.\n\n**[Q1] Does this retrieval include images containing multiple target objects for retrieval as well?**\n\nCIRR partially contains such tasks, for example: \u201cChange the type of dog and have it walking to the left in dirt with a leash.\u201d (multiple conditions at the same time). However, basically, as CIRR benchmarks are built upon captions written by human annotators, there are no specific subsets or benchmarks to target multiple target objects. Conceptually, CompoDiff can handle multiple target objects if the target is given in a sentence (the same as the other methods). If target conditions are given in multiple sentences, we can iteratively edit the latent feature, i.e., `orig_feature` -> (CompoDiff) -> `edited_feature_by_sent_1` -> (CompoDiff) -> `edited_feature_by_sent_2`. However, we do not have a proper quantitative benchmark to compare the methods in such scenario.\n\n**[W2] [W3] Enhance writing quality: backgrounds and training paradigm**\n\nThanks for your suggestion. We agree that the current manuscript can be a bit difficult to understand without enough background. We will revise the paper in the rebuttal period. We will inform the reviewer when we upload the revised paper.\n\n**[W4] [Q2] Learnable text prompts**\n\nThanks for the suggestion. We believe that the intuition behind this comment means that \u201cWill the quality of the text embedding affect to the final retrieval performances?\u201d, and the answer is \u201cyes\u201d. It can be proven by our experiments (Table 6) on changing the text encoder for the condition of CompoDiff diffusion model. In the table, we can observe that using a better text encoder boosts up the CIR performance with a large gap (e.g., 38.20 -> 44.11 for FashionIQ recall and 29.19 -> 39.25 for CIRR recall). It means that if we can use better text information, the overall performance will be improved.\n\nHowever, unfortunately, it is not straightforward to apply learnable text prompts to our method. It is because our text encoder is not used for feature matching as other learnable text prompt literature, but the text embedding is used for the cross-attention of the diffusion model, and the supervision for the classifier-free guidance of our diffusion model. It is not trivial to apply learnable prompts to such scenario, especially for classifier-free guidance. We have searched related works, but we cannot find any sound method to be applied to CompoDiff. If the intuition behind this question is the relationship between the text information quality and the retrieval performance as our assumption, we hope our experimental result can answer the question."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1626/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700487807611,
                "cdate": 1700487807611,
                "tmdate": 1700487807611,
                "mdate": 1700487807611,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "kypDsXX5U1",
            "forum": "0NruoU6s5Z",
            "replyto": "0NruoU6s5Z",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1626/Reviewer_FhVu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1626/Reviewer_FhVu"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a composed image retrieval method, called CompoDiff, based on the diffusion model and proposes a large-scale dataset, called SynthTriplets18M, for the composed image retrieval task. In the experiments, the qualitative and quantitative results demonstrated that the performance of the proposed CompoDiff in terms of composed image retrieval exceeds the comparison method. Comparing the results of the model trained with different scales of data, it shows that a large amount of training data can improve the model effect."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper proposes a novel CIR method that can additionally limit the scope of the search image based on the input mask and other conditions.\n2. This method can also control the balance between retrieval accuracy and retrieval efficiency without training, as well as control the impact of each condition on the retrieval results.\n3. This paper proposes a dataset that promotes the development of CIR-related research and illustrates, to a certain extent, the impact of dataset size on methods."
                },
                "weaknesses": {
                    "value": "1. The authors raised the problem of requiring triples for training: but it was not solved well, and the authors just proposed a larger data set.\n2. The experimental results show that the effect of the proposed data set and other data sets are similar at the same level, and after the data volume reaches a certain level, continuing to increase the size of the data set may not significantly improve the model performance. It negates the value of the data set to a certain extent.\n3. The data set used by the comparison method is inconsistent with the data set used by the proposed method, which is not quite fair."
                },
                "questions": {
                    "value": "1. Can additional comparison experiments be conducted, for example, both the proposed method and the comparison method are trained on SynthTriplets18M to illustrate the effectiveness of the proposed method?\n2. The paper mentions that the size of the data set is very important. Can experiments regarding the size of the data set be conducted in other methods to illustrate the effectiveness of the data set?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1626/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698772075004,
            "cdate": 1698772075004,
            "tmdate": 1699636091288,
            "mdate": 1699636091288,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "bD9GDWeGJX",
                "forum": "0NruoU6s5Z",
                "replyto": "kypDsXX5U1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1626/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1626/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate the reviewer for their positive comments and constructive feedbacks. We will address all the raised concerns by the reviewer, and will revise our paper as soon as possible (Hopefully before Tuesday).\n\n**[W1] The authors just proposed a larger dataset**\n\nOur main claim is the problem of requiring **pre-collected and human-verified** triplets, **not triplets themselves**. We will tone down our argument in the introduction section as: *\u201cobtaining human-verified high-quality triplets can be costly\u201d*. We will revise our paper as soon as possible. Meanwhile, we would like to emphasize that the dataset scale-up is one of our main contributions: it is challenging to make a synthetic dataset that shows the scaling law, i.e., more data points lead to better performances. Our dataset construction process achieves this property by employing the keyword-based caption generation that ensures the diversity of the captions. As shown in Table 4, the previous data generation process by IP2P shows inferior model performance than our generation process with the same scale (27.2 vs. 31.9 in the Fashion IQ average recall). We will discuss more details in the next bullet.\n\n**[W2] The value of the dataset is limited to a certain extent**\n\nWe would like to emphasize that the scale shown in Table 4 is already very large. Note that 1M is the scale of ImageNet, and 10M is ten times larger than ImageNet. It is even larger than ImageNet 21K (11.8M images). Scaling up the dataset from 10M to 18.8M shows a bit saturated performance improvements in the benchmark, but we would like to argue that the benchmark number is somewhat noisy in CIR. When we perform a qualitative study on the LAION-2B image index, we observe that the retrieval quality becomes better by increasing the dataset scale from 10M to 18.8M. The reason why the given benchmark scores look saturated is that FashionIQ and CIRR datasets have specific domain characteristics that cannot be solved without training on the datasets. For example, the real-world queries that we examine are mainly focused on small editing, addition, deletion, or replacement. However, because the datasets are constructed by letting human annotators write a modification caption for a given two images, the text conditions are somewhat different from the real-world CIR queries. For example, the CIRR dev set has some text conditions like: \u201cshow three bottles of soft drink\u201d (different from the common real-world CIR text conditions), \u201csame environment different species\u201d (ambiguous condition), \u201cChange the type of dog and have it walking to the left in dirt with a leash.\u201d (multiple conditions at the same time). These types of text conditions are extremely difficult to be solved by a zero-shot manner, but we need to access to the CIRR training dataset.\n\nWe are not certain about the \u201cother datasets\u201d from the comment \u201cThe experimental results show that the effect of the proposed data set and other data sets are similar at the same level\u201d, but if other datasets denote the CIR datasets, such as CIRR or FashionIQ training set, we would like to emphasize that it is not a fair comparison with ours. We aim to solve \u201czero-shot\u201d CIR, i.e., without accessing the expensive human-collected triplets. Directly comparing our zero-shot CIR results and the supervised CIR results on the target dataset is not fair. Note that CLIP ViT-B/16 ImageNet zero-shot classification performance is 80.9 (where the training dataset scale is 400M), worse than that of DeiT-B/16 (a supervised method on 1.2M ImageNet training set) 84.2, but we do not argue that the value of CLIP zero-shot performance is negated.\n\n**[W3] Inconsistent training dataset for comparison methods / [Q1] Additional comparison experiments for the comparison method are trained on SynthTriplets18M**\n\nWe already fit the training datasets for other triplet-based fusion methods, such as ARTEMIS and Combiner. For a fair comparison in terms of backbone size, we have conducted additional experiments with CompoDiff with the same backbone:\n\n|                   | FIQ R@10 | FIQ R@50 | CIRR R@1 | CIRR R_s@1 | CIRCO mAP@5 | CIRCO mAP@10 | CIRCO mAP@25 | GeneCIS R@1 |\n|-------------------|----------|----------|----------|------------|-------------|--------------|--------------|-------------|\n| ARTEMIS (RN50)    | 33.24    | 47.99    | 12.75    | 21.95      | 9.35        | 11.41        | 13.01        | 13.52       |\n| Combiner (RN50)   | 34.30    | 49.38    | 12.82    | 24.12      | 9.77        | 12.08        | 13.58        | 14.93       |\n| CompoDiff (RN50)  | 35.62    | 48.45    | 18.02    | 57.16      | 12.01       | 13.28        | 15.41        | 14.65       |\n| CompoDiff (ViT-L) | 37.36    | 50.85    | 19.37    | 59.13      | 12.31       | 13.51        | 15.67        | 15.11       |\n| CompoDiff (ViT-G) | 39.02    | 51.71    | 26.71    | 64.54      | 15.33       | 17.71        | 19.45        | 15.48       |\n\n(cont.)"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1626/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700487466408,
                "cdate": 1700487466408,
                "tmdate": 1700666499177,
                "mdate": 1700666499177,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "biaw7OUXzN",
                "forum": "0NruoU6s5Z",
                "replyto": "kypDsXX5U1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1626/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1626/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Revision has been uploaded"
                    },
                    "comment": {
                        "value": "Dear Reviewer FhVu,\n\nThanks for your constructive and valuable comments on our paper. We would like to notify the reviewer that the revised paper has been uploaded. The revised contents are highlighted in magenta. We have revised our paper to address the reviewer's concerns.\n\n- **[W1] The authors just proposed a larger dataset**: We clarify that previous CIR methods are based on pre-collected human-verified triplet dataset in the Introduction. We also emphasize that human verification is not scalable, while our dataset is easily scalable automatically to an infeasible scale for manual collection (e.g., more than 1M). The related discussions are in Section 5.3, Appendix C.3.\n- **[W2] The value of the dataset is limited to a certain extent**: We add a related discussion in Section 5.3, Appendix C.3. Particularly, we add a discussion of why the existing benchmarks are somewhat unreliable benchmarks for evaluating authentic zero-shot CIR performances in Appendix C.3. We also clarify that our main evaluation is zero-shot, while FashionIQ-trained counterpart is fully supervised in the Introduction.\n- **[W3] Inconsistent training dataset for comparison methods / [Q1] Additional comparison experiments for the comparison method are trained on SynthTriplets18M**: We clarify that it is impossible to train Pic2Word and SEARLE in Section 5.1 and Appendix C.3. We also added CompoDiff RN50 results for a fair comparison with other RN50-based methods on SynthTriplets18M.\n- **[Q2] Impact of the dataset scale to the other methods**: We add the experiments and discussions in Appendix D.4 (Table D.8)\n\nPlease feel free to ask anything to us if the reviewers think the revised paper is insufficient. We are open to discussion and will address all the concerns of the reviewers."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1626/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700665658580,
                "cdate": 1700665658580,
                "tmdate": 1700666988687,
                "mdate": 1700666988687,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "H7Mc3Fsmd2",
            "forum": "0NruoU6s5Z",
            "replyto": "0NruoU6s5Z",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1626/Reviewer_AJF4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1626/Reviewer_AJF4"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses the limitation of the small dataset and small categories of conditions on the Composed Image Retrieval (CIR) task through a new diffusion-based model (CompoDiff) and a large-scale CIR dataset (SynthTriplets18M). They show the generalizability of their dataset training CompoDiff on the existing CIR benchmarks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Clear presentation of their approach\\\nThis paper tackles the lack of a dataset for this task, which raises the generalizability of this task. It is a clear objective to present a large-scale dataset. To construct on a large scale, they utilize a large generative model such as Stable Diffusion to produce a synthetic dataset. This reviewer can agree with the direction of their approach, and it contributes to this community.\n\n\n- Flexibility of their model\\\nNot only improving the performance on the existing benchmarks of CIR, they also suggest a more flexible manner of CIR including negative texts or masks, which give a large potential for its application."
                },
                "weaknesses": {
                    "value": "- A small improvement using better backbone architecture\\\nFor a fair comparison, it is hard to agree that their model (ViT-L) performs better than the previous arts whose backbones (RN50) are much lighter (Table. 2). Therefore, this reviewer recommends comparing them in the backbone with similar capacity (same backbone is the best option) as much as possible.\n- Efficiency comparison with the previous arts\\\nEven though they show inference time varying the diffusion steps, this reviewer suggests the comparison of latency or flops with the previous arts. Their intra-model analysis is also w\n- An insufficient contribution of CompoDiff\\\nAs far as this reviewer\u2019s understanding, COmpoDiff is a minor modified version of Stable Diffusion. This reviewer considers their flexibility for the conditions also stems from the power of Stable Diffusion. Also, this reviewer wonders what the performance of the Stable Diffusion trained on SynthTriplets18M is on the CIR benchmark."
                },
                "questions": {
                    "value": "The questions are naturally raised in the weaknesses section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1626/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1626/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1626/Reviewer_AJF4"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1626/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699257745214,
            "cdate": 1699257745214,
            "tmdate": 1699636091218,
            "mdate": 1699636091218,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "3sF1FbNcgE",
                "forum": "0NruoU6s5Z",
                "replyto": "H7Mc3Fsmd2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1626/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1626/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their positive feedback and constructive comments. We will address all the concerns raised by the reviewer, and will revise our paper as soon as possible (Hopefully before Tuesday).\n\n**[W1] Comparison with the same backbone**\n\nThanks for the comment. We would like to emphasize that CompoDiff has the same backbone architecture (e.g., ViT-L) as Pic2Word and SEARLE, which are designed for zero-shot CIR. We have conducted CompoDiff with RN50 to compare with ARTEMIS and Combiner with the same backbone as the reviewer suggested. The table below shows that in most of the metrics, CompoDiff still outperforms the comparison methods with large gaps. For example, CompoDiff RN50 shows 18.02 CIRR R@1, while ARTEMIS and Combiner show 12.75 and 12.82, respectively. If we scale up the backbone to ViT-L and ViT-G, the scores become 19.37 and 26.71, respectively. It shows that the improvement by CompoDiff is not by a larger backbone but by our novel diffusion-based retrieval strategy. We will include the full results in the paper.\n\n|                   | FIQ R@10 | FIQ R@50 | CIRR R@1 | CIRR R_s@1 | CIRCO mAP@5 | CIRCO mAP@10 | CIRCO mAP@25 | GeneCIS R@1 |\n|-------------------|----------|----------|----------|------------|-------------|--------------|--------------|-------------|\n| ARTEMIS (RN50)    | 33.24    | 47.99    | 12.75    | 21.95      | 9.35        | 11.41        | 13.01        | 13.52       |\n| Combiner (RN50)   | 34.30    | 49.38    | 12.82    | 24.12      | 9.77        | 12.08        | 13.58        | 14.93       |\n| CompoDiff (RN50)  | 35.62    | 48.45    | 18.02    | 57.16      | 12.01       | 13.28        | 15.41        | 14.65       |\n| CompoDiff (ViT-L) | 37.36    | 50.85    | 19.37    | 59.13      | 12.31       | 13.51        | 15.67        | 15.11       |\n| CompoDiff (ViT-G) | 39.02    | 51.71    | 26.71    | 64.54      | 15.33       | 17.71        | 19.45        | 15.48       |\n\nIn addition, we would like to emphasize that our main contribution is two-fold: (1) the diffusion-based CIR model and (2) a new synthetic triplet dataset. We believe that a weakness in the first contribution does not harm the second one.\n\n**[W2] Efficiency comparison**\n\nThanks for the suggestion. We agree with the comment. Below, we add the comparison table for the comparisons of the inference speed of comparison methods. The table will be added to the revised paper soon.\n\n| Method            | Inference time for a single batch (secs) |\n|-------------------|----------------------|\n| Pic2Word (ViT-L)  | 0.02                 |\n| SEARLE (ViT-L)    | 0.02                 |\n| Compodiff (ViT-L) | ~~0.23~~ 0.12                 |\n| ARTEMIS (RN50)    | 0.005                |\n| Combiner (RN50)   | 0.006                |\n\nIn the table, we can confirm that CompoDiff is practically useful with high throughput (about ~~230ms~~ 120ms). ~~Note that these numbers highly depend on the hardware. For example, in Table 5 in our main paper, we report the average inference time as 120ms with a better machine. In the above table, we measured the numbers with a bit worse machine than the machine used for Table 5.~~ *The previous numbers are based on batch size 32, while the other numbers are based on batch size 1. We fixed the table to avoid confusion*\n\nOne of the advantages of our method is that we can control the trade-off between retrieval performance and inference time. Note that it is impossible for the other methods. If we need a faster inference time, even with a worse retrieval performance, we can reduce the number of diffusion steps. More detailed experiments for the trade-off are shown in Table 5.\n\nAlso, we would like to emphasize that one of our main contributions is a novel and efficient conditioning for the diffusion model. Instead of using a concatenated vector of all conditions and inputs as the input of the diffusion model, we use the cross-attention mechanism for conditions and leave the input size the same as the original size. As shown in Table C.5., our design choice is three times faster than the naive implementation."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1626/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700486594218,
                "cdate": 1700486594218,
                "tmdate": 1700660397022,
                "mdate": 1700660397022,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]