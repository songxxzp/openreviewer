[
    {
        "title": "ENHANCING MULTIVARIATE TIME SERIES FORECAST- ING WITH MUTUAL INFORMATION-DRIVEN CROSS- VARIABLE AND TEMPORAL MODELING"
    },
    {
        "review": {
            "id": "l7eOlpgzUL",
            "forum": "gyJpajLkX2",
            "replyto": "gyJpajLkX2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8290/Reviewer_hxYF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8290/Reviewer_hxYF"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a framework for multivariate time series forecasting called InfoTime, which combines the Cross-variable Decorrelation Aware feature Modeling (CDAM) and the Temporal correlation Aware Modeling (TAM) approaches. The CDAM module filters unnecessary cross-variable correlations by minimizing the mutual information between the latent representation of a single univariate sequence and other series, and the TAM module performs auto-regressive forecasting and captures the temporal correlations across varied timesteps. The InfoTime framework outperforms existing models and can discern cross-variable dependencies while avoiding the inclusion of unnecessary information."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper provides detailed formula derivation to prove the model design.\n2. The framework achieves promising results that outperform existing models and improves performance for both channel-mixing and channel-independence models."
                },
                "weaknesses": {
                    "value": "1. The experiments are not extensive. It seems there are some cherry-pickings in particular configurations. How do the performance of ETTh2 and PEMS in Table 1? Some experiments (Table 2,3,4 and Figure 4) only use the results on partial datasets and baseline models. The prediction visualization of Figure 1 (comparing Informer and PatchTST) seems quite different from the reported results in the PatchTST paper.  I hope the authors can elaborate more on it.\n2. This paper essentially needs further polishing. There are many screenshots as figures, uncompiled citations (in related works), and confusing notations (e.g. unstated $I^i$, confused use of $X_i$ and $X^i$, unreasonable usage of the superscript: $X^o$).\n3. The model benchmark tested in the proposed framework can be too few to be representative. The latest models in different categories are encouraged to be included, such as Transformers (Autoformer), linear models (TiDE, TSMixer), and TCN-based models (TimesNet). Datasets with more variables (such as Solar-Energy) are also recommended since the method essentially addresses the cross-variable issue.\n4. The term \"single-step forecaster\" can be confusing. I think it in fact outcomes the \"multi-step\" prediction. And it seems $\\lambda=1$ is favored in most cases (even though experiments in Section 4.3 are not extensive). How to support the motivation of Equation 12?"
                },
                "questions": {
                    "value": "1. About the model analysis. Are there some explainable results showing that the model takes full advantage of the multivariable correlation while avoiding unnecessary noise?\n2. As shown in Table 4. The performance promoted by CDAM is marginal (PatchTST on ETTh1). Is there any explanation for it? Besides, how does solely CDAM promote the performance, especially on datasets with more variables (such as Traffic)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8290/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8290/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8290/Reviewer_hxYF"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8290/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697860880960,
            "cdate": 1697860880960,
            "tmdate": 1700729723870,
            "mdate": 1700729723870,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Jgwu3xTxE0",
                "forum": "gyJpajLkX2",
                "replyto": "l7eOlpgzUL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8290/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8290/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate the reviewer's comments. We update our paper and address your concerns here:\n\n**Q1: The experiments are not extensive. It seems there are some cherry-pickings in particular configurations. How do the performance of ETTh2 and PEMS in Table 1? Some experiments (Table 2,3,4 and Figure 4) only use the results on partial datasets and baseline models. The prediction visualization of Figure 1 (comparing Informer and PatchTST) seems quite different from the reported results in the PatchTST paper. I hope the authors can elaborate more on it.**\n\n**A1:** Thanks you for the detailed comments, we will answer the question in three parts:\n\n**1. How do the performance of ETTh2 and PEMS in Table 1?** \n\nGreat suggestions. We have made revisions to the manuscript accordingly. First, we conducted extensive experiments to evaluate the effectiveness of InfoTime on the ETTh2 dataset. The results, shown in the updated Table below (also Table 2 and Table 6 of our manuscript), demonstrate that InfoTime can clearly improve the performance of base models on the ETTh2 dataset. \n\nMoreover, we have made further improvements to Table 3 in order to better illustrate the performance improvement of InfoTime on datasets with clear correlations between channels. Following the advice from the reviewer, we have included the experimental results of Stationary and Crossformer in the revised manuscript. \nWith these additions, the revised Table 3 allows for a more comprehensive evaluation of the performance of the models on the PEMS datasets, which are known to have clear correlations between channels.\n\nFurthermore, we have added the ablation results of the aforementioned base models in Table 9. This provides insights into the contribution of InfoTime in improving the performance of both Channel-Independence models and Channel-mixing models.\n\n\nBased on these experimental results, it can be observed that in datasets such as the PEMS datasets, where there is a clear correlation between variables, Channel-Independence models generally perform worse than Channel-mixing models. However, InfoTime is able to enhance the performance of both types of models.\n\n\n\n|Length | 96| | 192 | | 336 | | 720 | |\n|:----: | :----:| :----:| :----:|:----:|:----:| :----:| :----:|:---:|\n|Metric|MSE|MAE|MSE|MAE|MSE|MAE|MSE|MAE|\n|PatchTST| 0.274| 0.335| 0.342| 0.382| 0.365| 0.404| 0.391| 0.428 |\n|+InfoTime| **0.271**|**0.332**| **0.334**| **0.373**| **0.357**| **0.395**| **0.385**| **0.421**| \n|RMLP| 0.290| 0.348| 0.350| 0.388| 0.374| 0.410| 0.410| 0.439 |\n|+InfoTime| **0.271**| **0.333**| **0.335**| **0.374**| **0.358**| **0.395**| **0.398**| **0.432** |\n|Informer| 3.755| 1.525| 5.602| 1.931| 4.721| 1.835| 3.647| 1.625 |\n|+InfoTime| **0.336**| **0.390**| **0.468**| **0.470**| **0.582**| **0.534**| **0.749**| **0.620** |\n|Stationary| 0.362| 0.393| 0.481| 0.453| 0.524| 0.487| 0.512| 0.494 |\n|+InfoTime| **0.286**| **0.335**| **0.371**| **0.388**| **0.414**| **0.425**| **0.418**| **0.437** |\n|Crossformer| 0.728| 0.615| 0.898| 0.705| 1.132| 0.807| 4.390| 1.795 |\n|+InforTime| **0.333**| **0.386**| **0.455**| **0.453**| **0.554**| **0.513**| **0.757**| **0.619** |\n\n**2. Some experiments only use the results on partial datasets and baseline models** We add the full experimental results of the Channel-Independence models in **Table 6**, and we also add the ablation results for all base models used in this paper in **Table 7 and 8**. We believe that these extensive experimental results are more convincing. We hope that the added experiments can help you to better understand InfoTime.\n\n**3. The prediction visualization of Figure 1 (comparing Informer and PatchTST) seems quite different from the reported results in the PatchTST paper**. \n\nWe believe there may be some misunderstanding here. The dataset used in Figure 1 for prediction visualization is the PEMS08 dataset, which is adopted from [SCINet](https://drive.google.com/drive/folders/17fwxGyQ3Qb0TLOalI-Y9wfgTPuXSYgiI). However, we do not find an evidence that the PatchTST paper has reported results on the PEMS08 dataset. In fact, the PatchTST paper reported results on the traffic dataset which is linked to [http://pems.dot.ca.gov](http://pems.dot.ca.gov). Although PEMS08 and the traffic dataset may share some correlation, they are indeed different datasets. We hope the reviewer provide more details on this issue."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8290/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700695253888,
                "cdate": 1700695253888,
                "tmdate": 1700705137579,
                "mdate": 1700705137579,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ebyagogZFK",
                "forum": "gyJpajLkX2",
                "replyto": "l7eOlpgzUL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8290/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8290/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Q4: The term \"single-step forecaster\" can be confusing. I think it in fact outcomes the \"multi-step\" prediction. And it seems $\\lambda=1$ is favored in most cases (even though experiments in Section 4.3 are not extensive). How to support the motivation of Equation 12?**\n\n**A4:** Thanks for the valuable question. We will answer this question in two parts:\n\n**1. The term \"single-step forecaster\" can be confusing** We appreciate your comment regarding the term \"single-step forecaster.\" To clarify, a single-step forecaster refers to a non-autoregressive model that generates multi-step predictions in a single step.  In contrast, autoregressive forecasters predict one time step at a time, which can lead to error accumulation issues. While single-step forecasters alleviate this problem, they lack the ability to model the correlation between future time steps effectively.\n\n**2. And it seems $\\lambda=1$ is favored in most cases (even though experiments in Section 4.3 are not extensive). How to support the motivation of Equation 12?** We acknowledge the reviewer's observation that $\\lambda=1$ appears to be favored in most cases, even though the experiments in Section 4.3 were not extensive. In response to this concern, we have conducted additional experiments and revised Figure 4 (modify a mistake in the orginal version) accordingly to provide a more comprehensive analysis of the impact of different $\\lambda$.\n\nThe updated results presented in Figure 4 clearly demonstrate that the models' performance shows minimal improvement when $\\lambda \\geq 0.8$. In fact, we observed that when $\\lambda$ was set to 0.8, the model's performance was slightly better than with $\\lambda=1.0$. Similar trends were observed in other datasets as well, further reinforcing the notion that deviating from $\\lambda=1$ does not significantly affect the model's performance. \n\n We appreciate the valuable feedback provided by the reviewer and have incorporated it into our revised submission. Thank you for your question."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8290/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700695477494,
                "cdate": 1700695477494,
                "tmdate": 1700702622027,
                "mdate": 1700702622027,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dEf3JS6pMF",
                "forum": "gyJpajLkX2",
                "replyto": "WmNpOFp7Ep",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8290/Reviewer_hxYF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8290/Reviewer_hxYF"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate the authors effort in answering my questions. Extensive experiments are included to validate the points. Most of my concerns have been well addressed and I would like to raise my score to 6."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8290/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700729752523,
                "cdate": 1700729752523,
                "tmdate": 1700729752523,
                "mdate": 1700729752523,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "UvqqD4EfHw",
            "forum": "gyJpajLkX2",
            "replyto": "gyJpajLkX2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8290/Reviewer_b8jU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8290/Reviewer_b8jU"
            ],
            "content": {
                "summary": {
                    "value": "This paper considers utilizing information bottleneck as a regularizer for training Channel Mixing neural network models. The proposed idea aims to help learning better representations for multivariate time series, as the covariates may contain useful information about each other.\nThe developed approach augmented few state of the art models, and showed increased performance on several real-world datasets."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "It is shown in experiments that the overfitting is indeed a problem and the proposed approach seems to be helping alleviating it. \n\nThe ablation study where CDAM and TAM frameworks\u2019 individual contributions are also shown to matter."
                },
                "weaknesses": {
                    "value": "The objective function approximation seems to be a bit ad hoc, and the section on TAM component is not easy to follow. \n\nWhile TAM seems to be useful, not quite sure how downsampling and the additional approximations help. It would have been better to see a more detailed analysis of this step. \n\nAlso it is not quite clear how the initial forecast was obtained."
                },
                "questions": {
                    "value": "What is the purpose of Eq. (2)? Since the unconstrained form is used, I think that this discussion could start from Eq. (3).\n\nThe objective function approximation in Eq. (8) seems to be a bit ad hoc. It is not an upper bound on the entire objective function, but rather a combination of individual approximations. Discussion on this choice were not given in the paper. Is there some reason that makes approximation of the entire function prohibitively difficult? My main concern is that if these bounds are not tight the approximation could be way off.\n\nPage 6, first paragraph: Is it CDAM or TAM?\n\nPage 6: How is $\\hat{Y}$ generated with a single step forecaster? Does that require an initial training of the neural network, or is this a different neural network that is trained before the actual network? Without an initial neural network, how can we obtain $\\hat{Y}$ and train using Eq. (13)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8290/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698794912882,
            "cdate": 1698794912882,
            "tmdate": 1699637030707,
            "mdate": 1699637030707,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "QYj8TSPlYC",
                "forum": "gyJpajLkX2",
                "replyto": "UvqqD4EfHw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8290/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8290/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We would like to sincerely thank Reviewer b8jU for providing a detailed and insightful suggestions.\n\n**Q1:What is the purpose of Eq. (2)? Since the unconstrained form is used, I think that this discussion could start from Eq. (3).**\n\n**A1:** Thank you for your insightful suggestions and feedback on our work. We appreciate your suggestions and would like to address your question regarding the inclusion of Equation (2) in our paper.\n\nEquation (2) in our work is derived from the paper [\"Deep Variational Information Bottleneck\"] (https://arxiv.org/pdf/1612.00410.pdf). This equation represents the unconstrained form of the optimization problem, where the objective is to maximize the mutual information between the inputs $Y^{i},X^{i}$ and the latent variables $Z^{i}$. The aim is to capture as much relevant information as possible from the inputs of other channels.\n\nHowever, we acknowledge your suggestion to start the discussion from Equation (3) instead. Equation (3) introduces the information bottleneck regularizer, which provides a more detailed understanding of the trade-off between extracting relevant information and preserving unnecessary information.\n\nIn light of your feedback, we agree that it would be beneficial to remove Equation (2) and start the discussion directly from Equation (3) in future revisions of the paper. \n\nThank you once again for your valuable suggestions. We appreciate your time and effort in reviewing our work. \n\n**Q2: The objective function approximation in Eq. (8) seems to be a bit ad hoc. It is not an upper bound on the entire objective function, but rather a combination of individual approximations. Discussion on this choice were not given in the paper. Is there some reason that makes approximation of the entire function prohibitively difficult? My main concern is that if these bounds are not tight the approximation could be way off.**\n\n**A2:** We would like to thank you for the valuable feedback on our work. Regarding the objective function approximation, we acknowledge that the method we proposed may appear ad hoc at first glance. However, we would like to emphasize that the approach we employed for the objective function was carefully designed based on comprehensive analysis and extensive experimentation. In our work, the objective is to maximize \n$R_{IB}=\\frac{1}{C}\\sum_i R^{i}_{IB}=\\frac{1}{C}\\sum_i [ I(Y^{i},X^{i};Z^{i})-\\beta I(X^{o};Z^{i})] , i \\in  1,2,...,C $\n\nBy utilizing Equation (4), we can modify the objective function as follows:\n $\\sum_{i} R_{IB}^{i} =\\sum_{i}[ E_{p(z^{i},y^{i},x^{i})} \\left[ \\log p(y^{i}|x^{i},z^{i})\\right]+E_{p(z^{i},x^{i})} \\left [ \\log p(x^{i}|z^{i}) \\right] + H(Y^{i},X^{i}) -\\beta I(X^{o};Z^{i})], i \\in 1,2,...C$  \n Since $H(Y^{i},X^{i})\\geq 0$, we have \n$\\sum_{i}  R_{IB}^{i} \\geq \\sum_{i=1}[ E_{p(z^{i},y^{i},x^{i})} \\left[ \\log p(y^{i}|x^{i},z^{i})\\right]+E_{p(z^{i},x^{i})} \\left [ \\log p(x^{i}|z^{i}) \\right] -\\beta I(X^{o};Z^{i})], i \\in 1,2,...C$. \nBy introducing the variational lower bound $I_{v}(X^{i},Y^{i};Z^{i})$ and upper bound $I_{vCLUB-S}(X^{o};Z^{i})$, we obtain \n$R_{IB}=\\sum_{i} R_{IB}^{i} \\geq \\sum_{i}[ I_{v}(X^{i},Y^{i};Z^{i})-\\beta I_{vCLUB-S}(X^{o};Z^{i})], i \\in 1,2,...,C$. \nTherefore, $\\sum_{i=1}^{C}[ I_{v}(X^{i},Y^{i};Z^{i})-\\beta I_{vCLUB-S}(X^{o};Z^{i})]$ serves as the lower bound for $R_{IB}$. Consequently, we can maximize $R_{IB}$ by minimizing $L_{IB}=-\\sum_{i=1}^{C}[ I_{v}(X^{i},Y^{i};Z^{i})-\\beta I_{vCLUB-S}(X^{o};Z^{i})]$.\n\nWe hope that this clarification provides a better understanding of the rationale behind our objective function approximation. Through extensive experimentation and evaluation, we have observed that minimizing $L_{IB}$\neffectively maximizes $R_{IB}$, leading to competitive results.\n\n**Q3: Page 6, first paragraph: Is it CDAM or TAM?**\n\n**A3:** Thanks for your carefully checking, it's TAM, and we have modified it."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8290/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700699427106,
                "cdate": 1700699427106,
                "tmdate": 1700701916135,
                "mdate": 1700701916135,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8u4agfPVlI",
                "forum": "gyJpajLkX2",
                "replyto": "UvqqD4EfHw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8290/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8290/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Q4: Page 6: How is generated with a single step forecaster? Does that require an initial training of the neural network, or is this a different neural network that is trained before the actual network? Without an initial neural network, how can we obtain $\\hat{Y}$ and train using Eq. (13)?**\n\n\n**A4:** Thank you for your insightful question. In our work, the single-step forecaster is a neural network, typically a fully-connected network, that utilizes the historical data of the i-th channel $X^{i}$ and the cross-variable feature $Z^{i}$ to generate $\\hat{Y^{i}}$.\n\nIn contrast to separate pre-training, we directly optimize the single-step forecaster along with the other neural networks in our model. This approach allows us to jointly train all the networks and optimize their parameters simultaneously.\n\n\nThanks for your suggestion once again. We have taken your suggestions into account and made significant improvements to our description and analysis of the TAM in the revised version.\n\n\nI hope this revised answer captures the essence of your intended response."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8290/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700701979544,
                "cdate": 1700701979544,
                "tmdate": 1700702460440,
                "mdate": 1700702460440,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Wg8MPzj1Pl",
            "forum": "gyJpajLkX2",
            "replyto": "gyJpajLkX2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8290/Reviewer_Gm16"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8290/Reviewer_Gm16"
            ],
            "content": {
                "summary": {
                    "value": "This work aims to improve multivariate time series forecasting with transformers by focusing on the process of mixing channels and moving beyond single-step forecasting. They find that specifically modeling the relationships of channels with their proposed Cross-Variable Decorrelation Aware Feature Modeling can reduce the issues posed by indiscriminate channel-mixing or channel-independent methods.  Additionally, by modeling the temporal relations between subsequences in the forecast sequence with Temporal Correlation Aware Modeling, the authors create a framework for improving on existing work across standard benchmarks and various prediction lengths and model types."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Thanks to the authors for their submission: it contains useful research that shows good research practices while explaining an interesting and novel idea within multivariate time series forecasting. The results of this work will be informative to other researchers and are significant in improving our understanding of applying transformer methods to time series forecasting. Some specific strengths of this research:\n\n- Novelty: InfoTime, which both captures cross-variable relationships and temporal dependencies is a novel approach to the multivariate time series forecasting problem that provides superior results to prior work. While I am uncertain of the novelty of the TAM part of the framework, I believe that combining these and the information theoretic approach of CDAM is new.\n- Relevance: MTSF is a complex task, and the paper acknowledges and addresses two significant challenges: mixing channels in an efficient way and mitigating errors that accumulate over time. By providing specific solutions (CDAM and TAM), the paper contributes to improving the accuracy and effectiveness of MTSF models which are also used in many real-world tasks.\n- The authors conduct comprehensive experimental analyses on real-world datasets to demonstrate the effectiveness of the InfoTime framework. These datasets are standard benchmarks in this area of research and the validation appears to be done through a very similar process to most other work making it easier to compare. The results consistently show that InfoTime outperforms existing Channel-mixing and channel-independence benchmarks, achieving superior accuracy and mitigating overfitting issues.\n- Theory: The paper leverages concepts from information theory, such as Mutual Information and Information Bottleneck, to guide the development of CDAM and TAM. This theoretically grounded approach enhances the understanding and interpretability of the proposed framework.\n- The paper was well written and provides a clear description of the proposed work and comparison with other methods."
                },
                "weaknesses": {
                    "value": "1/ More comparison with statistical forecasting methods or non-transformer methods, either empirically or theoretically, could help introduce the benefits of InfoTime in a more thorough manner. While Zeng et. al (2022) and other methods are briefly discussed for their direct forecasting strategy; it could be helpful to compare InfoTime on the DLinear model to see how CAM varies from the DMS approach. Perhaps this is why RMLP was introduced, but it\u2019s not clear what motivated the construction of RMLP and if differs at all from previous approaches which could be more suitable baseline models. \n\n2/ The implications of the lower bound for (Iv(Xi, Yi; Zi) are not clear, nor is the sampling strategy well motivated in the paper. Additional explanation of the implications could help to improve this.\n\n3/ The experimentation work is not detailed. There are a number of factors that are unclear: the reported informer MSE and MAE on ETTh1, ETTm1, ETTm2 for example are quite a bit higher than in the original Informer, Non-stationary Transformer, and Crossformer papers (>1 for O=720 Informer by authors vs <0.3 in original paper). Perhaps more discussion of the training setup and how it might differ from past benchmarks and why it shows a much higher error could help separate out the actual performance gains given by InfoTime.\n\n4/ Given the large tables of evaluations, it would be helpful to introduce some visual aides to help with understanding the comparisons, perhaps showing the % improvement over other methods for an aggregation of all benchmarks.\n\nNits:\nsp. \u2018varibales\u2019 (pg. 5)"
                },
                "questions": {
                    "value": "1/ There is a lot of variation in results of benchmarks (ETTh1, ETTm1, ETTm2, Weather, Traffic, Electricity) across various papers. Have the authors considered running 5-fold cross validation on these benchmarks to better understand the error bounds around performance claims and then running tests to evaluate the statistical significance of the improvements?\n2/ How does the time complexity of CDAM, TAM, and InfoTime compare with the original base models - particularly as the time series get longer?\n3/ The paper introduces a trade-off parameter B in CDAM to control the balance between retaining important information and eliminating irrelevant information from the latent representation. How could this parameter be determined or optimized in practice? Are there any insights into choosing an appropriate value for B based on experimental results or theoretical considerations?\n4/ The paper introduces a variational lower bound (Iv(Xi, Yi; Zi)) for the mutual information term in CDAM. Could you explain the practical implications of maximizing this lower bound during training? How does optimizing this lower bound improve the model's ability to capture cross-variable dependencies, and what trade-offs or challenges may arise in the optimization process?\n5/ Additionally on the upper bound, the authors state that they choose to adopt the sampled vCLUB and minimize I_{vCLUB-S}(Xo; Zi). Could they expand on why this sampling strategy was chosen and what the actual derived upper bound might be?\n6/ While the benchmarks used are standard for transformer-based evaluation, there are other baselines that could shed more light on the characteristics of the model and whether it may help resolve some of the difficulties with transformer approaches to forecasting. Could the authors shed any light on if they considered benchmarks like M3, M4 and what might be missing from the current evaluations?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8290/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699034810831,
            "cdate": 1699034810831,
            "tmdate": 1699637030574,
            "mdate": 1699637030574,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "WcDNQ1AyIH",
                "forum": "gyJpajLkX2",
                "replyto": "Wg8MPzj1Pl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8290/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8290/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Q1:The experimentation work is not detailed. There is a lot of variation in results of benchmarks (ETTh1, ETTm1, ETTm2, Weather, Traffic, Electricity) across various papers. Have the authors considered running 5-fold cross validation on these benchmarks to better understand the error bounds around performance claims and then running tests to evaluate the statistical significance of the improvements? There are a number of factors that are unclear: the reported informer MSE and MAE on ETTh1, ETTm1, ETTm2 for example are quite a bit higher than in the original Informer, Non-stationary Transformer, and Crossformer papers (>1 for O=720 Informer by authors vs <0.3 in original paper). Perhaps more discussion of the training setup and how it might differ from past benchmarks and why it shows a much higher error could help separate out the actual performance gains given by InfoTime.**\n\n\n\n**A1:** Thank you for your question. We would like to provide further clarification on our experimental work. Firstly, we ensured the reliability and consistency of our results by repeating each experiment three times and reporting the average as the final result. Additionally, we have included the experimental settings of the base models in Appendix A.1, which provides transparency and allows for a better understanding of our methodology. To maintain a fair comparison, we used the same parameter settings as the Informer, Non-stationary Transformer, and Crossformer papers. The experimental scripts we used were obtained from the respective repositories [DLinear](https://github.com/cure-lab/LTSF-Linear/blob/main/scripts/EXP-LongForecasting/Formers_Long.sh), [Non-stationary Transformer](https://github.com/thuml/Nonstationary_Transformers/tree/main/scripts), and [Crossformer](https://github.com/Thinklab-SJTU/Crossformer/tree/master/scripts),. However, we did make a modification by setting the input length $I$ to 96, which may differ from the experimental settings in the original papers. Additionally, for PatchTST and RMLP, we set the input length to 336, as longer input lengths tend to yield better performance for Channel-Independence models. For PEMS03, PEMS04, and PEMS08 datasets, we set $I=336$ for all of these models since all of them perform better in a longer input length.\n\nRegarding your concern about the Informer results, we would appreciate it if you could specify the dataset you are referring to. We have thoroughly checked the results of Informer and have not identified any issues. It is worth noting that our experiments focus on multivariate forecasting rather than univariate forecasting, which may impact the comparison with the original Informer results.\n\nI hope this revised response addresses your concerns more clearly. If you have any further questions or require additional clarification, please feel free to ask.\n\n\n**Q2:  Given the large tables of evaluations, it would be helpful to introduce some visual aides to help with understanding the comparisons, perhaps showing the % improvement over other methods for an aggregation of all benchmarks.**\n\n**A2:** Thanks for your meaningful suggestion. We appreciate your feedback, and we have taken it into consideration. In response, we have made updates to the evaluation by including the average Mean Squared Error (MSE) and Mean Absolute Error (MAE) for four different prediction lengths, as well as the relative reduction of MSE and MAE. You can find this information in **Table 1** and **Table 6**. We hope that these additions will provide a clearer understanding of the effectiveness of InfoTime. If you have any further questions or suggestions, please feel free to let us know. We value your input and strive to continuously improve our work."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8290/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700694856147,
                "cdate": 1700694856147,
                "tmdate": 1700694856147,
                "mdate": 1700694856147,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6lb6vEjRmh",
                "forum": "gyJpajLkX2",
                "replyto": "Wg8MPzj1Pl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8290/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8290/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Q3: How does the time complexity of CDAM, TAM, and InfoTime compare with the original base models - particularly as the time series get longer**\n\n\n\n**A3:** Thank you for the insightful question. In our experiments on the Informer model and ETTh1 dataset, we tested the running time (in seconds) of each epoch for both the base models and InfoTime. By default, we set N=4. Upon analyzing the results, we found that TAM is relatively time-consuming, while CDAM is less consuming. Additionally, we observed that InfoTime does not introduce additional time consumption as the length increases. Although InfoTime requires more time to train, we firmly believe that it is valuable in enhancing the performance of the base models. It is worth noting that future research on how to make InfoTime more lightweight is a meaningful and interesting area of exploration. By optimizing its time complexity, InfoTime has the potential to become even more efficient and practical for forecasting longer time series. Thank you again for bringing up this important point.\n\n|     Predicted Length      |   96   | 192 |\n|:-------------------------:|:------:| :----: |\n|         Original          |  8.5   |   10.5| \n|          TAM N=1          |  9.3   |  11.4 |\n|          TAM N=2          |  11.2  | 13.4  |\n|          TAM N=3          |  14.2  | 15.5  |\n|          TAM N=4          |  20.0  | 22.5  |\n|           CDAM            |  10.8  | 12.7  |\n|       InfoTime            |  23.5  | 24.5 |\n\n**Q4: The paper introduces a trade-off parameter B in CDAM to control the balance between retaining important information and eliminating irrelevant information from the latent representation. How could this parameter be determined or optimized in practice? Are there any insights into choosing an appropriate value for B based on experimental results or theoretical considerations?**\n\n\n\n**A4:** Thanks for your question. The determination and optimization of the trade-off parameter B in CDAM is an important aspect of leveraging the balance between retaining important information and eliminating irrelevant information from the latent representation. In fact, we consider $\\beta$ as a hyper-parameter that needs to be adjusted manually and remains constant during training. While it would be exciting to have the model automatically adjust $\\beta$, which is not explored in our work. However, there have been other research works that propose dynamically adjusting $\\beta$ during training, such as [Generating Sentences from a Continuous Space](https://aclanthology.org/K16-1002/), [Cyclical Annealing Schedule: A Simple Approach to Mitigating KL Vanishing](https://aclanthology.org/N19-1021/), [ControlVAE: Controllable Variational Autoencoder](https://proceedings.mlr.press/v119/shao20b/shao20b.pdf). \n\nIn our experiments, we find that a large value of $\\beta$ ($\\beta \\geq 1e3$ usually) is needed to prevent overfitting and improve performance, as shown in Figure 4 of our paper. This observation suggests that deep-learning-based models have sufficient capacity to capture historical features, and it is crucial to focus on preventing the model from learning irrelevant features."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8290/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700694979564,
                "cdate": 1700694979564,
                "tmdate": 1700702302951,
                "mdate": 1700702302951,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KpgQBApPMj",
                "forum": "gyJpajLkX2",
                "replyto": "Wg8MPzj1Pl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8290/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8290/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Q5: The paper introduces a variational lower bound ($I_{v}(X^{i}, Y^{i}; Z^{i})$) for the mutual information term in CDAM. Could you explain the practical implications of maximizing this lower bound during training? How does optimizing this lower bound improve the model's ability to capture cross-variable dependencies, and what trade-offs or challenges may arise in the optimization process?**\n\n**A5:** Thanks for your question, we will answer your question in three parts.\n\n**1.** The variational lower bound $I_{v}(X^{i}, Y^{i}; Z^{i})$ is equivalent to $E_{p(z^{i},y^{i},x^{i})}[p_{\\theta}(y^{i}|x^{i},z^{i})]$+$E_{p(z^{i},x^{i})}[p_{\\theta}(x^{i}|z^{i})]$, the first term $E_{p(z^{i},y^{i},x^{i})}[p_{\\theta}(y^{i}|x^{i},z^{i})]$ is the negative log-likelihood of the prediction of $Y^{i}$ given $Z^{i}$ and $X^{i}$, and the second term aims to reconstruction of $X^{i}$ given $Z^{i}$. Therefore, maximizing the variational lower bound can help the model to extract sufficient information of $X^{i}$ while ensuring prediction performance.\n\n**2.** To improve the model's ability to capture cross-variable dependencies, it is crucial to simultaneously maximize the variational lower bound $I_{v}(X^{i}, Y^{i}; Z^{i})$ and minimize the upper bound $I_{vCLUB-S}(X^{o};Z^{i})$. Minimizing the variational upper bound reduces the mutual information between $Z^{i}$ and $X^{o}$, while maximizing the variational lower bound requires $Z^{i}$ to contain sufficient information for reconstructing $X^{i}$ and predicting $Y^{i}$. By striking a balance between these two objectives, CDAM can extract the most relevant information from $X^{o}$ while eliminating irrelevant information, thereby enhancing its ability to capture cross-variable dependencies.\n\n**3.** However, optimizing CDAM presents a challenge in selecting the appropriate value for the trade-off parameter $\\beta$. Although experiments have shown that a larger $\\beta$ can improve model performance, it also leads to a reduction in the extraction of cross-variable features. Therefore, determining the optimal value for $\\beta$ requires further exploration to strike the right balance between prediction performance and cross-variable feature extraction.\n\n**Q6: Additionally on the upper bound, the authors state that they choose to adopt the sampled vCLUB and minimize $I_{vCLUB-S}(X^{o}; Z^{i})$. Could they expand on why this sampling strategy was chosen and what the actual derived upper bound might be?**\n\n\n**A6:** Thank you for your question. In comparison to the [VUB](https://arxiv.org/pdf/1612.00410.pdf) and [L1OUT](https://proceedings.mlr.press/v97/poole19a/poole19a.pdf) methods, vCLUB demonstrates a superior ability to approximate the upper bound. Consequently, we have opted to utilize vCLUB as the variational upper bound in our paper. Additionally, we have found that vCLUBSample offers greater efficiency than vCLUB. Hence, we have chosen to employ vCLUBSample in our study."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8290/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700695017938,
                "cdate": 1700695017938,
                "tmdate": 1700702331716,
                "mdate": 1700702331716,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rlxtFGhlQ6",
                "forum": "gyJpajLkX2",
                "replyto": "Wg8MPzj1Pl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8290/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8290/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Q7:More comparison with statistical forecasting methods or non-transformer methods, either empirically or theoretically, could help introduce the benefits of InfoTime in a more thorough manner. While Zeng et. al (2022) and other methods are briefly discussed for their direct forecasting strategy; it could be helpful to compare InfoTime on the DLinear model to see how CAM varies from the DMS approach. Perhaps this is why RMLP was introduced, but it\u2019s not clear what motivated the construction of RMLP and if differs at all from previous approaches which could be more suitable baseline models. While the benchmarks used are standard for transformer-based evaluation, there are other baselines that could shed more light on the characteristics of the model and whether it may help resolve some of the difficulties with transformer approaches to forecasting. Could the authors shed any light on if they considered benchmarks like M3, M4 and what might be missing from the current evaluations?**\n\n\n\n**A7:** Thank you for your valuable feedback and suggestions. \nThe DMS method introduced by Zeng et al. is consistant with the 'single-step forecaster' in our paper which means that predict multi-steps in a single step. AS disscussed in our work, the single-step forecaster or DMS assumes that the future time steps are independent of each other, therefore, they have the following formulation: $p(y^{i}|z^{i},x^{i})=\\prod_{j=1}^{P}p(y^{i}_{j}|z^{i},x^{i})$. However, it is important to note that future time steps are not truly independent, and although the single-step forecaster can effectively alleviate the error accumulation problem, it cannot model the correlation between future sequences, which is why we propose the TAM module to address this limitation.\n\nIn our paper, we propose the linear model RMLP, which outperforms DLinear. As a result, we chose to use RMLP as the base linear model for comparison. Additionally, we also evaluated the effectiveness of InfoTime on the DLinear (linear model), TSMixer (linear model), and TimesNet (linear model) and included the Solar-energy dataset in our analysis. The results, as shown in the table below, demonstrate that InfoTime consistently improves the performance of DLinear, TSMixer, and TimesNet across different datasets. This highlights the efficacy of InfoTime in enhancing the forecasting capabilities of the Linear models and CNN-based models. Moreover, we appreciate your suggestion to include more benchmarks such as M3 and M4. We found that the M3 and M4 datasets are very different from the datasets used in our work. It is very meaningful to verify the effect of our model on these datasets in our future revision."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8290/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700695090580,
                "cdate": 1700695090580,
                "tmdate": 1700702357408,
                "mdate": 1700702357408,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]