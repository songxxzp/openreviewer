[
    {
        "title": "Resource Efficient Self-Supervised Learning for Speech Embeddings"
    },
    {
        "review": {
            "id": "5fhTpdfoJ3",
            "forum": "Wsab3NhIwC",
            "replyto": "Wsab3NhIwC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7951/Reviewer_3Nqf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7951/Reviewer_3Nqf"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a non-contrastive self-supervised learning (SSL) method for speech models. The core idea of this method is to leverage the Barlow-Twins (BT) training technique, which tracks the EMA of the model and compares it with the original to compute loss. Specifically, the method encourages the correlation matrix to become identity, both in a batch-wise and time(frame)-wise manner. Because additional time-axis presents in speech data, the authors show two ways to incorporate time information (time unrolling and time merging). By combining these losses with latent-level augmentation (like SpecAug) and hyper-parameter optimization, the proposed method achieves comparable speech recognition performance to previous models such as Wav2Vec2 and Data2Vec2 while utilizing fewer GPU resources."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* This paper appears to be the first adaptation of a BT-like approach for speech SSL domain. The method naturally integrates the time dimension, making the method more tailored to speech.\n* It is good to see the research that addresses the resource-intensive nature of SSL training. Especially, the suggestion of training with shorter sequence length is a novel contribution not much explored in previous works."
                },
                "weaknesses": {
                    "value": "* Despite the title of this paper mentioning \u201cspeech embeddings\u201d, experiments only evaluate the learned representations in the context of the ASR task. A broader evaluation including common SSL benchmarks such as SUPERB[1] would provide a more comprehensive understanding.\n* Related works on SSL do not include recent SSL papers such as WavLM[2] or BEST-RQ[3].\n* I am not quite sure that speech SSL methods are either contrastive or non-contrastive. For example, HuBERT is more like a BERT-style Masked Language Modeling (MLM) approach rather than a contrastive one. This line of work includes w2v-BERT, WavLM, and BEST-RQ. As HuBERT / WavLM are gaining popularity, I think a comparison with these methods would be beneficial.\n* The authors discuss static and dynamic scaling techniques for balancing the loss; however, there are no corresponding experimental results.\n* (minor) The reference style is inconsistent across different sections of the paper, which makes it difficult to read. Please consider unifying the style throughout the paper.\n\n**References**\n\n[1] SUPERB: Speech Processing Universal PERformance Benchmark\\\n[2] WavLM: Large-Scale Self-Supervised Pre-training for Full Stack Speech Processing\\\n[3] BEST-RQ: Self-Supervised Learning with Random-Projection Quantizer for Speech Recognition"
                },
                "questions": {
                    "value": "* The performance gap of \u2018non-contrastive\u2019 vs. \u2018sequentially combined\u2019 is not small for low-data scenarios. This raises the question of whether the non-contrastive approach sufficiently provides information for speech recognition. Is the performance gap caused by contrastive learning\u2019s ability, or, is it a by-product of longer training? I\u2019d like to hear your thoughts on this.\n* Regarding the numbers in Table 3, for sequentially combined cases, where do 3040 (2376+664) and 380 (297+83) come from? It does not seem that there are clear explanations for these numbers.\n* The authors mention the Appendix in Table 5, but I cannot find the Appendix."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7951/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697893822869,
            "cdate": 1697893822869,
            "tmdate": 1699636976994,
            "mdate": 1699636976994,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "UQVGbCTWn0",
                "forum": "Wsab3NhIwC",
                "replyto": "5fhTpdfoJ3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7951/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7951/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> Despite the title of this paper mentioning \u201cspeech embeddings\u201d, experiments only evaluate the learned representations in the context of the ASR task. A broader evaluation including common SSL benchmarks such as SUPERB[1] would provide a more comprehensive understanding.\n\nWe have added some results and are planning to add more if time permits.\n\n| Model | SLU | KS |\n| -------- | ------- | ------- |\n| Data2vec-2 | 73.3% | 98.5% |\n| Wav2vec-2 | 73.1% | 98.0% |\n| Ours | 73.4% | 98.0% |\n\n\n\n> Related works on SSL do not include recent SSL papers such as WavLM[2] or BEST-RQ[3].\n\nThanks for the suggestion. We will extend the related work and add a proper discussion about these works in the related work. \n\n> I am not quite sure that speech SSL methods are either contrastive or non-contrastive. For example, HuBERT is more like a BERT-style Masked Language Modeling (MLM) approach rather than a contrastive one. This line of work includes w2v-BERT, WavLM, and BEST-RQ. As HuBERT / WavLM are gaining popularity, I think a comparison with these methods would be beneficial. \n\nWe agree with the reviewer that comparison with other masking based SSL approaches would benefit the paper. We did not add the other baselines since Data2Vec-2 showed SOTA compared to all others. However, we will add the suggested SSL approaches to the paper and compare them with our approach.\n\n> The authors discuss static and dynamic scaling techniques for balancing the loss; however, there are no corresponding experimental results.\n\nWe were not able to perform ablation on those two loss scalings since we couldn\u2019t converge the model with static scaling. We will clarify this in the paper.\n\n> (minor) The reference style is inconsistent across different sections of the paper, which makes it difficult to read. Please consider unifying the style throughout the paper.\n\nThanks for pointing this out. We will fix this issue.\n\n> The performance gap of \u2018non-contrastive\u2019 vs. \u2018sequentially combined\u2019 is not small for low-data scenarios. This raises the question of whether the non-contrastive approach sufficiently provides information for speech recognition. Is the performance gap caused by contrastive learning\u2019s ability, or, is it a by-product of longer training? I\u2019d like to hear your thoughts on this.\n\nWe observed that during the pre-training the models do not converge further after around 350k steps. Therefore, we believe that the performance gain comes with the combination of contrastive and non-contrastive learning. \n\n> Regarding the numbers in Table 3, for sequentially combined cases, where do 3040 (2376+664) and 380 (297+83) come from? It does not seem that there are clear explanations for these numbers.\n\n3040 = 2376 (i.e., GPU hours for Wav2Vec-2) + 664 (i.e., GPU hours with our approach). \n380 = 297 (wall clock time for Wav2Vec-2) + 83 (wall clock time for our approach)\nWe will clarify this in the paper. \n\n> The authors mention the Appendix in Table 5, but I cannot find the Appendix.\n\nThat\u2019s our fault. We will fix this in the revised version of the paper."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7951/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700713708159,
                "cdate": 1700713708159,
                "tmdate": 1700713708159,
                "mdate": 1700713708159,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "LILjBaMogG",
            "forum": "Wsab3NhIwC",
            "replyto": "Wsab3NhIwC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7951/Reviewer_iuxy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7951/Reviewer_iuxy"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a non-contrastive learning approach to self-supervised learning from speech. More specifically, the proposed method extends the Barlow-Twins methodology so the loss is defined over sequential data. The method also requires fewer resources, while providing competitive performances, especially in low-resource languages."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The proposed time unrolling and time merging methods appear to be an adequate extension of the Barlow-Twins method, which was originally proposed for non-sequential data. \n\n- The proposed method shows consistent performance; it either competes with the sota performance or improves it. \n\n- The overview of the proposed method is summarized well in Fig 1."
                },
                "weaknesses": {
                    "value": "- It's not clear which of the two terms is contributing more to the performance of the model, in the proposed loss function L_U and L_M. \n\n- In general, the paper doesn't provide an in-depth justification for the claims, and relegate the explanation to the reference, such as the implication and importance of gradient stopping. \n\n- It seems that the proposed method could save some GPU time, but not too significantly (1.3X less).\n\n- The presentation of the loss function is somewhat abrupt, lacking explanation.\n\n- Literature review could be more organized."
                },
                "questions": {
                    "value": "- One of the main claims is that the proposed method provides a new SOTA result on the low-resource labeled data, which is good. However, it's not clear why the proposed method cannot compete with the Data2Vec2 method in the high-resource labeled data experiments."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7951/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698720910826,
            "cdate": 1698720910826,
            "tmdate": 1699636976866,
            "mdate": 1699636976866,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "dcXZe4C6Am",
                "forum": "Wsab3NhIwC",
                "replyto": "LILjBaMogG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7951/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7951/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> It's not clear which of the two terms is contributing more to the performance of the model, in the proposed loss function L_U and L_M. In general, the paper doesn't provide an in-depth justification for the claims, and relegate the explanation to the reference, such as the implication and importance of gradient stopping.\n\nWe were not able to perform ablation on those two loss scalings since we couldn\u2019t converge the model with static scaling. Therefore, we could not even show the individual contribution of the two losses. We will clarify this in the paper.\n\n> It seems that the proposed method could save some GPU time, but not too significantly (1.3X less).\n\nWe agree with the reviewer that mentioning this absolute improvement in that way may look less exciting or impactful. However, we also wish to highlight one important thing: training SSL models is notoriously expensive, and a 1.3X improvement in speed can very quickly translate into days of training and cost savings, especially from a multi-GPU training perspective. We also know well that deployed or usable SSL models are not the outcome of a single training run, meaning that these needed resources must be multiplied by the number of trials. This is a 1.3X reduction in time, energy consumption and cost. From this regard, any improvement is something positive for the community. The refinement of today\u2019s models comes from the progressive incremental improvements of training efficiency [1] \u2013 and we believe that a 1.3X improvement already is a significant step for anyone interested in training a SSL model with a limited GPU budget. \n\n[1] Vyas, A., Hsu, W. N., Auli, M., & Baevski, A. (2022). On-demand compute reduction with stochastic wav2vec 2.0. arXiv preprint arXiv:2204.11934.\n\n> The presentation of the loss function is somewhat abrupt, lacking explanation. \n> Literature review could be more organized.\n\nThanks for the suggestion. We will address these comments in the revised version of the paper.\n\n>One of the main claims is that the proposed method provides a new SOTA result on the low-resource labeled data, which is good. However, it's not clear why the proposed method cannot compete with the Data2Vec2 method in the high-resource labeled data experiments.\n\nWe believe that Data2Vec-2 was well optimised to achieve SOTA WER results, which require a very thorough hyper-parameter optimization (HPO). However, we do not perform rigorous HPO due to limited computational resources. Still we show that our approach marginally higher WER for LibriSpeech splits, except for dev-clean where all methods achieve the same WER (i.e., 2%)."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7951/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700713463436,
                "cdate": 1700713463436,
                "tmdate": 1700713463436,
                "mdate": 1700713463436,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "SyAtsy7jkM",
            "forum": "Wsab3NhIwC",
            "replyto": "Wsab3NhIwC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7951/Reviewer_QXLT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7951/Reviewer_QXLT"
            ],
            "content": {
                "summary": {
                    "value": "- self-supervised learning (SSL) approaches have helped to improve speech model performance\n- these techniques have included both contrastive and non-contrastive methods\n- non-contrastive methods (like Data2Vec2 vs Wav2Vec2) have improved performance and reduced training time, but still suffers from significant GPU resource constraints\n- this work introduces a non-contrastive approach which is an extension of the Barlow-Twins methodology to reduce training time and resource requirements for self-supervised model training\n- to adapt speech for the Barlow-Twins method, they use time-unrolling ([B, T, F] -> [B * T, F]) and time-merging ([B, T, F] -> [T * F, B]) approaches when computing two different cross-correlation terms for the losses\n- the primary comparisons are done with wav2vec2, data2vec2, non-contrastive (their approach), and sequentially combined (wav2vec2 training then non-contrastive)\n- these comparisons are done when fine-tuning on LibriSpeech 960h, train-clean-100h, and LibriLight-10h\n- 960h: the results are slightly better w/o LM but w/ LM they're competitive\n- for the 10h and 100h settings, the result trends broadly follow:\n         - non-contrastive consistently outperforms wav2vec2\n         - sequentially combined outperforms non-contrastive\n         - but sequentially combined and data2vec2 are more competitive with each other\n- the main benefits come from requiring less resources by being able to use smaller batch sizes (reducing training time) as well as fewer GPUs"
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Straightforward motivation, modification/adaptation of an existing idea, and execution. Primarily isolating changes to the loss, while keeping architectural changes minimal."
                },
                "weaknesses": {
                    "value": "Despite the similarities to Wav2Vec2 and Data2Vec2, it would be nice to include more non-contrastive comparisons (especially since the application focus is on speech), these would ideally include at least one of HuBERT and WavLM. Since the takeaway here seems to be about reducing resource requirements while maintaining high-quality performance, comparison with these popular approaches both in terms of training resources required and inclusion in the WERs table would be helpful.\n\nAlso, in the abstract, SSLs are mentioned as being great for a variety of tasks. Seeing the performance of this non-contrastive approach on not only ASR but other speech tasks as well could help to distinguish it from Data2Vec2 (since it often needs to be sequentially combined with wav2vec2 to match Data2Vec2 performance on the \"other\" partition of dev or test). These SSLs are useful in a variety of cases, so an idea of the general performance hit (in service of resource savings) on these other tasks would be helpful."
                },
                "questions": {
                    "value": "Resource and performance comparisons with HuBERT and WavLM would be helpful (for more non-contrastive comparisons).\n\nIt would be nice to see the trade-off between training time (or # of GPUs) and performance between these models (i.e. if you speed up the training recipe of the contrastive vs non-contrastive approaches does the performance degrade similarly in cases where you have even more significant resource constraints than those given)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7951/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698827593865,
            "cdate": 1698827593865,
            "tmdate": 1699636976709,
            "mdate": 1699636976709,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rytSvpdtWi",
                "forum": "Wsab3NhIwC",
                "replyto": "SyAtsy7jkM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7951/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7951/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> Despite the similarities to Wav2Vec2 and Data2Vec2, it would be nice to include more non-contrastive comparisons (especially since the application focus is on speech), these would ideally include at least one of HuBERT and WavLM. Since the takeaway here seems to be about reducing resource requirements while maintaining high-quality performance, comparison with these popular approaches both in terms of training resources required and inclusion in the WERs table would be helpful.\n\nWe agree with the reviewer that comparison with HuBERT and WavLM as baselines would benefit the paper. We did not add the other baselines since Data2Vec-2 showed SOTA compared to all others. However, we will add the suggested SSL approaches to the paper and compare them with our approach in table 1, 2, 3, and 4. \n\n> Also, in the abstract, SSLs are mentioned as being great for a variety of tasks. Seeing the performance of this non-contrastive approach on not only ASR but other speech tasks as well could help to distinguish it from Data2Vec2 (since it often needs to be sequentially combined with wav2vec2 to match Data2Vec2 performance on the \"other\" partition of dev or test). These SSLs are useful in a variety of cases, so an idea of the general performance hit (in service of resource savings) on these other tasks would be helpful.\n\nFollowing the reviewer's advice, we added two new tasks from the SUPERB benchmark to compare our model to wav2vec2 and data2vec2: Spoken Language Understanding with SLURP and Keyword Spotting with Google Speech command. Following the open-source recipe of these tasks on SpeechBrain we fine-tuned three different models to obtain the results reported on the following Table. SLURP results are reported with the introduced SLU-F1 score while Keyword Spotting is a simple accuracy. In both cases, higher is better. Finally, we would like to highlight that other accepted articles on SSL, such as BEST-RQ, are also only evaluated on ASR. \n\n| Model | SLU | KS |\n| -------- | ------- | ------- |\n| Data2vec-2 | 73.3% | 98.5% |\n| Wav2vec-2 | 73.1% | 98.0% |\n| Ours | 73.4% | 98.0% |"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7951/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700713258017,
                "cdate": 1700713258017,
                "tmdate": 1700713258017,
                "mdate": 1700713258017,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "80sKIjcp7z",
            "forum": "Wsab3NhIwC",
            "replyto": "Wsab3NhIwC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7951/Reviewer_CDH9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7951/Reviewer_CDH9"
            ],
            "content": {
                "summary": {
                    "value": "While wav2vec-style contrastive learning has shown to be very successful for ASR, it requires a lot of resources and time for training. In the vision domain, Barlow Twins, a solution that naturally avoids collapse, has shown to be able to achieve better (or competitive) performance compared with contrastive learning (e.g., SimCLR) while using much smaller batch size. However, Barlow Twins style training is under-explored in the ASR/audio domain.\n\nThe authors explored using BT to speech representation learning and achieved competitive performance compared with wav2vec2; The authors further combined their approach with wav2vec2 to further boost the performance. The authors claim the proposed methods can reduce training time, GPU usage and improve convergence."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This is, if not the first, among the early explorations that applies Barlow-Twins methodology to learn representation for ASR; The adoption of BT into sequential representation learning is not trivial. Previously, BT was mostly used to learn a global representation of a sequence.\n\n\n\n\u2014 The authors show that the proposed methods are more resource efficient and achieve comparative performance. a) It improves convergence, b) it reduces training time, c) it significantly reduces GPU training times, d) it requires smaller batch size thus reducing memory requirements.\n\n\n\n\u2014 The authors also found that combining the proposed method with a wav2vec-style contrastive learning approach is helpful."
                },
                "weaknesses": {
                    "value": "The proposed approach, though has some computational benefits when compared to Data2vec2, it achieves clearly worse performance compared to Data2vec2.\n\nThe authors try to combine wav2vec2 pre-training and the proposed method, which can significantly improve the performance, but the performance is still worse than Data2vec2. What's more, after combining with wav2vec2 pre-training, the computational resources needed would increase drastly. \n\n\n\n\n\n\u2014 Regarding Time Unrolling and time merging losses: To calculate both the F by F and B by B correlation matrix, the calculation can become a burden when sequence length T is big. In this work, the authors propose to crop the audio into 5-seconds. However, these limitations could affect learning in a large context.\n\n\n\nThe authors do not test their model on tasks other than ASR."
                },
                "questions": {
                    "value": "See Weaknesses section"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7951/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699204129843,
            "cdate": 1699204129843,
            "tmdate": 1699636976586,
            "mdate": 1699636976586,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "TQvYQ6t2Yw",
                "forum": "Wsab3NhIwC",
                "replyto": "80sKIjcp7z",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7951/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7951/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> The proposed approach, though has some computational benefits when compared to Data2vec2, it achieves clearly worse performance compared to Data2vec2. The authors try to combine wav2vec2 pre-training and the proposed method, which can significantly improve the performance, but the performance is still worse than Data2vec2. What's more, after combining with wav2vec2 pre-training, the computational resources needed would increase drastly.\n\nWe would like to highlight that training SSL models is notoriously expensive, and a 1.3X improvement in speed can very quickly translate into days of training and cost savings, especially from a multi-GPU training perspective. We also know well that deployed or usable SSL models are not the outcome of a single training run, meaning that these needed resources must be multiplied by the number of trials. This is a 1.3X reduction in time, energy consumption and cost, which is significant for a large-scale SSL training. From this regard, any improvement is something positive for the community. At the same time,  we do not claim SOTA for all LibriSpeech splits, but we do show our model achieves competitive results. Additionally, our model also achieves SOTA on low resource settings. \n\n\n> Regarding Time Unrolling and time merging losses: To calculate both the F by F and B by B correlation matrix, the calculation can become a burden when sequence length T is big. In this work, the authors propose to crop the audio into 5-seconds. However, these limitations could affect learning in a large context.\n\nWe agree with the reviewer that large sequence lengths could add an overhead for loss computation. However, we do not observe any impact on WER when using 7 secs sequence length compared to 5 secs. In particular, we based this choice on a published article investigating the impact of variable sequence length for SSL pretraining: \u201cMatch to Win: Analysing Sequences Lengths for Efficient Self-supervised Learning in Speech and Audio\u201d from SLT 2022. \n\n\n> The authors do not test their model on tasks other than ASR.\nFollowing the reviewer's advice, we added two new tasks from the SUPERB benchmark to compare our model to wav2vec2 and data2vec2: Spoken Language Understanding with SLURP and Keyword Spotting with Google Speech command. Following the open-source recipe of these tasks on SpeechBrain we fine-tuned three different models to obtain the results reported on the following Table. SLURP results are reported with the introduced SLU-F1 score while Keyword Spotting is a simple accuracy. In both cases, higher is better. Finally, we would like to highlight that other accepted articles on SSL, such as BEST-RQ, are also only evaluated on ASR. \n\n| Model | SLU | KS |\n| -------- | ------- | ------- |\n| Data2vec-2 | 73.3% | 98.5% |\n| Wav2vec-2 | 73.1% | 98.0% |\n| Ours | 73.4% | 98.0% |"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7951/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700713132479,
                "cdate": 1700713132479,
                "tmdate": 1700713132479,
                "mdate": 1700713132479,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]