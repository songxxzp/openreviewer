[
    {
        "title": "Machine Unlearning for Image-to-Image Generative Models"
    },
    {
        "review": {
            "id": "jUqPFqcOWk",
            "forum": "9hjVoPWPnh",
            "replyto": "9hjVoPWPnh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2859/Reviewer_hK5e"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2859/Reviewer_hK5e"
            ],
            "content": {
                "summary": {
                    "value": "This paper explores the concept of \"machine unlearning,\" a method to deliberately forget data from models to comply with regulations. While existing methods have focused on classification models, this paper introduces a framework for machine unlearning in image-to-image generative models. It presents a computationally efficient algorithm supported by theoretical analysis that effectively removes information from \"forget\" samples without significant performance degradation on \"retain\" samples. The algorithm's effectiveness is demonstrated on large-scale datasets (ImageNet-1K and Places-365) and is notable for not requiring retain samples, aligning with data retention policies. This work is a pioneering effort in the systematic exploration of machine unlearning tailored for image-to-image generative models."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The idea of using the E_0 to generate L_F and L_R is interesting and a smart way to overcome the problems of estimating MI.\n- The theoretical analysis of the paper is well written and looks (beside of some remarks) sound to me.\n- Having done so much experiments using different image datasets"
                },
                "weaknesses": {
                    "value": "I understand that the work was only done for I2I generation, which is a complex part already, however I was wondering if there is really no related work in this domain. I am not an expert in this domain but would propose to have a more in depth look at the literature for more related work."
                },
                "questions": {
                    "value": "Abstract: \u201eprimarily focused on classification models, leaving the landscape of unlearning for generative models relatively unexplored.\u201c: Is this true? Are there any references which support this statement?\n\nIntro: \u201eInformally speaking, we define a generative model as having \u201ctruly unlearned\u201d an image when it is unable to faithfully reconstruct the original image when provided with only partial information (see Figure 1 for an illustrative example where the partial information involves center cropping2).\u201c: I wonder if you did any analysis if the \u201enoise\u201c is fully uncorrelated with the part of the image before the unlearning? Maybe there are still some high level corrections left. In learning fair representations people do train secondary models on the representations and try to predict the sensitivity attribute from the fair representations. I wonder if such study can be used to check if a secondary autoencoders could not re-generate the cropped image from the generated noise.\n\nEquation 3: why is alpha not between 0-1 so you have (1-alpha) x term0 - alpha term1?\n\nTheorem 1: Why is having the same decoder not a problem? Couldn\u2019t the decoder not fully remember the images in some way or could you not extract some of the images by some information extraction techniques?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Privacy, security and safety",
                        "Yes, Legal compliance (e.g., GDPR, copyright, terms of use)"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "The work is directly related to privacy and legal compliance. The correctness of the method and the way how it gets used needs to be verified in detail for any potential application."
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2859/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2859/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2859/Reviewer_hK5e"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2859/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697711950447,
            "cdate": 1697711950447,
            "tmdate": 1699636229645,
            "mdate": 1699636229645,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "CIoyYQWFeS",
                "forum": "9hjVoPWPnh",
                "replyto": "jUqPFqcOWk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2859/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2859/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer hK5e(1/2)"
                    },
                    "comment": {
                        "value": "We really appreciate your encouragement for our work. Please see our responses to your questions below. \n\n\n**Question 1** Abstract: \u201cprimarily focused on classification models, leaving the landscape of unlearning for generative models relatively unexplored.\u201c: Is this true? Are there any references which support this statement?\n \n**Response 1** We provide the following categories of references to support our claim that *\u2019the landscape of unlearning for generative models is relatively unexplored\u2019*:\n\n- Many recent review/survey papers on machine unlearning. As shown in [1,2,3,4], there are only three papers targeting unlearning for generative models out of hundreds of cited papers in these papers.\n\n- Open-source collections of machine unlearning papers [5]. Out of 254 papers shown in this collection, only eight papers focus on the unlearning for language generation and only 9 papers target the unlearning for image generation. Specifically, for unlearning on image generation models, most papers only explore GANs and none of them addresses the image-to-image generation. In contrast, our approach is generally applicable to GANs, MAE and diffusion models and is the first to unlearn on image-to-image generative models. \n \n**Question 2** Intro: \"Informally speaking, we define a generative model as having 'truly unlearned' an image when it is unable to faithfully reconstruct the original image when provided with only partial information (see Figure 1 for an illustrative example where the partial information involves center cropping2).\": I wonder if you did any analysis if the \"noise\" is fully uncorrelated with the part of the image before the unlearning? Maybe there are still some high-level corrections left. In learning fair representations people do train secondary models on the representations and try to predict the sensitivity attribute from the fair representations. I wonder if such study can be used to check if a secondary autoencoders could not re-generate the cropped image from the generated noise.\n \n**Response 2** We appreciate for this valuable suggestion for a more thorough evaluation of our approach. To this answer this question, we conducted the following experiment for VQ-GAN:\n\n- We first conduct the unlearning for VQ-GAN for ImageNet to obtain the *unlearned model*, under the same setup mentioned in Section 4.1.\n\n- Given the center-cropped input images (cropping central $8\\times 8$ patches), we then use this *unlearned model* to generate the images on both forget set and retain set (i.e., image upcropping/inpainting). Here we call them *FIRST* generated images.\n- Given these *FIRST* generated images, we only keep the reconstructed central $8\\times 8$ patches (cropping the outer ones) as the new input to the original VQ-GAN (i.e., the model before unlearning) and get the newly generated images (here we call them *SECOND* generated images). We then evaluate the quality of *SECOND* generated images for both forget set and retain set and report the results. \n- We also conduct the above process for the *original model* (i.e., before unlearning) as the baseline/reference. The results are given below:\n\n| |FID: $D_R$|FID: $D_F$|IS: $D_R$|IS: $D_F$|CLIP: $D_R$|CLIP: $D_F$|\n|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n|Original Model|36.14|28.96|27.28|27.95|0.37|0.49|\n|Unlearned Model|36.74|156.68|26.62|7.67|0.37|0.28|\n\n> **NOTE:**  Lower FID, higher IS, and higher CLIP values indicate higher image quality.\n\nAs shown above, compared to the original model, given the *noise* (i.e., *FIRST* generated images on the forget set) from the unlearned model as the input, the *SECOND* generated images has very low quality in terms of all these three metrics. This means that the *noise* indeed are not correlated with the real forget images.\n\nIn contrast, the performance on the retain set is almost the same before and after unlearning. This indicates that our approach indeed preserves the knowledge on the retain set well.\n\nWe will add these results and provide some visualizations in the revised paper."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2859/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700283697125,
                "cdate": 1700283697125,
                "tmdate": 1700283697125,
                "mdate": 1700283697125,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Oigf5PSazz",
            "forum": "9hjVoPWPnh",
            "replyto": "9hjVoPWPnh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2859/Reviewer_4Xrc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2859/Reviewer_4Xrc"
            ],
            "content": {
                "summary": {
                    "value": "This proposes a computationally-efficient unlearning approach, that demonstrates negligible performance degradation on the retain samples, while effectively removing the information from the forget samples. The authors also provide rigorous theoretical analysis for this image-to-image generative model. Empirical studies on two large-scale datasets, ImageNet-1K and Places-365 show the advantages on image-to-image task. Overall, it is good paper. The main concern is how to get the forget set and how to set the hyper parameter $\\sigma$?"
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Quality/Clarity: the paper is well written and easy to follow. And both the experimental results and theoretical analysis demonstrate its advantages.\n\nOriginality/significance: the algorithm is new and underpinned by rigorous theoretical analysis. It systematically explores machine unlearning for I2I generative models."
                },
                "weaknesses": {
                    "value": "1. The main concern is that the paper assumption is ideal and its hyper parameter \\sigma, which may not hold in practice.\n2. From Lemma 1, $\\sigma$ should be from the forget set? In the appendix C.4, however it sets $\\sigma=I$, which is not estimated from forget set?"
                },
                "questions": {
                    "value": "The main concern is that the paper assumption is ideal and its hyper parameter \\sigma, which may not hold in practice."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2859/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698817607061,
            "cdate": 1698817607061,
            "tmdate": 1699636229569,
            "mdate": 1699636229569,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "UU1QaxqAns",
                "forum": "9hjVoPWPnh",
                "replyto": "Oigf5PSazz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2859/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2859/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 4Xrc"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the feedback and encouragement! We address your question point-by-point below.\n\n\n**Question** The main concern is that the paper assumption is ideal and its hyper parameter $\\Sigma$, which may not hold in practice.\n\n\n\n**Response** Regarding the assumptions used in our paper, we tried our best in making them align with practice. Specifically, one of the main assumptions is that the original model can do a perfect reconstruction for the cropped images. This assumption is somehow ideal, yet practically reasonable, especially given the recent progress in diffusion models [1,2]. We also note that since our work is the first to target unlearning for image-to-image generative models, we hope it will pave the way for follow-up works that will relax some of our assumptions.  The theoretical results we provide offer a solid basis for such future extensions.  \n\nWe also agree with Reviewer that, ideally, we should use the exact $\\Sigma$ of the images from the forget set. However, there are some computational barriers to using the exact $\\Sigma$ for a high-resolution image dataset. Specifically, consider a commonly used 256$\\times$256 resolution for image generation tasks, the distribution of the generated images will have $256\\times 256\\times 3 \\approx 2\\times 10^5$ dimensions. The size of the covariance matrix $\\Sigma$ for such a high-dimensional distribution is around $(2\\times 10^5)^2=4\\times 10^{10}$, which requires around 144GB memory if stored in float precision thus is not practical.\n\n\nConsequently, to address the computational barrier of $\\Sigma$, we use some approximated methods derived from some empirical observations on some small datasets.\nSpecifically, we compute the exact $\\Sigma$ for some small-scale image dataset, including MNIST and CIFAR10/100.\n\n- *Off-diagonal elements*: as shown in Figure C.6 at Appendix C.4, the values on the diagonal of exact $\\Sigma$ for these datasets are generally much larger than the off-diagonal elements; hence, we truncate the off-diagonal elements to zero. \n- *Diagonal elements*: since the input images for deep networks are typically normalized, we may set the diagonal elements to one. \n\nIn short, using $I$ to approximate $\\Sigma$ is a practical approximation alternative due to the extremely high computational costs of $\\Sigma$ for high-resolution images. \nFor future work, given our theoretical analysis, we believe that our approach will achieve better results (lower image quality) on forget set if we can find a way to use the exact $\\Sigma$. Hence, we plan to explore the potential to reduce the computation of exact $\\Sigma$ with low-rank approximation thus enabling the use of more accurate data-driven $\\Sigma$.\n\nWe will add the above clarifications to the revised paper. We hope our response can address your concerns. We look forward to further discussions if you have any other questions. \n\n[1] Song, Bowen, et al. \"Solving inverse problems with latent diffusion models via hard data consistency.\" arXiv preprint arXiv:2307.08123 (2023).\n\n[2] Wallace, Bram, Akash Gokul, and Nikhil Naik. \"Edict: Exact diffusion inversion via coupled transformations.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2859/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700109419155,
                "cdate": 1700109419155,
                "tmdate": 1700109419155,
                "mdate": 1700109419155,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "EsnU2ONVzo",
                "forum": "9hjVoPWPnh",
                "replyto": "Oigf5PSazz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2859/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2859/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Kindly expecting further discussions"
                    },
                    "comment": {
                        "value": "Dear Reviewer 4Xrc,\n\nThanks again for constructive feedback on our paper. We hope that our previous responses have successfully addressed your concerns. We would like to remind you that the author-reviewer discussion period is closing very soon. If there are any remaining questions or concerns, we are looking forward to further discussions!\n\nBest regards,\n\nThe Authors of Paper 2859"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2859/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700633927661,
                "cdate": 1700633927661,
                "tmdate": 1700633927661,
                "mdate": 1700633927661,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "h3dwi8ZbqV",
            "forum": "9hjVoPWPnh",
            "replyto": "9hjVoPWPnh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2859/Reviewer_L8mN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2859/Reviewer_L8mN"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes a mechanism for machine unlearning for image-to-image generative models. The work proposes a simple framework, minimizing the $l_2$ loss between embeddings from the forget training set and normal distribution, and maintaining the $l_2$ loss for the retain set embeddings. The results are provided on VQ-GAN, Diffusion Models and Masked Autoencoders on Imagenet-1K and Places-365 dataset comparing to reasonable baselines where the method shows good performance on retain set, while performance on forget set deteriorates as expected."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This work is focused on a timely and important problem as there is more discussing about regulating generative AI. For the current lawsuits facing several companies for their data practices on generative models and upcoming laws surrounding data retention policies, machine unlearning approaches may offer a possible solution.\n2. The experimental across different models and datasets are thorough, and helpful for future work. The work shows comparisons across several reasonable baselines, across different image-to-image generative models. The results are shown across different relevant metrics such as FID, Inception Score and CLIP Distance."
                },
                "weaknesses": {
                    "value": "1. It's unclear if the machine unlearning setup considered in this work is practical. For machine unlearning setups, the gold standard to mimic is a model trained only on the retain set. In this work, the approach performs this task by minimizing the embedding distance for the forget set to normal distribution. Does this lead to overall worse performance than the gold standard model (where only retain set is used to train from scratch)? It would be good to include this as a baseline for comparison. \n2. The proposed approach itself is not too novel, while the work may have applied it first on image-to-image generative models.  This approach can be thought of a simple extension student-teacher or continual learning framework, and similar ideas have been explored in classification literature [1, 2].\n3. It's unclear if the model shows strong performance throughout. While the performance on the retain set stays strong, it's unclear if **worst** performance on the forget set is a good metric. This again focuses on going back to the first point, the unlearning paradigm should only be focused on achieving performance of model trained *only* on retain set. Thus, if the unlearned model performs much worse on the forget set than the model trained only on the retain set this may not be a good metric.\n\n[1] Zhang, Xulong, et al. \"Machine Unlearning Methodology base on Stochastic Teacher Network.\"\u00a0_arXiv preprint arXiv:2308.14322_\u00a0(2023).\n[2] Zhang, Yongjing, et al. \"Machine Unlearning by Reversing the Continual Learning.\"\u00a0_Applied Sciences_\u00a013.16 (2023): 9341."
                },
                "questions": {
                    "value": "1. The experimental setup for retain sample availability is not quite clear to me. Why are retain samples selected from the remaining 800 random classes? Also, this confused me if all the 1000 classes, or only 200 classes are used for the main experiments to train the original model."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2859/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2859/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2859/Reviewer_L8mN"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2859/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698978353991,
            "cdate": 1698978353991,
            "tmdate": 1699636229512,
            "mdate": 1699636229512,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7IJP6OYzMc",
                "forum": "9hjVoPWPnh",
                "replyto": "h3dwi8ZbqV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2859/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2859/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer L8mN (1/2)"
                    },
                    "comment": {
                        "value": "We thanks for your constructive feedback for our work. We address the questions point-by-point below.\n\n\n\n**Question 1**: It's unclear if the machine unlearning setup considered in this work is practical. For machine unlearning setups, the gold standard to mimic is a model trained only on the retain set. In this work, the approach performs this task by minimizing the embedding distance for the forget set to normal distribution. Does this lead to overall worse performance than the gold standard model (where only retain set is used to train from scratch)? It would be good to include this as a baseline for comparison.\n\n\n**Response 1**: This is a very fundamental question, and we understand the reviewer\u2019s concern to the definition/setup of our unlearning approach. Indeed, we did ask the same questions ourselves. First, we agree that most of previous works consider the training only on the retain set is the gold baseline for *classifications tasks*. However, recently it has been shown that this golden standard bear several concerns:\n\n- Some recent works show that for *\u2018some unlearning algorithms can generate models indistinguishable from the golden baseline beyond any arbitrarily small threshold while still exposing the deleted data\u2019* for some cases [1]. In other words, the indistinguishability of training on retain set only cannot guarantee the data has been totally removed from the model, which is the goal in our paper.\n\n\n- Retraining these generative models from scratch on retain set is completely beyond our computation capacity. For the three models evaluated in our paper, it takes more than 10K GPU hours ($>$50 days on our 8-GPU server; estimated from the experimental setup in [2,3,4])  to train the model from scratch. \n\n- Given these considerations, our setup/definition primarily focuses on the practical scenarios other than the indistinguishability from the retrained model. For example, if the training set for the original model has some pornographic images by accident, our methods can make the unlearned model avoid generating pornographic images in a very efficient way other than expensive retraining the model from scratch. This way, the model's developers can avoid the potential violation of contents control. In short, our main goal is making the unlearned model not generate the samples from the forget set without hurting the performance on the retain set. Under this perspective, the performance drops on the forget set shown in our results is expected and dedicatedly designed. Moreover, the results show that on retain set, our approach has negligible performance drop compared to the model before unlearning. \n\nAs for the performance comparison with the golden standard model, our method will have worse performance **on the forget set** than this golden baseline (the performance on the retain set is expected to be similar to the golden standard model), which is dedicatedly designed by our approach. \n\nFinally, we remark that the unlearning for generative models is still in its emerging stage; we do hope our works could contribute to a better setup/definition for this problem space. \n\n\n\n[1] Chourasia, Rishav, and Neil Shah. \"Forget Unlearning: Towards True Data-Deletion in Machine Learning.\" ICML 2023. https://icml.cc/virtual/2023/poster/23753\n\n\n[2] Li, Tianhong, et al. \"Mage: Masked generative encoder to unify representation learning and image synthesis.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\n\n[3] He, Kaiming, et al. \"Masked autoencoders are scalable vision learners.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022.\n\n[4] Saharia, Chitwan, et al. \"Palette: Image-to-image diffusion models.\" ACM SIGGRAPH 2022 Conference Proceedings. 2022."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2859/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700110457931,
                "cdate": 1700110457931,
                "tmdate": 1700110457931,
                "mdate": 1700110457931,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lz8AaBaHu9",
                "forum": "9hjVoPWPnh",
                "replyto": "h3dwi8ZbqV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2859/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2859/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Kindly expecting further discussions"
                    },
                    "comment": {
                        "value": "Dear Reviewer L8mN,\n\nThanks again for constructive feedback on our paper. We hope that our previous responses have successfully addressed your concerns. We would like to remind you that the author-reviewer discussion period is closing very soon. If there are any remaining questions or concerns, we are looking forward to further discussions!\n\nBest regards,\n\nThe Authors of Paper 2859"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2859/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700633960118,
                "cdate": 1700633960118,
                "tmdate": 1700633960118,
                "mdate": 1700633960118,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "p4xxtaaqJ9",
            "forum": "9hjVoPWPnh",
            "replyto": "9hjVoPWPnh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2859/Reviewer_uD61"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2859/Reviewer_uD61"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a study on machine unlearning for image-to-image generative models, an area not extensively explored previously. The paper introduces a theoretically sound and computationally efficient algorithm for unlearning that ensures minimal impact on the performance of retained data while effectively removing information from data meant to be forgotten. Demonstrated on ImageNet-1K and Places-365 datasets, the algorithm uniquely operates without needing the retained data, aligning with stringent data privacy policies. The research claims to pioneer a theoretical and practical approach to machine unlearning in the context of generative models."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Strengths:\n- The paper boasts a clear and logical structure, helping readers' comprehension of the concepts presented.\n- It ventures into the relatively untapped domain of applying machine unlearning to image-to-image (I2I) generative tasks.\n- The authors have bolstered their approach with a solid theoretical foundation, enhancing the credibility and robustness of their proposed method."
                },
                "weaknesses": {
                    "value": "Weakness:\n- The study does not align with the foundational concept of machine unlearning, which typically necessitates a comparison between the unlearned and retrained models as per references [1-7]. Although the authors justify this divergence due to the high costs associated with retraining generative models, this deviates from the core goal of machine unlearning aimed at addressing privacy concerns. Particularly, the approach presented in this paper generates conspicuous Gaussian noise over the 'forgotten' data, which may inadvertently signal that the data was previously part of the training set, contradicting privacy preservation goals. A more compelling motivation might be found in text-to-image (T2I) scenarios [8] where the goal is to prevent the generation of inappropriate content, or in image-to-image (I2I) applications [9] that showcase practical utility. It would be more meaningful\u2014and privacy-compliant\u2014if the model could reconstruct unremarkable images that don't trace back to the original training data, rather than reconstruct images with evident distortions signaling prior data use.\n- The evaluation process presented in the paper is incomplete with regard to established machine unlearning protocols. Typically, an unlearned model's performance is assessed using three distinct datasets: a test dataset to determine its generalization capability, a retained dataset to evaluate performance on non-forgotten data, and a forget dataset to check the efficacy of the unlearning process. The paper's Table 1 appears to only present results for the latter two, omitting the crucial evaluation on the general test dataset.\n- The consideration of relevant baselines in the paper is lacking. Reference [9] describes unlearning in the context of image-to-image (I2I) generative models, which appears to be closely related to the work at hand. A comparison or a clear explanation of why the methods from [9] cannot integrate into the proposed framework would strengthen the current approach by situating it within the broader research landscape and justifying its unique contributions.\n- The scope of the study with respect to the application of machine unlearning in image-to-image (I2I) generative models appears to be inaccurately broad. The term \"machine unlearning for I2I generative models\" suggests a wide range of applications; however, the paper primarily focuses on the image inpainting task. It would be more precise to either expand the variety of I2I applications examined in the study or to specifically define the scope as \"machine unlearning for image inpainting tasks\" to reflect the content more accurately. This would ensure clarity in the paper's contributions and avoid overgeneralization of the results.\n\n\n>[1] Graves, Laura, Vineel Nagisetty, and Vijay Ganesh. \"Amnesiac machine learning.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 35. No. 13. 2021.\n>\n>[2] Chundawat, Vikram S., et al. \"Zero-shot machine unlearning.\" IEEE Transactions on Information Forensics and Security (2023).\n>\n>[3] Chen, Min, et al. \"Boundary Unlearning: Rapid Forgetting of Deep Networks via Shifting the Decision Boundary.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\n>\n>[4] Warnecke, Alexander, et al. \"Machine unlearning of features and labels.\" arXiv preprint arXiv:2108.11577 (2021).\n>\n>[5] Jia, Jinghan, et al. \"Model sparsification can simplify machine unlearning.\" arXiv preprint arXiv:2304.04934 (2023).\n>\n>[6] Kurmanji, Meghdad, Peter Triantafillou, and Eleni Triantafillou. \"Towards Unbounded Machine Unlearning.\" arXiv preprint arXiv:2302.09880 (2023).\n>\n>[7] Golatkar, Aditya, Alessandro Achille, and Stefano Soatto. \"Eternal sunshine of the spotless net: Selective forgetting in deep networks.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.\n>\n>[8] Gandikota, Rohit, et al. \"Erasing concepts from diffusion models.\" arXiv preprint arXiv:2303.07345 (2023).\n>\n>[9] Moon, Saemi, Seunghyuk Cho, and Dongwoo Kim. \"Feature unlearning for generative models via implicit feedback.\" arXiv preprint arXiv:2303.05699 (2023)."
                },
                "questions": {
                    "value": "- Could you please add one baseline mentioned in the weakness to the paper (Table 1)? \n- Why easing concepts can be thought about as a noisy label method? Please give more explanations."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2859/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2859/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2859/Reviewer_uD61"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2859/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699151331846,
            "cdate": 1699151331846,
            "tmdate": 1700508782946,
            "mdate": 1700508782946,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "EhpF3lB4dp",
                "forum": "9hjVoPWPnh",
                "replyto": "p4xxtaaqJ9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2859/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2859/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer uD61(1/3)"
                    },
                    "comment": {
                        "value": "We thanks for Reviewer's feedback and comments. Please see our responses to your questions below. \n\n\n**Question 1** The study does not align with the foundational concept of machine unlearning, which typically necessitates a comparison between the unlearned and retrained models as per references [1-7].\n\n\n**Response 1** This is a fundamental question, and we understand the reviewer's concern about the definition/setup of our unlearning approach. Indeed, we did ask the same questions ourselves. First, for *classifications tasks*, we agree that most of previous work considers that the unlearned model should be indistinguishable to the model retrained only on the retain set (golden baseline). However, there are three points we want to bring to this reviewer\u2019s attention: \n\n\n- First, even the indistinguishability to the golden standard may have some issues. Indeed, recent work has shown that *\u2018some unlearning algorithms can generate models indistinguishable from the golden baseline beyond any arbitrarily small threshold while still exposing the deleted data\u2019*[10]. In other words, the indistinguishability of training on the retain set alone *cannot* guarantee the information is indeed removed from the model. \n\n- Second, retraining these generative models from scratch on the retain set is beyond our (and average user, in general) computational capabilities. More precisely, for the three models evaluated in our paper, it would take more than 10K GPU hours ($>$50 days on our 8-GPU server; estimated from the experimental setup in [11,12,13]) to train the model from scratch; this is clearly infeasible given the resources we have at hand and the tight deadline for this rebuttal period. \n\n\n\n- Given these considerations, our setup/definition primarily focuses on the practical scenarios of copyright protection and/or pornographic/violence content control, other than the indistinguishability from the retrained model. For example, if the original training set for the original model got some pornographic images by accident, our method can make the unlearned model not generate those pornographic images in a very efficient way (as opposed to expensively retraining the model from scratch). Moreover, the results show that on the retain set, our approach has negligible performance drop compared to the model before unlearning. Consequently, we believe our approach is sound for copyright protection and pornographic/violence content control hence very useful in real applications. \n\n**Question 2**: Although the authors justify this divergence due to the high costs associated with retraining generative models, this deviates from the core goal of machine unlearning aimed at addressing privacy concerns. Particularly, the approach presented in this paper generates conspicuous Gaussian noise over the 'forgotten' data, which may inadvertently signal that the data was previously part of the training set, contradicting privacy preservation goals. A more compelling motivation might be found in text-to-image (T2I) scenarios [8] where the goal is to prevent the generation of inappropriate content, or in image-to-image (I2I) applications [9] that showcase practical utility. It would be more meaningful\u2014and privacy-compliant\u2014if the model could reconstruct unremarkable images that don't trace back to the original training data, rather than reconstruct images with evident distortions signaling prior data use.\n\n**Response 2**:  This is a very insightful comment (*the approach presented in this paper generates conspicuous Gaussian noise over the 'forgotten' data, which may inadvertently signal that the data was previously part of the training set, contradicting privacy preservation goals.*). We note that, under our setup, given an image with noise distortions, one cannot conclude whether it is in the training set or not. For example, suppose our method makes the model forget the concept *car*; given any image of a car, the corresponding generated image typically will have noise distortions, but it is possible that this specific image is not used for training. Even though one can infer that some concepts are from the forget set, that still meets our main goal, i.e., the unlearned model does not generate the unwanted images."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2859/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700283115535,
                "cdate": 1700283115535,
                "tmdate": 1700283115535,
                "mdate": 1700283115535,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "raSDSytEbE",
                "forum": "9hjVoPWPnh",
                "replyto": "p4xxtaaqJ9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2859/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2859/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer uD61(3/3)"
                    },
                    "comment": {
                        "value": "**Question 4**: The evaluation process presented in the paper is incomplete with regard to established machine unlearning protocols. Typically, an unlearned model's performance is assessed using three distinct datasets: a test dataset to determine its generalization capability, a retained dataset to evaluate performance on non-forgotten data, and a forget dataset to check the efficacy of the unlearning process. The paper's Table 1 appears to only present results for the latter two, omitting the crucial evaluation on the general test dataset.\n\n**Response 4**: This is a very important question. We would like to clarify that we *actually performed* such a test (the first evaluation in the reviewer's question) in our paper, but simply described it in a slightly different way. \n\nSpecifically, out of 1000 classes from ImageNet-1K, our experimental setup selects 100 classes as the retain set $D_R$ and another 100 classes as the forget set $D_F$; the remaining 800 classes $D_O$ are not considered either as retain or forget sets. We evaluate our method with the following setup: In Table 1 and  Table 2 (rows *Proxy* $D_R$) of our paper, we assume that we don\u2019t have access to the images from 100 classes from $D_R$, and then we use images from the other 800 classes $D_O$ to serve as the proxy retain set for our unlearning approach. Moreover, the performance/test on the retain set is always conducted on the 100 classes from original $D_R$. \n\nIn fact, for this setup, we can re-consider the original $D_R$ as the *general test set* and re-consider the $D_O$ as the new $D_R$. As shown in Table 1 and Table 2 (rows *Proxy* $D_R$), our approach is evaluated on the 100 classes from *general test set* and it shows negligible performance drop on the general test set. \n\n\n**Question 5**: The scope of the study with respect to the application of machine unlearning in image-to-image (I2I) generative models appears to be inaccurately broad. The term \"machine unlearning for I2I generative models\" suggests a wide range of applications; however, the paper primarily focuses on the image inpainting task. It would be more precise to either expand the variety of I2I applications examined in the study or to specifically define the scope as \"machine unlearning for image inpainting tasks\" to reflect the content more accurately. This would ensure clarity in the paper's contributions and avoid overgeneralization of the results.\n\n\n**Response 5**: We appreciate this reviewer's suggestion. We will modify our title to 'Machine Unlearning for Image Completion Generative Tasks'. \n\n\n**Question 6**: Why easing concepts can be thought about as a noisy label method? Please give more explanations.\n\n**Response 6**: the reason we use Gaussian noise as the optimization target for the forget set is motivated by our theoretical analysis. Specifically, we prove that Gaussian noise is the unique optimal solution for unlearning on image-to-image generative models if we want to destroy the performance on the forget set. This is indeed one of the main contributions of this paper.\n\nThanks again for your comments; we would be happy to provide more clarifications and answer any follow-up questions.\n\n[1] Graves, Laura, et al. \"Amnesiac machine learning.\" AAAI 2021.\n\n[2] Chundawat, Vikram S., et al. \"Zero-shot machine unlearning.\" IEEE Transactions on Information Forensics and Security (2023).\n\n[3] Chen, Min, et al. \"Boundary Unlearning: Rapid Forgetting of Deep Networks via Shifting the Decision Boundary.\" CVPR  2023.\n\n[4] Warnecke, Alexander, et al. \"Machine unlearning of features and labels.\" arXiv preprint arXiv:2108.11577 (2021).\n\n[5] Jia, Jinghan, et al. \"Model sparsification can simplify machine unlearning.\" arXiv preprint arXiv:2304.04934 (2023).\n\n[6] Kurmanji, Meghdad, et al. \"Towards Unbounded Machine Unlearning.\" arXiv preprint arXiv:2302.09880 (2023).\n\n[7] Golatkar, Aditya, et al. \"Eternal sunshine of the spotless net: Selective forgetting in deep networks.\" CVPR  2020.\n\n[8] Gandikota, Rohit, et al. \"Erasing concepts from diffusion models.\" arXiv preprint arXiv:2303.07345 (2023).\n\n[9] Moon, Saemi, et al. \"Feature unlearning for generative models via implicit feedback.\" arXiv preprint arXiv:2303.05699 (2023).\n\n[10] Chourasia, Rishav, et al. \"Forget Unlearning: Towards True Data-Deletion in Machine Learning.\" ICML 2023. https://icml.cc/virtual/2023/poster/23753\n\n[11] Li, Tianhong, et al. \"Mage: Masked generative encoder to unify representation learning and image synthesis.\" CVPR  2023.\n\n[12] He, Kaiming, et al. \"Masked autoencoders are scalable vision learners.\" CVPR 2022.\n\n[13] Saharia, Chitwan, et al. \"Palette: Image-to-image diffusion models.\" ACM SIGGRAPH 2022 Conference Proceedings. 2022."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2859/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700283400788,
                "cdate": 1700283400788,
                "tmdate": 1700283420433,
                "mdate": 1700283420433,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "l1UtdCdBN3",
                "forum": "9hjVoPWPnh",
                "replyto": "raSDSytEbE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2859/Reviewer_uD61"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2859/Reviewer_uD61"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the clarifications!"
                    },
                    "comment": {
                        "value": "Thank you for your detailed responses and clarifications to my previous queries. I appreciate the effort you've put into addressing each point, which has largely resolved my initial questions and concerns. As a result, I am inclined to raise my score. However, I would like to offer some additional suggestions and points for consideration.\n\nRegarding Q1:\nI understand and acknowledge your motivations concerning the challenges of retraining, which are central to your paper's premise. However, I still hold reservations about the first point in your response as a robust support for your method. While I appreciate the practical implications of your approach, I believe that a more solid theoretical foundation or guarantee that your method effectively removes the targeted information from the model would strengthen your argument. Furthermore, showcasing a practical application, as hinted at in your motivation, could significantly enhance the paper's impact and relevance. Demonstrating this application would not only align with your motivations but also provide tangible evidence of your method's effectiveness.\n\nRegarding Q3:\nYour response here is well-received. Discussing the distinctions between machine unlearning (MU) in classification and generation contexts is crucial, and your explanations are compelling. The comparison of methods between these contexts enriches the paper\u2019s content. However, I noticed that reference [5], which proposes adding a sparse regularization term to the general machine unlearning objective function, seems like it could be directly adapted to a more general case. (Maybe need to be double-checked by authors). I suggest adding this part to the revision. Including this analysis would provide a more comprehensive view of how your method situates within the broader landscape of MU techniques."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2859/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700508760780,
                "cdate": 1700508760780,
                "tmdate": 1700508760780,
                "mdate": 1700508760780,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QbkpEcMrRK",
                "forum": "9hjVoPWPnh",
                "replyto": "p4xxtaaqJ9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2859/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2859/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Follow-up discussion (1/2)"
                    },
                    "comment": {
                        "value": "Dear Reviewer uD61,\n \nWe thank you very much for raising the rating for our paper. We appreciate your new comments and suggestions very much. Please see our responses to your new questions below:\n \n**Question**: Regarding *Q1*: I understand and acknowledge your motivations concerning the challenges of retraining, which are central to your paper's premise. However, I still hold reservations about the first point in your response as a robust support for your method. While I appreciate the practical implications of your approach, I believe that a more solid theoretical foundation or guarantee that your method effectively removes the targeted information from the model would strengthen your argument. Furthermore, showcasing a practical application, as hinted at in your motivation, could significantly enhance the paper's impact and relevance. Demonstrating this application would not only align with your motivations but also provide tangible evidence of your method's effectiveness.\n \n \n**Response**: Ideally, for a given unlearned model, the generated images ($\\hat{X}_F$) on the forget set should have no information related to the corresponding ground truth images ($X_F$). From the perspective of mutual information (MI), this means that the mutual information between $\\hat{X}_F$ and $X_F$ is 0, i.e., $I(X_F;\\hat{X}_F)=0$.\n \nIn our paper, our theoretical analysis and proposed optimization methods aim to remove the related information from the generated images ($\\hat{X}_F$) by gradually maximizing its mutual information w.r.t a Gaussian Noise. To further verify the effectiveness, we estimate the mutual information between the reconstructed images and the corresponding ground truth images on both forget set and retain set.\n \nWe note that estimating the mutual information for high-dimensional images is extremely difficult. To this end, we follow the standard practice and use one of the state-of-the-art approximate estimation methods which compute a variational lower bound of exact mutual information [14]. Moreover, we also compute the variational mutual information bound between the ground truth images and Gaussian noise for a reference. The results are shown below:\n \n|MI Estimator| SMILE_1.0 | SMILE_5.0 |\n|:-:|:-:|:-:|\n| Original Model: $I(X_F;\\hat{X_F})$  | 10.95     | 12.20     |\n| Unlearned Model: $I(X_F;\\hat{X_F})$ | 5.68      | 5.79      |\n| $I(X_F;\\mathcal{N})$                | 4.24      | 6.04      |\n| Original Model: $I(X_R;\\hat{X_R})$  | 12.54     | 16.98     |\n| Unlearned Model: $I(X_R;\\hat{X_R})$ | 12.83     | 17.05     |\n| $I(X_R;\\mathcal{N})$                | 5.29      | 4.42      |\n\n> **NOTE:**  SMILE_1.0 and SMILE_5.0 are using the same SMILE mutual information estimator but with different hyper-parameters [14]. \n\n\nAs shown above, after unlearning on the forget set, the mutual information between the reconstructed images ($\\hat{X}_F$) and the corresponding ground truth images ($X_F$) $I(X_F;\\hat{X}_F)$ is much lower than the original model (i.e., before unlearning). Moreover, after unlearning, $I(X_F;\\hat{X}_F)$ is very close to the $I(X_F;\\mathcal{N})$.\n \nAs for the retain set, the $I(X_R;\\hat{X}_R)$ is roughly the same as before unlearning, which means that the information on the retain set is well preserved.\n \nFinally, per reviewer\u2019 suggestion, we plan to add a companion demo for the final version of our paper; this demo will show our approach in action on a few generic applications, hence illustrate our method\u2019s effectiveness."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2859/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700671158064,
                "cdate": 1700671158064,
                "tmdate": 1700671653896,
                "mdate": 1700671653896,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CVcaOvdytC",
                "forum": "9hjVoPWPnh",
                "replyto": "p4xxtaaqJ9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2859/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2859/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Follow-up discussion (2/2)"
                    },
                    "comment": {
                        "value": "**Question**: Regarding *Q3*: Your response here is well-received. Discussing the distinctions between machine unlearning (MU) in classification and generation contexts is crucial, and your explanations are compelling. The comparison of methods between these contexts enriches the paper\u2019s content. However, I noticed that reference [5], which proposes adding a sparse regularization term to the general machine unlearning objective function, seems like it could be directly adapted to a more general case. (Maybe need to be double-checked by authors). I suggest adding this part to the revision. Including this analysis would provide a more comprehensive view of how your method situates within the broader landscape of MU techniques.\n \n**Response**: Indeed, we mentioned that [5] is orthogonal to our approach. To further demonstrate this point, we combine our approach together with the $L1$-regularization techniques introduced in [5]. We use the so called '*linear decaying*' trick on page 6 of [5] and compare the performance with and without $L1$-regularization for VQ-GAN, under the same setup mentioned in Section 4.1. Due to time limitations, we conduct the test only on a subset of the validation set with 5 images per class instead of the entire set (50 images/class). The results are shown below:\n \n \n| 4$\\times$4 Patches | FID: $D_R\\downarrow$ | FID: $D_F\\uparrow$ | IS: $D_R\\uparrow$ | IS: $D_F\\downarrow$ | CLIP: $D_R\\uparrow$ | CLIP: $D_F\\downarrow$ |\n|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n| Ours (No $L1$) | 73.7 | 73.2 | 12.1 | 11.4 | 0.84 | 0.76 |\n| Ours+$L1$ | 73.7 | 73.0 | 12.2 | 11.1 | 0.84 | 0.76 |\n \n| 8$\\times$8 Patches | FID: $D_R\\downarrow$ | FID: $D_F\\uparrow$ | IS: $D_R\\uparrow$ | IS: $D_F\\downarrow$ | CLIP: $D_R\\uparrow$ | CLIP: $D_F\\downarrow$ |\n|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n| Ours (No $L1$) | 85.3 | 142.2 | 13.0 | 9.6 | 0.75 | 0.57 |\n| Ours+$L1$ | 85.8 | 139.7 | 12.9 | 9.6 | 0.74 | 0.57 |\n \nAs shown above, by combining our approach with $L1$-regularization from [5], the results are roughly the same as the case without $L1$-regularization.\n \nWe also tried different coefficients $\\lambda$ for the $L1$-regularization. The above results are the coefficients that achieve optimal trade-off between the performance on the retain set and forget set. We found that the performance on the retain set is pretty sensitive to the value of $\\lambda$; specifically, the performance on the retain set will have a significant drop if the $\\lambda$ is relatively large (e.g., $>10^{-6}$). We have the following comment regarding this observation:\n \n- Previous work reveals that *sparsity of networks is relatively high for fairly simple and naive tasks (e.g., CIFAR-10/100, ImageNet, GLUE, etc.)*[15]. In other words, if the network is designed for more challenging tasks such as image generation, then there is less sparsity inside the network compared to the classification tasks. This may be the reason why $L1$-regularization from [5] doesn't achieve the same improvements in image generative tasks as in classification tasks.\n \nThank you again for your constructive feedback. We would be happy to provide more clarifications and answer any follow-up questions.\n \n[5] Jia, Jinghan, et al. \"Model sparsification can simplify machine unlearning.\" arXiv preprint arXiv:2304.04934 (2023).\n \n[14] Song, Jiaming, et al. \"Understanding the limitations of variational mutual information estimators.\" ICLR 2020.\n \n[15] Liu, Shiwei, et al. \"Sparsity May Cry: Let Us Fail (Current) Sparse Neural Networks Together!.\" ICLR 2023."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2859/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700671215021,
                "cdate": 1700671215021,
                "tmdate": 1700671298179,
                "mdate": 1700671298179,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]