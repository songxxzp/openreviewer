[
    {
        "title": "ELoRA: Efficient Low-Rank Adaptation with Random Matrices"
    },
    {
        "review": {
            "id": "CFmZ5JdSeS",
            "forum": "NjNfLdxr3A",
            "replyto": "NjNfLdxr3A",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5820/Reviewer_rHtj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5820/Reviewer_rHtj"
            ],
            "content": {
                "summary": {
                    "value": "> **TL;DR:** The proposed ELoRA algorithm achieves a 10x reduction in trainable parameters compared to LoRA, while maintaining performance levels. This paper should be accepted in its current form. Addressing my concerns and questions would improve my score.\n\nThe paper introduces an efficient finetuning method, ELoRA, for pretrained language models, addressing the storage challenge when deploying numerous adapted models. ELoRA achieves a 10x reduction in trainable parameters compared to LoRA, while maintaining performance levels, as demonstrated on the GLUE benchmark. It leverages shared random low-rank matrices and small layer-wise trainable scaling vectors to reduce memory usage. This approach is ideal for scenarios requiring multiple finetuned models, such as cloud-based personalized AI services, by improving serving efficiency. The method's potential application in various domains and further optimizations like dynamic parameter allocation are areas for future research."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "* **S.1.** The paper is well written and is easy to follow. The illustrations and results are informative and clear.\n* **S.2.** The proposed ELoRA algorithm seems novel and tackles an important problem.\n* **S.3.** The experiments are conducted on multiple datasets and architectures with 5 different random seeds. ELORA achieves high accuracy while requiring significantly less parameters compared to existing algorithms.\n* **S.4.** The experiments are thorough and an ablation study is conducted."
                },
                "weaknesses": {
                    "value": "* **W.1.** The experiments are conducted solely on NLP tasks with LLMs. Adding experiments on different domains such as diffusions on CV would help.\n* **W.2.** The paper does not provide a theoretical analysis on the ELoRA algorithm or its limitations."
                },
                "questions": {
                    "value": "* **Q.1.** In Table 2 for the RTE dataset the LoRA algorithm achieves significantly higher results on the RoBERTa-base model compared to the the other algorithms. Is this a mistake?\n* **Q.2.** Why are the PEFT algorithms achieving higher accuracy compared FT on the RTE dataset? Is this just overfitting?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5820/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698359854604,
            "cdate": 1698359854604,
            "tmdate": 1699636614462,
            "mdate": 1699636614462,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GMXbnZWwXW",
                "forum": "NjNfLdxr3A",
                "replyto": "CFmZ5JdSeS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5820/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5820/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to rHtj"
                    },
                    "comment": {
                        "value": "Thank you for your constructive feedback and detailed review. We have taken your suggestions and expanded our experiments to encompass not just NLP tasks but also applications in computer vision, demonstrating the versatility of our approach. Our responses below address each of your points, with parts on the vision experiments and the theoretical analysis moved to the common reply.\n\n\n>Adding experiments on different domains such as diffusions on CV would help.\n\nPlease see our response in the \"Experiments on vision models\" part of the [common reply](https://openreview.net/forum?id=NjNfLdxr3A&noteId=xpnpKQguRK).\n\n>Theoretical analysis on the ELoRA algorithm or its limitations\n \nPlease see our response in the \"Theoretical framework/expressivity of ELoRA\" part of the [common reply](https://openreview.net/forum?id=NjNfLdxr3A&noteId=xpnpKQguRK).\n\n \n\n\n>In Table 2, on the RTE task LoRA is significantly better than other algorithms. Is this a mistake?\n\nResults reported for LoRA are taken from the paper that introduced the method [1]. Our reproduced results are _close_ to this value, which may indicate that it's not a mistake.\n\n\n>Why are the PEFT algorithms achieving higher accuracy compared FT on the RTE dataset? Is this just overfitting?\n\nIndeed, overfitting may be the reason of lower performance of FT compared to PEFT methods. It may also explain higher performance of ELoRA on certain tasks, as it comes with highly reduced number of trainable parameters, potentially acting as regularisation.\n\n\n**References**\n- [1] LoRA: Low-Rank Adaptation of Large Language Models, Edward J. Hu et al., 2021"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5820/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700330915793,
                "cdate": 1700330915793,
                "tmdate": 1700330915793,
                "mdate": 1700330915793,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "tOoSgZEDQz",
            "forum": "NjNfLdxr3A",
            "replyto": "NjNfLdxr3A",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5820/Reviewer_iJDt"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5820/Reviewer_iJDt"
            ],
            "content": {
                "summary": {
                    "value": "This paper propose a way of reparametrization of the low-rank matrices in a LoRA method, by freezing a single pair of randomly initialized matrices shared across all adapted layers, and introducing trainable scaling vectors that allow for layer-wise adaptation."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Introduced a simple but elegant way to reduce trainable parameters in a LoRA. This effectively reduces storage cost due to introduced parameters for different downstream tasks."
                },
                "weaknesses": {
                    "value": "- Performance can be hurted using this method, even the evaluation is only on a limited set of GLUE benchmarks. \n\n- The method should be evaluated on generative tasks as well, including translation and question answering. Classification task and summarization task are known to be more resilient to the lose of model capacity or trainable parameters. \n\n- The paper is not well written. Figure 3 and Figure 4 are each on a single but different task. Why not reporting a mean score on GLUE?\n\n- The paper lacks explanation on why Table 3, with fewer tunable parameters, ELoRA has better instruction finetuning scores than LoRA. It is a bit of counterintuitive."
                },
                "questions": {
                    "value": "- Are there any theoretical proof that random matrices with scaling vectors would work better than LoRA?\n\n- Consider evaluating the method on Generative tasks?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5820/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698800124269,
            "cdate": 1698800124269,
            "tmdate": 1699636614304,
            "mdate": 1699636614304,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0DmuScpNrm",
                "forum": "NjNfLdxr3A",
                "replyto": "tOoSgZEDQz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5820/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5820/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to iJDt"
                    },
                    "comment": {
                        "value": "Thank you for your feedback and time to review our paper. Based on this, we have further conducted more experiments on generative E2E task and MT-Bench. Please find our detailed responses to each of your points raised below:\n\n>Performance can be hurted using this method, even the evaluation is only on a limited set of GLUE benchmarks.\n\nThe primary objective of our method is to significantly reduce the number of trainable parameters, and therefore storage requirements, with *comparable* performance to other methods. As detailed in Figure 3, our analysis acknowledges the parameter-performance trade-off inherent in both LoRA and ELoRA methods. While in theory, ELoRA _could_ perform worse than LoRA, in practice we see that with a sufficient number of trainable parameters, ELoRA's performance aligns with that of LoRA, despite our method using much less parameters. \n\nIn addition, we have also increased the types of evaluations by comparing our method on E2E generation  (see answer below) and on instruction following tasks - additionally to the results reported in the initial version of our paper, we are introducing evaluation with MT-Bench, a well-established instruction-following benchmark. Similarly to the initial experiment, we evaluate model finetuned on the *alpaca-cleaned* dataset, this time taking the full training set (40k+ samples), instead of 10k subset. Results are shown in Table 1. \n\n\n**Table 1 Instruction tuning of more models and evaluated on MT-Bench**\n\n| Model        | Method | Parameters | Score |\n|--------------|--------:|------------:|-------|\n| Llama 13B    |       | -          | 2.61  |\n||||\n| Llama 7B     | LoRA   | 159.9M     | 5.03  |\n| Llama 7B     | ELoRA  | 1.6M       | 4.77  |\n||||\n| Llama 13B    | LoRA   | 250.3M     | 5.31  |\n| Llama 13B    | ELoRA  | 2.4M       | 5.22  |\n||||\n| Llama2 7B    | LoRA   | 159.9M     | 5.19  |\n| Llama2 7B    | ELoRA  | 1.6M       | 5.08  |\n||||\n| Llama2 13B   | LoRA   | 250.3M     | 5.77  |\n| Llama2 13B   | ELoRA  | 2.4M       | 5.93  |\n\nFrom this table, we make the following observations:\n - Base models do not perform well compared to instruction tuned versions (scores of 2.6 vs 5.3)\n - ELoRA generally closely matches LoRA's performance. In particular for the larger model sizes, such as Llama 13B the scores are very similar (5.22 vs 5.31) and for Llama2 13B ELoRA even outperforms LoRA (5.93 vs 5.77).\n\n>Consider evaluating the method on Generative tasks?\n\nWe have conducted additional experiments on a generative E2E task with GPT2, medium and large variants, finetuned with LoRA and ELoRA methods, adhering to the setup described in the original LoRA paper [1]. For LoRA we use hyperparameters reported in [1], and for ELoRA we select a rank equal to 1024 and tune the learning rate. Results after 5 epochs are reported in the following table:\n\n| Method               | Parameters | BLEU  | NIST   | METEOR | ROUGE_L | CIDEr  |\n|---------------------|------------|-------|--------|--------|---------|--------|\n| LoRA (GPT2-M)        | 0.35M      | 68.92 | 8.69 | 46.42  | 71.34   | **2.51** |\n| ELoRA (GPT2-M)    | **0.10M**     | **70.08** | **8.81** | **46.55**  | **71.45**   | 2.50 |\n|||||\n| LoRA (GPT2-L)         | 0.77M      | 70.14 | 8.84 | 46.68  | **71.85**   | 2.52 |\n| ELoRA (GPT2-L)     | **0.17M**     | **70.28** | **8.85** | **46.89**  | 71.63   | **2.54** |\n\nAs we can see in the table, our method **performs on-par or better on E2E-NLG, despite using only 22%-28% of the number of trainable parameters of LoRA**. We thank the reviewer for suggesting this additional experiment and will add these results to the paper.\n\n\n\n>Figure 3 and Figure 4 are each on a single but different task. \n\nWhile Figures 3 and 4 are independent and address different aspects, we agree that consistency could enhance the clarity of our presentation.   Consequently, we will update Figure 4 to showcase data from the RTE task, making it consistent with Figure 3.\n\n>Why not report a mean score on GLUE?\n\nIn terms of reporting on the GLUE score, our methodology is in line with that of the LoRA paper [1], which opts to report median values. This approach enables a straightforward comparison between our results and those reported in the LoRA paper and is more robust to outliers.\n\n\n>Why is ELoRA better than LoRA in Table 3?\n\nOne possible explanation is that ELoRA is less prone to overfitting and may achieve better results on particular tasks, especially in low-data regime. This is similar to the existing findings of PEFT methods often achieving better results than full finetuning on some tasks.\n\n>Theory about ELoRA\n\nPlease see our response in the \"Theoretical framework/expressivity of ELoRA\" part of the [common reply](https://openreview.net/forum?id=NjNfLdxr3A&noteId=xpnpKQguRK).\n\n**References**\n- [1] LoRA: Low-Rank Adaptation of Large Language Models, Edward J. Hu et al., 2021"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5820/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700330681682,
                "cdate": 1700330681682,
                "tmdate": 1700330681682,
                "mdate": 1700330681682,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "7ZNJSSnLbz",
            "forum": "NjNfLdxr3A",
            "replyto": "NjNfLdxr3A",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5820/Reviewer_JAXk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5820/Reviewer_JAXk"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel parameter-efficient LLM finetuning algorithm which is considerably more parameter-efficient than LoRA. The idea is to use 2 random matrices shared across all of the layers in the LLM, but to use two sets of diagonal scaling matrices to modulate the shared random matrices differently for each layer. The results indicate that the proposed method requires roughly an order of magnitude less parameters while maintaining or exceeding the performance of LoRA."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1) The paper is very easy to read\n2) The method is simple and easy to understand\n3) The experimental results are extensive. All claims are backed up by experimental evidence. Ablation experiments give further insights"
                },
                "weaknesses": {
                    "value": "I don't really see any weaknesses in this paper"
                },
                "questions": {
                    "value": "1) Have you tried extending this method to finetuning large image models? What about large multimodal models?\n2) What are the training time benefits of your method? Is there a GPU memory benefit? Is there a latency benefit? Does your method train faster than LoRA because it has less parameters?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5820/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699282865373,
            "cdate": 1699282865373,
            "tmdate": 1699636614192,
            "mdate": 1699636614192,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "tZxAbhRM9H",
                "forum": "NjNfLdxr3A",
                "replyto": "7ZNJSSnLbz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5820/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5820/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to JAXk"
                    },
                    "comment": {
                        "value": "Thank you for your positive feedback and your suggestions to further improve the paper! Most importantly, we have now conducted experiments using Vision Transformers and shown benefits of ELoRA in terms of GPU memory. Please see below for our responses to your questions.\n\n>Have you tried extending this method to finetuning large image models? \n\nPlease see our response in the \"Experiments on vision models\" part of the [common reply](https://openreview.net/forum?id=NjNfLdxr3A&noteId=xpnpKQguRK).\n\n>What are the training time benefits of your method? Is there a GPU memory benefit? Is there a latency benefit? Does your method train faster than LoRA because it has less parameters?\n\nFirst, regarding inference latency, both LoRA and ELoRA exhibit no differences. In both methods, the trainable matrices and vectors can be merged into the original weights, ensuring that the inference latency remains on par with the base model. There would, however, be benefits for our lighter-weight adapters, e.g. when serving many of them concurrently [1].\n\nTo evaluate the training time and GPU memory benefits of our method, we conducted a comparison between LoRA and ELoRA while fine-tuning LLaMA 7B with the same rank (64). The results are summarized in the table below:\n\n| Resource         | LoRA    | ELoRA (ours)  |\n|------------------|---------|---------|\n| Training Time    | 568 min | 578 min |\n| GPU Memory Usage | 23.42GB | 21.69GB |\n\nWhile ELoRA includes more operations than LoRA because of the additional vector multiplies in the forward pass, we find that it only results in a modest 1.8% increase in training time.\nFor the GPU memory, we observe a **7.4% reduction in VRAM usage with ELoRA**, as it does not require storing optimizer states and gradients for shared random matrices.\nWe will add this to the paper and again thank the reviewer for raising this point.\n\n\n**References**\n- [1] Sheng et al. S-LoRA: Serving Thousands of Concurrent LoRA Adapters. Nov 2023, ArXiv"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5820/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700330266669,
                "cdate": 1700330266669,
                "tmdate": 1700330266669,
                "mdate": 1700330266669,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "xhSadcuUkF",
            "forum": "NjNfLdxr3A",
            "replyto": "NjNfLdxr3A",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5820/Reviewer_Bg6K"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5820/Reviewer_Bg6K"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes ELoRA, a new method for parameter-efficient finetuning that trains only the scaling vectors for low-rank parametrized versions of linear layers. The proposed method reaches the same quality as LoRA on a variety of benchmarks while having more than 10 times fewer trainable parameters."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The work proposes a simple idea that works quite well in practice, according to the experiments.\n* Authors have a comprehensive evaluation setup, testing ELoRA across several model sizes and datasets and comparing it with baselines.\n* The influence of each component of ELoRA is investigated within a detailed ablation study.\n* Overall, the paper is well-written and all the contributions are easy to understand."
                },
                "weaknesses": {
                    "value": "* The idea of using random matrices for parameter-efficient finetuning has been proposed previously in the context of text-to-image generation [1]. Although the ideas of the submission and that paper are quite different (also, the paper is quite recent), in my opinion, it would be great to mention LiDB (Lightweight DreamBooth) in the related work section to give the reader additional context.\n* My primary concern with respect to the experiments is that the instruction tuning evaluation is performed mostly through GPT-4 queries on a very small dataset. In my opinion, this approach suffers both from the lack of reproducibility (as we know, the model behind the GPT-4 API might change in the future) and a narrow coverage of possible contexts that might result in high variance of the metric. I think that using more established benchmarks (such as MMLU or BIG-Bench) would give a better picture of how instruction-finetuned models perform after PEFT.\n* Similarly, I think that broader evaluation of instruction tuning should include a larger set of models than just LLaMA-2. Personally, I would move Table 4 and Table 5 to the appendix (they take a lot of space and are not directly related to the topic of the paper) and replace that with additional experiments, ideally for models with higher parameter counts.\n\n[1] HyperDreamBooth: HyperNetworks for Fast Personalization of Text-to-Image Models. Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Wei Wei, Tingbo Hou, Yael Pritch, Neal Wadhwa, Michael Rubinstein, Kfir Aberman. 2023"
                },
                "questions": {
                    "value": "* Technically, it should be possible to have different random matrices in different layers (the cost of storing one random seed per layer should be negligible), which might increase the capacity of the model even further. I wonder if you have explored this idea in preliminary experiments? It is quite surprising to me that a single random structure is sufficient for PEFT across all layers in a Transformer model \u2014 perhaps there is something we can infer from that.\n* I think that the work would benefit from a bit more analysis behind the reasons of why random projections work that well in the context of parameter-efficient finetuning. At the very least, it might be interesting to compare the structure of the learned weights for LoRA and ELoRA trained with a single rank: intuitively, the latter should approximate the former, but is that true in practice?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5820/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699538723653,
            "cdate": 1699538723653,
            "tmdate": 1699636614085,
            "mdate": 1699636614085,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7SWmYKpMmZ",
                "forum": "NjNfLdxr3A",
                "replyto": "xhSadcuUkF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5820/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5820/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Bg6K (part 1)"
                    },
                    "comment": {
                        "value": "Thank you for your feedback and your pointers and questions. Please find our answers and our newly conducted experimental results below.\n\n>It would be great to mention LiDB (Lightweight DreamBooth) in the related work section.\n\n\nThank you for pointing this out. We acknowledge the paper's relevance and will include it in our updated related work section.\n\n\n\n>Instruction tuning evaluated on a very small dataset.\n\nWhile using GPT-4 as-a-judge is well-established (e.g. [1,2]), we have now run **additional experiments**: We have evaluated our instruction-tuned models under the well-known multi-turn evaluation, MT-Bench [1], which offers a more robust evaluation compared to the previous method and is one of the key metrics for comparing instruction-tuned methods.\nSimilarly to the initial experiment, we evaluate model finetuned on the *alpaca-cleaned* dataset, this time taking the full training set (40k+ samples), instead of 10k subset. As shown in Table 1 below, we see that our model closely matches the performance of LoRA-based finetuning here too, despite using much less parameters.\n\n>Instruction tuning: Models beyond LLaMA-2 7b.\n\nWe have further increased the **model diversity**: Our additional experiments include two variants of LLaMA-1 and LLaMA-2, specifically the 7B and 13B models. This demonstrates ELoRA's applicability across different model sizes.\n\nIn addition, we now **compare against the baseline model**: We've included results for the base LLaMA 13B model without instruction tuning as a baseline [3].\n\n\n\n\n**Table 1 Instruction tuning of more models and evaluated on MT-Bench**\n\n| Model        | Method | Parameters | Score |\n|--------------|--------:|------------:|-------|\n| Llama 13B    |       | -          | 2.61  |\n||||\n| Llama 7B     | LoRA   | 159.9M     | 5.03  |\n| Llama 7B     | ELoRA | 1.6M       | 4.77  |\n||||\n| Llama 13B    | LoRA   | 250.3M     | 5.31  |\n| Llama 13B    | ELoRA  | 2.4M       | 5.22  |\n||||\n| Llama2 7B    | LoRA   | 159.9M     | 5.19  |\n| Llama2 7B    | ELoRA  | 1.6M       | 5.08  |\n||||\n| Llama2 13B   | LoRA   | 250.3M     | 5.77  |\n| Llama2 13B   | ELoRA  | 2.4M       | 5.93  |\n\nFrom this table, we make the following observations:\n - Base models do not perform well compared to instruction tuned versions (scores of 2.6 vs 5.3)\n - ELoRA generally closely matches LoRA's performance. In particular for the larger model sizes, such as Llama 13B the scores are very similar (5.22 vs 5.31) and for Llama-v2 13B ELoRA even outperforms LoRA (5.93 vs 5.77).\n\nThank you for suggesting this. We will add these results to the paper."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5820/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700329749159,
                "cdate": 1700329749159,
                "tmdate": 1700329749159,
                "mdate": 1700329749159,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]