[
    {
        "title": "Hindsight PRIORs for Reward Learning from Human Preferences"
    },
    {
        "review": {
            "id": "2J7Kk2l2bF",
            "forum": "NLevOah0CJ",
            "replyto": "NLevOah0CJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7430/Reviewer_7k1k"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7430/Reviewer_7k1k"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a new credit assignment strategy for efficiently learning a reward function in preference-based reinforcement learning (PbRL). The preference-based reward model relies on the well-known Bradley-Terry model, and the basic loss consists of the standard cross-entropy loss between the predicted and the true preference labels. The novel contribution of this paper is an additional loss term that redistributes the expected discounted return to each state-action pair in a particular manner. Specifically, a transformer-based forward dynamics model is learned as an auxiliary task, and the expected discounted return is redistributed to be proportional to the attention weights of the state-action pairs. This additional loss term serves as the prior for reward learning, and the authors hypothesize that it leads to sample efficiency and overall policy performance. The empirical evaluation on Deep Mind Control (DMC) and MetaWorld control tasks suggest positive results."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "A major strength of the paper is that the proposed framework is general in the sense that it can be applied to different baseline PbRL algorithms to improve sample efficiency and performance.\nIn the beginning it was not intuitive to believe that the return should be redistributed according to attention weights of the forward dynamics, but the explanation provided in Appendix I is somewhat convincing that the attention weights are reflective of critical events that summarize the whole trajectory, and thus it is those critical state-action pairs that contribute the most to the success/failure of the trajectory."
                },
                "weaknesses": {
                    "value": "The authors propose an intriguing approach to boost the performance of PbRL, but some of the empirical results presented in Section 5 reveal some weaknesses. In particular, in the Drawer Open / 4000 task in Figure 2, the success rate of the proposed approach plateaus after 400k steps and is eventually surpassed by some other baselines. This might imply that the hindsight prior eventually hurts the performance of the learned policy as more preference labels become available. Similarly, in Section 5.3 the authors find that a large coefficient on the prior loss leads to a collapse of the learned policy. Those observations indicate that such a prior is assistive of policy learning only up to a certain point (e.g. relatively small preference data or small prior coefficient), and it may eventually hurt the performance if we exceed those bars.\n\nIn terms of the presentation of the paper, there is much room for improvement. First, some citations and references are missing and appearing as \u201c?\u201d or \u201c??\u201d. Second, some performance plots in Section 5 have too many curves of similar colors and widths, making it difficult to extract information from them. Specifically, I suggest that the authors use different line styles for Figure 2 (in particular for SAC since it\u2019s an oracular baseline). The left two figures of Figure 3 are also hard to read as there are 8 line plots on each tiny figure. Third, the explanation of hindsight prior is confusing and needs elaboration/clarification. (a) The transformer uses H attention heads, but H does not appear in the definition of the attention matrix A. (b) Equation (4) uses the notation $R_{target}$ and $\\hat{R}$, but it is unclear how it is related to $\\hat{\\mathbf{r}}_{\\psi}$ and $\\mathbf{r}_{target}$ in the line above."
                },
                "questions": {
                    "value": "1) In the explanation of equation (3), the authors define H as the entropy. Do you mean \u201ccross entropy\u201d there?\n\n\n2) A recent study [1] finds that reporting the mean and variance for performance evaluation of RL policies is insufficient, and suggests reporting confidence intervals or performance profiles as more objective measures. Have you considered them at all instead of the t-test?\n\n\n[1] Agarwal, Rishabh, Max Schwarzer, Pablo Samuel Castro, Aaron C. Courville, and Marc Bellemare. \"Deep reinforcement learning at the edge of the statistical precipice.\" Advances in neural information processing systems 34 (2021): 29304-29320."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7430/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7430/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7430/Reviewer_7k1k"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7430/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698292978005,
            "cdate": 1698292978005,
            "tmdate": 1699636891993,
            "mdate": 1699636891993,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jQRP4nDNh2",
                "forum": "NLevOah0CJ",
                "replyto": "2J7Kk2l2bF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7430/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7430/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer7k1k"
                    },
                    "comment": {
                        "value": "We thank our reviewer for their recommendation to accept our work at ICLR. We are happy and completely agree with our reviewer that one of the main features of Hindsight PRIOR is its generality to be extended to other domains of interest. \n\nOn the performance plateau on Drawer Open domain : \nWe showcase all of our experiments on 5 seeds. Upon inspecting the performance of individual seeds, we find that the performance drop is mainly due to a single seed <Seed value>. We would like to highlight that we strictly follow prior works for setting up our experiments (i.e. number of seeds etc). Further, while our reviewer presents an interesting hypothesis, the other experiments (8 of 9 domains) do not seem to \u201cplateau\u201d. However, our reviewer is correct in pointing out that PRIOR-obsession can be harmful, but we make that point to highlight the insufficiency of the PRIORs to capture human preference and the need of human feedback data. We believe that our reviewer\u2019s comments indicate the importance of balancing the loss terms especially as the training progresses, which is an important concern for much of Machine Learning that presents auxiliary learning objectives. \n\nOn the explanation to H term : \nIn Equation 3, H is the entropy term used by TWM (their Equation 4) to learn environment dynamics by encouraging similarity between the predicted latent next state and the true latent next state. They overload the notation of H to mean entropy (for entropy regularization) and cross entropy for latent state predictor. We borrow the notation from TWM work for consistency with their work. This objective is separate from the cross-entropy objective used to learn the reward function in PbRL.\n\nTMW paper: Robine, J., H\u00f6ftmann, M., Uelwer, T., & Harmeling, S. (2022, December). Transformer-based World Models Are Happy With 100k Interactions. In International Conference on Representation Learning 2023.\n\nOn our evaluation protocol : \n\nWe thank our reviewer for pointing out the study by Agarwal et al. We would be happy to report confidence intervals in the final version of our work to maintain high quality of reproducibility and reporting of results. Our choice of current evaluation is motivated by high standards of reproducibility as we follow existing works PEBBLE, SURF for the evaluation protocol. The study [1] suggests the need for CI for RL where there exists a large inconsistency in evaluation protocols and how CIs can help. Since PbRL is relatively nascent compared to the large body of RL works, we chose to follow existing state of the art methods for consistent evaluation (in terms of plotting the metrics, hyperparameters, choice of domains etc.). Moreover, the benefits of Hindsight PRIOR can be well understood and appreciated with the current evaluation protocol and given the stark difference in performance our conclusions should remain unchanged. Finally, we agree with our reviewer that PbRL community should also report CIs and we will be happy to include it in our manuscript/appendix upon acceptance of our work. \n\nWe are thankful to our reviewer for helpful comments on improving the presentation of the paper and we have made respective changes based on our reviewer\u2019s comments."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7430/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700140322520,
                "cdate": 1700140322520,
                "tmdate": 1700152340409,
                "mdate": 1700152340409,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OmOPwjnW07",
                "forum": "NLevOah0CJ",
                "replyto": "2J7Kk2l2bF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7430/Reviewer_7k1k"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7430/Reviewer_7k1k"
                ],
                "content": {
                    "title": {
                        "value": "Response to Authors"
                    },
                    "comment": {
                        "value": "Thank you for your comments. I agree with authors on the performance of the PRIORs and the evaluation protocol.\n\nRegarding the word \"entropy,\" the authors of the TWM paper indeed overloads the notation $H$  but does use the term \"cross entropy\" where appropriate (see their explanations around equations (3) and (4) in the TWM paper). Entropy, by definition, depends only on a single distribution, whereas cross entropy depends on two distributions. The distinction here is quite clear and should not be confused. Please make sure in your manuscript to distinguish the two words, even if you overload the notation. And if you choose to overload the notation, please clarify so in the paper.\n\nPlease let me know when your updated manuscript is available on OpenReview. Thanks again for your hard work and comments."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7430/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700507541670,
                "cdate": 1700507541670,
                "tmdate": 1700507567928,
                "mdate": 1700507567928,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Ungjaff4y8",
            "forum": "NLevOah0CJ",
            "replyto": "NLevOah0CJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7430/Reviewer_gM8u"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7430/Reviewer_gM8u"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses the credit assignment problem in preference-based Reinforcement Learning (PbRL) algorithms. Given sparse feedback, it is challenging to determine where rewards should be assigned in a trajectory, i.e., which states are significant. The proposed solution combines the classical PbRL algorithm PEBBLE with a prior obtained from a world model. This approach assumes that states receiving high attention in the world model are likely to be rewarding, assigning them higher weight when estimating the reward function. The algorithm is evaluated on simulated problems from the DMC suite and MetaWorld control."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The issue of preference-based RL and the credit assignment problem is highly relevant, particularly considering the need for numerous samples to accurately estimate the reward function.\n\nThe idea of utilizing the attention layers of the learned world model to identify rewarding state-action pairs is creative and seemingly novel, offering a straightforward but effective solution.\n\nThe approach outperforms other methods, demonstrating its effectiveness in comparison.\n\nThe paper is well-written and presents its content in an understandable manner."
                },
                "weaknesses": {
                    "value": "The primary limitation of this work, as acknowledged in the paper, is its reliance on the assumption that states deemed important by the world model are also significant for reward design. While this insight is valuable, the contribution of the paper might be relatively modest, given that the primary novelty lies in a straightforward implementation of this assumption and its evaluation. With that, the quality of the contribution may not fully meet the requirements for acceptance at ICLR.\n\nThe paper would benefit from additional work to clarify the extent to which the learned attention in world models aids in task characterization for interpretability and transferability, as these are key applications of reward learning (building on Q3 in the paper).\n\n__Typos:__\n\nPage 2, \"Learning World Models\": \"us it to\" should be corrected to \"use it to.\"\n\nPage 4, \"a local minima\" (plural) should be corrected to \"a local minimum.\""
                },
                "questions": {
                    "value": "How do you expect the performance of your algorithm to compare with more sample-efficient algorithms, such as few-shot preference learning [1]?\n\n[1] Hejna, Joey & Sadigh, Dorsa (2023). Few-shot preference learning for human-in-the-loop rl. In Conference on Robot Learning (pp. 2014-2025). PMLR."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7430/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698780142073,
            "cdate": 1698780142073,
            "tmdate": 1699636891879,
            "mdate": 1699636891879,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "FxaJQISoGo",
                "forum": "NLevOah0CJ",
                "replyto": "Ungjaff4y8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7430/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7430/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer gM8u"
                    },
                    "comment": {
                        "value": "We thank our reviewer for their insightful comments and appreciate the feedback that the paper is \u201ccreative, novel, straightforward yet effective\u201d and the manuscript well-written.\n\nOn the quality of our contribution : \nWe argue that simple ideas and straightforward and intuitive implementation, as appreciated by our reviewer (and other reviewers), are strong indicators of extensibility and reusability of the work by the community. For example, past works gaining traction in PbRL community like PEBBLE (realizes a way to use SAC in PbRL), SURF (realizes a way to use a simple semi-supervised objective), etc. also offer a simple, intuitive and effective novelty that can be easily evaluated, extended and leveraged in future works.\n\nWe present Hindsight PRIOR as a combination of several key contributions including our insight into the usefulness of world model attention for reward learning (as appreciated by all our reviewers), a no-cost-to-human way of obtaining priors through our insight, in highlighting the limitation of dependence on the standard cross-entropy loss, and a \u201ccreative\u201d method of incorporating computed PRIORs into the reward learning process via predicted return redistribution. Finally, as noted by all our reviewers, we showcase the advantage of our work over several baselines and perform a thorough evaluation on domains interesting to the PbRL community.\n\nOn Interpretability, Transferability and multi-task PbRL extensions :\nWhile our reviewer raises an interesting point on extended evaluation of Q3, we argue that it should be considered as an \u201cadditional\u201d evaluation, or a next step, rather than necessary for this paper. While interpretability is an important area for Machine Learning, measuring the interpretability of a learned reward function on tasks such as locomotion and manipulation is an open area of research. Similarly, while transferability of rewards is an important area to consider, we follow our baselines and focus on addressing the challenge of sample inefficiency in a single domain. When transferability is an important requirement (such as, when optimizing across a suite of tasks and human preferences can also transfer) the PbRL setting needs to be augmented. That is, the aim becomes to exploit the sub-structure across tasks within the domain for improvement and we appreciate the reviewer highlighting that our method may help with this. In [1] the work assumes knowledge of a suite of tasks and \u201cextends meta learning framework over preference learning\u201d. Hindsight PRIOR can certainly be leveraged in such a setting as we leverage a world model where the challenge would be to learn a world model (shared across tasks) to obtain PRIORs. Our reviewer presents a great natural extension of Hindsight PRIOR for multi-task preference setups, as our work exploits the dynamics information within a task and would be extended to exploiting dynamics sub-structure across tasks. However, due to our experimental setup (information available for single task v/s multi-task) and goal (leveraging preference feedback on current task v/s leveraging task preference similarity across tasks) versus their\u2019s a direct comparison is not suitable. \n\nWe believe the above addresses you questions/concerns. Please let us know if our response missed any part of your questions/concerns."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7430/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700059103299,
                "cdate": 1700059103299,
                "tmdate": 1700059103299,
                "mdate": 1700059103299,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ydZN07YJw0",
                "forum": "NLevOah0CJ",
                "replyto": "FxaJQISoGo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7430/Reviewer_gM8u"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7430/Reviewer_gM8u"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your comments. I still believe that the contribution, i.e., using the attention variables from the world model straight-forwardly as a prior for PbRL without theoretical contribution or reward design analysis, is rather weak for an ICLR paper."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7430/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700656046766,
                "cdate": 1700656046766,
                "tmdate": 1700656046766,
                "mdate": 1700656046766,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "otzSPwVbk8",
            "forum": "NLevOah0CJ",
            "replyto": "NLevOah0CJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7430/Reviewer_4UMz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7430/Reviewer_4UMz"
            ],
            "content": {
                "summary": {
                    "value": "This work presents Hindsight PRIOR, a novel technique to guide credit assignment to improve reward inference in Preference-based Reinforcement Learning. The key contribution in this paper is the utilization of attention weights from a transformer-based world model to estimate state importance and the formulation of return redistribution to be proportional to the attention-deduced state importance. The authors present information regarding related work, their approach, and an empirical evaluation in the Deep Mind Control and MetaWorld Control Suites. The results with a synthetic labeler are positive, displaying PRIOR achieves high success across a variety of tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+ The proposed method is an improvement to PbRL frameworks. Given the references in the paper and how humans may utilize attention similarly to transformer models, utilizing attention weights to redistribute return may improve preference-based reinforcement learning with  end-users.\n+  This paper is well-written and contains sufficient detail to understand the proposed approach.\n+ The evaluation is extensive, and touches on several important questions beyond simple performance."
                },
                "weaknesses": {
                    "value": "- It would be beneficial to note exactly how many trajectory labels such a framework requires. This would help detail whether such a framework would be feasible with actual end-users. Further, including actual tests utilizing this framework with human end-users would provide further evidence that PRIOR works well.\n- Along this thread, it seems the simultaneous learning of a highly parameterized world model and reward model is accomplished faster than other works that simply inferring a reward model, as shown by the sample-efficiency in policy learning. Could you comment on why this is the case? I'm unsure if this relates to a paragraph on page 2 referencing the choice of architecture of the reward network.\n- As PRIOR utilizes PEBBLE as its backbone algorithm, this should be touched on in the related work.\n-  In the evaluation, there are several references that are not labeled correctly and lead to ??. As several of these baselines are not referenced or explained previously, it leads to confusion regarding the results.\n- Could you provide justification on why the attention coefficient for the state and action should be equally waited within the \\alpha coefficient?"
                },
                "questions": {
                    "value": "Please address the weaknesses above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7430/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7430/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7430/Reviewer_4UMz"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7430/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698802440552,
            "cdate": 1698802440552,
            "tmdate": 1699636891745,
            "mdate": 1699636891745,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0rZNR5dMZz",
                "forum": "NLevOah0CJ",
                "replyto": "otzSPwVbk8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7430/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7430/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 4UMz"
                    },
                    "comment": {
                        "value": "We would like to thank the reviewer for their insightful comments, and are happy that they found our work well-written, and our evaluation extensive and answering questions beyond performance.\n\nOn the amount of trajectory labels required by PRIOR :\nWe would like to highlight that the number of steps in Figures 2, 3, and 4 are the number of policy steps (x-axis). Following the backbone PEBBLE, at each step policy learning (SAC update) happens, every few RL steps the reward learning step is executed (for example, 5000 steps for Metaworld) along with query for new human feedback data, and every few RL steps the world model update happens (2000 steps for all experiments). The total amount of feedback is given by the end of policy training. Additional details on reward update schedules are available in Appendix E.1 for our reviewers. Therefore, the world model training steps are \u201cin-between\u201d RL episodes. Since the feedback schedule is based on the RL training steps, the plots highlight feedback efficiency and performance improvements shown by PRIOR. However, the training of the world model indeed poses an additional compute requirement but the additional world model training\u2019s wall-clock time is insignificant compared to other costs like RL training, human feedback time (in a real world setup) etc. We recall that our choice of the world model backbone (TWM) is designed to be sample efficient as is shown by authors of TWM work on Atari benchmark. Moreover, such a cost can be easily reduced as the world model can be trained parallelly with RL training (and the two processes need to be synced only at certain episodes). Finally, we implemented a \u201cserialized\u201d version which performs each step i.e. policy learning, reward update, world model update in a sequential manner and found that the wall-clock time difference was no more than an hour when compared to baseline PEBBLE.\n\nIntuition on balancing the auxiliary loss term : \nAn important feature of Hindsight PRIOR is that the loss coefficient was set by intuition, as the reviewer correctly recalls, to make the two losses \u201cequally weighted\u201d. While better hyperparameter sweep strategies can be used to potentially improve the performance, our intuition was simple that, in the absence of evidence to the contrary, both the loss components should contribute equally to the final optimization and yield a more balanced training. Based on our reviewers suggestion, future research can indeed look at the benefits of dynamically balancing the auxiliary loss.\n\nOn User Study : \nWe thank the reviewer for their suggestion on a human subject study with PRIOR. While additional experiments (as our reviewer highlights) can be helpful in understanding PRIOR\u2019s gains, we believe that our current set of experiments thoroughly investigate Hindsight PRIOR. Specifically, we conduct experiments using \u201cmistake oracle\u201d with high degree of mistake value (i.e. oracle feedback is flipped with some $\\epsilon$) and showcase the robustness of PRIOR over baseline methods to different frequencies of mistake. While a user study may help us find the exact number of feedback, our aim is to present PRIOR as a general approach and highlight the significant performance improvements over baselines.\n\nWe apologize for the missing references which will be corrected during the rebuttal phase (along with other constructive suggestions on manuscript presentation from all our reviewers).\n\nWe believe the above addresses you questions/concerns. Please let us know if our response missed any part of your questions/concerns."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7430/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700059071293,
                "cdate": 1700059071293,
                "tmdate": 1700059071293,
                "mdate": 1700059071293,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GCiFhrLSqL",
                "forum": "NLevOah0CJ",
                "replyto": "0rZNR5dMZz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7430/Reviewer_4UMz"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7430/Reviewer_4UMz"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal Response"
                    },
                    "comment": {
                        "value": "Thank you for your response! Your responses have addressed my questions and concerns."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7430/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700581249442,
                "cdate": 1700581249442,
                "tmdate": 1700581249442,
                "mdate": 1700581249442,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]