[
    {
        "title": "ADOPT: Modified Adam Can Converge with the Optimal Rate with Any Hyperparameters"
    },
    {
        "review": {
            "id": "aMeCDQWrxt",
            "forum": "sJCIv4aUQu",
            "replyto": "sJCIv4aUQu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2942/Reviewer_vPft"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2942/Reviewer_vPft"
            ],
            "content": {
                "summary": {
                    "value": "The authors provide a new algorithm as a variant of Adam. Different from Adam that needs to carefully choose the hyperparameters to ensure convergence, ADOPT can converge at the optimal rate with any hyperparameters.  Moreover, the authors relax the condition that gradients are uniformly bounded into the condition that the second-order moment of gradients is bounded. The experiments show that the proposed algorithm is compatible with the other algorithms."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The proposed algorithm can converge with arbitrary hyperparameters without divergence issues.\n\n2. The proposed algorithm performs compatible with popular algorithms."
                },
                "weaknesses": {
                    "value": "1.  For relaxing the condition of gradient assumption, it is misleading to claim that  \"the convergence is established without the bounded stochastic gradient assumption\". In fact, in the paper, the authors replace the uniformly bounded gradients with bounded second-order moments, which still bounds the gradients of the expected function. Further, [1] shows the convergence of Adam without the assumption of gradients uniformly bounded or second-order moments bounded.\n\n2. In practice, people will use random shuffle instead of random sampling. However, the convergence results only hold for random sampling. In fact, there is a counter-example for the proposed algorithm that can not converge when we use a random shuffle. Let $\\beta1 = \\beta2 = 0$, and $f_1(x) = 1.9x$ and $f_2(x) = f_3(x) = -x$. With the constraint that $x \\in [-1,1]$ the optimal solution should be 1 instead of -1. Thus, although the algorithm can converge in the random sampling setting, it does not deal the case in the practical case.\n\n\n[1] Li, Haochuan, Ali Jadbabaie, and Alexander Rakhlin. \"Convergence of Adam Under Relaxed Assumptions.\" arXiv preprint arXiv:2304.13972 (2023)."
                },
                "questions": {
                    "value": "See weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2942/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698637534747,
            "cdate": 1698637534747,
            "tmdate": 1699636237918,
            "mdate": 1699636237918,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "IXamf38YXO",
                "forum": "sJCIv4aUQu",
                "replyto": "aMeCDQWrxt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2942/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2942/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer vPft"
                    },
                    "comment": {
                        "value": "Thank you for your insightful comments. \nWe will answer your questions to address your concerns.\nPlease also refer to the general response, in which the revision during the rebuttal period is explained.\n\n**Our assumptions seem stronger than existing analyses on Adam**\n\nAs you mentioned, our Assumption 4 requires the true gradient $\\nabla f$ to be bounded although the stochastic gradient $g$ does not need to be bounded.\nHence, the difference between Assumptions 4 and 5 is that Assumptions 4 does not assume the gradient noise $\\|| g - \\nabla f \\||$ to be bounded; instead, it assumes the variance of the stochastic gradient, i.e., $\\mathbb{E} [ \\|| g - \\nabla f \\||^2 ]$, to be bounded, which is weaker than the bounded noise assumption.\n\nAlthough Li et al. (2023) do not assume the true gradient $\\nabla f$ to be bounded, they instead assume that the gradient noise $\\|| g - \\nabla f \\||$ is bounded, which is not assumed in our analysis.\nThey also provide a result where the bounded noise assumption is relaxed to a sub-Gaussian noise norm assumption, but the convergence rate gets a little worse in that case.\n\nTherefore, our assumptions are not quite stronger in total compared to existing analyses on Adam (e.g., Li et al. (2023)), and our result of ADOPT's optimal convergence without depending hyperparameters is much stronger than existing results on Adam, which require problem-dependent tuning of hyperparameters.\nWe also discuss the relation to existing analyses in detail in the general response and Appendix A in the revised paper, so please also refer to them.\n\n**Counter example in the case of random shuffling**\n\nThank you for pointing it out.\nWe did not recognize that there is a counter example where ADOPT cannot converge when applying without-replacement sampling (a.k.a. random shuffling) instead of with-replacement sampling.\nSince random shuffling violates Assumption 2, it is out of scope of our theory; hence the existence of such a counter example does not contradict to our theoretical findings.\nHowever, as you mentioned, practitioners often use random shuffling, so we have come to think that the existence of a counter example in the random shuffling setting should be mentioned in the paper.\nTherefore, we have added a discussion on it in Appendix B in the revised version.\n\nWe would like to emphasize again that the non-convergent issue in such a counter example can be easily avoided by using with-replacement sampling.\nMoreover, the difference between with- and without-replacement sampling becomes negligible when the dataset size is large enough, so it does not affect the practical performance very much.\nIn fact, our experiments except for the toy example are performed using without-replacement sampling, but divergent behaviors are not observed.\n\nWe would be glad to respond to any further questions and comments that you may have.\n\nThanks."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2942/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700441894775,
                "cdate": 1700441894775,
                "tmdate": 1700441894775,
                "mdate": 1700441894775,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "yN5AzfJd6O",
                "forum": "sJCIv4aUQu",
                "replyto": "aMeCDQWrxt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2942/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2942/Authors"
                ],
                "content": {
                    "title": {
                        "value": "A Gentle Remainder to Reviewer vPft"
                    },
                    "comment": {
                        "value": "Thank you again for your efforts in reviewing our paper and your constructive comments. The discussion period will end soon, so please let us know if you have further comments about our reply to your feedback.\n\nThanks."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2942/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700626208495,
                "cdate": 1700626208495,
                "tmdate": 1700626208495,
                "mdate": 1700626208495,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "KNnoSjQDLj",
            "forum": "sJCIv4aUQu",
            "replyto": "sJCIv4aUQu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2942/Reviewer_BAKG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2942/Reviewer_BAKG"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new Adam-variant algorithm that aims to fix the non-convergence issues of Adam. Specifically, it uses the second moment estimation at previous iteration to perform the preconditioning. The preconditioning is done on the gradient instead of on the momentum as in the case of Adam. The preconditioned gradient is used in the update of momentum, and the momentum (not preconditioned momentum) is used in the update of parameters. The authors show that the modified algorithm (namely ADOPT) can converge for all hyperparameters."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is very well written and can be easily understood. It clearly explains the technical challenges arise in analyzing Adam, and how to resolve them. The resulting algorithm seems to be simple and intuitive following the analysis."
                },
                "weaknesses": {
                    "value": "- The theoretical contribution is not very significant. Even though the bounded gradient assumption is relaxed, the paper requires a bound on the gradient norm squared. For example, the bounded gradient assumption is also not required in the recent work by Yushun Zhang et al.[1]. Besides this, the proof technique seems to be quite similar to that of Zhou et al. [2]. \n\n- The experimental gain seems to be marginal. For the results in Table 1 and Table 2, it is better to add standard deviations to clearly contrast the results of different algorithms. \n\n[1] Adam Can Converge Without Any Modification On UpdateRules\n\n[2] AdaShift: Decorrelation and Convergence of Adaptive Learning Rate Methods"
                },
                "questions": {
                    "value": "N/A"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2942/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698736898333,
            "cdate": 1698736898333,
            "tmdate": 1699636237835,
            "mdate": 1699636237835,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "cN7a8Jfrfg",
                "forum": "sJCIv4aUQu",
                "replyto": "KNnoSjQDLj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2942/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2942/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer BAKG"
                    },
                    "comment": {
                        "value": "Thank you for your insightful comments. \nWe will answer your questions to address your concerns.\nPlease also refer to the general response, in which the revision during the rebuttal period is explained.\n\n**On the assumptions of our analysis compared to existing ones**\n\nAs you mentioned, our Assumption 4 is stronger than the growth condition adopted in Zhang et al. (2022).\nZhang et al. (2022) instead assume that the stochastic gradient $g$ is Lipschitz continuous, which is a stronger assumption than ours, in which only the true gradient $\\nabla f$ is assumed to be Lipschitz.\nMoreover, Zhang et al. (2022) focus on the finite-sum problems, whereas we deal with general nonconvex problems.\nTherefore, our assumptions are not quite stronger in total compared to existing analyses on Adam, and our result of ADOPT's optimal convergence without depending hyperparameters is much stronger than existing results on Adam, which require problem-dependent tuning of hyperparameters.\nWe also discuss the relation to existing analyses in detail in the general response and Appendix A in the revised paper, so please also refer to them.\n\n**Proof technique seems to be similar to that of Zhou et al. (2019)**\n\nZhou et al. (2019) only consider a single convex optimization problem in their analysis, but our analysis deals with general smooth nonconvex optimization problems; hence the scope of our analysis is much broader than theirs.\nMoreover, techniques used for the proofs are also very different, so we think that our theoretical contribution is not minor compared to Zhou et al. (2019).\nAbout the theoretical contribution, we have added a detailed discussion in the general response and Appendix A in the revised paper, so please also refer to it.\n\n**Experimental gain seems marginal and error bars are missing in Tables 1 and 2**\n\nIn the revised version, we have added error bars in Tables 1 and 2, which may help to show that the experimental gain is not so marginal.\nIn addition, to clearly show the superiority of ADOPT over Adam (and AMSGrad), we have revised the toy experiment.\nPlease see the general response for a detailed experimental settings.\nThis experiment clearly shows that Adam tends to fail to converge when the gradient noise is large, even if $\\beta_2$ is chosen to be close to 1 (e.g., 0.999), whereas ADOPT can always converge to the solution without careful tuning of hyperparameters. \n\nWe would be glad to respond to any further questions and comments that you may have.\n\nThanks."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2942/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700441930410,
                "cdate": 1700441930410,
                "tmdate": 1700441930410,
                "mdate": 1700441930410,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GfWFSu11Qy",
                "forum": "sJCIv4aUQu",
                "replyto": "KNnoSjQDLj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2942/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2942/Authors"
                ],
                "content": {
                    "title": {
                        "value": "A Gentle Reminder to Reviewer BAKG"
                    },
                    "comment": {
                        "value": "Thank you again for your efforts in reviewing our paper and your constructive comments. The discussion period will end soon, so please let us know if you have further comments about our reply to your feedback.\n\nThanks."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2942/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700626147401,
                "cdate": 1700626147401,
                "tmdate": 1700626147401,
                "mdate": 1700626147401,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "h7ILYUlptI",
            "forum": "sJCIv4aUQu",
            "replyto": "sJCIv4aUQu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2942/Reviewer_nLrX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2942/Reviewer_nLrX"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes ADOPT. Compared with Adam, ADOPT uses a decoupled second-order moment estimator and applies exponential averaging after calculating the adaptive update directions. A wide range of experiments show great potential for the proposed algorithm."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "The proposed ADOPT is a novel algorithm that deserves the attention of deep learning researchers.  The experiments contain a wide range of machine-learning tasks and show promising results."
                },
                "weaknesses": {
                    "value": "The major issue is in the theoretical analysis. I feel that both (5) and (14) are vacuous. From assumption 4, we know $||\\nabla f||^2\\le G^2$. However, (14) basically says\n$$\n\\min_{t=1,\\cdots,T}||\\nabla f(\\theta_t)||^2\\le O\\left(G^2\\frac{\\alpha L}{\\epsilon}\\right)\n$$\nIn practice, the learning rate $\\alpha$ is not too small, $\\epsilon$ has the order of 1e-8, the Lipschitz constant $L$ is very large, so $\\frac{\\alpha L}{\\epsilon}$ is often far greater than $1$. So, the upper bound in (14) is even larger than $G^2$, which is correct by assumption. Therefore, the theoretical analysis is not meaningful.\n\nThe authors may try to look at the theoretical analysis in Shi et al., 2020; Zhang et al., 2022 to derive more meaningful bounds."
                },
                "questions": {
                    "value": "As ADOPT contains two changes compared to Adam, the authors should consider adding some ablation studies in section 5 to see which change has the most significant impact on the performance. Such studies are very informative for the future improvement of adaptive stepsize algorithms. \n\nAlso, in Figure 1 and 3, ADOPT has larger variances. Does this mean ADOPT is less robust than Adam?\n\nDid authors consider adding the bias correction step in the algorithm?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2942/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698790476852,
            "cdate": 1698790476852,
            "tmdate": 1699636237761,
            "mdate": 1699636237761,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "tl91h5FTUF",
                "forum": "sJCIv4aUQu",
                "replyto": "h7ILYUlptI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2942/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2942/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer nLrX"
                    },
                    "comment": {
                        "value": "Thank you for your insightful comments. We will answer your questions to address your concerns.\nPlease also refer to the general response, in which the revision during the rebuttal period is explained.\n\n**Convergence bounds are too loose**\n\nWe agree that the bounds in the original submission were too loose to be meaningful, so we have revised the theorems to derive tighter bounds.\nWe believe that this will address your main concern.\n\n**Ablation study on each algorithmic modification**\n\nThank you for an insightful suggestion. \nWe have added an ablation study on algorithmic modifications from Adam to ADOPT in Section 5.\nAs can be seen in Figure 2 in the revised paper, ADOPT fails to converge if either of the two modifications (i.e., decorrelation between $g_t$ and $v$, and change of order of momentum and scaling operation) is not applied.\nThis result aligns with the theoretical finding, and helps it to be more convincing.\n\n**Large variance of empirical results**\n\nIn Figure 1 of the original submission, ADOPT seems to have high variance when $\\sigma = 10$ compared to Adam and AMSGrad, but we do not think that it is not because ADOPT is less robust.\nFirst of all, Adam does not even converge to the correct solution, so the variance comparison is less meaningful.\nAs for AMSGrad, its variance seems lower than ADOPT, but this is because $v_t$ of AMSGrad is non-decreasing in terms of $t$, so the effective learning rate tends to be lower than ADOPT.\nIf the learning rate of ADOPT is set to a lower value, the variance of ADOPT will also decrease.\n\nIn addition, ADOPT's high variance in Figure 3 of the original submission is just due to the nature of deep reinforcement learning (RL).\nIn deep RL, the variance tends to be larger as the agent obtains high reward, so this phenomenon is quite natural.\n\nMoreover, in the revision during the rebuttal period, we have added error bars in Tables 1 and 2, and we observe that the variance of ADOPT is not higher than baselines (e.g., AdamW).\n\n**On the bias correction technique**\n\nIn fact, we have tried to add the bias correction technique of Adam to our ADOPT, and it empirically worked well.\nHowever, it did not bring performance improvement (also did not harm the performance) in practice, so we exclude it from our implementation for simplicity.\nOur hypothesis on why the bias correction is less meaningful for ADOPT is that $v_t$ is initialized to $1$ for ADOPT to prevent it from being too small at the first parameter update, so the effect of bias correction is limited compared to Adam.\nMoreover, as for the momentum $m_t$, initialization with $0$ works like a warm-up of the learning rate, so it may have a positive effect for stable training especially in the early phase of optimization.\nAlthough there might be a better way to introduce a technique like the bias correction for ADOPT, we leave it for future work.\nWe have added a discussion on the bias correction in Appendix C in the revised version.\n\nWe would be glad to respond to any further questions and comments that you may have.\n\nThanks."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2942/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700441943095,
                "cdate": 1700441943095,
                "tmdate": 1700441943095,
                "mdate": 1700441943095,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "335lsVaRHi",
                "forum": "sJCIv4aUQu",
                "replyto": "h7ILYUlptI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2942/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2942/Authors"
                ],
                "content": {
                    "title": {
                        "value": "A Gentle Reminder to Reviewer nLrX"
                    },
                    "comment": {
                        "value": "Thank you again for your efforts in reviewing our paper and your constructive comments. The discussion period will end soon, so please let us know if you have further comments about our reply to your feedback.\n\nThanks."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2942/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700626058094,
                "cdate": 1700626058094,
                "tmdate": 1700626058094,
                "mdate": 1700626058094,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "x9YmiAz0Xe",
            "forum": "sJCIv4aUQu",
            "replyto": "sJCIv4aUQu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2942/Reviewer_Y8m7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2942/Reviewer_Y8m7"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a new adaptive gradient method called ADOPT, which addresses the non-convergence issue of popular methods like Adam and RMSprop.  The method modifies the calculation of second moment estimates and the order of momentum calculation and scaling operations. Extensive numerical experiments demonstrate that ADOPT achieves competitive or superior results compared to existing methods across various tasks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper introduces a new adaptive gradient method ADOPT that is as easy as the implementation of Adam, and enjoys easy convergence proofs.\n\nThe paper gives in-depth analysis for the convergence of ADOPT with toy examples, in comparison with the failure cases of Adam.\n\nThe paper conducts comprehensive numerical experiments on various tasks, demonstrating the competitive performance of ADOPT \n compared to the widely used Adam."
                },
                "weaknesses": {
                    "value": "First, the convergence of Adam has been established without any modification, e.g., Defossez et al. 2022 \u201cA simple convergence proof of Adam and AdaGrad\u201d, Wang et al. 2022 \"Provable Adaptivity in Adam\" and Zhang et al. 2022 \"Adam Can Converge Without Any Modification On Update Rules\". The convergence of a modified version of Adam is not significant from theoretical sense unless the ADOPT can beat the performance of Adam in practice. \n\nFrom the empirical results, the performance of ADOPT is not superior over Adam very much. People may be reluctant to use ADOPT in practice. As for the title \"convergence with any hyperparameters\", the paper does not verify the performance of ADOPT is not sensitive to hyper-parameters in practice."
                },
                "questions": {
                    "value": "See the weakness"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2942/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699201728598,
            "cdate": 1699201728598,
            "tmdate": 1699636237670,
            "mdate": 1699636237670,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5hx0XMSwLW",
                "forum": "sJCIv4aUQu",
                "replyto": "x9YmiAz0Xe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2942/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2942/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Y8m7"
                    },
                    "comment": {
                        "value": "Thank you for your insightful comments. We will answer your questions to address your concerns.\nPlease also check the general response, in which the revision during the rebuttal period is explained.\n\n**On theoretical contribution**\n\nAs you mentioned, some previous works (e.g., Zhang et al. (2022)) demonstrate that the original Adam can converge without any modifications.\nHowever, they require to choose hyperparameters (e.g., $\\beta_1$ and $\\beta_2$) in a problem-dependent manner, as we stated in Sections 1 and 2.2.\nThis constraint is problematic even in a practical sense, because we do not have access to the problem-specific parameters (e.g., $L$ and $G$ in our paper), so we need to heuristically tune the hyperparameters for each problem.\nHence, to assure the convergence in general cases, algorithmic modifications are still essential as pointed out by Reddi et al. (2018).\nAlthough there have been some investigations on modifying Adam to assure convergence (e.g., AMSGrad), such methods require additional assumptions (e.g., bounded stochastic gradient) for convergence guarantees.\nOur theoretical contribution is on demystifying the fundamental cause of divergence of Adam, and proposing a modification without imposing a strong assumption like bounded stochastic gradient.\n\nTo clarify these points, we have added a detailed discussion in Appendix A in the revised paper, so please also refer to it for more details.\n\n**Empirical investigations on hyperparameter sensitivity**\n\nWe agree that our original experiments lacked the comparison of hyperparameter sensitivity between Adam and ADOPT, so we have revised our toy experiment in Section 5 (see the general response for detailed settings).\nIn the experiment, we observe that Adam's convergence is sensitive to the choice of $\\beta_2$ depending on the problem setting, whereas our ADOPT successfully converge without depending on the hyperparameter choice.\n\nWe would be glad to respond to any further questions and comments that you may have.\n\nThanks."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2942/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700441957687,
                "cdate": 1700441957687,
                "tmdate": 1700535534531,
                "mdate": 1700535534531,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RjND16cigN",
                "forum": "sJCIv4aUQu",
                "replyto": "x9YmiAz0Xe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2942/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2942/Authors"
                ],
                "content": {
                    "title": {
                        "value": "A Gentle Reminder to Reviewer Y8m7"
                    },
                    "comment": {
                        "value": "Thank you again for your efforts in reviewing our paper and your constructive comments. The discussion period will end soon, so please let us know if you have further comments about our reply to your feedback.\n\nThanks."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2942/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700625976475,
                "cdate": 1700625976475,
                "tmdate": 1700625976475,
                "mdate": 1700625976475,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]