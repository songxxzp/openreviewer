[
    {
        "title": "DEEP UNSUPERVISED DOMAIN ADAPTATION FOR TIME SERIES CLASSIFICATION: A BENCHMARK"
    },
    {
        "review": {
            "id": "Z91NkkQr3P",
            "forum": "xsts7MRLey",
            "replyto": "xsts7MRLey",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1464/Reviewer_wzDS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1464/Reviewer_wzDS"
            ],
            "content": {
                "summary": {
                    "value": "This work explores the use of unsupervised domain adaptation (UDA) for time series classification (TSC), with a particular focus on deep learning methods. In UDA, which has been extensively explored in vision and natural language applications, two domains of data exist: a labelled source domain and an unlabelled target domain that has some form of shift in the time series data (e.g. differences in data used for training and data used during deployment). The objective is to leverage the labelled source data to make predicts in the target domain.\n\nIn addition to five existing datasets, this work proposes the use of seven new datasets for UDA TSC (taken from existing sources). This collection of datasets serves as a benchmarking evaluation tool for assessing the efficacy of different UDA TSC deep learning approaches, notably with different algorithms, hyperparameter optimisation approaches, and model backbones. Consistent experimentation is used to compare the performance of these different approaches and make observations of which elements contribute the most to performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "**Originality**  \nO1. The main novelty of the work lies in the proposal of additional datasets and a consistent, fair framework for evaluation the UDA TSC methods. This also extends to the insights that can be drawn from this evaluation.  \nO2. There is also some originality in the deep UDA methods that are used, notably using consistent a consistent backbone (InceptionTime) across different approaches.  \n\n**Quality**  \nQ1. The experimental setup is well-structured and makes steps to ensure fairness across all algorithms (e.g. limiting GPU time for training/hyperopt).  \nQ2. Results analysis provides some comparisons between the choice of different classifiers, hyperopt methods, and backbones.  \n\n**Clarity**  \nC1. Clear descriptions of all the different elements of the experiments are given (models, hyperopt methods, datasets, and pipelines).  \nC2. Figures are communicative and support conclusions drawn from the work.\n\n**Significance**  \nS1. This work could serve as a stable baseline for further developing UDA TSC deep learning approaches, helping to progress the area of research.  \nS2. Insights into the performance of different methods (e.g. InceptionRain seemingly being the strongest method) is useful for establishing the current SOTA and assessing the relative performance."
                },
                "weaknesses": {
                    "value": "**Presentation of Results**  \nP1. While Figure 1 compares model performance within hyperopt methods (a, b, c), it does not provide an overall comparison of all models with all hyperopt methods. As such, it is difficult to determine which complete approach (model + tuning approach) actually has the best performance. An additional critical difference diagram comparing the (5?) top methods for each tuning approach would make this much clearer.  \nP2. While the figures provide some information, and the Appendix gives a full set of results for each dataset, it remains difficult to assess the margins between the approaches. A summary table of average accuracy across all datasets for each experimental configuration would be beneficial in conveying this information.  \nP3. Further variations of Figure 4 for other models/datasets would be useful to see if the revealed trend is consistent.  \nP4. I think the violin plots in Figure 7 are a strong way of communicating the results, and potentially should be moved to the main body if possible. As mentioned above, combining the some selection of the top methods for each tuning approach into a single plot would further aid comparison.  \n\n**Significance**  \nS1. I believe this work has the most potential if the evaluation is released to allow further development of methods. I appreciate the source code is planned to be released upon acceptance, but potentially taking this a step further and allowing for easy reproducibility/extensibility would improve the impact of the work and help progress the research area."
                },
                "questions": {
                    "value": "1. To what extent is there dataset imbalance in the datasets? Additional results, for example using balanced accuracy, may be warranted if dataset imbalanced is high. At the very least, a discussion on any dataset imbalances would be helpful. I appreciate F1 score results are given in the appendix."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1464/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1464/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1464/Reviewer_wzDS"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1464/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698313415353,
            "cdate": 1698313415353,
            "tmdate": 1699636075031,
            "mdate": 1699636075031,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "RCYXqdno0L",
                "forum": "xsts7MRLey",
                "replyto": "Z91NkkQr3P",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1464/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1464/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your valuable comments and suggestions. \nWe particularly appreciate all the strengths you mentioned about our work. \n\nHere are our replies to the weaknesses you raised.\n\n**Presentation**\n- P1. We acknowledge the reviewer's suggestion to incorporate an *additional critical difference diagram* for a comprehensive comparison of all experiments, presenting an overall perspective on performance. This point has been addressed in the Appendix A.2.1. The average accuracy rank across all algorithms and model selection methods delineates three clusters: \\\ni) the first cluster comprises most of the algorithms trained using the target-only method, \\\nii) a second cluster includes OTDA and VRADA, identified as the less performing algorithms across the three hyperparameter selection methods, and \\\niii) a third cluster encompasses the remaining algorithms, fairly/visually distributed between the source-only and IWCV methods. Due to space constraints, we kept this further analysis in the Appendix only.\n- P2. We have included a comprehensive table, where each column corresponding to a specific method (Source Risk, IWCV, Target Risk), in the Appendix A.1.2. The table aims to provide *a summary by averaging accuracy scores across datasets for each classifier*, accompanied by reporting of standard deviations. \n- P3. We have added *variations of Figure 4* with different datasets and UDA algorithms in Appendix A.2.5, as well as an analysis of these graphs. They confirm a certain correlation of the source risk and IWCV with the target risk, but show a plateau for the source risk, and some different optimum for IWCV, thus explaining the gap in performance compare to target risk.\n-P4. We agree with the reviewer's comment regarding the effectiveness of the plots in Figure 8 for conveying results. \n    Due to space limitations, these additional plots have been placed in the appendix alongside all the extra graphs.\n\n**Significance**\n- S1. We completely agree with this point and we would like to answer with the following elements. First, the results are part of the git repository in a CSV format, and will be available when we open-source it for checking reproducibility and going further in the analysis. Furthermore,  the implementation of our benchmark relies on existing tools to strengthen its *reliability, scalability and modularity* and that we believe will help other research to easily add datasets and/or algorithms. These tools include Hydra for the configuration of the pipelines and parameters, HuggingFace for the datasets and evaluation of the models, PyTorch with HuggingFace, scikit-learn and POT for training the algorithms. As a proof of its modularity, we can mention that it only took us a few hours to add an additional algorithm (SASA) to the benchmark and run the experiment with it during the current rebuttal phase.\n\n**Questions**\n- Q1. We appreciate your highlighting of this analytical aspect. To address this, we have included a new section in the Appendix A.1.3 specifically dedicated to emphasizing the *dataset imbalances*. In this section, we introduced a scoring metric, denoted as I, which quantifies the degree of imbalance, where a score of 0 indicates a highly balanced dataset, and a score of 1 signifies high imbalance. The results are presented in a table format, summarizing the number of classes, samples, and I scores for both the source (train, test) and target (train, test) datasets. It is important to note that only three datasets exhibit significant imbalance, posing additional challenges to the classification task. Note that two out of these three datasets are part of the original established set of datasets for UDA TSC."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1464/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700260398780,
                "cdate": 1700260398780,
                "tmdate": 1700296972730,
                "mdate": 1700296972730,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OmsLZybbqk",
                "forum": "xsts7MRLey",
                "replyto": "RCYXqdno0L",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1464/Reviewer_wzDS"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1464/Reviewer_wzDS"
                ],
                "content": {
                    "title": {
                        "value": "Follow Up to Rebuttal for Reviewer wzDS"
                    },
                    "comment": {
                        "value": "Thank you very much for the response and additional work. The authors have done a good job of addressing my concerns and providing additional figures. I have no further follow up questions."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1464/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700478973321,
                "cdate": 1700478973321,
                "tmdate": 1700478973321,
                "mdate": 1700478973321,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "lqVdnkGAgt",
            "forum": "xsts7MRLey",
            "replyto": "xsts7MRLey",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1464/Reviewer_hmFL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1464/Reviewer_hmFL"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors present benchmark research on deep unsupervised domain adaptation (UDA) for time series classification (TSC). Specifically, seven new datasets are introduced for this TSC UDA task, and experiments of several existing TSC UDA baselines are tested on these datasets."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "Strength:\n\n1.\tThe paper introduces 7 new datasets for TSC UDA task.\n\n2.\tThe paper conduct experiments on several existing UDA baselines on the new datasets.\n\n3.\tThe paper has potential to be a benchmark for the following TSC UDA research."
                },
                "weaknesses": {
                    "value": "Weakness:\n\n1.\tThe major concern of the work is on the technical novelty. All the datasets and baselines (including hyper-parameter tuning methods) are from existing literatures, and there is no novel technical contribution proposed. \n\n2.\tFor UDA TSC, some important related works are missing, for instance (to name a few), unsupervised video domain adaptation [ref1], transfer gaussian process [ref2], and time-series domain adaptation [ref3]. The authors may need to present a more comprehensive related work section to discuss more related works. \n\n3.\tMore analyses on the new datasets are expected, for instance, the domain discrepancy analyses (both marginal and conditional can be involved) on different domain pairs. From the experiments results of the source only baseline, the domain discrepancy differs considerably among different domain pairs, see, Table 5 domain 0 and domain 3, Table 9 domain 9 and domain 18. \n\n4.\tMore analyses on the comparison results are also expected. For instance, analyzing why some baselines achieve positive transfer on some tasks but negative transfer on others, e.g., see OTDA/VRADA in table 9. It would be more interesting to see constructive insights or conclusions that can benefit the community. \n\n[ref1] Video Unsupervised Domain Adaptation with Deep Learning\n\n[ref2] Adaptive Transfer Kernel Learning for Transfer Gaussian Process Regression\n\n[ref3] Time Series Domain Adaptation via Sparse Associative Structure Alignment"
                },
                "questions": {
                    "value": "Please refer to the weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1464/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698635943999,
            "cdate": 1698635943999,
            "tmdate": 1699636074965,
            "mdate": 1699636074965,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "hkpbanMkZd",
                "forum": "xsts7MRLey",
                "replyto": "lqVdnkGAgt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1464/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1464/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your valuable comments and suggestions. Here are our replies to the weaknesses you raised:\n1. We agree with the reviewer that the technical novelty is limited, but we think this is expected from any benchmark study, as the goal is to assess the quality of existing algorithms on data which should be available openly to the community. Still, we have not limited ourselves to existing TSC UDA dataset and we adapt 7 diverse time series datasets to UDA settings which have never been used before for UDA. Moreover, we have not limited ourselves to simply comparing existing methods, as this could be expected from a benchmark, but we proposed the usage of the Inception backbone that, not only achieves the best performance, but also allows a fair comparison between SOTA methods having different backbones.\n2. We thank the reviewer for pointing out these references. We believe the work on  UDA for video data [ref1] is a particular field and should not be included here, as it is a combination of image (spatial) and time series (temporal) problems and therefore goes beyond the scope of our study. Nevertheless we have added the latter paper as part of our references of UDA for computer vision tasks. \n    Furthermore regarding the Gaussian Process Regression [ref2], although the paper tackles time series and tabular data, it focuses on regression (data extrapolation) rather than classification tasks and does not tackle the unsupervised DA setting. Nevertheless we have now mentioned regression DA task in the introduction section.\n    Finally, regarding the SASA model [ref3], we strongly agree with the reviewer that this reference is relevant to our benchmark. We therefore have added the Section A.4 in the appendix where we present the results of SASA (with both LSTM and Inception backbones) over all 54 datasets (accounting to almost 1296 hours of gpu time during the current rebuttal phase). However, we decided to keep it outside the main benchmark as it is more suited for the task of detecting an event within a time series, and not classifying the complete input time series, since it is making the decision based on the last few time stamps, which probably explains its bad performance in our study. Comments were also added in the main paper to mention SASA. \n3. We strongly acknowledge the significance of quantifying the shift between various source and domain pairs, as it holds the potential to offer valuable insights into when domain adaptation proves beneficial and which algorithm is most effective. However, given that quantifying the shift in DA is already difficult, adding to that the complex dynamics of a time series data, we believe that this domain shift quantification is an ongoing area of research that we plan to address in our future work. The complexities stem from the unique nature of time series data, where shifts can manifest in time, features, or a combination of both. Measuring domain discrepancy, e.g. through metrics like Maximum Mean Discrepancy or Wasserstein distance, on different domain pairs may not provide sufficient information about time series shift. As an illustration of this difficulty, OTDA leverages this Wasserstein distance, which quantifies the shift, but does not work well for time series data in our benchmark.\n    We found that using the performance of Inception (i.e. with no adaptation) was a quick and easy proxy to compute the shift, which can be seen in Figure 3, although we cannot assess how good a proxy it is.\n    We also added a paragraph in Appendix A.1.1 to explain some of the discrepancies for specific cases (e.g. when we know that one domain is more noisy than others). \n4. We believe that there is already some interesting insight for the community even if they are indeed quite general. We cannot really enter too much into details of combination of algorithm/dataset.\\\n  i. The limited impact of IWCV compared to Source Risk, which is now additionally supported by the added Figure 6 that mix the methods.\\\n  ii. The limited impact of backbone, additionally supported by the comparison for different dataset theme and method.\\\n  iii. The fair comparison between algorithms with the same common Inception backbone, and the good performance of InceptionRain.\\\n    We have also added some more analysis in the Appendix following the other comments."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1464/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700260302676,
                "cdate": 1700260302676,
                "tmdate": 1700296347453,
                "mdate": 1700296347453,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Q1T3fk9Ov4",
                "forum": "xsts7MRLey",
                "replyto": "hkpbanMkZd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1464/Reviewer_hmFL"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1464/Reviewer_hmFL"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the authors' response."
                    },
                    "comment": {
                        "value": "I really appreciate the authors' reponse to my comments. However, I am still not convinced. \n\nRegarding point 1, as a paper submitted to the technical track of ICLR, technical contribution of the paper is the key point to be evaluated. The reviewer suggests the authors to conisder some review/benchmark specific tracks which are more matched with the content of the current paper. \n\nRegarding point 2, I am not sure how the authors define the \"time series\" data as video data is definitly temproal data although it contains more spatial information. Video UDA is also a popular research problem. I highly suggest the authors to include this research line into their work for a more comprehensive benchmark study. [ref2] can be easily adapted to classification as regression is actually a more general task. I agree that it is not specific to UDA setting, but i encourage the authors to have a discussion in related work. Moreover, unsupervised domain adpatation regression is a clear related research line that should be included. \n\nRegarding point 3, the reviewer agrees that quantifying the domain shift of temporal domains is not easy, and thus it would be a good point to increase the technical value of the paper. I highly encourage the authors to work out this point which may not only introduce more technical contributions but also support the emprical analyses.  \n\nLastly, I agree the paper contains interesting points as I stated in the strength. However, I also have some concerns which lead me to the current score. I would suggest the authors to consider some review/benchmark tracks with my comments taken into account in the revision."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1464/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700449030261,
                "cdate": 1700449030261,
                "tmdate": 1700449030261,
                "mdate": 1700449030261,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "uWnBkqUE5l",
            "forum": "xsts7MRLey",
            "replyto": "xsts7MRLey",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1464/Reviewer_KuTn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1464/Reviewer_KuTn"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a thorough benchmarking study on time-series unsupervised domain adaptation, primarily focusing on deep learning techniques. It examines the impact of model backbones and hyperparameter tuning approaches. Furthermore, the authors evaluate various existing unsupervised domain adaptation methods across multiple domains, including seven new benchmark datasets."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The study delivers a detailed benchmark on unsupervised domain adaptation for time-series data, delving into the effect of domain adaptation algorithms, model backbones, and hyperparameter tuning strategies.\n2. The paper evaluates a range of unsupervised domain adaptation methods on datasets from diverse domains, including seven newly introduced datasets and existing benchmarks."
                },
                "weaknesses": {
                    "value": "1. The discussion on the effect of model backbone in the paper is limited primarily to the Inception model. A broader examination involving diverse backbone models is crucial to substantiate the claim that \"backbones do not have a significant impact\".\n2. More discussions on different types of unsupervised domain adaptation methods would be beneficial. Specifically, it would be informative to explore under what specific conditions certain domain adaptation approaches may outperform others.\n3. Additional discussions regarding the choice of model backbones is helpful too. For example, I am curious if Inception is the best model backbone over all domains, or we need different model backbones for different time-series domains and data characteristics.\n4. Figure 2 (b) and 5 should be merged since they lead to similar findings."
                },
                "questions": {
                    "value": "See Weaknesses above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1464/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1464/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1464/Reviewer_KuTn"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1464/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698810719114,
            "cdate": 1698810719114,
            "tmdate": 1699636074893,
            "mdate": 1699636074893,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "vpilXWU6vN",
                "forum": "xsts7MRLey",
                "replyto": "uWnBkqUE5l",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1464/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1464/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your valuable comments and suggestions. Here are our replies to the weaknesses you raised:\n 1. Note that we compare Inception with 3 different architectures that are proposed in the literature (CoDATS, Raincoat and CoTMix). Because it is hard to compare the different backbones of different methods directly, we chose the Inception backbone as reference. Following your remark, we added a section (Appendix 2.6) and three plots in Figure 11, that compare the 2 additional backbones individually with Inception. The same conclusion holds for each of them. Furthermore With this result, we believe that there is enough argument for the limited impact of backbone in time series UDA. Additionally, a proper comparison of another classical backbone would lead to around 5 months of GPU\u2019s time. \n2. We agree with the reviewer about the necessity of a deeper analysis. However, we believe part of this analysis relies on an estimation of the shift between the datasets to understand how much shift in the data each method can support. The estimation of the shift being still an open issue and difficult to tackle, the best we proposed here is an analysis about the difficulty of a dataset, which we can assess with accuracy scores as well as standard deviation from pairs of source-target.\n3. Firstly, the InceptionTime algorithm has been found to be the best deep learning algorithm in a recent benchmark for Time Series Classification [ref1], which is the reason why we chose this backbone for a common comparison of the UDA approaches.\n    Following your comment, we added in Section A.2.6 the Figure 12 to provide a better comparison about the differences between various types of dataset (Machinery, Medical, Motion and Remote sensing) and add comments, especially on a potential bias toward Motion dataset for existing methods due to existing and already established UDA TSC datasets. The latter observation justifies the need to expand the list of UDA TSC datasets and improve generalization to different TSC domains, which is one of the main contributions of this paper. \n4. Indeed, both figures have similar finding and can be merged but we still think that focusing specifically on Raincoat, which is the best method, is still important to support our claim: it is the Raincoat\u2019s UDA method behind the better performance rather than its backbone.\n\n[ref1]: Ruiz, A.P., Flynn, M., Large, J. et al. The great multivariate time series classification bake off: a review and experimental evaluation of recent algorithmic advances. Data Min Knowl Disc 35, 401\u2013449 (2021)"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1464/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700260167862,
                "cdate": 1700260167862,
                "tmdate": 1700297183618,
                "mdate": 1700297183618,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CCaeJP2iyj",
                "forum": "xsts7MRLey",
                "replyto": "vpilXWU6vN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1464/Reviewer_KuTn"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1464/Reviewer_KuTn"
                ],
                "content": {
                    "comment": {
                        "value": "Thank the authors' efforts for the response and additional experiments. I still have some concerns after reading the rebuttal. \n\n1. Model Type Comparisons: The current analysis incorporates three to four specific models, which provides a valuable starting point. However, for a benchmark study, more comprehensive comparisons across different model types would be better. For example, how do CNN, RNN, Transformer based models compare with each other? How does the depth of the network affect performance? \n2. Analysis of UDA Methods Selection: I hope to see some analysis on the selection of UDA methods. For example, it would be great if we could have a figure that illustrates the performance of different UDA methods under varying degrees of distribution shift from source to target.\n3. The author wrote \"we added a section (Appendix 2.6) and three plots in Figure 11\", but I only see two plots in Figure 11."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1464/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700556044326,
                "cdate": 1700556044326,
                "tmdate": 1700556044326,
                "mdate": 1700556044326,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]