[
    {
        "title": "Prioritized Soft Q-Decomposition for Lexicographic Reinforcement Learning"
    },
    {
        "review": {
            "id": "NuHQbvzf9r",
            "forum": "c0MyyXyGfn",
            "replyto": "c0MyyXyGfn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5595/Reviewer_rq81"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5595/Reviewer_rq81"
            ],
            "content": {
                "summary": {
                    "value": "The paper addresses the challenges of lexicographic MORL problems by introducing a novel approach called prioritized soft Q-decomposition. This technique leverages the value function of previously learned subtasks to constrain the action space of subsequent subtasks. The experimental results conducted on both simple and complex scenarios substantiate the efficacy of this method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is well organized and easy to read.\nThe proposed method, in its simplicity, manages to be effective in tackling the problem."
                },
                "weaknesses": {
                    "value": "The proposed method appears to be sensitive to the parameter \u03b5, and the manual selection of this parameter is non-trivial. \nAdditionally, the paper falls short in providing a comparative analysis with existing lexicographic MORL algorithms."
                },
                "questions": {
                    "value": "1. The paper employs equation 7 to approximate \u03b5-optimality based on equation 1, but the relationship between the two equations is not very clear. Including more theoretical insights could enhance the paper's rigor.\n2. It's worth pondering whether \u03b5i should be state-dependent. Is it possible to find a constant \u03b5i that precisely represents the task requirements? If not, it might indicate that the action space is overly restricted in some states, hindering exploration in subsequent tasks, or, conversely, that undesired actions cannot be excluded in certain states."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5595/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698290104439,
            "cdate": 1698290104439,
            "tmdate": 1699636576688,
            "mdate": 1699636576688,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "pdn974jCay",
                "forum": "c0MyyXyGfn",
                "replyto": "NuHQbvzf9r",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5595/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5595/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer rq81"
                    },
                    "comment": {
                        "value": "We kindly thank reviewer *SKG7* for their review of our work. In the following, we answer the brought forward criticism and questions.\n\n### **$\\varepsilon$ threshold sensitivity**\n\n*Reviewer rq81 is concerned about the sensitivity of our method with respect to manually selected thresholds.*\n\nOur method is indeed dependent on the manually selected $\\varepsilon_i$ thresholds, however, this is the case for all $\\varepsilon$-lexicographic MORL methods, since these thresholds are part of the problem definition. In the limitation section of our paper, we suggest multiple approaches to finding adequate values for $\\varepsilon_i$ and note these are fundamentally problem-dependent, the same way that a reward function is.\n\nSee also our answer \"Threshold ablation\" to reviewer *tsYf*. \nWe have included an additional ablation study with different $\\varepsilon_i$ values in the revised manuscript (see Appendix H.3), to further illustrate how these scalars affect performance and agent behavior.\nWe further note that we describe how adequate $\\varepsilon_i$ thresholds can be selected in Section 6 of the paper.\n\n### **Comparison to existing lexicographic RL algorithms**\n\n*Reviewer rq81 suggests additional comparison with lexicographic RL algorithms.*\n\nAs we write in the related work section, we believe to contribute the first algorithm for **continuous** action-space lexicographic MORL problems. As stated in our answer \"8. Discrete PSQD version\" to reviewer *SKG7*, a discrete version of our algorithm would be conceptually similar to existing works and we do not consider this comparison informative for learning in continuous spaces.\n\n### 1. Relationship between Eq.(1) and Eq. (7)\n\nEquation (1) defines a general lexicographic constraint and can also be found in related works on lexicographic RL. Equation (1) simply states that the optimization of subtask $i$ is constrained to a set $\\Pi$ of policies, where all policies in $\\Pi$ are also (near-) optimal to all higher-priority subtasks ${1, \\dots, i-1}$. \n\nSince the explicit computation of $\\Pi$ is not practical, especially for probabilistic policies with continuous action spaces, in Eq. (7) we instead make a state-based version of Eq. (1), where the performance measures $J_i$ are the Q-functions. This is also consistent with existing related work on lexicographic RL. \n\nWe hope this clarifies the relationship of these two equations.\n\n### 2. State-dependent $\\varepsilon_i$\n\n> It's worth pondering whether \u03b5i should be state-dependent.\n\nThis is indeed an interesting discussion and an interesting research topic.\n\nHowever, we want to highlight that, although $\\varepsilon_i$ are fixed scalars, the Q-functions to which these thresholds relate are very much state-dependent. In effect, this results in indifference spaces that are very large (i.e. permissive) in some states, while only being restrictive in states *where it really matters*. For example, when far from the obstacle, the lexicographic constraint on obstacle avoidance allows nearly every action, however when close to the obstacle, it forbids the selection of those actions that would lead to obstacle collision (since those actions are clearly sub-optimal for the obstacle-avoidance task).\n\nThis can also be seen in Figure 7 in the appendix, where we visualize the agent and the indifference space at multiple locations in the environment. Most of these are relatively close to the obstacle, nevertheless, the indifference space in Subfigure 7.h is much larger and more permissive than in Subfigure 7.d or 7.f, due to varying proximity between the agent and the obstacle.\n\nWe hope this addresses reviewer *rq81*'s questions regarding our work and again thank them for their review."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5595/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700239681567,
                "cdate": 1700239681567,
                "tmdate": 1700239681567,
                "mdate": 1700239681567,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zn3biALwkx",
                "forum": "c0MyyXyGfn",
                "replyto": "pdn974jCay",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5595/Reviewer_rq81"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5595/Reviewer_rq81"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the author's response.\n\nI still have some concerns regarding Eq. (7) and Eq. (1). Are they theoretically equivalent? Alternatively, does Eq. (1) yield Eq. (7) with a state-dependent $\\epsilon(s)$, , where the function $\\epsilon(s)$ is related in some way to the original $\\epsilon$ in Eq. (1)?"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5595/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700571880343,
                "cdate": 1700571880343,
                "tmdate": 1700571880343,
                "mdate": 1700571880343,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "3AwBrnuLq1",
            "forum": "c0MyyXyGfn",
            "replyto": "c0MyyXyGfn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5595/Reviewer_SKG7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5595/Reviewer_SKG7"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes an algorithm called prioritized soft Q-decomposition (PSQD) to solve complex lexicographic multi-objective reinforcement learning (MORL) problems. In the setting, n subtasks are prioritized, and the available policy set is reduced as each subtask is optimized in the predefined order. Instead of explicitly representing the available policy sets, the authors consider restricting action space to satisfy the lexicographic order. For implementation, Soft Q-learning (SQL) is adopted to deal with continuous action space. Numerical results show that PSQD performs better than the other baselines in the considered environments, where there are two subtasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The authors provide mathematical formulations on the main paper to support the soundness of the algorithm.\n- This paper contains a dense appendix for detailed explanations.\n- The authors provide extensive study on previous works."
                },
                "weaknesses": {
                    "value": "1. Presentation needs to be improved. While the considered setting - lexicographic MORL - is clear, the flow of the algorithm is hard to understand. High-level pictograms can help the readers understand the content.\n\n2. There is no pseudocode, so I am confused about the implementation of the algorithm. As far as I understand, the proposed algorithm is one of the following:\n\nCandidate 1) For subtask 1 to n-1, run parallel SQL in equation (12). Then restrict action space satisfying epsilon-optimalty of subtasks 1 to n-1. In the restricted action set, run SQL for subtask n in equation (12) and recover optimal policy using (13).\n\nCandidate 2) For subtask 1, run SQL in equation (12). Acquire restricted action space A_1.  Run (12) on A_1 and acquire A_2. ... After acquiring A_{n-1}, run (12) and (13) for subtask n.\n\nWhich one is right? Please provide a pseudocode.\n\n3. For clarity of the setting, I want to raise several fundamental discussions regarding the lexicographic setting.\n- When lexicographic MORL setting is considered in practice?\n- What is the clear difference between constrained (MO)RL?\n- Who decides the priority order? Is it valid to assume that we always know the priority order?\n- How do orders of subtasks affect the final performance in experiments? If the order is crucial, discussion on setting order is important.\n- What if there are tasks that are not prioritized (e.g., \"equally\" important, or \"we do not know\")?\n\n4. Experiments deal with only two subtasks. Can the authors show another environment containing more than two subtasks?\n\n5. For reproducibility, it would be better for the authors to provide anonymous source code.\n\n6. There is confusion in eq. (7). Does Q_i in eq. (7) mean the optimally trained one (i.e., Q_i^*)?"
                },
                "questions": {
                    "value": "Please check the weakness part. Additional questions are as follows.\n\n7. In number 2 in Weakness, if one of the candidates is PSQD, do the authors think that the other one is also a valid algorithm? (It looks like candidate 1 does not use order information of 1 > ... > n-1). \n\n8. If SQL is used, PSQD can be extended to discrete action space since the integral is changed to summation in equation 5. Then we may compare PSQD with the previous work of Skalse et al (2022). \nAlso in that discrete action case, do authors think that action set refinement is still valid?\n\n9. Confusing notation in eq (1). J_0, epsilon_0, Pi_0 is not explicitly defined."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5595/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5595/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5595/Reviewer_SKG7"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5595/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699087965906,
            "cdate": 1699087965906,
            "tmdate": 1700730365985,
            "mdate": 1700730365985,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "saWR4gz0ou",
                "forum": "c0MyyXyGfn",
                "replyto": "3AwBrnuLq1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5595/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5595/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer SKG7"
                    },
                    "comment": {
                        "value": "We kindly thank reviewer *SKG7* for their review of our work. In the following, we answer the brought forward criticism and questions.\n\n### 1. & 2.\n\nWe fully agree with reviewer SKG7 that pseudocode and pictograms will benefit understandability. \n\nTherefore, in the revised version of the manuscript in Appendix \"D PSQD algorithm details\", we have added algorithm pseudocode for the pre-training step, the adaptation step, a pictographic overview of our framework (Fig. 5), and additional details regarding our method. We will release our full codebase once the paper is accepted, which will benefit the understandability and reproducibility of our work.\n\n### 3\n+ 3.1 We believe lexicographic MORL has numerous benefits over traditional MORL which relies on scalarization: Lexicographic MORL algorithms allow for the transfer of simple subtask agents to complex lexicographic problems, benefits interpretability by decomposition, and offer constraint-satisfaction guarantees that can make for a safe exploration framework. Our method is for continuous action-space MDPs, unlike existing lexicographic RL methods for discrete action-space MDPs. We make an important, fundamental contribution by extending lexicographic MORL to continuous action-space MDPs, which have many applications in real-world problems, like robot control.\n\n+  3.2 Lexicographic MORL can be seen as a special form of constrained MORL, where the constraints are of a lexicographic nature and for a chain-like directed, acyclic graph. This is not the same as, e.g. constrained policy optimization (Achiam et al, 2017), which places multiple constraints on the policy that are not lexicographically ordered. Furthermore, in lexicographic MORL, the constraints are based on the learned subtask solutions (e.g. Q-functions) and not based on manually defined functions, as is usually the case in constrained MORL.\n\n+  3.3 The RL practitioner/designer decides on priority order. Priority order is part of the problem definition, in the same way that defining the MDP (state-space, action-space, reward function) is. Defining a lexicographic RL problem requires a priority order, in the same way that defining a regular RL problem requires a reward function. If the RL practitioner can not decide the order of two subtasks, it is possible to treat those subtasks with equal priority by simply summing those corresponding reward functions. We discuss this more in our response below to question 3.5.\n\n+ 3.4 We have experiments with three subtasks in Appendix G.2. These experiments show that changing the priority order changes the resulting behavior, which is the expected result. Changing the priority order changes the problem definition, therefore comparing the performance of multiple such agents does not make sense. Related to this is also our new ablation study on the priority thresholds in Section H.3 in the appendix. \n\n+ 3.5 If tasks are of equal importance, then it is valid to sum their reward functions. We can then learn a single Q-function $Q_{1+2}$ for $r_1 + r_2$. We can use $Q_{1+2}$ like any subtask Q-function and define a corresponding $\\varepsilon_{1+2}$ threshold for this Q-function. Keep in mind, however, that simply adding the subtask reward functions can produce unexpected behavior, especially when these subtasks Q-functions are semantically incompatible. We argue that practitioners should consistently order tasks by priority. If the tasks are compatible, then the lexicographic constraints do not hurt performance, and if they are incompatible, we have performance guarantees for the higher-priority tasks, which is not the case for MORL methods that don't use priority. \n\n### 4.\nWe have experiments that use more than two subtasks in Appendix G.2.\n\n### 6.\n\n$Q_i$ in Eq. (7) refers to the subtask Q-functions of an agent that is lexicographically optimal for all $n-1$ higher-priority tasks. These are not optimal Q-functions $Q_1^*, \\dots, Q_n^*$. We have changed the sentence preceding Eq. (7) to emphasize this.\n\n### 8.\n\nAs discussed in the introduction and related work section, the primary contribution of our paper is an algorithm for *continuous* action-space MDPs. A discrete version of PSQD would be conceptually similar to existing methods like value-based lexicographic MORL (Skalse et al. 2022), lexicographic value iteration (Wray et al. 2015), or TLO (Zhang et al, 2023), which all require finite action spaces.\n\n### 9.\n\nThis is the standard lexicographic constraint consistent with related work. As we write in the paper $J_i$ are some performance measures (like value functions, Q-functions), the symbol $\\pi$ is defined as policy, and the symbol $\\varepsilon_i$ is defined, in the next line, a performance threshold scalar for subtask $i$.\n\nWe again thank reviewer *SKG7* for their review of our work, which we believe allowed us to improve the understandability and reproducibility of our work, due to the added pseudocode and picographic overview."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5595/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700239304210,
                "cdate": 1700239304210,
                "tmdate": 1700239304210,
                "mdate": 1700239304210,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "OoC6vt98UA",
            "forum": "c0MyyXyGfn",
            "replyto": "c0MyyXyGfn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5595/Reviewer_tsYf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5595/Reviewer_tsYf"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a new methods to learn Reinforcement Learning policies that obey lexicographic subtask constraints. To make this efficient, the presented method creates zero shot Q-functions and strategies for the priority constrained task by composing transformed versions of the individual subtasks through their limitation to the indifference space of actions for the higher priority tasks and the transformation of the reward and Q value function to infinitely penalize such action choices. The resulting one shot version of each task value function and policy can then be adapted offline or online to achieve potentially near-optimal performing policies.\nThe main contributions in the paper are in the novel methodology and corresponding learning algorithm to form RL strategies by composing subtask functions such as to obey priority constraints expressed in the value function of the higher level tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper tries to address two very important problems in Reinforcement Learning for control tasks in: i) providing an effective means of composing overall behavior from learned subtasks without significant need for new data collection and re-learning, and ii) to enforce strict priority constraints during composition as well as subsequent learning to optimize from the initial one-shot policy. These abilities are very important in the area of robot control in order to be able to enforce learned safety and performance constraints when new task compositions have to be learned.\nThe approach seems overall sound (although the description is lacking in a few places) and the results demonstrate that the method can form both single-shot strategies and more optimized policies using additional off-line (or on-line training) training that maintains the highest priority constraint."
                },
                "weaknesses": {
                    "value": "The main weaknesses of the paper are in a lack of discussion of the full range of situations in the presentation of the underpinnings of the framework and in the incomplete description of the experiments presented in the paper.\nThe former here seems the biggest weakness and mainly relates to the complete absence of a discussion and consideration of cases where subtasks might not be compatible and thus the indifference space of actions might become empty. It would be very important for the authors to discuss this situation and how the algorithm would react under those circumstances. This is even more important as in the description of the decomposed learning task only the highest and the lowest priority subtasks are discussed  (where in the case of contradictory tasks, the lowest priority task would no longer have a usable actions space and could therefore not have a policy). \n\nIn terms of the experiment presentation, it would be very useful if the main paper could contain at least a basic description of the environment, the action space, and in particular the subtasks and corresponding reward functions. For the latter (the reward functions), even the Appendices do not seem to contain more than an rough description of the principles of the reward function of the obstacle subtask. It would be very important for reproducability but also for a better understanding of the reader if the authors were to include the exact reward function for each subtask as well as the \\epsilon thresholds that were used for the experiments presented in the paper (and these should be in the main paper).\n\nAnother slight weakness is that while the paper indicates that the pick of thresholds \\epsilon is difficult, it does not provide any analysis of this. A brief ablation in term of \\epsilon for the obstacle task in the 2D navigation experiment would have been very useful, as would be a brief discussion how such thresholds might be picked and what the tradeoffs of different picks are."
                },
                "questions": {
                    "value": "The main questions arise out of the weaknesses stated above:\nHow does the proposed approach deal with incompatible subtasks ? Does it simply eliminate all tasks with empty indifference action sets for higher priority tasks and then operate in the same way as presented in the paper ?\n\nHow sensitive is the approach to the specific choice of \\epsilon thresholds ? Is there a way that an ablation could be performed that would investigate the sensitivity of the top priority task's threshold in the navigation experiments ?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5595/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699292644796,
            "cdate": 1699292644796,
            "tmdate": 1699636576481,
            "mdate": 1699636576481,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "YC9qxQEx99",
                "forum": "c0MyyXyGfn",
                "replyto": "OoC6vt98UA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5595/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5595/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer tsYf"
                    },
                    "comment": {
                        "value": "We kindly thank reviewer *tsYf* for their review of our work. In the following, we answer the brought forward criticism and questions.\n\n### **Reproducibility and reward functions**\n\n*Reviewer tsYf is concerned that environments and reward functions are not sufficiently described.*\n\nInitially, we did not explicitly state the reward functions because, for reproducibility reasons, we plan to publish our complete codebase alongside the camera-ready version of the paper, which contains the reward functions and the complete environment specification. For completeness and to address this concern, we have now detail the reward functions in section G.1 in the appendix. \n\nWe hope this eliminates the concern w.r.t reproducibility.\n\n### **Discussion on incompatible subtasks**\n\n*Reviewer tsYf wishes for a more thorough discussion on situations where subtasks are incompatible and is concerned that our proposed method might fail in such cases since they believe that the indifference spaces would become empty.*\n\nWe fully agree that this discussion is important, interesting, and benefits the understandability of our method. To address this point, we have added a new discussion section, based on the following points, to the revised manuscript in Appendix E.\n\n- Firstly, when subtasks are *semantically* incompatible (e.g. when $r_2 = -r_1$, where $r_1$ rewards going to the left and $r_2$ rewards going to the right), it should be recognized that the MORL task is fundamentally ill-posed since no agent can behave optimally, at the same time, for two objectives that are entirely incompatible. This inherent challenge exists independently of the optimization method.\n- Secondly, we want to highlight an advantage of our method over MORL methods without task priorities: If two objectives are semantically incompatible, our agent attempts to solve the lower-priority task as best as possible, while the constraint ensures that the higher-priority task is still solved optimally (up to the $\\varepsilon$ threshold). If the lower-priority task rewards driving to the left, but the higher-priority task rewards driving to the right, our agent *has* to drive to the right, due to the lexicographic constraint. A non-lexicographic MORL agent will instead behave unpredictably in an attempt to maximize the incompatible subtasks at the same time (see Russel & Zimdars 2003 paper \"Q-Decomposition for RL agents\").  In fact, semantically incompatible subtasks were precisely the original motivation for lexicographic MORL (Multi-criteria Reinforcement Learning, 1998, G\u00e1bor, Kalm\u00e1r and Szepesv\u00e1ri).\n- Lastly, with our method, the indifference-space can never be empty (although very small). This is due to our problem definition (threshold *relative* to subtask Q-function) and the iterative nature of our learning algorithm. By first adapting the second-highest subtask solution using the lexicographic constraint of the highest-priority subtask, the second-highest priority subtask solution only assigns high value to actions that are in the highest priority subtask's indifference space. Thus, after adaptation, the indifference spaces of subtasks overlap, even when the original subtask rewards are semantically incompatible. Generally speaking, for some priority level $i$, in our adaptation step, all **higher-priority** subtask solutions are already \"compatible\" because they have already been adapted. Then, even in the extreme case where we set $\\varepsilon_i = 0$, all **lower-priority** tasks can still \"chose\" from all optimal actions for task $r_i$. If, on the other, *absolute* performance thresholds were used that are not relative to the (adapted) subtask Q-functions, the intersection of those indifference spaces could indeed be empty. \n\n### **Threshold ablation**\n\n*Reviewer tsYf wonders about the sensitivity of our method to differing $\\varepsilon_i$ thresholds and suggests an ablation study.*\n\nThe behavior that our agent learns is indeed governed by the $\\varepsilon_i$ thresholds since these thresholds give rise to the action indifference space for each subtask. Intuitively, **larger** $\\varepsilon_i$ values mean that the performance for task $r_i$ is allowed to **degrade more**, in favor of increased performance for lower-priority tasks. Conversely, lower $\\varepsilon_i$ values mean that the performance for task $r_i$ must remain **more optimal**, while lower-priority tasks are more restricted in their subtask optimization. \nWe discuss in Section 6 of the paper how adequate values for $\\varepsilon_i$ can be found practically. \n\nTo address this concern, we have conducted and added an ablation study on different $\\varepsilon$ values in the appendix, Section H.3.\nThis experiment empirically confirms the above-described relationship between $\\varepsilon_i$ scalars and subtask performance.\n\n\n\nWe again thank reviewer *tsYf* for their review of our work, we firmly believe the review allowed us to add important and beneficial content to our paper."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5595/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700238048055,
                "cdate": 1700238048055,
                "tmdate": 1700239895925,
                "mdate": 1700239895925,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]