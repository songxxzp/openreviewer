[
    {
        "title": "Demonstration-Regularized RL"
    },
    {
        "review": {
            "id": "HskecbvjVE",
            "forum": "lF2aip4Scn",
            "replyto": "lF2aip4Scn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9260/Reviewer_UXmS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9260/Reviewer_UXmS"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies two new hybrid setting which are novel/uncommon in the literature: (i) demonstration regularized RL and (ii) Demonstration Regularized RLHF.\n\nIn (i) both expert demonstrations from an $\\epsilon$-optimal policy and online access to an MDP with reward function are possible. In (ii) the reward function is not available but it can be inferred thanks to a Preference Based Model introduced in Assumption 4."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Both the newly introduced settings are interesting and matches practical situations."
                },
                "weaknesses": {
                    "value": "There are several technical weaknesses in my opinion.\n\nThe main weakness is in my opinion that the setting seems \n\n1) The lower bound in Theorem 2 turns unfortunately vacuous in the limit of $\\gamma \\rightarrow 0$.\n\n2) It is unclear why the class of linear policies at the third line of Section 3.2 is considered to be not learnable. In fact, under this choice [1] proves in their Theorem 5 that it is possible to recover an $\\epsilon$-suboptimal policy compared to the expert with behavioural cloning.\n\n3) Corollary 3 requires the expert to be $\\mathcal{O}(\\epsilon)$ optimal but I would expect that, given the reward knowledge, it should be possible to prove a sample complexity bound without the assumption on the $\\mathcal{O}(\\epsilon)$ optimality of the expert.\nTo see this think to the case of any BPI algorithm which requires no expert at all to learn an $\\epsilon$-optimal policy.\n\n4) I think that Lemma 11 should be referred as the standard performance difference lemma.\n\n5) Just before Corollary 3, it is said  that \"UCBVI-Ent+ algorithm for regularized BPI. It is a modification of the algorithm UCBVI-Ent by\nTiapkin et al. (2023) with improvement sample complexity\". However, it is not explained which is the crucial difference between the two algorithms. In particular, also the settings are different because UCBVI-Ent+ uses reward information while UCBVI-Ent can be used only for maximum entropy exploration and not to solve Regularized MDPs but this difference is not explained in the main text.\n\n\n[1] ( Rajamaran et al., 2021 ) On the Value of Interaction and Function Approximation in Imitation Learning."
                },
                "questions": {
                    "value": "Q1) Is it possible to prove a bound which does not require the assumption that the expert is $\\epsilon$ optimal ?\n\nQ2) Why the regularization is needed in the tabular case but not in the linear one ? I am referring to Section 3.2\n\nQ3) How can UCBVI-Ent+ achieve $\\mathcal{O}(\\epsilon^{-1})$ sample complexity according to Theorem 5 while UCBVI-Ent achieves a worst sample complexity of $\\mathcal{O}(\\epsilon^{-2})$ ?\n\nQ4) What are the definitions of $\\pi^{t,(h)}$ and $\\tilde{\\pi}^t$ in Algorithm 3?\n\nQ5) In the setting of Demonstration Regularized RLHF is it necessary to have the coefficients defined in equation 3 in the bound ?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9260/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9260/Reviewer_UXmS",
                        "ICLR.cc/2024/Conference/Submission9260/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9260/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698098479889,
            "cdate": 1698098479889,
            "tmdate": 1700512688316,
            "mdate": 1700512688316,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "E3DpiMzwe3",
                "forum": "lF2aip4Scn",
                "replyto": "HskecbvjVE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9260/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9260/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Authors, Part 1"
                    },
                    "comment": {
                        "value": "We would like to thank reviewer UXmS for the careful reading and the constructive feedback.  Please find below our response to the main points raised in the review.\n\n- The lower bound in Theorem 2 turns unfortunately vacuous in the limit of $\\gamma \\to 0$.\n\nA fixed $\\gamma$ enables us to establish a general lower bound by constraining the space of policies we aim to learn. Thus, the minimax lower bound with respect to the space of all policies is lower-bounded by the minimax lower bound with respect to the space of restricted policies. Therefore, choosing any fixed $\\gamma$ (for example, $\\gamma = 1/N$) allows us to deduce a valid and reasonable lower bound.\n\n- It is unclear why the class of linear policies at the third line of Section 3.2 is considered to be not learnable. In fact, under this choice [1] proves in their Theorem 5 that it is possible to recover an $\\varepsilon$-suboptimal policy compared to the expert with behavioural cloning.\n\nIt is important to emphasize that the behavior cloning defined in our paper aims not to find an $\\varepsilon$-optimal policy but to reconstruct the initial behavioral policy in trajectory KL-distance. To make the algorithm implementable, defining a hypothesis class with policies that are differentiable or at least continuous with respect to the learnable parameters becomes necessary. In the setting of purely greedy policies, the mentioned linear class of policies lacks these properties, thus preventing the efficient minimization of the log-loss. We will add an additional discussion and a reference to [1].\n\n\n- Corollary 3 requires the expert to be $O(\\varepsilon)$-optimal but I would expect that, given the reward knowledge, it should be possible to prove a sample complexity bound without the assumption on the optimality of the expert. To see this think to the case of any BPI algorithm which requires no expert at all to learn an $O(\\varepsilon)$-optimal policy.\n\n\nWe acknowledge that the assumption of nearly optimal experts might be considered strong. If it is known a priori that the expert is far from optimal, a viable approach is to simply set $\\lambda=0$ and employ a stopping rule from UCBVI-BPI (M\u00e9nard et al. (2021)).\n\nHowever, it's important to emphasize that the primary goal of this paper is to provide an analysis of an algorithm actively used in practice, where these assumptions are deemed reasonable.\n\n\n- I think that Lemma 11 should be referred as the standard performance difference lemma.\n\nWe agree that Lemma 11 is already known in literature, so we will add the corresponding reference.\n\n- Just before Corollary 3, it is said that \"UCBVI-Ent+ algorithm for regularized BPI. It is a modification of the algorithm UCBVI-Ent by Tiapkin et al. (2023) with improvement sample complexity\". However, it is not explained which is the crucial difference between the two algorithms. In particular, also the settings are different because UCBVI-Ent+ uses reward information while UCBVI-Ent can be used only for maximum entropy exploration and not to solve Regularized MDPs but this difference is not explained in the main text.\n\nWe add a separate comment (available to all reviewers) regarding the key differences between UCBVI-Ent and UCBVI-Ent+."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9260/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699965119103,
                "cdate": 1699965119103,
                "tmdate": 1699965119103,
                "mdate": 1699965119103,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GaOIk2tCT9",
                "forum": "lF2aip4Scn",
                "replyto": "LJHdsXVWhh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9260/Reviewer_UXmS"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9260/Reviewer_UXmS"
                ],
                "content": {
                    "comment": {
                        "value": "Dear authors,\n\nThanks for your detailed response !\n\nAt the moment I don't feel like rising my score. I think there is value in the current submission but I think that the presentation of the results do not match the acceptance standard. Since this is a theoretical paper it would be important to convey the main new proof techniques to attain the improved results.\n\nHowever, this is not done in the main text but only in the very long Appendix.\n\nBe sure however that I will discuss with the other reviewers to check whether they share my viewpoint or not.\n\nBest,\n\nReviewer UXmS"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9260/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700041782211,
                "cdate": 1700041782211,
                "tmdate": 1700041782211,
                "mdate": 1700041782211,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wgYjBdFiDj",
                "forum": "lF2aip4Scn",
                "replyto": "k46fdpwct3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9260/Reviewer_UXmS"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9260/Reviewer_UXmS"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Authors,\n\nThanks for adding some discussions about the linear expert condition and explaining the differences between UCBVI-Ent and UCBVI-Ent+.\n\nI understand that the 9 pages limit can be restrictive for all your contributions. However, I think that the main text should be structures to convey a clearer presentation of maybe fewer contribution.\n\nTo this end, my suggestion would be to first state the result in Corollary 3 using the existing results in Tiapkin et al. 2023 (UCBVI-Ent) and shows which would be the result in that case.\n\nThen, you can explain the algorithm UCBVI-Ent+ (adding the pseudocode in the main text) and show the improvement in sample complexity compared to the one obtained using UCBVI-Ent and explain that the reason for the improvement is the fact that UCBVI-Ent can leverage the strong convexity of the KL divergence.\n\nThe current paragraph in red is still bit difficult to parse in my opinion and the pseudocode for UCBVI-Ent+ is needed.\n\nProbably, implementing these changes will require to move the RLHF part to the Appendix and just mention this extension in the main text. \n\nImplementing these changes would make the paper way easier to appreciate in my opinion.\n\nFinally, I have a question. Would have been possible to adapt RL-Explore-Ent to the regularised MDP setting to attain the same sample complexity without the need of introducing a new algorithm in this work ?\n\nBest,\nReviewer UXmS"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9260/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700470692025,
                "cdate": 1700470692025,
                "tmdate": 1700470692025,
                "mdate": 1700470692025,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hK4ep3imWx",
                "forum": "lF2aip4Scn",
                "replyto": "oCbCsAMury",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9260/Reviewer_UXmS"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9260/Reviewer_UXmS"
                ],
                "content": {
                    "comment": {
                        "value": "dear authors,\n\nthanks a lot for the revision !\n\ni think it solves my concerns about the presentation because now the randomization idea is well presented and it is easier to understand how lambda should be set to obtain the bound for the demonstration regularized RL setting.\n\nas a result i will rise my score to 6.\n\nI think that the main remaining limitation is the assumption that the expert is epsilon optimal. Hopefully the authors will address this question in some future work.\n\nBest,\nReviewer"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9260/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700512611034,
                "cdate": 1700512611034,
                "tmdate": 1700512611034,
                "mdate": 1700512611034,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "t81cK8U25y",
            "forum": "lF2aip4Scn",
            "replyto": "lF2aip4Scn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9260/Reviewer_Muxw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9260/Reviewer_Muxw"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies demonstration-regularized RL where an agent is supposed to find a near optimal policy given an offline dataset that is collected from an expert policy. The paper theoretically shows that given $N^E$ expert samples, the sample complexity of finding a $\\epsilon$-optimal policy reduces by a factor of $1/N^E$ in both tabular and linear MDPs. Moreover, the paper extends the proposed method to RLHF and theoretically justify the efficiency of it."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper provides comprehensive theoretical results on various settings in demonstration-regularized RL.\n2. The results are nice and show a strong benefit using expert demonstrations."
                },
                "weaknesses": {
                    "value": "1. I prefer that there is a separated \"related works\" section such that the presentation is clear.\n2. The contributions from the algorithm design part seem not significant. The algorithm is a combination of imitation learning and regularized RL.\n3. The results highly depends on the performance of the expert policy. However, in real life applications, obtaining expert demonstrations is usually expensive, and there might be far less offline demonstrations that that considered in this paper. Specifically, from Corollary 3, it seems that the benefit occurs when $N^E>H^3SA$, which is close to the typical sample complexity in standard RL, which might be too much in real applications."
                },
                "questions": {
                    "value": "While the paper provides the lower bound for the imitation learning, is there any lower bound for the regularized RL?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9260/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9260/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9260/Reviewer_Muxw"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9260/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698788271921,
            "cdate": 1698788271921,
            "tmdate": 1700582080931,
            "mdate": 1700582080931,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "XjFz3dys2s",
                "forum": "lF2aip4Scn",
                "replyto": "t81cK8U25y",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9260/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9260/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We would like to thank reviewer Muxw for the careful reading and the constructive feedback.  Please find below our response to the main points raised in the review.\n\n- The contributions from the algorithm design part seem not significant. The algorithm is a combination of imitation learning and regularized RL.\n\nWe are particularly interested in algorithms that are scalable and easily implementable. The simplicity of our algorithm enables its straightforward implementation beyond tabular and linear settings. Our primary focus in this work is to analyze the existing practical pipeline and demonstrate its provable efficiency.\n\nOn the other hand, our algorithm, UCBVI-Ent+, introduces a novel time step randomization to control the widths of the confidence intervals, as described in a separate comment available to all reviewers. In our view, this represents a significant algorithmic contribution, enabling the achievement of optimal complexity bounds.\n\n\n- The results highly depends on the performance of the expert policy. However, in real life applications, obtaining expert demonstrations is usually expensive, and there might be far less offline demonstrations that that considered in this paper. Specifically, from Corollary 3, it seems that the benefit occurs when $N^E > H^3 SA$, which is close to the typical sample complexity in standard RL, which might be too much in real applications.\n\n\nRegarding Corollary 3, it is important to emphasize that with significantly less expert data, reconstructing the expert policy with a small error becomes impossible, as indicated in the lower bound (Theorem 2). The application of behavior cloning techniques makes no sense with such a limited amount of data. However, with a reasonable amount of expert data, our approach demonstrates convergence to the optimal policy, surpassing the behavior cloning outcome.\n\nIn the RLHF setup, it's crucial to highlight that even a relatively small amount of expert data results in a straightforward and practical algorithm. This algorithm delivers a policy which is close  to the optimal one when provided with a reasonably large preference dataset. In contrast to existing theoretical algorithms for RLHF, our approach distinguishes itself by not necessitating the solution of complex min-max problems to achieve pessimism. Instead, it aligns more closely with approaches actively applied in practical settings."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9260/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699964942248,
                "cdate": 1699964942248,
                "tmdate": 1699964942248,
                "mdate": 1699964942248,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "i2b9msAzok",
                "forum": "lF2aip4Scn",
                "replyto": "XjFz3dys2s",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9260/Reviewer_Muxw"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9260/Reviewer_Muxw"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the response. After reading the revision and other general answers, I appreciate the contribution from the behavior cloning and the corresponding lower bound. In addition, other contribution also seems good. I have decided to raise my score to 6."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9260/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700582065749,
                "cdate": 1700582065749,
                "tmdate": 1700582065749,
                "mdate": 1700582065749,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "LOXTQUZkV0",
            "forum": "lF2aip4Scn",
            "replyto": "lF2aip4Scn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9260/Reviewer_zApY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9260/Reviewer_zApY"
            ],
            "content": {
                "summary": {
                    "value": "The paper studied demonstration-regularized reinforcement learning (RL), where the learner first performs behavioral cloning on expert-generated demonstrations using maximum likelihood estimation. Then during the online interaction with the underlying environment, the learner penalizes deviation of the learned policy from the one learned in the behavioral cloning phase. The paper provided a theoretical analysis of these two phases. For behavioral cloning, the authors show that the KL divergence between the learned policy and the expert policy decreases linearly as the number of demonstrations grows. This holds for both tabular MDPs and linear MDPs under certain assumptions. Then based on this result, the authors further studied the regularized online learning scenario and the RLHF setting. In both cases, the authors are able to prove a fast convergence rate for the proposed algorithms."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "(1) The paper performed a strong and solid theoretical study of behavioral cloning for both tabular and linear MDPs. The authors proved that the KL-divergence between the learner policy and the expert policy decreases linearly as the number of demonstrations grows. The authors also complemented the above positive results with a lower bound on the convergence rate. In terms of the dependency on the number of demonstrates, the upper bound and lower bound match. This is a nice and great result. Based on that, the authors further performed analysis on their proposed demonstration-regularized RL algorithms and the RLHF algorithms, and both achieved surprisingly fast convergence rates. Overall, the paper has made significant technical contributions.\n\n(2) The paper studied a very novel, interesting, yet challenging problem. The topic is of particular interest to the theoretical RL community, and I can forsee that the results of this paper significantly push the frontiers of RL theory and will drive more research along this line."
                },
                "weaknesses": {
                    "value": "(1) It would be great to include some empirical studies, although this is purely a theory paper."
                },
                "questions": {
                    "value": "(1) Can you provide some empirical results to validate the theoretical findings of this paper?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9260/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9260/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9260/Reviewer_zApY"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9260/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699133294199,
            "cdate": 1699133294199,
            "tmdate": 1699637165992,
            "mdate": 1699637165992,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "NFrHqKpJ3r",
                "forum": "lF2aip4Scn",
                "replyto": "LOXTQUZkV0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9260/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9260/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank reviewer zApY for the positive and encouraging feedback. We will add numerical results for our tabular algorithm."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9260/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699964865373,
                "cdate": 1699964865373,
                "tmdate": 1699964865373,
                "mdate": 1699964865373,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "TbtN4zSMhO",
            "forum": "lF2aip4Scn",
            "replyto": "lF2aip4Scn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9260/Reviewer_XPKe"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9260/Reviewer_XPKe"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose KL-regularized online RL algorithms and provide an upper bound on the sample complexity of this algorithm in tabular MDP and linear MDP settings."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The authors provide a thorough analysis of the algorithms, to justify the efficiency of the RL algorithms with access to an expert dataset."
                },
                "weaknesses": {
                    "value": "1. There is limited discussion on the relationship between the pure online learning version of LSVI-UCB and UCBVI+.\n2. The paper can be better organized, there are too many references pointing towards the appendix."
                },
                "questions": {
                    "value": "I am not an expert in RL theory, I am a bit confused by the results presented in Corollary3 and Theorem6, where the bound presented in Corollary3 depends on $N^E$, whereas the bound in Theorem6 does not. Intuitively, I suppose the sample complexity would eventually depend on the size of the expert dataset. Can authors explain this?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9260/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9260/Reviewer_XPKe",
                        "ICLR.cc/2024/Conference/Submission9260/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9260/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699299266252,
            "cdate": 1699299266252,
            "tmdate": 1700605434974,
            "mdate": 1700605434974,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "LqPqBBjcuP",
                "forum": "lF2aip4Scn",
                "replyto": "TbtN4zSMhO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9260/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9260/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We would like to thank reviewer XPKe for the careful reading and the constructive feedback.  Please find below our response to the main points raised in the review.\n\n- Limited discussion on LSVI-UCB-Ent and UCBVI-Ent algorithms;\n\nUnfortunately, the limited discussion on the regularized BPI algorithms is a byproduct of constrained space. We've included a general comment highlighting the novel features of UCBVI-Ent+ and LSVI-Ent algorithms compared to their non-regularized counterparts. Additionally, we present an adaptation of the UCBVI-Ent algorithm for general regularized MDPs in a separate comment dedicated to all the reviewers.\n\nMoreover, it's important to stress that the primary contribution of the paper lies not solely in the regularized BPI algorithms but in their combination with behavior cloning techniques. This combination illustrates the convergence properties of simple and implementable algorithms that are already widely utilized in practice.\n\n- The paper can be better organized, there are too many references pointing towards the appendix.\n\nWe would greatly appreciate any suggestions from the reviewer regarding the reorganization of the main results of the paper, considering the constraint of a 9-page limit.\n\n- Why Theorem 6 does not depend on the size of the expert dataset $N^{\\mathrm{E}}$?\n\nTheorem 6 is devoted to the sample complexity of best policy identification for KL-regularized MDPs with respect to any reference policy and any regularization coefficient $\\lambda > 0$. To specialize these bounds to Corollary 3, we need to take a behavior-cloning policy $\\pi^{\\mathrm{BC}}$ as a reference one and choose the regularization parameter  $\\lambda$ depending on the number of expert trajectories $N^{\\mathrm{E}}$ (choice $\\lambda^\\star$ from Theorem 3) and the desired accuracy $\\varepsilon$ as $\\mathcal{O}(N^{\\mathrm{E}} \\varepsilon / (SAH))$ (as described in Corollary 3)."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9260/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699964810400,
                "cdate": 1699964810400,
                "tmdate": 1699964810400,
                "mdate": 1699964810400,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GpuNrZLgdH",
                "forum": "lF2aip4Scn",
                "replyto": "4LZO2TsZ5V",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9260/Reviewer_XPKe"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9260/Reviewer_XPKe"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the clarification and the revision. I would like to raise my rating to 6."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9260/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700605392324,
                "cdate": 1700605392324,
                "tmdate": 1700605392324,
                "mdate": 1700605392324,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]