[
    {
        "title": "Accelerated Convergence of Stochastic Heavy Ball Method under Anisotropic Gradient Noise"
    },
    {
        "review": {
            "id": "pVrkcvqkve",
            "forum": "CIqjp9yTDq",
            "replyto": "CIqjp9yTDq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3508/Reviewer_JMNK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3508/Reviewer_JMNK"
            ],
            "content": {
                "summary": {
                    "value": "Motivated by empirical successes of stochastic heavy ball (SHB) method and the prevalence of the anisotropic gradient noise in training neural networks, the paper presented last iterate convergence rate for the SHB method on quadratic objectives under the anisotropic gradient noise assumption. The results implies SHB with polynomially decaying learning rate provides $\\tilde{O}(\\sqrt{\\kappa})$ acceleration rate with respect to stochastic gradient descent (SGD) when batch size is large, where $\\kappa$ denotes the conditional number."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is well-written, and proofs are neat and easy to follow. \n\nThe paper formally established a lower bound for SGD tailored for the scope of investigation, which served as a concrete comparison benchmark for SHB method.\n\nThe paper presents novel theoretical results of SHB for quadratic loss. The result implies $\\tilde{O}(\\sqrt{\\kappa})$ acceleration in comparison to SGD when batch size is big. The results is also nearly optimal in comparison to heavy ball method in deterministic setting. \n\nThe experiment results matches with the theoretical bound: acceleration guarantee with large batch size. The advantage of SHB with large batch size is also well-motivated in practice. \n\nThe analysis follows the classical bias variance decomposition paradigm. The novelty is to quantifying bias and variance with some linear operator, and then analyze the property of those linear operators.\n\nAlthough the scope of theoretical investigation is limited at quadratic loss with anisotropic gradient noise, the author justified the broad implication of such assumptions to real applications."
                },
                "weaknesses": {
                    "value": "Potential minor typos:\n\npg5: eqn 3.11: missing - sign before $\\beta$. (Same typo appears at pg 21 under the proof of Lemma 13 at two places)\n\npg6: Algorithm 1 line 6, batch size $M$ instead of $m$ being consistent \n\npg27: the equality applies (C.18) and (C.21): the last row of the matrix $2d$.\n\n\nEverything is rigorous, but the notation might be a bit overly complicated for what was actually being used in order to establish Theorem 2. For example: number of stages $n_{\\ell}$, the stage lengths $\\set{ k_1, \\cdots,  k_{n_{\\ell}} }$. Theorem 2 actually set all stages the same length, and each stage has the same learning rate."
                },
                "questions": {
                    "value": "No"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3508/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3508/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3508/Reviewer_JMNK"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3508/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697867725812,
            "cdate": 1697867725812,
            "tmdate": 1699636304400,
            "mdate": 1699636304400,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "yKowGETRjB",
                "forum": "CIqjp9yTDq",
                "replyto": "pVrkcvqkve",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3508/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3508/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer JMNK, thanks!"
                    },
                    "comment": {
                        "value": "We would like to extend our sincere appreciation to the reviewer for all the constructive and positive feedbacks on our contribution, which definitely helps us improve the paper concretely and motivates us to do our best. Thanks!\n\n**Overall:**\n\nWe have made corresponding changes in our revised paper to address the mentioned potential issues.\n\n**Weaknesses**\n\nThanks very much for pointing out those typos and organization issues! We have uploaded a revised version of our paper to reflect the changes."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3508/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700253632168,
                "cdate": 1700253632168,
                "tmdate": 1700253632168,
                "mdate": 1700253632168,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "8C7QAIjkK5",
            "forum": "CIqjp9yTDq",
            "replyto": "CIqjp9yTDq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3508/Reviewer_Vpgj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3508/Reviewer_Vpgj"
            ],
            "content": {
                "summary": {
                    "value": "The paper investigates effect of Polyak-type momentum in SGD with large batch size. The authors show that momentum, combined with a stage-wise geometric decay stepsize schedule, improves the linear convergence part of SGD. Although the analysis is done on convex quadratic functions, it takes a step towards better understanding of SGD with momentum."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is well-written and easy to follow. The motivation and main results are clearly presented, with enough intuition given after each result. The technical results seem solid and are supported by empirical evidence."
                },
                "weaknesses": {
                    "value": "There is still a gap between real-life SGD applications and the analysis. Most SGD applications are for nonconvex (even nonsmooth) problems, where Hessian may even not exist and the notion of condition number is uncommon."
                },
                "questions": {
                    "value": "1. Assumption 3 seems to suggest \"noise is small in the coordinates that result in ill-conditioning\". What do you think is the difficulty in extending it to commonly used assumptions (e.g., bounded variance)?\n2. In your first experiment, It seems that SGD performs worse when $M$ gets large. To me it feels that it is because your range of grid search for $\\eta_t$ is invariant for all batchsizes. I'm curious whether SGD performs better when you multiply the stepsize by $\\sqrt{M}$.\n3. Nonconvex models are actually used in the second experiment. Although the authors claim that the strongly convex quadratic optimization model approximates the landscape near local optimum, Figure 1 actually suggests that benefits of momentum are significant at the beginning of the training procedure. Could you elaborate more on this?\n4. Some recent researches show that even without decay of learning rate [1], momentum SGD also outperforms SGD using large batch size. What do you think might contribute to this phenomenon?\n5. I would suggest the authors remove (or delay to the end of the appendix) proofs for auxiliary results, most of which are well-known results. Some notations should be properly defined (such as $\\mathbf{X} \\succeq \\mathbf{Y}$ and Hessian norm $\\\\|\\cdot\\\\|_\\mathbf{H}$)\n\n**Minor typos and stylistic issues**\n\n1. Algorithm 1. Line 6\n\n   $m$ => $M$\n\n2. Experiment\n\n   initialize $\\mathbf{w}^0$ from $(-1,1) \\Rightarrow (-1,1)^d$.\n\n3. Proof of Lemma 3 \n\n   $\\succ$ should be $\\succeq$\n\n4. Page 29\n\n   genarality => generality\n\n**References**\n\n[1] Wang, R., Malladi, S., Wang, T., Lyu, K., & Li, Z. (2023). The marginal value of momentum for small learning rate sgd. *arXiv preprint arXiv:2307.15196*."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3508/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3508/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3508/Reviewer_Vpgj"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3508/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698633753396,
            "cdate": 1698633753396,
            "tmdate": 1699636304326,
            "mdate": 1699636304326,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "x9iUr5a0Pf",
                "forum": "CIqjp9yTDq",
                "replyto": "8C7QAIjkK5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3508/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3508/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer Vpgj (part 1/2)"
                    },
                    "comment": {
                        "value": "We would like to offer our sincere thanks for the reviewer's constructive comments and recognition on our contributions.\n\n**Overall**:\n\nRegarding the raised questions, We have provided additional experimental results and relevant discussions to address the concerns. The details are as follows.\n\n\n**Q1: What do you think is the difficulty in extending Assumption 3 to commonly used assumptions (e.g., bounded variance)?**\n\nThat's an interesting question. I think one major difficulty arises from bias-variance decomposition.\n\nFirst, for SGD, a more general assumption $E[n_t n_t^\\top] \\preceq \\sigma^2 D$ can be well handled, where the rest standard proofs for quadratics lead to $\\sigma^2 D_{jj} / (\\lambda_j T) \\le \\sigma^2 D_{jj} / (\\mu T)$ for each dimension, summing them up across dimensions and we obtain the common upper bound $O(\\sigma^2 / (\\mu T))$ given $\\sum_j D_{jj} = 1$.\n\nFor SHB in our proof, one thing is that bias-variance decomposition requires the noise and the hessian to be simultaneously diagonalizable, as shown in (C.19). If this part is resolved, we shall obtain similar results as SGD and recover the $~O(\\sigma^2 / (\\mu T))$ bound, given the rest part of the proof in our paper providing similar guarantees.\n\nAlso, it is worth noticing that generally our setting is technically harder than normal settings of bounded variance. For bounded variance, standard techniques such as [1] can be considered to apply, which we believe to lead to a results similar to existing literatures. We choose Assumption 3 because the corresponding result is still unknown and the setting is technically more difficult.\n\n**Q2: In your first experiment, ... I'm curious whether SGD performs better when you multiply the stepsize by $\\sqrt{M}$.**\n\nThanks for the question! Actually we tried larger learning rates before, such as initial learning rate $\\eta_0 = 10.0$, but the loss soon exploded. This is no surprise given it matches the theoretical insight that learning rate $\\eta_t$ must be no greater than $2/L$, no matter how large the batch size is.\n\nWe also conducted additional experiments of multiplying the stepsize by $\\sqrt{M}$. The loss also exploded for $M \\ge 32$. According to our experience, increasing the stepsize generally helps SGD to converge faster, but there is an upper limit for this increased stepsize.\n\n\n**Q3: Nonconvex models are actually used in the second experiment. Although the authors claim that the strongly convex quadratic optimization model approximates the landscape near local optimum, Figure 1 actually suggests that benefits of momentum are significant at the beginning of the training procedure. Could you elaborate more on this?**\n\nThat's a very good question. We provide further experimental results on cifar10 to verify SGD and SHB's behavior near optimum, where we first run SHB to for 50 epochs (batch size 2048, constant learning rate $\\eta = 0.1$) to approach the optimum. We then conduct the exactly same hyperparameters search process as our original experimental settings, with batch size 2048 and number of epochs 100.\n\nAs shown in following table, both SGD and SHB has a much lower loss as it is near the optimum, but SHB's loss is an order of magnitude lower than SGD, along with a higher test accuracy.\n\n|| ResNet-18 || MobilenetV2 || DenseNet-121 ||\n| -- | -- | -- | -- | -- | -- | -- |\n| Method |  Training loss | Test acc (%) | Training loss | Test acc (%) | Training loss | Test acc (%) |\n| SGD | $4.13 \\times 10^{-2}$  | $90.14$ | $3.40 \\times 10^{-2}$ | $90.29$ | $3.93 \\times 10^{-3}$ | $92.91$ |\n| SHB  | $2.08 \\times 10^{-3}$ | $91.10$ | $6.32 \\times 10^{-3}$ | $91.07$ | $2.51 \\times 10^{-4}$  | $93.58$ |\n\nIn addition, our intention of the original set of experiments is to demonstrate the effectiveness of momentum in common non-convex settings, whose assumption may not be perfectly aligned with our theory as the reviewer implied.\n\n\n[1]: Aybat, Necdet Serhat, et al. \"A universally optimal multistage accelerated stochastic gradient method.\" Advances in neural information processing systems 32 (2019)."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3508/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700253437905,
                "cdate": 1700253437905,
                "tmdate": 1700254295657,
                "mdate": 1700254295657,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4fyA90jVjS",
                "forum": "CIqjp9yTDq",
                "replyto": "ZzlLvnjPoR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3508/Reviewer_Vpgj"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3508/Reviewer_Vpgj"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your response"
                    },
                    "comment": {
                        "value": "Thank you for the response. And based on the response, I keep my current evaluation of the paper."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3508/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700368647245,
                "cdate": 1700368647245,
                "tmdate": 1700368647245,
                "mdate": 1700368647245,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ds1Cg85eax",
            "forum": "CIqjp9yTDq",
            "replyto": "CIqjp9yTDq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3508/Reviewer_dqiw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3508/Reviewer_dqiw"
            ],
            "content": {
                "summary": {
                    "value": "This study presents an analysis of the convergence properties of the stochastic HB method when applied to quadratic objective functions. Specifically, when we make an assumption about the presence of anisotropic noise, it demonstrates that the stochastic HB method exhibits a significantly faster rate of convergence. Earlier research has already established that this anisotropic noise model can manifest in neural networks when their parameters are in proximity to the optimal values."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper for the first time shows that in the anistorpic setting for the quadratic objective function, the stochastic HB method can achieve accelerated rate. This is a novel contribution."
                },
                "weaknesses": {
                    "value": "The paper should delve further into the circumstances under which the anisotropic setting is applicable to machine learning models. The content of Theorem 1 may seem redundant as its proof is not contingent on stochasticity; it essentially establishes a lower bound for gradient descent (GD), a well-known result. Therefore, its inclusion in the paper is somewhat unclear. It would be more appropriate to allocate space to discuss the technical innovations of the main theorem's proof within the main body of the paper. Additionally, there are several typographical errors in the proofs, including indexing, which should be rectified."
                },
                "questions": {
                    "value": "In the paper you mentioned that this rate achieved for the objective in the large enough batch regime. However the acceleration is oblivion to the mini-batch size. So I was wondering why you emphasise on this in the paper."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3508/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698718272688,
            "cdate": 1698718272688,
            "tmdate": 1699636304192,
            "mdate": 1699636304192,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "3mo5BnMxDI",
                "forum": "CIqjp9yTDq",
                "replyto": "ds1Cg85eax",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3508/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3508/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer dqiw"
                    },
                    "comment": {
                        "value": "Thank you very much for all the constructive comments and recognition on our contributions. We really appreciate it.\n\n**Overall**\n\nRegarding the mentioned concerns, we have provided relevant discussions and a revised version of our paper to address the issues.\n\n**Weakness**\n\nThanks for all the constructive suggestions! We have made corresponding revisions based on the mentioned valuable advices, including\n\n1) Shorten the paragraph for Theorem 1.\n2) Add two short paragraphs after Corollary 3 to discuss about the intuition of this paper's technical innovation in proofs.\n3) Rectify the typos pointed out by the reviewer.\n\nAny further comments would be greatly appreciated.\n\n\n**Q: In the paper you mentioned that this rate achieved for the objective in the large enough batch regime. However the acceleration is oblivion to the mini-batch size. So I was wondering why you emphasise on this in the paper.**\n\nThat's a very good question. It is because without a sufficiently large batch size, the acceleration effect in the bias term will become much less obvious, and may even be completed covered by the large variance term. This phenomenon is similar to the one in [1] mentioned by reviewer Vpgj, where under small or medium batch size settings, momentum's empircal benefit is not evident.\n\n[1] Wang, R., Malladi, S., Wang, T., Lyu, K., & Li, Z. (2023). The marginal value of momentum for small learning rate sgd. arXiv preprint arXiv:2307.15196"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3508/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700253794169,
                "cdate": 1700253794169,
                "tmdate": 1700253857635,
                "mdate": 1700253857635,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "l04CvOu2G0",
            "forum": "CIqjp9yTDq",
            "replyto": "CIqjp9yTDq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3508/Reviewer_Uq7v"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3508/Reviewer_Uq7v"
            ],
            "content": {
                "summary": {
                    "value": "This paper analyzes the performance of SGD and SHB (Stochastic Heavy Ball) in stochastic optimization with a quadratic objective. A lower bound of the convergence rate of SGD is given, which argued to be strictly worse than the upper bound of convergence rate of SHB also proved in the paper. More precisely, SGD is shown to take at least $O(\\kappa)$ iterations to converge, while SHB requires only $\\tilde{O}(\\sqrt{\\kappa})$ iterations with proper learning rate schedule."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The contrast between SGD and SHB does seem plausible. I checked the proof briefly and found no obvious mistakes."
                },
                "weaknesses": {
                    "value": "1. The upper bound of SHB seems very similar (and worse by a logarithmic factor) than the quadratic setting in [Can et al. (2019)](https://arxiv.org/pdf/1901.07445.pdf). I would suggest adding a detailed comparison. Also the convergence rate is sublinear, which is not very satisfactory.\n2. The batch size is required to be of order $\\Omega(1/\\epsilon)$, which does not seem realistic, as $\\epsilon$ is usually exponentially small."
                },
                "questions": {
                    "value": "My foremost concern is the comparison with previous literature, which I discussed in the `Weaknesses` part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3508/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3508/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3508/Reviewer_Uq7v"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3508/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698811384814,
            "cdate": 1698811384814,
            "tmdate": 1699636304114,
            "mdate": 1699636304114,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "HFIIgccQWK",
                "forum": "CIqjp9yTDq",
                "replyto": "l04CvOu2G0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3508/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3508/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer Uq7v"
                    },
                    "comment": {
                        "value": "We would like to appreciate for all the time and effort the reviewer has spent on checking our proofs, along with recognition on our writing. Thanks!\n\n**Overall**:\n\nAccording to the reviewer's suggestion, we have included the mentioned paper in related works, together with a corresponding comparison and relevant discussion.\n\nOn the other hand, we have noticed that there maybe some misunderstandings or incorrect points in the review, so we would like to provide further explanations to clarify our contributions.\n\n\n**Weakness 1.1: The upper bound of SHB seems very similar (and worse by a logarithmic factor) than the quadratic setting in Can et al. (2019). I would suggest adding a detailed comparison.**\n\nThanks for the suggestion! We've added corresponding discussions and comparisons with the aforementioned paper in the \"Related Work\" section.\n\nTo make things clearer, we also summarize the comparisons here. Briefly speaking, the proofs in [Can et al. (2019)](https://arxiv.org/pdf/1901.07445.pdf) has **different settings** as ours. Among all the results related to quadratic objectives:\n\n  - Theorem 3, 5, 9 focus on the **deterministic version** of Stochastic Heavy Ball (SHB) and Nesterov's Accelerated Gradient (NAG), which has no stochastic gradient noise and thus can achieve accelerated linear convergence rate with no surprise.\n  - Theorem 4, 7, 11 measure the **distributional convergence**, i.e. convergence speed of parameter distributions towards a stable distribution. Notice that this does NOT imply parameter convergence to optimum or expected excess risk $E[f(w)] - f(w_*)$ convergence to 0.\n  - Theorem 8, 12 provides an upper bound in terms of the expected loss/expected excess risk, which has an extra non-convergent term thus is **much worse** than our bounds.\n\n\n**Weakness 1.2: Also the convergence rate is sublinear, which is not very satisfactory.**\n\nWe would love to obtain linear convergence if possible. Regrettably, it is theoretically non-achievable under stochastic gradient settings, as pointed by lower bounds mentioned in [1] and [2]. This lower bound is called statistical minimax rate, which not only applies to SHB and NAG, but also **all** possible optimization methods based on stochastic gradient oracles. Briefly speaking, all of them have a $\\Omega(1/T)$ lower bound for convergence rate in terms of expected excess risk $E[f(w)] - f(w_*)$.\n\nIt is worth noticing that the provided convergence rate in our paper is already near-optimal when compared with the aforementioned lower bounds (up to log factors).\n\nThe mentioned paper [Can et al. (2019)](https://arxiv.org/pdf/1901.07445.pdf) may give readers a false impression that linear convergence is achievable for SHB or NAG, where they are actually only achievable in distributional convergence or deterministic settings, but not for stochastic optimization, as discussed in our responses for Weakness 1.1.\n\n\n**Weakness 2: The batch size is required to be of order $\\Omega(1/\\epsilon)$, which does not seem realistic, as $\\epsilon$ is usually exponentially small.**\n\nWe would like to kindly remind the reviewer that there is no such requirement in our paper. Notice that in Corollary 3, we only need the number of training samples $MT$ to be $\\Omega(1/\\epsilon)$, where $M$ is the batch size and $T$ is the number of iterations. This is achievable since we adopt decaying learning rates instead of constant learning rates, where the generated variance decreases when learning rates gradually diminish during training.\n\n\n\n[1]: Ge, Rong, et al. \"The step decay schedule: A near optimal, geometrically decaying learning rate procedure for least squares.\" Advances in neural information processing systems 32 (2019).\n\n[2]: Wu, Jingfeng, et al. \"Last iterate risk bounds of sgd with decaying stepsize for overparameterized linear regression.\" International Conference on Machine Learning. PMLR, 2022."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3508/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700252381523,
                "cdate": 1700252381523,
                "tmdate": 1700252381523,
                "mdate": 1700252381523,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "u9EmWOBu5c",
                "forum": "CIqjp9yTDq",
                "replyto": "l04CvOu2G0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3508/Reviewer_Uq7v"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3508/Reviewer_Uq7v"
                ],
                "content": {
                    "comment": {
                        "value": "I would like to express my appreciation to the authors for their very patient response to my review, and my apologize for a few comments that are unclear or incorrect. The authors did make an impressive effort to address my concerns, but some questions persist.\n\n1. I appreciate the efforts of the authors to provide a very careful comparison with Can et al. (2019). This helps to clarify the situations, but aside from the obvious differences the authors mentioned, there are some deeper connections that need to be addressed. For example, the point that there is an extra non-convergent term in Can et al. (2019), which uses a constant learning rate, seems easily remedied by using a decaying learning rate (hinted in Remark 23 therein).\n\n2. By sublinear convergence I meant the first term $\\mathbb{E}(f(w_0)-f(w_*))\\cdot\\exp(2\\log(T)-\\Omega(T/\\log T))$ rather than the variance term (which, of course, cannot be linear). Usually something like $\\exp(-\\Omega(T))$ is more welcomed here, so that the algorithm converges linearly before reaching the variance-dominated regime. Anyway, this point is only a minor complaint which does not affect my assessment of this paper. But I greatly appreciate the authors' patience in answering this point which I did not word properly in detail.\n\n3. I apologize for the careless mistake claiming that $M=\\Omega(1/\\epsilon)$. I appreciate again the authors' patience for forgiving my carelessness. \n\nMy previous evaluation is based on the thought that Theorem 1 is novel, which, as the other reviewers pointed out, does not depend on stochasticity and follows from the corresponding (well-known) result for GD. Considering that the contribution of Theorem 2 is fair but not significant enough compared with Can et al. (2019), I will keep my score unless there is strong evidence to refute this point."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3508/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700267011945,
                "cdate": 1700267011945,
                "tmdate": 1700339570960,
                "mdate": 1700339570960,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "MvELAYPkkw",
                "forum": "CIqjp9yTDq",
                "replyto": "l04CvOu2G0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3508/Reviewer_Uq7v"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3508/Reviewer_Uq7v"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks to the authors for their detailed response. Concerning the interpretation of Can et al. (2019), I would like to remind that distributional convergence is **stronger** than convergence in expectation, as $\\mathbb{E}f(w) - f(w_*)\\le \\frac{L}{2}\\mathbb{E}\\|w-w^*\\|^2\\le \\frac{L}{2}W_2(\\nu(w), \\delta_{w_*})^2$."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3508/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700314293210,
                "cdate": 1700314293210,
                "tmdate": 1700314409544,
                "mdate": 1700314409544,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "K7egVp6LPt",
                "forum": "CIqjp9yTDq",
                "replyto": "l04CvOu2G0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3508/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3508/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response  to reviewer Uq7v R3, false claim and two mistakes"
                    },
                    "comment": {
                        "value": "Thanks for your prompt reply. Regrettably, this claim is simply wrong.\n\nConvergence in distribution generally does not imply convergence in optimization and the two objectives measuring the convergence are not the same. If the reviewer insist on claiming one is stronger, we would encourage the reviewer to write down a step-by-step proof.\n\nThere are two mistakes the reviewer is making here.\n\nFirst, in advanced probability theory, convergence in distribution is indeed stronger than convergence in expectation. However, in the context of optimization, this is totally different. The tool is simply applied in a wrong way here. In Can et al. (2019), it proved that the initial distribution converges to a stable distribution in terms of Wasserstain Distance. Let's omit the details and say that if the tool could be applied here, then we get:\n- The expectation of the distribution, i.e. $\\mathbb{E}[w_t]$, converges to the expectation of the stable distribution $\\hat{w}$.\n\nWhether $\\hat{w}$ is $w_*$ remains unknown. Also, even $\\hat{w} = w_*$ and $\\mathbb{E}[w_t] \\to w_*$, it doesn't imply $\\mathbb{E}[f(w_t)] - f(w_*) \\to 0$, just consider the simple case of $w_t \\in U(-1,1)$ (uniform distribution) and $f(w) = w^2$.\n\nThe second mistake is the right hand side of the second inequlity $W_2(\\nu(w), \\delta_{w_*})^2$. Notice that in Can et al. (2019), it only says that the distribution converges to a stable distribution, whether this stable distribution is Dirac delta distribution at optimum $w_*$ still remains unknown."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3508/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700323534245,
                "cdate": 1700323534245,
                "tmdate": 1700323735881,
                "mdate": 1700323735881,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "EnK0mqrMFI",
                "forum": "CIqjp9yTDq",
                "replyto": "l04CvOu2G0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3508/Reviewer_Uq7v"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3508/Reviewer_Uq7v"
                ],
                "content": {
                    "comment": {
                        "value": "I wish I could have the privilege of omitting most of the details, since most of them are contained in Can et al. (2019). An argument that is completely similar to my previous comments will show $\\mathbb{E} f(w_t) - f(w_*) \\lesssim W_2^2(\\nu(w_t), \\nu_\\infty) + \\mathbb{E} \\|w_\\infty - w_* \\|^2$, where $w_\\infty\\sim\\nu_\\infty$ is the stable distribution. The MSE $\\mathbb{E}\\|w_\\infty-w_*\\|^2$ is computed explicitly in (87), which can be adapted to (stagewise) decaying learning rate with only notational change. The authors may also notice that the proof in Appendix C.2 of  Can et al. (2019) is actually very similar to this paper."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3508/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700325635021,
                "cdate": 1700325635021,
                "tmdate": 1700325950460,
                "mdate": 1700325950460,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "AS6BR5TY20",
                "forum": "CIqjp9yTDq",
                "replyto": "l04CvOu2G0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3508/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3508/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer Uq7v R5"
                    },
                    "comment": {
                        "value": "Thank you for the prompt reply. We encourage the reviewer to write down the full proof rigorously and step-by-step.\n\nAmong various issues in the above proof idea, one is that the reviewer failed to take into account the effect of small learning rates, where the convergence is a logarithmic number of iterations only when the learning rate is large, such as constant learning rate $\\alpha_{HB}$ defined in (12) of Can et al. (2019)."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3508/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700328348536,
                "cdate": 1700328348536,
                "tmdate": 1700328530123,
                "mdate": 1700328530123,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "r729IcWlHD",
                "forum": "CIqjp9yTDq",
                "replyto": "l04CvOu2G0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3508/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3508/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Uq7v R6"
                    },
                    "comment": {
                        "value": "We appreciate your prompt reply.\n\nAgain, as we pointed out in Response 2 (R2) to the reviewer, there is absolutely no evidence that Can et al. (2019) implies $E[f(w_T)] - f(w_*)$ matching stochastic lower bound. The reviewer's claim is completely unsubstantiated."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3508/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700329188418,
                "cdate": 1700329188418,
                "tmdate": 1700329200551,
                "mdate": 1700329200551,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "e8owKR4wtx",
                "forum": "CIqjp9yTDq",
                "replyto": "r729IcWlHD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3508/Reviewer_Uq7v"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3508/Reviewer_Uq7v"
                ],
                "content": {
                    "comment": {
                        "value": "All my previous comments are clear evidence for this implication, as I have already written a proof showing that $\\mathbb{E}f(w_t)-f(w_*)$ converges to $\\mathbb{E}\\|w_\\infty - w_*\\|^2$, and Remark 23 argued that it's possible to make $\\mathbb{E}\\|w_\\infty - w_*\\|^2=O(1/k)$ with decaying learning rate (to be absolutely rigorous, though it was worded like $\\operatorname{Var}(w_\\infty)=O(1/k)$, the literature it pointed and the techniques therein actually controlled $\\mathbb{E}\\|w_\\infty - w_*\\|^2$). I will stop here as the discussions have become less constructive, but will come back if there is strong evidence otherwise."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3508/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700330244765,
                "cdate": 1700330244765,
                "tmdate": 1700330244765,
                "mdate": 1700330244765,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "R1xjSpF0MK",
                "forum": "CIqjp9yTDq",
                "replyto": "l04CvOu2G0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3508/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3508/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Uq7v R7"
                    },
                    "comment": {
                        "value": "The prompt reply is greatly appreciated. As the reviewer repetitively referred to Remark 23 in [Can et al. (2019)](https://arxiv.org/pdf/1901.07445.pdf), we would like to point out some key aspects that the review missed in the remark.\n\n- First, this is for Nesterov's Accelerated Gradient (AG), not Stochastic Heavy Ball (SHB).\n\n- Also, we highly recommend the reviewer to double-check the proof for the remark, is $\\alpha = \\log^2(k) / (\\mu k^2)$ a decaying learning rate?\n\nAs we provide techniques for SHB + decaying learning rates, differences in those key aspects matter a lot."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3508/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700332261262,
                "cdate": 1700332261262,
                "tmdate": 1700332471154,
                "mdate": 1700332471154,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QlfnYdqJ2B",
                "forum": "CIqjp9yTDq",
                "replyto": "R1xjSpF0MK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3508/Reviewer_Uq7v"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3508/Reviewer_Uq7v"
                ],
                "content": {
                    "comment": {
                        "value": "When I was typing a full proof here, I realized that there were several lemmas doing the same modification I was thinking of, much of which is already present in the paper. This greatly helps me understand the authors' points. I see that we have disagreements on what kind of technical changes should be considered significant, so let's find a more constructive way to reach a consent. I will avoid saying this paper is a \"simple\" (which seems too subjective) variant of Can et al. (2019) and will revise all my previous comments accordingly. Instead, I think we can agree on the following points:\n\n1. The bias term was already shown to converge linearly in $O(\\sqrt{\\kappa})$ iterations in Can et al. (2019), and the main contribution of this paper is to show that the variance term can be made $O(1/T)$ using decaying learning rate.\n\n2. Technically, the main contribution is Lemma 19, and the anisotropic noise assumption (which made it possible to have a $\\kappa$-free variance term).\n\nI think these points are worth better emphasizing in the paper to clearly position the contributions with the literature. That being said, I retain my opinion that these contributions are fair but fall marginally below the threshold for ICLR's high standard."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3508/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700339362017,
                "cdate": 1700339362017,
                "tmdate": 1700339362017,
                "mdate": 1700339362017,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1tt8A66uEC",
                "forum": "CIqjp9yTDq",
                "replyto": "l04CvOu2G0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3508/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3508/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Uq7v R8"
                    },
                    "comment": {
                        "value": "We thank for the reviewer's active participation during the discussion session.\n\n### **Modifications**\n\nTo reach a consensus, we have made corresponding modifications in a new revised version of our paper (version 3/V3) asked by the reviewer. Specifically,\n\n  - In the \"Related Work\" section, we provide the explicit form of accelerated distributional convergence for Can et al. (2019).\n  - In our contribution (Section 1.1), we emphasize more on the variance term $\\tilde{O}(d \\sigma^2/T)$.\n  - In paragraphs after Corollary 3, which explains the intuition behind one of our key technical contributions, we add a specific reference to the Lemma that provides the contribution.\n\n### **Consensus**\n\nRegarding the reviewer's proposed two points, we think the reviewer now understands most parts correctly. However, there are still some missing points.\n\nTo avoid confusion, we would like to provide more clarification about the background and context:\n  - **Lemma 19 in V1 and Lemma 8 in V2/V3**: For Lemma 19, we assume the reviewer is referring to the version before rebuttal. This lemma corresponds to Lemma 8 in our revised versions during the rebuttal.\n  - **Accelerating bias is easy**: Showing the bias term to be converged linearly in $O(\\sqrt{\\kappa})$ for constant learning rates is straightforward, as it is essentially the same for deterministic HB, which is covered in almost every textbook about optimization. The main contribution of Can et al. (2019) is that it proves accelerated linear convergence rates for distributional convergence.\n  - **Balancing bias and variance is hard**: It is highly non-trivial to obtain accelerated linear convergence in bias while still achieving near-optimal rates in variance for SHB + decaying learning rates on quadratics, as to the best of our knowledge, no such work existed before.\n  - **Other technical contributions**: The techniques of using schedulers are also adopted in our proofs to balance bias and variance. Combining it with SHB is non-trivial as well.\n\n### **About Top Conferences**\n\nWe would also like to provide the reviewer with a list of past papers published in top conferences for reference, serving as a background for this subfield and top conferences' attitude towards it:\n\n  - **COLT 2018**: Modified SHB + averaging + least square regression (implies anisotropic noise on quadratics), achieving optimal rate and acceleration in certain types of noises [1]\n  - **NeurIPS 2019**: SGD + step decay + least square regression (implies anisotropic noise on quadratics), achieving near-optimal rate [2]\n  - **ICLR 2022**: SGD + step decay/improved scheduler + strongly convex least square regression & anistropic noise on quadratic, achieving near-optimal rate generally and optimal-rate for certain types of instances [3]\n  - **ICML 2022 (Oral)**: SGD + step decay/polynomial decay + overparameterized linear regression (implies anisotropic noise on quadratics), a detailed analysis of lower bounds and upper bounds [4]\n\nAs SHB covers SGD, our result is technically much harder. Also to the best of our knowledge, this is the first time that an accelerated convergence rate is obtained while still achieving near-optimal variance for SHB + decaying learning rates on quadratics. It also implies SHB benefits in practice, which is mostly negative results in past literature.\n\nWe really appreciate the reviewer's prompt replies during the discussion and hope this can clarify the significance of our contributions.\n\n[1]: Jain, Prateek, et al. \"Accelerating stochastic gradient descent for least squares regression.\" Conference On Learning Theory. PMLR, 2018.\n\n[2]: Ge, Rong, et al. \"The step decay schedule: A near optimal, geometrically decaying learning rate procedure for least squares.\" Advances in neural information processing systems 32 (2019).\n\n[3]: Pan, Rui, Haishan Ye, and Tong Zhang. \"Eigencurve: Optimal Learning Rate Schedule for SGD on Quadratic Objectives with Skewed Hessian Spectrums.\" International Conference on Learning Representationsk(2022).\n\n[4]: Wu, Jingfeng, et al. \"Last iterate risk bounds of sgd with decaying stepsize for overparameterized linear regression.\" International Conference on Machine Learning. PMLR, 2022."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3508/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700381553313,
                "cdate": 1700381553313,
                "tmdate": 1700408163535,
                "mdate": 1700408163535,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]