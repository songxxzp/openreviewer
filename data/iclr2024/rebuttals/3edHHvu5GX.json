[
    {
        "title": "Adaptive Visual Scene Understanding: Incremental Scene Graph Generation"
    },
    {
        "review": {
            "id": "kTIkIlq5M7",
            "forum": "3edHHvu5GX",
            "replyto": "3edHHvu5GX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3278/Reviewer_SDSd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3278/Reviewer_SDSd"
            ],
            "content": {
                "summary": {
                    "value": "The paper investigates different approaches towards continual learning of image scene graphs, where the continual learning of object vocabulary, predicate vocabulary and the combination can increase over time. The authors have experimented with several baselines, proposed several metrics ranging from Recall to mRecall; forgetfulness to generalizability with increasing task numbers."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The problem of image scene graph generation is inherently long-tailed. On top of that, the continuous learning on objects and predicates and their combination makes the problem even more challenging. Their experiments are methodical. The overall idea of the continual learning of them are based on the observation that these are long-tailed distributions and the task definition, dataset creation are all motivated by standard long-tailed learning paradigms which long-tailed class-incremental learning."
                },
                "weaknesses": {
                    "value": "The writing is convoluted in some places. Specially the scene graph generation backbone 3.2 wasn't clear after the first read. The references to CNN-SGG and SGTR aren't separated, and caused a bit of confusion."
                },
                "questions": {
                    "value": "As evident by several studies, the mean recall usually improves at the cost of recall in SGG literature. What is the impact of the long-tailed incremental learning in the forgetfulness of Recall? A recent paper proposed a one-stage method [1] which provided a good balance between Recall and mean Recall. Is it possible to utilize similar backbone so we know the effect of incremental learning on both Recall and mean Recall in a balanced way. Table 2 in [1] shows that mean recall of [1] is more than twice than that of training baseline of Fig S.8 in the current paper.  \n\n\n[1] Desai et al, \"Single-Stage Visual Relationship Learning using Conditional Queries\", NeurIPS 2022."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3278/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698482808361,
            "cdate": 1698482808361,
            "tmdate": 1699636276504,
            "mdate": 1699636276504,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xamDXb6j9b",
                "forum": "3edHHvu5GX",
                "replyto": "kTIkIlq5M7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3278/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3278/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Authors"
                    },
                    "comment": {
                        "value": "**SDSd.1 Weaknesses: The writing is convoluted in some places. Specially the scene graph generation backbone 3.2 wasn't clear after the first read. The references to CNN-SGG and SGTR aren't separated, and caused a bit of confusion.**\n\nWe acknowledge the reviewer's feedback regarding the convoluted explanation of the scene graph generation backbones. To provide a clearer understanding, we offer a more concise and separate explanation for each backbone.\n\nFor SGTR, the approach uniquely formulates the task as a bipartite graph construction problem. Starting with a scene image (Ii), SGTR utilizes a 2D-CNN and transformer-based encoder to extract image features. These features are then incorporated into a transformer-based decoder, predicting object and subject nodes (Oi). Predicate nodes (Ri) are formed based on both image features and object node features, and a bipartite graph (Gi) is constructed to represent the scene collectively. The correspondence between object nodes (oi) and predicate nodes (rk) is established using the Hungarian matching algorithm (Kuhn, 1955). Experimental results are based on the average over three runs, and the implementation leverages public source codes from (Li et al., 2022b) and (Wang et al., 2021b) with default hyperparameters. Refer to Section A.2.1 for detailed training and implementation specifics.\n\nAs for CNN-SGG, it employs Faster-RCNN (Girshick, 2015) to generate object proposals from a scene image (Ii). The model extracts visual features for nodes and edges from these proposals, and through message passing, both edge and node GRUs output a structured scene graph. The training occurs in two stages within a supervised framework. In the initial stage, only object detection losses from Faster-RCNN (Ren et al., 2015) are applied to Oi, incorporating cross-entropy loss for object class and L1 loss for bounding box offsets. In the subsequent stage, the visual feature extractor (VGG-16, pre-trained on ImageNet) and GRU layers are trained to predict final object classes, bounding boxes, and relationship predicates using cross-entropy loss and L1 loss. \n\nWe have also clearly separated the explanation of each backbone in the revised main manuscript in Sec. 3.2. We have clearly referenced the training and implementation details for both backbones in the revised main manuscript as well in Sec. 3.2."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3278/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700668586313,
                "cdate": 1700668586313,
                "tmdate": 1700668586313,
                "mdate": 1700668586313,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "0VBdMbQG3r",
            "forum": "3edHHvu5GX",
            "replyto": "3edHHvu5GX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3278/Reviewer_FdjE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3278/Reviewer_FdjE"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates the continual learning problem in SGG, and conducts experiments to show the performance of existing methods in the proposed continual learning scenarios."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The proposed scenario is realistic and is worth to investigate.\n2. The paper is easy to follow."
                },
                "weaknesses": {
                    "value": "1. Clarity on Relationship with SGG Tasks: The paper lacks clarity in establishing the direct correlation between its observations and the Scene Graph Generation (SGG) tasks. While the identified issues like catastrophic forgetting, the efficacy of replay methods, and addressing long-tail problems are extensively explored in existing research, the unique challenges in integrating continual learning, long-tail problems, and SGG remain unclear. The paper falls short in delineating the specific challenges arising from the amalgamation of these factors.\n\n2. Inadequate Support for Conclusions: The paper argues that replay-based methods underperform on S3 due to models focusing on detecting more in-domain object boxes. However, this conclusion lacks direct substantiation. The mixed nature of the test datasets across all tasks in S3 complicates such straightforward assertions. Further information and experimental evidence are necessary to strengthen and clarify this particular assertion.\n\n3. Limited Contribution to Method Design: The primary contribution of this paper lies in evaluating continual learning algorithms within the proposed scenarios. Nevertheless, this contribution, while valuable, may not meet the rigorous criteria for publication in esteemed venues such as ICLR. This is primarily because it lacks the introduction of novel methods or groundbreaking observations that can serve as a source of inspiration and guidance for future method development."
                },
                "questions": {
                    "value": "1. Could the authors delve further into elucidating the unique challenges that arise in SGG tasks due to continual learning scenarios? Clarifying these challenges could significantly strengthen the paper's contribution.\n\n2. It would be beneficial if the paper explored how the insights garnered from this paper could inspire or guide the design of enhanced methodologies for continual SGG scenarios."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3278/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698675707651,
            "cdate": 1698675707651,
            "tmdate": 1699636276409,
            "mdate": 1699636276409,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "FJWmaGfJSm",
                "forum": "3edHHvu5GX",
                "replyto": "0VBdMbQG3r",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3278/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3278/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Authors"
                    },
                    "comment": {
                        "value": "**FdjE.1 Weaknesses: Clarity on Relationship with SGG Tasks: The paper lacks clarity in establishing the direct correlation between its observations and the Scene Graph Generation (SGG) tasks. While the identified issues like catastrophic forgetting, the efficacy of replay methods, and addressing long-tail problems are extensively explored in existing research, the unique challenges in integrating continual learning, long-tail problems, and SGG remain unclear. The paper falls short in delineating the specific challenges arising from the amalgamation of these factors.**\n\n**FdjE.1 Questions: Could the authors delve further into elucidating the unique challenges that arise in SGG tasks due to continual learning scenarios? Clarifying these challenges could significantly strengthen the paper's contribution.**\n\nWe appreciate the reviewer's feedback and would like to provide a detailed response to address the concerns. Challenges faced by us are as follows :- \n\n* Creating diverse tasks for CSEGG poses a non-trivial challenge, primarily due to the intricate relationships between objects and the potential entanglement of tasks. An essential aspect of task creation involves meticulous data analysis to address this complexity. For instance, dividing objects like \"Man\" and \"Horse\" into separate tasks can diminish the frequency of the relevant relationship \"riding\" since a new task may lack either \"Man\" or \"Horse.\" Blindly partitioning the dataset based on random assignment of objects or relations to tasks risks eliminating numerous pertinent triplets. Careful analysis was indispensable in crafting each learning scenario to ensure the maximum inclusion of relevant training examples in each task. Consider, for instance, a task with objects like \"Horse,\" \"Car,\" and relationships like \"on.\" Such a scenario fails to yield contextually relevant triplets. Therefore, meticulous data analysis was crucial during the creation of each learning scenario. In designing Learning Scenario 1, preserving the long-tailed nature of the dataset across all tasks was a priority. This was imperative to prevent an imbalance where one task would be overloaded with long-tailed classes, leaving the remaining tasks with a shortage of training examples. In Learning Scenario 2, the division of objects and relations was carefully orchestrated to maximize the number of images for both tasks while ensuring the inclusion of contextually relevant triplets. In the case of CSEGG, Learning Scenario 3 involved a detailed analysis to determine the optimal number of objects in each task. This aimed at maximizing the number of training examples and ensuring an equitable distribution of common relations across all tasks. The intricate analysis undertaken to design each task within the learning scenarios is a pivotal challenge specific to CSEGG. Along with this, we overcame implementation challenges as we are the first to implement various SGG backbones in the continual learning setting. \n* A significant hurdle we encountered in our study was the absence of continual learning methods specifically tailored to minimize forgetting and optimize the performance of CSEGG models. In response to this challenge, we introduced a novel generative replay method termed \"Replays via Analysis by Synthesis\" (RAS) detailed in Sec. 4. This method harnesses the capabilities of generative models to artificially produce exemplars representing the classes present in each task. Subsequently, these exemplars are employed to train the model in subsequent tasks, effectively mitigating catastrophic forgetting. The application of RAS is particularly pertinent to SGG, as evident from our observations with CSEGG baselines (which is consistent with continual learning literature covering tasks like detection and classification), where replay methods exhibited superior performance in addressing catastrophic forgetting. However, practical considerations, such as limited storage and privacy concerns in real-world scenarios, posed challenges for traditional replay methods. To address these limitations, we adopted a generative replay approach that generates images solely from the textual descriptions of triplets in each task. This not only enhances user privacy but also allows for the generation of a flexible number of images, eliminating storage constraints as these images are used once and not stored. A unique challenge specific to CSEGG arose during this process. In contrast to image classification scenarios where generated images typically require no annotations, the case of SGG demands annotations for object locations, object labels, and relation labels. To surmount this challenge, we leveraged each task's trained model to generate annotations for the synthetically created images. This combined notation, along with the generated image, formed the basis for training the SGG model, effectively addressing the issue of catastrophic forgetting in the CSEGG context."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3278/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700667881045,
                "cdate": 1700667881045,
                "tmdate": 1700668108816,
                "mdate": 1700668108816,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "2C8N9GCBQh",
            "forum": "3edHHvu5GX",
            "replyto": "3edHHvu5GX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3278/Reviewer_pXA5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3278/Reviewer_pXA5"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes comprehensive studies on several new settings of the scene graph generation task, in which the relationships, scene, and object incremental scenarios are considered. It conducts experiments that combine continuous learning with current two-stage and transformer-based SGG methods and analyzes their performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "I think continual scene graph generation is quite practical. \n\nThe authors provide some introduction and analyses about the learning scenarios, evaluation methods and metrics, and results using current SGG algorithms combined with continuous learning. These analyses are quite essential."
                },
                "weaknesses": {
                    "value": "It seems the organization and writing are quite disordered and difficult to follow. For example, the authors claim scenario 1 has 5 tasks. However, detailed definitions of these tasks are missing. Similar problems exist for the other two scenarios.\n\nThe first contribution seems to over-claim. It seems the images, object classes and relationships inherit from the visual genome dataset. I do not know what is new."
                },
                "questions": {
                    "value": "see weakness"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3278/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698763923399,
            "cdate": 1698763923399,
            "tmdate": 1699636276342,
            "mdate": 1699636276342,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "IOhWcOMU8B",
                "forum": "3edHHvu5GX",
                "replyto": "2C8N9GCBQh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3278/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3278/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**pXA5.1 Weaknesses: It seems the organization and writing are quite disordered and difficult to follow. For example, the authors claim scenario 1 has 5 tasks. However, detailed definitions of these tasks are missing. Similar problems exist for the other two scenarios.**\n\nWe apologize to the reviewer for any confusion resulting from the organization and writing of the manuscript. The meaning of each learning scenario is now clarified in Sec. 3.1 of the main manuscript. Detailed definitions of all tasks for each learning scenario, along with data statistics, are provided in Sec. A.1 and Sec. A.3 and visualized in Figures S4, S5, S6. We acknowledge the oversight of not referencing these details in Sec. 3.1 and have rectified it in the revised manuscript by adding references to the detailed definition section for each learning scenario in Sec. 3.1.\n\n\n**pXA5.2 Weaknesses: The first contribution seems to over-claim. It seems the images, object classes and relationships inherit from the visual genome dataset. I do not know what is new.**\n\n\nWe acknowledge the reviewer's concern regarding the first contribution and appreciate the opportunity to clarify. Our primary contribution, as outlined in \"Main Contribution 1\" in the main manuscript, is not centered around providing a new dataset with images and notations for Scene Graph Generation (SGG). Our focus uniquely centers on the intricacies and dynamics of continual learning within the domain of Scene Graph Generation (SGG). By introducing innovative splits for Visual Genome (VG), we have tailored the dataset to better support the demands of continual learning. Consequently, our efforts extend the capabilities of VG, resulting in the creation of the novel dataset, CSEGG. This dataset stands as the pioneering resource designed specifically for examining the nuances of continual learning within the realm of SGG.\n\nThe challenge arises from the inherent long-tail nature of the VG dataset, making it non-trivial to divide. Our main contribution lies in carefully designing and creating the splits for each continual learning scenario, addressing the long-tailed distribution within VG. This involves selecting combinations of triplets that enable meaningful training samples for each task, ensuring the model can effectively learn from the dataset. For example, in creating Learning Scenario S3, detailed data analysis was conducted to identify splits that maximize common relations across tasks while also providing testing samples with entirely new objects and the same relations.\n\nAlong with specific data division, the design of the individual scenarios is quite unique and specific to study a fixed purpose. Learning Scenario 1 (S1) simulates a real-world scenario where an agent learns new relations over time, focusing on the continual learning impact on SGG when introducing new relations. This is essential for a model to identify new relations between known objects without relearning them, as detailed in Sec. A.1.1. Learning Scenario 2 (S2) replicates scenarios in which new objects and relationships are gradually introduced over time. This study is crucial for real-world applications, ensuring a model can perform a specific task with new objects and relationships, as discussed in Sec. A.1.2. Learning Scenario 3 (S3) evaluates the model's generalization capability, testing its ability to apply existing knowledge to unseen data. This is vital for practical applications, such as a robot using existing spatial relation knowledge to navigate unfamiliar objects, detailed in Sec. A.1.3.\n\nIn summary, our contribution is not the images or notations themselves but rather the thoughtful design of the continual learning scenarios and the specific data division within VG to analyze CSEGG models under the challenging conditions posed by the dataset's long-tailed nature."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3278/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700667472533,
                "cdate": 1700667472533,
                "tmdate": 1700667472533,
                "mdate": 1700667472533,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "M1M31j4apV",
            "forum": "3edHHvu5GX",
            "replyto": "3edHHvu5GX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3278/Reviewer_LwTF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3278/Reviewer_LwTF"
            ],
            "content": {
                "summary": {
                    "value": "This work divides up the Visual Genome dataset into three scene graph-based continual learning scenarios. It then evaluates some baseline approaches on the benchmarks: two scene generation backbone architectures (SGTR and CNN-SGG), three sampling strategies for learning (LVIS, BLS, and EFL), as well as five continual learning approaches (naive, EWC, PackNet, Replay, and joint training)."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* Scene graphs present an interesting (novel to my knowledge) domain for continual learning.\n* The learning scenarios make sense and are well motivated with comparison to (somewhat distant) real-world scenarios.\n* The work pays attention to the distribution of attributes/data for each task per learning scenario (e.g., relationships within a task are long-tailed).\n* The choice of baselines and evaluation metrics seems sound to me.\n* I particularly liked Figure 4 showing an overview of SGTR and the continual learning baseline algorithms.\n* The authors have made their code available."
                },
                "weaknesses": {
                    "value": "* The authors missed connections to the meta-learning and curriculum design literature. From that lens, claims such as \"CSEGG methods improve generalization abilities\" seem a bit unsurprising.\n* The dataset is somewhat small in scale. 150 objects and 50 relationships might not be enough to pose a sufficient continual learning channel. The benchmark also reuses images between tasks in a learning scenario, which is not ideal.\n* Though there is some interpretation of the baselines, I struggled to see what the community should take away about them. \n* I'm also uncertain how the benchmark might encourage future algorithm or model developments.\n* The writing introduces a lot of terms (in bold text). Some of this could be done better, for instance:\n  * Please enumerate the continual learning algorithms in Section 3.3.\n  * Some terms that are introduced are never referenced again in the main text (e.g., Forward and Backward Transfer).\n* It is confusing when \"long-tailed\" in mentioned in the context of dataset creation as well as learning (e.g. the title of Section 4.2 is ambiguous. In Section 3.2, perhaps it would be clearer to say \"Techniques for sampling to deal with long-tailed data\"?)."
                },
                "questions": {
                    "value": "* There are other datasets which come with scene graphs as you laid out on your Related Work section. Why not use those?\n* The caption for Fig 1 suggests objects are nodes and relations are edges. But Fig 1a shows them as nodes of different color. Could you please ensure consistency? Figure 1b intends to show that new objects and relations emerge over time, but some of the uncolored objects (e.g. man, tree) have not appeared previously. So which objects/relations are colored seems arbitrary?\n* Could you please walk us through the immediate implications of your work for the community, rather than distant possibilities?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3278/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3278/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3278/Reviewer_LwTF"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3278/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698962021213,
            "cdate": 1698962021213,
            "tmdate": 1700844402618,
            "mdate": 1700844402618,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9h8cVbOYhm",
                "forum": "3edHHvu5GX",
                "replyto": "M1M31j4apV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3278/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3278/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response by Authors"
                    },
                    "comment": {
                        "value": "**LwTF.1 Weaknesses: The authors missed connections to the meta-learning and curriculum design literature. From that lens, claims such as \"CSEGG methods improve generalization abilities\" seem a bit unsurprising.**\n\nWe highly appreciate your detailed feedback, and we would like to provide additional clarification on how our work distinguishes itself from traditional meta-learning and incorporates curriculum design elements.\n\nOur primary focus is on evaluating the generalization testing performance of CSEGG models, specifically their ability to perform on unseen data. As outlined in Sec. 3.1, In S3 our models are trained across four tasks, each introducing 30 new object classes and a consistent set of 35 relationships. Notably, the object classes in each task are unique and not repeated in any other task. The test set is a standalone set comprising 30 objects not present in the training data, with only the relationships being common across all tasks and the standalone test. This design allows us to assess the model's capability to identify known relationships between objects it has never encountered during training. Importantly, our approach does not involve task adaptation during training and our model undergoes no training on data from the same domain as the test data, highlighting its domain independence and setting it apart from traditional meta-learning methods\n\nFurthermore, we have incorporated the impact of curriculum learning in Learning Scenario 1 (S1). In Sec. A.5.4 of the manuscript, we discuss the effect of curriculum learning in S1, identifying that curriculum learning affects the performances of CSEGG models as seen in Fig. S16. It also aligns with the conclusions in [1], where curriculum learning was found to have a substantial effect on class-incremental learning. We hope these clarifications address your concerns and provide a more comprehensive understanding of our approach.\n\n\n[1] Parantak Singh, You Li, Ankur Sikarwar, Weixian Lei, Daniel Gao, Morgan Bruce Talbot, Ying Sun, Mike Zheng Shou, Gabriel Kreiman, and Mengmi Zhang. Learning to learn: How to continuously teach humans and machines. arXiv preprint arXiv:2211.15470, 2022\n\n**LwTF.2 Weaknesses: The dataset is somewhat small in scale. 150 objects and 50 relationships might not be enough to pose a sufficient continual learning channel. The benchmark also reuses images between tasks in a learning scenario, which is not ideal.**\n\nWe respectfully disagree with the reviewer's comments on the scale of the dataset. The term \"small\" is relative and context-dependent. For instance, MS COCO [1] is considered a substantial dataset in computer vision, playing a crucial role in numerous advancements in the field. While it may be deemed \"small\" when compared to ImageNet [2], its size is still significant and allows for quality research and development.\n\nAdditionally, our choice of the Visual Genome (VG) dataset [3] is intentional and aligned with the specific requirements of our research. VG is a dedicated scene graph generation dataset designed for tasks similar to SGG. Our aim is to explore the effects of continual learning on SGG, a novel approach that hasn't been extensively studied before. The size of the dataset is not necessarily a limitation; rather, it aligns with the scope of our research objectives. VG provides a sufficient basis for studying continual learning in SGG, allowing us to investigate the model's performance across different tasks.\n\nThe reuse of images between tasks is a deliberate choice, as explained in Sec. A.1.5 of our manuscript. We draw parallels to human learning scenarios, where a parent guides a baby to recognize various objects in a bedroom. Despite encountering the same scenes, the focus is on continual learning, instructing the baby to detect and recognize individual objects sequentially. This intentional reuse of images serves as a valuable aspect of our experimental design, reflecting real-world learning scenarios.\n\n[1] Lin, Tsung-Yi, et al. \"Microsoft coco: Common objects in context.\" Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13. Springer International Publishing, 2014.\n[2] Deng, Jia, et al. \"Imagenet: A large-scale hierarchical image database.\" 2009 IEEE conference on computer vision and pattern recognition. Ieee, 2009.\n[3] Krishna, Ranjay, et al. \"Visual genome: Connecting language and vision using crowdsourced dense image annotations.\" International journal of computer vision 123 (2017): 32-73."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3278/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700664843077,
                "cdate": 1700664843077,
                "tmdate": 1700664843077,
                "mdate": 1700664843077,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ZXmoIdyxn8",
            "forum": "3edHHvu5GX",
            "replyto": "3edHHvu5GX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3278/Reviewer_TSA3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3278/Reviewer_TSA3"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a scene graph generation dataset for continual learning based on already existing dataset of visual genome. They \nproposed three learning scenarios such as incrementing relationships classes, incrementing objects and relationship classes and generalization on relationship between unseen objects over some tasks. They implemented continual learning for scene graph generation over this dataset using a transformer-based approach and a classic two-stage approach and reported their results on 8 evaluation metric including R@K and mR@K."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Applying continual learning setting on scene graph generation tasks seems to have great potentials for tasks such as robotic navigation etc.\n2. Curated a dataset for continual learning setting from an existing benchmark SGG dataset (VG)\n3.The codes and dataset will be publicly available"
                },
                "weaknesses": {
                    "value": "1. The overall presenation of the paper is difficult to follow and not organized well. (Also too many bold letters for section and figure names)\n2. The authors has proposed three learning scenarios and 8 evaluation metrics. All the learning scenarios has multiple tasks (data separations). However, given that there is a lot to report and include, the results are not summarized well in a tabular form. And it is very difficult to follow how each component is contributing in different metrics and leanring scenrios over the tasks.\n3. Most of the results are written in textual description. Summarizing them in a tabular form and discussing the interesting finding might help the readers to understand the numbers better."
                },
                "questions": {
                    "value": "The concerns in the weakness section"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3278/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3278/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3278/Reviewer_TSA3"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3278/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699756258415,
            "cdate": 1699756258415,
            "tmdate": 1699756258415,
            "mdate": 1699756258415,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Ipfyljq9Qt",
                "forum": "3edHHvu5GX",
                "replyto": "ZXmoIdyxn8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3278/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3278/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response by Authors"
                    },
                    "comment": {
                        "value": "**TSA3.1 Weaknesses: The overall presenation of the paper is difficult to follow and not organized well. (Also too many bold letters for section and figure names)**\n\nWe apologize to the reviewer for any confusion resulting from the organization and writing of the manuscript. We have addressed the issues in the revised manuscript.\n\n**TSA3.2 Weaknesses: The authors has proposed three learning scenarios and 8 evaluation metrics. All the learning scenarios has multiple tasks (data separations). However, given that there is a lot to report and include, the results are not summarized well in a tabular form. And it is very difficult to follow how each component is contributing in different metrics and leanring scenrios over the tasks.**\n\nWe apologize to the reviewer for any confusion resulting from the organization and writing of the manuscript. We have addressed the issues in the revised manuscript. We have included Tab. 1 giving an overview of all learning scenarios. \n\n**TSA3.3 Weaknesses: Most of the results are written in textual description. Summarizing them in a tabular form and discussing the interesting finding might help the readers to understand the numbers better**\n\nWe apologize to the reviewer for any confusion resulting from the organization and writing of the manuscript. We have addressed the issues in the revised manuscript. We have also included results of S1 and S2 in tabular form in Tab. 2 with all the metrics."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3278/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700664534446,
                "cdate": 1700664534446,
                "tmdate": 1700664534446,
                "mdate": 1700664534446,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]