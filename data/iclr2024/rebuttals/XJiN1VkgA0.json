[
    {
        "title": "Generating with Confidence: Uncertainty Quantification for Black-box Large Language Models"
    },
    {
        "review": {
            "id": "O66TS4u7OJ",
            "forum": "XJiN1VkgA0",
            "replyto": "XJiN1VkgA0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8527/Reviewer_sn9f"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8527/Reviewer_sn9f"
            ],
            "content": {
                "summary": {
                    "value": "The paper explores how to detect whether text generated from an LLM is accurate or not, without having access to the models probabilities (so called \"black box\" inference). It does so based on decoding multiple outputs (token sequences) for a single input via sampling and then exploring various measures for how much these multiple outputs agree with each other. \nIt reports experimental results across a range of different models on the task of question answering (QA). QA is chosen to make it easier to evaluate the accuracy of model outputs. \nRelevant literature from other ML fields which have made greater progress than LLMs/NLP in quantifying model uncertainty are referenced."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "On the whole the paper is well written. The topic of uncertainty quantification of LLMs is well motivated. The paper builds of some very recent work in this field, and reports good empirical results across several models which indicate the proposed methods for measuring black-box uncertainty are promising. \nRepeating the experiments and reporting mean + stdDev is helpful for gaining confidence in the reported numbers. \nIt's interesting to see another example of how poorly white-box probabilities reflect accuracy, per tables 1,2,3."
                },
                "weaknesses": {
                    "value": "The paper mentions several times that uncertainty is not the same as confidence. Confidence scores are defined based on the token probabilities, which are not available in black-box inference settings. The paper however isn't very clear in how uncertainty and confidence are not the same thing, with multiple sentences referring to \"uncertainty/confidence\" as though they are the same, and section 4.2. not written in a way that helps clarify the working definitions of how these differ either. This is frustrating for the reader. \n\nThe main limitation is only using question answering to test the models here. There are a spectrum of NLG tasks, and I agree with the authors that caring about uncertainty is less likely for completely open generation. However there are tasks like data-to-text (ie knowledge grounded) NLG, where the outputs need to be trusted, and it remains an open issue whether these methods reported here are transferable to tasks other than QA. The QA task is also interesting in that it is extracting answers from the LLMs weights, rather than using the LLM as a general tool for e.g. the data-to-text task, where the info is given as structural inputs."
                },
                "questions": {
                    "value": "* In the introduction it's presented that LLMs are gaining attention -> it's crucial to quantify their uncertainty.  After reading the paper, I still have to ask why? It seems _crucial_ to improve their accuracy. Is the only rational for improving their uncertainty so that their accuracy can be improved, e.g. by blocking uncertain outputs? Or is there other reasons as well?\n* How is the issue of model calibration different?\n* Do you think these results transfer to other tasks such as data-to-text? Or do you think there may be different results beyond QA tasks?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8527/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698177659827,
            "cdate": 1698177659827,
            "tmdate": 1699637066147,
            "mdate": 1699637066147,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wH2ksas0lq",
                "forum": "XJiN1VkgA0",
                "replyto": "O66TS4u7OJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8527/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8527/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses (1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the constructive review, and appreciating for our experiments and general motivation. We are also thankful for the useful feedback. \nWe have made revisions to our manuscript as well as performed additional experiments following the reviewer's suggestions. \n\n## Uncertainty vs Confidence\n\nApologize for this confusion. \nTo clarify, in short, $U$ is a property of the model's perceived posterior and depends only on $x$ (see L3-5 in Section 3.1, and Eq 1).\n$C$ depends on both $x$ and the answer $\\mathbf{s}$ (L2-L3 of Section 3.2 and Eq 2).\nWe have updated Section 3.2 to further clarify this.\n\n> Confidence scores are defined based on the token probabilities, which are not available in black-box inference settings\n\nJust want to clarify that Eq(1) and Eq(2) are only one possible uncertainty/confidence (that are however mainstream).\nExact definition is just anything taking the form of $U(x)$ and $C(x,\\mathbf{s})$.\nWe hope the updated section 3.2 and the example helps clarify this.\n\n## Non-QA NLP applications\n\n(We assume \"data-to-text\" means tasks like captioning. Please let us know if this is a misunderstanding.)\nOther NLP applications like data-to-text are definitely something we plan to explore next.\nThis paper mostly just follows existing UQ literature and uses QA as it is the easiest to perform evaluation on it. \nEvaluation in general is still quite hard.\nIn fact, we have seen very recent explorations on using model-predicted likelihoood as confidence scores on X-ray report generation task (but they also judge the quality of the report with ROUGE).\n\nWe believe that a lot of the ideas are transferrable and we would like note that CoQA is to some extent \"data-to-text\" except that the data is in textual form as well.\nOur methods do not assume that the input has to be text, as we are only comparing the generations' similarities.\nIf the input is data, we think there are two possible solutions: \nWe skip the data (image or audio) and directly use Jaccard or NLI model on the question and answers, OR;\nWe fine-tune a domain-specific NLI model by training a \"bridging\" module that \"translates\" the other modalities to embeddings that the NLI model understands."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8527/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700326305289,
                "cdate": 1700326305289,
                "tmdate": 1700326305289,
                "mdate": 1700326305289,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9hRbYECV3G",
                "forum": "XJiN1VkgA0",
                "replyto": "O66TS4u7OJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8527/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8527/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses (2/2)"
                    },
                    "comment": {
                        "value": "## Why UQ for NLG is important\n> In the introduction it's presented that LLMs are gaining attention -> it's crucial to quantify their uncertainty. After reading the paper, I still have to ask why? It seems crucial to improve their accuracy. Is the only rational for improving their uncertainty so that their accuracy can be improved, e.g. by blocking uncertain outputs? Or is there other reasons as well?\n\nTo begin with, improving the accuracy is important, but selective generation (i.e. blocking uncertain outputs) is not just about improving accuracy.\nFor example, an influential work [1] in selective classification (an area with a similar motivation, but with much more literature) aims at providing risk guarantees after the selection.\n\nSecondly, if improving accuracy *on the un-rejected samples* is important, then finding a good UQ measure for such selective generation task is also important as a better UQ measure could lead to higher accuracy at the same rejection rate.\nA bad UQ, such as the \"Random\" baseline in Table 1, leads to no improvement.\n\nFinally, selective generation/classification is one of the most important applications of UQ, but in general UQ is about conveying this information about uncertainty to the decision maker (human or algorithm) [2].\nIn UQ literature, people tend to focus on AUROC to check whether the UQ measure is reliable, and we also use AUARC (a metric related to selective generation) as we think it is more directly related to real-world applications.\nThey are both machinery to probe the reliability of UQ measure, whose true importance is due to that decision-making processes intrinsically need to have a reliablity measure on the prediction. \n\n[1] Geifman, Yonatan, and Ran El-Yaniv. \"Selective classification for deep neural networks.\" Advances in neural information processing systems 30 (2017).\n\n[2] Seoni, Silvia, Vicnesh Jahmunah, Massimo Salvi, Prabal Datta Barua, Filippo Molinari, and U. Rajendra Acharya. \"Application of uncertainty quantification to artificial intelligence in healthcare: A review of last decade (2013\u20132023).\" Computers in Biology and Medicine (2023): 107441.\n\n## UQ vs Calibration\n> How is the issue of model calibration different?\n\n(See end of Section 2 for a discussion, and a newly added Appendix C.5 for calibration experiments.)\nThe UQ we focus on tries to find a measure of uncertainty or confidence that is *discriminative* of what's uncertain and what's not. \nIn other words, we care about whether the *ranking* of our UQ measure could effectively predict the reliability of the answers.\nThis is also why most literature uses AUROC as an evaluation metric.\nCalibration cares about the discrepancy between the predicted probability and the frequency.\n(Such \"discriminative-ness\" is often referred to as \"sharpness\" and is considered orthogonal to calibration [3].)\nIn the extreme case - if we know the model's accuracy is 0.6 on average, and we keep predicting a answer's confidence is 0.6 regardless of the answer, we get something that is prefectly calibrated, yet this constant confidence is not useful because it doesn't tell us which answer is more reliable than others (i.e. not \"sharp\").\n\n[3] Gneiting, T., Balabdaoui, F., and Raftery, A. E. Probabilistic forecasts, calibration and sharpness. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 69 (2):243\u2013268, 2007."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8527/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700326326976,
                "cdate": 1700326326976,
                "tmdate": 1700326326976,
                "mdate": 1700326326976,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "23cEu7rUCR",
            "forum": "XJiN1VkgA0",
            "replyto": "XJiN1VkgA0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8527/Reviewer_YpC9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8527/Reviewer_YpC9"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents an innovative approach for generating confidence and uncertainty scores within the context of natural language generation (NLG), while working under the realistic constraint of no white-box access to the underlying model. In this scenario, where only the generated sequences are observable and the language model (LLM) can be queried repeatedly for confidence and uncertainty assessments, the proposed method leverages pair-wise similarity or entailment probabilities between generated samples to construct a weight graph. Spectral clustering is employed on that graph to compute confidence and uncertainty scores. Intriguingly, the paper's results suggest that this method can outperform traditional white-box techniques in certain instances, highlighting its potential for enhancing NLG performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Explores an important problem: with the advent of LLMs it is necessary to address how to compute confidence and uncertainty estimates which was easier to obtain in traditional deep learning setting. \n- Paper is well written and easy to follow \n- The paper proposes an interesting idea to use spectral clustering to produce uncertainty and confidence measures.\n- Evaluates performance on different QA datasets and different LLMs as well."
                },
                "weaknesses": {
                    "value": "- It might be helpful to measure expected or adaptive calibration error to show this method outputs confidence scores which are better in comparison to other methods. \n- Relevant work: https://arxiv.org/pdf/2306.13063.pdf, might be helpful to include comparison between their method and your proposed method."
                },
                "questions": {
                    "value": "- As pointed out in the weakness, for confidence scores measuring ECE or ACE might be helpful in showcasing the effectiveness of the approach. \n- In cases where we have access to model embeddings, I think same method of clustering could be applied where we use similarity between embeddings instead of entailment prob/Jaccard similarity. Would this method give better confidence or uncertainty estimates in those settings as well?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8527/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8527/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8527/Reviewer_YpC9"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8527/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698802146556,
            "cdate": 1698802146556,
            "tmdate": 1699637066005,
            "mdate": 1699637066005,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "CviBd4Mzi5",
                "forum": "XJiN1VkgA0",
                "replyto": "23cEu7rUCR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8527/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8527/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the constructive review and acknowledging our contributions vis-a-vis experiments and presentation, as well as for providing useful feedback.\nIn pursuance to the suggestions made by the reviewer, we have made revisions to our manuscript, and performed some additional experiments.\n\n## ECE or ACE (calibration measures)\n\nThank you for the suggestion.\nWe tried to distinguish the focus of our paper at the end of Section 2.\nJust like in our baseline papers, we focus on the uncertainty estimation, and we care more about the *ranking* of such measures - that is, whether a high confidence means highly reliable prediction or accuracy.\nAll uncertainty or confidence estimates could be conformalized or calibrated to bear concrete meaning in the frequentist sense, by using a calibration dataset.\nThere is a large collection of calibration literature that we can leverage here. \nIn repsonse to your suggestion, we have also run additional experiments using histogram binning to calibrate all methods, and report the ACE in the Appendidx C.3. \n\n## Additional Reference\nThank you for the suggestion and we have included this reference to our revision for completeness.\nAfter reading the paper, we found that this method is more suitable for questions with exact answers (e.g. multiple choice or arithmetics) as the consistency-based component requires comparing exact matches of predictions, which is quite hard to achieve for examples like Figure 6 in the Appendix.\nThe \"verbalized confidence\" is similar to the P(true) baseline we tried.\n\n## Use of model embeddings\n\n> Would this method (using model embeddings) give better confidence or uncertainty estimates in those settings as well?\n\nThank you for the question.\nWhile we did not perform the experiments ourselves (and is not a black-box method), we noticed that our baseline paper's (Kuhn et al. 2023) official implementation tried this idea.\nAs it was not used nor reported in their paper, it is probably not performing well."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8527/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700326262340,
                "cdate": 1700326262340,
                "tmdate": 1700326262340,
                "mdate": 1700326262340,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Aexoj64XZI",
            "forum": "XJiN1VkgA0",
            "replyto": "XJiN1VkgA0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8527/Reviewer_LJF2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8527/Reviewer_LJF2"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a new method for uncertainty quantification for NLG when the model used is a black-box LLMs (no access to logits). The framework used consists of three steps (1) response samples generation, (2) pairwise similarity calculation between responses, and (3) uncertainty and confidence score estimation. Using this framework, the authors compare different estimation methods using number of semantic sets, sum of eigenvalues, degree matrix, and eccentricity. For the pairwise similarity, they compare the standard Jaccard similarity as well as model-based method using predicted probabilities from a natural language inference (NLI) model. Experiment results show that the proposed uncertainty and confidence score estimation outperform the baselines, and sometimes better than white-box methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- A novel approach to estimate uncertainty and model confidence.\n- Thorough experiments on multiple QA datasets and public LLMs."
                },
                "weaknesses": {
                    "value": "- The paper addresses an interesting question that relevant to most of the current LLM works. However, I feel the work is not fully complete, with many results but lack of analysis and insights on what to be the main findings of the paper. There are also some missing details, e.g. what DeBERTa model used for NLI? Was it trained using NLI data?\n- There is a discussion regarding uncertainty vs. model confidence and why it matters, but along the way the differentiation is not clear - Sec 3.2 does not even differentiate the two in details, where I expect that the discussion should be there because of the subsection title.\n- There are many variations/ablations regarding the methods used, i.e. similarity metrics and also uncertainty/confidence measurement but little analysis on which perform best, why, and what is the recommendation for future work if they want to quantify uncertainty / model confidence.\n- I am also a bit concerned regarding evaluation with GPT model. Although the authors perform human verification on a subset, but the subset is pretty small (33 samples per dataset). I think many of these datasets are factual, thus human evaluation might not be as tricky as say creative writing task, so having more human verification on the results would make the claim stronger. There isn't also explanation about inter-annotator agreement on the verification. Though, I appreciate that the authors acknowledge the limitation of using GPT models for evaluation."
                },
                "questions": {
                    "value": "- What is the reasoning for a_{NLI, contra}? If you use a standard NLI model which has three classes, Eq 4 (right) would take into account the entailment and neutral class (since it is 1-p_{contra}, is that correct?\n- Unclear why uncertainty is used for expected accuracy and model confidence for individual accuracy.\n\nSuggestions for paper:\n- Provide examples on when the uncertainty and confidence estimation aligns/doesn't align with the prediction\n- More depth analysis on why a particular method (similarity metric/quantification method) outperform the other methods used in the experiment"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8527/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8527/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8527/Reviewer_LJF2"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8527/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698832996249,
            "cdate": 1698832996249,
            "tmdate": 1700732661153,
            "mdate": 1700732661153,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "dKwLKhyG4m",
                "forum": "XJiN1VkgA0",
                "replyto": "Aexoj64XZI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8527/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8527/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses"
                    },
                    "comment": {
                        "value": "We are grateful to the reviewer for the constructive comments.\nTo begin, we would like to emphasize that this paper focuses on the uncertainty quantification of the LLM, and the experiment setups largely follow the white-box baseline (Kuhn et al. 2023).\nWe have made revisions to our manuscript as well as performed additional experiments following the reviewer's suggestions. \nBelow we present responses to each of the points raised:\n\n## Clarification of Details.\n\n> what DeBERTa model used for NLI? Was it trained using NLI data?\n\nFor a fair comparison, we followed (Kuhn et al. 2023) and used the same official pretrained DeBERTa-large on hugginface, which was fine-tuned on MNLI data.\nMore training details of this model could be found on the model card of `deberta-large-mnli` on huggingface.\n\n\n\n## Uncertainty vs Confidence\n\nWe apologize for the confusion here.\nThe structure was that 3.1 defines uncertainty and 3.2 defines confidence (which is different).\n$U$ is a property of the model's perceived posterior and depends only on $x$ (see L3-5 in Section 3.1, and Eq 1).\n$C$ depends on both $x$ and the answer $\\mathbf{s}$ (L2-L3 of Section 3.2 and Eq 2).\nThese are commonly used terms in two somewhat disconnected domains and we have made updates to Section 3.2 to emphasize this distinction.\n\n## Recommended Variation / Why a particular method outperforms\n\nThank you for the constructive question/suggestion.\nIn section 5.3, under *Uncertainty Measures*, our conclusion was that the similarity measure (Section 4.1) seems more important than variants proposed in Section 4.2.\nWe hypothesize that \"entail\" works better because generally the output space is huge, and for two generations, \"not contradicting\" is likely not a sufficient indicator of low-uncertainty (as they can both be bad random responses).\nWe have updated section 5.3 and Appendix B.7 with examples to reflect this hypothesis.\n\n## GPT evaluation\n\nWe would like to clarify that the number of evaluations is *99* per dataset instead of 33 (33 per (model, dataset) pair  * 3 models)\nThe number of total human evaluations is chosen basing on (Kuhn et al. 2023), which evaluated 200 answers for two datasets (we have 297 on three).\nWe tried our best to verify the reliability of GPT evaluation, but the verification process is actually quite slow, as for CoQA we need to read the passage first before judging the answers, and even for TriviaQA or NQ, sometimes seemingly unrelated answers are just two names of the same person and require careful research.\nWe agree that a larger-scale human evaluation with mulitple annotators is more desirable, but the main focus of this paper is UQ, and we think the topic of automatic evaluation is an important research area by itself.\nExisting UQ literature almost always use purely lexical measures like ROUGE-based automatic evaluation, and we believe the GPT evaluation is better (please refer to Appendix B.3 for a discussion).\n\n\nTo address your concern, we increased the number of annotations to 200 per dataset (50 per (model, dataset) * 4 models) and the new accuracies are 89.4/96.5/93 for coqa/trivia/nq.\nUsing the same annotations, the ROUGE-L based metric's accuracy is 81.5/93.5/85. \nIf we construct a confidence interval we could reject that \"GPT and ROUGE are equally good\" (our estimate is that further shrinking the confidence interval by half requires about 20 hours of annotation time).\nWe found that llama2 (newly added) tends to generate different expressions of the same answer, which is sometimes judged incorrect by GPT.\nExcluding llama2, the GPT accuracies are 90.6/98/94 for coqa/trivia/nq.\n\n\n\n## Other Questions\n\n\n> What is the reasoning for a_{NLI, contra}? If you use a standard NLI model which has three classes, Eq 4 (right) would take into account the entailment and neutral class (since it is 1-p_{contra}, is that correct?\n\n$a_{NLI, contra}$ accounts for both neutral and entailment. \nWe could consider the difference between $a_{NLI, contra}$ and $a_{NLI, entail}$ as whether we think \"neutral\" is closer to \u201csimilar\u201d or \u201cdissimilar\u201d.\n\n> Unclear why uncertainty is used for expected accuracy and model confidence for individual accuracy.\n\nThe setting of uncertainty + individual accuracy was in Table 11 (now 10), and we discuss in Appendix C.2 why we choose not to include it in the main text. \nEssentially, since $U$ does not depend on a particular generation (i.e. it is a property of the model's posterior, for a random generation), we think it should be used to predict the (estimated) expected accuracy (which also depends on the posterior/a random generation).\n\n> Provide examples on when the uncertainty and confidence estimation aligns/doesn't align with the prediction\n\nWe've added more examples (e.g. Fig 5-7) in the Appendix in the updated pdf, but we are not entirely sure with what you mean by aligning with the prediction.\nCould you check if the new examples address your concerns, or clarify your suggestion? Thank you!"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8527/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700326239633,
                "cdate": 1700326239633,
                "tmdate": 1700326239633,
                "mdate": 1700326239633,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "EfQdXrG2qo",
                "forum": "XJiN1VkgA0",
                "replyto": "dKwLKhyG4m",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8527/Reviewer_LJF2"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8527/Reviewer_LJF2"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your response and clarification. The paper is clearer now and I've updated my score accordingly."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8527/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700732711542,
                "cdate": 1700732711542,
                "tmdate": 1700732711542,
                "mdate": 1700732711542,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "dJgUfjK6Y4",
            "forum": "XJiN1VkgA0",
            "replyto": "XJiN1VkgA0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8527/Reviewer_1J6D"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8527/Reviewer_1J6D"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposed several metrics to estimate the model output uncertainty for a given question without access to the logic, i.e., close models (openAI models). The proposed metric uses three types of similarity methods to estimate multiple model generations for a question, then uses the pair-wise similarity score to estimate the model\u2019s uncertainty on a given question with different metrics like the eigenvalue of the graph laplacian. They later evaluate the metrics compared with baselines and white-box uncertainty measures. The results showed improvement."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper proposed an interesting idea of estimating black-box generation output without access to the output logits.\n2. The motivation is strong, and the paper is timely as the wide usage of close source models estimating model uncertainty without logits could help downstream tasks avoid using uncertain model output.\n3. The proposed method is intuitive and reasonable to be applied to this situation."
                },
                "weaknesses": {
                    "value": "1. Some more analysis should be conducted, e.g., the impact of similarity functions and intuitions of why one similarity function is more preferred than the other.\n2. Some quantitative examples would be great, such as what kind of questions are evaluated as uncertain as the proposed metric vs the baselines, etc.\n3. The presentation and writing of the paper can be more intuitive. Tables 1, 2, and 3 are very hard to read; an alternative is to include the important numbers and put the rest into the appendix."
                },
                "questions": {
                    "value": "1. Is there any reason for the definition of the degree matrix?\n2. Have you tried other similarity measures, such as cosine similarity, using existing word embeddings?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8527/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8527/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8527/Reviewer_1J6D"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8527/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699035350846,
            "cdate": 1699035350846,
            "tmdate": 1699637065771,
            "mdate": 1699637065771,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "sHK00i2EnQ",
                "forum": "XJiN1VkgA0",
                "replyto": "dJgUfjK6Y4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8527/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8527/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses"
                    },
                    "comment": {
                        "value": "We appreciate the positive feedback on the soundness and overall contribution\nof our work---we are glad that the reviewer found it intuitive and interesting.\nHowever, we would also like to take this opportunity to respond to some concerns regarding the clarity/presentation of our work. \nWe have made revisions to our manuscript as well as performed additional experiments following the reviewer's suggestions. \n\n\n## Analysis on the impact of similarity functions\n\nThank you for this constructive suggestion.\nWe had some discussion in Appendix B.7.\nTo clarify this further, we have updated section 5.3 and Appendix B.7 with some examples to reflect our hypothesis. Newly added text is reflected in blue. \n\n\n\n## Discussion of advantages over baselines\n\nThank you for the suggestion.\nThe only two black-box baselines are NumSet nad LexiSim.\nNumSet is discrete, so our method (EigV in particular) has an advantage over it as it's more granular.\nLexiSim is purely lexical based, and we have some discussion of why meaning-based measures are more useful in Appendix right before B.4.\nWe have also updated the appendix B.7 to include discussion and example of why the proposed measures are better than the baselines.\n\n## Readability of Table 1-3\n\nThank you for the feedback! \nIf you notice, we had already moved most of the results to the Appendix, but due to the large number of experiments we ran it is quite hard to reduce the information. \nThe general message is that the middle rows (\"Ours\") tend to perform better.\nWe have followed your suggestion and also moved AUROC (Table 3) to the Appendix now.\n\n\n## Questions\n\n\n> Have you tried other similarity measures, such as cosine similarity, using existing word embeddings?\n\n\nThis is an interesting idea and seems related to the Jaccard similarity.\nWe just finished running some experiments using the largest Glove embedding (glove.840B.300d) where an answer's embedding is computed as the average embedding of its words, and it performs significantly worse than Jaccard.\nWhile we haven't fully explored this idea due to limited time, we did notice that the baseline paper (Kuhn et al. 2023) seems to have tried the sentence embeddings in their code release but didn't use it in the paper, and we assumed that sentence embedding-based method didn't work well.\n\n\n> Is there any reason for the definition of the degree matrix?\n\nThe degree matrix (Eq.7) was used because it seems like quite a straightforward measure of ``dispersion'', as $C_{deg}$ is just the average distance to other nodes."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8527/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700326217070,
                "cdate": 1700326217070,
                "tmdate": 1700326217070,
                "mdate": 1700326217070,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]