[
    {
        "title": "Con4m: Unleashing the Power of Consistency and Context in Classification for Blurred-Segmented Time Series"
    },
    {
        "review": {
            "id": "AoYTXdOzYB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3266/Reviewer_JdCs"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3266/Reviewer_JdCs"
            ],
            "forum": "EvBx5whpzJ",
            "replyto": "EvBx5whpzJ",
            "content": {
                "summary": {
                    "value": "This paper proposes a blurred-segmented time series classification framework, Con4m, that forces label coherence and prediction behavior between two consecutive predictions. It also incorporates curriculum learning and gradual label change to cope with label inconsistency in transitions. Con4m shows its superiority in two public dataset and one private dataset with ablation studies for each component."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper covers a novel time-series data, blurred-segment time series.\n2. Proposes practical framework for time series classification with noises."
                },
                "weaknesses": {
                    "value": "1. Model degradation by label inconsistency in transition is not validated. The number of timestamp where transition occurs is very small comparing to the length of a whole time series. Does it really harm the model performance significantly? Plus, when annotating SleepEDF, multiple doctors are already recruited to make an agreement in their annotations, which can reduce inconsistency in state transition regions.\n\n2. Methods seem to be a heuristic without enough justification and not novel. In neighbor class consistency discrimination, there could be so many ways to achieve it but there is no explanation on the design choice the authors made. Also, the theory does not support the reason why $\\ell_2$ loss should be used.\n\n3. Experiment setting is not convincing. The labels are disturbed synthetically and one of three datasets is a private dataset, which cannot be reproducible."
                },
                "questions": {
                    "value": "1. What is the dimension of $x_t$? Is it different from $x_1,\\ldots,x_L$? Is $x_t \\in \\mathbb{R}^{L \\times d}$ where $d$ is the number of feature?\n\n2. The function fitting incurs more computations in training loop. Can you elaborate on computational complexity?\n\n3. At which layer $\\ell_1$ and $\\ell_2$ is applied?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3266/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697332524338,
            "cdate": 1697332524338,
            "tmdate": 1699636275130,
            "mdate": 1699636275130,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "dm0DuHU92D",
                "forum": "EvBx5whpzJ",
                "replyto": "AoYTXdOzYB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3266/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3266/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Questions"
                    },
                    "comment": {
                        "value": "**Question 1:**\n\n- In the expressions related to mutual information and probability, $x_t$ represents the random variable of the input raw data. We have adopted the `\\displaystyle \\r` command as suggested in the ICLR template.\n- In Figure 1, $x \\in \\mathbb{R}^{L \\times W \\times d}=\\{ x_1, \\dots, x_L \\}$, where $L$ represents the number of consecutive time segments, $W$ represents the window size (number of time points) in each segment, and $d$ represents the dimensionality of the input features.\n- All of $x_1,\\dots,x_L$ are realizations of $x_t$.\n\n**Question 2:**\n\n- Function fitting does introduce additional computational overhead. However, **since we are fitting a Tanh function and the gradients do not backpropagate to the parameters of the encoder, such overhead is acceptable within the entire model**.\n- Let's assume the number of consecutive time segments in the input is $L$, the hidden representation dimension is $D$, and the number of classification categories is $C$. The Tanh function fitting involves $I$ internal iterative steps. The function fitting module fits the predictions for each category separately, where the prediction for each category in each time segment is a constant. Therefore, the time complexity of the function fitting module is $\\mathcal{O}(ICL)$. If we only consider the final prediction linear layer in the encoder, which maps the hidden representation to the prediction logits, its complexity is $\\mathcal{O}(DCL)$.\n- Hence, we suggest controlling $I<D$ in practical usage to effectively manage the time complexity of the function fitting module. In our experiments, we set the default parameters to $I=100$ and $D=128$.\n\n**Question 3:**\n\nAs depicted on the left side of Figure 2, both $\\ell_1$ and $\\ell_2$ jointly guide the learning process of the entire encoder."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3266/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699969323597,
                "cdate": 1699969323597,
                "tmdate": 1699969323597,
                "mdate": 1699969323597,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "o4Z8dT14vK",
                "forum": "EvBx5whpzJ",
                "replyto": "AoYTXdOzYB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3266/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3266/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Weaknesses"
                    },
                    "comment": {
                        "value": "Dear Reviewer JdCs,\n\nThanks for your questions and we carefully address your concerns as follows:\n\n**Weakness 1:**\n\nSolving the state transition problem holds significant practical significance.\n\n- **Harmonizing inconsistent boundary labels is an important means of reducing annotation costs.** As you mentioned, SleepEDF, being a widely used publicly available dataset, requires highly accurate labels. Achieving this level of precision necessitates the labor-intensive process of expert annotation, which is time-consuming. In the era of large-scale models, data and labels have become the core fuel for model performance. Unlike the field of NLP, medical annotations are precious and scarce. To reduce annotation costs and enable multiple institutions to collaborate on building the foundational model, harmonizing the annotations of different doctors is an essential step.\n- **Inconsistent boundary labels can also lead to a decrease in model performance.** Samples near the boundaries also appear throughout the entire time series. For instance, in SEEG data, there exists waveform overlap between the seizure waves and the interictal waves. As demonstrated in the case study presented in the paper, the repeated occurrence of samples near the boundaries confuses the model during training. Learning label consistency with contextual information can reduce the interference caused by inconsistent labels, allowing the model to converge more consistently on training samples.\n- **Accurately identifying boundaries is necessary for certain application scenarios.** For example, in the seizure detection task based on SEEG data, doctors need to determine the lesion areas by considering the temporal order of seizure waves generated by different channels (corresponding to different brain tissues). This is because the seizure waves will propagate from the lesion areas to surrounding brain tissues. The spread occurs rapidly, making it crucial to finely identify the boundaries of seizure onsets. Inconsistent labels prevent the model from providing precise and consistent predictions at the boundaries.\n\n**Weakness 2:**\n\n- The core insight of our theory lies in the selection of valuable contextual information. While people often focus solely on contextual information at the data level, **our theoretical framework allows for the incorporation of contextual information at the label level into the model's design**. In Section 3.2, lines 134-144, we mention that the theoretical variable $x_{\\mathbb{A}\\_t}$ can be replaced by $y_{\\mathbb{A}\\_t}$, benefiting from the analytical framework of information theory.\n- Based on theoretical analysis, the design in Section 3.2 revolves around incorporating $y_{\\mathbb{A}_t}$ into the model's predictions. Introducing the neighbor class consistency discrimination task is the most intuitive approach. We explicitly discriminate whether contextual samples belong to the same category through supervised learning. Based on the output weights of the discriminator, **we directly incorporate the label information of contextual samples into the final predictions through a linear weighted sum**. This design is straightforward, concise, and effective.\n\n**Weakness 3:**\n\n- As you mentioned, publicly available datasets like SleepEDF require highly accurate labels. The absence of BST datasets is due to previous works ignoring the issue of label inconsistency. It is precisely because of this absence that we propose a novel label disturbance method for time series data (lines 219-226). Our experiments (lines 266-272) demonstrate that the label disturbance method we propose is more essential and challenging for time series data. **We have also included the codes in the attachment on how to disturb the publicly available datasets, which can be used by others.**\n- To validate the model's performance on more realistic datasets with inconsistent labels, we conducted experiments on SEEG. Apart from the data of the subjects in the test group, the data of other subjects were not manually harmonized (lines 239-242). **These experimental results can more realistically reflect the impact of label inconsistency on the model.** Due to strict ethical review, SEEG is highly sensitive medical data that cannot be publicly released at the moment. However, we are actively working towards collaborations with hospitals and subjects, hoping to share this data in a safer and restricted manner."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3266/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699969589708,
                "cdate": 1699969589708,
                "tmdate": 1699969589708,
                "mdate": 1699969589708,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "p2KlKp87aq",
                "forum": "EvBx5whpzJ",
                "replyto": "o4Z8dT14vK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3266/Reviewer_JdCs"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3266/Reviewer_JdCs"
                ],
                "content": {
                    "title": {
                        "value": "Response to author's rebuttal"
                    },
                    "comment": {
                        "value": "Thanks for your response. I have further question as follows.\n\n1. How much more time does Con4m take for its training comparing to training an encoder without fitting tanh? Is it the difference negligible?\n\n2. Samples near the boundaries do not always appear throughout the entire time series. Transitions occur rarely and the data point neighboring boundaries would be also distinct when comparing to in-segment data points. If there are some examples, can you please refer the section or line number? Or, is there any numerical result that the label transition cause the performance degradation?"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3266/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700469054229,
                "cdate": 1700469054229,
                "tmdate": 1700469054229,
                "mdate": 1700469054229,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "NGkKXDrVgO",
            "forum": "EvBx5whpzJ",
            "replyto": "EvBx5whpzJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3266/Reviewer_zaSc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3266/Reviewer_zaSc"
            ],
            "content": {
                "summary": {
                    "value": "The paper focuses on a time series classification problem in a novel setting of \"blurred segmented\" time series where each time series is exhaustively segmented and each segment is labeled with one of the given states. The notion of \"blur\" stems from the blurry transition boundaries between the two consecutive states in a given time series. The ultimate goal (to my understanding) is to train a Time Series Classification (TSC) model which can automate the segmentation and labeling process on such time series.  To train such a TSC model, the training data is comprised of labeled BS time series where the labels of all segment are manually annotated by multiple domain experts. The key feature of the proposed solution  is a novel deep learning attention-layer based architecture which is capable of leveraging the contextual dependencies between adjacent segments of time series. The proposed approach is evaluated against multiple baselines on three real-world datasets  (two public and one private) from healthcare and neuro-science domain which also appear to be the key applications of such work. The evaluation results seem to indicate better performance of the proposed approach in comparison to the baselines."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The problem setting of time series classification on blurred segmented time series is quite relevant for many domains. \n2. The proposed approach seems to be outperforming the state-of-the-art time series classification baselines on multiple real-world datasets. \n3. Ablation studies seem to justify the value of various components of the approach."
                },
                "weaknesses": {
                    "value": "1. The paper is not written well and a bit difficult to follow. To begin with, the term \"blurred segmented time series\" is not concretely defined throughout the manuscript. To the best of my knowledge, this term is not ubiquitous in ML community. Further, the introduction does not clearly define the problem formulation.  In particular, it is not clear whether the end result is to classify the individual segments or classify an entire time series which comprises of multiple segments. The problem formulation is not mathematically defined even in subsequent sections which keeps a reader busy guessing. Further, several terms like samples vs segments vs timestamps , state vs labels are confusingly used at several places which makes it super difficult to understand the exact problem formulation. It is also not clear whether a segment is of fixed length or varying length. \n\n2. The motivation regarding too much noise in the labels in the segments due to label inconsistencies on boundary segments is also not super convincing. For instance, why can't one simply get rid of  such boundary segments and train the model only on cleaner samples?\n\n3. The theoretical justification section also lacks rigor and not quite convincing. In particular, the authors use mutual information definitions to make arguments in support of choosing augmented features from a neighborhood segment window. However, those arguments are very superficial and lack rigor (see detailed comments below).\n\n4. The description of proposed approach is also quite difficult to follow. Several key notations are not well defined (e.g. what are V_s and V_t) and I had to read the papers in related work (e.g. Xu et al. 2022) in quite detail from where the ideas are borrowed. Even then, certain components of the approach such as neighbor class consistency discrimination are yet not clear to me."
                },
                "questions": {
                    "value": "Specific comments/questions: \n1. Page 3, line 86-87: This statement doesn't sound quite valid to me. What does it mean to say that we need to increase p(x_{A_t}|x_t)? We aren't talking about one specific value of x_{A_t} here, it's a distribution, right?  And ultimately all the terms are being summed up over all possible values of x_{A_t}. Similar concern for KL divergence argument. Basically, the justification given  in the support of design of proposed approach is not convincing and needs more rigor. \n\n2. Page 3, lines 87-88: What do you exactly mean by \"easier to predict\"? Do you mean adding small noise to the samples? Perhaps being more specific here along with some citations will help.\n\n3. A mathematical problem statement is dearly missing in Section 3. \n\n4. Section 3.1: What is meant by a \"smoother representation\"? Perhaps you meant to say that the representation function should be \"temporally smooth\" so that the neighboring segments get embedded close-by in the embedding space? \n\n5. Section 3.3, Lines 235 - 238: What is the significance of every group? How are you exactly getting 12 and 6 cross-validation results? \n\n6. In section 4.5 (case study), what is the length of each segment? Is it fixed to 2 s? If so, how come SREA and MiniRocket has sub-segments of labels of lengths<2s? Or are we not labeling the entire segment with the same label?\n\n7. Section 4.2, In lines 266-272: Is the noise coming due to challenging boundary disturbances similar to random noise as introduced in this experiment?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3266/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698575621006,
            "cdate": 1698575621006,
            "tmdate": 1699636275035,
            "mdate": 1699636275035,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "S6DY7SXk3I",
                "forum": "EvBx5whpzJ",
                "replyto": "NGkKXDrVgO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3266/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3266/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Weaknesses"
                    },
                    "comment": {
                        "value": "Dear Reviewer zaSc,\n\nThank you sincerely for your thorough and valuable comments. We have carefully addressed your concerns and confusion by making relevant clarifications and modifications in the following response. We will also update the corresponding parts in the original manuscript and submit a revised version shortly.\n\n**Weakness 1 & Question 3:**\n\nApologies for any confusion caused. Allow us to provide the following clarifications:\n\n- Blurred-Segmented Time Series (BST) refers to a type of time series data that exhibits distinct characteristics different from traditional Time Series Classification (TSC) data. In practical applications. BST data often arises, and it is characterized by two main aspects\u2014Blurred Transitions and Prolonged State Durations. We establish this definition in lines 23-29 within the main context.\n\n- We divide a time interval into several consecutive segments and mainly focus on classification for each individual segment. We appreciate your feedback and we've incorporated a formal problem definition at the beginning of Section 3 as follows:\n\n  >**Definition 1.** Given a time interval comprising of $T$ consecutive time points, denoted as $s=\\\\{ s_1,s_2,\\dots,s_T \\\\}$, a sliding window length of $w$ and slide length $r$ is employed for segmentation. The time interval is partitioned into $L$ time segments, represented as $x=\\\\{ x_i=\\\\{ s_{(i-1) \\times r + 1},\\dots,s_{(i-1) \\times r + w} \\\\} | i=1,\\dots,L \\\\}$. The model is tasked with predicting labels for each time segment $x_i$.\n\n- In our paper, we aim to employ 'segment' and 'state' when referring to specific scenarios, and in the context of machine learning, we prefer the terms 'sample' and 'label'. According to the problem definition above, our model focuses on predicting labels for each time segment. Therefore, 'sample' is equivalent to 'segment'. Also, the terms 'state' and 'label' are synonymous in our context, with each state having a corresponding label. Thank you for your recommendation and we will standardize and streamline the use of these terms in our paper.\n\n- According to the problem definition, each time segment maintains a fixed length in the same dataset, as indicated in Table 1.\n\n**Weakness 2:**\n\n- The nature of BST data inherently lacks a clear definition of boundaries. As described in lines 30-37, due to the absence of standardized quantification criteria, different doctors exhibit experiential variations in annotating boundaries. Moreover, this variation itself is ambiguous and lacks a precise definition. Therefore, the removal of boundary samples is not feasible in practice.\n- Employing a fixed length to remove boundary data is a crude and uncontrollable approach. Likewise, eliminating uncertain boundary samples through expert consensus would be prohibitively costly and unsustainable.\n- Some scenarios require precise boundary labels. For instance, in the seizure detection task based on SEEG data, doctors need to consider the temporal sequence of seizure occurrences in different brain regions to devise subsequent surgical strategies. If the model does not account for boundary labels during the training phase, it will struggle to accurately identify the class of samples at the boundaries during the inference phase."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3266/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700136029867,
                "cdate": 1700136029867,
                "tmdate": 1700136078757,
                "mdate": 1700136078757,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZprzVTtrvl",
                "forum": "EvBx5whpzJ",
                "replyto": "NGkKXDrVgO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3266/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3266/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Weaknesses"
                    },
                    "comment": {
                        "value": "**Weakness 3 & Question 1-2:**\n\nThank you for your valuable inquiry. We apologize for the oversight in this section. We have reorganized Theorem 1 as follows:\n\n**Theorem 1.** *Introducing a contextual sample set that maximizes the predictive ability of labels yields the maximum information gain.*\n\n*Proof.* Expanding $\\mathbb{I}(y_t; x_{A_t}|x_t)$, we have:\n$$\n\\begin{align}\n\\mathbb{I}(y_t; x_{A_t}|x_t) &= \\sum_{x_t} p(x_t) \\sum_{x_{A_t}} \\sum_{y_t} p(y_t,x_{A_t}|x_t) \\log{\\frac{p(y_t,x_{A_t}|x_t)}{p(y_t|x_t) p(x_{A_t}|x_t)}} \\\\\\\\ \n&= \\sum_{x_t}p(x_t) \\sum_{x_{A_t}} \\sum_{y_t} p(y_t|x_t,x_{A_t}) p(x_{A_t}|x_t) \\log{\\frac{p(y_t|x_t,x_{A_t})}{p(y_t|x_t)}} \\\\\\\\ \n&= \\sum_{x_t} p(x_t) \\sum_{x_{A_t}} p(x_{A_t}|x_t) D_{KL} (p(y_t|x_t,x_{A_t}) \\Vert p(y_t|x_t)).\n\\end{align}\n$$\nGiven a fixed classification sample $x_t$ and the inherent distribution $p(y_t|x_t)$ of the dataset, the KL divergence is a convex function that attains its minimum at $p(y_t|x_t,x_{A_{t}})=p(y_t|x_t)$. As $p(y_t|x_t,x_{A_{t}})$ approaches the boundary of the probability space, indicating a stronger predictive ability for $y_t$, the value of KL divergence increases. Due to the convexity of KL divergence, there exists a contextual sample set in the dataset that maximizes $D_{KL}(p(y_t|x_t,x_{A_{t}}) \\\\| p(y_t|x_t))$. We denote this sample set as $A_{t}^{\\*}$ and the maximum KL divergence value as $D_{t}^{\\*}$. Additionally, we note that $\\sum_{x_{A_{t}}} p(x_{A_{t}}|x_t)=1$. Hence, we can obtain the upper bound for the information gain $\\mathbb{I}(y_t;x_{A_{t}}|x_t) \\le \\sum_{x_t} p(x_t) \\sum_{x_{A_{t}}} p(x_{A_{t}}|x_t) D_{t}^{\\*} \\le \\sum_{x_t} p(x_t) D_{t}^{\\*}$. To achieve this upper bound, the model needs to introduce a contextual sample set $A_{t}^{\\*}$ for each sample that maximally enhances its label's predictive ability. Moreover, the model needs to reach an optimal selection strategy distribution $p(x_{A_{t}^{\\*}}|x_t)=1, p(x_{A_{t}}|x_t)=0$ (for $A_t \\neq A_{t}^{\\*}$).\n\n---\n\nAccording to Theorem 1, the model needs to find the optimal contextual sample set that enhances the predictive ability of each sample's label. In this paper, we utilize learnable weights to allow the model to adaptively select potential contextual sample sets. After aggregating contextual sample information, we train the model using sample labels. Through explicit supervised learning, the model can enhance its predictive ability for samples end-to-end while optimizing the selected contextual sample set. On the other hand, benefiting from an information-theoretic perspective, $x_{A_{t}}$ in Theorem 1 not only includes the raw data of contextual samples but also incorporates their label information ($y_{A_{t}}$). Therefore, we can introduce contextual information at both the data level (Section 3.1) and the label level (Section 3.2) to enhance the model's predictive ability.\n\nIn the specific implementation, to balance computational costs, we limit the selection of contextual samples within a continuous and finite time range. At the data level, we introduce a learnable Gaussian kernel function to adaptively aggregate neighboring samples with a Gaussian prior, obtaining a smoother representation over time. At the label level, we explicitly train a neighbor class consistency discriminator. We aggregate the predicted results of contextual samples using the output probabilities of this discriminator as weights."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3266/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700136350617,
                "cdate": 1700136350617,
                "tmdate": 1700136350617,
                "mdate": 1700136350617,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uvyNnoSeVp",
                "forum": "EvBx5whpzJ",
                "replyto": "NGkKXDrVgO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3266/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3266/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Weaknesses & Questions"
                    },
                    "comment": {
                        "value": "**Weakness 4:**\n\n- As shown in the leftmost part of Figure 1, we implement the two-branch Con-Attention. $Q$, $K$ and $V$ vectors represent the query, key and value of the self-attention mechanism respectively. To distinguish between different computational branches, we use $s/S$ to represent the branch based on Gaussian prior, and $t/T$ to represent the branch based on vanilla self-attention. $\\sigma$ is the learnable scale parameter of the Gaussian kernel function. $T^l$ and $S^l$ are the aggregation coefficients of the two branches in the $l$-th layer.\n- For the neighbor class consistency discrimination part, our framework allows for the incorporation of contextual information at the label level into the model's design. According to Theorem 1, we aim to identify a set of contextual samples that maximizes the model's predictive capability at the label level. Since directly optimizing the aggregation at the label level is challenging, we adopt the approach of aggregating predictions of samples belonging to the same class to enhance the model's predictive capability. This approach is inspired by the observation that for graph neural networks based on the homophily assumption, aggregating neighbor information belonging to the same class can improve predictive performance [1,2]. Therefore, we explicitly train a discriminator to determine whether two samples belong to the same class. The model selects a contextual sample set based on the discriminator's output probabilities and weights their predictions based on the probability of being classified as the same class.\n\n[1] McPherson, Miller, Lynn Smith-Lovin, and James M. Cook. \"Birds of a feather: Homophily in social networks.\" *Annual review of sociology* 27.1 (2001): 415-444.\n\n[2] Zhu, Jiong, et al. \"Beyond homophily in graph neural networks: Current limitations and effective designs.\" *Advances in neural information processing systems* 33 (2020): 7793-7804.\n\n**Question 4:**\n\nThank you for seeking clarification. And your insight aligns exactly with what we mean. We will incorporate this clarification into our paper.\n\n**Question 5:**\n\nEach group is of the same significance in our analysis. To obtain the cross-validation results, we randomly partition the subjects within the dataset into non-overlapping subsets for both training and testing. For fNIRS, we first divide the data into 4 groups by subjects and follow the 2 training - 1 validation - 1 testing (2-1-1) setting to conduct cross-validation experiments. Therefore, there are $C^2_4 \\times C^1_2=12$ experiments in total. Similarly, we divide the Sleep data into 3 groups and follow the 1-1-1 experimental setting. Therefore, we carry out $C^1_3 \\times C^1_2=6$ experiments. The full contents are also elaborated in lines 679-685 of Appendix G.\n\n**Question 6:**\n\n- The reference coordinates \"2s 4s ...\" in Section 4.5 serve as a guide, while the actual window length (segment length) of SEEG data is 1s with a slide length of 0.5s, as detailed in Table 1.\n\n- During the labeling of time points from time segments, to reduce the error, we adopt a simple strategy for the time points at the intersection of two consecutive time segments. Specifically, the labels of the time points in the first half of the overlapping data align with the preceding time segment, while the latter half aligns with the subsequent time segment. For example, if the segment 0 consists of time points \"1 2 3 4\" with label \"a\", and the segment 1 comprises \"3 4 5 6\" with label \"b\". Then, time point 3 is assigned label \"a\", and time point 4 is assigned label \"b\".\n\n  | Label     | a    | a    | a    | b    | b    | b    |\n  | --------- | ---- | ---- | ---- | ---- | ---- | ---- |\n  | Segment 0 | 1    | 2    | 3    | 4    |      |      |\n  | Segment 1 |      |      | 3    | 4    | 5    | 6    |\n\n**Question 7:**\n\nDetailed implementation of the boundary disturbance is shown in lines 219-226 under the heading \"Label disturbance\". There exists a fundamental distinction between boundary disturbance and random noise. The boundary disturbance is employed to simulate scenarios where labels are inconsistent. The disturbance is introduced by disturbing the boundary points between different states. It is noted that only the results depicted in Figure 3(b) involve in random noise, which shows that boundary disturbance poses a more intrinsic and challenging problem compared to random noise for time series data. All of the other experiments are based on boundary disturbance."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3266/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700136555015,
                "cdate": 1700136555015,
                "tmdate": 1700136555015,
                "mdate": 1700136555015,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "D0vmgw1Tva",
                "forum": "EvBx5whpzJ",
                "replyto": "S6DY7SXk3I",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3266/Reviewer_zaSc"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3266/Reviewer_zaSc"
                ],
                "content": {
                    "title": {
                        "value": "Follow-up questions to authors' responses"
                    },
                    "comment": {
                        "value": "I would like to thank authors for their detailed clarifications. Also thanks for explicitly clarifying the problem statement.\nSome follow-up comments/questions on your response to Weakness 2: \n1. I am not quite convinced as to why eliminating uncertain boundary samples through expert consensus would be costly and unsustainable? In fact, you do the same to generate ground truth on SEEG dataset. \n2. For SEEG data, you mention that the precise boundary labels are indeed important, however your results are evaluated using the expert-consensus heuristic, which is what could have been used as a layman's approach. Ideally, it would be more assertive if there are any unique findings of proposed algorithm (which an expert consensus baseline will fail to identify) that are validated by a domain expert. \nA convincing argument on at least one of the two points will be desirable."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3266/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700637406720,
                "cdate": 1700637406720,
                "tmdate": 1700637406720,
                "mdate": 1700637406720,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0Y3mOVIo6D",
                "forum": "EvBx5whpzJ",
                "replyto": "ZprzVTtrvl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3266/Reviewer_zaSc"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3266/Reviewer_zaSc"
                ],
                "content": {
                    "title": {
                        "value": "Follow-up comments to author's responses to Weakness 3 & Question 1-2."
                    },
                    "comment": {
                        "value": "I am afraid but the above explanation is still not satisfactory due to the following reasons: \n1. Firstly, the statement of theorem is not crisply defined. What do you exactly mean by \"maximizes the predictive ability\" ? Is there a way for you to mathematically state your theorem?\n2. You mention: \"Due to the convexity of KL divergence, there exists a contextual sample set in the dataset that maximizes KL-divergence term\". A convex function has a global minima, how can you say the same for maxima? For given x_t, you can have multiple choices of x_{A_t} that can maximize the KL divergence term. Basically it's not clear how are you getting any advantage out of the fact that the KL divergence (for given x_t) is convex in x_{A_t}. \n3. You mention: \"As p(y_t|x_t, x_{A_t}) approaches the boundary of the probability space, indicating a stronger predictive ability for y_t, the value of KL divergence increases\". The validity of this statement will be easier to assess once you clarify the notion of \"stronger predictive ability\". It sounds like you mean to say the distribution tends to attain extreme probability values for y_t, but this certainly needs to be clarified. \n4. You mention: \"Moreover, the model needs to reach an optimal selection strategy distribution p(x_{A_t} | x_t) = 1 for x_{A_t} = x_{A_t}*\". How does this necessarily align with the original statement in the theorem which only focuses on x_{A_t} that maximizes the \"predictive ability of y\". Are you trying to say that the two requirements are independent and can be simultaenously met? If so, please explain as that's not intuitive, at least to me."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3266/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700639762222,
                "cdate": 1700639762222,
                "tmdate": 1700639762222,
                "mdate": 1700639762222,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "eoe09WaTFm",
            "forum": "EvBx5whpzJ",
            "replyto": "EvBx5whpzJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3266/Reviewer_5NtK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3266/Reviewer_5NtK"
            ],
            "content": {
                "summary": {
                    "value": "Blurred-segmented time series (BST) data has continuous states with inherently blurred transitions, leading to annotation noise. Existing time series classification methods do not consider label inconsistency and contextual dependencies between consecutive classified samples. To address these issues, the paper first theoretically identifies the value of contextual information, and then proposes a transformer-based model that incorporates contextual information from BST data. Moreover, the paper adopts curriculum learning techniques to train the model under annotation noise. Experiments show the proposed method achieves better classification accuracy than baseline methods on three datasets under different levels of label noise."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ The problem setting is new and realistic. The paper consider time series classification on the new blurred-segmented time series data which has inherent contextual dependencies between classified samples. Without relying on the common i.i.d. assumption on samples, the proposed method boosts classification accuracy by explicitly exploiting the neighboring samples of a target sample. \n\n+ The proposed method exploits both contextual information and noisy labels and can be applied to many realistic time series classification problems.\n\n+ The experiments are well-designed and extensive. Results show that the proposed method outperforms baselines on the three datasets with different levels of label noise. Ablation studies show that each of the proposed components are effective in improving the time series classification accuracy."
                },
                "weaknesses": {
                    "value": "- The clarity of the paper can be improved.\n  Proposition 1 is a basic mutual information inequality. It is unclear how the mutual information $I(y_t;x_t,x_{\\mathbb{A}_t})$ relates to the performance of a model.\n\n- The proof of Theorem 1 mismatches with the claim.\n  The proof only analyzes in what cases can $I(y_t;x_{\\mathbb{A}_t}|x_t)$ be increased.\n  How the predictive capability for the labels is defined? How do we know the contextual information enhances the predictive capability? And what is the connection between predictive capability and the mutual information gain?\n\n- The motivation for the proposed method is not clear. For example, why using a Gaussian kernel function can better align with  $p(x_{\\mathbb{A}_t}|x_t)$ and $p(y_t|x_t, x_{\\mathbb{A}_t})$?"
                },
                "questions": {
                    "value": "1. In Proposition 1, how the mutual information $I(y_t;x_t,x_{\\mathbb{A}_t})$ relates to the performance of a model.\n\n2. In Theorem1, how the predictive capability for the labels is defined? How do we know the contextual information enhances the predictive capability? And what is the connection between predictive capability and the mutual information gain?\n\n3. What is the computational complexity of the proposed method? Can the proposed method scale to longer time series?\n\n4. Why using a Gaussian kernel function can better align with $p(x_{\\mathbb{A}_t}|x_t)$ and $p(y_t|x_t,x_{\\mathbb{A}_t})$?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3266/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698726046289,
            "cdate": 1698726046289,
            "tmdate": 1699636274922,
            "mdate": 1699636274922,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "IzAvfQNqQ8",
                "forum": "EvBx5whpzJ",
                "replyto": "eoe09WaTFm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3266/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3266/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Weaknesses"
                    },
                    "comment": {
                        "value": "Dear Reviewer 5NtK,\n\nThank you for acknowledging our work. In the following response, we provide some clarifications and explanations:\n\n**Weakness 1 & Question 1:**\n\n- Specifically, mutual information measures the intrinsic properties of a dataset. For example, $\\mathbb{I}(y_t;x_t)$ quantifies the correlation between sample and label distributions in the dataset from a probabilistic perspective. $\\mathbb{I}(y_t;x_t,x_{A_{t}})$, on the other hand, describes the correlation between sample and label distributions when contextual samples are introduced. The theoretical framework is built upon the assumption that the model can perfectly capture these correlations (line 79). In other words, we seek the theoretical upper bound of the model's performance.\n- A larger value of mutual information indicates stronger correlation between variables. For instance, in the inequality $\\mathbb{I}(y_t;x_t,x_{A_{t}}) \\ge \\mathbb{I}(y_t;x_t)$, introducing contextual samples $x_{A_{t}}$ enhances the correlation between the target sample $x_t$ and its label. In classification tasks, a higher correlation between input samples and labels implies that input samples are more easily distinguishable by their labels. In an ideal scenario, the model also possesses a higher upper bound on its performance to discriminate between samples.\n\n**Weakness 2 & Question 2:**\n\nBased on the previous answer, the increase in $I(y_t;x_{A_t}|x_t)$ determines the extent to which the upper bound of the model's performance improves. The purpose of Theorem 1 is to elucidate the specific contextual sample set that can maximize the correlation between the target sample and its label (i.e., $I(y_t;x_{A_t}|x_t)$). The following explanation is based on our latest version of Theorem 1 (Please see https://openreview.net/forum?id=EvBx5whpzJ&noteId=ZprzVTtrvl). In our proof, predictive capability is equivalent to the uncertainty of the conditional probability distribution of the label given the input sample. As the distribution $p(y_t|x_t,x_{A_{t}})$ approaches the boundary of the probability space, meaning the predicted probability for a certain class approaches 1 (and the probabilities for other classes approach 0), the uncertainty decreases. According to the proof, lower uncertainty corresponds to larger values of KL divergence. The upper bound of $I(y_t;x_{A_t}|x_t)$ is achieved when the KL divergence is maximized, indicating the lowest uncertainty. Therefore, we conclude that introducing contextual samples that maximally enhance predictive capability for the label results in the maximum information gain.\n\n**Weakness 3 & Question 4:**\n\n- According to Theorem 1, the model needs to find the optimal contextual sample set that enhances the predictive ability of each sample's label. In this paper, we utilize learnable weights to allow the model to adaptively select potential contextual sample sets. After aggregating contextual sample information, we train the model using sample labels. Through explicit supervised learning, the model can enhance its predictive ability for samples end-to-end while optimizing the selected contextual sample set.\n- The Gaussian kernel function serves as our prior knowledge for selecting the contextual sample set. BST data exhibits temporal persistence for each state (line 38). By paying closer attention to and aggregating neighboring samples, the model can acquire temporally smoother representations of time segments. Smoother representations lead to smoother predictive probabilities. This benefits not only the prediction of consecutive time segments belonging to the same category with the same label but also aligns with the gradual nature of class transitions (lines 110-114). According to the conclusions of the latest version of Theorem 1, the use of the Gaussian kernel function allows for a more targeted selection of the contextual sample set, thereby enhancing the model's predictive capability. We will revise these statements in the new version accordingly."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3266/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700227348448,
                "cdate": 1700227348448,
                "tmdate": 1700227348448,
                "mdate": 1700227348448,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "k3OYhxToig",
                "forum": "EvBx5whpzJ",
                "replyto": "eoe09WaTFm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3266/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3266/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Questions"
                    },
                    "comment": {
                        "value": "**Question 3:**\n\nThe time complexity of Con4m can be divided into two main parts. The primary computational cost is on the same order as vanilla Transformer. Assuming the number of consecutive input time segments is $L$, the hidden representation dimension is $D$, the number of classes in the classification task is $C$, and the local iteration count of the function fitting module is $I$.\n\n- Con-Transformer: The time complexity of vanilla self-attention is $\\mathcal{O}(LD^2+L^2D)$, and the time complexity of the Gaussian kernel branch is $\\mathcal{O}(LD^2+L^2)$.\n- Coherent class prediction: The time complexity of Neighbor Class Consistency Discrimination is $\\mathcal{O}(LDC)$, and the time complexity of the Tanh function fitting is $\\mathcal{O}(ICL)$.\n\nTherefore, the computational cost and bottlenecks of Con4m are similar to vanilla Transformer. Large sequence lengths (numbers of time segments) and large hidden representation dimensions both increase the model's computational complexity. In contrast to the natural language processing field, where sequences are typically discrete, time series data consists of continuous numerical values. This allows for controlling the sequence length $L$ by adjusting the length of each time segment (patch). Thus, Con4m can be scaled to longer time series by appropriately tuning the patch length, the number of CNN layers, and the size of the convolutional kernels."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3266/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700227433721,
                "cdate": 1700227433721,
                "tmdate": 1700227433721,
                "mdate": 1700227433721,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CWtE30ILHZ",
                "forum": "EvBx5whpzJ",
                "replyto": "IzAvfQNqQ8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3266/Reviewer_5NtK"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3266/Reviewer_5NtK"
                ],
                "content": {
                    "title": {
                        "value": "Follow-up comments to author's responses"
                    },
                    "comment": {
                        "value": "I would like to thank authors for their clarifications. Here are some follow-up comments:\n\n- \"the samples are more easily distinguishable by the labels\" (line 79) does not make sense to me. In the data, is there any uncertainty for a sample to have a certain label? \n\n- I am not able to fully appreciate the novelty of Theorem 1. It seems that Theorem 1 does not bring too much insight. \nThe goal is to increase the predictive ability of a model using contextual samples. Theorem 1 just claims that the model needs to find the optimal contextual sample set that enhances its predictive ability (line 99)."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3266/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700682412122,
                "cdate": 1700682412122,
                "tmdate": 1700682412122,
                "mdate": 1700682412122,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "l6Q6tluekC",
            "forum": "EvBx5whpzJ",
            "replyto": "EvBx5whpzJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3266/Reviewer_2FMY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3266/Reviewer_2FMY"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors study the blurred segmented time series (BST) data prediction problem. The authors theoretically clarify the connotation of valuable contextual information. Based on these insights, prior knowledge of BST data is incorporated at the data and class levels into the model design to capture effective contextual information. Moreover, the authors also propose a label consistency training framework to harmonize inconsistent labels. The authors have performed extensive experiments on real datasets to demonstrate the effectiveness of the proposed method in handling the time series classification task on BST data."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.\tThe authors propose a new framework to handle the time series classification task on blurred segmented time series data.\n\n2.\tThe authors provide some theoretical analysis about the connotation of the valuable contextual information.\n\n3.\tIn the proposed framework, prior knowledge of the BST data at both the data and class levels are incorporated into the proposed model to capture the effective contextual information.\n\n4.\tThe authors have performed extensive experiments on 3 real datasets to demonstrate the effectiveness of the proposed method."
                },
                "weaknesses": {
                    "value": "1.\tSome assumption of the proposed method seems a little strong. In Section 3.2, for the prediction behavior constraint, it is assumed that consecutive time segments span at most 2 classes within a suitably chosen time interval. The time interval may have a big impact on the model performance. However, it is not clear how to choose a suitable time interval for each dataset. The authors also need to perform experiments studying the impacts of the time interval on different datasets.\n\n2.\tThe experimental analysis seems not consistent enough. In Figure 3(b), the analysis about random disturbance is studied on fNIRS and Sleep datasets. In Table 3, the ablation studies are performed on Sleep and SEEG datasets.\n\n3.\tThe experimental analysis is not sufficient. Compared with existing methods, one advantage of the proposed method is to exploit the prior information at both the data and class levels. The authors are suggested to perform experiments studying the performance of the proposed method with only considering the prior information at data level and class level respectively."
                },
                "questions": {
                    "value": "As discussed in Section 3.2, the time interval may have a big impact on the model performance. How to choose a suitable interval for each dataset?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3266/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699070388680,
            "cdate": 1699070388680,
            "tmdate": 1699636274842,
            "mdate": 1699636274842,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "uDXFwhGdRH",
                "forum": "EvBx5whpzJ",
                "replyto": "l6Q6tluekC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3266/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3266/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Weaknesses"
                    },
                    "comment": {
                        "value": "Dear Reviewer 2FMY,\n\nThank you for your comments and we address your concerns as follows:\n\n**Weakness 1:**\n\nThank you for your valuable suggestion.\n\n- With domain knowledge, given the prior visual window size of manual annotation, we recommend selecting a slightly larger window size as the time interval length for two reasons:\n\n  - Typically, the annotation window does not contain data spanning more than two classes.\n  - Opting for a slightly larger window allows the model to leverage more contexts.\n\n  For instance, in the case of SEEG data, the doctors tend to use a 15-seconds visual window to identify the seizure waves. And based on empirical evidence, the duration of seizures is generally not shorter than 15 seconds. Hence, we chose a 16-second time interval to try to avoid the occurrence of multiple label transitions.\n\n- We are currently conducting experiments with time intervals scaled at 0.5, 1.5, and 2 times the initially selected length. Upon completion of the experiments, we will promptly provide the results and analysis.\n\n  To ensure the comparison to existing experimental parameters as fair as possible:\n\n  - We maintain identical experiment groups.\n\n  - We sample the same number of time intervals based on three different time interval lengths respectively. We proportionally scale both the window length and slide length, ensuring consistency in the number of time segments across all experiments.\n\n  - The hyperparameters of the main model and baselines remain consistent.\n\n**Weakness 2:**\n\nIn Figure 3(b), we compare the performance of Con4m under the settings of random disturbance and boundary disturbance. The fNIRS and Sleep data are publicly available with accurate labels, allowing us to experiment with different disturbance strategies. However, for the SEEG data, we only have precise annotations for the test subjects, while the training data includes inconsistent annotations from different doctors (lines 238-242). Therefore, we cannot apply the random disturbance strategy to the SEEG training data. In the ablation experiments, we opted for the experimental settings with more significant boundary disturbance to clearly validate the effectiveness of each component of the model. Additionally, due to the constraints of the cognitive experimental setup, the transitions between different classes in the fNIRS data are more explicit, making them less susceptible to boundary disturbance. Hence, we selected Sleep-20%/40% and SEEG as the experimental groups for the ablation experiments.\n\n**Weakness 3:**\n\nWe have conducted relevant experimental analyses. In the ablation experiments, - Con-T refers to replacing the Con-Transformer module with vanilla Transformer, thereby removing the prior information at the data level. - Coh-P indicates the simultaneous removal of the Neighbor Class Consistency Discrimination module and the function fitting module, meaning the elimination of the contextual information at the label level. - Fit represents the removal of only the function fitting module, implying the exclusion of prior trend information at the label level. Conversely, + Con-T and + Coh-P denote the retention of data-level and label-level contextual information, respectively."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3266/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700233256053,
                "cdate": 1700233256053,
                "tmdate": 1700233256053,
                "mdate": 1700233256053,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "G7jlVcW8M8",
                "forum": "EvBx5whpzJ",
                "replyto": "l6Q6tluekC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3266/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3266/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Weakness 1"
                    },
                    "comment": {
                        "value": "We conduct the experiment about time interval length by selecting well-performing baselines and comparing them with Con4m. The results of the $F_1$ scores for each model are presented below:\n\n|            | fNIRS |       |       |       | Sleep |       |       |       | SEEG  |       |       |       |\n| ---------- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- |\n| $\\times$   | 0.5   | 1.0   | 1.5   | 2.0   | 0.5   | 1.0   | 1.5   | 2.0   | 0.5   | 1.0   | 1.5   | 2.0   |\n| Sel-CL     | 61.07 | 63.86 | 63.90 | 64.50 | 59.06 | 63.48 | 66.34 | 68.24 | 61.18 | 60.50 | 59.18 | 58.13 |\n| MiniRocket | 60.32 | 61.28 | 60.37 | 61.49 | 57.17 | 62.00 | 65.30 | 67.57 | 60.96 | 62.39 | 61.19 | 61.90 |\n| SREA       | 67.55 | 70.10 | 69.46 | 70.91 | 47.13 | 48.81 | 49.98 | 52.30 | 56.14 | 55.21 | 52.09 | 50.28 |\n| Con4m      | 68.79 | 71.28 | 71.80 | 73.15 | 63.39 | 68.02 | 70.41 | 72.11 | 65.97 | 72.00 | 73.81 | 73.45 |\n\nBased on the experimental results, the following observations can be made:\n\n1. **It is not recommended to choose time intervals shorter than the manually annotated visualization window.** For both fNIRS and Sleep data, all models perform the worst when the time interval was set at 0.5 times the window size, with a significant decrease in performance. This may be due to insufficient information captured by the smaller window size.\n2. **Within the range from the visualization annotation window to the average durations of the shortest class, various time intervals can be chosen.** For both fNIRS and Sleep data, models show an improvement in performance as the time interval increases. This improvement is particularly significant in the Sleep data, which may be attributed to the labeling scheme assigning a label to each 30-second time window.\n3. **It is recommended to consider both the characteristics of the data itself and the requirements of practical applications.** For the higher sampling rate and complexity of SEEG data, MiniRocket demonstrates more stable performance. However, Sel-CL and SREA show a decrease in performance as the window size increases. This can be attributed to the non-stationary nature of SEEG, which involves more diverse waveform variations with larger window sizes. Con4m, by considering contextual sample information, maintains relatively stable performance and even achieves higher performance. Additionally, to ensure comparability, we do not adjust the model parameters. When dealing with larger time intervals, it is necessary to consider increasing the model's parameter size appropriately.\n4. In summary, Con4m consistently outperforms other models and exhibits a consistent performance improvement trend across all data. **This indicates the rationality and superiority of Con4m in addressing the classification problem in BST data.**"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3266/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700465745059,
                "cdate": 1700465745059,
                "tmdate": 1700465745059,
                "mdate": 1700465745059,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]