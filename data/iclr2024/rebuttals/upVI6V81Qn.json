[
    {
        "title": "Structural Adversarial Objectives For Self-Supervised Representation Learning"
    },
    {
        "review": {
            "id": "cVgXmSbaOq",
            "forum": "upVI6V81Qn",
            "replyto": "upVI6V81Qn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3531/Reviewer_XvEG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3531/Reviewer_XvEG"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a self-supervised representation learning method for GANs that involves additional structural modeling responsibilities and a smoothness regularizer imposed on the network. The method encourages the discriminator to structure features at two scales by aligning distribution characteristics (mean and variance) and grouping local clusters. The proposed method is free from hand-crafted data augmentation schemes and is shown to produce effective discriminators that compete with networks trained by contrastive learning approaches in terms of representation learning."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Studying representation learning from a generative perspective is interesting and promising.\n- The overall organization and writing of the paper are well, making it easy to understand the work.\n- The effectiveness of the method was experimentally verified on small datasets."
                },
                "weaknesses": {
                    "value": "- The motivation behind the proposed method is not sufficiently clear for me. Despite the authors providing an ablation study, the principles behind the different losses are not well explained. I expect the authors to provide a more reasonable motivation to help readers understand the necessity of the proposed method beyond experimental results.\n- The paper is the lack of discussion and comparison with the relevant work, ContraD [1], which splits the discriminator into feature learning and real/fake discrimination, similar to the motivation of the work.\n-  The generation performance of the proposed method is unsatisfactory, according to the FID results in Table 4. While there is an improvement compared to the outdated BigGAN, it is not an appropriate baseline for current comparison. Since the authors have compared their proposed method to StyleGAN2-ADA, to substantiate their claim of improved image generation quality, it would be beneficial for them to compare it to StyleGAN2-ADA on the same architecture.\n\n[1] Jeong, Jongheon, and Jinwoo Shin. \"Training gans with stronger augmentations via contrastive discriminator.\" arXiv preprint arXiv:2103.09742 (2021)."
                },
                "questions": {
                    "value": "- Why did the authors choose to implement JSD as the loss function? Could a distance metric like Wasserstein-2 distance, commonly used in FID, also based on the assumption of Gaussian distributions, can be used?\n- Given that the loss function involves the computation of covariance and Jacobian matrices, which can be computationally expensive, could the authors provide a comparison of training time and overheads with the baselines?\n- Can the authors conduct parameter analysis experiments to provide guidance on the selection of hyperparameters?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3531/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3531/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3531/Reviewer_XvEG"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3531/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698658525575,
            "cdate": 1698658525575,
            "tmdate": 1699636307190,
            "mdate": 1699636307190,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "bZsg9ec9ov",
                "forum": "upVI6V81Qn",
                "replyto": "cVgXmSbaOq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3531/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3531/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your feedback! \n\n**Q1**: Motivation of proposed methods.\n\n**A1**: Our proposed loss functions seek to synchronize the distributions of real and generated images, as in Equation 2. Additionally, we aim to automate learning of semantic representations. This is achieved through the structuring of non-collapsed representations by optimizing their covariance (Eqn.2) and by creating local clusters through feature grouping (Eqn.6). Figure 2 illustrates the effectiveness of our proposed objectives, showing that by optimizing the discriminator with these objectives, the learned embedding aligns semantically similar data and separates dissimilar data.\n\n**Q2**: Comparison to related work, ContraD\n\n**A2**: We appreciate the reference to the ContraD study and plan to include it in our related work section. ContraD enhances representation learning in GAN frameworks by incorporating view-consistency contrastive learning methods. Their goal differs remarkably from our approach, which demonstrates that adversarial training objectives can inherently encapsulate feature learning capabilities. Our method presents a concise learning paradigm that does not rely on any form of view-consistency objectives, whether explicit or implicit.\n\n**Q3**: Generation quality and evaluating our method on StyleGAN2-ADA\n\n**A3**: In our work, the primary focus is on representation learning rather than image generation. This is why we used a ResNet-18 as the discriminator in all experiments, ensuring consistent comparison with other representation learning frameworks. Although the choice of generator is secondary, it is crucial that it does not become a limiting factor in our pipeline. We opted for BigGAN's generator with increased feature channels for this reason. We did experiment with StyleGAN's generator but found it less effective for our purposes.\n\n**Q4**: Why did the authors choose to implement JSD as the loss function? Could a distance metric like Wasserstein-2 distance, commonly used in FID, also based on the assumption of Gaussian distributions, can be used?\n\n**A4**: The JSD was chosen for its properties of symmetry, bounded values and allowing for the simultaneous optimization of mean and variance. Although the Wasserstein-2 distance similarly optimizes both mean and variance, its unbounded nature can lead to greater instability compared to JSD, making JSD a more stable and suitable choice for our objectives.\n\n**Q5**: Breakdown of the running time.\n\n**A5**: We have benchmarked the running times of each section in the discriminator's (D's) round, with the times reported as average values over 100 runs. The measurements were taken using a batch size of 256 and a single A40 GPU. The time taken for each section is as follows:\n\n- Forward pass of the discriminator: 0.02s\n\n- Forward pass of the generator: 0.07s\n\n- Computing the model's approximated Jacobian: 0.27s\n\n- Computing the adversarial loss: 0.007s\n\nRunning time for a single forward-backward pass is:\n\n- A Complete D's round with our objectives: 0.78s\n\n- A Complete D's round with vanilla GAN objectives: 0.29s\n\nOur proposed objectives require about 2.7x the time of the GAN baseline during training, a reasonable time budget given the significant improvement in feature learning ability. It should be noted that, during inference time, there is no additional running time overhead compared to the GAN baseline.\n\n**Q6**: Hyperparameter sweep\n\n**A6**: We run ablation experiments by sweeping through the choices of $\\lambda_h,\\lambda_c, \\lambda_s$ in CIFAR-10 and show the results as follows:\n\n|$\\lambda_c$|$\\lambda_s$|$\\lambda_h$| Linear-SVM|\n|:---:|:---:|:---:|:---:|\n|3 | 5 | 2 | 89.1|\n|3 | 5 | 4 | 89.8|\n|2 | 5 | 4 | 88.7|\n|1 | 5 | 4 | 88.6|\n|1 | 3 | 4 | 88.7|\n\n**Table C: Ablation experiments on hyper-parameters.** Our method is insensitive to the hyperparameter change within a reasonable range."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3531/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700613222099,
                "cdate": 1700613222099,
                "tmdate": 1700613222099,
                "mdate": 1700613222099,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "RucyYmSdJB",
            "forum": "upVI6V81Qn",
            "replyto": "upVI6V81Qn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3531/Reviewer_HQAz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3531/Reviewer_HQAz"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a self-supervised framework with adversarial objectives and a regularization approach. The proposed framework does not rely on hand-crafted data augmentation schemes, which are prevalent across contrastive learning methods. The proposed method achieved competitive performance with recent contrastive learning methods on CIFAR-10, CIFAR-100 and ImageNet-10."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Interesting Topic. Getting rid of hand-crafted data augmentation schemes is undoubtedly beneficial for contrastive representation learning.\n\n- Nice ablations. The paper includes comprehensive ablations on data augmentation dependence, other generative feature learners and system variants."
                },
                "weaknesses": {
                    "value": "My main concern is about the main experiments on representation learning performance (Table 1).\n\n- It is not clear why the authors only include toy datasets (CIFAR-10, CIFAR-100 and ImageNet-10) in this table, while they have include experiments on larger datasets(e.g., ImageNet-100) in other tables. Given that the representation learning benchmarks in the baseline methods are all conducted on ImageNet-1k, I don't believe Table 1 is a fair comparison. \n\n- It is also not clear why the authors use SVM and K-M for evaluating the learned representations in Table 1 and do not include linear probing, which is commonly used in the representation literature. \n\nOthers:\n- The reconstruction-based self-supervised methods (e.g., MAE), which have been shown to outperform contrastive learning methods on ImageNet-1k, also do not rely on hand-crafted data augmentations. Hence, to demonstrate the contribution of this work, it is necessary to show that the proposed method can provide performance gain over them on large-scale datasets.\n\n- I think the authors missed a very relevant related work (not my paper) which should be discussed and compared with: Li et al. MAGE: MAsked Generative Encoder to Unify Representation Learning and Image Synthesis. CVPR 2023."
                },
                "questions": {
                    "value": "See weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3531/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698696858970,
            "cdate": 1698696858970,
            "tmdate": 1699636307090,
            "mdate": 1699636307090,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "dCLdCwVP7P",
                "forum": "upVI6V81Qn",
                "replyto": "RucyYmSdJB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3531/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3531/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your feedback!  \n\n**Q1**: Large-scaled experiment on ImageNet-1K.\n\n**A1**: Please see the large-scale experiments section in our general reply.\n\n**Q2**: It is also not clear why the authors use SVM and K-M for evaluating the learned representations in Table 1 and do not include linear probing, which is commonly used in the representation literature.\n\n**A2**: We utilized Linear-SVM for evaluation, applying the default hyperparameters provided by SKlearn. This approach, compared to linear probing that uses mini-batch gradient descent, minimizes randomness by running over the entire dataset in a single batch. As Linear-SVM is also a linear classifier, we anticipate both classifiers reaching similar end states, a claim supported by the almost identical results in Table 2 for both Linear-SVM and Linear probing. We employed K-Means clustering to evaluate the features under more challenging conditions than linear classifiers, offering a unique perspective on the quality of the learned features.\n\n**Q3**: The reconstruction-based self-supervised methods (e.g., MAE), which have been shown to outperform contrastive learning methods on ImageNet-1k, also do not rely on hand-crafted data augmentations. Hence, to demonstrate the contribution of this work, it is necessary to show that the proposed method can provide performance gain over them on large-scale datasets.\n\n**A3**: Masked Autoencoders (MAEs) implictly employ the view-consistency objectives through a process of implicit matching between corrupted and clean images. On the other hand, they are sensitive to hyperparameters like patch size and mask ratio. In linear probing evaluations, MAEs underperform compared to contrastive learning methods. This is evidenced in MAGE [1], where the MAE-VIT-B achieves a 68$\\%$ probing accuracy, in contrast to the 76.7$\\%$ achieved by the MoCo v3.\n\n**Q4**: I think the authors missed a very relevant related work (not my paper) which should be discussed and compared with: Li et al. MAGE: MAsked Generative Encoder to Unify Representation Learning and Image Synthesis. CVPR 2023.\n\n**A4**: We thank reviewer for the suggestion of MAGE and we plan to include it in our related work section. MAGE enhances representation learning through masked autoencoders by adding a contrastive learning method. This differs significantly from our approach, which aims to demonstrate a straightforward learning paradigm free from any form of view-consistency objectives, whether explicit or implicit.\n\n\n[1] MAGE: MAsked Generative Encoder to Unify Representation Learning and Image Synthesis"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3531/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700613087656,
                "cdate": 1700613087656,
                "tmdate": 1700613087656,
                "mdate": 1700613087656,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "3EWHPIBxu3",
            "forum": "upVI6V81Qn",
            "replyto": "upVI6V81Qn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3531/Reviewer_QTsv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3531/Reviewer_QTsv"
            ],
            "content": {
                "summary": {
                    "value": "Authors propose a approach within the framework of generative adversarial networks (GANs) to enhance self-supervised representation learning. They introduce objectives for the discriminator that include additional structural modeling responsibilities. These objectives guide the discriminator to learn to extract informative representations while still allowing the generator to generate samples effectively. The proposed objectives have two targets: aligning distribution characteristics at coarse scales and grouping features into local clusters at finer scales. Experimental results on datasets  demonstrate the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1)  This paper successfully combines two objectives into GANs to learn a good represenation. \n\n2) The paper shows good figures which is easy to follow.\n\n3) Authors compare with strong baselines, and support the effectiveness of the proposed method."
                },
                "weaknesses": {
                    "value": "My concerns include the following:\n\n1) The cluster property is well-known in the discriminator. Since DCGAN already show it, so I think it is not new in this paper to present it.\n\n2) The presented method is not two much interesting, even authors give a comprehensive analysis. \n\n3) The used datasets are small.I would like to use big datasets to support the proposed method.\n\n4) Also the frameworks are out of fashion. I think the well-known architecture (e.g., stylegan) is more convincing. \n\n5) There are not much visualization results ."
                },
                "questions": {
                    "value": "My main question is about the proposed method. The paper is not new, and has less contribution to this community."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3531/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698945374490,
            "cdate": 1698945374490,
            "tmdate": 1699636307030,
            "mdate": 1699636307030,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0ZPBFL298Z",
                "forum": "upVI6V81Qn",
                "replyto": "3EWHPIBxu3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3531/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3531/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your feedback! \n\n**Q1**: The cluster property is well-known in the discriminator. Since DCGAN already show it, so I think it is not new in this paper to present it.\n\n**A1**: While the discriminator's feature learning capability in traditional GAN frameworks, like DCGAN, is acknowledged, our work focuses on enhancing this aspect significantly. As evidenced in Table 4, conventional GANs exhibit limited feature learning ability. Our innovations in training objectives and regularization techniques have remarkably improved feature learning performance, as shown by the increase in K-Means clustering scores from 29.69$\\%$ to 80.11$\\%$, and Linear classifier scores from 69.31$\\%$ to 89.76$\\%$.\n\n**Q2**: The presented method is not two much interesting, even authors give a comprehensive analysis. The paper is not new, and has less contribution to this community.\n\n**A2**: We respectfully disagree with the assessment of our paper's novelty and contribution. Our approach, diverging from view-consistency objectives, explores a more generalized yet challenging direction in feature learning. Our method demonstrates significant improvements over GAN baselines in feature learning, showcasing a substantial advancement in the field.\n\n**Q3**: The used datasets are small.I would like to use big datasets to support the proposed method.\n\n**A3**: Please see the large-scale experiments section in our overall reply.\n\n**Q4**: Also the frameworks are out of fashion. I think the well-known architecture (e.g., stylegan) is more convincing.\n\n**A4**:  Our primary goal is to advance representation learning, which is why we used a ResNet-18 as the discriminator across all experiments for consistent comparison with other representation learning frameworks. Although the choice of generator is secondary, it is crucial that it does not become a limiting factor in our pipeline. We opted for BigGAN's generator with increased feature channels for this reason. We did experiment with StyleGAN's generator but found it less effective for our purposes.\n\n**Q5**: There are not much visualization results .\n\n**A5**: We have included various visualizations in our paper, such as the learned embedding (Fig 1(b)), the dynamic updates of the learned embedding in a synthetic scenario (Fig 2), and generated images in the Appendix."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3531/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700612983403,
                "cdate": 1700612983403,
                "tmdate": 1700612983403,
                "mdate": 1700612983403,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "lFkBl7i3qO",
            "forum": "upVI6V81Qn",
            "replyto": "upVI6V81Qn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3531/Reviewer_VSHz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3531/Reviewer_VSHz"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces novel regularization for training GANs to improve the representation learning capability of the discriminator. The representation is competitive with popular contrastive techniques, demonstrated by a variety of experiments."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "originality\nquality \nclarity \nsignificance\n\n* This paper proposes a reasonable extension to GAN training, clustering rather than real/fake prediction, with novelty in the application to GANs.\n* The spectral norm of the Jacobian seems novel.\n* The paper is generally well-written\n* The use of GANs for representation learning is compelling"
                },
                "weaknesses": {
                    "value": "The aims of the paper are not constantly clear throughout:\n* From the intro: \"...also improves the quality of samples produced by the generator.\"\n* From 3.1 \"their motivation is to improve image generation quality rather than learn semantic features \u2014 entirely different from our aim.\"\nSomewhat weakening this contribution of the paper.\n\nThe biggest issue is lack of the major comparison dataset in vision representation learning: full ImageNet. I was quite surprised to see this data missing for a few reasons:\n1) It's commonly used in existing literature.\n2) BigGAN (of which the proposed works architecture is inspired by) is trained on ImageNet.\n3) The compared methods are significantly hampered with such small data. \n\nI'd like going to focus on the Masked Auto Encoder (MAE) paper, as I'm quite familiar with that work. The reduced training dataset size of ImageNet10, as well as smaller patch size, is a fairly large deviation. Furthermore, there's no mention of what representation space is used from the MAE: all image patches? the CLS token? While these are fine details, they are crucial for fair comparison. I'm not as familiar with the other compared methods, but given the issues with MAE, I am concerned for the those other methods as well.\n\n\nIt's not clear to me if the proposed method is successful at achieving good representation learning on only small datasets, or broadly. As noted in the StyleGAN2-ada paper, CIFAR-10 is a data limited benchmark. \n\n\nMinor:\n* The use of z,z^g is a little confusing, as z usually refers to the generators input and z^g even moreso."
                },
                "questions": {
                    "value": "The Fine-grained clustering is a bit confusing, can you explain how the memory bank works in greater detail? Is z^b the discriminator representation of the real images encoded into the latent space? The nomenclature is not clear. A plain english explanation as to what the loss function is accomplishing would be illuminating as well."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3531/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698991220047,
            "cdate": 1698991220047,
            "tmdate": 1699636306937,
            "mdate": 1699636306937,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "VivclZmkRZ",
                "forum": "upVI6V81Qn",
                "replyto": "lFkBl7i3qO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3531/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3531/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your feedback! \n\n**Q1**: Our description on generation quality and comparison with ICGAN\n\n**A1**: Our work primarily focuses on representation learning, not on enhancing generation quality. Improved image quality emerges as a secondary benefit from our proposed objectives. These goals are not necessarily in conflict, though their relative importance might vary depending on application. Unlike ICGAN, which uses an external feature learner to improve their generation quality, our framework is designed to learn representations from adversarial training. We refer to ICGAN in our paper to highlight the fundamental differences between their approach and ours.\n\n**Q2**: Large-scaled experiment on ImageNet-1K.\n\n**A2**: Please see the large-scale experiments section in our overall reply.\n\n**Q3**: Implementation details of compared methods. \n\n**A3**: We provide implementation details for the methods we compared, including MAE, in Appendix A.2. For MAE, we used the average of all patch tokens from the encoder's output as the representation for the linear classifier.\n\n**Q4**: Clarification on notations $z$ and $z^g$\n\n**A4**: We use $z$ to denote the encoder's final representation, and $v$ as the input for the generator. This is in contrast to the traditional GAN framework, which only has a single latent space as the generator's input. Our model includes an additional latent space, which is the encoder's output.\n\n**Q5**: Explanation of the Memory Bank\n\n**A5**: The memory bank holds the encoder's representations of real images. It is implemented as a first-in-first-out queue, updated continuously by replacing the oldest features with new ones from the current mini-batch.\n\n**Q6**: Explanation of the proposed adversarial objectives\n\n**A6**: Our objectives aim to align the distribution between real and generated images (Eqn.2 for image generation) and to facilitate automatic learning of semantic representation. The latter is achieved by imposing structure on the non-collapsed representation, by optimizing covariance (Eqn.4), and forming local clusters by grouping features (Eqn.6). The effectiveness of our proposed objective is illustrated in Figure 2, where we show that by optimizing the discriminator with these objectives, the learned embedding aligns semantically similar data and separates dissimilar data."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3531/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700612922628,
                "cdate": 1700612922628,
                "tmdate": 1700612922628,
                "mdate": 1700612922628,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]