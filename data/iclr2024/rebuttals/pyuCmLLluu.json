[
    {
        "title": "Typing to Listen at the Cocktail Party: Text-Guided Target Speaker Extraction"
    },
    {
        "review": {
            "id": "TAEqX5Mt9D",
            "forum": "pyuCmLLluu",
            "replyto": "pyuCmLLluu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5734/Reviewer_ZMpA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5734/Reviewer_ZMpA"
            ],
            "content": {
                "summary": {
                    "value": "This paper deals with the problem of target speaker extraction, where a speech by a speaker having the designated properties is extracted from speech mixtures. Although the previous methods for this task provide a cue of the speaker properties as speeches or images, the proposed method employs text prompts instead and encodes them with LLMs (more specifically, LLaMA-2). Once we can obtain feature embedding for representing the speaker properties, we can extract the target speech by injecting this feature embedding into the existing target speaker extractors. Experimental evaluations with the original dataset demonstrate that the proposed method outperformed the existing target speaker extraction named TD-SpeakerBeam in terms of SDR."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Target speaker (or speech) extraction is one of the hot topics in audio signal processing and many excellent techniques have already been proposed. This work extends those existing methods by introducing text prompts. As presented in Section 2, text prompts (1) are intuitive for humans, (2) have flexibility for designating the target speaker property, and (3) can make effective use of emerging large language models with amazing abilities.\n\n2. This paper is well written, well structured and easy to follow."
                },
                "weaknesses": {
                    "value": "1. I could not understand what is the most significant technical hurdles for incorporating text prompts into the existing target speaker extraction methods.\n    - As far as I know, almost all the previous methods rely on feature embeddings for representing speaker properties to be extracted, which indicates that we do not mind which modality should be used for speaker representations. More specifically, universal sound selector [Ochiai+ Interspeech2022] has almost the same structure as the proposed model presented in Figure 3. This type of integration seems to be natural if we try to feed text prompts into target speaker extraction.\n    - The proposed method provides a simple baseline for text-based target speaker extraction. However, I could not find any special tricks for this purpose.\n\n2. Experimental comparisons with other methods should be presented.\n    - I understand that the authors could not find any other baselines for text-based target speaker extraction. However, the author should introduce a simple baseline different from the proposed method for demonstrating the effectiveness of the proposed method (but it might be difficult since the proposed method itself is a naive baseline).\n    - As presented before, universal target selector [Ochiai+ Interspeech2022] can be easily applied to text-based target speaker extraction. If the authors believe that the proposed method has a technical novelty against this work, experimental comparisons with it is mandatory for demonstrating the effectiveness.\n    - Also, ConceptBeam [Ohishi+ ACMMM2022] presented a different network architecture for target speech extraction. Although this work mainly focuses on the use of images for representing the speech property, it can be easily applied to other modalities such as texts."
                },
                "questions": {
                    "value": "Please check the above Weakness part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5734/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698484845183,
            "cdate": 1698484845183,
            "tmdate": 1699636600727,
            "mdate": 1699636600727,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zwEXlq8z7i",
                "forum": "pyuCmLLluu",
                "replyto": "TAEqX5Mt9D",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5734/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5734/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ZMpA (1/2)"
                    },
                    "comment": {
                        "value": "We appreciate your comments and would like to address your concerns with our following responses.\n\n## Response to Weakness 1\n\nThank the reviewer for these insightful comments. Since the inception of SpeakerBeam [1], target speaker extraction systems have adhered to the encoder + fusion + extractor framework. Many top-ranked approaches in the Microsoft Deep Noise Suppression Challenge at ICASSP 2023 adopt this structure. Therefore, this paper does not question or modify this established framework. Instead, we aim to address the inherent limitations of the entire target speaker extraction process and to chart new territories of possibility. Existing target speaker extraction (TSE) systems primarily face limitations related to privacy and dependency on high-quality voice prints. Specifically, the need for significant amounts of recorded voice raises privacy concerns. Additionally, the systems' performance can degrade significantly when suitably recorded voices are unavailable. In this work, we aim to address these difficulties in leveraging text-based cues for target speaker extraction by addressing the following two research questions:\n\n1. What kind of text cues are useful for target speaker extraction? We devised text cues (absolute/relative/semantic content) inspired by human auditory scene analysis.\n2. How do we enable the target speaker extraction system to understand these text cues? We attempted to use LLMs as a tool for text understanding and adopting LoRA-related fine-tuning techniques to achieve this.\n\nWe believe notable works are typically simple and direct, yet they can potentially create substantial impact or yield transformative changes. Our research addresses many challenges and uncovers a wealth of exciting opportunities that are currently inadequately supported by existing target speaker extraction systems, as outlined below:\n\n- **Privacy issue**: Privacy has always been a public concern, especially in using a speaker's voice. However, voiceprints are critical identification information. Mainstream systems for speaker extraction, which predominantly rely on registered speech, would demand up to 10~30 seconds of recorded voice, e.g., a series of extraction systems, such as [2] and [3], are among the top-ranked extraction systems in Microsoft Deep Noise Suppression Challenge at ICASSP 2023. This requirement for users to provide registered speech when implementing mixed speaker extraction presents a significant challenge. It raises considerable concern, as individuals are naturally reticent about their voices being commodified and circulated in the marketplace. The landscape changes when we consider text information as an alternative. Unlike voiceprints, text does not carry personal identity information, making it a substantially more acceptable option. In addition, the text is undoubtedly the least costly compared to cues of other modalities, e.g., target angle, image, and video.\n- **Length of registered voices**: Our obstacles extend beyond privacy. The efficacy and applicability of the extraction system are also impeded by its dependency on registered voices. In numerous instances, obtaining high-quality and sufficiently lengthy registered voices proves to be a daunting task. We may find ourselves in situations where no registered voices are available. Such circumstances can lead to a substantial degradation in the performance of the target speaker extraction system [1] and complete system failure in the most extreme cases. In response to this challenge, our research ventured into uncharted territory. We recommend incorporating text as an independent extraction cue or in tandem with the registered voice. This represents a pioneering and inventive approach that has not been recognized or explored in the solution strategies.\n- **First to handle speech**: While text has been employed to extract audio events around or even before the development of this paper [4], the issue of speaker extraction has persistently lacked an effective resolution. This is a particularly critical concern as voice forms the foundational mode of human communication. Compared with audio events, which each possess unique and distinct spectral structures, the spectral structures of different voices are strikingly similar. This fundamental difference in our goals is akin to distinguishing between the barks of a Teddy Bear dog and a Samoyed, as opposed to simply differentiating between human voices and dog barks, as the sound extraction model might. To address this issue, we have proposed a novel method that pivots on metadata extraction, considering elements such as absolute or relative attributes and semantic content. This represents the first attempt to use text to navigate these complex challenges. The design of our methodology stems from an intrinsic understanding of how humans distinguish between different auditory objects."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5734/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700480257080,
                "cdate": 1700480257080,
                "tmdate": 1700536344174,
                "mdate": 1700536344174,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "38ldnDOiGv",
                "forum": "pyuCmLLluu",
                "replyto": "TAEqX5Mt9D",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5734/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5734/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ZMpA (2/2)"
                    },
                    "comment": {
                        "value": "(Following the previous)\n\n- **Text as a valuable auxiliary tool**: Our experimental findings indicate that text can indeed function as a valuable auxiliary tool, bolstering the efficacy of target speaker extraction systems. It's crucial to acknowledge that registered voices typically require prior recording. In addition, the speech signal of the same speaker might have highly different characteristics in different conditions due to such factors as acoustic environment or emotional state. It is very challenging to make TSE systems robust enough for such intra-speaker variability [5]. Nevertheless, we can employ text to inform the model about the current state of the speaker within the mixture of speech. This method is undeniably more direct and efficient, providing us with a robust tool to tackle the challenge of voice extraction more effectively.\n\n## Response to Weakness 2\n\nThank you for suggesting the two relevant works. We have added them to the manuscript. \nWe appreciate your feedback and acknowledge the challenges in identifying an appropriate baseline for comparison. \n\nThe work [Ochiai] centers on audio event extraction, utilizing labels of each audio event as extraction cues. As previously stated, the foundational framework for target speaker/audio event extraction is well established. This is also grounded in the SpeakerBeam model. The innovation of this research primarily lies in its use of acoustic event classes to extract sound events. However, this model is not directly transferrable to speech. In sound events, all speakers are classified into a single category. In addition, this approach cannot focus on the content of the speech, such as through descriptions embedded in transcription snippets. It also does not accommodate general natural language queries.\u00a0To establish a comparison with this approach, which employs one-hot representation, we removed data in our dataset associated with extraction through transcription snippets. We only kept the data can be distilled as a one-hot representation to demonstrate the discriminative capabilities of LLMs. The updated experimental results are shown here. \n\n| Entry   | Input (Audio) | Input (Text) | Transcription Snippet (50%, 80%, 100%) | Gender | Language | Far-near | Loudness |\n| ------- | ------------- | ------------ | -------------------------------------- | ------ | -------- | -------- | -------- |\n| Universal Sound Selector (Ochiai et al.)       | N             | One-Hot      | No Support                             | 10.54  | 8.88     | 10.25    | 8.96     |\n| LLM-TSE | Y             | N            | 7.30                                   | 10. 17 | 8.87     | 9.77     | 7.75     |\n| LLM-TSE        | N             | Y            | 2.70, 3.97,7.48                        | 10.40  | 9.38     | 10.57    | 8.89     |\n| LLM-TSE        | **Y**             | **Y**            | **7.96,9.81,10.05**                        | **10.87**  | **9.72**     | **10.66**    | **9.41**     |\n\nNoteworthy, however, using one-hot representations introduces the limitations:\n- One-hot representations can only express attributes with clear classifications, e.g., language/gender/loudness. However, if we want to use other cues, like transcription snippets, one-hot representations are unable to accomplish this\n- One-hot representations lack flexibility. Large language models (LLMs) can assist the target speaker extraction system understand user text inputs, allowing for more generic and diverse expressions. For example, the input of LLM-TSE can be easily extended to open-ended questions, such as \"separating the speaker based on the 3-4 second segment in the mixed speech,\" something that one-hot representations are completely incapable of.\n\nWe appreciate that you pointed out ConceptBeam, a system designed to operate in situations where multiple speakers are discussing various semantic concepts or topics within a mixed audio setting. The system leverages related images to the discussed topic to aid speech extraction. While the architecture of this system, which incorporates an encoder, a fusion module, and an extractor, does bear some similarity to our design, the operational conditions differ significantly. It's important to highlight that the functionality of ConceptBeam is contingent upon the availability of pair-matched images that correspond with the semantic content of the speech. This condition isn't typically satisfied in many practical applications. This unique prerequisite significantly constrains the adaptability of the ConceptBeam system when compared with methods that employ registered voices or our proposed text-based approach.\n\n[1] K. \u017dmol\u00edkov\u00e1 et al., SpeakerBeam, IEEE JSTSP\n\n[2] Y. Ju et al., TEA-PSE 3.0, ICASSP 2023\n\n[3] J. Yu et al., TSpeech-AI System, ICASSP 2023\n\n[4] C. Li et al., Target sound extraction, ICASSP 2023\n\n[5] \u017dmol\u00edkov\u00e1 et al, Neural Target Speech Extraction: An overview, IEEE SPM"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5734/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700481213438,
                "cdate": 1700481213438,
                "tmdate": 1700536993492,
                "mdate": 1700536993492,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KlcL8ChZ8B",
                "forum": "pyuCmLLluu",
                "replyto": "TAEqX5Mt9D",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5734/Reviewer_ZMpA"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5734/Reviewer_ZMpA"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the effort"
                    },
                    "comment": {
                        "value": "Thanks for the effort in the rebuttal. I have read through all the author replies and I found that all the replies are reasonable."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5734/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700729585021,
                "cdate": 1700729585021,
                "tmdate": 1700729585021,
                "mdate": 1700729585021,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "UtuB4vtPLf",
            "forum": "pyuCmLLluu",
            "replyto": "pyuCmLLluu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5734/Reviewer_tdLn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5734/Reviewer_tdLn"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a target speech extraction (TSE) method that uses text-based cues from a large language model (LLM). \nThe model consists of a standard masking extractor with encoder and decoder, where the mask is generated using an audio cue embedding of enrollment speech as well as text cue from the LLM. \nThe model is trained with a range of text prompts to deal with various instructions for TSE tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* The use of LLM enables the framework to handle flexible instructions and rich information to extract the target speech. \n* The method demonstrates improved separation performance in terms of SI-SDR score."
                },
                "weaknesses": {
                    "value": "The method is technically sound but exhibits a marginal novelty. \nMask generation using embeddings of auxiliary information is a common practice today such as (Liu et al. 2022; Oishi et al. 2023). \n\nWhat has become fundamentally possible given the power of LLMs? \nIf we restrict some form of templates of prompts to feed into the LLM for a handful of separation scenarios to specify gender/language/loudness etc., such information may be provided as one-hot representations."
                },
                "questions": {
                    "value": "The ablation study in Table 1 shows some variations of LLM-TSE: audio + text, text-only and audio-only. \n1. Can we train the model with audio + text and switch to text-only or audio-only in inference time? \nIt seems the embedding vectors are concatenated and used for mask generation. \nIn this case, I am wondering how the lack of either modality is handled. \n\n2. How does audio-only LLM-TSE differ from TD-SpeakerBeam?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5734/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5734/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5734/Reviewer_tdLn"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5734/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698659464624,
            "cdate": 1698659464624,
            "tmdate": 1699636600609,
            "mdate": 1699636600609,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kdEbULS952",
                "forum": "pyuCmLLluu",
                "replyto": "UtuB4vtPLf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5734/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5734/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer tdLn"
                    },
                    "comment": {
                        "value": "## Response to Weakness\n\nWe appreciate your insightful comments and the opportunity to clarify our work's scope. As our first venture into text-guided target speaker extraction, we aimed to showcase the potential of our proposed framework by examining a wide range of application scenarios and auditory cues. These include absolute attributes (like language and gender), relative attributes (such as loudness and distance), and semantic cues (i.e., transcription snippets). These cues are extensively used by the human auditory system to tackle the cocktail party problem [1].  Text-based systems open up new possibilities and address issues inherent in traditional target speaker extraction systems:\n\n1. Resolving privacy concerns by using text instead of voiceprints.\n2. Reducing dependency on the length of registered voices by using text as an extraction cue.\n3. Use text can serve as a valuable auxiliary tool to improve the efficacy of target speaker extraction systems, overcoming intra-speaker variability by informing the model about the speaker's current state.\n\nWhile one-hot representations can handle some of these scenarios, they have notable limitations when dealing with text-based cues. Specifically, one-hot representations are constrained to express attributes with distinct classifications, such as language, gender, and loudness. However, they fall short when encoding more nuanced cues, such as transcription snippets that remain less disrupted in an audio sample.\n\nIn contrast, Large Language Models (LLMs) can help the target speaker extraction system understand user text inputs, thereby enabling more generic and diverse text-based cues. For instance, under our proposed framework, the input of LLM-TSE can be easily extended to open-ended questions like \"separate the speaker based on the 3-4 second segment in the mixed speech,\" a task beyond the capabilities of one-hot representations. Similarly, It has the potential to handle complex requests like \"I want to extract the speaker who initially spoke loudly about basketball, but later his voice softened, and his speech speed slowed down as he moved further from the microphone.\" Thus, the proposed LLM-based target speaker extraction framework offers a more adaptable and nuanced approach to speaker extraction, addressing the limitations of existing methods. We believe that our work could have a substantial impact on the field.\n\nFollowing your question, we added this part (distilling the text prompts into one-hot representations) of the experiment to confirm the LLM's ability to understand the text prompts related to speaker extraction. The integration of the one-hot representation is referenced from the work of Universal Sound Selector (Ochiai et al.) [2]. The performance of the text-based system is comparable to that of the one-hot-based system, but it supports more tasks.\n\n| Entry   | Input (Audio) | Input (Text) | Transcription Snippet (50%, 80%, 100%) | Gender | Language | Far-near | Loudness |\n| ------- | ------------- | ------------ | -------------------------------------- | ------ | -------- | -------- | -------- |\n| Universal Sound Selector (Ochiai et al.) [2]       | N             | One-Hot      | No Support                             | 10.54  | 8.88     | 10.25    | 8.96     |\n| LLM-TSE | Y             | N            | 7.30                                   | 10. 17 | 8.87     | 9.77     | 7.75     |\n| LLM-TSE        | N             | Y            | 2.70, 3.97,7.48                        | 10.40  | 9.38     | 10.57    | 8.89     |\n| LLM-TSE        | **Y**             | **Y**            | **7.96,9.81,10.05**                        | **10.87**  | **9.72**     | **10.66**    | **9.41**     |\n\n\n## Response to Question 1\n\nThank\u00a0you\u00a0for\u00a0your\u00a0question.\u00a0Our\u00a0model\u00a0can\u00a0handle\u00a0the\u00a0scenarios\u00a0where\u00a0either\u00a0modality\u00a0is\u00a0missing.\u00a0Specifically,\u00a0our\u00a0proposed\u00a0LLM-TSE\u00a0model\u00a0uses\u00a0a\u00a0combination\u00a0of\u00a0audio+text\u00a0(50%),\u00a0text-only\u00a0(20%),\u00a0and\u00a0audio-only\u00a0(30%)\u00a0data\u00a0for\u00a0training.\u00a0After\u00a0the\u00a0model\u00a0had\u00a0been\u00a0trained,\u00a0we\u00a0tested\u00a0it across three\u00a0scenarios:\u00a0audio-only,\u00a0text-only,\u00a0and\u00a0audio+text,\u00a0as\u00a0shown\u00a0in\u00a0Table\u00a01.\u00a0 This\u00a0presents\u00a0a\u00a0unique\u00a0advantage\u00a0of\u00a0our\u00a0system,\u00a0as\u00a0the\u00a0quality\u00a0of\u00a0the\u00a0registered\u00a0audio\u00a0might\u00a0be\u00a0poor\u00a0or\u00a0missing\u00a0due\u00a0to\u00a0privacy\u00a0concerns.\u00a0In\u00a0these\u00a0cases,\u00a0the\u00a0proposed\u00a0LLM-TSE\u00a0model\u00a0can\u00a0provide\u00a0additional\u00a0text-based\u00a0cues\u00a0to\u00a0augment\u00a0the\u00a0performance\u00a0of\u00a0target\u00a0speaker\u00a0extraction.\n\n## Response to Question 2\n\nFor a fair comparison, our audio-only LLM-TSE is the same as TD-SpeakerBeam, and we have its open-sourced code from https://github.com/BUTSpeechFIT/speakerbeam. We would like to highlight that instead of improving the audio TSE model, our main objective in this work lies in incorporating text-based cues to address the issues of the target speaker extraction system.\n\n[1] The What, Where and How of Auditory-object Perception, Bizley et al., Nature Reviews Neuroscience.\n\n[2] Listen to What You Want: Neural Network-based Universal Sound Selector, Ochiai et al. Interspeech 2020."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5734/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700479690629,
                "cdate": 1700479690629,
                "tmdate": 1700538435016,
                "mdate": 1700538435016,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1PeEOYzC7V",
            "forum": "pyuCmLLluu",
            "replyto": "pyuCmLLluu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5734/Reviewer_K7zB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5734/Reviewer_K7zB"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a new approach for target speaker extraction called LLM-TSE that incorporates natural language input to guide the extraction process. This aims to enhance flexibility and performance compared to existing methods that rely solely on pre-registered voiceprints.\n\nThe user can provide text input describing various cues about the target speaker, such as gender, language, volume, distance, or even transcription snippets. This text input is encoded by a large language model to extract useful semantic information.\n\nThe text embeddings are fused with optional pre-registered voiceprint embeddings and passed to an extractor module to selectively extract the target speaker from the input mixture.\n\nExperiments demonstrate competitive performance using text cues alone, and SOTA results when combined with voiceprints. The method shows particular gains when text provides complementary contextual cues beyond the voiceprint."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Enhanced flexibility: Can utilize text cues alone without needing pre-registered voiceprints. Allows incorporating a wide range of perceptual cues through natural language descriptions.\n\nImproved controllability: Text input can be used to direct the model to extract or remove a target speaker, going beyond just extracting a pre-registered voice.\n\nSOTA performance: Achieves top results on benchmark datasets, outperforming previous target speaker extraction methods.\nRobustness to acoustic mismatches - Integrating contextual cues from text descriptions enhances robustness when enrollment conditions differ from test conditions.\n\nBroadened applicability: Relies less on requiring voiceprints a priori, expanding applicability to more real-world scenarios where pre-registration is unavailable.\n\nNovel paradigm: This signifies an important advancement in guided and adaptable target speaker extraction, laying the groundwork for future cocktail party research.\n\nLeverages large language model: Utilizes powerful pre-trained LLM to effectively interpret semantic concepts from natural language descriptions."
                },
                "weaknesses": {
                    "value": "Lack of psychoacoustic analysis: No analysis related to human auditory perception. For example, see the ICASSP 2023 Deep Noise Suppression challenge. \n\nLimited perceptual cues: Does not yet handle more complex cues like pitch, emotion, timbre, age, topic of conversation, etc. Relies on predefined attributes.\n\nEvaluation on simulated data: Performance needs further evaluation on real-world noisy conditions with multiple concurrent speakers. A real-world test set like used in the ICASSP 2023 Deep Noise Suppression Challenge should be used. \n\nResults (Table 1) are compared with only two other models. It is a stretch to say this is SOTA.\n\nConstrained to simple descriptions: Cannot handle abstract or open-ended perceptual descriptions beyond basic attributes.\n\nComputational complexity: Large language models have high computational costs.\n\nBrittleness of LLMs: LLMs can exhibit biased and unreliable behavior. Robustness needs verification.\n\nSingle speaker extraction: Framework focused on extracting one target speaker, not multiple."
                },
                "questions": {
                    "value": "None"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5734/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699336556795,
            "cdate": 1699336556795,
            "tmdate": 1699636600525,
            "mdate": 1699636600525,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "iaVtXD9TNi",
                "forum": "pyuCmLLluu",
                "replyto": "1PeEOYzC7V",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5734/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5734/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer K7zB"
                    },
                    "comment": {
                        "value": "Thank you for your encouraging review and insightful suggestions. \n\nWe have carefully considered each of your comments and are grateful for the expertise you have brought to this evaluation. We believe they provide valuable directions for future research. We plan to delve deeply into these areas in our continued work to make meaningful contributions to the field. For clarity and completeness, we have added a section in the revised manuscript (**Section A.2**) discussing these future directions. We believe this not only acknowledges the potential of your suggestions but also provides a roadmap for researchers who may wish to build upon our work.\n\nOnce again, we are grateful for your time, expertise, and constructive critique that will undoubtedly help improve our research in the future."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5734/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700526352928,
                "cdate": 1700526352928,
                "tmdate": 1700526352928,
                "mdate": 1700526352928,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "EPSXYYev5i",
            "forum": "pyuCmLLluu",
            "replyto": "pyuCmLLluu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5734/Reviewer_gyG3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5734/Reviewer_gyG3"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed to tackle the target speaker extraction problem by using text-guided approach. Compared to the conventional target speaker extraction that uses the enrolled speech from a specific speaker, the proposed text-guided approach aimed at instructing the model with input text information. A large language model is used to extract semantic cues from text information which is either fused with the audio cues or acting interpedently when used as the prompt for extracting the target speaker\u2019s voice. These input texts were generated by creating some question templates and then expanding it using ChagGPT-3.5-Turbo. The experiment part demonstrates better SI-SDR performance compared to a baseline system."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1, The paper is well written, and the presentation is clear. \n2, The method is innovative which enables the interaction of text prompt and the target speaker extraction."
                },
                "weaknesses": {
                    "value": "1, Lack of references and comparison methods. Text-guided speech extraction is essentially a multi-modal speech extraction method, including text and speech modalities. The motivation is to leverage multiple distinctive clues to extract the target speech sound. While not using exact text and speech, such multi-modality-based speech extraction paper has been proposed before [1][2] which include text, image, and sound modalities. It\u2019s necessary to compare such existing methods in terms of performance rather than only comparing it with the speech-only-guided target speech extraction method. \n\n2, The experiment design is limited in the sense that these text prompt such as gender, language, far-near and loudness, are not typical cues which can be widely used for target speaker extraction. For example, if all the speakers are of the same gender in a conversation, which target speaker will be extracted? If all speakers speak the same language, how to extract the target speaker? For these scenarios, speakers are of different genders or speaking different languages, the target speaker extraction problem is indeed an easy problem. A more practical design scheme is needed for the input text. \n\nref\n[1] Ohishi, \u201cConceptBeam: concept driven target speech extraction\u201d, ACMMM, 2022. \n[2] Li et al., \u201cTarget sound extraction with variable cross-modality cues\u201d, ICASSP, 2023."
                },
                "questions": {
                    "value": "For the audio cue encoder, have you tried a pre-trained speaker embedding module, such as d-vector model used for speaker recognition?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5734/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5734/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5734/Reviewer_gyG3"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5734/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699507300418,
            "cdate": 1699507300418,
            "tmdate": 1699636600435,
            "mdate": 1699636600435,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "uF3nTWT07m",
                "forum": "pyuCmLLluu",
                "replyto": "EPSXYYev5i",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5734/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5734/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer gyG3 (1/2)"
                    },
                    "comment": {
                        "value": "We appreciate your comments and would like to address your concerns with our following responses.\n\n## Response to Weakness 1\n\nThank you for suggesting the two relevant works. We have added them to the manuscript. We fully agree on the value of comparative evaluations. However, it's important to note that our method and those introduced by [Ohishi et al.] and [Li et al.] are designed to **address significantly different tasks**. Consequently, a direct comparison might not provide a meaningful or equitable evaluation of their merits and capabilities.\n\n###  Differences From the Work of [Li et al.]\nFirst of all, as you have mentioned, [Li et al.] have made strides in audio event extraction. However, our research focus diverges significantly from theirs. [Li et al.]'s model, while effective in its domain, does not accommodate the complexities inherent in speaker extraction, which is the main objective of our work. Their model treats speech as a singular, undifferentiated category. In contrast, our model is specifically engineered to identify individual speakers within the broad realm of speech. This fundamental difference in our goals is akin to distinguishing between the barks of a Teddy Bear dog and a Samoyed, as opposed to simply differentiating between human voices and dog barks, as [Li et al.]'s model might. This illustrates the intricate challenges our model is designed to overcome.\n\nOur work specifically addresses unique complications introduced by speech extraction, such as:\n\n1. Determining effective text prompts for speaker extraction.\n2. Ensuring comprehension of text inputs, which can be highly variable.\n3. Navigating the interaction between text and speech cues. Should our model work with speech cues, as is common, or could it function solely on text, a route necessitated by difficulties in acquiring high-quality, registered voices or privacy concerns?\n\nOur model is innovative in its application of text as a tool to tackle these challenges, drawing on human perception of auditory differences. This presents a substantial technical hurdle. Future work could explore additional cues like open-ended questions to enhance our system's performance further. We believe this provides a clearer picture of the distinctiveness and significance of our work.\n\n### Differences From ConceptBeam\n\nConceptBeam is indeed a system designed to operate in situations where multiple speakers are discussing various semantic concepts or topics within a mixed audio setting. The system leverages related images to the discussed topic to aid speech extraction. While the architecture of this system, which incorporates an encoder, a fusion module, and an extractor, does bear some similarity to our design, the operational conditions differ significantly. It's important to highlight that the functionality of ConceptBeam is contingent upon the availability of pair-matched images that correspond with the semantic content of the speech. This condition isn't typically satisfied in many practical applications. This unique prerequisite significantly constrains the adaptability of the ConceptBeam system when compared with methods that employ registered voices or our proposed text-based approach.\n\n### Compare with One-Hot System\n\nWhile a direct comparison with these methods may not be feasible, we drew inspiration from [Li et al.]'s work and related Universal Sound Selector (Ochiai et al.) [3]  to encapsulate attribute-based questions (such as language, gender, loudness, and distance) into a one-hot representation. This was used as a baseline to assess the comprehension capabilities of Language Models (LLMs). However, we must acknowledge the limitations inherent in using one-hot representations:\n- One-hot representations are only capable of expressing attributes with distinct classifications, for instance, language, gender, and loudness. If we want to employ other cues, like transcription snippets, one-hot representations prove insufficient.\n- One-hot representations lack adaptability. LLMs can aid the target speaker extraction system in interpreting user text inputs, thus facilitating the injection of more generic and diverse semantic cues. For example, the input of LLM-TSE can be effortlessly extended to support open-ended questions, such as \"isolate the speaker based on the 3-4 second segment in the mixed speech,\" a task beyond the capacity of one-hot representations."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5734/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700479033765,
                "cdate": 1700479033765,
                "tmdate": 1700538190861,
                "mdate": 1700538190861,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qtDYK5H7SW",
                "forum": "pyuCmLLluu",
                "replyto": "EPSXYYev5i",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5734/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5734/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer gyG3 (2/2)"
                    },
                    "comment": {
                        "value": "| Entry   | Input (Audio) | Input (Text) | Transcription Snippet (50%, 80%, 100%) | Gender | Language | Far-near | Loudness |\n| ------- | ------------- | ------------ | -------------------------------------- | ------ | -------- | -------- | -------- |\n| Universal Sound Selector (Ochiai et al.) [3]       | N             | One-Hot      | No Support                             | 10.54  | 8.88     | 10.25    | 8.96     |\n| LLM-TSE | Y             | N            | 7.30                                   | 10. 17 | 8.87     | 9.77     | 7.75     |\n| LLM-TSE        | N             | Y            | 2.70, 3.97,7.48                        | 10.40  | 9.38     | 10.57    | 8.89     |\n| LLM-TSE        | **Y**             | **Y**            | **7.96,9.81,10.05**                        | **10.87**  | **9.72**     | **10.66**    | **9.41**     |\n\n## Response to Weakness 2\n\nThank you for your insightful question, which allows us to clarify further the scope and potential of our proposed framework for real-world applications. As an initial venture into text-guided target speaker extraction, we aimed for a broad examination of application scenarios and auditory cues. Specifically, we scrutinized absolute attributes (like language and gender), relative attributes (such as loudness and distance), and semantic content (via transcription snippets). These are all extensively utilized by the human auditory system to tackle the cocktail party problem [4, 5].\n\nIn real-world scenarios, when one type of auditory cue is insufficient to discern a sound source, humans naturally shift their focus to other cues. Our LLM-TSE model is motivated by this adaptive behavior. It is designed as a versatile text-based speaker extraction framework, capable of flexibly choosing and smoothly transitioning between auxiliary cues based on different situations.\n\nHowever, we agree with your assertion that refining the text prompts to cover more practical scenarios carries great significance. This indeed calls for substantial research efforts to design comprehensive evaluation benchmarks. As stated in our conclusion, we are actively integrating additional cues such as timbre, emotional state, sentiment scoring, speech velocity (calculated using Whisper timestamps), pitch, and energy (evaluated with Librosa). These improvements aim to enhance the applicability of our framework further. We look forward to sharing our progress in future work.\n\n## Response to Questions\n\nThank you for your valuable question. We have not yet experimented with pre-trained speaker embedding modules, such as d-vector or x-vector, as our audio cue encoder. While neural network-based embeddings like x-vectors or d-vectors are designed and trained specifically for speaker classification tasks and indeed contain speaker-specific information, it remains uncertain whether these representations are optimal for TSE tasks [6]. In our work, we've chosen a more common setting. Our audio cue encoder, which performs speaker embedding extraction, takes an enrollment utterance as input. It typically includes a pooling layer that converts frame-level features into a single vector, mirroring the functionality of the aforementioned embedding extractors. This neural network is trained concurrently with the main network using a shared objective function. The benefit of this approach is that the embeddings are trained explicitly for TSE, thus gathering crucial information specifically for this task. Contrastingly, pre-trained embedding extractors like d/x-vectors are often trained on larger corpora, potentially offering greater robustness. A potential compromise might involve utilizing a pre-trained embedding extractor and fine-tuning it with the TSE task. To our knowledge, this approach has not been explored yet. It would be interesting to explore these pre-trained speaker embeddings, which can potentially offer greater robustness as they have been trained on larger corpora. Due to the time limitation during the rebuttal, we will leave this study as a future work.\n\n[3] Listen to What You Want: Neural Network-based Universal Sound Selector, Ochiai et al. Interspeech 2020.\n\n[4] The What, Where and How of Auditory-object Perception, Bizley et al., Nature Reviews Neuroscience.\n\n[5] Selective Attention in Normal and Impaired Hearing, Shinn-Cunningham et al., Trends in Amplification.\n\n[6] Neural Target Speech Extraction: An overview, Zmolikova et al, IEEE Signal Processing Magazine."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5734/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700479157398,
                "cdate": 1700479157398,
                "tmdate": 1700538457714,
                "mdate": 1700538457714,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]