[
    {
        "title": "CompA: Addressing the Gap in Compositional Reasoning in Audio-Language Models"
    },
    {
        "review": {
            "id": "ka5yXGSqpl",
            "forum": "86NGO8qeWs",
            "replyto": "86NGO8qeWs",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7640/Reviewer_Cw3v"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7640/Reviewer_Cw3v"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose and curate a new benchmark for evaluating the compositional reasoning capability in audio-language models. And further show that to train with curated data and modular contrastive with hard positives and negatives help to improve the compositional reasoning capability."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The curation of CompA-order and CompA-attribute benchmarks are relevant and needed contributions to the community. This helps to provide another perspective to examine the properties and limitations of current SOTA methods for audio-language models.\n- To provide both quantitative analysis of the current limitations in 2.1 and qualitative analysis in the last paragraph of 3.1 both provide solid motivations of this work."
                },
                "weaknesses": {
                    "value": "- It is a bit difficult to get an entire picture of number of different datasets curated and how each of them are curated, a lot of the information are buried in the appendix. Consider in the main paper including a table describing different datasets curated, how many of them for each subset used for training, and evaluation, their source, and methods (leverage ChatGPT, replace the sounding object, swap the orders, etc.), and then refer to the table in the narratives.\n- The data curation is one of the main contribution for this work, however currently a lot of details are left in the appendix. Consider integrating more information such as the source datasets, and some manipulated examples to the main narratives, this can help the reader to understand this work early on.\n- A minor suggestion: The figures and tables are far from the actual text in the appendix, consider reorganize them to make it easier to read.\n- A minor comment: A lot of the texts in the tables and the figures are very small and difficult to read."
                },
                "questions": {
                    "value": "- Regarding the data curated for training, there are CompA-661k, and AudioSet-compA, another 110k audio-text pairs for complex compositional audios? How are these different subsets used in different training scenario? It might worth adding a table to show clearly what combination of datasets, training strategies, whether there is hard negative and modular contrastive utilized, and link them directly to your abbreviation in the result table (CLAP, CompA-CLAP, CLAP-CompA-661k, etc.) Currently it is not easy to associate from the narratives to exactly how each variation in the result tables comes from.\n- In Table 1, what is the difference between CLAP (ours) and CLAP-CompA-661k (ours)?\n- For the common mistakes mentioned in the discussion, is it possible to perform some error analysis? Or confusion matrices to show these more quantitatively?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7640/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7640/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7640/Reviewer_Cw3v"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7640/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698504488533,
            "cdate": 1698504488533,
            "tmdate": 1700319189215,
            "mdate": 1700319189215,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ASVhh09VYW",
                "forum": "86NGO8qeWs",
                "replyto": "ka5yXGSqpl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7640/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7640/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Cw3v (1/2)"
                    },
                    "comment": {
                        "value": "We thank you for your thorough review and constructive feedback. We have tried to address each of your concerns point by point.\n\n## Weaknesses:\n>Q. It is a bit difficult to get an entire picture of number of different datasets curated and how each of them are curated, a lot of the information are buried in the appendix. Consider in the main paper including a table describing different datasets curated, how many of them for each subset used for training, and evaluation, their source, and methods (leverage ChatGPT, replace the sounding object, swap the orders, etc.), and then refer to the table in the narratives.\n\n**Ans:** Thank You for your suggestion. Due to space limitations, despite our best efforts, we could not include such a table in our revised version due to space constraints. However, we have made the following changes to make it clearer:\n- We have added Table 7 in the Appendix of our paper. This Table describes all the data used in our experiments, including their source and the stage of training in which they were used. We also show the Table in the Questions section.\n- We have made several writing changes in all sections to make it clearer which data is being used in which stage of training. The changes can be seen in the revised version of our paper.\n\n>Q. The data curation is one of the main contributions of this work. However, currently, a lot of details are left in the appendix. Consider integrating more information, such as the source datasets and some manipulated examples to the main narratives, this can help the reader to understand this work early on.\n\n**Ans:** Thank You for your suggestion. We have made the following changes to incorporate more details about data curation both in the main paper and appendix:\n1. We expanded the details about the annotation in Section 2 and included more details about the exact annotation process.\n2. In Appendix B3, we provide more details about annotator demographics.\n3. In Appendix B3, we list some rules adhered to during the annotation process.\n\nWe would also like to point out that both our proposed contrastive learning approaches are novel and also serve as the main contributions of the paper. Compa-CLAP serves as the first step toward solving compositional reasoning in audio-language models. Thus, to accommodate everything within the 9-page limit, we made a division by allocating 3 pages to the dataset description and 4 to Compa-CLAP.\n\n>Q. A minor suggestion: The figures and tables are far from the actual text in the appendix, consider reorganize them to make it easier to read.\n\n**Ans:** Thank You for your suggestion. We have reorganized the Figures and Tables in the appendix of the revised version of our paper to make it easier to read.\n\n>Q. A minor comment: A lot of the texts in the tables and the figures are very small and difficult to read.\n\n**Ans:** Thank You for your suggestion. We have revised the text in our figures in the revised version of our paper to make them clearer."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7640/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699914941466,
                "cdate": 1699914941466,
                "tmdate": 1699920323088,
                "mdate": 1699920323088,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dLugfoHq9H",
                "forum": "86NGO8qeWs",
                "replyto": "ka5yXGSqpl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7640/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7640/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Cw3v (2/2)"
                    },
                    "comment": {
                        "value": "### Questions:\n\n>Q. Regarding the data curated for training, there are CompA-661k, and AudioSet-compA, another 110k audio-text pairs for complex compositional audios? How are these different subsets used in different training scenario? It might worth adding a table to show clearly what combination of datasets, training strategies, whether there is hard negative and modular contrastive utilized, and link them directly to your abbreviation in the result table (CLAP, CompA-CLAP, CLAP-CompA-661k, etc.) Currently it is not easy to associate from the narratives to exactly how each variation in the result tables comes from.\n\nWe apologize for the confusion. Here is a table to make things clearer. We have also added this table to our Appendix as Table 7.\n\n| Dataset                     | Source                                  | Experiment                                                                       |\n|--|---|---|\n| Pre-training                |                                         |                                                                                  |\n| CompA-661K                  | Multiple (refer to Table 6 in Appendix) | Vanilla Contrastive Pre-training (Section 3.2)                                   |\n| CompA-AudioSet              | AudioSet Strong [1]                     | Contrastive Pre-training with compositionally aware hard negatives (Section 3.3) |\n| Modular Contrastive Dataset | AudioSet Strong [1]  (but synthetic)                   | Modular Contrastive Learning (Section 3.4)                                       |\n| Evaluation                  |                                         |                                                                                  |\n| CompA-order                 | AudioSet Strong [1]                           | Evaluation Dataset                                                               |\n| CompA-Attribute             | AudioSet Strong [1]                            | Evaluation Dataset                                                               |\n| CompA-Attribute             | WavJourney [3]                          | Evaluation Dataset                                                               |\n\nTable 1: List of various datasets used in our experiments, their sources, and the specific experiments they are used in.\n\nNote: As also mentioned in the paper, no audio used to create our CompA benchmark was included in the training set at any stage. For modular contrastive learning, we only take individual sliced acoustic events from the AudioSet Strong (not the entire audio), to generate our own synthetic data.\n\n>Q. In Table 1, what is the difference between CLAP (ours) and CLAP-CompA-661k (ours)?\n\n**Ans:** We apologize for the mistake and thank you for pointing this out. CLAP (ours) and CLAP-CompA-661k (ours) are the same models, and CLAP-CompA-661k (ours) in Table 1 (right) is a typo. Since we trained our version of CLAP on CompA-661k, we had initially named it CLAP-CompA-661k but later changed it to CLAP (ours).\n\n>Q. For the common mistakes mentioned in the discussion, is it possible to perform some error analysis? Or confusion matrices to show these more quantitatively?\n\n**Ans:** Thank You for the suggestion. We have added both quantitative and qualitative comparisons in Appendix C.4 in Tables 11, 12 and 13. In terms of qualitative analysis, we have highlighted specific acoustic compositions in the CompA-order and CompA-attribute benchmarks where we observed significant improvement in CompA-CLAP compared to the other baselines.\n\n| CompA-order (Acoustic Scene)                | CompA-attribute (Acoustic Scene)                     |  |\n|---|---|----|\n| Frog croaking followed by music playing.    | A man speaks then a woman panics                     | |\n| Man speaking before bell ringing.           | A baby laughs while a woman make sounds              | |\n| A man speaking followed by gunshot          | A child sneezes and an adult laughs                  | |\n| Tiger growling followed by people speaking. | Adult female is speaking and a young child is crying | |\n\nTable 2: Some examples of acoustic compositions in the CompA-order and CompA-attribute benchmarks where we observed significant improvement in CompA-CLAP compared to the other baselines\n\n**References:**\n\n[1] Hershey, Shawn, et al. \"The benefit of temporally-strong labels in audio event classification.\" ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2021.\n\n[2] Gemmeke, Jort F., et al. \"Audio set: An ontology and human-labeled dataset for audio events.\" 2017 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, 2017.\n\n[3] Liu, Xubo, et al. \"Wavjourney: Compositional audio creation with large language models.\" arXiv preprint arXiv:2307.14335 (2023)."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7640/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699916148398,
                "cdate": 1699916148398,
                "tmdate": 1700004414057,
                "mdate": 1700004414057,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UWmOxOLxqK",
                "forum": "86NGO8qeWs",
                "replyto": "ka5yXGSqpl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7640/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7640/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Request to review the rebuttal"
                    },
                    "comment": {
                        "value": "Dear reviewer Cw3v, \n\nThank you for the time you spent reviewing our paper. We have submitted our response to your concerns, including a revised version of our paper. Please let us know your comments and if you have any more concerns. Thank You again!"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7640/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700171868226,
                "cdate": 1700171868226,
                "tmdate": 1700171868226,
                "mdate": 1700171868226,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DT9KSVWgLD",
                "forum": "86NGO8qeWs",
                "replyto": "hLHvgD6OvR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7640/Reviewer_Cw3v"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7640/Reviewer_Cw3v"
                ],
                "content": {
                    "comment": {
                        "value": "Thank the authors for your efforts to address the questions and prepare for open sourcing the benchmark. I am increasing my ratings."
                    }
                },
                "number": 25,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7640/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700319164467,
                "cdate": 1700319164467,
                "tmdate": 1700319164467,
                "mdate": 1700319164467,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Caq3E3G9fQ",
            "forum": "86NGO8qeWs",
            "replyto": "86NGO8qeWs",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7640/Reviewer_Xast"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7640/Reviewer_Xast"
            ],
            "content": {
                "summary": {
                    "value": "Summary: This paper presents a new benchmark, CompA, for measuring two forms of compositionality in audio-language models. These are ordering (sequencing of acoustic events within a clip) and attribute binding (determining which objects perform acoustic sources make which sounds in a clip). They also propose a novel pipeline to train a new contrastive model, CompA-CLAP, by augmenting existing datasets with (a) hard negatives in terms of both forms of compositionality (ordering and attribute binding) and (b) data augmentation by concatenating and overlaying audio clips with known labels, where all text is generated via LLM. Using their proposed benchmark, they show improvements from the novel data pipeline."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "I am posting my full review here, as I believe it easier to discuss both strengths and weaknesses together.\n\n# Overall\n\nOverall, the paper is an ambitious effort to propose a new benchmark measuring a known limitation of existing audio-language models, and also to achieve improved performance on this benchmark. However, reading the paper, it also feels that important details are sometimes lacking both on the benchmarking and the modeling/empirical sides of the paper, in part due to this decision to cover so much ground. It does not appear that the missing information is available in the supplementary material either. The proposed method also performs below random baseline on 2 of the 6 proposed benchmark metrics, but this does not appear to be discussed in the main text. I recommend adding some important clarifying details about the benchmark, clarifying experimental decisions and details, and reducing the \"sales pitch\" framing of the paper. I am enthusiastic about the direction and would encourage the authors to continue to refine the benchmark and their work. However, I feel that major revisions to the paper, and perhaps the benchmark, are currently in order, and that it is probably not ready for acceptance in its current form.\n\n# Major Comments\n\n* The tone of the paper often leans too strongly toward \"sales pitch\" for my taste. I would recommend to tone down this language, and focus on making quantifiable and verifiable claims based on the data and methods in the paper. Just a sampling of the phrases the authors use to dismiss other work, minimize problems, or promote theirs:\n\n  - \"only a word changed\" (a few sentences after the claim \"the audios have the exact same words\")\n  - \"it is trivial for ALMs to perform well on these benchmarks\"\n  - \"CLAP undergoes only minor degradation in retrieval performance\"\n  - \"25% synthetic...but highly curated by expers\" (what is *highly* curated?)\n  - \"a trivial but ineffective solution\"\n  - \"a highly custom prompt\"\n  - \"Our version outperforms ... by significant margins\" (no measures of significance used)\n\n* Since the authors are not only introducing a novel model, but actually proposing their dataset to serve as a benchmark, more justification and detail are needed. For example:\n  - There is no mention in the paper of where the benchmark can be accessed, how it can be used by other works, what format it is provided in, etc. This is essential information for a public benchmark (it is ok to maintain anonymity by describing where the benchmark *will* be available upon acceptance, if the description is sufficiently clear).\n  - The two components of the benchmark consist of only 400 and 200 samples respectively. Is this sufficiently large for a reliable benchmark for the field? Even existing benchmarks in the audio-text domain consist of thousands of examples (AudioSet, AudioCaps, MusicCaps). If 200-400 samples is considered sufficient, please provide empirical evidence. For reference, a Clopper-Pearson confidence interval for even 400 samples would be quite wide: a width of roughly 0.1 for a 95% CI depending on the approximation used, when the success rate is 50%.\n  - It is critical that the paper provide more detail on the annotation process in the main text. The current version provide almost no information, except that four annotators participated. Please describe, for example: number of annotators *per example*; the exact annotation procedure performed. Rater agreement metrics (in supplementary) would be useful but are not required. More detail is also needd regarding the screenshot in Figure 6; it is not clear at all what task the raters are even performing. Much of Section A.3 is in the passive tense, so it is also hard to tell who performed what tasks (\"a manual inspection was done\",\"Wavjourney was employed\"). Many of these details should also appear in the main text.\n  - The abstract states that the benchmark consists of \"a majority of real-world samples\", and elsewhere it is mentioned that 90% of audio are from real-world samples. (1) Can you provide results separated by real-world vs. not real-world? (2) What is the nature of the \"non real-world\" data?\n  - The choice of metrics seems not entirely clear. It would be helpful to either (1) cite works which already use similar metrics, to motivate their used based on wide preexisting adoption in the ALM community, or (2) provide more detailed motivation for why the metrics are appropriate. For example, why is \"group score\" the conjunction of audio score and text score and not, e.g., their harmonic mean in the style of an F-score?\n  - There are no quantifiable measurements of incertainty or variability in the comparisons presented in the paper. The authors say that their results are averaged over 3 random seeds; can they provide the (variance, stddev) over these seeds? What about Clopper-Pearson or other appropriate confidence intervals for the metrics reported? As is, it is impossible to assess the significance of the differences between metrics in Table 2.\n\n* Some important experimental design decisions in the paper are not clearly motivated and not validated with empirical studies. This includes:\n  - Why did the authors only fine-tune CLAP (when they have already shown the CLAP weights to be ineffective) instead of training it from scratch, when the authors apparently have access to the full original pretraining datasets?\n  - Why do the authors fine-tune two separate times, instead of performing a single phase of joint fine-tuning?\n  - Why does the fine-tuning occur (hard negatives (3.3) --> synthetic pairs (3.4)) instead of the reverse ordering?\n\nAll of these decisions could be validated empirically, but I do not see these results in the paper.\n\n* Both examples in Figure 4 both seem to be flawed training example. (1- Left) The negatives both appear to be correct. All of these examples (the true caption, and the negatives) describe three sound events occurring simultaneously. (2 - right) \"Tiger roars amidst thunker\" and \"Tiger roar followed by\" cannot both be positive labels for the same sample - only one can be correct. Both of these issues seem to raise concerns about the quality of the data used here; particularly since the \"highly custom prompt\" described does not appear to be shared in the paper.\n\n* Having the \"CompA-CLAP\" row in Table 2 be bold is misleading. In particular: \"CLAP (ours)\" outperforms ComA-CPA in attribute audio score. Additionally, CompA-CLAP performs below random baseline on audio and group scores in CompA-attribute. This should be highlighted clearly in the paper and abstract (which currently says that \"CompA-CLAP significantly improves over all our baseline models on the CompA benchmark\" but does not mention its below-chance performance on 2 of the 6 benchmark task metrics).\n\n* The paper makes several additional changes to the CLAP model\n\n# Minor Comments\n\n\n* \"Sequence\" and \"attribute binding\" should be defined clearly and early in the paper (ideally before these phrases are used).\n\n* Caption in Figure 2 should describe the metrics being presented in the Figure clearly. The Figure is also missing clear y-axis labels. The phrase \"minor\" is not quantifiable. This figure could also be improved by (a) measures of variability such as error bars or Clopper-Pearson confidence intervals or (b) baselines to show that the CLAP models actually degrade to \"trivial\" performance (is R@1 of 0.42 on AudioCaps val, or R@1 of 0.35 on AudioCaps test, \"trivial\"?).\n\n* A lot could be done to make the paper more self-contained. As-is, it is missing a great deal of relevant background information, which is exacerbated by the deferral of the related work to Section 5.\n\n* \"to assure high quality, we don\u2019t concatenate or overlay random acoustic events but ask an LLM to create unique audio scenes based on the available labels\" - please clarify both the overall procedure here, and how this assures high quality.\n\n* \"where augmenting LAION-audio-630K with 2 million audios from AudioSet improved performance on benchmarks only marginally.\" Please clarify what is meant in this sentence.\n\n* The \"experimental protocol\" sections in 3.2 and 2.4 here do not describe protocols; please clarify or remove these sections. I would recommend a single \"experimental protocol\" section, since these subsections only describe individual components of the overall experimental setup.\n\n# Typos etc.\n\n* Section 1 should reference that prior work is deferred to, and discussed in, Section 5.\n* \"attribute-binding\" vs \"attribute binding\" both used in the paper.\n* \"The group score evaluates of the model performed well for the evaluation instance in the benchmark\" - this is not a meaningful description.\n* 3.4: \"due the the inherenly complex and non-linear nature of audios in the wild\" - please clarify what is meant here, and how nonlinearity relates to the hardness of an example.\n\n# Edit after discussion\n\nI have increased my scores for soundness (2->3) and presentation (2->3) along with my overall rating (3->6) after the author responses, which made significant changes to the paper which both improved it and addressed several of my concerns."
                },
                "weaknesses": {
                    "value": "See above."
                },
                "questions": {
                    "value": "See above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7640/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7640/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7640/Reviewer_Xast"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7640/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698646198949,
            "cdate": 1698646198949,
            "tmdate": 1700269439531,
            "mdate": 1700269439531,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "DjPORLQteY",
                "forum": "86NGO8qeWs",
                "replyto": "Caq3E3G9fQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7640/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7640/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Xast (1/6)"
                    },
                    "comment": {
                        "value": "We thank you for your thorough review and constructive feedback. We have tried to address each of your concerns point by point.\n\n### Major Comments:\n>Q. The tone of the paper often leans too strongly toward \"sales pitch\" for my taste. I would recommend to tone down this language, and focus on making quantifiable and verifiable claims based on the data and methods in the paper. Just a sampling of the phrases the authors use to dismiss other work, minimize problems, or promote theirs:\n\n>- \"only a word changed\" (a few sentences after the claim \"the audios have the exact same words\")\n\n**Ans:**  The two claims refer to two different types of captions.  The captions for the audios with two pairs have the exact same words but in a different order. For audios with three pairs, only a single word that defines the preposition between the events is changed. We have incorporated this change in the revised version of the paper.\n\n>- \"it is trivial for ALMs to perform well on these benchmarks\" \n\n**Ans:** We have replaced this instance of the word \u201ctrivial\u201d with 'easy' to convey the intended meaning. We have made the necessary changes in the revised version of our paper.\n \n>- \"CLAP undergoes only minor degradation in retrieval performance\"\n\n**Ans:**  We have changed the wording of the caption and also added metrics to it in the revised version of our paper (Figure 1). \n\n>-\"25% synthetic...but highly curated by expers\" (what is highly curated?)\n\n**Ans:** The term \"highly curated\" implied a strong level of involvement and careful selection by experts in the generation process and validation process. We have addressed this by using clearer language, specifying that the audios are \"carefully validated by experts who also supervise the generation process.\" This revision provides a more detailed and precise description of the expert's involvement in both the validation and supervision of the synthetic audio generation.\n\n>- \"a trivial but ineffective solution\" \n\n**Ans:**  We have replaced this instance of the word \u2018trivial\u2019 with the word \u201csimple\u201d to convey the intended meaning. We have made the necessary changes in the revised version of our paper.\n\n>- \"a highly custom prompt\"\n\n **Ans:**  The term \"highly\" was used to emphasize the thorough and thoughtful nature of the prompt customization process. However, to provide a clearer and more concise description and tone down the language, we have now simplified it to \"custom prompt,\" which still conveys the idea that the prompt was carefully tailored for the task without explicitly using the word \"highly.\" (Section 2.3)\n\n>- \"Our version outperforms ... by significant margins\" (no measures of significance used) \n\n**Ans:**  Our version of CLAP outperforms [1] on all existing retrieval benchmarks for both text-to-audio and audio-to-text retrieval by 0.15%-4.67%, and CompA-order and CompA-attribute by 11.85%-23.8%. We have incorporated this change in the revised version of the paper.\n\n>Q. Since the authors are not only introducing a novel model, but actually proposing their dataset to serve as a benchmark, more justification and detail are needed. For example:\n\n >- There is no mention in the paper of where the benchmark can be accessed, how it can be used by other works, what format it is provided in, etc. This is essential information for a public benchmark (it is ok to maintain anonymity by describing where the benchmark will be available upon acceptance, if the description is sufficiently clear).\n\n**Ans:** Post acceptance, we plan to release the entire dataset on our GitHub with CC-BY-NC 4.0 License. Together, with the benchmark, we also plan to release all evaluation codes and training codes for benchmark.\n\nTo ensure uniformity in scores, we plan to release a website like the SUPERB Benchmark for spoken language processing tasks (https://superbbenchmark.org/). Researchers will be able upload their models for automatic evaluation and see how their models performs on private and public leaderboards. Since evaluation can be done on a CPU, this would not be a great overhead. The leaderboard will host scores by averaging across 3 runs and report standard deviation across scores.\n\nNote: We are not authors of the SUPERB benchmark.\n\nFor now, we have provided a sample of our dataset in a presentation (.pptx file) in the supplementary materials and have also included an anonymous link to access the complete dataset in the supplementary readme file.\n\n**References:**\n\n[1] Yusong Wu, Ke Chen, Tianyu Zhang, Yuchen Hui, Taylor Berg-Kirkpatrick, and Shlomo Dubnov. Large-scale contrastive language-audio pretraining with feature fusion and keyword-to-caption augmentation. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 1\u20135. IEEE, 2023."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7640/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699923924990,
                "cdate": 1699923924990,
                "tmdate": 1700260416755,
                "mdate": 1700260416755,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SaFLn0rc5c",
                "forum": "86NGO8qeWs",
                "replyto": "Caq3E3G9fQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7640/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7640/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Xast (4/6)"
                    },
                    "comment": {
                        "value": ">Q. Why do the authors fine-tune two separate times, instead of performing a single phase of joint fine-tuning?\n\n**Ans:** We implement a two-stage training strategy to address specific challenges observed in the fine-tuning process as pointed out by Goyal et al. (2022)[1], Wortsman et al. (2022)[2], and Kumar et al. (2022). The first stage involves fine-tuning the model while sampling a single positive and negative per audio. This initial phase focuses on handling hard negatives in a batch. The second stage follows, fine-tuning the model with multiple positive and negative audi- text pairs, addressing the complexity of having multiple positive and hard negative sentences simultaneously. This two-stage learning approach is designed to allow the model to first learn to handle hard negatives individually before tackling the more intricate task of managing multiple positive and hard negative instances concurrently. Empirically, we find 2 stage training to perform better than single stage training.\n\n>Q. Why does the fine-tuning occur (hard negatives (3.3) --> synthetic pairs (3.4)) instead of the reverse ordering?\n\n**Ans:** The core rationale behind performing hard negative training first and modular contrastive training next is a Coarse-to-Fine Contrastive Learning approach. Hard negative training with an entire caption as a positive or a negative in the batch is an easy task and teaches CLAP to distinguish between entirely different contexts and concepts. On the other hand, modular contrastive training, a bit more difficult task, makes CLAP focus on specific compositional relationships of the entire scene, thereby refining its ability to discern subtle distinctions and nuances within similar contexts.\nTable 3 provides results when the order is changed. Empirical results prove that the order matters for training. \n\n|                            | CompA-Order |       |       | CompA-attribute |       |       |\n|----------------------------|-------------|-------|-------|-----------------|-------|-------|\n| Model                      | Text        | Audio | Group | Text            | Audio | Group |\n| CompA-CLAP (ours)          | 40.70       | 35.60 | 33.85 | 44.28           | 22.52 | 15.13 |\n| CompA-CLAP (order changed) | 38.35       | 32.75 | 24.65 | 41.52           | 19.15 | 12.17 |\n\nTable 3: Comparison of CompA-CLAP results with the order of compositionally aware hard negatives and modular contrastive learning changed.\n\n>Q. All of these decisions could be validated empirically, but I do not see these results in the paper.\n>- Both examples in Figure 4 both seem to be flawed training example. (1- Left) The negatives both appear to be correct. All of these examples (the true caption, and the negatives) describe three sound events occurring simultaneously. (2 - right) \"Tiger roars amidst thunker\" and \"Tiger roar followed by\" cannot both be positive labels for the same sample - only one can be correct. Both of these issues seem to raise concerns about the quality of the data used here; particularly since the \"highly custom prompt\" described does not appear to be shared in the paper.\n\n**Ans:** We apologize for any confusion and would like to provide a more detailed clarification regarding Figure 6. The figure illustrates three distinct acoustic events: \"Tiger roar,\" \"Thunder,\" and \"Human conversation.\" The ultimate objective is to combine these acoustic events to construct a coherent acoustic scene, specifically \"Tiger roar followed by human conversation amidst thunder.\" As thunder serves as a background element, \"Tiger roars amidst thunder\" is considered a valid positive scenario. Furthermore, within the acoustic scene, the tiger roar precedes the human conversation, making \"Tiger roar followed by human conversation\" another valid positive sample. You can find more examples in Appendix, Table 9.\n\n>- The paper makes several additional changes to the CLAP model.\n**Ans:** Yes we only make a single change to the original CLAP model proposed by Wu et al. (2023). We replace the RoBERTa encoder with an instruction-tuned Flan-T5-large encoder (Chung et al., 2022) as this has shown to further improve performance. This is detailed in Section 3.2 Methodology paragraph."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7640/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699924141219,
                "cdate": 1699924141219,
                "tmdate": 1699925291989,
                "mdate": 1699925291989,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "u2HuSCs8np",
                "forum": "86NGO8qeWs",
                "replyto": "Caq3E3G9fQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7640/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7640/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Request to review the rebuttal"
                    },
                    "comment": {
                        "value": "Dear reviewer Xast,\n\nThank you for the time you spent reviewing our paper. We have submitted our response to your concerns, including a revised version of our paper. Please let us know your comments and if you have any more concerns. Thank You again!"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7640/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700171899598,
                "cdate": 1700171899598,
                "tmdate": 1700171899598,
                "mdate": 1700171899598,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DGNvLtsnEM",
                "forum": "86NGO8qeWs",
                "replyto": "u2HuSCs8np",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7640/Reviewer_Xast"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7640/Reviewer_Xast"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal follow up [1/2]"
                    },
                    "comment": {
                        "value": "Thank you to the authors for the detailed response and the major edits made to incorporate the suggestions and questions from my initial review. The authors' response addressed many of my concerns (i.e. about tone of the paper, experimental details, empirical and experimental considerations) and provided some critical details which have improved the paper considerably.\n\nI am open to raising my score to the paper due to the improvements from these significant changes. I would also like to highlight some concerns that have not yet been addressed from my original review:\n\n* From original review: \"There is no mention in the paper of where the benchmark can be accessed, how it can be used by other works, what format it is provided in, etc. This is essential information for a public benchmark (it is ok to maintain anonymity by describing where the benchmark will be available upon acceptance, if the description is sufficiently clear).\"\n\n  * ^ this concern has not been addressed. The authors' response to this point above seems to be to an entirely different point, perhaps this is an oversight; if not, I do not understand how it addresses the original concern about how the benchmark will be shared or accessed. Again, I must emphasize how critical this is: if the authors are proposing a *benchmark*, that benchmark needs to be thoughtfully constructed for maximal ease of use, highly accessible, and (ideally) this would be verifiable to reviewers during this process. However, given the need ot preserve anonymity in the review process, I am also willing to accept a reasonable and realistic description of how the authors *plan to* make this benchmark available, provided there is sufficient detail (format, source, plan to ensure availability, documentation, etc.).\n\n* I acknowledge the authors' response to my concerns on dataset size. However, I still don't believe that this addresses the point. I certainly acknowledge the difficulty of constructing a human-annotated audio benchmark, but this does not change the statistical fact that obtaining precise estimates from the benchmark will not be possible, beyond a certain point, due to the sample size. However, I also acknowledge that the *quality* of the benchmark is perhaps even more important than size, beyond a certain point, and the dataset does appear to have high-quality annotations -- which the authors clarifications helped elucidate - thank you for the updated details in Section 2.\n\n  - Since the authors construct 3 random seeds to effectively compensate for the small size of the benchmark, perhaps they can also recommend a procedure to others to follow along these lines (in the supplement). This would ensure uniformity in how future researchers utilize the benchmark and help make results comparable across future works that utilize it.\n\n  - Related to the above point, the authors' response doesn't seem to address the fact that there are two separate benchmarks consisting of 400 and 200 samples each -- not a single benchmark of 600 samples. This matters a lot when it comes to measures of statistical certainty, which usually decay quadratically in variance with the test set size.\n\n  - Related to the authors' point about data scarcity: I do not feel that audio data is so scarce as to justify the small size. There are several massive video datasets (e.g. YT-8M) which include audio, and many large sound effects datasets (including all of LAION CLAP's training datasets, some containing hundreds of thousands of clips) which could be used as additional pools, particularly if single-source audio could be easily pseudolabeled and then summed to form multi-sound clips.\n\n* Real world vs. synthetic: Thank you for the clarification. This table should be included in the paper (I don't see it there?) - as it does seem to indicate that the synthetic data is systematically easier.\n\n* choice of metrics: Thank you for the clarification. Are these clarifications and citations in the paper?\n\n* Variability of performance metrics: These considerably strengthen the results; thank you for adding them to Table 2. Why are the metrics not reported for the remaining models and baselines, and is it possible to add them?\n\n* Fine-tuning details: Again, the author clarifications helped a great deal here. Please make sure these clarifications are included in the paper, as it was not at all obvious to me that this setup was the way to proceed here (even as a researcher familiar with the fine-tuning and foundation modeling literature). Again, \"Table 2\" from the author response should be in the paper (supplement is fine)."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7640/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700257664135,
                "cdate": 1700257664135,
                "tmdate": 1700257664135,
                "mdate": 1700257664135,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cv35XGD2UC",
                "forum": "86NGO8qeWs",
                "replyto": "DGNvLtsnEM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7640/Reviewer_Xast"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7640/Reviewer_Xast"
                ],
                "content": {
                    "title": {
                        "value": "2/2"
                    },
                    "comment": {
                        "value": "* Thank you for the clarification and updates surrounding Figure 6. I am a bit confused about the annotations in the new figure. I see an unambiguous annotation of the form: A_1 = (A_1^1 + A_1^2)*A_1^3. It seems that the text label for this (\"Tiger roar followed by human conversation amidst thunder\") is ambiguous (is the thunder happening amidset the human conversation, or is the thunder happening amidst the human conversation AND the tiger roar). The formal label shows that \"amidst\" modifies both - can the authors offer any insight into how this linguistic ambiguity may affect either the data, or the model learned from it?\n\n* The \"Overview & Background\" sections are a nice addition and have improved the papers self-containedness a great deal.\n\n* The authors' clarification regarding my question \"where augmenting LAION-audio-630K with 2 million audios from AudioSet improved performance on benchmarks only marginally\" would be useful in the paper; I don't see a similar clarification there and would suggest to add it to improve clarity. (Indeed, many of the authors' helpful clarifications to my review don't seem to be in the paper, but I feel that they would improve its clarity and would suggest to add them, as space allows.)\n\nAgain, thanks to the authors for their extensive and thorough response, it did improve the paper considerably."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7640/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700257726906,
                "cdate": 1700257726906,
                "tmdate": 1700257726906,
                "mdate": 1700257726906,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gunbuqYBqQ",
                "forum": "86NGO8qeWs",
                "replyto": "lLkZvM7EiQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7640/Reviewer_Xast"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7640/Reviewer_Xast"
                ],
                "content": {
                    "title": {
                        "value": "Increasing score to 6"
                    },
                    "comment": {
                        "value": "Thank you to the authors for their detailed responses. I am going to increase my score to 6, as my main reservations have been addressed."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7640/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700269335012,
                "cdate": 1700269335012,
                "tmdate": 1700269335012,
                "mdate": 1700269335012,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "u3JlKBAktn",
            "forum": "86NGO8qeWs",
            "replyto": "86NGO8qeWs",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7640/Reviewer_jJm8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7640/Reviewer_jJm8"
            ],
            "content": {
                "summary": {
                    "value": "This paper simultaneously propose new datasets (CompA) and a novel training method (CompA-CLAP) for the task of compositional reasoning in Audio Language Models (ALMs). The dataset contribution is separated between two subsets, CompA-order, which is targeted at the temporal ordering of audio events, and CompA-attribute, which targets attribute reasoning. Additionnally, the authors show that most ALM fail at grasping compositonal reasoning and propose large improvements on this problem with a new method, CompA-CLAP which both introduce a specific loss and a methodology for modular contrastive learning.\n\nOverall, the paper is very well written and the methodology is very soundly presented by both exhibiting a profound issue of ALM, proposing novel datasets and even a large improvement in terms of learning methodology. My major (and almost only) criticism comes from the fact that the concept of \u00ab compositionality \u00bb presented on this paper is rather largely focused on \u00ab temporal \u00bb compositionality (even in the attribute case), and the paper would gain from a broader presentation of different possible types of compositionality. However, I still strongly believe that this paper would be a good addition to ICLR and recommend for acceptance."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "The paper is very rich in new proposals, but all of these remain soundly theoretically grounded. Also, the proposal of new highly curated datasets are also an always-welcome addition for the community."
                },
                "weaknesses": {
                    "value": "As previously stated, my major (and almost only) criticism comes from the fact that the concept of \u00ab compositionality \u00bb presented on this paper is rather largely focused on \u00ab temporal \u00bb compositionality. Although I understand that it is mandatory to make choices as this is still a very young research topic, I think the paper would gain in strength if a more clear definition of various types of compositionality would be presented."
                },
                "questions": {
                    "value": "With regards to generating examples, although I understand that using prompts and GPT can perform a large array of tasks, your temporal task could be quite easily simulated (collating examples together, such as done with the Mixup augmentations). Could you quantify gains and differences with those more straightforward approaches ?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7640/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698662151778,
            "cdate": 1698662151778,
            "tmdate": 1699636928632,
            "mdate": 1699636928632,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "VDYGc7e9Te",
                "forum": "86NGO8qeWs",
                "replyto": "u3JlKBAktn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7640/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7640/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer jJm8"
                    },
                    "comment": {
                        "value": "We appreciate your acknowledgment of CompA's importance, clarity, novelty, effectiveness, and consistent performance gains. We're grateful for your recommendation to accept our paper. We have tried to address each of your concerns point by point.\n\n### Weaknesses:\n>Q. As previously stated, my major (and almost only) criticism comes from the fact that the concept of \u00ab compositionality \u00bb presented on this paper is rather largely focused on \u00ab temporal \u00bb compositionality. Although I understand that it is mandatory to make choices as this is still a very young research topic, I think the paper would gain in strength if a more clear definition of various types of compositionality would be presented.\n\n**Ans:** Thank you for your valuable suggestion. We have added a subsection in the Appendix discussing different types of compositionality observed in audio. The same can be found in Appendix A. \n\n### Questions:\n>Q. With regards to generating examples, although I understand that using prompts and GPT can perform a large array of tasks, your temporal task could be quite easily simulated (collating examples together, such as done with the Mixup augmentations). Could you quantify gains and differences with those more straightforward approaches ?\n\n**Ans:** Thank You for the question. Below, we have compared the results with traditional text augmentation methods, such as switching nouns and verbs using the Spacy library. We find that GPT-generated negatives outperform traditional text augmentation techniques. We hypothesize that this is due to the lower-quality and incoherent negatives generated by traditional NLP methods. We compare negatives from various LLMs and traditional NLP methods in Table 8 in the Appendix. Additionally, we provide code in the Supplementary material.\n\n|                       | CompA-Order |       |       | CompA-attribute |       |       |\n|-----------------------|-------------|-------|-------|-----------------|-------|-------|\n| Model                 | Text        | Audio | Group | Text            | Audio | Group |\n| CompA-CLAP (GPT-4 - ours)     | 40.70       | 35.60 | 33.85 | 44.28           | 22.52 | 15.13 |\n| CompA-CLAP (Traditional NLP) | 35.25       | 30.45 | 17.80 | 39.37           | 17.26 | 12.54 |\n\nTable 1: Comparison of CompA-CLAP with negatives generated by GPT-4 and Traditional NLP methods."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7640/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699914634403,
                "cdate": 1699914634403,
                "tmdate": 1699920265391,
                "mdate": 1699920265391,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "hg271fpt48",
            "forum": "86NGO8qeWs",
            "replyto": "86NGO8qeWs",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7640/Reviewer_x6Uk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7640/Reviewer_x6Uk"
            ],
            "content": {
                "summary": {
                    "value": "Objects in an image have spatial relationships (which occur where in the image) where as events in an audio clip have temporal and attribute relationships (which sound occur when, who made which sound). The authors claim that the current audio-language models (ALMs) poorly model such temporal relationships. They propose a novel benchmark, \"CompA\" to test compositional reasoning (temporal as well as attribute relationships) abilities of ALMs. CompA is comprised of two parts. CompA-order to evaluate temporal relationship and CompA-attribute to evaluate attribute binding. Finally, authors introduce a model (CompA-CLAP) to improve ALM compositional reasoning abilities. CompA-CLAP is a fine-tuned CLAP model, where in the authors include compositionally aware hard-negatives for contrastive learning.These hard-negatives are derived using text-LLMs for the test set and template-based methods for the training set."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* The submission identifies a gap in current audio-language model evaluation i.e., temporal and attribute relationships of audio events, and proposed the novel CompA benchmark to evaluate the same. \n* Introduced a novel CompA-CLAP model which learns audio event relationships through contrastive learning and goal motivated negative sample design.\n* Well furnished details in appendices for reproducibility."
                },
                "weaknesses": {
                    "value": "* The core technical contribution of submission boils down to - \"selecting appropriate negatives for the task at hand\", an established idea in contrastive learning, thus raising the question of novelty in terms of technical contribution.\n* The structuring of manuscript could be better. e.g.,\n  - Transition from section 3.3 to 3.4 is not clear in the first reading.\n  - Every section reads like a mini-paper with it's own background, methodology. May be add a paragraph at the beginning of section to prep the reader of upcoming sections and their differences."
                },
                "questions": {
                    "value": "* Have the author's considered rich transcription approach? Since authors are using strongly annotated data, can the model be trained to output a sequence like ..  <event1_start> <event2_start><event2_end><event1_end> .. here event2 occurs in the midst of event1. This would avoid the process of carefully designing negatives and the model would still learn temporal relationships among audio events. Attributes can also be modeled in a similar fashion. I would imagine this format could be more flexible in terms of attribute modeling. (e.g., personA-talking vs personA-singing and so on.. ). An LLM should have no problem taking such an output sequence and convert it to human-readable format (either implicitly or explicitly). \n* Can the author's clarify if the order of hard negative training and modular contrastive training matter. (section 4: 2nd paragraph)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7640/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7640/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7640/Reviewer_x6Uk"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7640/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699407739824,
            "cdate": 1699407739824,
            "tmdate": 1700702757207,
            "mdate": 1700702757207,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "CLxr6zh6VY",
                "forum": "86NGO8qeWs",
                "replyto": "hg271fpt48",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7640/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7640/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer x6Uk (1/2)"
                    },
                    "comment": {
                        "value": "We thank you for your thorough review and constructive feedback. We have tried to address each of your concerns point by point.\n\n### Weaknesses: \n>Q. The core technical contribution of submission boils down to - \"selecting appropriate negatives for the task at hand\", an established idea in contrastive learning, thus raising the question of novelty in terms of technical contribution.\n\n**Ans:** We agree that hard-negative-based contrastive learning is a well-established idea in contrastive learning. However, as also stated in our paper, we would like to argue that just adding hard negatives is not our primary technical contribution. Our primary contributions are as follows:\n\n1. (Technical) Section 3.3: We modify the formulation of naive contrastive learning with hard negatives such that hard negative captions for a particular audio are ignored by other audios in the batch.\n2. (Technical) Section 3.4: We propose an entirely new algorithm called \u201cmodular contrastive learning.\u201d Modular Contrastive Learning has two primary innovations:\n    - We propose a novel and simple methodology of data generation that overcomes the data scarcity issue of compositional audios: Template-based synthetic creation of audio-caption pairs.\n    - Our proposed method promotes fine-grained composition learning. Each audio in the batch has multiple positives that describe compositional relationships between certain acoustic events in the batch. Each audio in the batch also has multiple negatives for each positive. An example of this can be seen in Figure 4 (Figure 6 in revised version).\n    - We modify the contrastive learning formulation such that each audio in the batch ignores the positives and negatives for all other audio.\n3. (Non-Technical) Section 2: We propose two benchmarks, CompA-order and CompA-attribute, to evaluate compositional reasoning in Audio-Language Models (ALMs). The expert-annotated benchmarks, which are also the first of their kind, are complex to curate and annotate and annotate and represent a significant advancement in an understudied field. By focusing on this niche, our benchmark paves the way for a more nuanced understanding and development of ALMs.\n\n> Q. The structuring of manuscript could be better. e.g.,\n>- Transition from section 3.3 to 3.4 is not clear in the first reading.\n>- Every section reads like a mini-paper with its own background, methodology. May be add a paragraph at the beginning of section to prep the reader of upcoming sections and their differences.\n\n**Ans.:** Thank you for bringing these concerns to our attention! In response, we've incorporated a few introductory lines at the beginning of each section to concisely outline the main theme and facilitate smoother transitions between sections, all while considering space constraints."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7640/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699914002204,
                "cdate": 1699914002204,
                "tmdate": 1699920211863,
                "mdate": 1699920211863,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2IrzV9OTXf",
                "forum": "86NGO8qeWs",
                "replyto": "hg271fpt48",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7640/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7640/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer x6Uk (2/2)"
                    },
                    "comment": {
                        "value": "### Questions:\n>Q. Have the author's considered rich transcription approach? Since authors are using strongly annotated data, can the model be trained to output a sequence like .. <event1_start> <event2_start><event2_end><event1_end> .. here event2 occurs in the midst of event1. This would avoid the process of carefully designing negatives, and the model would still learn temporal relationships among audio events. Attributes can also be modeled in a similar fashion. I would imagine this format could be more flexible in terms of attribute modeling. (e.g., personA-talking vs personA-singing and so on.. ). An LLM should have no problem taking such an output sequence and convert it to human-readable format (either implicitly or explicitly). \n\n**Ans:** Yes, we have. Indeed, this is the exact methodology we employ to generate AudioSet-CompA, a novel dataset contribution of our paper (described in Section 3.3). As the Section specifies, we generate AudioSet-CompA by prompting GPT-4 for time-aligned compositional captions for audios in the AudioSet strong dataset. The AudioSet strong dataset has time-aligned labels, i.e., the start and the end time for each acoustic event in the 10-second audio is specified. An example is:\u201d [('Male speech, man speaking', 2.532, 3.207),('Cluck', 3.087, 3.388),('Generic impact sounds', 3.181, 3.388)]\u201d Thus, we prompt GPT-4 with these time-aligned labels and ask it to generate a coherent caption from this, and indeed, GPT-4 performs accurately at this task. The prompt is provided in Appendix B.5.\n\nFor compositionally aware hard harmful generation, detailed in Section 3.3, we do not employ this as we already have captions available for each audio. Thus, we directly prompt GPT-4 to generate hard negatives from this caption. The prompt is provided in Appendix B.5.\n\n>Q. Can the author's clarify if the order of hard negative training and modular contrastive training matter. (section 4: 2nd paragraph).\n\n**Ans:** The core rationale behind performing hard negative training first and modular contrastive training next is a Coarse-to-Fine Contrastive Learning approach. Hard negative training with an entire caption as a positive or a negative in the batch is an easy task and teaches CLAP to distinguish between entirely different contexts and concepts. On the other hand, modular contrastive training, a bit more difficult task, makes CLAP focus on specific compositional relationships of the entire scene, thereby refining its ability to discern subtle distinctions and nuances within similar contexts.\n\nTable 1 provides results when the order is changed. Empirical results prove that the order matters for training. \n\n|                            | CompA-Order |       |       | CompA-attribute |       |       |\n|----------------------------|-------------|-------|-------|-----------------|-------|-------|\n| Model                      | Text        | Audio | Group | Text            | Audio | Group |\n| CompA-CLAP (ours)          | 40.70       | 35.60 | 33.85 | 44.28           | 22.52 | 15.13 |\n| CompA-CLAP (order changed) | 38.35       | 32.75 | 24.65 | 41.52           | 19.15 | 12.17 |\n\nTable 1: Comparison of CompA-CLAP results with the order of compositionally aware hard negatives and modular contrastive learning changed."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7640/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699914036989,
                "cdate": 1699914036989,
                "tmdate": 1699920233813,
                "mdate": 1699920233813,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FKK3Bk5VOp",
                "forum": "86NGO8qeWs",
                "replyto": "hg271fpt48",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7640/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7640/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Request to review the rebuttal"
                    },
                    "comment": {
                        "value": "Dear reviewer x6Uk, \n\nThank You for your time in reviewing our paper. We have submitted our response to your concerns, including a revised version of our paper. Please let us know your comments and if you have any more concerns. Thank You again!"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7640/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700171808503,
                "cdate": 1700171808503,
                "tmdate": 1700171808503,
                "mdate": 1700171808503,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FWnbDVfLcP",
                "forum": "86NGO8qeWs",
                "replyto": "2IrzV9OTXf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7640/Reviewer_x6Uk"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7640/Reviewer_x6Uk"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks to authors for providing detailed responses. Provided Table1 and associated explanation clarifies the importance of ordering. As for the former question of training the model to output a sequence like  <event1_start> <event2_start><event2_end><event1_end>, my question was not wrt to data generation. It was related to training paradigm itself. If the aim is to make the model learn temporal and attribute relationships, can it be done by making the model output a sequence line  <event1_start> <event2_start><event2_end><event1_end> instead of the approach presented in the paper."
                    }
                },
                "number": 32,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7640/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700678550398,
                "cdate": 1700678550398,
                "tmdate": 1700678550398,
                "mdate": 1700678550398,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mUdWj9z2zr",
                "forum": "86NGO8qeWs",
                "replyto": "hg271fpt48",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7640/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7640/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank You for the question. We are grateful for your time and glad we could address your concerns regarding the paper. We hope that addressing all your concerns can impact our score positively.\n\nWe acknowledge that the suggested rich transcription methodology can be used for training CLAP. However, below, we would like to answer why we did not try the rich transcription methodology in our experiments. However, we assure you that we will add this experiment to the final version of our paper as requested.\n\n1) CLAP-like models (like the one employed in experiments in our paper) are trained to understand natural language. Understanding natural language helps in free-form text-to-audio retrieval and vice-versa, which is used in downstream applications. Outputting such a sequence and training the model on such a sequence using contrastive learning affects natural language understanding of the model and might make the model act as a \u201cbag of words\u201d. Let's take an example:\n\n    Rich Transcription: <human> <bird></bird></human>\n\n    Two compositionally different captions: \n     - A person speaking in a serene outdoor setting, intermittently interrupted by the chirping of birds.\"\n     - Birdsong dominates the soundscape with faint, distant human voices in the background.\n\n    Rich transcriptions like the one provided can translate to compositionally different audios by highlighting the unique combination and interaction of various sound elements within the scene. Thus, this would hinder learning compositionality and specific nuances in audio.\n\n2) Training using rich transcriptions avoids the model learning variabilities in free-form natural language. This hinders real-world downstream applications.\n3) Rich transcriptions, though beneficial in learning temporal ordering, do not provide a straightforward way to capture noun attributes. Natural language captions often describe important nuances in audios through attributes binded with nouns, such as \u201csad\u201d in \u201csad song\u201d or \u201cbaby\u201d in \u201cbaby tiger\u201d. Thus, training with hard negative natural captions makes CLAP understand various audio cues, and training with rich transcriptions might make CLAP overfit to only focusing on temporal ordering. \n4) Training this model would need an extra step in inference time to convert natural language into the rich transcription format since the model was trained to understand only rich transcriptions.\n5) Elaborating on our motivation for Modular Contrastive Learning (Section 3.4) - Real-world audio is often complex and unpredictable with multiple acoustic events. This hinders the model from learning fine-grained attribute-binding and order information with captions or rich transcriptions. Thus, though rich transcriptions can be an alternative to compositionally-aware hard negative training (Section 3.3), Modular Contrastive Learning still solves this significant issue that rich transcription methodology still faces.\n\nWe would also like to mention that our proposed approach for hard-negative-based contrastive learning, which employs compositionally different hard negative captions as negatives, has also been implemented by several prior works in the vision-language space [1,2,3]. All these works have also been cited in our paper. However, our exact formulation for loss calculation is different and innovative and is elaborated in the rebuttal - Response to Reviewer x6Uk (1/2).\n\n[1] Paiss, Roni, et al. \"Teaching clip to count to ten.\" arXiv preprint arXiv:2302.12066 (2023).\n\n[2] Yuksekgonul, Mert, et al. \"When and Why Vision-Language Models Behave like Bags-Of-Words, and What to Do About It?.\" The Eleventh International Conference on Learning Representations. 2022.\n\n[3] Momeni, Liliane, et al. \"Verbs in action: Improving verb understanding in video-language models.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023."
                    },
                    "title": {
                        "value": "Reply to Official Comment by Reviewer x6Uk"
                    }
                },
                "number": 33,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7640/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700682858773,
                "cdate": 1700682858773,
                "tmdate": 1700697984411,
                "mdate": 1700697984411,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tarOCQ1lsS",
                "forum": "86NGO8qeWs",
                "replyto": "mUdWj9z2zr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7640/Reviewer_x6Uk"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7640/Reviewer_x6Uk"
                ],
                "content": {
                    "comment": {
                        "value": "Although there are ways to bind attributes to nouns even in rich transcription, overall, i agree with your arguments (especially 1 & 2). Thank you for additional clarification. Changed my final score."
                    }
                },
                "number": 35,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7640/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700702706777,
                "cdate": 1700702706777,
                "tmdate": 1700702706777,
                "mdate": 1700702706777,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]