[
    {
        "title": "DATS: Difficulty-Aware Task Sampler for Meta-Learning Physics-Informed Neural Networks"
    },
    {
        "review": {
            "id": "tHZ91ps1UU",
            "forum": "EvyYFSxdgB",
            "replyto": "EvyYFSxdgB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4221/Reviewer_LK9J"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4221/Reviewer_LK9J"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a sampling strategy for meta-learning in physics-informed neural networks (PINNs). The key idea is to conduct sampling based on the difficulty. The authors provide an analytical solution to optimize the sampling probability, with a regularization term. Experiments show improved performance over uniform sampling. An ablation study has been presented to help understand the method. The method also shows better performance under the same budget."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Meta-learning for PINN is a promising solution to generalize PINNs so that we do not have to train from scratch for any new PDE.\n2. The proposed sampling strategy is general and can be combined with the existing meta-learning strategy.\n3. Experiments show the proposed method outperforms the uniform sampling."
                },
                "weaknesses": {
                    "value": "1. It is necessary to report training costs in terms of running time. Although the method can improve the sampling efficiency, the sampling strategy itself could be more time-consuming than uniform sampling. It is unclear whether the method can bring actual benefits in training time compared with the baseline.\n2. The method is only tested on three benchmarks. Results on more benchmarks are encouraged to test the generalizability of the proposed method, such as Heat, Wave, and Advection."
                },
                "questions": {
                    "value": "What is the actual training time improvement?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4221/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698557972877,
            "cdate": 1698557972877,
            "tmdate": 1699636389136,
            "mdate": 1699636389136,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jkNAH0OHBW",
                "forum": "EvyYFSxdgB",
                "replyto": "tHZ91ps1UU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4221/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4221/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer LK9J"
                    },
                    "comment": {
                        "value": "Dear Reviewer LK9J,\n\nThank you for your constructive feedback on our manuscript. We are pleased to address the concerns you have raised regarding our proposed sampling strategy for meta-learning in physics-informed neural networks (PINNs).\n\n1. **Training Cost Reporting and Actual Training Time Improvement**:\nWe acknowledge the importance of evaluating the computational efficiency of our method alongside its performance benefits. That is the reason we included the  subsection \"computational cost\" in section 5.3. However, it is important for us to clarify that the motivation for DATS is not to improve the sampling \u201cefficiency\u201d in terms of computational cost of the meta-learning. Instead, the goal is to improve the sampling \u201cefficacy\u201d for meta-learning PINNs to improve the accuracy of the PINNs both for PDE configurations included in the meta-learning as well as for those outside the meta-training distributions (improving generalization).  Our findings showed that, when using the same budget for residual points, DATS improved the PINNs\u2019 performance significantly with only a modest overhead to determine task difficulty ( a 3-8% increase in the overall training time). More importantly, to achieve the same PINN performance obtained by uniform sampling, DATS can reduce the number of residual points needed to a range from less than 1% to 40% of what is needed using uniform sampling, effectively reducing computational burden. \n\n2. **Benchmarks**:\nWhile we acknowledge the potential benefits of expanding the dataset, we would like to  clarify that we have conducted experiments on **four**benchmark datasets including \"Burgers\", \"Convection\", \"Reaction Diffusion\" and \"Helmholtz\" that are commonly used in PINN literature: this selection ofPDEs is on par with those published [1,2]. In addition, we have added another challenging PDE, the 3D Navier-Stokes equation. Please see details in bullet one of Overall Response for the added results and disucssion.\n\n1:[Huang, Xiang, et al. \"Meta-auto-decoder for solving parametric partial differential equations.\" Advances in Neural Information Processing Systems 35 (2022): 23426-23438.]\n\n2:[Kim, Jungeun, et al. \"DPM: A novel training method for physics-informed neural networks in extrapolation.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 35. No. 9. 2021.]"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4221/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700673971253,
                "cdate": 1700673971253,
                "tmdate": 1700673988737,
                "mdate": 1700673988737,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "QWjTDSSJYH",
            "forum": "EvyYFSxdgB",
            "replyto": "EvyYFSxdgB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4221/Reviewer_USEY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4221/Reviewer_USEY"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a novel approach to Physics-Informed Neural Networks (PINNs) with a focus on unsupervised learning for parameterized Partial Differential Equations (PDEs). The authors propose optimizing the probability distribution of task samples to minimize error during meta-validation. They transform the problem theoretically into a discretized form suitable for optimization and introduce two optimization strategies: optimizing the residual points sampled and the loss weight across different tasks. The paper presents experiments on several equations, showcasing improvements over baseline methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Clarity and Presentation: The paper is well-written, making the novel method and its distinctions from baselines clear. The method is explained in a step-by-step manner, which is easy to follow.\n2. Innovative Approach: The meta-learning method for PINNs is a fresh take on solving parameterized PDEs, and it is well grounded in theory.\n3. Comprehensive Experiments: The authors conduct experiments on various equations, providing a thorough evaluation of their method.\n4. Effective Visualization: The results are presented in a clear manner, with visualizations that aid in understanding the improvements made."
                },
                "weaknesses": {
                    "value": "1. Mischaracterization of Data Loss: In Section 3, the paper inaccurately defines data loss as the loss of boundary conditions. This is a mischaracterization, especially for Neumann or Robin boundary conditions, which only penalize normal derivatives rather than resulting in data loss.\n\n2. Formatting and Clarity of Figures: Some figures in the paper could be improved for better clarity and understanding. For instance:\n(1) Figures 3 and 4 would benefit from added grids and key values annotated directly on the figures.\n(2) The scales in Figure 5 are too small to read, making it difficult to interpret the results.\nThe authors should review and adjust these figures to enhance clarity.\n\n3. Lack of Comparison with State-of-the-Art: The paper could be strengthened by including a comparison with state-of-the-art methods in the field, providing a clearer context of the method's performance.\n\n4. Limited Discussion on Limitations: The paper does not adequately discuss the limitations of the proposed method, which is crucial for readers to understand the potential challenges and boundaries of the approach.\n\n5. Potential Overfitting: Given the nature of the meta-learning approach, there could be a risk of overfitting to the tasks at hand. The paper could benefit from a discussion on how this risk is mitigated or how the method performs under such circumstances."
                },
                "questions": {
                    "value": "1. Clarify Mischaracterizations: The authors should revisit Section 3 to correct the mischaracterization of data loss and provide a more accurate description.\n\n2. Improve Figure Formatting: Enhancements should be made to the figures to improve readability and clarity, as this will aid in better conveying the results and contributions of the paper.\n\n3. Include Comparison with State-of-the-Art: Adding comparisons with leading methods in the field will provide a clearer benchmark of the proposed method's performance.\n\n4. Discuss Limitations and Potential Overfitting: A section discussing the limitations of the method and addressing potential overfitting concerns would add depth to the paper and provide a more balanced view of the approach.\n\nWith these enhancements, the paper would offer a more comprehensive and clear presentation of the proposed method, its strengths, and its potential areas for improvement."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "Not apply."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4221/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698726006035,
            "cdate": 1698726006035,
            "tmdate": 1699636389055,
            "mdate": 1699636389055,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "dcBF58jVsx",
                "forum": "EvyYFSxdgB",
                "replyto": "QWjTDSSJYH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4221/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4221/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer USEY (Part 1)"
                    },
                    "comment": {
                        "value": "Dear Reviewer USEY,\n\nThank you for your thorough review of our manuscript and for recognizing the strengths of our work. We have addressed each of the points raised in your review to enhance the quality and clarity of our manuscript.\n\n1.**Clarifications and Corrections in Section 3**: \n\nThanks for pointing this out. We realize that the term \"data loss\" in the context of Physics-Informed Neural Networks (PINNs) was not precisely used and may have led to confusion, particularly in relation to boundary conditions.\nIn PINNs, \"data loss\" traditionally refers to the component of the loss function that quantifies the error in the neural network's predictions against known solutions or measurements. Specifically, for PINNs, data loss encompasses the error in satisfying initial conditions, boundary conditions (including Dirichlet, Neumann, or Robin), and any interior constraints.\nFor Dirichlet conditions, the data loss is indeed a direct measure of the discrepancy between the network outputs and the true boundary values. However, for Neumann or Robin conditions, which involve derivatives, the term \"data loss\" should more accurately reflect the error in satisfying the prescribed fluxes or a combination of flux and function values on the boundaries. We acknowledge that our initial description may have oversimplified this aspect by not distinguishing between the types of boundary conditions. In the revised manuscript we removed the term \"data loss\" and replaced it with \"initial and boundary condition loss\". Eqs (2-3) are revised accordingly to include different types of boundary conditions.\n\n2.**Improvements in Figure Formatting**:\n\nWe have revised Figures 3 and 4 to include grids, key values and have enlarged the fonts and scales. Furthermore, tables of results (Tables C4, C5, and C6) are added in appendix C to aid with understanding of these figures. Figure 5 is updated with enlarged fonts and scales and all figures in the appendix will be updated accordingly for the camera ready version."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4221/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700673898997,
                "cdate": 1700673898997,
                "tmdate": 1700673898997,
                "mdate": 1700673898997,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "NrmwvKMOky",
            "forum": "EvyYFSxdgB",
            "replyto": "EvyYFSxdgB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4221/Reviewer_Z9PQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4221/Reviewer_Z9PQ"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a difficulty-aware task sampler (DATS) for meta-learning of PINNs. The model takes the variance of the difficulty of solving different PDEs into consideration by optimizing the sampling probability of meta-learning. An analytic approximation of the relationship of meta-model and sampling probability is provided to enhance learning. DATS is shown to improve the overall performance of meta-learning PINNs."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1) Quality: The performance of the approach seems good in the empirical part of the paper and the ablation study is detailed.\n2) Originality: The proposed two strategies to utilize $p^*$ are interesting and the comparison and analysis are comprehensive."
                },
                "weaknesses": {
                    "value": "The main weakness is the clarity and correctness in both methodology and experiments\n\n1) Lack of intuitive explanation for the math derivations in section 4.1, making it hard to follow.  For example, the $w_i=  \\langle g_{tr}, g_{val} \\rangle$ in Eq.9 may be intuitively interpreted as \"assign the weight according to gradient similarity between train and valid\", I guess.\n\n2) There are some misleading typos and unexplained assumptions in section 4.1.\n* The LHS of Eq(7) should be $l_{\\text {val }, \\lambda}\\left(\\theta^{t+1}\\right)$, but not $l_{\\text {val }, \\lambda}\\left(\\theta^*\\right)$ written in Line 5 of this paragraph.\n* And similarly, the LHS of Eq(8) should be $p^{t+1}(\\lambda)$ not $p^{*}(\\lambda)$.\n* In Line4-5 of this paragraph, the gradient descent of training loss is defined as $\\theta^{t+1} = \\theta^t-\\eta \\int_\\lambda p(\\lambda) \\nabla_\\theta l_{t r, \\lambda}\\left(\\theta^t\\right) d \\lambda$, which assumes that training loss is defined as in Eq(11), the so-called DATS-w. But  DATS-rp loss in Eq(12) does not follow this assumption, thus the analysis is invalid for it. \n* The authors assume that the proposed iterative scheme for $p^*,\\theta^*$ converges and that adding regularization further stabilizes the convergence, without explanations. Intuitively, the first-order Taylor expansion is used in Eq(7), thus the step size $\\theta$ should be small enough to stabilize it. Additionally, the discrete approximations may also introduce errors.\n\n3) The experimental performance of sampling strategies (Section 5.2 and Appendix C) is not reported clearly. There are only figures in the main text. Fig.3,4,6 are difficult to extract information from since the lines and shades overlap heavily.  And since there are no digital numbers available, it is hard to compare the results. For example, in Fig.C.12, it seems Self-pace has the same performance as DATs."
                },
                "questions": {
                    "value": "1) In Fig.3,4,6,  Why do the uniform and self-paced baselines only have higher and lower bounds of errors, not curves at different residual budgets?\n\n2) How does the DATS compare with the Reduced Basis Method, e.g., [1]?\n\n3) Does DATS also perform well on more difficult PDEs, such as with discontinuity and high-dimension? Examples includes the shock tube of compressible Euler's equation and  2d/3d N-S equation.\n\nRefs:\n[1] Chen, Yanlai, and Shawn Koohy. \"GPT-PINN: Generative Pre-Trained Physics-Informed Neural Networks toward non-intrusive Meta-learning of parametric PDEs.\" Finite Elements in Analysis and Design 228 (2024): 104047."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4221/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4221/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4221/Reviewer_Z9PQ"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4221/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698830994914,
            "cdate": 1698830994914,
            "tmdate": 1700742554192,
            "mdate": 1700742554192,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0RuKPcVEZW",
                "forum": "EvyYFSxdgB",
                "replyto": "NrmwvKMOky",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4221/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4221/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Z9PQ (Part1)"
                    },
                    "comment": {
                        "value": "Dear Reviewer Z9PQ,\n\nThank you for your critical assessment and valuable comments on our manuscript. We have revised the manuscript, taking your feedback into careful consideration, and would like to present the following detailed clarifications and justifications:\n\n**Major clarifications and revisions**\n\nWe have made major revisions and clarifications on the following points and included details in the Overall Response to all reviewers. Please refer to the details therein.\n\n   - Additional results on a complex PDE (3D N-S) as suggested.   \n   - Additional results on a comparison with the Reduced Basis Method as suggested. \n   - Clarification on the validity of DATA-rp derivation: \n\n**Additional clarifications and revisions**\n\n1. *Intuitive Explanation for Mathematical Derivations in Section 4.1* \n\nWe have attempted to add additional explanations to aid the understanding of the mathematical framework throughout (in blue fonts). For instance, after Eq (9), we have added the following: *Intuitively, w_{i} measures the gradient similarity between the training loss of task \\lambda_{i} and the validation loss across all tasks. This results in a higher w_{i} to a PDE configuration \\lambda_{i} that is most beneficial for reducing the validation loss across all tasks.*\n\n2. *The use of iterative optimization*:\n   - Indeed, the optimization objective in Eq (6) describes the overall objective for our formulated problem, while starting with Eqs (7) and onward, we have moved into the derivation of iterative solutions to Eq (6) and thus the introduction of superscript t notating the iterations. We have added this clarification leading to the derivation of Eq (7), and revised all relevant notations in the following sections. \n    - We did not discuss the convergence about the iterative optimization approach used, because gradient-based iterative optimization underlies the training of almost all neural networks including bi-level nested optimization commonly used in meta-learning \u2013 from this aspect, our proposed iterative schedule for solving p and \\theta did not introduce additional assumptions that deserve special attention different from those established for iterative numerical optimization. \n    - However, we do acknowledge that the first-order Taylor expansion used in Eq (7) is a key piece in our derivation that sets us apart from a typical iterative solution to the objectives in Eq (6), which simplifies the optimization procedure but does introduce an approximation that deserves further expansion in the manuscript . In fact, the use of a single step of gradient descent in the inner-level optimization (of theta_t) to simplify the outer-level optimization (of lambda) via first-order Talor expansion is not an uncommon solution to nested optimization: for instance, it also underpins the first-order MAML (model agnostic meta-learning) algorithms such as REPTILE [1], which already showed that this first-order approximation has the same leading-order terms as the original optimization. We have added this connection and citation to the text following Eq (8). \n      - Indeed for the Taylor Expansion in Eq (7) to hold, the step size in theta need to be small; this step size is mainly controlled by the hyperparameter eta (while the integral term controls the direction).  This fact was also acknowledged in Reptile [1]. Experimentally,  we have conducted extensive empirical evaluations to determine appropriate ranges for \u03b7, ensuring that within these ranges, our iterative scheme consistently converges. The main body of experiments are conducted using \u03b7 = 1e-3 based on these observations.\n       - The incorporation of the Kullback\u2013Leibler (KL) divergence as a regularization term stabilizes the task probability assignments, ensuring that the iterative updates do not lead to erratic changes in p(\\lambda). The hyperparameter \\beta modulates the strength of this regularization. This balance is important for the stability of the convergence, as it prevents the optimization from becoming overly aggressive or conservative. We have conducted extensive experiments for tuning this hyperparameter. These evaluations are included in Table.1 as part of the ablation study.\n\n[1] Alex Nichol and Joshua Achiam and John Schulman, On First-Order Meta-Learning Algorithms"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4221/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700673738233,
                "cdate": 1700673738233,
                "tmdate": 1700673738233,
                "mdate": 1700673738233,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UP8fUWTZzn",
                "forum": "EvyYFSxdgB",
                "replyto": "YjJRgE8sH1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4221/Reviewer_Z9PQ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4221/Reviewer_Z9PQ"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the replies and the additional experiments, I decided to raise my score to 6."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4221/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700742530277,
                "cdate": 1700742530277,
                "tmdate": 1700742530277,
                "mdate": 1700742530277,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]