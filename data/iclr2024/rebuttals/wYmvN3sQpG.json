[
    {
        "title": "Benign Oscillation of Stochastic Gradient Descent with Large Learning Rate"
    },
    {
        "review": {
            "id": "rRN8CU8W4r",
            "forum": "wYmvN3sQpG",
            "replyto": "wYmvN3sQpG",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5736/Reviewer_yxwb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5736/Reviewer_yxwb"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies feature learning when the learning rate is large, showing that when there is a strong oscillation in the training, only data points with weak signals are learned"
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "I think the message and the insight is sufficiently novel and relevant -- how large learning helps \"selecting\" a subset of data points to be learned. \n\nThe data generation model of strong and weak patches is also quite interesting and can serve as a good starting point for future works\n\nAlso, the signal to noise ratio of the data and gradient plays an important role in the proof and discussion and, therefore, I think the result does catch some level of essence of SGD training in neural networks. Although, on this point, I think the authors should compare with the study of how signal-to-noise ratio affects the learning at a large learning rate in (1) https://arxiv.org/abs/2107.11774 and (2) https://arxiv.org/abs/2303.13093"
                },
                "weaknesses": {
                    "value": "In my opinion, what prevents me from recommending this paper is that the problem setting feels too artificial. The following are the specific problems in my opinion\n\n1. The problem setting. First of all, what is a ReLU^2 activation function? I have never seen this. Is ReLU more essential to the proof or is the quadratic effect more essential to the proof? Does the result hold for ReLU? Does the result hold for the quadratic activation? This is unanswered in the paper. To me, this activation neither feels theoretically appealing nor practically relevant\n\n2. Problem setting 2. The second layer is not trained. Essentially, we understand that training with one layer is very different from training multiple layers simultaneously. For example, see https://arxiv.org/abs/2205.11787\n\n3. Problem setting 3. The model architecture -- the authors refer to the model as a \"CNN,\" which just does not feel right to me. I doubt if the majority of readers would agree that this model is indeed a CNN. \n\nIn summary, the point is the same, the problem setting is not sufficiently simple to be considered theoretically essential, nor is it sufficiently realistic. This significnatly limits the relevance and significance of the results obtained"
                },
                "questions": {
                    "value": "See the weakness section"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5736/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698312174019,
            "cdate": 1698312174019,
            "tmdate": 1699636600900,
            "mdate": 1699636600900,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1fln2953N6",
                "forum": "wYmvN3sQpG",
                "replyto": "rRN8CU8W4r",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5736/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5736/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer yxwb (Part 1)"
                    },
                    "comment": {
                        "value": "Thanks very much for your review! We now address all your concerns and questions for our work in the following.\n\n**Q1: What is a $\\rm ReLU^2$ activation function? Is $\\rm ReLU$ more essential to the proof or is the quadratic effect more essential to the proof? Does the result hold for $\\rm ReLU$? Does the result hold for the quadratic activation? This is unanswered in the paper. To me, this activation neither feels theoretically appealing nor practically relevant.**\n\n**A1:** First, we clarify that the $\\rm ReLU^q$-style activation (or ReLU with a polynomial smoothing) is actually widely adopted in existing works on deep learning theory, see e.g., [1, 2, 3, 4, 5, 6, 7] and the references therein. We followed the setup of this line of works for our study on large learning rate NN training. Therefore, we respectfully disagree with your comment that this activation is not theoretically appealing.\n\nMoreover, we need the $\\rm ReLU^2$ activation to make the loss Hessian vary locally throughout the training process, which is important to enable the benign oscillation phenonemon we aim to study. This is consistent with the well-known edge-of-stability work [8, 9, 10], where they show that SGD with different learning rates tend to find the solutions with different Hessians. In contrast, with $\\rm ReLU$ activation, we get only a locally quadratic objective and the loss sharpness will remain locally unchanged for our model, then the SGD/GD may lack the ability to explore unreached region but will be stuck at a local region (i.e., it will either smoothly converge or oscillate locally).\n\nFinally, our analysis can still work if we use a pure quadratic activation and the key proof techniques will be similar. The major reason of using ReLU activations is to be aligned with the existing line of related research.\n\n**References:** \n\n[1] Zhong, Kai, et al. \"Recovery Guarantees for One-hidden-layer Neural Networks.\" International Conference on Machine Learning. PMLR, 2017.\n\n[2] Lyu, Kaifeng, and Jian Li. \"Gradient Descent Maximizes the Margin of Homogeneous Neural Networks.\" International Conference on Learning Representations, 2019.\n\n[3] Fan, Fenglei, Jinjun Xiong, and Ge Wang. \"Universal Approximation with Quadratic Deep Networks.\" Neural Networks 124 (2020): 383-392.\n\n[4] Cao, Yuan, et al. \"Benign Overfitting in Two-layer Convolutional Neural Networks.\" Advances in neural information processing systems 35, 2022.\n\n[5] Shen, Ruoqi, S\u00e9bastien Bubeck, and Suriya Gunasekar. \"Data Augmentation as Feature Manipulation.\" In International conference on machine learning, pp. 19773-19808. PMLR, 2022.\n\n[6] Allen-Zhu, Zeyuan, and Yuanzhi Li. \"Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning.\" The Eleventh International Conference on Learning Representations, 2023.\n\n[7] Zou, Difan, et al. \"Understanding the Generalization of Adam in Learning Neural Networks with Proper Regularization.\" The Eleventh International Conference on Learning Representations, 2023.\n\n[8] Cohen, Jeremy, et al. \"Gradient Descent on Neural Networks Typically Occurs at the Edge of Stability.\" International Conference on Learning Representations, 2020.\n\n[9] Arora, Sanjeev, Zhiyuan Li, and Abhishek Panigrahi. \"Understanding Gradient Descent on the Edge of Stability in Deep Learning.\" International Conference on Machine Learning. PMLR, 2022.\n\n[10] Wang, Zixuan, Zhouzi Li, and Jian Li. \"Analyzing Sharpness Along GD Trajectory: Progressive Sharpening and Edge of Stability.\" Advances in Neural Information Processing Systems 35, 2022."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5736/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700072236334,
                "cdate": 1700072236334,
                "tmdate": 1700078697425,
                "mdate": 1700078697425,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LYjxdBchHn",
                "forum": "wYmvN3sQpG",
                "replyto": "rRN8CU8W4r",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5736/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5736/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer yxwb (Part 2)"
                    },
                    "comment": {
                        "value": "**Q2: Why only train one layer? Essentially, we understand that training with one layer is very different from training multiple layers simultaneously. For example, see [1].**\n\n**A2**: We only train the hidden layer because for our model since this procedure is *sufficient* to characterize the key difference between the dynamics and the generalization properties between small learning rate training and large learning rate training, which is the main focus of our paper.\n\nMoreover, we point out that the result for one-layer setup in [1] you mentioned does not apply to our problem, since the so called training with one layer setup in [1] considers the neural tangent kernel (NTK) regime with linear models. In contrast, our model focuses on the feature learning perspective with quadratic activation (nonlinear), so that we are able to show the following three types of training dynamics:\n\n - when the learning rate $\\eta$ is small, the dynamic converges smoothly to a nearby local minimum;\n - when the learning rate $\\eta$ is reasonably larger, the oscillation happens during which the model gradually learns all useful patterns from data by learning the weak signal;\n - when the learning rate $\\eta$ is too large, despite that we have not directly shown in our paper, it is obvious to see the dynamic will diverge.\n\nThen this three phase dynamic is quite similar to the non-linear dynamic shown in [1] for the two layer neural quadratic model case. Intuitively, the reason is that the $\\rm ReLU^2$ activation we adopted has already provided the source of nonlinearity we want, which gives a locally varying loss Hessian and enables the benign oscillation under reasonably large learning rate training. With this in hand, we have been able to show the benefits of large learning rate training and the mechanism behind it\n\nIt is indeed interesting to consider training a multiple layer neural network. But that complicates the analysis and may not change the key message we would like to convey. So we consider only training the hidden layer in this work. We will add the related work [1] and more discussions on this in the revision.\n\n**References:** \n\n[1] Zhu, L., Liu, C., Radhakrishnan, A., & Belkin, M. (2022). Quadratic Models for Understanding Neural Network Dynamics. arXiv preprint arXiv:2205.11787.\n\n**Q3: The authors refer to the model as a \"CNN\", which just does not feel right to me. I doubt if the majority of readers would agree that this model is indeed a CNN.**\n\n**A3:** This specific style of model has been widely adopted and named as \"CNN\" in a sequence of pivotal works, to name a few, see [1, 2, 3, 4, 5, 6, 7]. This specific model is a surrogate of the real-world CNN architecture and is convenient for theoretical study. More specifically, each $\\mathbf w\\_{j,r}$ in the hidden layer $\\sigma(\\langle\\mathbf{w}\\_{j,r},\\mathbf{x}\\rangle)$ can be regarded as a filter of a CNN, and different patches of the data share the same filter because of the \"weight sharing\" principle of the CNN architecture.\n\n**References:**\n\n[1] Du, Simon S., Jason D. Lee, and Yuandong Tian. \"When is a Convolutional Filter Easy to Learn?\" International Conference on Learning Representations, 2018.\n\n[2] Du, Simon, et al. \"Gradient Descent Learns One-hidden-layer CNN: Don\u2019t Be Afraid of Spurious Local Minima.\" International Conference on Machine Learning. PMLR, 2018.\n\n[3] Cao, Yuan, and Quanquan Gu. \"Tight Sample Complexity of Learning One-hidden-layer Convolutional Neural Networks.\" Advances in Neural Information Processing Systems 32, 2019.\n\n[4] Shen, Ruoqi, S\u00e9bastien Bubeck, and Suriya Gunasekar. \"Data Augmentation as Feature Manipulation.\" In International conference on machine learning, pp. 19773-19808. PMLR, 2022.\n\n[5] Allen-Zhu, Zeyuan, and Yuanzhi Li. \"Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning.\" The Eleventh International Conference on Learning Representations, 2023. \n\n[6] Zou, Difan, et al. \"Understanding the Generalization of Adam in Learning Neural Networks with Proper Regularization.\" The Eleventh International Conference on Learning Representations, 2023.\n\n[7] Cao, Yuan, et al. \"Benign Overfitting in Two-layer Convolutional Neural Networks.\" Advances in neural information processing systems 35, 2022."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5736/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700072341880,
                "cdate": 1700072341880,
                "tmdate": 1700072359744,
                "mdate": 1700072359744,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kWZUeXfHMO",
                "forum": "wYmvN3sQpG",
                "replyto": "rRN8CU8W4r",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5736/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5736/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer yxwb"
                    },
                    "comment": {
                        "value": "Dear reviewer yxwb, we thank you again for your time reviewing our paper! We have dealt with all your concerns regarding our problem setup including the model architecture and the training protocol, and we have added new references to our draft according to **Q2** and **Q4** (see Appendix A). As the end of author-review discussion period is approaching, we would appreciate it if you can kindly let us know whether your previous concerns have been fully addressed. We are happy to answer any further questions. As you also mentioned that our work conveys sufficiently novel message and insight for large learning rate NN training, if you think our work is worthy of a higher score, we would be immensely grateful!"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5736/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700510587566,
                "cdate": 1700510587566,
                "tmdate": 1700510587566,
                "mdate": 1700510587566,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "AUFm9ndTOd",
                "forum": "wYmvN3sQpG",
                "replyto": "kWZUeXfHMO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5736/Reviewer_yxwb"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5736/Reviewer_yxwb"
                ],
                "content": {
                    "title": {
                        "value": "Review part 2"
                    },
                    "comment": {
                        "value": "Thanks for the detailed reply. \n\nWhile I feel more positive about the work, here is my main concern. It seems to me that the most essential component in the proof is that the activation is quadratic (and not that it is ReLU). The authors (perhaps) mention that the proof can be extended to the case of ReLU^q type activations, but it appears to me that the extension only works if $q$ is strictly larger than 1. Namely, the theory only applies to superlinear activation functions -- which almost never works in practice. More importantly, it **cannot** be applied to linear or sublinear types of activations. \n\nIn summary, the paper shows that benign oscillation exists in activations that practically fails and there is no benign oscillation in activations that practically works, which contradicts real-life observations. Thus, let me submit one additional question here: can the authors prove that for $q\\leq 1$, ReLU^q also exhibits benign oscillation?"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5736/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700706752704,
                "cdate": 1700706752704,
                "tmdate": 1700706752704,
                "mdate": 1700706752704,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "s2GvJfIPf7",
                "forum": "wYmvN3sQpG",
                "replyto": "rRN8CU8W4r",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5736/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5736/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response 2 to Reviewer yxwb (Part 2)"
                    },
                    "comment": {
                        "value": "- (Large learning rate case) From the update (1) regarding the strong signal $\\mathbf{u}$, by using the same argument as in the illustrative deduction in Section 3, we can show that the oscillation of $f$ around $1$ will accumulate the loss derivatives under large learning rate, that is, $\\sum_{t}{\\ell^{\\prime}}^{(t)}$ is increasing (see the first conclusion of Lemma 17 and its proof). Applying this fact to the update formula (2) regarding the weak signal $\\mathbf{v}$, we are able to show that $h_2^{(t)}\\cdot\\sigma(\\langle\\mathbf{w}^{(t)},\\mathbf{v}\\rangle)$ can keep increasing, which follows from the same proof of the second conclusion of Lemma 17. This demonstrates the benefits of large learning rate oscillating training in helping the model to learn the weak features.\n- (Small learning rate case) In contrast, training two layer $\\mathrm{ReLU}$ activation NN with small learning rate cannot learn the weak signal. One can consider the same two-phase analysis as in our paper. In Phase 1, with a small initialization (the initialization scalings for $\\mathbf{h}$ and $\\mathbf{w}$ need to be similar to enable learning both layers), the NN is trying to learn both signals simultaneously and the growth of both signals are at least exponentially fast in early stages (note that \n${{h_1^{(t)}}^2\\|\\|\\mathbf{u}\\|\\|_2^2+\\sigma(\\langle\\mathbf{w}^{(t)},\\mathbf{u}\\rangle)^2\\ge h_1^{(t)}\\sigma(\\langle\\mathbf{w}^{(t)},\\mathbf{u}\\rangle)\\|\\|\\mathbf{u}\\|\\|_2}$ and similar result can hold for $h_2^{(t)}\\sigma(\\langle\\mathbf{w}^{(t)},\\mathbf{v}\\rangle)$). However, due to the difference in the strength of the two signals, when Phase 1 quickly ends, the NN only learns the strong signal but makes little progress in learning the weak signal. Next, in Phase 2, we can prove that the dynamic smoothly converges to a local minima by exploiting the learned strong signal before it can make sufficient progress in learning the weak signal. In combination of Phase 1 & 2, we can show that training the two-layer NN with $\\mathrm{ReLU}$ activation fails to learn the weak signal under small learning rate. \n\nThus, there is still a division of generalization between large learning rate training and small learning rate training if we consider training both layers with standard $\\rm ReLU$ activation. This shows the potential extendibility of our proof of the benign oscillation phenomenon, at least for training two-layer $\\rm ReLU$ neural networks. We will add the discussion on the potential extensions to more general settings in the revision.\n\n**References:** \n\n[1] Zhong, Kai, et al. \"Recovery Guarantees for One-hidden-layer Neural Networks.\" International Conference on Machine Learning. PMLR, 2017.\n\n[2] Lyu, Kaifeng, and Jian Li. \"Gradient Descent Maximizes the Margin of Homogeneous Neural Networks.\" International Conference on Learning Representations, 2019.\n\n[3] Fan, Fenglei, Jinjun Xiong, and Ge Wang. \"Universal Approximation with Quadratic Deep Networks.\" Neural Networks 124 (2020): 383-392.\n\n[4] Cao, Yuan, et al. \"Benign Overfitting in Two-layer Convolutional Neural Networks.\" Advances in neural information processing systems 35, 2022.\n\n[5] Shen, Ruoqi, S\u00e9bastien Bubeck, and Suriya Gunasekar. \"Data Augmentation as Feature Manipulation.\" In International conference on machine learning, pp. 19773-19808. PMLR, 2022.\n\n[6] Allen-Zhu, Zeyuan, and Yuanzhi Li. \"Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning.\" The Eleventh International Conference on Learning Representations, 2023.\n\n[7] Zou, Difan, et al. \"Understanding the Generalization of Adam in Learning Neural Networks with Proper Regularization.\" The Eleventh International Conference on Learning Representations, 2023."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5736/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700732006520,
                "cdate": 1700732006520,
                "tmdate": 1700732285197,
                "mdate": 1700732285197,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "DbyZHXuM46",
            "forum": "wYmvN3sQpG",
            "replyto": "wYmvN3sQpG",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5736/Reviewer_ahqJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5736/Reviewer_ahqJ"
            ],
            "content": {
                "summary": {
                    "value": "The authors study the effect of large learning rate SGD on a prediction task with a one hidden layer CNN network. More specifically, they study a binary classification problem with weak and strong signal patches and theoretically argue that there is a generalization separation between small and large learning rate SGD training:\n- for small learning rate, SGD is unable to learn the weak signal\n- for large learning rate, SGD is able to learn the weak signal     \n\nThey argue that the oscillatory behavior of large learning rate SGD is a crucial feature of this fact. \nThey also illustrate these results with experiments."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The study of SGD with large learning rates is believed to be crucial to understand the good learning properties of neural networks. Yet, its analysis is in many cases very difficult to handle: the oscillations, the movement due to noise are technically challenging to analyse. \n\nIn this perpective, the authors do a very good job providing a setup for which they 'prove' that such a dynamical behavior leads SGD to good generalization.\n\nOn top of this, I find the paper very well written, the setup very clear and the explanation with the toy model on Section 3 very good."
                },
                "weaknesses": {
                    "value": "In my opinion, here is a weakness of the paper:\n\n- the authors claim to perfectly prove that the oscillatory behavior of the SGD iterates lead it to learn the weak signal, but they need to assume that the iterate oscillate in the first place. While they show that this oscillation is *consistent* with their hypothesis, it could be great that the authors comment a bit more of this necessity to assume this. How difficult would it be to remove this hypothesis ? Is the problem to control that the iterates do not diverge ? Or on the contrary that they do no converge locally ?\n\n- If this is the second option, could the authors add some (bounded) label noise at each iteration to show this? This is what is explained to be done in a series of paper like *Label noise (stochastic) gradient descent implicitly solves the Lasso for quadratic parametrisation* L. Pillaud-Vivien et al., COLT, 2022. Could the authors comment on this?"
                },
                "questions": {
                    "value": "- On top of the questions raised above, I really would like to understand whether the setup described above has can also guide on what appears to be a simple problem like diagonal linear networks where the oscillations lead to sparse features.\n\nDoes the analysis of the authors shed light onto this problem ?\n\n- Also, it appears that the initialization is very small ($d^{-1/2}$ with a large $d$). Can the authors comment on the necessity of such an unusual initialization ?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5736/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698708372646,
            "cdate": 1698708372646,
            "tmdate": 1699636600784,
            "mdate": 1699636600784,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "o7qEGJkwFF",
                "forum": "wYmvN3sQpG",
                "replyto": "DbyZHXuM46",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5736/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5736/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ahqj (Part 1)"
                    },
                    "comment": {
                        "value": "Thank you very much for your detailed review and your support of our work! We address all your concerns and questions in the following. \n\n**Q1: While the authors show that the oscillation is consistent with their hypothesis, it could be great that the authors comment a bit more of this necessity to assume this. How difficult would it be to remove this hypothesis? Is the problem to control that the iterates do not diverge? Or on the contrary that they do not converge locally? If this is the second option, could the authors add some (bounded) label noise at each iteration to show this?**\n\n**A1:** Thanks for your questions! We will explain the necessity of making this assumption in the following. In short, it is intuitively possible to derive a sufficient condition that enables the stable oscillation. However, proving a rigorous and clean sufficient condition for the multiple-data setting would be highly complicated. We thus resort to summarizing this specific mode of oscillation as an assumption in order to better present the key idea and finding of our work. \n\nNow we answer your questions in detail.\n\nWe first explain why we need this assumption. As we illustrate in the single training data setup (Section 3), the oscillation (in terms of the model parameter) tend **not** to cancel with each other, but to have a strict positive value, which turns out to be the driving force of the weak signal learning. *Intuitively*, the amount of the accumulation of oscillation will determine the amount of weak signal learning: \n$$\n\\langle \\mathbf{w}\\_{y, r}^{(t\\_1)},y \\mathbf{v} \\rangle  \\geq  \\langle \\mathbf{w}\\_{y,r}^{(t\\_0)}, y\\mathbf{v}\\rangle\\cdot\\exp\\bigg(\\underbrace{\\Omega\\bigg(\\sum_{t = t_0}^{t_1}\\big(1 - y\\cdot f(\\mathbf{x};\\mathbf{W}^{(t)})\\big)\\bigg)}_{(\\star)\\geq 0\\text{ by Section 3}}\\bigg)\n$$\nTherefore, it boils down to mathematically characterize how fast the oscillation accumulation $(\\star)$ would grow, which is **not** simply to only control the iterates from divergence or to prove no local convergence. What we want is a control over the magnitude of the oscillation (i.e., $\\delta$) so that the summation $(\\star)$ can be lower bounded.\n\nIntuitively, for the single training data setup, one can achieve this by properly scaling the initialization and the learning rate. i.e., controlling the learning rate by using the loss smoothness parameters at the initialization and the nearby local minima. Then, as the magnitude of the oscillation can be expressed as a joint function of the loss smoothness, the learning rate, and the initialization, we can then control the magnitude via properly scaling the learning rate and the initialization so as to give a sufficient condition for certain oscillation magnitude.\n\nHowever, in the general multiple data setup, the training dynamic when oscillation happens could be very difficult to precisely characterize, and the specific modes of oscillation could be quite sensitive to the choice of the learning rate and initialization (different LR and initialization can result in different modes of oscillation and may require different analysis). To this end, we took the cleanest and least troublesome way by assuming that the magnitude of the oscillation is uniformly bounded away by a some $\\delta>0$, which then leads to the linear accumulation we show in the paper (e.g., Eq. (7) in Section 3). This saves us from diving into the explicit mathematical characterization of the oscillation dynamic and better explain the key message regrading the benefits of oscillation we would like to convey."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5736/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700071890276,
                "cdate": 1700071890276,
                "tmdate": 1700071890276,
                "mdate": 1700071890276,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "EQ8zrA2DLF",
                "forum": "wYmvN3sQpG",
                "replyto": "DbyZHXuM46",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5736/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5736/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ahqj"
                    },
                    "comment": {
                        "value": "Dear reviewer ahqj, we thank you again for your time reviewing our paper and your support of our work! We have revised our draft according to **Q2** (see Appendix A) and **Q3** (see Appendix C). As the end of author-review discussion period is approaching, we appreciate it if you can kindly let us know whether your previous concerns have been fully addressed. We are happy to answer any further questions. Thanks!"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5736/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700510499980,
                "cdate": 1700510499980,
                "tmdate": 1700510499980,
                "mdate": 1700510499980,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "iAWzxr7owB",
            "forum": "wYmvN3sQpG",
            "replyto": "wYmvN3sQpG",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5736/Reviewer_1fN5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5736/Reviewer_1fN5"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the generalization benefit of using large learning rates in stochastic gradient descent. Specifically, in the setting of a two-layer convolutional neural network, it is shown that although using a large learning rate causes the loss value to oscillate, it indeed further enables SGD to do feature learning. The main result is proven under the assumption of a feature-noise data generation model, where the strong and weak features are separated by the $\\ell_2$ norm. Empirical evaluations are also provided to support the theoretical results."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper is well written and easy to follow, and the authors have explained the theoretical intuition clearly through the example of a single training data in Section 3.\n\n- The generalization benefit of large learning rates is an important topic in the community. This paper presents an interesting result in this direction, though the feature-noise data generation model seems somewhat artificial.\n\n- The theoretical analysis in this paper is very solid."
                },
                "weaknesses": {
                    "value": "- It is not clear if the feature-noise data generation model is a natural model that contains separate strong feature, weak feature, and noise. \n\n- The network structure seems to be tailored to the feature-noise data generation model, as it applies a separate weight vector to each of the patch. Although the authors claimed that the main results can be further generalized, but it is not immediately clear how.\n\n- The oscillating condition that $|y_{i_t} f(\\mathbf{x}_{i_t};\\mathbf{W}^{(t)} - 1|\\geq \\delta$ is proposed as an assumption, rather than being proved."
                },
                "questions": {
                    "value": "1. How to justify the feature-noise data general model? E.g., is this a natural assumption for real-world dataset like CIFAR-10?\n\n2. The authors discussed some necessary conditions of Assumption 4 on the loss oscillation. Are there any comprehensible sufficient conditions?\n\n3. The authors mentioned a bit about the edge-of-stability regime. Can the authors expand a bit more on this? For example, how is the oscillation assumption related to EoS?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5736/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699145906243,
            "cdate": 1699145906243,
            "tmdate": 1699636600691,
            "mdate": 1699636600691,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "CkS7ukPuuI",
                "forum": "wYmvN3sQpG",
                "replyto": "iAWzxr7owB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5736/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5736/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 1fN5 (Part 1)"
                    },
                    "comment": {
                        "value": "Thank you very much for your appreciation of our work! We address all your concerns and questions in the following. \n\n**Q1: How to justify the feature-noise data general model? E.g., is this a natural assumption for real-world dataset like CIFAR-10?**\n\n**A1:** The original version of this multi-patch data model can be found in the pivotal work in feature learning [1] and our version is simplified to only three patches with strong/weak noises and signals. We put the answer to this problem in two folds:\n1. How original data model in [1] can approximate real world image-classification data? In the multi-view feature model therein, the authors assume that the images from given class should exhibits some specific signals with high probability. For example, for an image that represents a car, it is highly possible that \"wheel\", \"window\", or \"headlight\" appears some where in the image. Moreover, \"wheel\" and \"wheel\" usually share high similarity between images, wheras \"wheel\" and \"window\" (or other irrelvant noise/feature) are typically uncorrelated due to the high-dimensionality. This allows us to encode the features with fixed and mutually orthogonal signal vectors and set the noise orthogonal to all the signals. The reviewers can refer to Section 2.3 in [1] for a brief introduction and Appendix A for technical specifications to the data model. We also note that the authors in [1] mentioned that this data model can be considered as the intermediate ouput of previous well-trained convolution layer.\n2. How does our model reasonably simplify the multi-view feature data model? In short, we proposed several simplifications towards the data model to make our presentation easier to follow without losing the generality. \n    - Simplification 1: We only consider the binary classification with three-patches image data. One can imagine that all the information that is irrelavant to the binary classificaiton of our interests is covered in the noise patch. \n    - Simplification 2: Fixed position signal and orthogonal Gaussian noise. In the original multi-view data model [1], the signals appear randomly among all possible patches, which aligns with the real images. But CNN is unable to identify this positional information because of the \"weight sharing\" principle, thus our simplication is reasonable.\n    - Simplification 3: No feature noise. The original model assume that the important features are corrupted (additively) by other irrelavant features or noises. However, in our setting, this is not essential in presenting our key mesaage. Suchg simplification can reduce some unnecessary technical issues.\n\n**References:**\n\n[1] Allen-Zhu, Z., & Li, Y. (2022, September). Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning. In The Eleventh International Conference on Learning Representations."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5736/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700071334880,
                "cdate": 1700071334880,
                "tmdate": 1700071334880,
                "mdate": 1700071334880,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Pv7hbXQoKQ",
                "forum": "wYmvN3sQpG",
                "replyto": "iAWzxr7owB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5736/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5736/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 1fN5"
                    },
                    "comment": {
                        "value": "Dear reviewer 1fN5, we thank you again for your time reviewing our paper and your support of our work! As the end of author-review discussion period is approaching, we appreciate it if you can kindly let us know whether your previous concerns have been addressed. We are happy to answer any further questions. Thanks!"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5736/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700510422317,
                "cdate": 1700510422317,
                "tmdate": 1700510422317,
                "mdate": 1700510422317,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xgZRmmYUT6",
                "forum": "wYmvN3sQpG",
                "replyto": "Pv7hbXQoKQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5736/Reviewer_1fN5"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5736/Reviewer_1fN5"
                ],
                "content": {
                    "title": {
                        "value": "Reply to rebuttal"
                    },
                    "comment": {
                        "value": "I thank the authors for their explanation. I think it's important and helpful to incorporate the justification of the settings an assumptions in the main text. I don't have further questions, and I'll keep my rating."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5736/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700701831650,
                "cdate": 1700701831650,
                "tmdate": 1700701831650,
                "mdate": 1700701831650,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]