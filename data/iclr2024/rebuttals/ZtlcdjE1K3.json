[
    {
        "title": "DECOUPLE QUANTIZATION STEP AND OUTLIER-MIGRATED RECONSTRUCTION FOR PTQ"
    },
    {
        "review": {
            "id": "C5JXESmZUz",
            "forum": "ZtlcdjE1K3",
            "replyto": "ZtlcdjE1K3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2422/Reviewer_wiuF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2422/Reviewer_wiuF"
            ],
            "content": {
                "summary": {
                    "value": "PTQ (Post-Training Quantization) is a technique that exports a pre-trained full precision neural network into low-bit. It is a promising way to reduce size of neural networks and costs of inference. However, PTQ can cause accuracy degradation with extremely low-bit settings. To solve this problem, the paper proposes two methods that are for improving PTQ process. \n\nIn the quantization process, weight parameters of a target network are quantized before inference. The paper tackles that there isn\u2019t any previous work that focuses on how to quantize weight parameters better. With a simple yet clear experiment, the paper shows that decoupling quant step and dequant step, and fine-tuning dequant step only lead to performance gain. In addition, the paper points out that outlier activation values that are clipped out after quantization affect performance a lot. To utilize these outlier values, the paper proposes OMR (Outlier-Migrated Reconstruction) that adds several channels to filters and operates them with outlier values which are shifted into safe clipping ranges. It has the same effect as increasing the number of bits.\n\nThe paper surpasses other previous works only with the decoupling quant step and dequant step. In addition, with OMR, the paper shows that higher accuracy can be obtained by sacrificing efficiency."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper explains the proposed method well with several figures and formulas.\n- The paper shows performance improvement with simple methods that are easy to follow.\n- The paper presents various experiments for presenting the efficiency of the proposed method."
                },
                "weaknesses": {
                    "value": "- OMR leads to enlarging the size of neural networks. Therefore, the application of the proposed method can be limited. However, the paper doesn\u2019t provide analysis in terms of costs which helps in understanding the trade-off between the size of a model and its performance.\n- It doesn\u2019t seem appropriate that the arrangements and citations of figures and tables. For instance, the first citation of Figure 2 is on page 2, and Figure 2 is on page 5."
                },
                "questions": {
                    "value": "- In Outlier-Migrated Reconstruction, how can the sensitivity of each channel be measured, if only a portion of channels will be added?\n- It seems that OMR is designed to mitigate outlier of activations and outlier of weight parameters can\u2019t be utilized with this method. Does OMR help mitigate outlier of weight parameters?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2422/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2422/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2422/Reviewer_wiuF"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2422/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698646203696,
            "cdate": 1698646203696,
            "tmdate": 1700694999408,
            "mdate": 1700694999408,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "I2AzFoaMPF",
                "forum": "ZtlcdjE1K3",
                "replyto": "C5JXESmZUz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2422/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2422/Authors"
                ],
                "content": {
                    "title": {
                        "value": "OMR\u2019s Computational Cost and Sensitivity Criterion,  re-polished Figure and OMR on Weight"
                    },
                    "comment": {
                        "value": "Thanks for your instructive review.\n\n## 3.1 OMR\u2019s Computational Cost\n\nOur OMR does no need the Fake Quant process. It can be finished in **1 CPU-second** based on offline methematical transformation. It introduces extra FLOPs during Real Quant.\n\n- For k=0.2 or k=0.5, where we save 20% or 50% channles' outliers, if\nFLOPs of original OMR-Solvalble structures is 10M, then FLOPs of\nOMR-applied ones is 12M or 15M.\n- Trade-off analysis on MobileNet-V2 between FLOPs/Params and its performance is as follows. We denote the FLOPs 327M, params 3.5M of MobileNet-V2 on W2A2 as 1.0, on which of other methods is based. Note that \"W2A3\" can not run on 2-bit hardware while our **OMR can run on 2-bit hardware with W2A3 performance** as said on our paper PDF.\n    \n    | MobileNet-V2 | W2A2 | FLOPs | Params | Run on 2bit Hardware |\n    | --- | --- | --- | --- | --- |\n    | NWQ+Ori_Net | 26.42 | 1.0 | 1.0 | \u221a |\n    | NWQ+Channel-plus -Net | 32.02 | 2.0 | 2.0 | \u221a |\n    |**OMR_0.0+Ori-Net(DJOS)**|**31.43**|**1.0**|**1.0**|\u221a|\n    | OMR_0.3+Ori-Net** | 36.33 | 1.3 | 1.3 | \u221a |\n    | **OMR_0.5+Ori-Net** | **39.35** | 1.5  | 1.5 | \u221a |\n    | OMR_0.7+Ori-Net | 39.75 | 1.7 | 1.7 | \u221a |\n    | OMR_1.0_Ori-Net | 41.65 | 2.0 | 2.0 | \u221a |\n- **To balance FLOPs-vs-Acc trade-off, OMR can be only applied\non the most important channels of the most important OMR-Solvable\nstructures with a given FLOPs limit,** which can be explored in the future.\n\n## 3.2 Arrangements and Citations of Figures and Tables\n\nDue ICLR\u2019s limited space, the placement and citation of figures and tables is a little far. To make full use of limited PDF space, we place experiment tables as together as possible. It is a better choice to place Figure 2 on page 2, thus readers can better understand our OMR\u2019s motivation. We has re-arranged our Figures as suggested, which can be seen on our re-polished paper PDF.\n\n## 3.3 Sensitivity Criterion on OMR\n\nIf we only select the most sensitive channels on the most sensitive OMR-Solvable structures, OMR can be more efficient.\n\nThe sensitivity criterion on channels is as follows, where we choose the sum of each channels\u2019 activation in outlier range$[X_{clip}, 2X_{clip}]$, then we sort each channel in order and choose the top-k percent for OMR.  \n\n$$\nS_i=\\sum_j{||x_{ij}||^2},\n$$ where $x_{i,j}$ is the $j$-th values in range $(X_{clip}, 2X_{xlip}]$  of $i$-th channel.\n$$ \nS^\\*={sort}(S_i|i=1,...,N), S^{\\*k}={Top\\\\_K}(S^{\\*})\n$$\n\n\nOther sensitivity criterion can also be applied, which can be explored as a future work.\n\n## 3.4 OMR can be Used on Weight\n\nIt is right that OMR can also be used on weight like activation. **The core of OMR, migrating outliers into safe clipping range then compensating in the following layers, can be extended to weight**. Detailed analysis is as follows,\n\n- When OMR is used on activation, we want to save outlier activation in range $[X_{clip}, 2X_{clip})$. According to equal mathematical transformation, we can transform activation modification into weight modification. We can modify the $i$-th and $i+1$-th weight as follows, thus a functionally-identical network is caused without retraining except for more outlier activation being preserved.\n\n$$\n\\begin{equation}W_{i,j}^{l\\prime} = \\begin{cases}W_{i,j}^{l}, & \\text{if } 0<j\u2264c_{out}; \\\\\\\\W_{i,j-c_{out}}^{l}, & \\text{if } c_{out}<j\u22642c_{out}.\\end{cases}\nb_{j}^{l\\prime} = \\begin{cases}b_{j}^{l}, & \\text{if } 0<j\u2264c_{out}; \\\\\\\\b_{j-c_{out}}^{l}-X_{clip}, & \\text{if } c_{out}<j\u22642c_{out}.\\end{cases}\n\\end{equation}\n$$\n\n$$\n\\begin{equation}W_{i,j}^{l+1\\prime} = \\begin{cases}W_{i,j}^{l}, & \\text{if } 0<i\u2264c_{out}; \\\\\\\\W_{i-c_{out},j}^{l}, & \\text{if } c_{out}<i\u22642c_{out}.\\end{cases}\n\\end{equation}\n$$\n\n- When OMR is used on weight $W$, we want to save outlier weight in range $[W_{clip}, 2W_{clip)}$. We can directly transform $i$-th and $i+1$-th weight as follows,\n    \n    $$\n    \\begin{equation}W_{i,j}^{l\\prime} = \\begin{cases}W_{i,j}^{l}, & \\text{if } 0<j\u2264c_{out}; \\\\\\\\W_{i,j-c_{out}}^{l}-W_{clip}, & \\text{if } c_{out}<j\u22642c_{out}.\\end{cases}\n    b_{j}^{l\\prime} = \\begin{cases}b_{j}^{l}, & \\text{if } 0<j\u2264c_{out}; \\\\\\\\b_{j-c_{out}}^{l}, & \\text{if } c_{out}<j\u22642c_{out}.\\end{cases}\n    \\end{equation}\n    $$\n    \n    $$\n    \\begin{equation}W_{i,j}^{l+1\\prime} = \\begin{cases}W_{i,j}^{l}, & \\text{if } 0<i\u2264c_{out}; \\\\\\\\W_{i-c_{out},j}^{l}, & \\text{if } c_{out}<i\u22642c_{out}.\\end{cases}\n    \\end{equation}\n    $$"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2422/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700624216843,
                "cdate": 1700624216843,
                "tmdate": 1700637126203,
                "mdate": 1700637126203,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JJTwjByovY",
                "forum": "ZtlcdjE1K3",
                "replyto": "C5JXESmZUz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2422/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2422/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer wiuF,\n\nThanks for your valuable reviews.\n\nIf you have any problem, we are online for response by the last second of 22nd November. We are reaching out to see if our response adequately addresses your concerns. We greatly value your comments and are committed to refining our work. Any further feedback would be helpful in polishing our final revisions. \n\nThank you for your time and expertise.\n\nSincerely,\n\nAuthors"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2422/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700636303697,
                "cdate": 1700636303697,
                "tmdate": 1700636303697,
                "mdate": 1700636303697,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gkCR03XtZw",
                "forum": "ZtlcdjE1K3",
                "replyto": "C5JXESmZUz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2422/Reviewer_wiuF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2422/Reviewer_wiuF"
                ],
                "content": {
                    "comment": {
                        "value": "Thank the authors for the answers to the reviewer's concerns and questions.\n\nLastly, the reviewer will ask the authors one short question.\n\nI agree with the reviewer pf4t that OMR seems similar with OCS.\n\nWhat is the critical difference between OMR and OCS?"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2422/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700650602215,
                "cdate": 1700650602215,
                "tmdate": 1700650602215,
                "mdate": 1700650602215,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SXTsXHoZht",
                "forum": "ZtlcdjE1K3",
                "replyto": "C5JXESmZUz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2422/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2422/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer wiuF, \n\nThanks for your valuable feedback.\n\n## 1. OMR V.s. OCS\nOur OMR is motivated from OCS to overcome OCS's shortcoming of sacrificing the precision of inner values. \n\nIt is true that when integer activation is  [2,4,8,16], OMR share the same precision as OCS as reviewer pf4t. \n\nHowever, when input is [2,4,8,15], OCS will split split [2,4,8,15]->[1,2,4,7]+[1,2,4,7] or [2,4,8,15]->[1,2,4,8]+[1,2,4,8], thus there will be **an rounding error** for 15->7\\*2=14 or 15->8\\*2=16, while OMR will split [2,4,8,15]->[1,2,4,8]+[1,2,4,7], thus no error is caused for 15->8+7.\n\nThe main problem is that when the bin's number of inputs is larger than quantization levels, OCS saves outliers by sacrificing the precision of inner values.\n\nFor a more detailed example with the whole fake quantization: x=[0.0, 0.1, 0.2, 0.3, *0.4*, *0.5*, *0.6*, *0.7*], step=0.1, 2-bit, Thus outliers is [0.4,0.5,0.6,0.7].\n- Normal 2-bit: \n  - $x_{int}=clip[round(x/step), 0, 3]=[0, 1, 2, 3, 3, 3, 3, 3]$ quantization levels: 4\n  - $\\hat{x}=x_{int}\\*step=[0.0, 0.1, 0.2, 0.3, 0.3, 0.3, 0.3, 0.3]$ quantization levels: 4\n- OCS 2-bit:\n  - $x_{int}=clip[round((x/2)/step), 0, 3]=[0, 0, 1, 1, 2, 2, 3, 3]$ quantization levels: 4\n  - $\\hat{x}=(x_{int}\\*step)*2=[0.0, 0.0, 0.2,0.2,  0.4, 0.4, 0.6,0.6]$ quantization levels: 4\n- OMR 2-bit:\n  - $x_{int}=concat\\\\{clip[round((x)/step), 0, 3], clip[round((x-0.4)/step), 0, 3]\\\\}=[[0, 1,  2, 3],[0,1,2,3]]$\n  - $\\hat{x}=concat\\\\{x_{int}[0]\\*step, x_{int}[1]*step+0.4]\\\\}=concat\\\\{[0.0, 0.1, 0.2, 0.3], [0.4,0.5, 0.6, 0.7]\\\\}=[0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]$ **quantization levels: 8**\n\nTherefore:\n- **OMR is closer to (here equal to) original fp32 values during fake-quant with larger quantization levels**. \n  - [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7] $\\rightarrow$ [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]\n- OCS sacrifices [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7] $\\rightarrow$  [0.0, 0.0, 0.2,0.2,  0.4, 0.4, 0.6,0.6] during fake-quant.\n\nIn conclusion:\n- OCS does not enlarge quantization levels. OCS saves outliers while sacrifices the precision of inner values.\n- Our OMR, for the first time, proposes to save more outliers while preserve the precision of inner values.\n\n**Comparison visualization is re-polished in supplementary PDF Sec2.**\n\nIf you have any problem, we are online for response by the last second of 22nd November. We are reaching out to see if our response adequately addresses your concerns. We greatly value your comments and are committed to refining our work.\n\nSincerely,\n\nauthors"
                    },
                    "title": {
                        "value": "OMR V.s. OCS"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2422/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700660566803,
                "cdate": 1700660566803,
                "tmdate": 1700696430355,
                "mdate": 1700696430355,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nPmX28PKh2",
                "forum": "ZtlcdjE1K3",
                "replyto": "C5JXESmZUz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2422/Reviewer_wiuF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2422/Reviewer_wiuF"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the answer to my last question.\nIt helps understand OMR shows better expressibility compared to OCS.\nTherefore, I decided to raise my score to 6."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2422/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700695465034,
                "cdate": 1700695465034,
                "tmdate": 1700695465034,
                "mdate": 1700695465034,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "nVpuqlvzSN",
            "forum": "ZtlcdjE1K3",
            "replyto": "ZtlcdjE1K3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2422/Reviewer_pf4t"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2422/Reviewer_pf4t"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the Post-training quantization (PTQ) of deep learning models. Two main methods are proposed for weight and activation PTQ. For weight PTQ, DJOS is proposed to use a fixed quantization step for quantization and a learned step size for de-quantization. For activation PTQ, Outlier-Migrated Reconstruction (OMR) is proposed to split a channel into multiple channels to solve the outlier activations. The proposed methods are evaluated on ImageNet task and COCO task across various networks and bitwidth settings."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. The authors fully explore different settings of weight\u2019s quant-step into five cases, and study the performance of different settings.\n2. The Outlier-Migrated Reconstruction is straight-forward and effective for PTQ given a pre-defined bit-width."
                },
                "weaknesses": {
                    "value": "1. My main concern is about the novelty. The proposed Outlier-Migrated Reconstruction is well studied by previous works [1].\n2. It is hard for me to understand why DJOS works. If the quant-step is fixed and the dequant-step is learnable, it is the same with the learning process of BN. In other words, network quantization with BN has already a mechanism of learned dequant-step.\n3. In the experiments, the baselines of different methods are not the same, making the comparison unfair.\n4. It is better to study weight-only quantization (DJOS) and compare with previous methods to evaluate the effectiveness of DJOS.\n\n[1] Improving Neural Network Quantization without Retraining using Outlier Channel Splitting, ICML, 2020."
                },
                "questions": {
                    "value": "1. What's the performance of weight-only quantization using the proposed method?\n2. Which pretrained models are used as baseline?\n3. Could you provide the detailed quantization algorithm?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2422/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698755390362,
            "cdate": 1698755390362,
            "tmdate": 1699636177490,
            "mdate": 1699636177490,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "WTCHthaORd",
                "forum": "ZtlcdjE1K3",
                "replyto": "nVpuqlvzSN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2422/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2422/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Novelty of OMR and DJOS, Detailed Algorithm"
                    },
                    "comment": {
                        "value": "Thanks for your instructive review.\n## 2.1 Novelty of OMR\nWe compare OMR with OCS in Sec.4.3. It is true that OCS studied the outlier problem. However, the outlier problem is **not well studied by OCS** due to its shortcomings.\n- OCS does not enlarge quantization levels. OCS saves outliers **while sacrifices the precision of inner values**.\n    - OCS saves outliers by halving operation. OCS squeezes the outlier values into a narrower range, however, which also squeezes the inner values into a narrower range. Thus OCS save outliers at the sacrifice of the precision of inner values.\n    - OCS failed to help achieve better performance on low-bit quantization.\n- **Our OMR, for the first time, proposes to save more outliers while preserve the precision of inner values**. Thus OMR actually enlarge quantization levels and equals to earning one more unavailable bit, which is extremely helpful for 2,3-bit, quantization.\n- **Their comparison visualization can be seen in the supplementary PDF Sec-2**.\n## 2.2 DJOS V.s. BN\n- Why DJOS works:\n    Previous methods use a single and fixed quant-step of weight $s_w$. \n    For the first time, we propose 1) to make $s_w$ learnable; 2) to decouple single $s_w$ into $s_w$ and $s^\\prime_w$; 3) to make $s_w$ fixed and $s^\\prime_w$ learnable.\n    - Why making $s_w$ learnable :\n        - In general, more params under necessity during training will benefit model's optimization[1], especially under a extremely low-bit param space. Experiments also demonstrate decoupling brings one more parameter and brings lower reconstruction loss (both training and evaluation).\n    - Why decoupling single $s_w$ into $s_w, s^\\prime_w$ works:\n        - Since integer weight can be obtained before inference, we can safely decouple traditional single $s_w$ into $s_w, s^\\prime_w$. Integer activation need real-time calculation, thus we can not decouple activation\u2019s quant-step $s_x$.\n    - Why fixed $s_w$ and learnable $s^\\prime_w$ works:\n        - PTQ is in shortage of enough labeled training set and long time optimization,  and STE\u2019s mismatched gradient is a little far way from the floor operation. In extremely-low-bit quantization, these two problems make the optimization of $s_w$ extremely difficult. Thus a fixed $s_w$ is better than wrongly-updated $s_w$. Therefore, it will achieve better performance to make the dequant-step $s^\\prime_w$ learnable only.\n- Comparison Between DJOS and BN:\n    - **It is not true to say \u201cWhen we have BN, we do not need fake quantization\u201d. They are two orthogonal methods, BN helps for fp32 model training and DJOS helps for low-bit quantization.**\n    - Correlation: From the point of math formula, DJOS share similar form as BN.\n$$\n\\text{DJOS}:\\hat{w}=s^\\prime_w\\cdot \\\\{\\lfloor \\frac{x}{s_w}\\rfloor +h(\\alpha)\\\\}\\ \\ \\ \\ \\text{with $s_w$ fixed and $s^\\prime_w$ learnable}\n$$\n$$BN: {y}=\\gamma\\cdot \\frac{x-\\mu}{\\sigma}+\\beta, \\text{with $\\mu,\\sigma$ statistically updated and $\\gamma, \\beta$ learnable}\n$$\n- However, they are diffenrent in physical sense.\n    - BN is used to reduce the internal covariate shift, which **maps fp32 values(any distribution) into a $N(0,1)$ Gussian-like distribution then learn to recover them**. It is usually used on activation.\n    - DJOS is a fake quantization method, which **maps fp32 values into integer space then re-map them into concrete fp32 counterparts. DJOS does not change original distribution**. **DJOS is used on weight and can not be used on activation**, because integer weight can be obtained before inference while integer activation has to be computed on line during inference.\n\n[1] I. Goodfellow, 2016. Deep Learning. MIT Press\n## 2.3 Baseline Pretrained Model\nWe borrowed FP32 pretrained model from BRECQ(https://github.com/yhhhli/BRECQ),  also the same as QDROP and NWQ. Thus our comparison is fair.  PD-Quant states different FP32 Acc@1. Their FP32 Acc@1 on their paper  are as follows:\n| FP32 Acc@1| Mobile-V2 | Res-18 | Reg-600 | Mnas-2.0 |\n| --- | --- | --- | --- | --- |\n| DOMR(ours) | 72.49 | 71.08 | 73.71 | 76.68 |\n| BRECQ | 72.49 | 71.08 | 73.71 | 76.68 |\n| QDROP | 72.49 | 71.08 | 73.71 | 76.68 |\n| NWQ | 72.49 | 71.06 | 73.71 | 76.68 |\n| PD-Quant | 72.62 | 71.01 | 73.52 | 76.52 |\n\nWe can see the FP32 Acc@1 for Mobile-V2, Res-18, Reg-600, Mnas-2.0 in PD-Quant is slightly different from BRECQ, QDROP, NWQ and ours. Their **FP32 difference is no more than 0.2 percentage**. However, our DOMR is **at least 1 percentage better than others, at least 2 percentage better than PD-Quant**,  in W3A3 and W2A2. Thus DOMR\u2019s performance is still robust and it is fair to say DOMR set up a new SOTA for PTQ.\n## 2.4 Weight-Only Quantization with DJOS\nWe can see DJOS outperforms existing PTQ SOTA in weight-only quantization.\n|  | QDROP | PD-Quant | NWQ | DJOS |\n| --- | --- | --- | --- | --- |\n| W2A32-Res-18 | 66.02 | 66.66 | 67.11 | 67.83 |\n| W3A32-Res-18 | 70.04 | 70.13 | 70.16 | 70.38 |\n## 2.5 Detailed Algorithm can be seen in the supplementary PDF Sec-3."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2422/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700280075922,
                "cdate": 1700280075922,
                "tmdate": 1700280163029,
                "mdate": 1700280163029,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ATcrvdVt0G",
                "forum": "ZtlcdjE1K3",
                "replyto": "nVpuqlvzSN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2422/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2422/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Novelty of our OMR and DJOS"
                    },
                    "comment": {
                        "value": "Dear Reviewer pf4t, \n\nThanks for your valuable reviews.\n\nExcept for comments we provide before, the novelty of our OMR and DJOS, and our contribution on both weight and activation quantization is as what Reviewer RfJv commented: \n>\"The authors provide intriguing and valuable insights, particularly in the idea of enhancing both weights and activations quantization. The decoupling of weight step-sizes for optimization is a noteworthy contribution, and the notable performance enhancements achieved by introducing a single additional step-size in each layer is quite interesting.\"\n\nIf you have any problem, we are online for response by the last second of 22nd November. We are reaching out to see if our response adequately addresses your concerns. Thank you for your time and expertise.\n\nSincerely,\n\nAuthors"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2422/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700635902013,
                "cdate": 1700635902013,
                "tmdate": 1700636336389,
                "mdate": 1700636336389,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qLFoGZKvao",
                "forum": "ZtlcdjE1K3",
                "replyto": "WTCHthaORd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2422/Reviewer_pf4t"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2422/Reviewer_pf4t"
                ],
                "content": {
                    "title": {
                        "value": "Comments to the author response"
                    },
                    "comment": {
                        "value": "Thanks for the detailed response. But the response does not fully address my concerns. \n\n1. My main concern is about the novelty of OMR compared with OCS. I understand there are some minor differences between OMR and OCS, however, these differences are not critical. The main idea is to split the channel into two parts, i.e., $WX=WX_1 + W\u2019X_2$, however, how to split is not the key point. There are many other splitting ways. I also can\u2019t agree with the authors about the precision sacrificing/preserving of inner values. The OCS is like to split [2,4,8,16]->[1,2,4,8]+[1,2,4,8], while OMR is like to split [2,4,8,16]->[2,4,8,8]+[0,0,0,8]. We can\u2019t say OMR is more accurate than OCS for inner values.\n\n2. I also understand the difference between fake quantization and BN. My concern is that the learnable dequantization-step plays the same role as the *gamma parameter* of BN, which is a learnable affine transformation of features.\n\nBased on the above, I would like to keep my rating."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2422/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700647231101,
                "cdate": 1700647231101,
                "tmdate": 1700647231101,
                "mdate": 1700647231101,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HwAB1JaPu7",
                "forum": "ZtlcdjE1K3",
                "replyto": "nVpuqlvzSN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2422/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2422/Authors"
                ],
                "content": {
                    "title": {
                        "value": "OCS V.s. OMR, DJOS V.s. BN"
                    },
                    "comment": {
                        "value": "Dear reviewer pf4t, \n\nThanks for your valuable feedback.\n\n## 1. OMR V.s. OCS\nOur OMR is motivated from OCS to overcome OCS's shortcoming of sacrificing the precision of inner values. \n\nIt is true that when integer activation is  [2,4,8,16], OMR share the same precision as OCS. \n\nHowever, when input is [2,4,8,15], OCS will split split [2,4,8,15]->[1,2,4,7]+[1,2,4,7] or [2,4,8,15]->[1,2,4,8]+[1,2,4,8], thus there will be **an rounding error** for 15->7\\*2=14 or 15->8\\*2=16, while OMR will split [2,4,8,15]->[1,2,4,8]+[1,2,4,7], thus no error is caused for 15->8+7.\n\nThe main problem is that when the bin's number of inputs is larger than quantization levels, OCS saves outliers by sacrificing the precision of inner values.\n\nFor a more detailed example with the whole fake quantization: x=[0.0, 0.1, 0.2, 0.3, *0.4*, *0.5*, *0.6*, *0.7*], step=0.1, 2-bit, Thus outliers is [0.4,0.5,0.6,0.7].\n- Normal 2-bit: \n  - $x_{int}=clip[round(x/step), 0, 3]=[0, 1, 2, 3, 3, 3, 3, 3]$ quantization levels: 4\n  - $\\hat{x}=x_{int}\\*step=[0.0, 0.1, 0.2, 0.3, 0.3, 0.3, 0.3, 0.3]$ quantization levels: 4\n- OCS 2-bit:\n  - $x_{int}=clip[round((x/2)/step), 0, 3]=[0, 0, 1, 1, 2, 2, 3, 3]$ quantization levels: 4\n  - $\\hat{x}=(x_{int}\\*step)*2=[0.0, 0.0, 0.2,0.2, 0.4, 0.4, 0.6,0.6]$ quantization levels: 4\n- OMR 2-bit:\n  - $x_{int}=concat\\\\{clip[round((x)/step), 0, 3], clip[round((x-0.4)/step), 0, 3]\\\\}=[[0, 1,  2, 3],[0,1,2,3]]$\n  - $\\hat{x}=concat\\\\{x_{int}[0]\\*step, x_{int}[1]*step+0.4]\\\\}=concat\\\\{[0.0, 0.1, 0.2, 0.3], [0.4,0.5, 0.6, 0.7]\\\\}=[0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]$ **quantization levels: 8**\n\nTherefore:\n- **OMR is closer to (here equal to) original fp32 values during fake-quant with larger quantization levels**. \n  - [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7] $\\rightarrow$ [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]\n- OCS sacrifices [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7] $\\rightarrow$  [0.0, 0.0, 0.2,0.2,  0.4, 0.4, 0.6,0.6] during fake-quant.\n\nIn conclusion:\n- OCS does not enlarge quantization levels. OCS saves outliers while sacrifices the precision of inner values.\n- Our OMR, for the first time, proposes to save more outliers while preserve the precision of inner values.\n\n**Comparison visualization is re-polished in supplementary PDF Sec2.**\n\n\n## 2. DJOS V.s. BN\nWe can say learnable dequant-step plays a role of learnable affine transformation of features. However, during PTQ(NWQ, DDROP, AdaRound, Brecq), the BN has been merged into convolution to accelerate inference. Thus there has been no  learnable affine transformation parameters $\\gamma$, thus our learnable dequant-step is not redundant. We need a learnable dequant-step to better transform integer activation back to concrete fp32 counterparts with less quantization error.\n\n\nIf you have any problem, we are online for response by the last second of 22nd November. We are reaching out to see if our response adequately addresses your concerns. We greatly value your comments and are committed to refining our work.\n\nSincerely,\n\nauthors"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2422/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700659973061,
                "cdate": 1700659973061,
                "tmdate": 1700696239058,
                "mdate": 1700696239058,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "YkfxY7gJFU",
            "forum": "ZtlcdjE1K3",
            "replyto": "ZtlcdjE1K3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2422/Reviewer_RfJv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2422/Reviewer_RfJv"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a new method for improving the performance of Post-training Quantization (PTQ). The authors identify two key obstacles to overcoming the performance drop of PTQ in extremely low-bit settings: (i) Separate quantization step-size scale factors for weight tensor. The authors propose a method called DOMR, which decouples the scale factors of weights at quant/dequant processes, considering the fact that integer weights can be obtained early in the process before actual deployment. (ii) Handling outliers: Most existing methods ignore outliers in model activations that fall outside the clipping range, particularly in lightweight models and low-bit settings. The authors introduce a technique called Outlier-Migrated Reconstruction to save outliers within a pre-defined bitwidth. The experimental results demonstrate that DOMR outperforms existing methods and establishes a new state-of-the-art approach in PTQ. Specifically, it achieves a 12.93% improvement in Top-1 accuracy for the W2A2 configuration on MobileNet-v2. The authors also mention that they will release the code associated with their work, which is rather important considering the complexity of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper is exceptionally well-crafted, featuring clear and insightful illustrations that greatly aid readers in comprehending the presented concepts. \n- The authors conducts sufficient experiments. \n- The authors provide intriguing and valuable insights, particularly in the idea of enhancing both weights and activations quantization. The decoupling of weight step-sizes for optimization is a noteworthy contribution, and the notable performance enhancements achieved by introducing a single additional step-size in each layer is quite interesting."
                },
                "weaknesses": {
                    "value": "- The paper would benefit from improved clarity in its notations. The use of variables such as w_l, w_u, x_l, and x_u in Equation 2 is not well-defined. It's crucial for the authors to provide clear explanations and definitions for these lower and upper bounds of weights and activations.\n- The authors base their Outlier-Migrated Reconstruction (OMR) method on the assumption that the activation function is ReLU. However, it's essential to discuss the applicability of OMR to models that use non-ReLU functions. For instance, MobileNetV3 employs h-swish, and ViT utilizes GeLU. A more comprehensive discussion about the adaptability of OMR to such activation functions would enhance the paper's coverage. \n- The paper mentions that OMR involves duplicating channels for both weights and activations. This duplication may introduce additional computational overhead, but the paper lacks an in-depth analysis of this aspect. Providing a more detailed examination of the computational costs could be better. \n- The inclusion of an overall cost breakdown in Table 11 is appreciated. However, it would be even more informative if the authors could provide a specific breakdown of the runtime costs associated with Quant Time in the Fake Quantization process. This would offer a more granular view of the computational expenses involved in the proposed method, contributing to a deeper understanding of its practical implications."
                },
                "questions": {
                    "value": "It seems that using decoupled step sizes for weights can also generalize to quantization-aware training, have the authors experimented with this?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2422/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2422/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2422/Reviewer_RfJv"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2422/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698755661775,
            "cdate": 1698755661775,
            "tmdate": 1699636177386,
            "mdate": 1699636177386,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jHUPLEZGEa",
                "forum": "ZtlcdjE1K3",
                "replyto": "YkfxY7gJFU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2422/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2422/Authors"
                ],
                "content": {
                    "title": {
                        "value": "OMR on other activation function; Runtime Costs in Fake-Quant; DJOS on QAT"
                    },
                    "comment": {
                        "value": "Thanks for your instructive review.\n## 1.1 Mathematics Notation\nWe have added the mathematics notions, like $w_u, w_l,x_u,x_l$ in our re-polished paper PDF.\n## 1.2 OMR on ReLU, h-swish, GeLU and others.\nExcept for Conv-ReLU-Conv and positive nonlinear function like ReLU6/Sigmoid, **the core of OMR, migrating outliers into safe clipping range then compensating in the following layers, can be extented to other structures like Conv-Conv, Linear-Linear and nonlinear function like h-swish, GeLU.** \n\n$x_{int}^{min}(x_{int}^{max}),X_{clip}^{min}(X_{clip}^{max})$ denote the min(max) value(clipping bound) in integer and fp32 domain.  Activation fake quantization into B bits can be obtained by,\n$$\n\\hat{x}=s_x\\cdot x_{int}=s_x\\cdot clip\\\\{{\\lfloor\\frac{x}{s_x}\\rceil}, x_{int}^{min}, x_{int}^{max}\\\\}\n$$\nDetailed visualization is shown in **Sec4 of the re-polished supplementary PDF**.\n\n- When activation function is **ReLU**\n    - $x_{int}^{min}=0, x_{int}^{max}=2^B-1$ , $X_{clip}^{min}=0,X_{clip}^{max}=(2^B-1)*s_w$, The inner values is in range $[X_{clip}^{min}, X_{clip}^{max}]$. The outliers is in range $(X_{clip}^{max}, +\\infty)$. We want to save outliers in $(X_{clip}^{max},2X_{clip}^{max}]$. Thus we can migrate $x$ by $X_{clip}^{max}-X_{clip}^{min}=(2^B-1)*s_w$ along the negative $x$-axis to save values in  $(X_{clip}^{max},2X_{clip}^{max}]$. \n- When activation function is **GeLU, h-swish** or **no activation function**, activation $x$ distributes in both negative and positive axis:\n    - $x_{int}^{min}=-(2^{B-1}-1), x_{int}^{max}=2^{B-1}-1$ ,$X_{clip}^{min}=-(2^{B-1}-1)*s_w,X_{clip}^{max}=(2^{B-1}-1)*s_w$, The inner values is in range $[X_{clip}^{min}, X_{clip}^{max}]$. The outliers is in range $(-\\infty, X_{clip}^{min})$ and  $(X_{clip}^{max}, +\\infty)$. We want to save outliers in $[2X_{clip}^{min},X_{clip}^{min})$ and  $(X_{clip}^{max},2X_{clip}^{max}]$. Thus we can migrate $x$ by  $X_{clip}^{max}-X_{clip}^{min}$ along the negative $x$-axis to save values in $(X_{clip}^{max},2X_{clip}^{max}]$. We can migrate $x$ by $X_{clip}^{max}-X_{clip}^{min}$ along the positive $x$-axis to save values in $[2X_{clip}^{min},X_{clip}^{min})$.\n\n## 1.3 Detail of Computational Costs\nOur OMR does no need the Fake Quant process. It can be finished in **1 CPU-second** based on offline mathematical transformation. It introduces extra FLOPs during Real Quant.\n- For k=0.2 or k=0.5, where we save 20% or 50% channles' outliers, if\nFLOPs of original OMR-Solvalble structures is 10M, then FLOPs of\nOMR-applied ones is 12M or 15M.\n- Trade-off analysis on MobileNet-V2 between FLOPs/BOPs and its performance is as follows. We denote the FLOPs 327M and BOPs 1.8G of MobileNet-V2 on W2A2 as 1.0, on which of other methods is based. **Note** that \"W2A3\" can not run on 2-bit hardware while our **OMR can run on 2-bit hardware with W2A3 performance** as our paper PDF.\n    | Mobile-V2 | W2A2 | FLOPs | BOPs | Run on 2bit Hardware |\n    |-|-|-|-|-|\n    | NWQ+Ori_Net | 26.42 | 1.0 | 1.0 | \u221a |\n    | NWQ+Channel-plus -Net | 32.02 | 2.0 | 2.0 | \u221a |\n    | **OMR_0.0+Ori-Net(DJOS)** | **31.43** | **1.0** | **1.0** | \u221a |\n    | OMR_0.3+Ori-Net | 36.33 | 1.3 | 1.3 | \u221a |\n    | **OMR_0.5+Ori-Net** | **39.35** | 1.5  | 1.5 | \u221a |\n    | OMR_0.7+Ori-Net | 39.75 | 1.7 | 1.7 | \u221a |\n    | OMR_1.0_Ori-Net | 41.65 | 2.0 | 2.0 | \u221a |\n\n## 1.4 Runtime Costs in Fake-Quant\nThe specific breakdown of runtime costs of our DOMR(DJOS+OMR) in ResNet-18 fake quantization period is as follows\uff1a\n\n1.4.1. Fold BN into its preceding convolution in FP32 model: 0.02 second.\n\n1.4.2. Initialize the weight\u2019s single quant-step $s_w$ with MSE minimization, decouple the single $s_w$ into $s_w,s^\\prime_w$:  0.5 second\n\n1.4.3. OMR\u2019s sensitivity analysis on channels + choosing sensitive channels for OMR + weight adjustment of the current layer and the next layer: 0.42 minutes\n\n1.4.4. Jointly optimize $s_w^\\prime,s_x$, AdaRound param of weight $\\alpha$ on 1024 random calibration images for 20K iterations: 44 minutes\n\nAll the time consumed on our DOMR fake quantization: 0.02s + 0.5s + 0.42 min + 44 min \u2248 44.43 min\n## 1.5 DJOS on QAT(Quantization-Aware Training)\n| ResNet18 | W3A3 | W2A2 |\n|-|-|-|\n| LSQ_QAT | 69.57\u00b10.08 | 66.50\u00b10.21 |\n| LSQ_QAT+DJOS | 68.95\u00b10.07 | 62.57\u00b10.18 |\n\nWe re-implement LSQ-QAT W2A2 and W3A3 on ResNet-18. Experimental results over 3 runs can be seen above. We can see LSQ_QAT+DJOS performs worse than original LSQ_QAT. \n\nThe reason is that model weight in QAT is learnable and its distribution changes over training process. Whether to learn model weights is also one of the biggest difference for PTQ and QAT.\n\nThus a fixed quant-step $s_w$ of DJOS can not be adaptive to a weight distribution that changes over training process. However, in PTQ, model weight is almost fixed, with at most one quant-step changes due to AdaRound. Therefore,  with limited calibration set and limited optimization time in PTQ, a fixed quant-step $s_w$ will perform better on an almost-fixed weight distribution."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2422/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700571680749,
                "cdate": 1700571680749,
                "tmdate": 1700637185715,
                "mdate": 1700637185715,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "21BnT67km3",
                "forum": "ZtlcdjE1K3",
                "replyto": "YkfxY7gJFU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2422/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2422/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer RfJv,\n\nThanks for your valuable reviews.\n\nIf you have any problem, we are online for response by the last second of 22nd November. We greatly value your comments and are committed to refining our work. \n\nThank you for your time and expertise.\n\nSincerely,\n\nAuthors"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2422/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700636599305,
                "cdate": 1700636599305,
                "tmdate": 1700636599305,
                "mdate": 1700636599305,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wGdvseW1cJ",
                "forum": "ZtlcdjE1K3",
                "replyto": "21BnT67km3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2422/Reviewer_RfJv"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2422/Reviewer_RfJv"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your reply. While some of my concerns were addressed, I still recommend the authors provide results for non-relu models such as mbv3. At the same time, I partially agree with the comments of the other two reviewers that the difference in high-level idea between the proposed method and OCS is not significant. While this may not be a critical factor for me, I believe it merits thorough discussion in the paper, including the provision of experiments to analyze the quantization noise. I want to keep my rating at borderline acceptance."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2422/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700679626889,
                "cdate": 1700679626889,
                "tmdate": 1700679626889,
                "mdate": 1700679626889,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SprquaO7qj",
                "forum": "ZtlcdjE1K3",
                "replyto": "YkfxY7gJFU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2422/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2422/Authors"
                ],
                "content": {
                    "title": {
                        "value": "OMR is Novel, V.s. OCS"
                    },
                    "comment": {
                        "value": "Dear reviewer  RfJv, \n\nThanks for your valuable feedback. \n\nWe will add experiments for MobileNet-V3 to enhance our paper.\n\nThe difference of OMR and OCS is more significant than what review pf4t said before. **The novelty of OMR has been approved by reviewer wiuF just now.**  \n>Thanks for the answer to my last question. It helps understand OMR shows better expressibility compared to OCS. Therefore, I decided to raise my score to 6.\n\nDetailed analysis is as follows,\n\n## 1. OMR V.s. OCS\n**Our OMR is motivated from OCS to overcome OCS's shortcoming of sacrificing the precision of inner values. Our OMR, for the first time, proposes to save more outliers while preserve the precision of inner values.**\n\n\n\nIt is true that when integer activation is  [2,4,8,16], OMR share the same precision as OCS as reviewer pf4t. \n\nHowever, when input is [2,4,8,15], OCS will split split [2,4,8,15]->[1,2,4,7]+[1,2,4,7] or [2,4,8,15]->[1,2,4,8]+[1,2,4,8], thus there will be **an rounding error** for 15->7\\*2=14 or 15->8\\*2=16, while OMR will split [2,4,8,15]->[1,2,4,8]+[1,2,4,7], thus no error is caused for 15->8+7.\n\nThe main problem is that when the bin's number of inputs is larger than quantization levels, OCS saves outliers by sacrificing the precision of inner values.\n\nFor a more detailed example with the whole fake quantization: x=[0.0, 0.1, 0.2, 0.3, *0.4*, *0.5*, *0.6*, *0.7*], step=0.1, 2-bit, Thus outliers is [0.4,0.5,0.6,0.7].\n- Normal 2-bit: \n  - $x_{int}=clip[round(x/step), 0, 3]=[0, 1, 2, 3, 3, 3, 3, 3]$ quantization levels: 4\n  - $\\hat{x}=x_{int}\\*step=[0.0, 0.1, 0.2, 0.3, 0.3, 0.3, 0.3, 0.3]$ quantization levels: 4\n- OCS 2-bit:\n  - $x_{int}=clip[round((x/2)/step), 0, 3]=[0, 0, 1, 1, 2, 2, 3, 3]$ quantization levels: 4\n  - $\\hat{x}=(x_{int}\\*step)*2=[0.0, 0.0, 0.2,0.2,  0.4, 0.4, 0.6,0.6]$ quantization levels: 4\n- OMR 2-bit:\n  - $x_{int}=concat\\\\{clip[round((x)/step), 0, 3], clip[round((x-0.4)/step), 0, 3]\\\\}=[[0, 1,  2, 3],[0,1,2,3]]$\n  - $\\hat{x}=concat\\\\{x_{int}[0]\\*step, x_{int}[1]*step+0.4]\\\\}=concat\\\\{[0.0, 0.1, 0.2, 0.3], [0.4,0.5, 0.6, 0.7]\\\\}=[0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]$ **quantization levels: 8**\n\nTherefore:\n- **OMR is closer to (here equal to) original fp32 values during fake-quant with larger quantization levels**. \n  - [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7] $\\rightarrow$ [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]\n- OCS sacrifices [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7] $\\rightarrow$  [0.0, 0.0, 0.2,0.2,  0.4, 0.4, 0.6,0.6] during fake-quant.\n\nIn conclusion:\n- OCS does not enlarge quantization levels. OCS saves outliers while sacrifices the precision of inner values.\n- Our OMR, for the first time, proposes to save more outliers while preserve the precision of inner values.\n\n**Comparison visualization is re-polished in supplementary PDF Sec2.**\n\nIf you have any problem, we are online for response by the last second of 22nd November. We are reaching out to see if our response adequately addresses your concerns. We greatly value your comments and are committed to refining our work.\n\nSincerely,\n\nauthors"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2422/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700697479049,
                "cdate": 1700697479049,
                "tmdate": 1700731564158,
                "mdate": 1700731564158,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]