[
    {
        "title": "Few Heads are Enough"
    },
    {
        "review": {
            "id": "Y8SbFOYFze",
            "forum": "EyDPfGy4Wh",
            "replyto": "EyDPfGy4Wh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9177/Reviewer_dyhc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9177/Reviewer_dyhc"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents Expert Projection Attention (EPA). It reduces compute and memory requirement for attention by using MoE layers for values and output projections."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "the method is clearly illustrated and the analysis in 2.3 is helpful for understanding the difference."
                },
                "weaknesses": {
                    "value": "1. The author seems to misunderstand the position of flash-attention, see questions below.\n2. the scale of experiment is small, how would this method generalize to larger models such as llama?\n3. Some experiments have not been finished (Table 4)."
                },
                "questions": {
                    "value": "(1) In what sense flash-attention reduces compute. Do you mean FLOP or wall clock time? FA is exact attention and does not reduce FLOP, it is just a series of clever fusion. If it is wall clock time, then this paper should keep the definition consistent, and provide wall clock time analysis instead of MAC.\n(2) \"Unlike FlashAttention (Dao et al., 2022), it is research-friendly, because it does not hide the internal details of the attention mechanism inside a CUDA kernel.\" is an either arguably wrong or highly subjective judgement to flash-attention."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9177/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9177/Reviewer_dyhc",
                        "ICLR.cc/2024/Conference/Submission9177/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9177/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698441315276,
            "cdate": 1698441315276,
            "tmdate": 1700507494419,
            "mdate": 1700507494419,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "io4MReMXmi",
                "forum": "EyDPfGy4Wh",
                "replyto": "Y8SbFOYFze",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9177/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9177/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for her/his valuable time reviewing our work. Please find our response as follows.\n\n## Clarification on the weaknesses\n\n> 1. The author seems to misunderstand the position of flash-attention, see questions below.\n\nThis is a false accusation (we do not understand what caused the reviewer think of this), as we clarify below.\n\n> 2. the scale of experiment is small, how would this method generalize to larger models such as llama?\n\nIf the reviewer is asking us to run llama scale experiments, that is an unreasonable request.\nThe cost of training llama-scale models is typically estimated as several millions of dollars. This is clearly not feasible for a university lab, nor necessary to show the core principle of an idea: in fact, we already had invested much compute for all these experiments; it is not as if we only reported on small/toy tasks such as Penn TreeBank. We report results on two scales, 47M and 262M parameters, and two different attention mechanisms (Transformer XL and RoPE) on four different datasets, some of them typically used for training much bigger models: Enwik8, Wikitext 103, C4 and peS2o. We think this is more than enough to demonstrate the feasibility of the model, and large companies with access to a sufficient amount of computing power can scale it up if interested.\n\n> 3. Some experiments have not been finished (Table 4).\n\nWe are sorry about this (we had some technical problems with our cluster just before the deadline). The experiments have finished, and we updated Table 4 in the updated PDF.\nThe results confirm that our model outperforms the baseline in every case.\n\n## Response to the questions\n\n> (1) In what sense flash-attention reduces compute. Do you mean FLOP or wall clock time? FA is exact attention and does not reduce FLOP, it is just a series of clever fusion. If it is wall clock time, then this paper should keep the definition consistent, and provide wall clock time analysis instead of MAC.\n\nWe report MACs by consistently following our main baseline paper (MoA, Zhang et al, 2023: Mixture of Attention Heads: Selecting Attention Heads Per Token). MAC is a good metric as it is not implementation-dependent, while the actual wall clock time is highly implementation-dependent. This having said, we suspect that what might have caused the confusion was a sentence in the abstract \u201c...Flash-Attention reduces both compute\u2026\u201d. We admit that this formulation was confusing. We changed it to \u201c...Flash-Attention reduces both run-time\u2026\u201d\n\n> (2) \"Unlike FlashAttention (Dao et al., 2022), it is research-friendly, because it does not hide the internal details of the attention mechanism inside a CUDA kernel.\" is an either arguably wrong or highly subjective judgement to flash-attention.\n\nThis is not at all wrong or subjective.  As we also clarified in our response to Reviewer DLPx (explaining why our method is \u201cresearch friendly\u201d when we also need a CUDA kernel), any modification to the \u201cattention mechanism\u201d in FlashAttention requires proficiency in GPU programming: this is unquestionable.\n\nFor example, consider combining Transformer XL style relative positional encoding with FlashAttention. It requires loading positional encodings with different offsets for each row of the attention matrix, and combining it with the pre-computed query; constructing non-trivial memory access patterns that enable the reuse of positional encodings already loaded in shared memory\u2026 While all of this is doable, it is by no means trivial. Please check the Flash Attention GitHub repository (https://github.com/Dao-AILab/flash-attention) consists of 4500 lines of highly optimized CUDA code (an alternative Triton implementation around 1000 lines also exists).\nIn contrast, our method can be trivially combined with PyTorch implementations of RoPE , Transformer XL style attention, Alibi, softmax1 activation, (and also with Flash Attention), etc.  None of these require any modification to our CUDA kernel, because the CUDA kernel is only used for the V and O projections (and optionally K and Q).\n\n## General comments\n\nWe respectfully note that the provided review is very brief and lacks specific comments/criticisms on our method. This makes it impossible for us to understand the justifications for the scores of \u201cSoundness: 1 poor\u201d, \u201cContribution: 1 poor\u201d and a final score of 3 (while we acknowledge the rather low confidence score of 3). We emphasize that our EPA provides a novel method to significantly reduce both the compute (MACs) and memory of attention in a research-friendly way. As discussed above (under \u201cClarifications on the weaknesses\u201d), we rigorously tested it across various scales, datasets, and attention variants with positive outcomes. In light of this, we kindly request more detailed feedback on our methodology, or specific areas of concern that led to the current evaluation, or please consider revising the scores. Thank you for your consideration."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9177/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700035114935,
                "cdate": 1700035114935,
                "tmdate": 1700067217615,
                "mdate": 1700067217615,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OBVYlNkaHV",
                "forum": "EyDPfGy4Wh",
                "replyto": "Y8SbFOYFze",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9177/Reviewer_dyhc"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9177/Reviewer_dyhc"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors"
                    },
                    "comment": {
                        "value": "Thank you for getting back to the reviews. The reviewer is happy to see all experiments are finished now. \n\nGiven the current updates, the main question that affects whether the reviewer can raise the score is: Does the method achieve wall clock time speedup? \n\nMACs is not the correct measure, as reviewer obUv points out. Alternatively, if you don't think the current implementation leads to wall clock time speedup, you should give a reasonable implementation draft/plan that can lead to wall clock time speed up. If you do not have this plan in mind, you should change your position, and write something like \"future work can improve on this, given the MAC is theoretically lower.\" If this is the case, you should reconsider why you are mentioning flash attention, the reviewer is happy to review again if you decide to reposition."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9177/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700085260530,
                "cdate": 1700085260530,
                "tmdate": 1700087509620,
                "mdate": 1700087509620,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Xbxf0ZACIM",
                "forum": "EyDPfGy4Wh",
                "replyto": "YJ5hd7bwNI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9177/Reviewer_dyhc"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9177/Reviewer_dyhc"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for getting back to this important question. I have raised the score because the implementation has achieved wall clock time speedup, and has the potential to achieve more."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9177/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700247506413,
                "cdate": 1700247506413,
                "tmdate": 1700247506413,
                "mdate": 1700247506413,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9l1t1ELGkw",
                "forum": "EyDPfGy4Wh",
                "replyto": "Y8SbFOYFze",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9177/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9177/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you very much for the score update. Now naturally, we'd like to know the reasons/justifications why the reviewer still votes for borderline reject. \n\nBased on our discussion above, it seems that we thoroughly addressed and resolved all the concerns the reviewer had initially raised. We would like to respond to any remaining concerns. Thank you."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9177/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700249611707,
                "cdate": 1700249611707,
                "tmdate": 1700249925898,
                "mdate": 1700249925898,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Hcubt5f2bH",
                "forum": "EyDPfGy4Wh",
                "replyto": "9l1t1ELGkw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9177/Reviewer_dyhc"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9177/Reviewer_dyhc"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the question. This is a timely score update to reflect the fact that the important concern of wall clock time speedup has been addressed. I will go over all details again over tonight/weekend and summarize questions not yet been addressed."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9177/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700249915118,
                "cdate": 1700249915118,
                "tmdate": 1700249915118,
                "mdate": 1700249915118,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7R7BhAmF2j",
                "forum": "EyDPfGy4Wh",
                "replyto": "9l1t1ELGkw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9177/Reviewer_dyhc"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9177/Reviewer_dyhc"
                ],
                "content": {
                    "title": {
                        "value": "Response to Authors"
                    },
                    "comment": {
                        "value": "Hi there, thanks for the patience, the reviewer has re-read the paper again given the rebuttal, and some of the initial concerns are happily addressed.\n\nOne remaining important question, I have been thinking for a while, and finally find out is the weird position of flash attention. Concretely:\n(1) You are using flash-attention as a comparison from the very beginning of the paper, but your method is not comparing with it. If I understand correctly, your method has nothing to do with flash attention, and you could have started the paper with something else as motivations, such as MoA, or any MoE style attention. Motivating with flash attention seems highly irrelevant.\n(2) You seem to have very negative wordings against flash-attention, concretely, e.g. \"because it does not hide the internal details of the attention mechanism inside a CUDA kernel\". You have put flash-attention in a very awkward position, and I don't know whether this is your intention.\n(3) your accusation to flash-attention is still subjective, and I couldn't agree on your statement in last rebuttal, e.g. \"it does not allow a researcher to modify the attention mechanism itself (which is hard-coded within their CUDA kernel), while our method still leaves such a possibility (since our CUDA kernel only replaces the linear projection layers to obtain query/key/values).\"\n\nThis is not a scientific statement, there is no measure on how hard it is to modify a codebase. The closest measure is probably LoC. You can say you sample 20 papers modified based on flash-attention, and how many LoC they need, with a comparison to your method. In addition, you seems to be very confident on the difficulty on \"which requires proficiency in GPU programming\". Firstly, I don't know how to judge this difficulty. Do you want to randomly sample x people in this field and count how many percent of them don't know GPU programming? Secondly, this is wrong. There are (1) other implementation of flash-attention, e.g. Triton, that allows modification as easy as near PyTorch Level. (2) work on modifying attention pattern that does not change the source code of flash attention, e.g. longnet[1].\n\nAgain, the reviewer is trying to improve the paper, and have spent a lot of time on this. If you want to keep the flash attention flow, please be scientific. If you agree with my argument, please simply delete these sentences against flash attention. That will make the paper flow much better.\n\n[1] LONGNET: Scaling Transformers to 1,000,000,000 Tokens."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9177/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700466728490,
                "cdate": 1700466728490,
                "tmdate": 1700466728490,
                "mdate": 1700466728490,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "grNgJHQv0x",
                "forum": "EyDPfGy4Wh",
                "replyto": "Gx5tu2nnww",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9177/Reviewer_dyhc"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9177/Reviewer_dyhc"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Authors"
                    },
                    "comment": {
                        "value": "I have looked over on the new paper. This is much better and clearly.\n\nSummary to AC: there are three main concerns (1) flash attention position, (2) wall clock time confusion versus MACs, (3) experiment scale not large enough like llama.\n\nThe authors have addressed (1) and (2). I can agree on (3) that pretraining Llama scale is not practical, and given the number of dimension of experiments targeted on, I am fine with (3).\n\nI have raised my score to 6 to reflect this. \n\nFinal suggestion to Author:\n    In a strong paper, (3) should be addressed. In the future, you should consider how to design the method with fine-tuning paradigm. E.g, check how GQA paper positions this. There are ways to design new models, and reusing pretraining weights. In that case, your paper values will be much clearer."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9177/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700507426157,
                "cdate": 1700507426157,
                "tmdate": 1700507426157,
                "mdate": 1700507426157,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hYfed5yTWB",
                "forum": "EyDPfGy4Wh",
                "replyto": "WLkapwMNoD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9177/Reviewer_dyhc"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9177/Reviewer_dyhc"
                ],
                "content": {
                    "comment": {
                        "value": "thanks! good comment."
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9177/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700588395370,
                "cdate": 1700588395370,
                "tmdate": 1700588395370,
                "mdate": 1700588395370,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ZdjmMUtzMD",
            "forum": "EyDPfGy4Wh",
            "replyto": "EyDPfGy4Wh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9177/Reviewer_obUv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9177/Reviewer_obUv"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a method for selecting one head but keeping a small number of Q,K and V matrix.\nI really feel this paper is quite poorly written. First of all, the notations are extremely unclear especially in the method section. The authors have a schematic in\nFigure 1 but it's unclear what this schematic means. There is no explanation of different boxes.\n\nThe authors constantly compare to FlashAttention, they have not even a single run time comparison. \nI think the paper needs a complete re-write. The method section is using non-standard notation where is unclear what dimensions they are reducing to. The experiments do not talk about fine-tuning overheads, do not have a single timing results."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The idea at a high level looks decent. However, the poor writing and underwhelming evaluation really makes it hard to appreciate it."
                },
                "weaknesses": {
                    "value": "Please see the summary."
                },
                "questions": {
                    "value": "Please see the summary."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9177/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9177/Reviewer_obUv",
                        "ICLR.cc/2024/Conference/Submission9177/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9177/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698778448716,
            "cdate": 1698778448716,
            "tmdate": 1700485069302,
            "mdate": 1700485069302,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "byeF31heaT",
                "forum": "EyDPfGy4Wh",
                "replyto": "ZdjmMUtzMD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9177/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9177/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We would like to thank the reviewer for his/her valuable time.\n\nWe are surprised by the reviewer\u2019s comment on the quality of writing (\u201cthe poor writing and underwhelming evaluation really makes it hard to appreciate it.\u201d, \u201cneeds complete re-write\u201d). In contrast, Reviewer DLPx highlights the clarity as one of the strengths of the paper in his/her high-quality review, and even Reviewer dyhc states that \"the method is clearly illustrated\". We\u2019d like to hear more specific clarifications/suggestions on what confused the reviewer (including the comment regarding the notation).\n\nWe would like to point out that the reviewer\u2019s summary does not reflect what our algorithm does. We have a *few* heads (not 1), and the value and output projections (not \u201cQ, K, and V\u201d) are a mixture of experts.\n\nRegarding the timing results, we report the commonly used metric which is the number of \u201cMultiply-Accumulated (MAC) operations\u201d required for each method. We do this by following our main baseline paper (MoA, Zhang et al, 2023: Mixture of Attention Heads: Selecting Attention Heads Per Token). MAC is a good metric as it is not implementation-dependent. FlashAttention does not change the number of MAC operations, so it is the same as the baseline dense Transformer. If the reviewer is curious about the wall-clock time of our current implementation, we also measured the resource usage for both the baseline Transformer and EPA: please refer to Appendix 4 of our updated manuscript for more details. In short, EPA with the current (suboptimal) implementation is already capable of accelerating the entire training pipeline by a factor of 1.5-1.6, and reducing the total memory usage by about the same factor.\n\nWe kindly ask the reviewer to clarify what \u201cfine-tuning overheads\u201d we should talk about. In fact, because our method accelerates the Transformer architecture, it accelerates both the training and fine-tuning phases.\n\nOverall, we find the current review very brief, and lacks technical comments/criticisms about our method. The reviewer rates our work as 3 (reject) with a high confidence score of 4; this implies responsibility. We respectfully ask for much more specific criticisms and justifications for such a rating."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9177/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700034676341,
                "cdate": 1700034676341,
                "tmdate": 1700034676341,
                "mdate": 1700034676341,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cmKT1qHC6e",
                "forum": "EyDPfGy4Wh",
                "replyto": "byeF31heaT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9177/Reviewer_obUv"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9177/Reviewer_obUv"
                ],
                "content": {
                    "title": {
                        "value": "Clarifications."
                    },
                    "comment": {
                        "value": "First of all my apologies, I should have been more specific. I am sure different reviewers are entitled to their own opinion. I certainly believe you need to reposition this paper.  Let me be specific in my criticism. \n\n1. Figure 1,  unclear what you mean. There is limited text to support. For the figure I will expect a block of text describing what it means. \n\n2.  Criticism around notation - I followed your description till equation 10, then you say - \" As we\u2019ll show, it is not necessary to make all projections MoEs. In Section 3.1 we show that keeping\na single copy of the projections Q and K and reusing them for all experts is beneficial. We call this\nmethod Expert Projection Attention\", which to me means equation 10 might not be what actually you are using. Leaving me unable to understand what exactly do you mean and a proper definition.\n\n3. Regarding MAC operations and not runtime as a metric - We have time and time seen MAC operations are not a substitute for actual runtime operation, whether it be pruning, quantization or compression In Appendix 4 you have not detailed what is the baseline implementation you are using. What codebase is it ? Without that those numbers mean little. \n\n4. Fine-Tuning - My understanding is that you will need to perform some amount of fine-tuning before you can use your selected method, what is the overhead regarding this. \n\n5. Finally - Other reviewers have made a point about comparison regarding Flash Attention and comparison with it. I have similar concerns."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9177/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700080356140,
                "cdate": 1700080356140,
                "tmdate": 1700080356140,
                "mdate": 1700080356140,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "r8AT7kNQjs",
                "forum": "EyDPfGy4Wh",
                "replyto": "ZdjmMUtzMD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9177/Reviewer_obUv"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9177/Reviewer_obUv"
                ],
                "content": {
                    "title": {
                        "value": "Thanks"
                    },
                    "comment": {
                        "value": "Thanks for the clarification. \n\nI appreciate the clarification. But it brings back the question. MACs are not a great metric. It depends heavily on the hardware. \n\n\"Question regarding fine-tuning\" :  I am asking a very simple question, to use your method on say a model we would need to first perform some amount extra training or not ?\n\nDespite all three reviewers pointing out how your method is different than the goal of flash attention, i find it interesting that you are not willing to change your position. You have a approximation based method which prunes the number of heads in transformer layer with gating (at a high level), flash attention concerns with build a hardware friendly implementation of exact attention operation. Two very different things in my opinion (of course you can disagree), which is why all reviewers have been pointing out that there is fundamental difference. \"Research Friendliness\" is a subjective metric. I find it very hard to digest the positioning of your method."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9177/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700260814240,
                "cdate": 1700260814240,
                "tmdate": 1700261118408,
                "mdate": 1700261118408,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hRaNXRMNlR",
                "forum": "EyDPfGy4Wh",
                "replyto": "09sSDYH1L6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9177/Reviewer_obUv"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9177/Reviewer_obUv"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the clarifications."
                    },
                    "comment": {
                        "value": "Thanks for the clarifications and my apologies, regarding misconstruing your method to be pruning method. I understand that you are proposing architectural changes rather than fine-tuning or pruning. \n\nI don't think we will agree with MAC's. In my experience reduction in MAC's does not mean gain in runtime.  A clear example is Sparse Networks like those generated by Lottery Ticket Hypothesis. They provide massive reduction in FLOPs but almost no benefit in runtime. Therefore optimizing for MACs I think is not fruitful. \n\nI understand what your method does now. Thanks for the clarification.  Do you think a more fair comparison would have been multi-query attention and grouped-query attention. [1]\n\nNow coming to the question of me referring to other reviewers arguments. This is a standard practice in any reviewing. We collaborate over different points, you make it sound that it is not acceptable to you (I do not agree with your response). \n\n[1] https://arxiv.org/abs/2305.13245\n\nNow based on some of you additional experiments showing runtime improve I am bumping the score. But I still think your paper needs a lot repositioning and additional comparison with say grouped-query attention etc. I also hope rather than being belligerent you understand the position and use the review as a vessel to improve the paper."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9177/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700484976037,
                "cdate": 1700484976037,
                "tmdate": 1700484976037,
                "mdate": 1700484976037,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1UrFDA2P7D",
                "forum": "EyDPfGy4Wh",
                "replyto": "uTRvWaOK6J",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9177/Reviewer_obUv"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9177/Reviewer_obUv"
                ],
                "content": {
                    "title": {
                        "value": "Thanks"
                    },
                    "comment": {
                        "value": "I have consistently held my position against MAC's.\n\n```Regarding MAC operations and not runtime as a metric - We have time and time seen MAC operations are not a substitute for actual runtime operation, whether it be pruning, quantization or compression```. (First Comment) and you spent a bunch of time justifying MACs. It is on the insistence of the reviewers you updated your manuscript . Which I have acknowledged and based on that have updated the score."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9177/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700501643779,
                "cdate": 1700501643779,
                "tmdate": 1700501643779,
                "mdate": 1700501643779,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "HvwP4taCax",
            "forum": "EyDPfGy4Wh",
            "replyto": "EyDPfGy4Wh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9177/Reviewer_DLPx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9177/Reviewer_DLPx"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a modification to the attention mechanism by incorporating a mixture of experts in both the source (K, Q) and destination (V, O) projections. This modification enables the selection of fewer active heads, thereby reducing computational and memory costs during both training and inference. The paper is based on the premise that not all attention heads are necessary for a given task. By utilizing an expert to select the required heads, it is possible to decrease computation and memory expenses. The effectiveness of this algorithm is demonstrated by comparing its accuracy to that of the dense counterpart and by visualizing the attention matrices."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper is well-written and effectively highlights the issues with the current attention architecture in terms of computational and memory demands.\n2. The paper conducts experiments on various datasets and compares its results with existing baseline methods, including MOA.\n3. The paper conducts a thorough analysis of attention maps to facilitate a qualitative study and comparisons with conventional attention matrices."
                },
                "weaknesses": {
                    "value": "1. The paper refers to FlashAttention multiple times and compares against their CUDA kernel (SW designed to exploit HW efficiently) optimization vs algorithmic insight in this paper. I am not sure if its an apple-to-apple comparison since there are tons of other literature for transformers which aim to reduce computation/memory cost (like quantization/sparsity methods) and the paper doesn\u2019t compare against these.\n2. While authors compare against FlashAttention custom kernel implementation and mention that as a drawback, EPA algorithm itself requires a custom CUDA kernel with its own set of restrictions (pointed in the results section).\n3. For the EPA algorithm, the paper mentions that K/Q source experts are not necessary for good results and only output/value experts are required, which seems to contradict the disadvantages shown in 2.2 naive algorithm."
                },
                "questions": {
                    "value": "1. Can the authors compare against architectures other than TransformerXL? It is not evident from the text why only 1 architecture is chosen for comparison?\n2. It is not evident from the paper how nHead is chosen for a task. Most results demonstrated fixed the nHead to be 2 or 4. Did the authors perform smaller experiments to first search for optimal nHead before scaling up?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9177/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698794030470,
            "cdate": 1698794030470,
            "tmdate": 1699637154379,
            "mdate": 1699637154379,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "CwSAXm8Jq8",
                "forum": "EyDPfGy4Wh",
                "replyto": "HvwP4taCax",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9177/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9177/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We would like to thank the reviewer for the insightful review, and for positive comments on the clarity and methodology of the paper. Please find our responses as follows:\n\n## Clarification on the weaknesses\n\n> The paper refers to FlashAttention multiple times and compares against their CUDA kernel (SW designed to exploit HW efficiently) optimization vs algorithmic insight in this paper. I am not sure if its an apple-to-apple comparison since there are tons of other literature for transformers which aim to reduce computation/memory cost (like quantization/sparsity methods) and the paper doesn\u2019t compare against these.\n\nWe\u2019d like to clarify one very important point regarding the goal of this work. The general goal of typical research on efficient attention is to make/train/evaluate Transformer models faster. Here we emphasize that such methods also have another important potential consequence in **accelerating research** on attention based models (by making them resource efficient). We consider this as an extremely important aspect today, potentially allowing for \u201cresource-limited\u201d organizations (e.g., in academia) to still conduct research on reasonably large models.\nHowever, for a \u201cfast attention\u201d method to be effective in accelerating research, it also has to be \u201cresearch friendly\u201d: certain existing methods do not allow modifications of certain aspects of attention mechanisms, while others may be more flexible. Such flexibility is required for \u201cresearch friendliness\u201d.\n\nThis perspective is at the heart of our comparison with FlashAttention or with any other methods. We refer to FlashAttention simply as an example of a non-flexible acceleration method (and because it is arguably the most popular \u201cfast attention\u201d method today): it does not allow a researcher to modify the attention mechanism itself (which is hard-coded within their CUDA kernel), while our method still leaves such a possibility (since our CUDA kernel only replaces the linear projection layers to obtain query/key/values)---This should also bring an answer to the reviewer\u2019s second point: \n> While authors compare against FlashAttention custom kernel implementation and mention that as a drawback, EPA algorithm itself requires a custom CUDA kernel with its own set of restrictions\n\nFor example, FlashAttention can **not** be used with certain positional encoding (e.g. XL-style relative positional encoding) or activation functions (e.g. geometric attention). In contrast, our method can be trivially combined with RoPE, Transformer XL style attention, Alibi, (and also with Flash Attention), etc. None of these require any modification to our CUDA kernel.\n\n> For the EPA algorithm, the paper mentions that K/Q source experts are not necessary for good results and only output/value experts are required, which seems to contradict the disadvantages shown in 2.2 naive algorithm.\n\nWe are not sure to understand what \u201ccontradiction\u201d the reviewer refers to. Could you please clarify?\n\n## Response to the questions\n\n> Can the authors compare against architectures other than TransformerXL? It is not evident from the text why only 1 architecture is chosen for comparison?\n\nWe\u2019d like to draw the reviewer\u2019s attention to the fact that our experiments already include an architecture other than TransformerXL: standard (pre-layer norm) Transformer with RoPE positional encoding in Sec. 4, page 8.\nThe main reason why we chose Transformer XL as our main architecture is that it has significantly better performance with a moderately low context window size (we use context windows of 256 for small models and 512 for big). For RoPE, we double the context window size of the XL transformers (512 for small, 1024 for big) to obtain competitive models (and even in this case, they underperform the XL models).\n\n> It is not evident from the paper how nHead is chosen for a task. Most results demonstrated fixed the nHead to be 2 or 4. Did the authors perform smaller experiments to first search for optimal nHead before scaling up?\n\nNo, that is not how we chose $n_{head}$. Our search for $n_{head}$ works as follows: we start with $n_{head}=2$, and if the resulting model performs worse than the baseline, we double it to $n_{head}=4$. We never went beyond $n_{head} > 4$, as 2 or 4 was always enough to obtain good models. We added the corresponding explanation in the updated PDF in Appendix 2.1. Thank you for pointing this out.\n\n\nWe believe we\u2019ve addressed all the concerns raised by the reviewer. If you find our response convincing, please consider increasing the score. Otherwise, we\u2019ll be happy to discuss and/or clarify further."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9177/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700034559361,
                "cdate": 1700034559361,
                "tmdate": 1700067153820,
                "mdate": 1700067153820,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]