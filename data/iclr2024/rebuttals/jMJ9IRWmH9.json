[
    {
        "title": "Privacy Preserving API Fine-tuning for LLMs"
    },
    {
        "review": {
            "id": "McwBDInL9K",
            "forum": "jMJ9IRWmH9",
            "replyto": "jMJ9IRWmH9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5437/Reviewer_awQz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5437/Reviewer_awQz"
            ],
            "content": {
                "summary": {
                    "value": "Fine-tuning LLMs via API is a new trend in which users send their data to a server and let the server do the fine-tuning. This paper assumes the samples are pairs of features and labels \\{(x, y)\\}, and studies how to protect the privacy of the labels. The server is assumed to provide two API functions. The first is a forward function that returns activations ($h$). After receiving the activations, users compute the loss ($l$) by themselves and send $\\partial l/\\partial h$ to the server. The server then use the second backward API function that uses $\\partial l/\\partial h$ to do backpropagation to compute the gradients. The authors propose two empirical ways to prevent the server from inferring the labels from activations and gradients. Unfortunately, I have several concerns regarding the protection effectiveness."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.Privacy-preserving API fine-tuning is an important problem and is very challenging due to the two-party learning nature. This paper provides some preliminary exploration towards solving this problem.\n\n2.The ideas borrow some insights from the secure multi-party aggregation literature and are intriguing."
                },
                "weaknesses": {
                    "value": "1.Regarding the privacy-preserving backpropagation. Although $\\partial l/\\partial h$ is protected, the server still has clean $\\partial h/\\partial \\theta$. This still leaks information about the label. The 2-dimensional TSNE of $\\partial h/\\partial \\theta$ should be similar to $\\partial l/\\partial \\theta$, because multiplying $\\partial l/\\partial h$ is only a linear operator.\n\n2.Regarding the privacy-preserving forward. If the label can be predicted via only linear transformations of $h_i$, then it means the unaggregated $\\{h_i\\}$ leaks a lot of information about the label. E.g., the server can simply run some clustering algorithms, which will achieve good performance because $\\{h_i\\}$ are linearly separable w.r.t. the labels."
                },
                "questions": {
                    "value": "See above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5437/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697939883375,
            "cdate": 1697939883375,
            "tmdate": 1699636552760,
            "mdate": 1699636552760,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "EEkViAG9r7",
                "forum": "jMJ9IRWmH9",
                "replyto": "McwBDInL9K",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5437/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5437/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the review! Please find our response to your comments below:\n\n> Regarding the privacy-preserving backpropagation. Although $\\partial l / \\partial h$ is protected, the server still has clean $\\partial h/\\partial \\theta$. This still leaks information about the label. The 2-dimensional TSNE of $\\partial h/\\partial \\theta$ of should be similar to $\\partial l/\\partial\\theta$, because multiplying is only a linear operator.\n\nFor this concern, we agree that $\\partial h/\\partial \\theta$ is similar to $\\partial l/\\partial\\theta$. For this reason, we report $\\partial l/\\partial\\theta$ in Figure 3 ( in addition to $\\partial l / \\partial h$ before that). There, we observe that $\\partial l/\\partial\\theta$ is also obfuscated through private backpropagation. Please also note that while $\\partial l/\\partial\\theta$ and $\\partial h/\\partial\\theta$ are indeed linearly dependent, there is a  distinct linear dependency **for each sample and at each training step**: as a result, simply trying to recover them with TSNE is unlikely to be fruitful.\n\nIf there is any specific experiment we can run to check your hypothesis, we are happy to discuss that. \n\n\n> Regarding the privacy-preserving forward. If the label can be predicted via only linear transformations of $h_i$, then it means the unaggregated $h_i$ leaks a lot of information about the label. E.g., the server can simply run some clustering algorithms, which will achieve good performance because $h_i$ are linearly separable w.r.t. the labels.\n\nWe split this concern into two statements:\n\n1. If the label can be predicted via only linear transformation, then $h_i$ leaks information about the label\n\n2. If 1. is true, the server can run clustering that will achieve good performance for separating labels\n\nThe first statement is true: **if** the label can be predicted via linear transformation, then $h_i$ leaks labels **in the information-theoretic sense**. However, in our case **the label cannot be predicted with a linear transformation**. The algorithm described in Section 3.3 explicitly trains $h_i$ so that no fixed linear transformation can recover labels from them. This idea is formulated and discussed in the first half of page 7.\n\nFurthermore, even the label could be predicted linearly, this would not guarantee that a clustering algorithm would be able to recognize it. To illustrate this point, consider a uniform ball of data points with a radius of 1 that contains points with two labels. All points with label 1 are above a certain (e.g., random) separating hyperplane, and all points with label 0 are below that hyperplane. Therefore, the data is linearly separable. However, no clustering algorithm can distinguish between the correct hyperplane and any other hyperplane without knowing labels. Without labels, the data is a uniform ball that can be split equally well in any direction. \n\nIn preliminary experiments, we found that 2-cluster k-means has near-random performance for both P$^3$EFT and baseline methods. We will add these results in the revised version of the paper."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5437/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700574511668,
                "cdate": 1700574511668,
                "tmdate": 1700574511668,
                "mdate": 1700574511668,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "lBcY8JQ80g",
            "forum": "jMJ9IRWmH9",
            "replyto": "jMJ9IRWmH9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5437/Reviewer_DV4T"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5437/Reviewer_DV4T"
            ],
            "content": {
                "summary": {
                    "value": "The authors search for a way to fine-tune models over an API while keeping the labels private. The authors analyze the privacy of popular algorithms for parameter-efficient fine-tuning when training over an API."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "- The topic of privacy-preserving LLM is timing and essential."
                },
                "weaknesses": {
                    "value": "- No clear security model.\n- The idea seems to be wrong."
                },
                "questions": {
                    "value": "The reviewer has major concerns about the correctness of the idea. \n\n- In Section 3, the authors claimed that \"formulate a protocol for two-party\" in the 1st sentence. In the abstract, \"the client ..., and the server ....\" A client and a server constitute \"two-party\" already. However, Equation 2 in Section 3.2 contains \"two identical independent servers that offer backprop API.\" The number of parties is not corresponding. \n\n- As for the formulation, it looks like an application of the $n$-out-of-$n$ secret-sharing scheme. In particular, Equation 2 is essentially similar to Part 2 in [REF1]. Additionally, secret-shared backpropagation has already been solved in the early work [REF2].\n\n[REF1] https://www.cs.columbia.edu/~tal/4261/F19/secretsharingf19.pdf\n\n[REF2] Mohassel, Payman, and Yupeng Zhang. \"Secureml: A system for scalable privacy-preserving machine learning.\" 2017 IEEE symposium on security and privacy (SP). IEEE, 2017.\n\nCould the authors explicitly formulate the security model?\nCould the authors explain the difference between the proposed formulation and secret sharing?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N.A."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5437/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5437/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5437/Reviewer_DV4T"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5437/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698585345703,
            "cdate": 1698585345703,
            "tmdate": 1699636552652,
            "mdate": 1699636552652,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "s2igPMZx9B",
                "forum": "jMJ9IRWmH9",
                "replyto": "lBcY8JQ80g",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5437/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5437/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their feedback; we do our best to address their concerns below. We also ask the reviewer for clarifications on several replies.\n\n> No clear security model.\n\nWe firmly believe that we have provided a detailed portrayal of the attacker model in section 3.1 (i.e., \u201chonest, but curious\u201d model with details); however, we are open to improving it further. If there are any specific aspects that require clarification, kindly provide further details so that we can address them in the revised version.\n\n> Equation 2 in Section 3.2 contains \"two identical independent servers that offer backprop API.\" \n\nThank you for pointing out this oversight! We will update the submission to clarify that we consider two setups: one with a single server and the other with multiple servers. We will also specify which setup we are addressing in each specific part of the article.\n\n> In particular, Equation 2 is essentially similar to Part 2 in [REF1]. \n\nWe thank the reviewer for drawing our attention to the technique of secret sharing, and we agree that it has similarities with Equation 2. However, we believe that there is still a significant difference between these methods. Please see the upcoming General Response for details.\n\n> Additionally, secret-shared backpropagation has already been solved in the early work [REF2].\n\nWe agree that the algorithm proposed in [REF2] does address the challenges of private forward and backward passes in the case of two servers. However, in our opinion, their solution may not be suitable for the scenario of fine-tuning models via APIs. This article uses MPC-friendly approximation of softmax, but trains the model from scratch. Nevertheless, the original model was pretrained with original softmax, and in order to fine-tune it using their method, it would be necessary to replace softmax with an MPC-friendly approximation. However, in general, replacing a layer does not guarantee equivalent results, and the authors of the paper did not specifically investigate this particular scenario of fine-tuning.\n\nFurthermore, in their case, the algorithm always requires **two servers**, whereas our method, with slight modifications, operates effectively with **just one server** (second part of section 3.2)."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5437/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700576087340,
                "cdate": 1700576087340,
                "tmdate": 1700577030058,
                "mdate": 1700577030058,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "c1yIVl8g9d",
            "forum": "jMJ9IRWmH9",
            "replyto": "jMJ9IRWmH9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5437/Reviewer_7KRM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5437/Reviewer_7KRM"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed to fine-tune models over an API with privacy requirement on labels. Under a parameter-efficient fine-tuning framework, the paper analysed the possible ways the label information can be leaked, i.e., from gradients or intermediate activations. Experiments justified that the proposed method can defend against recent attack studies."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper is well presented and the targeted privacy-preserving in tuning/training is an interesting research topic. The authors have reviewed some recent advanced works, especially the ones related LLMs. The methodology is clearly stated, and the experimental results are basically convincing."
                },
                "weaknesses": {
                    "value": "My major concerns are two folds; one is the practical significance of the problem setting and connection to some related topics, and the other concern is novelty of methodology. Please see my detailed comments under Questions."
                },
                "questions": {
                    "value": "1.\tFrom my understanding, the connection of the problem setting with vertical federation learning is contrived in terms of predictive tasks. But I agree that in some scenarios, labels are valuable and privacy preserving might be necessary. In this sense, how about local differential privacy on labels or noisy label learning? Because they are also regarded as solutions to preserving labels. There should be at least some discussion on telling the readers what the advantages of the proposed method are over these existing strategies. \n2.\tFollowing 1, with access to the full features of target domain, this work is also related to source-free domain adaptation. I understand the applied loss takes label in this work and thus should be more informative than UDA. It would be better if the necessity of using labels could be clarified.\n3.\tThere is not much referring to the \u201clocal layers\u201d in Fig. 1. Are these layers learnable or fixed? Can you explain why it is rational to be learnable/fixed for clients in real scenarios?\n4.\tWhen taking about fine-tuning APIs in the paragraph 3-4, I think some recent works are missing, especially from the privacy preserving motivation.\n\n       [1] Earning Extra Performance from Restrictive Feedbacks, 2023\n       [2] Offsite-Tuning: Transfer Learning without Full Model, 2023\n\n5.\tFrom my understanding, the technique on gradient privacy preserving is based on zero-order optimization and the random weights for activation is like a code book maintained locally. Can you explain what the differences/novelties are compared to previous work in terms of the two techniques?\n6.\tIf an adversary knows how $ z$ is sampled and $g_h$ could be exposed via sum even the norm of $z$ is large. Noticed n parallel calls has been used as a workaround, it would be better if the cost and benefits trade-off is provided.\n7.\tPresentation issues. The last paragraph of page 3, $h$ is not well presented. $h\u2019$ is used in Fig. 1 while it is $h^*$ in the main text."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5437/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698748159732,
            "cdate": 1698748159732,
            "tmdate": 1699636552531,
            "mdate": 1699636552531,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GIU1x9JE1m",
                "forum": "jMJ9IRWmH9",
                "replyto": "c1yIVl8g9d",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5437/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5437/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> In this sense, how about local differential privacy on labels or noisy label learning? \n\n\nThank you for pointing out this oversight. We fully agree that the paper will benefit from including a discussion and comparison of our method with techniques based on noisy labels and differential privacy.\n\nPrevious work has shown that gradients with respect to activations are a very vulnerable aspect of split learning in terms of privacy. Techniques such as noisy labels and differential privacy are used to overcome this very problem [1,2]. However, these techniques rely on using noisy gradients instead of the true gradients, which can lead to a decrease in final quality (see e.g. non-private baselines in [3], [4]).\n\nIn contrast to previous work, parameter-efficient fine-tuning does not require computation of gradients with respect to **all parameters**. We only need to update a very small **subset of parameters**, therefore it is needed for the client to know true gradients just for that subset. This allows us to compute true gradients in a completely private manner without decreasing final quality.\n\n\n> Following 1, with access to the full features of target domain, this work is also related to source-free domain adaptation. I understand the applied loss takes label in this work and thus should be more informative than UDA. It would be better if the necessity of using labels could be clarified.\n\n\nThe majority of language models have been pretrained either on a masked word prediction or causal language modeling objective. As a result, their pretrained activations are not suitable for classification tasks such as sentiment classification or predicting logical relations between statements. We can provide one of our experiments as evidence for this claim: as a baseline, we trained only the classification head without adapters, and the results were significantly worse (Figure 4). Thus, we believe that unsupervised learning may not be very effective for this task. If you know of any results that show otherwise, we would greatly appreciate it if you could point us to them so that we can compare our method with them.\n\n\n\n> There is not much referring to the \u201clocal layers\u201d in Fig. 1. Are these layers learnable or fixed? Can you explain why it is rational to be learnable/fixed for clients in real scenarios?\n\n\nThese layers are learnable, they correspond to a standard learnable classification head for BERT fine-tuning. Running those computations takes negligible time compared to LLM and thus can be performed even by general purpose computers without dedicated accelerators (e.g. GPUs). Therefore, we believe that there is no need to freeze these layers as it would reduce the number of trainable layers and thus the overall model capacity. Additionally, to the best of our knowledge, training only the head is a standard solution for fine-tuning BERT for classification tasks. \n\n> When taking about fine-tuning APIs in the paragraph 3-4, I think some recent works are missing, especially from the privacy preserving motivation.\n\n\nWe appreciate your bringing these articles to our attention. We acknowledge that both of these methods are potentially much less vulnerable to potential attacks than our method, but they do have some drawbacks which we will discuss below.\n\nFrom our perspective, the first article presents a promising approach for our problem setting, but there may be issues with its application due to the large number of forward and backward passes required to obtain gradient estimates, which increases compute cost.\n\nThe second article also proposes an elegant method that is fully private for the client by design. This method still requires running a distilled model on the client side. However, the authors explicitly state in their limitations that they used a distilled model size that was not smaller than one-third of the size of the original model. This is still incredibly challenging for clients due to the large memory and computational resources required for training. In contrast, our method only requires the client to perform forward and backward passes through the head layers, which poses no problem. Therefore, we believe that our method is potentially more scalable."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5437/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700577138683,
                "cdate": 1700577138683,
                "tmdate": 1700577466817,
                "mdate": 1700577466817,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FpQDbccJ8t",
                "forum": "jMJ9IRWmH9",
                "replyto": "onkLLDRUxB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5437/Reviewer_7KRM"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5437/Reviewer_7KRM"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the response"
                    },
                    "comment": {
                        "value": "I would appreciate the author's rebuttal, which will help improve the overall quality of this paper. Most of my concerns are addressed. \n\nRegarding the zero-order optimisation, I meant Eq. 2 which recovers the gradient of $g_h$ should be an application of ZOO. Please correct me if I am wrong. So far, I will maintain my score because I think this paper should be further improved, especially from the privacy aspect."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5437/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700722797474,
                "cdate": 1700722797474,
                "tmdate": 1700722797474,
                "mdate": 1700722797474,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "9SIcok1ejO",
            "forum": "jMJ9IRWmH9",
            "replyto": "jMJ9IRWmH9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5437/Reviewer_aRLa"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5437/Reviewer_aRLa"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on the problem of preserving the privacy of client\u2019s training labels while using fine-tuning APIs.  This paper proposes a fine-tuning protocol that performs Low-Rank Adaptation (i.e., a parameter-efficient fine-tuning) in a setting where clients hold private labels and aim to finetune a model owned by a server without disclosing the labels of examples. The server provides forward and backward passes on their model. The proposed method and its description are very confusing, please see my understanding and comments in the weakness box."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The problem of preserving privacy while using fine-tuning APIs is an important problem particularly for large language models given that 1) many recent models are not released, but instead made available as proprietary services; 2) the local resources of clients are limited for fine-tuning."
                },
                "weaknesses": {
                    "value": "My main concern is that the description of the proposed method is confusing and missing lots of information. Figure 2 (which is supposed to be a visualisation of the proposed framework) makes it even more confusing by introducing new variables that were never used in the description. I have spent some time trying to understand and guess the missing information. See below my understanding of the proposed method: \n1) a client has local adapters and initializes them locally. How this initialization is done? I can think of two scenarios: 1) the initialization is done randomly; or 2) the initialization is done by copying the weights of adapters owned by the server. Scenario 2 does not make sense because this paper discusses that servers do not want to send their model to clients. Scenario 1 does not make sense either as in step 5 clients use the gradients w.r.t. the server adapter parameters.\n2) a client calls forward API call to compute features on each mini-batch of their data. It is not clear how these features are computed. I can think of three different scenarios: 1) the server has both pre-trained model and adapters so the server computes these features as the summation of the output of both of these modules' 2) the server uses only the pre-trained model to compute these features; or 3) the server uses only the adapters to compute these features. \n2) a client passes these features to the local \u201chead\u201d and computes task-specific loss function. What is this task-specific loss function?\n3) a client computes gradients of the task-specific loss function w.r.t. local head inputs\n4) a client passes those gradients to a server via backward API call to compute gradients w.r.t. adapter parameters.\n5) a client updates both local adapter parameters and local head parameters. How and which adapters parameters are updated? Please see my points in step 1.\n\nApart from the above main concern, I have other concerns:\n\n1- Overclaims: \n1) This paper claims \"privacy guarantees\" by saying that \"designing a two-party fine-tuning protocol that performs standard parameter-efficient fine-tuning with privacy guarantees\". However, there are no privacy guarantees provided, the privacy promise of this paper is ad-hoc and it is just based on increasing the number of servers, assuming they do not collude but assuming that they have the same model.\n2) The title of this paper \"PRIVACY-PRESERVING LLM FINE-TUNING OVER API\" is too generic, oversell and does not represent this work that only considers the privacy of labels.\n3) Where \"lower performance overhead\" is demonstrated \"This paper proposes P3EFT, a two-party split learning algorithm that takes advantage of existing PEFT properties to maintain privacy at a lower performance overhead\".\n\n2- The observation listed as one of the main contributions at the end of the introduction section (\"We observe that, despite fine-tuning less than 0.1% of model parameters, modern PEFT algorithms leak client\u2019s training labels against simple attacks that work for modern pretrained transformers\")  and its corresponding Figure 1, has been already demonstrated in existing works such as  Li et al. (2022) even in a more generic way as opposed to simple binary classification tasks that considered in this submission. \n\n3- Not self-contained. For example, a clear description of LoRA which is the main building block of the proposed framework is missing. \n\n4- Not clear what would be the novelty of the proposed privacy-preserving backpropagation in comparison to secret sharing in 2 party computation that have been heavily studied in the literature."
                },
                "questions": {
                    "value": "I have posted many questions regarding the proposed framework, please see the weakness box."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5437/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698768170125,
            "cdate": 1698768170125,
            "tmdate": 1699636552432,
            "mdate": 1699636552432,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "f89QVvfwb2",
                "forum": "jMJ9IRWmH9",
                "replyto": "9SIcok1ejO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5437/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5437/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for a detailed review!  In the following response, we aim to address your concerns.  Overall, we believe that most of the weaknesses stated in the review are questions and we do our best to answer them below.\n\n> Description of the proposed method\n\nThe reviewer asks how LoRA adapters are initialized and how they are used in computations. Overall, we define the multi-party fine-tuning procedure in Section 3.1, which follows standard LoRA fine-tuning. Our algorithm is an extension of this procedure.\n\nBefore the training, adapter weights and head weights are randomly initialized on the client side. \n \nDuring the training on each step:\n1. adapters first transferred along with the inputs from client to server\n2. server runs forward pass with given adapters\n3. server computes activations from the given inputs and adapters and returns them to the client \n4. client computes loss and **real** gradients w.r.t. to the received activations \n5. client transfers **noisy** gradients (produced with method described in S3.2) to the server \n6. server runs backward passes and obtains **noisy** gradients w.r.t to adapter weights\n7. server transfers **noisy** gradients w.r.t. to adapter weights to the client\n8. client computes **real** gradients w.r.t. adapter weights\n9. client executes a step of an arbitrary optimization algorithm.\n\nOverall, we agree that the paper can be improved by adding a dedicated description of the algorithm, and we will improve it in the nearest revision.\n\n\n> What is this task-specific loss function?\n\nFor SST2 and MRPC tasks use categorical cross-entropy. In general, this is the loss function used in the standard fine-tuning procedure for downstream tasks, as defined in the supplementary code of the GLUE benchmark paper.\n\n> How and which adapters parameters are updated?\n\nClient updates all adapter parameters after each backpropagation step, which is the standard way of training LoRA adapters defined in Hu et al., (2021) [1]\n\n> There are no privacy guarantees provided, the privacy promise of this paper is ad-hoc and it is just based on increasing the number of servers\n\nWhile we discuss the privacy of two-party fine-tuning scenario at the end of Section 3.2, we agree that the paper would benefit from making the claims more specific. We will rewrite them in the updated version.\n\n> assuming they do not collude but assuming that they have the same model\n\nWe would like to clarify that the second assumption (same model) is relatively common.\nIn NLP, there are popular repositories of public models that are used for fine-tuning, e.g. [2] [3]. Most fine-tuning runs reported in NLP research, including most of the runs in the LoRA papers (see [1, 4]) use open-access models. When fine-tuning in this setup, every server can download openly available model weights prior to training, and the client benefits from the shared compute power of the servers. Thus, if the first assumption holds (there are non-colluding servers), then these servers can get the same model.\n\n> The observation listed as one of the main contributions \u2026 has been already demonstrated in existing works such as Li et al. (2022) even in a more generic way as opposed to simple binary classification tasks that are considered in this submission.\n\nTo the best of our knowledge, Li et al. (2022) also consider the problem of binary classification. Additionally, in the work of Li et al. (2022), the goal is to prevent label leakage through gradients, while our work also considers activations. Finally, we state our contribution as analyzing the privacy of PEFT algorithms (specifically) in practical fine-tuning setups, while Li et al (2022) studies full model fine-tuning. We agree that Li et al (2022) is a highly related work and we cite it as such. However, it does not overlap with our contributions.\n\n> a clear description of LoRA which is the main building block of the proposed framework is missing.\n\nWe agree and will include that description in the nearest revision.\n\n> Not clear what would be the novelty of the proposed privacy-preserving backpropagation in comparison to secret sharing in 2 party computation\n\nWe appreciate the reviewer's input regarding the technique of secret sharing and agree that it bears some similarities to our method of privacy-preserving backpropagation. However, we maintain that there are important distinctions between us and secret sharing from multi-party  computation. Since another reviewer asked for the same comparison, we will discuss this in more detail in the upcoming General Response.\n\n\n\n[1] https://arxiv.org/abs/2305.14314\n\n[2] https://huggingface.co/\n\n[3] https://pytorch.org/hub/ \n\n[4] https://arxiv.org/abs/2305.14314"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5437/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700611294851,
                "cdate": 1700611294851,
                "tmdate": 1700611294851,
                "mdate": 1700611294851,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gIsm4nlK2o",
                "forum": "jMJ9IRWmH9",
                "replyto": "f89QVvfwb2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5437/Reviewer_aRLa"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5437/Reviewer_aRLa"
                ],
                "content": {
                    "title": {
                        "value": "Thank you"
                    },
                    "comment": {
                        "value": "I acknowledge that I read your responses."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5437/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700735795830,
                "cdate": 1700735795830,
                "tmdate": 1700735795830,
                "mdate": 1700735795830,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]