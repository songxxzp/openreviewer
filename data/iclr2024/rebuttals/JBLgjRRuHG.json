[
    {
        "title": "Efficient architectural aspects for text-to-video generation pipeline"
    },
    {
        "review": {
            "id": "P1SgJ3d8n4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9160/Reviewer_ubtc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9160/Reviewer_ubtc"
            ],
            "forum": "JBLgjRRuHG",
            "replyto": "JBLgjRRuHG",
            "content": {
                "summary": {
                    "value": "This paper proposes a video generation pipeline based on MoVQ video decoding scheme. It consists of two stages: keyframes synthesis and video frame interpolation. It also compares two temporal conditioning approaches and different configurations of MoVQ-based models."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The contributions of this method are summarised as below: \n\n1. an end-to-end text-to-video latent diffusion pipeline that consists of key frames generation and frame interpolation\n\n2. separate temporal blocks for temporal modelling\n\n3. temporal output masking and data augmentations for robust VFI\n\n4. investigation of video decoders"
                },
                "weaknesses": {
                    "value": "My concern is the lack of novelty. The concert comments are below:\n\n1. This pipeline is not new. Various methods have tried to utilize text2image diffusion models for video generation. For example, the proposed temporal conditional scheme could be found at ``AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning\n''.\n\n2. Concatenating key frames along the channel dimension in video frame interpolation part is similar to \"IMAGEN VIDEO: HIGH DEFINITION VIDEO\nGENERATION WITH DIFFUSION MODELS\". \n\n3. The introduction of temporal layers is similar to \"Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models\". \n\n4. What does \"efficient\" in the title mean?"
                },
                "questions": {
                    "value": "See weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9160/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697371568247,
            "cdate": 1697371568247,
            "tmdate": 1699637152865,
            "mdate": 1699637152865,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "mSUmv7dHK2",
            "forum": "JBLgjRRuHG",
            "replyto": "JBLgjRRuHG",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9160/Reviewer_R6HA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9160/Reviewer_R6HA"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new text-to-video generation method for temporal generation. New architecture using keyframe generation and frame interpolation with video decoder is proposed to handle the text-to-video scenario. Some sota results are acheived in terms of IS and CLIPSIM metrics."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "A novel scheme for video generation with customizations to ordinary text-to-video models. \nEvaluated several configurations for temporal blocks and MoVQ-based decoder."
                },
                "weaknesses": {
                    "value": "Not the best results are acheived in terms of CLIPSIM metrics.\nIS scores are not good compared with videogen."
                },
                "questions": {
                    "value": "Why the IS score is much lower than videogen ?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9160/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698224988970,
            "cdate": 1698224988970,
            "tmdate": 1699637152750,
            "mdate": 1699637152750,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "16wokFoLtX",
            "forum": "JBLgjRRuHG",
            "replyto": "JBLgjRRuHG",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9160/Reviewer_qUnt"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9160/Reviewer_qUnt"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a two-stage latent diffusion model, which contains keyframe generation and frame interpolation, for text-to-video generation tasks. Starting from a pretrained text-to-image model, separate temporal blocks are used to model the temporal information. A video decoder based on MoVQGAN is also adopted to improve the generation quality."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The topic of this paper is significant.\n2. The paper is overall clear and well-formulated.\n3. The training of the proposed model seems resource-efficient compared to most text-to-video methods, as only 8-16 A100 GPUs and 120k data pairs were adopted in training."
                },
                "weaknesses": {
                    "value": "1. There needs to be more explanations and analysis about the proposed methods and results. The authors should illustrate the possible reason why separate temporal blocks perform better than temporal layers.\n2. As the limitations of quantitative metrics, more visual results should be provided and compared with other text-to-video methods.\n3. The paper doesn't mention the number of parameters of the video generation model, and only 120k internal data pairs are utilized for training. The model maybe overfit to the training data. As the data domain is also agnostic, it's hard to decide whether the experiment results are evidential enough or not."
                },
                "questions": {
                    "value": "1. Could the authors give more explanations about the separate temporal blocks over temporal layers?\n2. As mentioned in the introduction part, the scarcity of open-source text-video datasets impedes the development of video generation. Is it possible to contribute the internal training data to the community?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9160/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698736400062,
            "cdate": 1698736400062,
            "tmdate": 1699637152640,
            "mdate": 1699637152640,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]