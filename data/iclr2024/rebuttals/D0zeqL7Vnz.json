[
    {
        "title": "Prompt Sketching for Large Language Models"
    },
    {
        "review": {
            "id": "HZvyr9F56u",
            "forum": "D0zeqL7Vnz",
            "replyto": "D0zeqL7Vnz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9320/Reviewer_pDQp"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9320/Reviewer_pDQp"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes prompt sketching, a method to first provide sketches to the language model, then ask the model to fill in certain variables. The authors did experiments on several reasoning tasks and some planning tasks (with state tracking), to show the proposed method outperform existing method like direct prompting and chain-of-thought prompting. The models used are InstructGPT-based (text-davinci-003) and Llama-2 Chat based."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The motivation of this paper is great, and the sketching idea is highly interesting. Currently most language models do decoding in an auto-regressive fashion and might not adhere to certain constraints in the input. Sketching can definitely help models better plan and output responses better fit into user constraints.\n\n- Some of the tasks explored are quite novel and interesting, like the interleaved reasoning tasks and the planning tasks (section 4.2), and the experiments do show they benefit from prompt sketching quite a bit."
                },
                "weaknesses": {
                    "value": "The biggest concern is the experiments in this paper, which do not clearly show the benefits of the proposed method:\n\nMost of the explored tasks, including logical reasoning, question answering, and arithmetic reasoning, use the *multi-variable* prompting method (BeamVar, Var) as the sketch (Figure 3), which is actually a variant of the self-consistency [1] method: sample multiple chain-of-thoughts and then aggregate. Hence a fair comparison should be between the proposed method and self-consistency-based chain-of-thought, under the exact same number of samples. \n- The novelty of the proposed method compare to self-consistency should be discussed in details in this paper. \n- Can the authors add self-consistency with the same number of samples as a baseline?\n- Comparing chain-of-thought prompting under BeamVar and prompt-sketching under BeamVar (this should be a more fair comparison with the same number of sampled thoughts), the proposed method does not yield much gains. Hence the authors should better discuss what is the main contribution of \"sketching\" over existing chain-of-thought.\n\n[1] Wang et al. Self-Consistency Improves Chain of Thought Reasoning in Language Models. ICLR 2023.\n\nIn section 4.2, some novel tasks are explored and could potentially show the benefits of the proposed sketching. However, the experiments are extremely small-scale (10 Sudoku puzzles, 10 Dungeon environments), so it is unclear whether the proposed method indeed outperform existing methods.\n\nPerformance gains: from Table 6, the confidence intervals are fairly large, and it is unclear which method is significantly better compared to the others. Can the authors clarify which result is statistically significant?\n\nComputational cost: can the authors discuss in more details on the exact computational cost used for the proposed method?"
                },
                "questions": {
                    "value": "- Can the authors add self-consistency with the same number of samples as a baseline?\n- Table 6, the confidence intervals are fairly large, and it is unclear which method is significantly better compared to the others. Can the authors clarify which result is statistically significant?\n- Computational cost: can the authors discuss in more details on the exact computational cost used for the proposed method?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9320/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9320/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9320/Reviewer_pDQp"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9320/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698687271182,
            "cdate": 1698687271182,
            "tmdate": 1700672557230,
            "mdate": 1700672557230,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "89d53StRW8",
                "forum": "D0zeqL7Vnz",
                "replyto": "HZvyr9F56u",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9320/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9320/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer pDQp Part I"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their insightful questions, which we address below, and are encouraged to hear that they find prompt sketches to be a highly interesting idea.\n\n**Q: How does Sketching with BeamVar/Var relate to Self-Consistency and can you add it as a baseline?**  \nThe self-consistency [1] decoding strategy improves LLM reasoning by greedily sampling multiple (chain-of-thought) reasoning paths and aggregating the resulting final answers, by (unweighted or weighted) majority vote. In contrast, sketch-aware decoders generate answers token-by-token or thought-by-thought (not path by path), can branch on the top-n alternatives at each step, and do not aggregate the final results. Instead, a single sequence is chosen as the one answer to an input, considering the model likelihood of the underlying token sequence only (no aggregation or majority voting). With this, self-consistency and sketching can be considered orthogonal approaches and could even be applied jointly, i.e., sample-decode a sketch multiple times and aggregate the results.  \nWe added a comparison of sketching and self-consistency to the revised manuscript (Appendix D). For this comparison, we choose $n=2$/$n=4$ samples for self-consistency, as this aligns with the computational overhead of our branching decoders BeamVar/Var with $n=2$. However, our results show that, in many cases, self-consistency with such a low number of samples does not even outperform argmax CoT which is the main baseline we outcompete with sketching. This aligns with the self-consistency paper itself [1], which reports that around 5-20 consistency samples are required to outperform simple argmax CoT. We thus find that within this compute budget ($n=2$/$n=4$), argmax CoT and thus sketching both outperform self-consistency decoding.\n\n**Q: Is CoT+BeamVar a better baseline for Sketching+BeamVar?**  \nComparing CoT+BeamVar with Sketching+BeamVar, we observe an accuracy increase of $8-12%$ with OpenAI models for AQuA, StrategyQA, Tracking Shuffled Objects and Matrix Shapes. On these tasks, CoT+BeamVar is thus clearly outperformed by sketching. For Date Understanding, Multistep Arithmetic, GSM8K on the other hand we indeed observe comparable performance with sketching+BeamVar. This can be explained by the two-part nature of the zero-shot CoT formulation [2] (first decoding reasoning and then the final answer). Applying a sketch-aware decoder like BeamVar to this kind of prompt already boosts performance, as BeamVar can exploit this two-variable structure. For a baseline comparison (no interaction between multiple variables), we thus have to look at Beam+CoT (one column to the left). There, we find that Multistep Arithmetic and GSM8K again perform worse. Only for Date Understanding do sketching and sketch-aware decoders not appear to bring particular benefit for OpenAI models, which is reflected in our overall summary: sketching outperforms sequential prompting on 7/8 tasks.\n\n**Q: Can you extend your evaluation of additional applications and further demonstrate the effectiveness of sketching in this context?**  \nWe have scaled our experiments on the additional applications (Section 4.2) by a factor of 10 and now evaluate a larger number of samples (100 samples per model/decoder configuration). We report the results in the updated manuscript. Overall, we observe comparable trends to before, i.e., sketch-aware decoders clearly show the ability to backtrack in Sudoku solving and solve the Dungeon Escape tasks more efficiently (fewer steps), when compared to naive argmax decoding. We have also added an additional experiment on TextWorld [3] exploration, which leverages our decoders for LLM-guided world exploration. Again, we find that sketch-aware decoders clearly outperform greedy argmax decoding, completing TextWorld quests of different lengths with 20% fewer steps. We further discuss our TextWorld results in Appendix B.2.\n\n**References**  \n[1] Self-consistency improves chain of thought reasoning in language models, X. Wang et al., ICLR\u201923   \n[2] Large language models are zero-shot reasoners, K. Takeshi et al., NeurIPS\u201922  \n[3] Textworld: A learning environment for text-based games, M.-A. C\u00f4t\u00e9 et al., IJCAI\u201918"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9320/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700561701970,
                "cdate": 1700561701970,
                "tmdate": 1700561718624,
                "mdate": 1700561718624,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "EJlaIyCIXa",
                "forum": "D0zeqL7Vnz",
                "replyto": "puinljOPYp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9320/Reviewer_pDQp"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9320/Reviewer_pDQp"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the clarification. I agree that for Interleaved Reasoning, the win is pretty clear and the underlying reason is clear as well (the model does more explicit state tracking via the sketches), however, for other tasks, the wins are fairly small (and seem to vary a lot based on which one of argmax/beamvar/var/beam is used). \n\nFor math tasks like AQuA or GSM8K, can the authors clarify exactly why sketching would help? From the prompts it seems like it's only forcing two things in addition to CoT: \n(1) the number of thought steps: how is this determined btw? I saw 12 is used for AQuA and 10 is used for GSM8K, are they randomly chosen? It's possible that by enforcing a certain number of thought steps, the model generates a *longer* thought process which leads to some of the gains. Can the authors provide some simple statistics like the avg # tokens in CoT and in sketching?\n(2) some formatting on the thoughts: this issue can actually easily be addressed by few-shot CoT (i.e., by showing models a few examples, the model can better adhere to the few-shot format). From Table 4 we can also see the gains become much smaller when few-shot is used."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9320/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700635454404,
                "cdate": 1700635454404,
                "tmdate": 1700635454404,
                "mdate": 1700635454404,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dlb3rignoT",
                "forum": "D0zeqL7Vnz",
                "replyto": "sJnEWNdM47",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9320/Reviewer_pDQp"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9320/Reviewer_pDQp"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the further clarification. Overall I think the idea proposed in this paper is novel and interesting, and the gain on interleaved reasoning is clear, hence I will increase my score.\n\nHowever, based on the responses I think this paper should better clarify its contribution on all the other reasoning tasks: \n1) the proposed example reasoning framework (\"on one hand.., on the other hand\") is not generally adopted for all tasks (only used in \"Information Essentiality\") and it is unclear why it could be beneficial on top of CoT. It also seems to be task-specific (e.g., some tasks require thinking both pros and cons) and needs to be manually designed by looking at task examples;\n2) most sketches adopt the \"for i in range(K): -THOUGHT\" framework, but it seems like multiple factors could contribute to the final gains: prompt length, formatting, backtracking, rather than just \"sketching\" which is the main claim of this paper. The number K also needs to be manually selected by looking at examples and model responses.\nI think this paper could be further improved by better clarifying its contributions, analysis on what actually contributes to the gains compared to existing methods, and more clear presentation on the methods/prompt used."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9320/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700672542805,
                "cdate": 1700672542805,
                "tmdate": 1700672542805,
                "mdate": 1700672542805,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "jDKqLYfM0z",
            "forum": "D0zeqL7Vnz",
            "replyto": "D0zeqL7Vnz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9320/Reviewer_tG6m"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9320/Reviewer_tG6m"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel prompting method (Prompt Sketching) which guides intermediate inference steps based on template. Prompt Sketching provides more control over generation and inference steps by putting deterministic chunks in decoding steps. In addition to the prompting strategy, authors suggests two variants of beam search (VAR, BEAMVAR) to adapt LLM to new decoding process. Experiments show its effectiveness in LLM reasoning tasks over CoT. Also, authors suggests types of task for which prompt sketching can be especially useful."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Simple prompting strategy to improve LLM reasoning performance\n- New applications are interesting and could be useful for launching practical AI services.\n- Structured outputs induced by prompt sketching have potential to automate various tasks beyond suggested applications.\n- The suggested method can reduce the number of model calls compared to stop-and-go and thus reduce the cost, which is practical."
                },
                "weaknesses": {
                    "value": "- Generating templates requires human intervention and may necessitate significant efforts until finding a template working well. Also, potentially, templates can overfit to evaluation datasets.\n- It does not work well for small LMs.\n- Evaluation results are given with limited amounts of data, which may harm the credibility of the results. Especially, confidence intervals in Table 6 look pretty large.\n- Most of new applications look already doable by guidance-ai (https://github.com/guidance-ai/guidance ), which is cited in the paper. Also, naive stop-and-go is not compared in main results."
                },
                "questions": {
                    "value": "- What\u2019s the Sequential Prompting used in Table 3? CoTs or stop-and-go?\n- Can templates be generated or suggested by LLM as well? I am also wondering if templates can be generated by retrieval.\n- Is the suggested method applicable to programming tasks?\n- Can Prompt sketching get help from demonstrations?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9320/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9320/Reviewer_tG6m",
                        "ICLR.cc/2024/Conference/Submission9320/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9320/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698688262638,
            "cdate": 1698688262638,
            "tmdate": 1700613935781,
            "mdate": 1700613935781,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "u9gzaYw9dD",
                "forum": "D0zeqL7Vnz",
                "replyto": "jDKqLYfM0z",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9320/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9320/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer tG6m Part I"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their insightful questions, which we address below, and are encouraged to hear that they find prompt sketches to be an intuitive format for chained queries.\n\n**Q: Can you comment on the overhead of writing sketches as opposed to prompts?**  \nSimilar to prompt engineering, templates have to be constructed, tested, and debugged with example data to ensure the best performance. With current models, however, this prompt/template engineering effort cannot be avoided, as a single input/sentence is typically not sufficient to ensure that the model conforms to very specific and precise requirements. In this context, we consider sketching a more controlled form of prompting, where the output format is guaranteed and not up to prompt engineering and randomness in the generation process. Nonetheless, we agree that a human effort has to be made to properly prompt, template, and use LLMs. See also Section 4.3 for more discussion of this aspect.\n\n**Q: Is prompt-sketching less effective with smaller LLMs?**  \nWe note that even with a smaller Llama-2 Chat model (13B parameters), sketching outperforms sequential prompting on 6/8 reasoning tasks while exhibiting even greater performance gains than for large OpenAI models (up to 8% vs. up to 4%). However, as we acknowledge in our evaluation, Llama-2 seems incapable of solving the Matrix Shape and AQuA reasoning tasks, irrespective of the prompting/sketching scheme, leading to inconclusive results on these datasets. Other tasks are not as strongly affected and show comparable trends to the OpenAI results. Further, we demonstrate in Appendix B.1 that sketching can be very effective with smaller models, for instance when asking a model to produce valid JSON output: For the small text-curie-001, sketching allows us to guarantee valid JSON output, whereas the same model with a corresponding prompts, is unable to produce any valid JSON at all. \n\nPotentially, our description in Section 4 was unclear in this regard. We have updated it in the rebuttal revision to make the above points more clear.\n\n**Q: Does the sample size in the evaluation impact the credibility of the results?**  \nWe understand your concerns regarding sample size and statistical significance for the OpenAI-specific part of our evaluation. Unfortunately, we cannot scale our OpenAI experiments further, due to cost constraints ($4000 for the results at the current scale). However, we have expanded our Llama-2-based experiments (Table 2) to include all decoder/dataset configurations and for 10 times as many samples as in the OpenAI experiments (1000 per dataset, or the full datasets in many cases). We also provide confidence intervals for the Llama results in the new appendix Table 8. In this much larger-scale Llama-2 setting, we observe comparable trends to the smaller-scale OpenAI experiment: Across tasks, sketching improves overall reasoning performance when compared to sequential prompting, while sketch-aware decoders can provide further gains in exchange for higher computational cost.\n\n**Q: How does the presented approach differ from Guidance?**  \nIndeed, practical tools like Guidance also enable a form of stop-and-go inference, which can be seen as a simple form of sketching. However, Guidance is an ad-hoc approach, where templates are simply decoded using multiple isolated LLM calls. In contrast, sketching theoretically anchors this approach as a multi-part sequence decoding problem, enabling sketch-aware decoders. Further, prior work (including Guidance) does not provide insights on whether this form of structural guidance positively impacts model reasoning capabilities. This is evaluated as the argmax version of prompt sketching. Our experimental results are thus also of interest to Guidance-like frameworks, as we show that structural guidance during inference can indeed improve reasoning capabilities, beyond syntax and structure. To the best of our knowledge, this has not been shown before and therefore improves the understanding of simple stop-and-go inference in general. We have updated the introduction of the paper to better reflect this."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9320/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700561095335,
                "cdate": 1700561095335,
                "tmdate": 1700561095335,
                "mdate": 1700561095335,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ww0KbiVZNE",
                "forum": "D0zeqL7Vnz",
                "replyto": "5hUr7tWtEA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9320/Reviewer_tG6m"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9320/Reviewer_tG6m"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the detailed response! Since clarifications nicely addressed my concerns, I have raised my score to 6."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9320/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700613968159,
                "cdate": 1700613968159,
                "tmdate": 1700613968159,
                "mdate": 1700613968159,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Lxd5h43qZ2",
            "forum": "D0zeqL7Vnz",
            "replyto": "D0zeqL7Vnz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9320/Reviewer_Apoy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9320/Reviewer_Apoy"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes templated prompt sketches for problems requiring structured generation from LLMs. Structurally constrained generation is an important but overlooked problem. The paper also proposes sketch-aware decoding that considers the structured variables in decoding, and releases the code as an open-source library."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The motivation is clear, and the proposed methods which performs beamsearch over the variables (to be generated) is reasonable. \n\nA thorough study of non-templated and stop-and-go method as well as the proposed method, using various decoding strategies, is provided.\n\nThe provided prompt sketches are useful for various tasks."
                },
                "weaknesses": {
                    "value": "The experiments show that stop-and-go inference works well, and the proposed method does not significantly improve performance despite the additional overhead. Further, on many of the tasks simple autoregressive CoT seems sufficiently close in performance.\n\nWhile the paper provides some additional applications for prompt sketches, the tasks and the performance on the tasks are not entirely convincing."
                },
                "questions": {
                    "value": "1. How is the custom decoding applied when using OpenAI API?\n2. I'm curious about the results if few-shot prompts are used."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9320/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698862540425,
            "cdate": 1698862540425,
            "tmdate": 1699637173198,
            "mdate": 1699637173198,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gpqWEUXRqz",
                "forum": "D0zeqL7Vnz",
                "replyto": "Lxd5h43qZ2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9320/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9320/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Apoy"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their encouraging response (clear motivation, thorough empirical study, useful for various tasks) and insightful observations and reply to their queries below.\n\n**Q: Can you comment on the effectiveness of sketching/sketch-aware decoders with different tasks and models, and how it compares to argmax+CoT?**  \nWe consider the contribution of our experimental results to be twofold:\n* We demonstrate that template-guided inference via simple argmax-decoded sketches (stop-and-go) and comparatively simple structural templates can already improve task accuracy on reasoning tasks (OpenAI: 7/8, Llama-2: 6/8) significantly (4% and 8% respectively). To the best of our knowledge, this has not been demonstrated before, as existing work on template-guided inference does not discuss the impact on reasoning performance. We thus, for the first time, empirically validate the effectiveness of template-guided (stop-and-go) inference on reasoning tasks, using our sketching framework. Computationally, argmax sketching is cheaper than sequential decoding for the same output (e.g., argmax+CoT), as in sketching/stop-and-go, the model does not have to autoregressively process fixed template parts (assuming sufficient model access and key-value caching). In comparison, simple sketching thus provides accuracy improvements without necessarily incurring any computational overhead (potentially even reducing it).\n* We also demonstrate that while argmax sketching/stop-and-go inference can be effective, sketch-aware decoding brings even further improvements. These decoders incur computational overhead, but, as with traditional beam search, can be worth the additional compute if downstream performance is absolutely crucial. Of course, in practice this trade-off depends entirely on the concrete use case, compute budget, and user requirements.\n\n**Q: Can you extend your evaluation of additional applications and further demonstrate the effectiveness of sketching in this context?**  \nWe have scaled our experiments regarding additional applications by a factor of 10, and now evaluate a larger number of samples (100 samples per model/decoder configuration). We report the results in the updated manuscript in Table 3. Overall, we observe comparable trends to before, i.e., sketch-aware decoders clearly show the ability to backtrack in Sudoku solving and solve the Dungeon Escape tasks more efficiently (fewer steps). We have also added an additional experiment on TextWorld [1] exploration (Table 3), which leverages our decoders for LLM-guided world exploration. Again, we find that sketch-aware decoders clearly outperform greedy argmax decoding, completing TextWorld quests of different lengths with ~20% fewer steps. We discuss our TextWorld result in appendix B.2.\n\n**Q: How is custom decoding possible with the OpenAI API?**  \nThe proposed decoders only require rather minimal support by the API/model. In particular, we require the possibility to extend all of the sequences in the beam and obtain logprobs for these sequences for at least their top $n$ continuations. For the OpenAI Completion API models we use in the paper, these capabilities are available.\n\n**Q: How does sketching perform when combined with few-shot prompting?**  \nSketching and few-shot prompting are orthogonal strategies, i.e., can be applied independently and jointly. We provide results on combining both in Appendix C.1. Overall, we find that few-shot prompting improves performance across the board, however, sketching still outcompetes sequential prompting, and in some cases zero-shot sketching even outperforms two-shot sequential prompting, suggesting that sketching can even serve as a replacement for few-shot demonstrations.\nWe hope to have been able to address all the reviewers\u2019 concerns, are happy to answer any follow-up questions they might have, and are looking forward to their reply.\n\n**References**  \n[1]  Textworld: A learning environment for text-based games, M.-A. C\u00f4t\u00e9 et al., IJCAI\u201918"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9320/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700559696156,
                "cdate": 1700559696156,
                "tmdate": 1700559696156,
                "mdate": 1700559696156,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "cyrnSHDsAr",
            "forum": "D0zeqL7Vnz",
            "replyto": "D0zeqL7Vnz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9320/Reviewer_AyRy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9320/Reviewer_AyRy"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new approach to decoding LLM outputs when chaining multiple LLM queries. Such chains of queries can be specified as *sketches*: natural-language prompts that contain *holes* that the LLM is meant to fill in. Each hole is associated with a stopping phrase, and a natural way to read the sketch is as specifying an interaction pattern, where we alternate between (1) deterministically extending an LLM's context window with the next (non-hole) chunk of the sketch, and (2) allowing the LLM to fill in the value of a hole by sampling tokens freely until it emits the stopping phrase for that hole. Because LLMs are autoregressive, this interaction pattern does not allow the LLM propose values for the holes in a way that is *aware* of future interactions in the sketch. To alleviate this problem somewhat, the paper presents two new decoding algorithms (variants of beam search) that optimize the joint log probability of the entire LLM context. On several benchmark tasks, the paper compares the zero-shot performance of LLMs with standard prompts + standard decoding algorithms, vs. with particular prompt sketches and the new proposed decoding algorithms."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* Prompt sketches are an intuitive format for specifying certain types of chained queries.\n\n* The paper identifies a connection between decoding for these prompt sketches and constrained decoding, and points out (correctly) that standard beam search is insufficient for this task. The variants of beam search that the paper introduces are largely sensible, and overcome the key barriers to performing beam search in the multi-variable setting\u2014namely, the fact that beams with the same number of tokens may be at different stages of the sketch, making their scores difficult to compare fairly.\n\n* Results are reported both for open models and closed (OpenAI) models. Many souped-up decoding methods require information that is not available through the OpenAI APIs, and it's nice that the authors have shown that a version of their approach (at least for small beam sizes) *can* be implemented atop the user-facing API (at least for text-davinci-003)."
                },
                "weaknesses": {
                    "value": "* I couldn't quite follow the motivation: what problem with existing decoding techniques is prompt sketching meant to address? Figure 1 comes closest to illustrating the problem, but it was not particularly compelling. (I am not sure which model was used to generate Figure 1, but I tried copying the prompt and constraint into text-davinci-003 and it had no trouble following the constraint.) To be sure, there are many sketches that I am sure GPT-3 would often fail to follow, even if the sketch were included in the prompt; you can encode arbitrarily difficult infilling problems into sketches. But the sketches presented in this paper are enforcing very simple formatting constraints on, e.g., the list of thoughts generated for a chain-of-thought prompt. What failure modes do you see when just explaining to the model that it should follow the desired format (e.g. by pasting the sketch into the context)? Do failures to follow the format cause failures in reasoning? How exactly do VAR and BEAM_VAR address these failures? (Can they really be doing much, at a beam size of only n=2?)\n\n\n* The experiments provide somewhat weak evidence for the value of the new decoding methods. In different tasks, it often seems to be the case that *one* of the methods outperforms an argmax baseline, whereas the *other* method underperforms the baseline, and which method wins varies from task to task. Even when the new decoding methods provide a modest advantage over argmax decoding, it is not clear whether the advantage is worth the added computational cost (or dollar cost, for OpenAI models).\n\n\n* I am not convinced the experimental setup is completely debugged. For example, in chain-of-thought for the \"date understanding\" task, a period is used as the stopping phrase for each thought. However, periods show up frequently in dates (e.g., \"Sept. 1\"), and this stopping-phrase is clearly causing the model to cut off thoughts early (page 21). Some experimental settings are also missing details; e.g., in the single-variable chain-of-thought prompts, it is unclear when the [COT] variable ends -- I did not see a stopping phrase reported.\n\n\n* Some of the algorithmic choices in VAR / BEAM_VAR were not sufficiently justified, and struck me as slightly odd. For example, the VAR algorithm shrinks the n^2 generations for a variable back down to a beam width of n *before* adding the next deterministic chunk. But I thought a key point of these algorithms was to enable the next deterministic chunk to provide a \"score\" for the proposed variable values; wouldn't it make more sense to rank all n^2 variable proposals by how well they fit with the next deterministic chunk, scale back down to n, and then generate proposals for the next variable?"
                },
                "questions": {
                    "value": "I'd appreciate your thoughts on the questions raised in the \"weaknesses\" section. In particular, it would be great to better understand example failure modes of simpler methods (e.g., argmax decoding for few-shot chain-of-thought prompting) and how prompt sketching addresses / avoids these failures."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9320/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9320/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9320/Reviewer_AyRy"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9320/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698868434295,
            "cdate": 1698868434295,
            "tmdate": 1699637173063,
            "mdate": 1699637173063,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Al8lbmQ9pD",
                "forum": "D0zeqL7Vnz",
                "replyto": "cyrnSHDsAr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9320/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9320/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer AyRy Part I"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their insightful questions, which we address below, and are encouraged to hear that they find prompt sketches to be an intuitive format for chained queries.\n\n**Q: Can you clarify the motivation for sketching and provide failure modes of existing models/decoding methods?**  \nAs pointed out by the reviewer, template-guided decoding can also be implemented by simply showing the LLM a template and asking it to produce a corresponding response. However, there are several limitations and failure modes with this approach:\n- *Instruction Following Capabilities:* Only big and powerful models are currently capable of this kind of instruction-following (e.g., see how `text-curie-001` fails for this kind of task in Table 3, Sudoku). In contrast, sketching also works with small models, which may not understand the template, but can still produce useful completions for individual template variables.\n- *Prompt Size:* Including large templates fully in the prompt will greatly increase the number of required tokens, possibly even exceeding the context window of the model. Further, LLM retrieval across large inputs remains difficult, which can lead to deviations from the provided template. In contrast, sketching guides the model through the template, step by step, without increasing the total number of tokens or requiring long-range retrieval.\n- *Accuracy, Reliability, and Automation:* Instruction-following models are still not perfect, which means templates may be violated and not followed strictly. In contrast, sketching provides a 100% guarantee that the template will be adhered to. This is crucial in automated settings, where the LLM\u2019s output format must always be correct to enable processing by other systems that expect a specific format (e.g., function calling).\n- *Dynamic Templates and Interactive Environment:* In some cases, providing the full template may also not be possible, as it may not be known fully ahead of time. For instance, templates may be dynamic, e.g., multiple repetitions of some template part, until a certain variable value is produced. Models can be instructed to respect such constraints, but this kind of prompting ultimately remains unreliable while sketching can enforce these properties strictly (cf. sketched CoT in the paper). Further, sketching is also applicable to interactive settings, where the template depends on and reacts to model output. For instance, in Table 3 we experimented with Dungeon-style graph traversal (Table 3), and in the updated revision, we added an additional experiment on TextWorld exploration. \n\n**Q: How exactly do BeamVar/Var address these failure modes (with $n=2$)?**  \nAs pointed out above, sketching alone already greatly alleviates these failure modes as it leads to strict adherence to the template and allows for scripted follow-up. Sketch-aware decoders (BeamVar/Var) additionally allow us to jointly score multiple variables (while standard decoders only allow the last variable to be scored conditioned on all preceding ones). This enables picking variables based on later sequences in the template, which is particularly important in dynamic environments where the template text depends on the value of prior variables. While modern RLHF models are very strong and typically assign high probabilities to the top-1 token, a small beam width of n=2, can already be useful to track an alternative hypothesis when ambiguities arise (demonstrated by our experiments on reasoning tasks). For our more advanced experiments with other applications (cf. Table 3), we apply BeamVar and Var with higher beam width (n=5 and n=3 respectively), as clarified in the revised draft, which enables the exploration of a broader hypotheses space.\n\n**Q: Why are you focusing on relatively simple formatting constraints?**  \nWe chose to focus on simple sketches, to examine whether structural support can improve model reasoning capabilities at all. Indeed, more complex or even grammar-guided approaches are possible, however, the design space of such sketches/reasoning grammars is very large and requires significant prompt engineering. In contrast, the main goal of this research is to establish foundational insight on whether structural guidance of the LLM reasoning process can improve its capabilities at all. Nonetheless, our sketching framework and decoders are general and capable of more complicated forms of template-guided inference, setting up future work to explore this direction."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9320/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700558787166,
                "cdate": 1700558787166,
                "tmdate": 1700558787166,
                "mdate": 1700558787166,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WhTPkyoMhD",
                "forum": "D0zeqL7Vnz",
                "replyto": "0sXuOic7WP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9320/Reviewer_AyRy"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9320/Reviewer_AyRy"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for these detailed responses!\n\nIt seems that the main contribution of the paper is to empirically test whether this kind of guided decoding (variants of which have previously been published in LMQL, Synchromesh, etc. as well as implemented in open-source libraries like Microsoft Guidance and Outlines) help LLMs to reason better. In particular, there appears to be a claim that enforcing relatively simple stylistic constraints on the output ('chains of thought are \\n-separated lists beginning with hyphens') improves reasoning.\n\nBut I still do not feel that the paper, or the response, provides satisfying insight into why that is or how generally such a conclusion might hold. As I asked in my review, I am still wondering:\n\n> Do failures to follow the format cause failures in reasoning?\n\nFor example, if not following a sketch template, how do chain-of-thought-prompted models fail? How often are failures in reasoning correlated with failures to follow the format? Why are these failures of reasoning fixable by forcing them to insert hyphens before each thought? How generally is that true?\n\nI will not argue against this paper's acceptance if other reviewers want to accept it, but I believe its thesis could be clarified and better defended empirically in future revisions."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9320/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700728867807,
                "cdate": 1700728867807,
                "tmdate": 1700728867807,
                "mdate": 1700728867807,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]