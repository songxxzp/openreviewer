[
    {
        "title": "Precision and Recall Reject Curves for Classification"
    },
    {
        "review": {
            "id": "HnER9f2CM8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1891/Reviewer_EkHi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1891/Reviewer_EkHi"
            ],
            "forum": "2CxkRDMIG4",
            "replyto": "2CxkRDMIG4",
            "content": {
                "summary": {
                    "value": "This submission focuses on the classifcation scenario with rejection option where accuracy-reject curve is usually used to evaluate and compare the performance of different certainty measures over a range of thresholds for accepting or rejecting classifcations."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. The motivation of this paper is reasonable, i.e., using precision-reject curve and recall-reject curve in the class-imbalance problems.\n\n2. Some experiments are performed to show the differences of the precision-reject curve and recall-reject curve from accuracy-reject curve.\n\n3. Some discussions are given."
                },
                "weaknesses": {
                    "value": "1. The contribution is very limited. Precision-reject curve and recall-reject curve are two intuitive extensions of accuracy-reject curve. It is not suggested to presented them independently in one paper. It will be better to present them as part of one paper that proposes a classifcation algorithm with rejection option.\n\n2. It is unclear why precision and recall are used. For class-imbalanced problems, F1 is the more commonly used measure.\n\n3. In experiments, authors should pay more attentions to the difference of precision and recall from accuracy (e.g., figure 2). For figure 1, it is hard to observe their differences."
                },
                "questions": {
                    "value": "If authors disagree my comments in weaknesses, clarifications can be presented in the rebuttal phase."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1891/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697338908993,
            "cdate": 1697338908993,
            "tmdate": 1699636119307,
            "mdate": 1699636119307,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "UcFgajaf28",
            "forum": "2CxkRDMIG4",
            "replyto": "2CxkRDMIG4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1891/Reviewer_YWY1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1891/Reviewer_YWY1"
            ],
            "content": {
                "summary": {
                    "value": "The article considers the problem of performance measurement under model uncertanty and extends a previously proposed method -- accuracy-rejet curves -- to precision and recall metrics rather than accuracy."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "Rejection options is an important area of research, particularly impactful in some domains like medicine."
                },
                "weaknesses": {
                    "value": "It is unclear how this is different from standard precision recall curves. In the binary classification case, given that precision and recall focus on the positive class only this work appears to simplify down to precision-recall curves. It therefore does not appear to propose anything novel that would help with choosing particularly thresholds for a task. It may be different in the case of multiple classes but none of  the experimental datasets have more than two classes."
                },
                "questions": {
                    "value": "How is this different from standard precision/recall curves? It seems equivalent since precision/recall focus on the positive class only and disregard the negative class."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1891/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698373031318,
            "cdate": 1698373031318,
            "tmdate": 1699636119238,
            "mdate": 1699636119238,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "3Jx9KXTFrb",
            "forum": "2CxkRDMIG4",
            "replyto": "2CxkRDMIG4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1891/Reviewer_uffc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1891/Reviewer_uffc"
            ],
            "content": {
                "summary": {
                    "value": "Accucracy-Reject-Curves (ARCs) are frequently used for evaluating classifiers with a reject option, i.e., where the classifier can choose to not classify an example. The authors argue that for imbalanced datasets, precision and recall reject curves, and evaluate them on four small datasets."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The assumptions upon which the work rests are valid, and it is quite likely that precision and/or recall reject curves are a good alternative to ARCs on imbalanced datasets."
                },
                "weaknesses": {
                    "value": "Having said this, this extensions is a very trivial extension, as it is well-known (as the authors also observe) that precision and recall (and F1) are better evaluation measures for imbalanced data. So this paper could, in my opinion, only be of interest if the performed evaluation is exceptionally thorough and valid, so that this is a paper that clearly establishes the value of this proposal. Unfortunately, I think the evaluation is lacking for several reasons:\n- the comparison is only on 4 small datasets, one of them as simple as two Gaussian clusters. The datasets are also rather small (less than 1000 instances), and - contrary to the claims of the paper - *not* heavily imbalanced (no class has less than 25% examples).\n- no systematic evaluation of the data space, such as comparisons for several different rates of unbalancedness\n- it is not clear why precision and recall (which are separate curves) have been selected, and not F1-reject curves, where one would only have to deal with one curve. These are, as the authors mention, also not new (although they have been proposed in a somewhat different context)\n- there are some obvious benchmark methods that could/should have been included. For example, a natural extension would be to use local reject options, which have different thresholds for the majority and minority class. What is the advantage of precision-reject curves over ARCs in such settings? In the limiting case, when the minority class has a regular threshold, and the majority class has a fixed threshold of 1 (i.e., is never predicted), wouldn't you get something like a precision-reject curve for the minority class as well?\n\nMinor:\n\nThe paper seems to be adapted from a version that used numeric citations. With author-year citations, constructs such as \"The authors of Artelt et al. (2022) show..\" are not good, this can be simply said as \"Artelt et al. (2022) show...\"."
                },
                "questions": {
                    "value": "None."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1891/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698789035930,
            "cdate": 1698789035930,
            "tmdate": 1699636119164,
            "mdate": 1699636119164,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "oaj5nzDl9g",
            "forum": "2CxkRDMIG4",
            "replyto": "2CxkRDMIG4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1891/Reviewer_zM2w"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1891/Reviewer_zM2w"
            ],
            "content": {
                "summary": {
                    "value": "In many real-world classification problems, one may want to provide a classifier the option to reject (or abstain from making a prediction) on samples that it is not certain about. In such cases, it is common to evaluate the trade-off between accuracy and rejection costs by plotting the accuracy as a function of rejection rates. However, in applications where there is severe class imbalance, plotting accuracy vs rejection rate may not be the best approach to evaluate the classifier. The paper proposes to instead use plots of precision and recall as a function of rejection rate, and presents some experiments to argue why these are better alternatives."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "- The paper is well-written and does a good job of covering prior work on classification with rejection.\n- Exploring alternate ways for evaluating a classifier with a reject option is an important problem to tackle."
                },
                "weaknesses": {
                    "value": "- The paper is limited in novelty. The fact that metrics based on precision and recall are better alternatives to accuracy under class imbalance is well known in the literature. Merely proposing their use in evaluating rejection-based classifiers does not make for a significant or novel contribution.\n- The experimental conclusions aren't very strong either. It is interesting that precision and recall curves show different trends compared to accuracy curves, but this observation alone does not make for a strong contribution. Besides, the datasets used are all small scale, containing a few hundred samples."
                },
                "questions": {
                    "value": "- In Figures 2-3, as you reject more samples, it looks like both precision and recall show a downward trend. I had initially expected one of these metrics to be favored more compared to the other. Is the downward trend because we reject more from the minority class compared to the majority class?\n- At 100% rejection rate (i.e. 0% acceptance rate), shouldn't all the metrics reach 100%. Wouldn't that be the more natural default value when all samples are rejected?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1891/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698794785553,
            "cdate": 1698794785553,
            "tmdate": 1699636119081,
            "mdate": 1699636119081,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]