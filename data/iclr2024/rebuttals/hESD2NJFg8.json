[
    {
        "title": "Label-free Node Classification on Graphs with Large Language Models (LLMs)"
    },
    {
        "review": {
            "id": "c2ESuNg2pC",
            "forum": "hESD2NJFg8",
            "replyto": "hESD2NJFg8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2601/Reviewer_LGrX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2601/Reviewer_LGrX"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a novel approach called LLM-GNN for label-free node classification on graphs, which combines the strengths of Graph Neural Networks (GNNs) and Large Language Models (LLMs) while mitigating their limitations. It addresses the challenge of obtaining high-quality labels for graph-structured data by leveraging LLMs' zero-shot learning capabilities. LLM-GNN actively selects nodes for annotation by LLMs, generates confidence-aware annotations, and refines annotation quality through post-filtering. The approach achieves impressive results on a massive-scale dataset, OGBN-PRODUCTS, without the need for costly human annotations."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1. LLM-GNN presents an innovative approach to node classification on graphs by harnessing the complementary strengths of GNNs and LLMs. It acknowledges the challenges of obtaining high-quality labels and proposes a label-free solution, which is a significant contribution to the field of machine learning.\n2. The paper demonstrates the cost-effectiveness of LLM-GNN by achieving high accuracy on a large dataset with annotation costs under 1 dollar. This cost-efficient approach is particularly relevant for real-world applications with resource constraints.\n3. LLM-GNN offers a comprehensive methodology that not only utilizes LLMs for annotations but also considers active node selection, confidence-aware annotations, and post-filtering. This approach ensures the quality, representativeness, and diversity of annotations, addressing key challenges in label-free node classification."
                },
                "weaknesses": {
                    "value": "1. Since LLMs generate annotations without access to ground truth labels, there is a risk of noisy annotations.  It would be better to investigate the robustness of LLM-GNN to noisy annotations and potential strategies for mitigating their effects.\n2.  LLM-GNN's performance is demonstrated on a specific dataset (OGBN-PRODUCTS), and while it achieves impressive results, its generalizability to other datasets or domains is not thoroughly explored in the paper. The effectiveness of the approach in different scenarios and with various types of graphs should be investigated to assess its broader applicability.\n3. There could be better with a detired comparison on the economic perspective."
                },
                "questions": {
                    "value": "see the weakness"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2601/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2601/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2601/Reviewer_LGrX"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2601/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698582844710,
            "cdate": 1698582844710,
            "tmdate": 1700493886070,
            "mdate": 1700493886070,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ez3Iqq9lnm",
                "forum": "hESD2NJFg8",
                "replyto": "c2ESuNg2pC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2601/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2601/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer LGrX (1/3)"
                    },
                    "comment": {
                        "value": "Q1: Since LLMs generate annotations without access to ground truth labels, there is a risk of noisy annotations. It would be better to investigate the robustness of LLM-GNN to noisy annotations and potential strategies for mitigating their effects. \n\n**Response** Thanks for your great comments. We'll first summarize our findings and show details in the next paragraph. (1) We find that normal cross-entropy loss has a smaller overfitting on LLMs' annotations compared to random noisy labels, which is one advantage of using LLMs as the annotators; (2) We find that (graph) noisy label learning methods do not show clear improvements compared to normal cross-entropy loss in our setting. **Simple normal and weighted cross-entropy loss works better**. \n\n(1) To show the characteristics of LLMs' annotations, we train GNN models with ground truth labels, LLMs' annotations, and synthetic noisy labels with the same annotation quality (for example, suppose LLMs correctly label 80% of the points, we will randomly replace the labels of 20% of the points with incorrect labels based on the ground truth labels. ) We put the training curves in Appendix J and show the maximum test accuracies and final test accuracies in the following table. We observe that, unlike synthetic noisy labels, the gap between maximum accuracy and final accuracy for LLMs is very small. On one hand, it shows one good point of LLMs' annotations that we don't use a validation set to find the stopping point. On the other, it implies that it may be difficult to design a loss function to improve the performance. \n\n|                        \t|      Cora      \t|       Cora       \t|    CiteSeer    \t|     CiteSeer     \t|\n|----------------------|--------------|----------------|--------------|----------------|\n|                        \t| Final accuracy \t| Maximum accuracy \t| Final accuracy \t| Maximum accuracy \t|\n|   Ground truth labels  \t|      0.82      \t|       0.825      \t|       0.7      \t|       0.73       \t|\n|   LLMs's annotations   \t|      0.705     \t|       0.71       \t|      0.65      \t|       0.68       \t|\n| Synthetic noisy labels \t|      0.58      \t|       0.76       \t|      0.51      \t|       0.62       \t|\n\n(2) We further compare the differences among different loss functions. We compare the following four baselines: random selection and normal cross-entropy loss; random selection and NCERCE loss (design for noisy label learning) [1]; RIM with normal cross-entropy loss; random selection and weighted cross entropy loss; original RIM with weighted loss. It should be noted that RIM is an active learning method designed to process synthetic noisy labels. From the table, we find that loss designed for noisy label learning doesn't present an advantage over normal and weighted cross-entropy loss. One possible reason for this behavior is that the noisy pattern of LLMs' annotations is much more complex than synthetic noisy labels (see Appendix G.1), while noisy label learning methods are usually evaluated on synthetic noisy labels. Another reason is that the number of training samples is limited so it's hard to capture the pattern of noisy annotations. Compared to noisy label learning loss, we find that a simple weighted cross entropy loss (\"WE\" in the table) works well in most cases. \n\n|                \t|  Cora \t| CiteSeer \t|\n|--------------|-----|--------|\n|    Random+CE   \t| 70.48 \t|   65.11  \t|\n| Random+NCE+RCE \t| 61.18 \t|   67.75  \t|\n|     RIM+CE     \t| 67.39 \t|   64.37  \t|\n|    Random+WE\t    | 71.85\t    |   66.72   |\n|       RIM      \t| 68.28 \t|   63.06  \t|\n\n\n[1] Ma X, Huang H, Wang Y, et al. Normalized loss functions for deep learning with noisy labels[C]//International conference on machine learning. PMLR, 2020: 6543-6553."
                    }
                },
                "number": 25,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2601/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700289250437,
                "cdate": 1700289250437,
                "tmdate": 1700289250437,
                "mdate": 1700289250437,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "COsg8LZULo",
                "forum": "hESD2NJFg8",
                "replyto": "c2ESuNg2pC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2601/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2601/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer LGrX (2/3)"
                    },
                    "comment": {
                        "value": "Q2: LLM-GNN's performance is demonstrated on a specific dataset (OGBN-PRODUCTS), and while it achieves impressive results, its generalizability to other datasets or domains is not thoroughly explored in the paper. The effectiveness of the approach in different scenarios and with various types of graphs should be investigated to assess its broader applicability.\n\n**Response** Thanks for your great comments. We are not quite sure whether we exactly understand some details of your remarks. We will first retell the key points and answer them accordingly. \n\nWe think you make the following key points: (1) Only the results of OGBN-Products are demonstrated. (2) Whether the proposed pipeline can be extended to more tasks like graph classification and more types of graphs beyond text-attributed graphs needs to be explored. \n\nFor the first point, we admit we focus on the performance of OGBN-Products in the abstract and introduction part, which may lead to confusion. **We also test five other datasets in the experiment part.** For the second question, we think our pipelines have the potential to be applied to **more types of graphs and more types of tasks**. \n\n\n(1) Other than the performance on OGBN-PRODUCTS, we also show the performance of our methods on other datasets like Cora, CiteSeer, Pubmed, WikiCS, and Arxiv in the paper, and we will show them in the following table. Specifically, Cora, CiteSeer, and Arxiv are about papers from the computer science domain. Pubmed is about the papers from the medical domain. WikiCS is about the Wikipedia page. We have included multiple datasets from different domains to show the general applicability of our methods. Moreover, our pipeline shows consistent effectiveness across these datasets. In the following table, we demonstrate that our pipelines can get good performance with a much lower cost (both money and time) and scale to large graphs. \n\n| Performance        \t| Cora  \t| CiteSeer \t| PubMed \t| WikiCS \t| Arxiv \t| Products \t|\n|--------------------|------|----------|--------|--------|-------|----------|\n| LLMGNN             \t| 75.54 \t| 69.06    \t| 81.95  \t| 66.09  \t| 66.14 \t| 74.91    \t|\n| LLMs-as-Predictors \t| 67.33 \t| 66.33    \t| 87.33  \t| 71     \t| 73.67 \t| 75.33    \t|\n\n| Costs              \t| Cora  \t| CiteSeer \t| PubMed \t| WikiCS \t| Arxiv \t| Products \t|\n|--------------------|-------|----------|--------|--------|-------|----------|\n| LLMGNN             \t| 0.11  \t| 0.1      \t| 0.25   \t| 0.16   \t| 0.63  \t| 0.74     \t|\n| LLMs-as-Predictors \t| 1.26  \t| 1.5      \t| 9.2    \t| 5.46   \t| 79    \t| 1952     \t|\n\n(2) We admit that our paper focuses on the node classification tasks on the text-attributed graphs. However, we think our pipeline has the potential to be extended to more types of tasks and graphs. \n\nFor different tasks like graph classification and knowledge graph QA, [1] does some exploration, and LLMs also show promising zero-shot capabilities. As a result, we may extend our pipelines to these tasks with some small modifications to the architectures. \n\nFor more types of graphs, in a recent paper [2], the authors introduced a prompt-based method that aligns all types of node features through natural language, allowing large language models to naturally process problems in the textual domain with notable success. For discrete features like bag of words, [3] also propose a potential way to let large language models handle them by converting the original features into clustering centers. These methods may be integrated with our work and extend our methods to more types of graphs. We think you make two valuable points for future directions. \n\n[1] Guo J, Du L, Liu H. GPT4Graph: Can Large Language Models Understand Graph Structured Data? An Empirical Evaluation and Benchmarking[J]. arXiv preprint arXiv:2305.15066, 2023.\n\n[2] Liu H, Feng J, Kong L, et al. One for All: Towards Training One Graph Model for All Classification Tasks[J]. arXiv preprint arXiv:2310.00149, 2023.\n\n[3] Zhao J, Zhuo L, Shen Y, et al. Graphtext: Graph reasoning in text space[J]. arXiv preprint arXiv:2310.01089, 2023."
                    }
                },
                "number": 26,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2601/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700289366990,
                "cdate": 1700289366990,
                "tmdate": 1700289366990,
                "mdate": 1700289366990,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nZKwKFmFYc",
                "forum": "hESD2NJFg8",
                "replyto": "c2ESuNg2pC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2601/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2601/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer LGrX (3/3)"
                    },
                    "comment": {
                        "value": "Q3: There could be better with a detailed comparison from the economic perspective.\n\n**Response** Thanks for your great comments, which inspire us to further compare both the economy and performance of our pipelines. Compared to other baselines, our methods can **achieve promising performance with very low costs**.  \n\n\nWe use the following data to validate our claims. The first table demonstrates the performance of each pipeline. For LLMGNN and GNN, we deliberately let them produce similar performances to conduct a fair cost comparison. For Pubmed*, we demonstrate the performance of LLMGNN with a larger budget since the performance of LLMs-as-Predictors is superior on this dataset. We can improve the performance of LLMGNN by slightly increasing the costs.\n|     (Accuracy)     \t|  Cora \t| CiteSeer \t| Pubmed \t| Pubmed* \t| WikiCS \t| Arxiv \t| Products \t|\n|------------------|-----|--------|------|-------|------|-----|--------|\n|       LLMGNN       \t| 75.54 \t|   69.06  \t|  74.98 \t|  81.95  \t|  66.09 \t| 66.14 \t|   74.9   \t|\n| LLMs-as-Predictors \t| 68.33 \t|   66.33  \t|  87.33 \t|  87.33  \t|   71   \t| 73.33 \t|   75.33  \t|\n|         GNN        \t| 75.54 \t|   69.06  \t|  74.98 \t|  81.95  \t|  66.09 \t| 66.14 \t|   74.9   \t|\n\nThe second table compares the number of training samples needed to achieve the performance in the first table. Here, PL stands for pseudo labels generated by LLMs. GT stands for ground truth labels.\n| (Number of   annotations) \t| Cora \t| CiteSeer \t| Pubmed \t| Pubmed* \t| WikiCS \t|  Arxiv \t| Products \t|\n|-------------------------|----|--------|------|-------|------|------|--------|\n|         LLMGNN (PL)        \t|  140 \t|    120   \t|   60   \t|   300   \t|   200  \t|   800  \t|    940   \t|\n|   LLMs-as-Predictors (PL)  \t| 2708 \t|   3186   \t|  19717 \t|  19717  \t|  11701 \t| 169343 \t|  2449029 \t|\n|          GNN (GT)         \t|  50  \t|    50    \t|   42   \t|   210   \t|   40   \t|   560  \t|    400   \t|\n\n\nFrom these two tables, we can see that **LLMGNN significantly reduces the costs compared to LLMs-as-Predictors**. Comparing GNN with LLMGNN, the costs of ground truth labels are hard to estimate. From a recent study [1], it shows that the annotations generated by ChatGPT are more than 20 times cheaper than human annotators. If we use this ratio, we can see that although LLMGNN requires more pseudo labels, the overall costs are still much cheaper than traditional GNN-based pipelines. \n\n[1] Gilardi F, Alizadeh M, Kubli M. Chatgpt outperforms crowd-workers for text-annotation tasks[J]. arXiv preprint arXiv:2303.15056, 2023."
                    }
                },
                "number": 27,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2601/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700289479792,
                "cdate": 1700289479792,
                "tmdate": 1700289479792,
                "mdate": 1700289479792,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1qq3axrUms",
                "forum": "hESD2NJFg8",
                "replyto": "c2ESuNg2pC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2601/Reviewer_LGrX"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2601/Reviewer_LGrX"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your response"
                    },
                    "comment": {
                        "value": "Thanks for your response which solved all of my concerns. I have raised my score accordingly."
                    }
                },
                "number": 31,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2601/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700493874472,
                "cdate": 1700493874472,
                "tmdate": 1700493874472,
                "mdate": 1700493874472,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "7SQadl2a8U",
            "forum": "hESD2NJFg8",
            "replyto": "hESD2NJFg8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2601/Reviewer_HZKD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2601/Reviewer_HZKD"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors study label-free node classification by combining LLMs with GNNs. Specifically, the proposed method first leverages LLMs to annotate a small portion of nodes, then uses GNNs with the pseudo-labels to classify the remaining large portion of nodes. The main challenges lie in how to actively select nodes and leverage LLMs to obtain reliable labels for those nodes. Three modules, including difficulty-aware active node selection, confidence-aware annotations, and post-filtering are proposed to tackle the challenges. Experimental results on text-attributed graphs demonstrate the effectiveness of the proposed method, especially compared with heuristically choosing annotated nodes."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The proposed method is among the first trials of combining LLMs with GNNs to solve a novel problem, i.e., label-free node classification.\n2. The proposed method is clearly described and the paper is easy to follow in general.\n3. The authors compare with various heuristic baselines and conduct analyses to demonstrate the efficacy of the proposed method."
                },
                "weaknesses": {
                    "value": "1.\tThough I acknowledge that the proposed method is a valid solution, the technical contribution of the paper is somewhat limited, especially considering that the three major components are largely based on heuristic observations, and the rest are based on existing LLMs and GNNs. It would make the paper stronger if some theoretical analyses could be provided for the proposed components.  \n2.\tThe authors should more explicitly mention that their proposed method only works for text-attributed graphs rather than any general graph, e.g., in the abstract and introduction. Otherwise, the paper may have overclaiming issues.  \n3.\tIn generating the initial node labels using LLMs, it seems that only the feature information is utilized and no structure is considered. Since it is well-known in the graph machine learning literature that both features and structures greatly affect the node labels, there exists a large room for improvement.    \n4.\tThere are some missing related works regarding zero-shot node classification such as [1-2], which should be added.  \n5.\tI also wonder how different LLMs affect the model (the reported results are all based on GPT-3.5-turbo).   \n\n[1] Zero-shot Node Classification with Decomposed Graph Prototype Network, KDD\u201921  \n[2] Dual Bidirectional Graph Convolutional Networks for Zero-shot Node Classification, KDD\u201922"
                },
                "questions": {
                    "value": "See Weaknesses above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2601/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2601/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2601/Reviewer_HZKD"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2601/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698646443339,
            "cdate": 1698646443339,
            "tmdate": 1700642012338,
            "mdate": 1700642012338,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "I7QRrBdqSE",
                "forum": "hESD2NJFg8",
                "replyto": "7SQadl2a8U",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2601/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2601/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Q1: Though I acknowledge that the proposed method is a valid solution, the technical contribution of the paper is somewhat limited, especially considering that the three major components are largely based on heuristic observations, and the rest are based on existing LLMs and GNNs. It would make the paper stronger if some theoretical analyses could be provided for the proposed components.\n\n**Response** Thanks for your great comments. We first summarize our responses and then provide more details. (1) We agree that theoretical analysis is important to support the effectiveness of our proposed methods. We use theoretical analysis to show why difficulty-aware selection can get high-quality annotations. We also add the following part into Appendix L. (2) We would like to strengthen our contributions: 1. We propose a flexible pipeline LLMGNN which demonstrates effectiveness with low costs; 2. We propose three effective strategies: difficulty-aware selection, confidence-aware annotations, and post-filtering to further enhance the effectiveness of LLMGNN.\n\nQ1 (1) One important heuristic used in our pipeline is difficulty-aware selection. Here, we analyze why it can enhance the annotation quality of LLMs. \n\n\n\nGiven the parameter distribution of LLMs $\\mathcal{Q}$, the parameter distribution of encoder $\\mathcal{P}$ (SBERT), we assume ground truth $Y\\_L\\in \\mathcal{R}^{N \\times M}$, pseudo label $Y \\in \\mathcal{R}^{N \\times M}$,  and node features encoded by the encoder $X\\in \\mathcal{R}^{N \\times d}$. Here, $M$ denotes the number of classes and $d$ denotes the hidden dimension. Our objective is to maximize the accuracy of annotations, which is thus to minimize the discrepancies between $Y$ and $Y\\_{L}$. \n\n$$\n\\\\min\\_{(n\\_{1},...,n\\_{k})}{\\\\mathbb{E}\\_{\\\\theta\\\\sim Q}{f(Y)}} = \\\\ell (Y,Y\\_{L})=\\\\sum\\_{i=1}^{k} \\\\ell({y}^{n\\_{i}} , y\\_{L}^{n\\_{i}})\n$$\n\nwhere $(n\\_{1},...,n\\_{k})$ is the index of the $k$ selected nodes, and $f(Y)$ is defined as follows, where ${x}^{n\\_{i}}$ represents the feature of node ${n\\_{i}}$, while $x\\_{L}^{n\\_{i}}$ represents the unknown latent embedding of node ${n\\_{i}}$. \n$$\nf(Y)=\\\\ell(Y,Y\\_{L})=\\\\sum\\_{i=1}^{k} \\\\ell({y}^{n\\_{i}} , y\\_{L}^{n\\_{i}})= \\\\ln(1+\\\\sum\\_{i=1}^{k}\\\\|{x}^{n\\_{i}} - x\\_{L}^{n\\_{i}}\\\\|^{2})= \\\\ln(1+\\\\|X-X\\_{L}\\\\|^{2})\n$$\n\n\nSince we only have access to the node features $X$ generated by encoder $\\theta$, we need to make a connection between $\\mathcal{Q}$ and $\\mathcal{P}$. \n\nLemma 1. For any annotation $y$ generated by LLMs, $\\mathbb{E}\\_{\\theta\\sim \\mathcal{Q}}f(y)\\leq \\log \\mathbb{E}\\_{\\theta\\sim \\mathcal{P}} \\exp(f(y))+KL( \\mathcal{Q} \\| \\mathcal{P})$\n\nProof:\n$$\n\\\\begin{aligned}\n&\\\\\\\\\n&{\\\\mathbb{E}\\_{\\\\theta^{\\\\prime}\\\\sim \\\\mathcal{P}}{f(y)}}=\\\\int f(y) p(y) d y=\\\\int f(y) \\\\frac{p(y)}{q(y)} q(y) d y=\\\\mathbb{E}\\_{\\\\theta\\\\sim \\\\mathcal{Q}} f(y) \\\\frac{p(y)}{q(y)}\\\\\\\\\n&\\\\text{Since}\\\\quad KL(\\\\mathcal{Q} \\\\| \\\\mathcal{P})=\\\\mathbb{E}\\_{\\\\theta\\\\sim Q} \\\\log \\\\frac{q(y)}{p(y)}\\\\\\\\\n&\\\\log \\\\mathbb{E}\\_{\\\\theta^{\\\\prime}\\\\sim \\\\mathcal{P}} f(y)= \\\\log\\\\mathbb{E}\\_{\\\\theta\\\\sim \\\\mathcal{Q}} f(y) \\\\frac{p(y)}{q(y)} \\\\geq\\\\mathbb{E}\\_{\\\\theta\\\\sim \\\\mathcal{Q}}\\\\log f(y) \\\\frac{p(y)}{q(y)}=\\\\mathbb{E}\\_{\\\\theta\\\\sim \\\\mathcal{Q}}\\\\log f(y)-KL( \\\\mathcal{Q} \\\\| \\\\mathcal{P}) \\\\\\\\\n\\\\end{aligned}\n$$\n\nThen, we transform the objective into \n$$\n\\\\min\\_{(n\\_{1},...,n\\_{k})} \\\\mathbb{E}\\_{\\\\theta^{\\\\prime}\\\\sim \\\\mathcal{P}} \\\\exp(f(Y)) \n$$\n\nAssuming the label distribution $\\\\mathcal{H}$, we further have\n$$\n\\\\min\\_{(n\\_{1},...,n\\_{k})} \\\\mathbb{E}\\_{Y\\\\sim \\\\mathcal{H}} \\\\mathbb{E}\\_{\\\\theta^{\\\\prime}\\\\sim \\\\mathcal{P}} \\\\exp(f(Y)) = \\\\min\\_{(n\\_{1},...,n\\_{k})} \\\\mathbb{E}\\_{Y\\\\sim \\\\mathcal{H}} \\\\mathbb{E}\\_{\\\\theta\\\\sim \\\\mathcal{P}}\\\\|X-X\\_{L}\\\\|^{2} \\\\\\\\\n$$\n\nAssuming $X$ and $X\\_{L}$ follows gaussian distribution, where $X\\sim N(\\mu\\_{i},\\sigma\\_{i});X\\_L\\sim N(\\mu\\_{j},\\sigma\\_{j})$, then\n\n$$\n\\\\begin{aligned}\n\\\\mathrm{E}\\\\left(\\\\|X-X\\_{L}\\\\|^{2}\\\\right) & =\\\\mathrm{E}\\\\left(\\\\left\\\\|\\\\left(X-\\\\mu\\_{i}\\\\right)-\\\\left(X\\_{L}-\\\\mu\\_{j}\\\\right)+\\\\left(\\\\mu\\_{i}-\\\\mu\\_{j}\\\\right)\\\\right\\\\|^{2}\\\\right) \\\\\\\\\n& =\\\\mathrm{E}\\\\left(\\\\left\\\\|X-\\\\mu\\_{i}\\\\right\\\\|^{2}\\\\right)+\\\\mathrm{E}\\\\left(\\\\left\\\\|X\\_{L}-\\\\mu\\_{j}\\\\right\\\\|^{2}\\\\right)+\\\\left\\\\|\\\\mu\\_{i}-\\\\mu\\_{j}\\\\right\\\\|^{2} \\\\\\\\\n& =n \\\\sigma\\_{i}^{2}+n \\\\sigma\\_{j}^{2}+\\\\left\\\\|\\\\mu\\_{i}-\\\\mu\\_{j}\\\\right\\\\|^{2}\n\\\\end{aligned}\n$$\n\nSince $(\\mu\\_{j},\\sigma\\_{j})$ is unknown, $\\mu\\_{i}$ is fixed, thus we want to minimize $\\sigma\\_{i}$.  Given an arbitrary node, $\\sigma\\_{i}$ can be viewed as the distance to the clustering centers. The smaller $\\sigma\\_{i}$ is, the smaller the corresponding $\\mathrm{E}\\left(\\|X-X\\_{L}\\|^{2}\\right)$ also becomes, indicating that the minimum value of $\\mathbb{E}\\_{\\theta\\sim \\mathcal{Q}}f(y)$ is attained when $\\sigma\\_{i}$ is at its smallest. This demonstrates why nodes closer to the clustering centers are \"preferred\" by LLMs and thus achieve better annotation quality."
                    },
                    "title": {
                        "value": "Response to Reviewer HZKD (1/6)"
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2601/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700288676684,
                "cdate": 1700288676684,
                "tmdate": 1700289057893,
                "mdate": 1700289057893,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nKeUpiglbI",
                "forum": "hESD2NJFg8",
                "replyto": "7SQadl2a8U",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2601/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2601/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Q1: Though I acknowledge that the proposed method is a valid solution, the technical contribution of the paper is somewhat limited, especially considering that the three major components are largely based on heuristic observations, and the rest are based on existing LLMs and GNNs. It would make the paper stronger if some theoretical analyses could be provided for the proposed components.\n\nQ1 (2) Besides LLMGNN we propose, we want to mention that as far as we know, we are the first to study how to improve the annotations generated by a real-world annotator. Previous works like [1] all use synthetic noisy labels, whose patterns are much simpler. \n\n[1] Zhang W, Wang Y, You Z, et al. Rim: Reliable influence-based active learning on graphs[J]. Advances in Neural Information Processing Systems, 2021, 34: 27978-27990."
                    },
                    "title": {
                        "value": "Response to Reviewer HZKD (2/6)"
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2601/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700288729101,
                "cdate": 1700288729101,
                "tmdate": 1700289068979,
                "mdate": 1700289068979,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kXwSqZ732O",
                "forum": "hESD2NJFg8",
                "replyto": "7SQadl2a8U",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2601/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2601/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Q2: The authors should more explicitly mention that their proposed method only works for text-attributed graphs rather than any general graph, e.g., in the abstract and introduction. Otherwise, the paper may have overclaiming issues.\n\n**Response** Thanks for your great comments. We agree that our pipelines focus more on text-attributed graphs, while our methods have the potential to be applied to more types of graphs. We add discussions on this problem in the revision. \n\nCurrently, for the task of node classification, most datasets feature text-type nodes, and thus our method is well-suited for the majority of datasets and tasks. Furthermore, our approach has the potential to be extended to more graphs with different types of node features. In a recent paper [1], the authors introduced a prompt-based method that aligns all types of node features through natural language, allowing large language models to naturally process problems in the textual domain with notable success. For discrete features like bag of words, [2] also propose a potential way to let large language models handle them by converting the original features into clustering centers. These methods may be integrated with our work and extend our methods to more types of graphs. Your point is incredibly insightful and valuable, and we consider it an excellent extension of our work. In our revised version, we have included a discussion about this issue in both the abstract, introduction, and preliminaries.\n\n[1] Liu H, Feng J, Kong L, et al. One for All: Towards Training One Graph Model for All Classification Tasks[J]. arXiv preprint arXiv:2310.00149, 2023.\n\n[2] Zhao J, Zhuo L, Shen Y, et al. Graphtext: Graph reasoning in text space[J]. arXiv preprint arXiv:2310.01089, 2023."
                    },
                    "title": {
                        "value": "Response to Reviewer HZKD (3/6)"
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2601/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700288767751,
                "cdate": 1700288767751,
                "tmdate": 1700289076021,
                "mdate": 1700289076021,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YoLNGbcDzK",
                "forum": "hESD2NJFg8",
                "replyto": "7SQadl2a8U",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2601/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2601/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Q3: In generating the initial node labels using LLMs, it seems that only the feature information is utilized and no structure is considered. Since it is well-known in the graph machine learning literature that both features and structures greatly affect the node labels, there exists a large room for improvement.\n\n**Response** Thanks for your great comments, which inspire us to have a further look at structure-aware prompts. We find that incorporating structural information in the annotation prompt **can not largely improve performance** with the current prompt design. \n\n\nWe demonstrate both the accuracy of LLMs' annotations (the ratio of LLMs' annotations matching ground truth labels) and the accuracy of LLM-GNN in the following table. \"No struct\" means the \"hybrid\" (combining both \"TopK\" and \"Most Voting\") prompt strategy used in our papers. \"Struct\" is the new version that combines the \"neighbor summarization\" strategy in [1] and the \"hybrid\" prompt strategy.\n\n|       (Annotation)            \t|         Cora    \t|       Cora   \t|      CiteSeer \t|     CiteSeer    \t|\n|-----------------|---------------|------------|-------------|---------------|\n|                   \t|      No Struct  \t|      Struct  \t|     No Struct \t|      Struct     \t|\n|        FeatProp   \t|   70.71 \u00b1 0.00  \t| 64.52 \u00b1 0.34 \t|  66.39 \u00b1 2.83 \t|  67.50  \u00b1 0.65  \t|\n|     FeatProp + PS \t|    73.81 \u00b1 1.52 \t| 63.69 \u00b1 1.11 \t|  78.12 \u00b1 1.07 \t|   78.12 \u00b1 1.45  \t|\n\n|         (LLMGNN)          \t|         Cora    \t|        Cora   \t|      CiteSeer \t|     CiteSeer    \t|\n|-----------------|---------------|-------------|-------------|---------------|\n|                   \t|      No Struct  \t|       Struct  \t|     No Struct \t|      Struct     \t|\n|        FeatProp   \t|    72.82 \u00b1 0.08 \t|  69.08 \u00b1 0.39 \t| 66.61 \u00b1 0.55  \t|   65.92 \u00b1 0.43  \t|\n|     FeatProp + PS \t|    75.54 \u00b1 0.34 \t|  67.55 \u00b1 0.70 \t|  69.06 \u00b1 0.32 \t|   67.80 \u00b1 0.45  \t|\n\nWe observe that:\n1. Incorporating structural information in the annotation prompts can not improve the performance, and may affect the effectiveness of post-selection. For example, the annotation quality even drops after adding structural information. \n2. Incorporating structural information will increase the costs of prompts since the length of prompts becomes longer. \n\nBased on these points, we think that using text-only annotation prompts for LLMs is appropriate in the current stage. However, we also admit that how incorporating structural information in LLMs is a valuable future direction.\n\n[1] Chen Z, Mao H, Li H, et al. Exploring the potential of large language models (llms) in learning on graphs[J]. arXiv preprint arXiv:2307.03393, 2023."
                    },
                    "title": {
                        "value": "Response to Reviewer HZKD (4/6)"
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2601/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700288879856,
                "cdate": 1700288879856,
                "tmdate": 1700289084733,
                "mdate": 1700289084733,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "oDRUppVrWW",
                "forum": "hESD2NJFg8",
                "replyto": "7SQadl2a8U",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2601/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2601/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Q4: There are some missing related works regarding zero-shot node classification such as [1-2], which should be added. \n\n**Response** Thanks for your great comments which inspire us to further explore the effectiveness of our paper under the **zero-shot node classification** setting. We first summarize our responses and explain the details. (1) We've cited those related works and add a discussion section. However, **there are some differences between the setting** of label-free node classification and zero-shot node classification; (2) For zero-shot node classification, our methods can still achieve promising performance. \n\n\n(1) The setting for label-free node classification involves training a GNN using annotations generated by LLMs, without any pre-existing labels. In contrast, the zero-shot node classification setting in these two papers involves training a model with training data for training classes (for example, A, B, C), which can then be generalized to new classes in the testing phase (for example, D, E, F). In practice, our label-free methods can be used to solve zero-shot node classification by directly annotating the new classes. However, those zero-shot node classification methods can not be directly applied to label-free node classification. \n\n(2) In the following table, we compare the performance of LLMGNN to zero-shot node classification baselines DGPN [1], DBiGCN [2], and GraphCEN [3] in zero-shot node classification settings. For LLMGNN, we use the featprop + Post Selection with 20% pruning ratio and 140 annotation budget. We take the experimental settings from [3]. \n\n|          \t| Cora  \t| CiteSeer \t|\n|----------|-------|----------|\n| DGPN     \t| 33.76 \t| 37.74    \t|\n| DBiGCN   \t| 45.08 \t| 38.57    \t|\n| GraphCEN \t| 48.43 \t| 40.77    \t|\n| LLMGNN   \t| 79.45 \t| 66.67    \t|\n\nWe can see that our methods can consistently beat all those baselines under the zero-shot node classification setting. We appreciate your comments which inspire us to demonstrate the strong capability of our methods in traditional zero-shot node classification problems. \n\n[1] Wang Z, Wang J, Guo Y, et al. Zero-shot node classification with decomposed graph prototype network[C]//Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining. 2021: 1769-1779.\n\n[2] Yue Q, Liang J, Cui J, et al. Dual Bidirectional Graph Convolutional Networks for Zero-shot Node Classification[C]//Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 2022: 2408-2417.\n\n[3] Ju W, Qin Y, Yi S, et al. Zero-shot Node Classification with Graph Contrastive Embedding Network[J]. Transactions on Machine Learning Research, 2023."
                    },
                    "title": {
                        "value": "Response to Reviewer HZKD (5/6)"
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2601/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700288944374,
                "cdate": 1700288944374,
                "tmdate": 1700289092568,
                "mdate": 1700289092568,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "y97jD8yAEA",
                "forum": "hESD2NJFg8",
                "replyto": "7SQadl2a8U",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2601/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2601/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Q5: I also wonder how different LLMs affect the model (the reported results are all based on GPT-3.5-turbo).\n\n**Response** Thanks for your great comments. We have added experiments using other large language models like GPT-4. Surprisingly, **we find that GPT4 brings no clear improvement compared to the results of GPT-3.5-turbo.**\n\nWe show the results in the following table. We can see that GPT4 only achieves clear improvements in **DA-GraphPart** on Cora. We appreciate your questions to help us have an exploration for this interesting phenomenon. \n\n\n|   (LLMGNN)   \t|     Cora     \t|     Cora     \t|    CiteSeer    \t|   CiteSeer   \t|\n|------------|------------|------------|--------------|------------|\n|              \t|    GPT3.5    \t|     GPT4     \t|     GPT3.5     \t|     GPT4     \t|\n|    Random    \t| 70.48 \u00b1 0.73 \t| 70.03 \u00b1 1.42 \t| 65.11   \u00b1 1.12 \t| 67.00 \u00b1 1.21 \t|\n|   GraphPart  \t| 69.54 \u00b1 2.18 \t| 70.65 \u00b1 0.98 \t|  66.59 \u00b1 1.34  \t| 65.66 \u00b1 0.74 \t|\n| DA-GraphPart \t| 69.64 \u00b1 0.30 \t| 75.06 \u00b1 0.08 \t|  69.88 \u00b1 1.81  \t| 69.92 \u00b1 1.10 \t|\n|   FeatProp   \t| 72.82 \u00b1 0.08 \t| 69.46 \u00b1 0.69 \t|  66.61 \u00b1 0.55  \t| 66.36 \u00b1 0.77 \t|\n|  PS-FeatProp \t| 75.54 \u00b1 0.34 \t| 71.76 \u00b1 0.33 \t|  69.06 \u00b1 0.32  \t| 69.15 \u00b1 1.06 \t|\n\nOne possible reason for this phenomenon is that the primary area where GPT-4 surpasses GPT-3.5 is in complex reasoning capabilities. However, in tasks such as text classification, the difference between the two is not very significant. This phenomenon has also been observed in text classification tasks in other fields, such as financial text classification [1]. \n\n[1] Loukas L, Stogiannidis I, Malakasiotis P, et al. Breaking the bank with chatgpt: Few-shot text classification for finance[J]. arXiv preprint arXiv:2308.14634, 2023."
                    },
                    "title": {
                        "value": "Response to Reviewer HZKD (6/6)"
                    }
                },
                "number": 24,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2601/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700289019446,
                "cdate": 1700289019446,
                "tmdate": 1700289100798,
                "mdate": 1700289100798,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ONr7eETS2T",
                "forum": "hESD2NJFg8",
                "replyto": "y97jD8yAEA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2601/Reviewer_HZKD"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2601/Reviewer_HZKD"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the rebuttal"
                    },
                    "comment": {
                        "value": "Thanks for the rebuttal, which I do think improve the quality of the paper. I have improved my score accordingly.\n\nI also have a follow-up question. In the newly provided results, it seems neither structural information nor advanced LLM can further improve the results. Does that somewhat implicate that, the task of assigning initial node labels is acturally very easy, so that GPT-3.5 with only textual information is nearly perfect  Or maybe the initial node labels themselves are not very critical, so that the results are not sensitive to them?"
                    }
                },
                "number": 30,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2601/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700468288817,
                "cdate": 1700468288817,
                "tmdate": 1700468288817,
                "mdate": 1700468288817,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FvKY6JUNjY",
                "forum": "hESD2NJFg8",
                "replyto": "jY2gjntQoh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2601/Reviewer_HZKD"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2601/Reviewer_HZKD"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the rebuttal"
                    },
                    "comment": {
                        "value": "Thanks for the clarification and I have improved my score accordingly. I encourage the authors to incorporate these discussions in the revision."
                    }
                },
                "number": 37,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2601/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700642103545,
                "cdate": 1700642103545,
                "tmdate": 1700642103545,
                "mdate": 1700642103545,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "qpFCDvj1cs",
            "forum": "hESD2NJFg8",
            "replyto": "hESD2NJFg8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2601/Reviewer_JunD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2601/Reviewer_JunD"
            ],
            "content": {
                "summary": {
                    "value": "Given a text attributed graph without labels, the paper proposes a cost-effective method that combines LLMs and GNNs to annotate the labels in four steps. In the first step, the paper selects the nodes that should be annotated and terms it  as difficulty aware selection. It combines active learning (selection) techniques along with a difficulty score which is based on the distance from the center of a cluster. In the second step, annotations are created for the selected nodes using an LLM which also generates a confidence score. In the third step, nodes are pruned such that the confidence score of LLMs is high without significantly changing the diversity of nodes (via change in entropy). Finally, a GNN is trained on the graph to generate labels for other nodes."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The idea of label-free annotation using LLMs on text attributed graphs is an interesting research direction introduced by the paper.\n\n- The paper has experimented with different datasets and incorporated various existing techniques to come up with a cost-effective model.\n\n- This paper appropriately balanced traditional graph active selection criteria with annotation quality by incorporating difficulty-aware active selection with post filtering to obtain training nodes from LLM."
                },
                "weaknesses": {
                    "value": "- Difficulty aware (DA) selection: According to the paper, LLMs annotation quality degrades when they have to annotate nodes which are away from the centers. It implies that the LLMs annotation quality would suffer in case of the diverse nodes (away from center).  However, GNNs accuracy will only improve if the nodes are diverse. Hence, difficulty aware selection i.e. use of c-density might not always help and, in fact, it may hinder in some cases. This is also evident from the results shown in Table 2 : Active_Selection_Methods and the corresponding  DA-Active_Selection_Methods show similar performance on average across different techniques (i.e., selection methods). Moreover, for at least 50% of the cases, the DA-method (row 2) performs poorly compared to the corresponding active learning method (row 1). \n\n- Though the accuracy from the LLM-GNN model is good (and of course, the model is efficient), it couldn't outperform LLM as a predictor (Table 3).\n\n- The methods are heuristics and do not have theoretical evidence."
                },
                "questions": {
                    "value": "- Post-filtering (PS): How useful are the confidence scores generated by the LLMs (Appendix F.1, table 6)? Showing the mean and variance of confidence score for each dataset may help in understanding its impact on performance.\n\n- Providing the number of nodes selected in each step in the experiment will also help in understanding the effects of the steps. For instance, could you please provide the number of selected nodes during active selection and DA-active selection? Also, what is the pruning ratio when applying PS? All this information can help in understanding the efficacy of the steps.\n\n- PS-DA-methods have not performed well in most cases compared to DA-methods (Table 2). Any insights on this can help in understanding these steps better.\n\nMinor Questions:\n\n- Detail explanation on f_{act}(vi) is missing\n\n- It is mentioned in page 5 that the detailed descriptions and full prompt examples are shown in Appendix D. However, it is missing any detailed descriptions.\n\nTypos:\n\n- Page 7: (4) Combing - supposed to be combining?\n\n- Page 3: GNN modelson- models on (space missing)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2601/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2601/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2601/Reviewer_JunD"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2601/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698683813412,
            "cdate": 1698683813412,
            "tmdate": 1700325671946,
            "mdate": 1700325671946,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "bLQGx4PW4Q",
                "forum": "hESD2NJFg8",
                "replyto": "qpFCDvj1cs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2601/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2601/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "W1: Difficulty aware (DA) selection: According to the paper, LLMs annotation quality degrades when they have to annotate nodes that are away from the centers. It implies that the LLMs annotation quality would suffer in the case of the diverse nodes (away from the center). However, GNNs accuracy will only improve if the nodes are diverse. Hence, difficulty aware selection i.e. use of c-density might not always help and, in fact, it may hinder in some cases. This is also evident from the results shown in Table 2 : Active\\_Selection\\_Methods and the corresponding DA-Active\\_Selection\\_Methods show similar performance on average across different techniques (i.e., selection methods). Moreover, for at least 50\\% of the cases, the DA-method (row 2) performs poorly compared to the corresponding active learning method (row 1).\n\n**Response** Thanks for your great comments. We want to first summarize our responses and then explain them in details. Applying the original DA may lead to clear performance degradation, which is due to the class imbalance problem by a small clustering center number $K$ on datasets like Pubmed. **This problem can be solved** by increasing $K$ to balance the diversity and annotation quality. \n\nThe reason for DA's bad performance on Pubmed is when performing K-means clustering with $C-Density$, the number of clustering centers $K$ is fixed as the number of classes. This approach can achieve good labeling quality in most cases. On datasets like Pubmed where the number of classes is small (only 3), the points selected by $C-Density$ tend to cause class imbalance (the annotation quality is still very good). To solve this problem, we find that by setting a larger value for the number of clustering centers $K$, the new $ C-Density^{\\prime}$ obtained can effectively solve this problem. We show the results in the following table. $k$ means the number of clustering centers to calculate $C-Density^{\\prime}$, $C$ means the number of classes, and budgets means the number of selected annotated nodes. We use AGE as a demonstration to show the idea. From the results, we can see that setting a larger $K$ can improve the performance of DA on Pubmed.\n\n|                  \t| \t Pubmed \t|\n|----------------|------|\n|        AGE       \t|  74.55 \t|\n|    DA-AGE(k=C)   \t|   55.36 \t|\n|   DA-AGE(k=2C)   \t|   57.38 \t|\n| DA-AGE(k=budget) \t|  75.71 \t|\n\n\nIn the following table, we demonstrate that DA can improve the performance of diversity-aware selection methods most of the time. \n\n|              \t|     Cora     \t|   CiteSeer   \t|    Pubmed    \t|    WikiCS    \t|\n|------------|------------|------------|------------|------------|\n|      AGE\t     \t| 69.15 \u00b1 0.38 \t| 54.25 \u00b1 0.31 \t| 74.55 \u00b1 0.54 \t| 55.51 \u00b1 0.12 \t|\n|    DA-AGE    \t| 74.38 \u00b1 0.24 \t| 59.92 \u00b1 0.42 \t| 74.20 \u00b1 0.51 \t| 59.39 \u00b1 0.21 \t|\n|      RIM     \t| 69.86 \u00b1 0.38 \t| 63.44 \u00b1 0.42 \t| 76.22 \u00b1 0.16 \t|  66.72\u00b1 0.16 \t|\n|    DA-RIM    \t| 73.99 \u00b1 0.44 \t| 60.33 \u00b1 0.40 \t| 79.17 \u00b1 0.11 \t|  67.82\u00b1 0.32 \t|\n|   GraphPart  \t| 68.57 \u00b1 2.18 \t| 66.59 \u00b1 1.34 \t| 77.50 \u00b1 1.23 \t| 67.28 \u00b1 0.87 \t|\n| DA-GraphPart \t| 69.35 \u00b1 1.92 \t| 69.37 \u00b1 1.27 \t| 79.49 \u00b1 0.85 \t| 68.72 \u00b1 1.01 \t|\n\nWe can see for 10/12 cases, DA significantly improves the performance. In other cases, it also achieves comparable performance."
                    },
                    "title": {
                        "value": "Response to Reviewer JunD (1/7)"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2601/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700286994762,
                "cdate": 1700286994762,
                "tmdate": 1700288473720,
                "mdate": 1700288473720,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UrNcwsgmYc",
                "forum": "hESD2NJFg8",
                "replyto": "qpFCDvj1cs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2601/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2601/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "W2: Though the accuracy from the LLM-GNN model is good (and of course, the model is efficient), it couldn't outperform LLM as a predictor (Table 3).\n\n**Response** Thanks for your great comments. Despite the superior performance of LLMs-as-Predictors in some datasets, it's not practical to be deployed in real-world scenarios because of **efficiency**. Our methods can **scale to large-scale graphs with promising performance**. \n\n Although the original LLMs-as-predictors can achieve good performance, it's not practical to be applied to even medium-scale datasets like Arxiv. In the original paper [1], it's only applied to a small subset of the datasets to evaluate the performance. Taking Arxiv as an example, considering OpenAI's current limit of 60 accesses per minute, completing all predictions on Arxiv would take 44 hours, whereas LLMGNN only requires a few minutes. Our methods extend the original pipeline to real-world large-scale datasets. \n\nThen, we compare the performance and the economic costs of these two pipelines to further demonstrate the effectiveness of our methods in the following two tables. The cost is estimated in units of dollars. On datasets like Citeseer, and Products, the gap between LLMGNN and LLMs-as-predictors is very small. On Cora, LLMGNN can even outperform LLMs-as-predictors. Only on PubMed, there's a relatively large gap between LLM-GNN and PubMed. It's because of LLMs' superior performance on this dataset, which is related to the shortcut prediction in attribute [1]. When LLMs' performance is superior, we can also add more budgets to further improve the performance of LLMGNN. ***For example, if we raise the budget in PubMed from 0.05 to 0.25, we can raise the accuracy of LLMGNN from 74.98 to 82.*** \n\n\n\n| Performance        \t| Cora  \t| CiteSeer \t| PubMed \t| WikiCS \t| Arxiv \t| Products \t|\n|--------------------|-------|----------|--------|--------|-------|----------|\n| LLMGNN             \t| 75.54 \t| 69.06    \t| 74.98  \t| 66.09  \t| 66.14 \t| 74.91    \t|\n| LLMs-as-Predictors \t| 67.33 \t| 66.33    \t| 87.33  \t| 71     \t| 73.67 \t| 75.33    \t|\n\n| Costs              \t| Cora  \t| CiteSeer \t| PubMed \t| WikiCS \t| Arxiv \t| Products \t|\n|--------------------|------|----------|--------|--------|-------|----------|\n| LLMGNN             \t| 0.11  \t| 0.1      \t| 0.05   \t| 0.16   \t| 0.63  \t| 0.74     \t|\n| LLMs-as-Predictors \t| 1.26  \t| 1.5      \t| 9.2    \t| 5.46   \t| 79    \t| 1952     \t|\n\nIn summary, the economic costs and efficiency of our methods are consistently much lower than LLMs-as-predictors. It's a valuable future direction to study how to further improve the performance of LLMGNN. \n\n\n[1] Chen Z, Mao H, Li H, et al. Exploring the potential of large language models (llms) in learning on graphs[J]. arXiv preprint arXiv:2307.03393, 2023."
                    },
                    "title": {
                        "value": "Response to Reviewer JunD (2/7)"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2601/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700287099077,
                "cdate": 1700287099077,
                "tmdate": 1700288482173,
                "mdate": 1700288482173,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fSdp01LcEP",
                "forum": "hESD2NJFg8",
                "replyto": "qpFCDvj1cs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2601/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2601/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "W3: The methods are heuristics and do not have theoretical evidence.\n\n**Response** Thanks for your great comments. We agree that theoretical analysis is important to support the effectiveness of our proposed methods. We show why Difficulty-aware selection can improve the annotation quality of LLMs. We also add the following part into Appendix L.\n\nGiven the parameter distribution of LLMs $\\mathcal{Q}$, the parameter distribution of encoder $\\mathcal{P}$ (SBERT), we assume ground truth $Y_L\\in \\mathcal{R}^{N \\times M}$, pseudo label $Y \\in \\mathcal{R}^{N \\times M}$,  and node features encoded by the encoder $X\\in \\mathcal{R}^{N \\times d}$. Here, $M$ denotes the number of classes and $d$ denotes the hidden dimension. Our objective is to maximize the accuracy of annotations, which is thus to minimize the discrepancies between $Y$ and $Y_{L}$. \n\n$$\n\\\\min_{(n_{1},...,n_{k})} {\\mathbb{E}_{\\theta\\sim Q}{f(Y)}} = \\\\ell(Y,Y\\_{L})=\\sum\\_{i=1}^{k} \\\\ell({y}^{n\\_{i}} , y\\_{L}^{n\\_{i}})\n$$\n\nwhere $(n_{1},...,n_{k})$ is the index of the $k$ selected nodes, and $f(Y)$ is defined as follows, where ${x}^{n_{i}}$ represents the feature of node ${n_{i}}$, while $x_{L}^{n_{i}}$ represents the unknown latent embedding of node ${n_{i}}$. \n$$\nf(Y)=\\\\ell(Y,Y_{L})=\\\\sum_{i=1}^{k} \\\\ell({y}^{n_{i}} , y_{L}^{n_{i}})= \\\\ln(1+\\\\sum_{i=1}^{k}\\\\|{x}^{n_{i}} - x_{L}^{n_{i}}\\\\|^{2})= \\\\ln(1+\\\\|X-X_{L}\\\\|^{2})\n$$\n\n\nSince we only have access to the node features $X$ generated by encoder $\\theta$, we need to make a connection between $\\mathcal{Q}$ and $\\mathcal{P}$. \n\nLemma 1. For any annotation $y$ generated by LLMs, $\\\\mathbb{E}\\_{\\\\theta\\\\sim \\\\mathcal{Q}}f(y)\\\\leq \\\\log \\mathbb{E}_{\\\\theta\\\\sim \\\\mathcal{P}} \\\\exp(f(y))+KL( \\\\mathcal{Q} \\\\| \\\\mathcal{P})$\n\nProof:\n$$\n\\\\begin{aligned}\n&\\\\\\\\\n&{\\\\mathbb{E}\\_{\\\\theta^{\\\\prime}\\\\sim \\\\mathcal{P}}{f(y)}}=\\\\int f(y) p(y) d y=\\\\int f(y) \\\\frac{p(y)}{q(y)} q(y) d y=\\\\mathbb{E}\\_{\\\\theta\\\\sim \\\\mathcal{Q}} f(y) \\\\frac{p(y)}{q(y)}\\\\\\\\\n&\\\\text{Since}\\\\quad KL(\\\\mathcal{Q} \\\\| \\\\mathcal{P})=\\\\mathbb{E}\\_{\\\\theta\\\\sim Q} \\\\log \\\\frac{q(y)}{p(y)}\\\\\\\\\n&\\\\log \\\\mathbb{E}\\_{\\\\theta^{\\\\prime}\\\\sim \\\\mathcal{P}} f(y)= \\\\log\\\\mathbb{E}\\_{\\\\theta\\\\sim \\\\mathcal{Q}} f(y) \\\\frac{p(y)}{q(y)} \\\\geq\\\\mathbb{E}\\_{\\\\theta\\\\sim \\\\mathcal{Q}}\\\\log f(y) \\\\frac{p(y)}{q(y)}=\\\\mathbb{E}\\_{\\\\theta\\\\sim \\\\mathcal{Q}}\\\\log f(y)-KL( \\\\mathcal{Q} \\\\| \\\\mathcal{P}) \\\\\\\\\n\\\\end{aligned}\n$$\n\nThen, we transform the objective into \n$$\n\\\\min\\_{(n_{1},...,n_{k})} \\\\mathbb{E}\\_{\\\\theta^{\\\\prime}\\\\sim \\\\mathcal{P}} \\\\exp(f(Y)) \n$$\n\nAssuming the label distribution $\\\\mathcal{H}$, we further have\n$$\n\\\\min\\_{(n_{1},...,n_{k})} \\\\mathbb{E}\\_{Y\\\\sim \\\\mathcal{H}} \\\\mathbb{E}\\_{\\\\theta^{\\\\prime}\\\\sim \\\\mathcal{P}} \\\\exp(f(Y)) = \\\\min\\_{(n_{1},...,n_{k})} \\\\mathbb{E}\\_{Y\\\\sim \\\\mathcal{H}} \\\\mathbb{E}\\_{\\\\theta\\\\sim \\\\mathcal{P}}\\\\|X-X_{L}\\\\|^{2} \\\\\\\\\n$$\n\nAssuming $X$ and $X_{L}$ follows gaussian distribution, where $X\\sim N(\\mu_{i},\\sigma_{i});X_L\\sim N(\\mu_{j},\\sigma_{j})$, then\n\n$$\n\\\\begin{aligned}\n\\\\mathrm{E}\\\\left(\\\\|X-X_{L}\\\\|^{2}\\\\right) & =\\\\mathrm{E}\\\\left(\\\\left\\\\|\\\\left(X-\\\\mu_{i}\\\\right)-\\\\left(X_{L}-\\\\mu_{j}\\\\right)+\\\\left(\\\\mu_{i}-\\\\mu_{j}\\\\right)\\\\right\\\\|^{2}\\\\right) \\\\\\\\\n& =\\\\mathrm{E}\\\\left(\\\\left\\\\|X-\\\\mu_{i}\\\\right\\\\|^{2}\\\\right)+\\\\mathrm{E}\\\\left(\\\\left\\\\|X_{L}-\\\\mu_{j}\\\\right\\\\|^{2}\\\\right)+\\\\left\\\\|\\\\mu_{i}-\\\\mu_{j}\\\\right\\\\|^{2} \\\\\\\\\n& =n \\\\sigma_{i}^{2}+n \\\\sigma_{j}^{2}+\\\\left\\\\|\\\\mu_{i}-\\\\mu_{j}\\\\right\\\\|^{2}\n\\\\end{aligned}\n$$\n\nSince $(\\mu_{j},\\sigma_{j})$ is unknown, $\\mu_{i}$ is fixed, thus we want to minimize $\\sigma_{i}$.  Given an arbitrary node, $\\sigma_{i}$ can be viewed as the distance to the clustering centers. The smaller $\\sigma_{i}$ is, the smaller the corresponding $\\mathrm{E}\\left(\\|X-X_{L}\\|^{2}\\right)$ also becomes, indicating that the minimum value of $ \\\\mathbb{E}\\_{ \\\\theta \\\\sim \\\\mathcal{Q}} f(y)$ is attained when $\\\\sigma_{i}$ is at its smallest. This demonstrates why nodes closer to the clustering centers are \"preferred\" by LLMs and thus achieve better annotation quality."
                    },
                    "title": {
                        "value": "Response to Reviewer JunD (3/7)"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2601/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700288214909,
                "cdate": 1700288214909,
                "tmdate": 1700288492262,
                "mdate": 1700288492262,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "450muOdgYN",
                "forum": "hESD2NJFg8",
                "replyto": "qpFCDvj1cs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2601/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2601/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Q1: Post-filtering (PS): How useful are the confidence scores generated by the LLMs (Appendix F.1, table 6)? Showing the mean and variance of the confidence score for each dataset may help in understanding its impact on performance.\n\n**Response** Thanks for your great comments. We show the mean and variance of confidence scores for each dataset in the following table and also put a figure for the Cora dataset in the revision (Appendix H). **The confidence generated by LLMs is well calibrated and can be used to reflect the reliability of annotations.** \n\nThe following table demonstrates the Mean, Variance, Mode, Maximum, and Minimum of the confidence for each dataset.\n|          \t|  Cora \t| CiteSeer \t| PubMed \t| WikiCS \t| Arxiv \t| Products \t|\n|--------|-----|--------|------|------|-----|--------|\n|   Mean   \t| 0.764 \t|   0.773  \t|  0.866 \t|   0.8  \t|  0.89 \t|   0.833  \t|\n| Variance \t| 0.019 \t|   0.014  \t|  0.006 \t|  0.012 \t| 0.011 \t|   0.006  \t|\n|   Mode   \t|  0.9  \t|    0.8   \t|   0.9  \t|   0.8  \t|  0.95 \t|    0.9   \t|\n|  Maximum \t| 0.967 \t|   0.958  \t|  0.983 \t|  0.942 \t| 0.992 \t|   0.942  \t|\n|  Minimum \t|  0.2  \t|   0.233  \t|  0.483 \t|  0.125 \t|  0.3  \t|   0.317  \t|\n\nFor the confidence calibration plot, we find that the hybrid prompt we use can generate accurate and diverse confidence scores, which demonstrates effectiveness."
                    },
                    "title": {
                        "value": "Response to Reviewer JunD (4/7)"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2601/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700288286011,
                "cdate": 1700288286011,
                "tmdate": 1700288502377,
                "mdate": 1700288502377,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "l8ZQJ1GZr0",
                "forum": "hESD2NJFg8",
                "replyto": "qpFCDvj1cs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2601/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2601/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Q2: Providing the number of nodes selected in each step in the experiment will also help in understanding the effects of the steps. For instance, could you please provide the number of selected nodes during active selection and DA-active selection? Also, what is the pruning ratio when applying PS? All this information can help in understanding the efficacy of the steps.\n\n**Response** Thanks for your great comments. For the number of nodes selected during active selection and DA-active selection, we adopt the same number of samples for each dataset as follows: the number of selected nodes is equal to the number of classes multiplied by $20$. For example, for Arxiv with $40$ classes, we select a total of $800$ nodes. For the pruning ratio, we drop 20\\% of the selected nodes for every dataset except Pubmed. For Pubmed, we drop 10\\% of the selected nodes because LLMs-as-predictors can achieve high accuracy on this dataset."
                    },
                    "title": {
                        "value": "Response to Reviewer JunD (5/7)"
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2601/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700288333664,
                "cdate": 1700288333664,
                "tmdate": 1700288510666,
                "mdate": 1700288510666,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CWn2mTHD8H",
                "forum": "hESD2NJFg8",
                "replyto": "qpFCDvj1cs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2601/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2601/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Q3: PS-DA-methods have not performed well in most cases compared to DA-methods (Table 2). Any insights on this can help in understanding these steps better.\n\n**Response** Thanks for your great comments. For the effectiveness of applying PS and DA together, we agree that directly combining them without tuning can't outperform DA in most cases. If we tune the hyper-parameters, we can get good performance. However, PS-DA introduces numerous hyper-parameters which makes tuning complicated. We further show using LLMs' confidence as the weight for loss function is a better way to be combined with DA methods.  \n* The reason that directly applying PS together with DA could lead to a performance decrease is that we don't tune the parameters. If we do not tune the hyperparameters, in some cases where DA has already selected a good candidate set, further adopting PS to remove some nodes will degrade the performance. \n* If we can tune the parameters, both DA and PS can be viewed as a special case of the more general PS-DA. However, in practice, PS-DA introduces many hyper-parameters, which makes it infeasible to tune the parameters without a validation set.  \n* Instead of combining PS and DA in a \"hard\" way by filtering nodes with LLMs' confidence, we find that a \"soft\" way which uses the confidence as the weight for loss function works better. In the following table, we show that compared to PS-DA, combining DA with weighted loss can effectively enhance the performance of DA. Compared to PS-DA, our new approach shares the same philosophy. PS-DA can be viewed as a hard selection process while weighted cross-entropy loss can be viewed as a soft selection. We find that such kind of soft selection can effectively solve the problem of hyper-parameter tuning. The full table can be viewed in revision. \"-W\" means the weighted loss. We find that using a weighted loss combination has two advantages: (1) no need to tune the hyper-parameters like PS-DA; and (2) better performance. \n\n\n|           \t|     Cora     \t|   CiteSeer   \t|\n|---------|------------|------------|\n|    AGE    \t| 69.15 \u00b1 0.38 \t| 54.25 \u00b1 0.31 \t|\n|   DA-AGE  \t| 74.38 \u00b1 0.24 \t| 59.92 \u00b1 0.42 \t|\n|  DA-AGE-W \t| 74.96 \u00b1 0.22 \t| 58.41 \u00b1 0.45 \t|\n| PS-DA-AGE \t| 71.53 \u00b1 0.19 \t| 56.38 \u00b1 0.14 \t|\n|    RIM    \t| 69.86 \u00b1 0.38 \t| 63.44 \u00b1 0.42 \t|\n|   DA-RIM  \t| 73.99 \u00b1 0.44 \t| 60.33 \u00b1 0.40 \t|\n|  DA-RIM-W \t| 74.73 \u00b1 0.41 \t| 60.80 \u00b1 0.57 \t|\n| PS-DA-RIM \t| 72.34 \u00b1 0.19 \t| 60.33 \u00b1 0.40 \t|"
                    },
                    "title": {
                        "value": "Response to Reviewer JunD (6/7)"
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2601/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700288399813,
                "cdate": 1700288399813,
                "tmdate": 1700288518206,
                "mdate": 1700288518206,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qGbj0kry07",
                "forum": "hESD2NJFg8",
                "replyto": "qpFCDvj1cs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2601/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2601/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Q4: Minor Questions: 1. Detail explanation on $f_{act}(vi)$ is missing; 2. It is mentioned in page 5 that the detailed descriptions and full prompt examples are shown in Appendix D. However, it is missing any detailed descriptions.\n\n**Response** Thanks for your great comments. $f_{act}(vi)$ can be any traditional graph active learning score functions, and we add a detailed explanation in the revision. Furthermore, we add more examples of the prompt and detailed descriptions in the appendix. \n\nQ5: Typos: Page 7: (4) Combing - supposed to be combining?; Page 3: GNN modelson- models on (space missing)\n\n**Response** Thanks for your great comments. We've fixed these typos in the revision."
                    },
                    "title": {
                        "value": "Response to Reviewer JunD (7/7)"
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2601/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700288424414,
                "cdate": 1700288424414,
                "tmdate": 1700288525884,
                "mdate": 1700288525884,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RmFn7JTxE9",
                "forum": "hESD2NJFg8",
                "replyto": "qGbj0kry07",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2601/Reviewer_JunD"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2601/Reviewer_JunD"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the rebuttal!"
                    },
                    "comment": {
                        "value": "Thank you for your effort on the rebuttal.\nBased on the quality of the paper and the clarifications in the rebuttal, I am raising my score."
                    }
                },
                "number": 28,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2601/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700325634335,
                "cdate": 1700325634335,
                "tmdate": 1700325634335,
                "mdate": 1700325634335,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "6f2Z4aNkfX",
            "forum": "hESD2NJFg8",
            "replyto": "hESD2NJFg8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2601/Reviewer_ugc7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2601/Reviewer_ugc7"
            ],
            "content": {
                "summary": {
                    "value": "This paper aims to combine Large Language Models(LLMs) and Graph Neural Networks(GNNs), leveraging their strengths. GNNs achieve promising performance when dealing with graph-structured data, while it needs abundant high-quality labels to ensure the performance. On the other hand, LLMs shows impressive zero-shot proficiency on text-attributed graphs, but it suffers from high inference costs and processing structural data. This paper suggests LLM-GNN, using LLM for annotation, providing training signals on GNN for further prediction. Moreover, the authors propose node selection strategy and confidence-aware annotation for efficient learning with high quality annotations."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The proposed method is well-motivated.\n- Each component supports motivation reasonably.\n- Well written paper, it is easy to follow."
                },
                "weaknesses": {
                    "value": "- The performance gain is incremental, especially for Difficulty-aware active node selection (DA).\n- Explanations about experiments are not enough, and some parts are unclear. Please refer to questions for details."
                },
                "questions": {
                    "value": "- In Figure 2 or Figure 12, the trend of decreasing accuracy as the distance between nodes and cluster centers increases seems somewhat weak in average accuracy. Even in Table 2, when DA is added to traditional graph active selection and when DA is added to the use of PS, there are many cases where performance actually decreases. I acknowledge that tuning was not performed, but there are still too many cases where performance declines. While the authors said that grid search would improve the performance in a specific case, it seems necessary to perform more tuning across a broader range of cases to clearly demonstrate the effectiveness of DA.\n- In Table 6, is it realistic to use labels in 1-shot example when we are considering the \"Label-free\" setting? Furthermore, how are the confidence scores determined in that example? More detailed explanation is required.\n- In Figure 4, why do some methods show an increase in performance after a budget of 70, followed by a sharp decline? Could this be attributed to class-imbalance issues during the selection process?\n- While there is an example in Figure 5, it would be beneficial to demonstrate on different datasets that LLM-GNN achieves competitive performance with GNNs trained on ground truth labeled data, while significantly reducing costs."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2601/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2601/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2601/Reviewer_ugc7"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2601/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698716455455,
            "cdate": 1698716455455,
            "tmdate": 1700631451669,
            "mdate": 1700631451669,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7IZp8qDIA6",
                "forum": "hESD2NJFg8",
                "replyto": "6f2Z4aNkfX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2601/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2601/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Q1-1: In Figure 2 or Figure 12, the trend of decreasing accuracy as the distance between nodes and cluster centers increases seems somewhat weak in average accuracy. \n\n\nR1-1: Thanks for your great question which inspires us to explain the details of our paper. There are line plots and bar plots in the figure. The changing trend is reflected by the line plot, which **shows a clear declining trend** while the bar plot only provides auxiliary information and may not affect the conclusion.\n\nFor Figure 2 and Figure 12, **the bar** shows the average values within each region, while the blue line represents the overall mean accuracy across different regions. Although the accuracy inside each bar doesn't show a clear trend, the line plot shows a clear decreasing trend across different regions. The nodes to be annotated are selected based on the accuracy across different groups (the blue line) rather than the accuracy inside each group (the bar). As a result, **the declining trend is clear** and difficulty-aware selection can effectively select those nodes with high-quality annotations."
                    },
                    "title": {
                        "value": "Response to Reviewer ugc7 (1/7)"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2601/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700285097362,
                "cdate": 1700285097362,
                "tmdate": 1700286772832,
                "mdate": 1700286772832,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FE0zkoUhfT",
                "forum": "hESD2NJFg8",
                "replyto": "6f2Z4aNkfX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2601/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2601/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ugc7 (2/7)"
                    },
                    "comment": {
                        "value": "Q1-2: Even in Table 2, when DA is added to traditional graph active selection and when DA is added to the use of PS, there are many cases where performance actually decreases. I acknowledge that tuning was not performed, but there are still too many cases where performance declines. While the authors said that grid search would improve the performance in a specific case, it seems necessary to perform more tuning across a broader range of cases to clearly demonstrate the effectiveness of DA.\n\nR1-2: Thanks for your great question. We first summarize our responses and then provide more details for the responses.\n(1) Applying the original DA may lead to clear performance degrade, which is due to the class imbalance problem by a small clustering center number $K$ on datasets like Pubmed. **This problem can be solved** by increasing $K$ to balance the diversity and annotation quality. (2) The original way to apply PS and DA together can not outperform DA. Although hyper-parameter tuning can definitely help, we agree that hyper-parameter tuning could not be flexible in practice. Therefore to further explore the potential of combining DA and PS, **we introduce a simple way to utilize the confidence as weight for loss functions and we find that the new variants can outperform DA in most cases without any hyper-parameter tuning.**\n\n\nR1-2 (1) For the effectiveness of DA, we admit that sometimes DA will degrade the performance, especially on the Pubmed dataset. The reason for this phenomenon is that when performing K-means clustering with $C-Density$, the number of clustering centers $K$ is fixed as the number of classes for labels. This approach can achieve good labeling quality in most cases. On datasets like Pubmed where the number of classes is small (only 3), the points selected by $C-Density$ tend to cause class imbalance (the annotation quality is still very good). To solve this problem, we find that by setting a larger value for the number of clustering center $K$, the new $C-Density^{\\prime}$ obtained can effectively solve this problem. We show the results in the following table. $k$ means the number of clustering centers to caluclate $C-Density^{\\prime}$, $C$ means the number of classes, budgets means the number of selected annotated nodes. We use AGE as a demonstration to show the idea. From the results, we can see that setting a larger $K$ can improve the performance of DA on Pubmed.\n\n|                  \t| Pubmed |\n|----------------|------|\n|        AGE       \t|  74.55 \t|\n|    DA-AGE(k=C)   \t|  55.36 \t|\n|   DA-AGE(k=2C)   \t|  57.38 \t|\n| DA-AGE(k=budget)|  75.71 \t|"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2601/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700285521889,
                "cdate": 1700285521889,
                "tmdate": 1700286792015,
                "mdate": 1700286792015,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vIu2lA3QGA",
                "forum": "hESD2NJFg8",
                "replyto": "6f2Z4aNkfX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2601/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2601/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "R1-2 (2) For the effectiveness of applying PS and DA together, we agree that directly combining them without tuning can't outperform DA in most cases. While hyper-parameter tuning can help, we also explore using LLMs' confidence as the weight for weighted cross-entropy loss that can effectively combine DA with the information generated by LLMs. we will first explain why PS-DA doesn't work. \n* The reason that directly applying PS together with DA could lead to a performance decrease is that we don't tune the parameters. If we do not tune the hyperparameters, in some cases where DA has already selected a good candidate set, further adopting PS to remove some nodes will degrade the performance. \n* In our experiments, the purpose of demonstrating PS-DA is to compare it with the use of PS and DA individually and to explain that without hyperparameter tuning on a validation set, using just one of PS or DA is a better choice. Among them, PS often achieves better quality because it can utilize the confidence information from LLMs, but it may lose some sample points due to filtering. Since DA does not need to use the confidence information from LLMs, it can employ a regular zero-shot prompt instead of a consistency prompt, making it relatively lower in cost. Choosing between DA and PS can be seen as a trade-off in terms of cost and the number of samples. DA and PS can surpass the original baseline in most cases. \n* In the following table, we show that compared to PS-DA, combining DA with weighted loss can effectively enhance the performance of DA. Compared to PS-DA, our new approach shares the same philosophy. PS-DA can be viewed as a hard selection process while weighted cross-entropy loss can be viewed as a soft selection. We find that such kind of soft selection can effectively solve the problem of hyper-parameter tuning. The full table can be viewed in revision. \"-W\" means the weighted loss. We find that using a weighted loss combination has two advantages: (1) no need to tune the hyper-parameters like PS-DA; and (2) better performance. \n\n\n|           \t|     Cora     \t|   CiteSeer   \t|\n| --------- | ------------ |------------|\n|    AGE    \t| 69.15 \u00b1 0.38 \t| 54.25 \u00b1 0.31 \t|\n|   DA-AGE  \t| 74.38 \u00b1 0.24 \t| 59.92 \u00b1 0.42 \t|\n|  DA-AGE-W \t| 74.96 \u00b1 0.22 \t| 58.41 \u00b1 0.45 \t|\n| PS-DA-AGE \t| 71.53 \u00b1 0.19 \t| 56.38 \u00b1 0.14 \t|\n|    RIM    \t| 69.86 \u00b1 0.38 \t| 63.44 \u00b1 0.42 \t|\n|   DA-RIM  \t| 73.99 \u00b1 0.44 \t| 60.33 \u00b1 0.40 \t|\n|  DA-RIM-W \t| 74.73 \u00b1 0.41 \t| 60.80 \u00b1 0.57 \t|\n| PS-DA-RIM \t| 72.34 \u00b1 0.19 \t| 60.33 \u00b1 0.40 \t|\n\nIn the following table, we demonstrate that DA can improve the performance of diversity-aware selection methods most of the time. \n\n|              \t|     Cora     \t|   CiteSeer   \t|    Pubmed    \t|    WikiCS    \t|\n|------------|------------|------------|------------|------------|\n|      AGE\t     \t| 69.15 \u00b1 0.38 \t| 54.25 \u00b1 0.31 \t| 74.55 \u00b1 0.54 \t| 55.51 \u00b1 0.12 \t|\n|    DA-AGE    \t| 74.38 \u00b1 0.24 \t| 59.92 \u00b1 0.42 \t| 74.20 \u00b1 0.51 \t| 59.39 \u00b1 0.21 \t|\n|      RIM     \t| 69.86 \u00b1 0.38 \t| 63.44 \u00b1 0.42 \t| 76.22 \u00b1 0.16 \t|  66.72\u00b1 0.16 \t|\n|    DA-RIM    \t| 73.99 \u00b1 0.44 \t| 60.33 \u00b1 0.40 \t| 79.17 \u00b1 0.11 \t|  67.82\u00b1 0.32 \t|\n|   GraphPart  \t| 68.57 \u00b1 2.18 \t| 66.59 \u00b1 1.34 \t| 77.50 \u00b1 1.23 \t| 67.28 \u00b1 0.87 \t|\n| DA-GraphPart \t| 69.35 \u00b1 1.92 \t| 69.37 \u00b1 1.27 \t| 79.49 \u00b1 0.85 \t| 68.72 \u00b1 1.01 \t|\n\nWe can see for 10/12 cases, DA leads to a significant gain. In other cases, it can also achieve comparable performance."
                    },
                    "title": {
                        "value": "Response to Reviewer ugc7 (3/7)"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2601/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700286180784,
                "cdate": 1700286180784,
                "tmdate": 1700286802420,
                "mdate": 1700286802420,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6iQPXTxJau",
                "forum": "hESD2NJFg8",
                "replyto": "6f2Z4aNkfX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2601/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2601/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Q2: In Table 6, is it realistic to use labels in 1-shot example when we are considering the \"Label-free\" setting? Furthermore, how are the confidence scores determined in that example? More detailed explanation is required.\n\n**Response** Thanks for your great question. We agree that the original version may lead to confusion since we do not clarify that we only use \"1-shot\" prompts to demonstrate there's no clear gap between \"zero-shot\" and \"1-shot\"  and **we use the \"zero-shot\" prompt in all experiments** of Section 4.\n\nIn the revision, we have updated the prompt example of zero-shot case in the appendix E. We agree that \"1-shot\" may be not appropriate for the \"label-free\" setting. However, we only use 1-shot prompts in Table 1 to demonstrate that there's no clear gap between the performance of zero-shot and 1-shot prompts. For the experiment part, we only focus on zero-shot prompts. \nFor the generation of multiple labels and their corresponding confidence scores in the 1-shot case, we leverage LLMs to automatically generate both the topK predictions and confidence scores according to the philosophy of [1]. We've revised the corresponding part of the paper. \n\n[1] Zhang Z, Zhang A, Li M, et al. Automatic chain of thought prompting in large language models[J]. arXiv preprint arXiv:2210.03493, 2022."
                    },
                    "title": {
                        "value": "Response to Reviewer ugc7 (4/7)"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2601/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700286221726,
                "cdate": 1700286221726,
                "tmdate": 1700286812731,
                "mdate": 1700286812731,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "h1w6Nc0bwc",
                "forum": "hESD2NJFg8",
                "replyto": "6f2Z4aNkfX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2601/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2601/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Q3: In Figure 4, why do some methods show an increase in performance after a budget of 70, followed by a sharp decline? Could this be attributed to class imbalance issues during the selection process?\n\n**Response** Thanks for your great comment which points out an important phenomenon in our experiments. We first summarize our responses and then provide more details for the responses. (1) For ground truth labels, increasing the budget can usually improve the performance. However, for noisy labels, it's reasonable that increasing the budget will lead to a performance drop since labels are noisy [1]. (2) The original scales of the x-axis and y-axis are inappropriate, making the declining phenomenon appear very pronounced, when in fact **the magnitude of the decline is relatively small**. (3) The main reason for the performance degradation is attributed to the **decline in annotation quality**, rather than the class imbalance. \n\nQ3 (1) Since we use LLMs as the annotators, two factors affect the final performance of LLMGNN: **number of annotated samples**, and **annotation quality**. For clean labels, usually, the performance will gradually increase with more budgets. However, for noisy labels, it's reasonable that performance drops with more labels since the overall annotation quality degrades [1]. \n\nQ3 (2) In the original figure, we use the x ticks [70, 280, 1120, 2240] with non-equal distance between every tick. This unreasonable design results in a visually pronounced appearance of the decline. We have redrawn the chart in the revision, and the corresponding results are presented in the table below, showing that the decline's magnitude is not particularly significant.\n\n\n|                            \t|   35  \t|   70  \t|  105  \t|  140  \t|  175  \t|  280  \t|  560  \t|  1120 \t|\n|--------------------------|-----|-----|-----|-----|-----|-----|-----|-----|\n|         Random (CE)        \t| 57.35 \t| 66.45 \t| 68.42 \t| 70.17 \t| 69.64 \t| 70.68 \t| 72.07 \t| 72.73 \t|\n|         Random(WE)         \t|  59.3 \t| 68.88 \t| 68.77 \t| 71.85 \t| 71.77 \t| 71.75 \t| 72.81 \t| 72.92 \t|\n|          PS-Random         \t|  55.6 \t|  66.7 \t| 69.86 \t| 71.49 \t| 71.07 \t| 72.87 \t| 73.81 \t| 72.19 \t|\n|          FeatProp          \t| 65.36 \t| 69.18 \t| 74.07 \t| 72.82 \t| 72.96 \t| 70.63 \t| 72.53 \t| 73.74 \t|\n|         PS-FeatProp        \t| 68.25 \t|  69.8 \t| 76.03 \t| 75.54 \t| 76.83 \t| 74.11 \t| 72.26 \t| 72.52 \t|\n\n[1] Song H, Kim M, Park D, et al. Learning from noisy labels with deep neural networks: A survey[J]. IEEE Transactions on Neural Networks and Learning Systems, 2022."
                    },
                    "title": {
                        "value": "Response to Reviewer ugc7 (5/7)"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2601/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700286352894,
                "cdate": 1700286352894,
                "tmdate": 1700286821950,
                "mdate": 1700286821950,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "enRMa9uYPH",
                "forum": "hESD2NJFg8",
                "replyto": "6f2Z4aNkfX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2601/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2601/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Q3: In Figure 4, why do some methods show an increase in performance after a budget of 70, followed by a sharp decline? Could this be attributed to class imbalance issues during the selection process?\n\nQ3 (3) The performance degradation is mainly due to the annotation quality drop with more budgets. For random selection, the drop is not obvious because of random sampling. For active selection like PS-Featprop, the annotation quality drops with more budgets since we fixed the dropping ratio to 20\\%. With more budgets, there will be correspondingly more wrong annotations. \n\n\n\nIn the following table, we demonstrate the annotation quality and entropy given by PS-Featprop. Entropy can be used to measure the imbalance of selected labels. \n\n|                    \t|  35  \t|   70  \t|  105  \t|  140  \t|  175  \t|  280  \t|  560  \t|  1120 \t|\n|------------------|----|-----|-----|-----|-----|-----|-----|-----|\n|       Entropy      \t| 2.42 \t|  2.42 \t|  2.48 \t|  2.55 \t|  2.51 \t|  2.54 \t|  2.53 \t|  2.56 \t|\n| Annotation quality \t|  75  \t| 76.79 \t| 80.95 \t| 75.27 \t| 79.29 \t| 78.12 \t| 73.21 \t| 71.99 \t|\n|         PS-FeatProp        \t| 68.25 \t|  69.8 \t| 76.03 \t| 75.54 \t| 76.83 \t| 74.11 \t| 72.26 \t| 72.52 \t|\n\nWe can see that the performance drop is more correlated with the annotation quality drop. To mitigate this problem, one possible solution is to adaptively increase the dropping rate of PS-Featprop, and we may increase the performance when there's more noisy labels. This method can only mitigate the issue rather than resolve it completely. We acknowledge that exploring how to further enhance the performance of LLMGNN under high labeling rates is a very meaningful direction for future work.\n\n|                            \t|   35  \t|   70  \t|  105  \t|  140  \t|  175  \t|  280  \t|  560  \t|  1120 \t|\n|--------------------------|-----|-----|-----|-----|-----|-----|-----|-----|\n|         PS-FeatProp        \t| 68.25 \t|  69.8 \t| 76.03 \t| 75.54 \t| 76.83 \t| 74.11 \t| 72.26 \t| 72.52 \t|\n| PS-FeatProp(adaptive drop) \t| 68.25 \t|  69.8 \t| 76.03 \t| 75.54 \t| 76.83 \t| 75.08 \t| 73.54 \t| 74.45 \t|\n\nAnother potential solution is to further improve the zero-shot performance of LLMs, like using more complex prompts or agents-related techniques [2]. Using Pubmed as an example, since LLMs can achieve near 90% accuracy on this dataset, increasing the budget can effectively improve the performance of LLMGNN. This shows that if LLMs can get good annotation quality, directly using randomly selected annotations can already get good performance. \n\n|   Pubmed  \t|   15  \t|   30  \t|   45  \t|   60  \t|   75  \t|  120  \t|  240  \t|  480  \t|\n|---------|-----|-----|-----|-----|-----|-----|-----|-----|\n|   Random  \t| 62.49 \t| 64.56 \t| 72.44 \t| 73.16 \t| 75.92 \t|  77.8 \t| 82.38 \t|  82.5 \t|\n| PS-Random \t| 62.84 \t| 63.28 \t| 72.24 \t| 73.13 \t| 75.49 \t| 79.06 \t|  82.8 \t| 81.81 \t|\n|  FeatProp \t| 54.67 \t| 72.02 \t|  75.2 \t| 76.47 \t| 74.39 \t| 79.76 \t| 81.49 \t| 82.86 \t|\n\n||  Cora \t| Pubmed \t|\n|------|-----|------|\n|LLMs-as-Predictors| 68.33 \t|  87.33 \t|\n\n[2] Xi Z, Chen W, Guo X, et al. The rise and potential of large language model based agents: A survey[J]. arXiv preprint arXiv:2309.07864, 2023."
                    },
                    "title": {
                        "value": "Response to Reviewer ugc7 (6/7)"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2601/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700286573978,
                "cdate": 1700286573978,
                "tmdate": 1700286831352,
                "mdate": 1700286831352,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "MKxWQDaf9N",
                "forum": "hESD2NJFg8",
                "replyto": "6f2Z4aNkfX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2601/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2601/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Q4: While there is an example in Figure 5, it would be beneficial to demonstrate on different datasets that LLM-GNN achieves competitive performance with GNNs trained on ground truth labeled data, while significantly reducing costs.\n\n**Response** Thanks for your great comments which help us to refine the details of our paper.  Compared to traditional GNN and LLMs-as-Predictors, **our methods can achieve promising performance with much lower costs.**\n\nWe demonstrate the economic comparison among LLM-GNN, LLMs-as-Predictors, and traditional pipelines to show the effectiveness. The first table demonstrates the performance of each pipeline. For LLMGNN and GNN, we deliberately let them produce a similar performance to conduct a fair cost comparison. For Pubmed*, we demonstrate the performance of LLMGNN with a larger budget since the performance of LLMs-as-Predictors is superior on this dataset, and increasing a little budget can improve the performance a lot. \n|     (Accuracy)     \t|  Cora \t| CiteSeer \t| Pubmed \t| Pubmed* \t| WikiCS \t| Arxiv \t| Products \t|\n|------------------|-----|--------|------|-------|------|-----|--------|\n|       LLMGNN       \t| 75.54 \t|   69.06  \t|  74.98 \t|  81.95  \t|  66.09 \t| 66.14 \t|   74.9   \t|\n| LLMs-as-Predictors \t| 68.33 \t|   66.33  \t|  87.33 \t|  87.33  \t|   71   \t| 73.33 \t|   75.33  \t|\n|         GNN        \t| 75.54 \t|   69.06  \t|  74.98 \t|  81.95  \t|  66.09 \t| 66.14 \t|   74.9   \t|\n\nThe second table compares the number of training samples needed to achieve the performance in the first table. Here, PL stands for pseudo labels generated by LLMs. GT stands for ground truth labels.\n| (Number of   annotations) \t| Cora \t| CiteSeer \t| Pubmed \t| Pubmed* \t| WikiCS \t|  Arxiv \t| Products \t|\n|-------------------------|----|--------|------|-------|------|------|--------|\n|         LLMGNN (PL)        \t|  140 \t|    120   \t|   60   \t|   300   \t|   200  \t|   800  \t|    940   \t|\n|   LLMs-as-Predictors (PL)  \t| 2708 \t|   3186   \t|  19717 \t|  19717  \t|  11701 \t| 169343 \t|  2449029 \t|\n|          GNN (GT)         \t|  50  \t|    50    \t|   42   \t|   210   \t|   40   \t|   560  \t|    400   \t|\n\n\nFrom these two tables, we can see that **LLMGNN significantly reduces the costs compared to LLMs-as-Predictors**. A recent study [1], shows that the annotations generated by ChatGPT are more than **20 times** cheaper than human annotators. If we use this ratio, we can see that although LLMGNN requires more pseudo labels, the overall costs are still much cheaper than traditional GNN-based pipelines. Using the OGBN-Products dataset as an example, if we estimate the cost, using LLMs-as-Predictors would incur an expense of 1572 dollars, while manual annotation would require at least 20 dollars (assuming each sample has 3 workers on a crowdsourcing platform), and it would also necessitate a considerable amount of waiting time. In contrast, using LLMGNN can complete annotation and prediction in a matter of minutes at a cost of less than one dollar.\n\n[1] Gilardi F, Alizadeh M, Kubli M. Chatgpt outperforms crowd-workers for text-annotation tasks[J]. arXiv preprint arXiv:2303.15056, 2023."
                    },
                    "title": {
                        "value": "Response to Reviewer ugc7 (7/7)"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2601/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700286731942,
                "cdate": 1700286731942,
                "tmdate": 1700515773269,
                "mdate": 1700515773269,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7xmgk8YPsi",
                "forum": "hESD2NJFg8",
                "replyto": "MKxWQDaf9N",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2601/Reviewer_ugc7"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2601/Reviewer_ugc7"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the rebuttal"
                    },
                    "comment": {
                        "value": "Thank you for addressing all of my concerns with your response. I raise my score accordingly."
                    }
                },
                "number": 35,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2601/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700631418893,
                "cdate": 1700631418893,
                "tmdate": 1700631418893,
                "mdate": 1700631418893,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]