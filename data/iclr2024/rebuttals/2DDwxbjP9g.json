[
    {
        "title": "In Defence Of Wasserstein"
    },
    {
        "review": {
            "id": "TPe6U4Facq",
            "forum": "2DDwxbjP9g",
            "replyto": "2DDwxbjP9g",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3136/Reviewer_hGFX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3136/Reviewer_hGFX"
            ],
            "content": {
                "summary": {
                    "value": "This papers offers a new look into the \"Do Wasserstein GANs approximate the Wasserstein distance\" debate. The authors argue, despite previous litterature, that Wasserstein GANs actually __do__ minimize the Wasserstein distance. To justify this conceptual shift, they look at the form of the distance w.r.t. the choice of discriminator architecture. In this sense, the authors propose the idea of \"patch Wasserstein distance\", which is a Wasserstein distance between groups of patches. In their paper, the authors argue that, when the discriminator takes the form of a CNN, the WGAN minimizes this novel distance."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The authors tackle an important and relevant point in the intersection of Generative Modeling and Optimal Transport, that is, whether or not Wasserstein GANs actually minimize the Wasserstein distance. This problem is of key importance for the theoretical understanding of popular, optimal transport-based generative neural networks. The authors' writing is engaging, and they present their ideas and arguments in a fluid way."
                },
                "weaknesses": {
                    "value": "__Major__\n\n- __W1.__ The \"Discrete Wasserstein GAN\" corresponds to computing the (primal) Wasserstein distance between mini-batches sampled from the noise distribution, that is, $x\\_{i} = f(z\\_{i})$ and mini-batches sampled from the data set, that is, $y\\_{i}$. However, such strategy was already described in two works: [Genevay, Peyr\u00e9 and Cuturi, 2018] and [Salimans et al., 2018]. Authors should include an appropriate citation to these works, and include a comparison with them in the discussion, highlighting, for instance, the effects of using exact OT (authors submission) in place of Entropic OT (previous work).\n\n- __W2.__ The authors discussion is confusing w.r.t. the estimation of Wasserstein distances. For instance, Eqns. 1 and 2 refer to continuous OT, where $P$ and $Q$ are two distributions/measures. However, Eqn. 3 refers to a discrete OT, or, more formally, to an empirical estimate of the Wasserstein distance. __The authors could improve notation__. For instance, for (continuous) $P$ and $Q$, one has the true value of the Wasserstein distance $W_{p}(P, Q)$, which cannot be estimated directly since $P$ and $Q$ are unknown or too complicated. In contrast, when approximating $P$ and $Q$ empirically, one has indeed the discrete OT cost $W_{p}(\\hat{P}, \\hat{Q})$, based on sampled drawn from $P$ and $Q$. Note that this is an __empirical estimate__ (thus a random variable) and may be differ from the true $W_{p}(P, Q)$. With this remark in mind, authors should update their notation and discussion, making clear __what is minimized throughout GAN optimization__. Furthermore, the authors could consider the work of [Fatras et al., 2020], who explore in approximating OT through mini-batches. In my view, this play a major role in the learning of GANs (and any OT-base minibatch optimization algorithm for that matter).\n\n- __W3.__ The theoretical results of the paper are a bit imprecise. On this point, I refer authors to my questions.\n\n- __W4.__ Some notions introduced by the authors are never defined properly.\n\n__Minor.__ Note, the minor points did not affect my overall score.\n\n- Abstracts should be 1 paragraph long.\n\n- __Notation.__ While using $W1$ for the Wasserstein distance is readable, authors should consider using subscripts (e.g., $W_{1}$). Also, for the sake of uniformity, authors should use $W_{1}$ along the paper (e.g., in Eqn. authors use $W(P, Q)$). Whenever authors discretize continuous OT, especially from samples, the authors should consider using \"$\\hat{\\cdot}$\", to highlight that the said quantity is an estimate rather than the true value.\n\n__References__\n\n[Genevay, Peyr\u00e9 and Cuturi, 2018] Genevay, A., Peyr\u00e9, G., & Cuturi, M. (2018, March). Learning generative models with sinkhorn divergences. In International Conference on Artificial Intelligence and Statistics (pp. 1608-1617). PMLR.\n\n[Salimans et al., 2018] Salimans, T., Zhang, H., Radford, A., & Metaxas, D. (2018, February). Improving GANs Using Optimal Transport. In International Conference on Learning Representations.\n\n[Fatras et al., 2020] Fatras, K., Zine, Y., Flamary, R., Gribonval, R., & Courty, N. (2020, June). Learning with minibatch Wasserstein: asymptotic and gradient properties. In International Conference on Artificial Intelligence and Statistics (pp. 2131-2141). PMLR."
                },
                "questions": {
                    "value": "- __Q1.__ The argmin in Eqn. 4 is taken w.r.t. which variable? Furthermore, I am curious if there is any relationship of this eqn. and the Barycentric Projection used in [Courty et al., 2017] (e.g., Eqn. 13 of the said reference)\n\n__Related to Weakness 3.__\n\n- __Q2.__ In __Theorem 2__ of the paper, the authors use \"approximately minimizing\" in their statement. On the paragraph following the proof the authors mention,\n\n> The term \u201dapproximately minimizing\u201d in the theorem statement is the same as in the original proof of WGANs\n\nCould you clarify to which proof the authors refer? Note that there are multiple theorems in the WGAN paper. Furthermore, I would like that the authors clarify what __approximately minimizing__ means. Does this comes from the fact of using mini-batches? Does this comes from the Lipschitzness constraint?\n\n- __Q3.__ __Theorem 3__ is defined in loose terms, and is overall confusing. First, it is not clear how the distributions playing a role in this theorem are defined. Second, the authors seem to consider the \"patch Wasserstein distance\" between samples from underlying distributions. In this sense one would expect that the statement would hold w.r.t. a given probability according to the choice of samples (e.g., PAC theory), but the authors never offer any such analysis. An example on why this raises problems is with statements of the sort _\"Since the number of patches is large in the M generated images and in the training set, and they come from the same distribution, then the Wasserstein distance over patches will be close to zero\"_. Note that this statement is not informative since one does not know _how close to zero_ the said distance gets. A similar remark can be made about the second sentence, _\"On the other hand, when we take the linear combination of $M / N$ different images we will almost always create additional patches that are not one of the $K$ possible patches in the distribution\"_. Here, even if we agree that the linear combination operation will generate patches not in the possible set of patches (and that is not guaranteed, as it depends on the set of patches being combined!), the affirmation that this occurs _almost always_ is very loose. Again, it should be defined as a probability over the choice of samples."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3136/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3136/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3136/Reviewer_hGFX"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3136/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697638598451,
            "cdate": 1697638598451,
            "tmdate": 1699636260726,
            "mdate": 1699636260726,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "VM0Lwt4kov",
                "forum": "2DDwxbjP9g",
                "replyto": "TPe6U4Facq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3136/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3136/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your very detailed review of our work. We agree with some of the concerns that you raised but reading your first two weakness points lead us to believe you may have misunderstood the discrete generative settings we investigate throughout the paper. We would sincerely appreciate it if you read our detailed response and tell us if this somehow sheds another light on our work.\n\n## Additional comments:\n- __Weakness 1:__ The papers you referred to solve the OT problem between sampled mini batches as an approximation of the OT between the entire data and generated distributions. As these papers show, approximating W1 over distributions by W1 over samples is dangerous and can lead to wrong conclusions. It is precisely for this reason that we analyze a scenario where both generated and data distribution are discrete and therefore we can solve the OT between the entire sets without resorting to samples. Specifically the OTmeans algorithm solves the OT problem between the M generated images and the entire dataset in each iteration.\n\n- __Weakness 2:__ As in our last answer, we suspect there is a misunderstanding of what DiscreteWGAN means. We never use samples to approximate the distributions empirically as the distributions are already discrete ones. This allows us in figure 2 to plot exact W1 and not approximations to W1 that are based on samples as was done in previous works. \n\n- __Weakness 3-4:__ We agree that Theorem 3 and its proof are written too informally and we will rewrite them in our final revision. See answer to questions below.\n- __Question 1:__ The argmin should be w.r.t $x_j$ we will rewrite this in our final revision with a slack variable \u2018z\u2019. We did not mean to specifically address Barycentric Projections .We would appreciate it if you direct us to the full reference you mentioned as we did not find it.\n\n- __Question 2:__ As you know, the WGAN iterative training procedure is claimed to minimize  the data/generated W1 distance only if \n    1. The discriminator family of functions defined by the dicsriminator\u2019s architecture is big \u201cenough\u201d.\n    2. The discriminator is trained to optimality before each generator step.\n    3. The approximation of the expectation of Kantorovich potentials in the dual form of W1 with minibatches is accurate.\n    4. The Lipshitz constraint on the discriminator is held.\n\n    Even then the SGD optimization may end in a local minima.\n    All these conditions should be held whether we are dealing with W1 between images or patches. Conditioned 1-2 are detailed in proposition-1 in the original GAN paper [1] and conditions 2-3 are detailed throughout the WGAN paper [2]. Of course we will find a way to make this claim clearer in our final revision.\n\n- __Question 3:__ This seems again to be based on the same misunderstanding above. We never use samples to approximate W1 but rather can compute it exactly since it is discrete. We need M,N to be large to avoid quantization errors, not for statistical efficiency. Consider the following simple example. Suppose we have (N=100) binary variables of which 60 are 1 and 40 are 0. If we have (M=10) binary variables of which 6 are 1 and 4 are 0, we can get 0 Wasserstein distance. But if we have (M=11) binary variables, then we cannot get 0 Wasserstein distance due to quantization effects. When both M and N are large, these quantization effects can be neglected.\n\n## Citations:\n[1] Goodfellow, Ian, et al. \"Generative adversarial networks.\" Communications of the ACM 63.11 (2020): 139-144.\n\n[2] Arjovsky, Martin, Soumith Chintala, and L\u00e9on Bottou. \"Wasserstein generative adversarial networks.\" International conference on machine learning. PMLR, 2017"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3136/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700065283406,
                "cdate": 1700065283406,
                "tmdate": 1700065283406,
                "mdate": 1700065283406,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Xh6HCEqNT7",
                "forum": "2DDwxbjP9g",
                "replyto": "VM0Lwt4kov",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3136/Reviewer_hGFX"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3136/Reviewer_hGFX"
                ],
                "content": {
                    "title": {
                        "value": "Response to Authors"
                    },
                    "comment": {
                        "value": "Dear authors,\n\nFirst I would like to acknowledge and apologize for my error. Indeed, I understood the principle behind DiscreteGAN wrongly.\n\nSecond, I indeed forgot the full reference of [Courty et al., 2017]. Here it is,\n\n> N. Courty, R. Flamary, D. Tuia and A. Rakotomamonjy, \"Optimal Transport for Domain Adaptation,\" in IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 39, no. 9, pp. 1853-1865, 1 Sept. 2017, doi: 10.1109/TPAMI.2016.2615921.\n\nThe equation I mentioned in my initial commentary is eq. 13.\n\nFurthermore, I am still not sure on how you measure the Wasserstein distance between $P$ and $Q$ exactly. For instance, in the generative modeling literature, one usually makes the hypothesis that the training set $\\set{y\\_{i}}\\_{i=1}^{N}$ comes from an underlying continuous distribution $P$ from which $y_{i}$ is sampled. In this sense, the generated distribution $Q_{\\theta}$ is the parametric distribution (parametrized by $\\theta$) that better approximates the data/underlying distribution $P$.\n\nGiven this motivation, in my view while one may fix the samples from the generated distribution $Q_{\\theta}$ and optimize w.r.t. $\\theta$, one still has a sampling effect on $P$. At worst case, considering fixed $N$ points from $P$ is still a (rather big) sample from the underlying distribution.\n\nCan the authors provide further discussion and motivation over these two points?\n\n__Note.__ I just found an inconsistency in page 3. Before presenting the OTMeans algorithm, the authors define $P$ as the generated distribution, whereas in theorem 1 $P$ is the training distribution."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3136/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700236574904,
                "cdate": 1700236574904,
                "tmdate": 1700236574904,
                "mdate": 1700236574904,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SXVvseK8UU",
                "forum": "2DDwxbjP9g",
                "replyto": "TPe6U4Facq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3136/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3136/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the reference and additional comments\n- Indeed equation 4 in our paper is equivalent to the barycentric mapping of the OT solution found on each teration of the OTMeans algorithm. Note that we did not use any general property of barycentric mappings and simply used the fact that for L2 norm the solution is a weighted average of \u201ctarget\u201d points. As noted in the text that follows eq. 13 in the paper you referenced, when the cost function is the squared l2 metric the barycentric mapping is the weighted average of all targets. We used the (non-squared) L2 loss for which the barycentric mapping is still a weighted average of the targets but the finding the weights requires using iterative reweighed least squares as mentioned in our paper.\n- When fitting parametric statistical models to data, it is common to consider three distributions. $P_{\\theta}$ is the parameterized distribution that we are fitting, $Q_{data}$ is the empirical distribution, and $Q$ is the distribution from which the data was sampled. You are certainly correct that ideally we want $P_{\\theta}$ to be as close as possible to $Q$ but of course we do not have access to it during training and so we resort to making $P_{\\theta}$ as close as possible to $Q_{data}$. See for example page 109 of the textbook \u201cProbabilistic Machine Learning \u201c by Kevin Murphy (draft available at: https://github.com/probml/pml-book/releases/latest/download/book1.pdf) which shows that Maximum Likelihood is equivalent to minimizing the $KL$ distribution between $P_{\\theta}$ and $Q_{data}$. Just as a method for maximizing the likelihood is evaluated by how well it minimizes  $KL(P_{\\theta}, Q_{data})$, a method for minimizing the Wasserstein distance should be evaluated by how well it minimizes $W1(P_{\\theta}, Q_{data})$. In our setting, both $P_{\\theta}$ and $Q_{data}$ are discrete so that we can evaluate $W1(P_{\\theta}, Q_{data})$ exactly. We will fix all $P/Q$ data/generated notation inconsistencies in our final revision of the paper."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3136/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700404855956,
                "cdate": 1700404855956,
                "tmdate": 1700405089498,
                "mdate": 1700405089498,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "b4m2lalXYO",
                "forum": "2DDwxbjP9g",
                "replyto": "SXVvseK8UU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3136/Reviewer_hGFX"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3136/Reviewer_hGFX"
                ],
                "content": {
                    "title": {
                        "value": "Response to Authors"
                    },
                    "comment": {
                        "value": "Dear authors,\n\nThanks again for the extended discussion.\n\nEven though at first I misunderstood the principle of your contribution, I had other concerns about the precision of your theoretical results. Based on my remarks, and those of other reviewers (e.g., Reviewer L98k), these concerns were not fully addressed during this phase of discussion. As a result, I keep my initial score.\n\nOverall, I think the authors consider an important and interesting problem, but the theoretical results need to be defined in more precise terms. On top of that, I think further theoretical discussion on considering discrete distributions should be added to the main paper."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3136/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700483424192,
                "cdate": 1700483424192,
                "tmdate": 1700483424192,
                "mdate": 1700483424192,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "bsXKNWi3Wb",
            "forum": "2DDwxbjP9g",
            "replyto": "2DDwxbjP9g",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3136/Reviewer_L98k"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3136/Reviewer_L98k"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes to analyze an intriguing property of Wasserstein GANs, which suggests that their success may be attributed to a partial failure in minimizing the Wasserstein distance. The authors demonstrate that, when applied to image data, the characteristics of the generated images are, in fact, influenced by the architecture of the discriminator. They use a discrete setting, where the generator takes as input a vector from a set of M vectors. Their study reveals the following findings:\n1. WGANs with fully connected discriminators do effectively minimize the Wasserstein distance.\n2. WGANs with CNN+GlobalAvgPool discriminators minimize a Patch-based Wasserstein distance.\n3. WGANs with CNN discriminators minimize location-specific patch Wasserstein distance."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* The authors analyze and showcase the impact of the discriminator's architecture and inductive bias on the images generated by a WGAN. Doing so, they partly answer the question of what do WGANs actually minimize, since there are suspicions that they do not minimize the Wasserstein distance.  \n\n* The paper is well written and easy to read."
                },
                "weaknesses": {
                    "value": "* Missing an important reference [1] which tackles and solves the same question. They prove theoretically that WGANs-GP actually correspond to the minimization of a \"congested transport distance\" rather than the standard Wasserstein distance. The authors claim that it explains the diversity observed in generated distributions, since this distance penalizes \"congestion\" in transport plans. What can you comment on this? \n[1] Tristan Milne and Adrian I. Nachman. \"Wasserstein gans with gradient penalty compute congested transport.\" Conference on Learning Theory. PMLR, 2022.\n\n* What about WGANs with transformer architectures, which also generate sharp images although they have no inductive bias for patch learning? Or other architectures like MLP-Mixer? Notably, TransGAN [2] uses a WGAN-GP loss and still generates high-quality images. \n\n[2] Jiang, Y., Chang, S., & Wang, Z.. Transgan: Two pure transformers can make one strong gan, and that can scale up. NeurIPS 2021.\n\n* About the interpretation of CNN-GAP and Theorem 2: Depending on the depth of the neural net and kernel size of convolutional filters, the receptive field of each patch might actually be the whole image. Then, why would it lead to different results than the fully connected discriminator? Wouldn't it be another reason, like optimizability of the neural net, explaining the fact that we have different results?  To verify this, it could be interesting to progressively increase the size of receptive field, and observe the changes in the generated images. \n\n* About the setting $M<<N$ : is it really realistic when GANs actually try to approximate a discrete distribution (empirical distribution with N training points) with a continuous one (generator with continuous latent space)?  How to connect the results with $M<<N$ to the realistic setting? What does it say about the realistic setting? This is not clear from the paper.\n\n* \"As is common in WGAN training we do not train the discriminator until convergence but rather do a single update after each mini-batch and use the gradient penalty to regularize the generator\" Actually, in WGANs, discriminators are generally either updated for several steps per generator update, either have a larger learning rate than generators (TTUR). This might lead to a problem in your setting (2.1). If the discriminators was trained optimally, we could expect the generator to overfit and to reproduce some images of the training set. How come does it not happen? Is it because of your setting M<<N, or could it happen because of suboptimal optimization? \n\n* The multi-scale minimization of SWD seems interesting but should be studied more in depth. Moreover, it should be explained more carefully, at least with some equations or with an algorithm.\n\n* Minor remarks on writing/formatting problems: title, some citations are not well included in the text (page 3: \"the POT package Flamary et al. (2021)\" or \"reweighted least squares. Weiszfeld (1937)\"), no end to the proof of Theorem 2, 'locaton' -> 'location' page 8, \"in defence of wasserstein\" in the Openreview title."
                },
                "questions": {
                    "value": "See weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3136/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3136/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3136/Reviewer_L98k"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3136/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698067765677,
            "cdate": 1698067765677,
            "tmdate": 1699636260649,
            "mdate": 1699636260649,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "I3vQvcHDSE",
                "forum": "2DDwxbjP9g",
                "replyto": "bsXKNWi3Wb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3136/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3136/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your very detailed review of our work. Below are our detailed responses to the weaknesses and questions you raised. We agree with many of your concerns but we believe that they do not detract from our main result. To the best of our knowledge, we are the first to show an analytical connection between the loss optimized by GANs and the discriminator architecture and to show that minimizing patch Wasserstein gives sharp images while minimizing image Wasserstein gives blurry images. We encourage you to read our answers and let us know if they are satisfactory and if so, to consider revising your rating.\n\n## Additional comments:\n- __Weakness 1:__ We agree that the work of [1] is very relevant to our work as it too, tries to answer the question raised in the same papers we reference. However it\u2019s worth noting that the gradient penalty is not essential to the success of WGANs nor to  any of our theoretical results. WGANs also generate sharp images with other regularizers such as weight clipping or spectral normalization so that [1] cannot explain their success. More importantly [1] does not address the influence of the architecture on the minimized distance which we have shown is crucial to understanding the success of WGANs.\n- __Weakness 2:__ Transformer architectures are indeed interesting but we disagree with the statement that \u201cthey have no inductive bias for patch learning\u201d. In fact, the key to the success of visual transformers is that they use image patches as tokens and many visual transformers used as classifiers give the same classification to an original image and an image in which the patches have been randomly permuted (see [3]). Thus our theorem 2 can be extended to prove that WGANs also minimize patch Wasserstein distance when using such visual transformers.\n- __Weakness 3:__ The size of the receptive field  is an interesting point we referred to shortly at the end of section 6 of the paper. We have actually conducted the experiment that you suggest with different receptive field sizes in the discriminator and found that the generated images change as expected by the theory. We will include these in the final version. Indeed many actual CNN architectures have a very big potential receptive field but  please see [2] where it is shown that the \u201ceffective\u201d receptive field of commonly used CNNs is much smaller than its potential size.\n- __Weakness 4:__ Regarding the continuous generator, please see the second paragraph of the \u201climitations and extensions\u201d section and our comment to all reviewers.\n- __Weakness 5:__ Training a GAN requires a delicate equilibrium between the two trained players, there is no secret ingredient to finding it and while usually TTUR or more discriminator updates are required this may change according to the used architecture. As you can see in our supplementary code we did use TTUR for most of our experiments. As for the second part of your comment: We did not train the discriminator to optimality. Hypothetically one should expect some optimization difficulties and even if the discriminator is overfit, it is always overfit with respect to the current generator which can easily deceive the discriminator in many non-natural ways on the next step. So we should not necessarily expect to see it generating copies of some of the dataset.\n- __Weakness 6:__ Our multiscale SWD optimization is very straightforward and only requires changing the patch size we use throughout training. We will add more details in our final revision. \n- __Weakness 7:__ Thank you for the formatting suggestions that we will correct in our final revision\n\n## Citations:\n[1] Tristan Milne and Adrian I. Nachman. \"Wasserstein gans with gradient penalty compute congested transport.\" Conference on Learning Theory. PMLR, 2022.\n\n[2] Brendel, Wieland, and Matthias Bethge. \"Approximating cnns with bag-of-local-features models works surprisingly well on imagenet.\" arXiv preprint arXiv:1904.00760 (2019).\n\n[3] Qin, Yao, et al. \"Understanding and improving robustness of vision transformers through patch-based negative augmentation.\" Advances in Neural Information Processing Systems 35 (2022): 16276-16289."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3136/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700064595096,
                "cdate": 1700064595096,
                "tmdate": 1700064595096,
                "mdate": 1700064595096,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0KfifkajBo",
                "forum": "2DDwxbjP9g",
                "replyto": "I3vQvcHDSE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3136/Reviewer_L98k"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3136/Reviewer_L98k"
                ],
                "content": {
                    "comment": {
                        "value": "I acknowledge your answers. Especially, I appreciate your answers about my concerns on 1) **the architecture**: your point is valid that there is still a bias for patch learning in transformers  ; 2) **the receptive field**: even though the receptive field is large, the effective receptive field might be according to Wieland and Bethge, 2017. \n\nHowever, I am still concerned that some experiments are lacking a rigorous demonstration of the proposed assertions.  1) About the **receptive field**: while the depth might not be useful to increase the receptive field, what about the kernel size of the convolutional filters? modern CNNs have shown increased capacity while using very large kernel sizes. 2) About the setting **M << N**: despite acknowledging the new figure presented by the authors, I would expect a more careful study of this issue. \n\nTherefore, at present, I do not consider increasing my rating."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3136/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700471929281,
                "cdate": 1700471929281,
                "tmdate": 1700471929281,
                "mdate": 1700471929281,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "5bUyfZ0Mdw",
            "forum": "2DDwxbjP9g",
            "replyto": "2DDwxbjP9g",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3136/Reviewer_ZrfD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3136/Reviewer_ZrfD"
            ],
            "content": {
                "summary": {
                    "value": "The paper contributes to a debate \"do Wasserstein GANs actually optimize Wasserstein distance and is that why they work well?\"\n\nIt's previously been noted that a batch of blurry Kmeans images achieve better Wasserstein distance than a batch of real images. It brought up a question if W1 is a sensible optimization metric for generative image modeling. This paper analyzes the influence of discriminator architecture on the exact mathematical objective. The authors claim that a fully connected discriminator indeed optimizes image Wasserstein distances and yields blurry images in agreement with the prior work. However, convolutional architectures optimize Wasserstein distance on the distribution of image patches which is different and doesn't lead to blurry images. Moreover, the authors compare CNN discriminator with Global Average Pooling layer vs FC layer and notice that it determines whether the GAN captures location information (ex: where eyes should be on human faces).\n\nIt is worth noting that the authors use a discrete version of WGAN in order to faciliate the exact computation of Wasserstein distance: they fix a certain number of latent vectors.\n\nThe authors also suggest a simple way to directly optimize this simplified version of Wasserstein distance. It supports their claims of the influence of discriminator objective on the generated image distribution."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Related work is great, it explains well the prior conversation about WGANs and why there are doubts whether it is a good thing to optimize the Wasserstein distance. The experiments are quite sound and easy to follow. The paper is well-written in general. I also liked that the authors support the WGAN experiments with alternative ways to directly optimize Wasserstein distance. I think the idea is also original enough to warrant a publication and will be useful for future research even if it doesn't have immediate practical implications like pushing SOTA."
                },
                "weaknesses": {
                    "value": "I suppose the main weakness is that the architectures analyzed in the paper are quite dated and simple compared to what people use nowadays. The datasets are pretty basic and small as well. The field moved a lot since the WGAN paper hence most likely their findings can't be applied to the state-of-the-art GANs. Nevertheless, I find this analysis interesting and insightful, it highlights that the architecture influences the exact mathematical function we optimize during training and affects the outcome in a more dramatic way than just \"this network gets better accuracy on ImageNet-like problems\"."
                },
                "questions": {
                    "value": "I suggest to add network architecture diagrams at least in supplementary material. Focusing on subtle differences in architectures without clearly explaining them is odd."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3136/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698407257498,
            "cdate": 1698407257498,
            "tmdate": 1699636260557,
            "mdate": 1699636260557,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "t3QRUlc3jU",
                "forum": "2DDwxbjP9g",
                "replyto": "5bUyfZ0Mdw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3136/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3136/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your very detailed review of our work. We really appreciate your attention to the importance of our research subject and its significance to future research. We believe that GANs are still widely used and their underlying mechanism is massively misunderstood. Understanding and simplifying it is a very important task in our view. We will be happy to answer any further questions."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3136/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700064228676,
                "cdate": 1700064228676,
                "tmdate": 1700064228676,
                "mdate": 1700064228676,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "NdNXLXeHHx",
            "forum": "2DDwxbjP9g",
            "replyto": "2DDwxbjP9g",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3136/Reviewer_scef"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3136/Reviewer_scef"
            ],
            "content": {
                "summary": {
                    "value": "This paper claims that the WGANs optimizes the Wasserstein distance between the real data and generated data distributions. The authors used a so-called discrete generator which generates data from a finite number of fixed random noises. They claimed that when the discriminator architecture is a fully connected network, then WGANs minimize the Wasserstein distance between real data and generated data distributions. If the discriminator architecture is a Convolutional Neural Network (CNN), then the WGANs minimize the Wasserstein distance between the patches in the real images and generated images."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The authors showed in Theorem 3 in this paper that when a discriminator is a CNN with global average pooling in the second last layer, the WGAN approximately optimizes the Wasserstein distance between patches of the real images and the generated images."
                },
                "weaknesses": {
                    "value": "Weakness:\n\n1. The theoretical result in Theorem 1 does not support the claim that when the discriminator architecture is a fully connected network, WGANs minimize the Wasserstein distance between real data and generated data distributions. \n    - My major concern is the discrete generator, that generates only $M$ samples. In fact, a generator could generate infinity number of samples. So, \"when each generated sample is a linear combination of at least $N/M$ training samples\" in Theorem 1 is not correct. Therefore, why previous works generates blurry images cannot be explained by the linear combinations of training samples introduced in Theorem 1. \n    - The authors obtained Theorem 1 from Eq. 4, which is a primal form of Optimal Transport (OT), but they claimed that WGANs produce the results in Theorem 1. In fact, a WGAN approximates the OT in the dual form, and the generator and discriminator in a WGAN perform updates alternatively. The finally generated data may be different from the generated data $x_j$ shown in Eq. 4. Therefore, Theorem 1 does not support the claim that when the discriminator architecture is a fully connected network, WGANs minimize the Wasserstein distance between real data and generated data distributions. \n\n2. The premises in Theorem 3 does not hold. \n    - The authors assume $M < N$ in Theorem 3, but as I mentioned above, $M$ should be infinity or should be at least much greater than $N$. Also, the authors should assume in Theorem 3 that the patch size of the generated images and the real images should be the same. \n\n3. WGANs have been proposed for many years. I cannot imagine that a WGAN generates poor images like those shown in Figs. 3 and 5. The authors should choose CNNs for the discriminator and generator. \n\n4. The authors uses the Sliced Wasserstein Distance (SWD) to approximate the Wasserstein distance between all the 16x16 patches. How close is a SWD to the real Wasserstein distance? Are there any evidences of literatures that indicating SWD and Wasserstein distance are consistent? \n\n5. Writing in several parts is not clear.\n    - The proof of Theorem 4, when $w_{jc} = 0$ for all $j \\ne i$. Here $i$ and $j$ are just two random indices. I don't understand when $j \\ne i$. \n    - $x_j = y_i$ in theorem 1 is not correct."
                },
                "questions": {
                    "value": "In the end of the first paragraph of Sec. 2.1, the authors use \"gradient penalty to regularize the generator.\" Is this a typo? Because, in general, we use the gradient penalty to regularize the discriminator.\n\n\nDo the patches have overlap in Theorems 2 and 4? \n\n\nTypos: \n\n\"in the layer last linear layer \" in Theorem 4."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3136/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698810579648,
            "cdate": 1698810579648,
            "tmdate": 1699636260429,
            "mdate": 1699636260429,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kc2oraGJPA",
                "forum": "2DDwxbjP9g",
                "replyto": "NdNXLXeHHx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3136/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3136/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your very detailed review of our work. We are concerned with some parts of your review which suggest some misunderstanding of the paper\u2019s claims in theorem 1 and the experiments following it. We would sincerely appreciate it if you read our detailed response and reread the paper and let us know if we can clear up further misunderstandings.\n\nNowhere in the paper do we claim that the theoretical results in theorem 1 \u201csupport the  claim that when the discriminator architecture is a fully connected network, WGANs minimize the Wasserstein distance between real data and generated data distributions.\u201d. Theorem 1 simply characterizes the optimal solution to the Wasserstein distance minimization problem when the generator is discrete. It says nothing about WGANs. \n\nBecause we are using a discrete generator, we can exactly compute the Wasserstein distance between any M generated images and the N training images. In figure 2, we plot this exact Wasserstein distance and show that for all the datasets, the solution found by WGANs with a FC discriminator (blue curve) is close to the optimal solution that is found using OT-means (green curve) . Furthermore, the images that are found by WGANs have the same characteristics that the optimal solution has (they are blurred, rather than sharp as predicted by theorem 1). We believe that this is strong evidence that the WGANs with an FC discriminator indeed minimize the Wasserstein distance. \n\nWe agree that our analysis is for the case of a discrete generator and this is a simplification that we explicitly introduced in page 3 of the paper in order to allow us to compute exact Wasserstein distances. We think it is unfair to say in your review  that \u201cour premises do not hold\u201d  or that \u201cM should be infinity\u201d. A WGAN with a discrete generator is still a WGAN and please note that ALL of the experiments we show in the paper are with a discrete WGAN generator. Yet the same discrete generator can generate blurry images (when the discriminator is FC) or sharp images (when the discriminator is convolutional). That is what we are trying to explain and we believe that our analysis is the first to provide an explanation.\n\nWe explicitly discuss how the results of our paper relate to WGANs with a continuous generator in the second paragraph of the \u201climitations and extensions\u201d section and in the comment to all reviewers. \n\n## Additional comments in response to your questions:\n\n- __Weakness 3:__ SOTA WGANs are indeed capable of generating much prettier images and they mostly use CNN generators. However, our goal here was to understand how GANs work and as with many theoretical analysis papers we use simpler architectures in order to get to the crux of the GAN mechanism. Moreover, there are no theoretical requirements that GAN should only work with a specific architecture and in fact, the original GAN paper [1] used the same FC architecture and the original WGAN paper [2] uses both FC and CNNs.\n\n- __Weakness 4:__ As far as we know, there is no direct connection between SWD and W1 distance apart from the definition of SWD as W1 over 1-dimensional projections of the data. They are both metrics and achieve zero if and only if the distributions are equal. We agree that this fact should be noted if we describe SWD as a \u201cproxy\u201d to the W1 distance.\n\n- __Weakness 5:__ \n    - We agree that both notations are quite confusing and we will fix them in our final revision\nWe abused the index $i$ here: When trying to define the function class $CNN_k$ for some index $k$ we define it using eq. 8 where $W_{jc}=0$ $\\forall j \\neq k$.\n    - The term $x_i=y_j$ will indeed be replaced by $\\forall i \\in [N] x_i=y_i$\n\n- __Question 1:__\n    - Of course, we meant that the gradient penalty regularizes the discriminator and not the generator. \n    - Overlap between patches should not affect the theoretical results as it will only change the distribution we are dealing with. In practice the CNN architecture we used, like most CNNs, uses convolutions with stride=2 thus the stride of the receptive field is $2^n$ where $n$ is the number of convolutions. For example in the CNN+GAP architecture we used, there are 3 convolutions with stride 2 and hence the stride of the receptive field is 8.\n\n## Citations:\n[1] Goodfellow, Ian, et al. \"Generative adversarial networks.\" Communications of the ACM 63.11 (2020): 139-144.\n\n[2] Arjovsky, Martin, Soumith Chintala, and L\u00e9on Bottou. \"Wasserstein generative adversarial networks.\" International conference on machine learning. PMLR, 2017."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3136/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700064110126,
                "cdate": 1700064110126,
                "tmdate": 1700066191360,
                "mdate": 1700066191360,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]