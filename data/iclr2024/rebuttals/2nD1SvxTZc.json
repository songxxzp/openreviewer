[
    {
        "title": "One-Versus-Others Attention: Scalable Multimodal Integration"
    },
    {
        "review": {
            "id": "AIJN7nTwhN",
            "forum": "2nD1SvxTZc",
            "replyto": "2nD1SvxTZc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission348/Reviewer_YfMJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission348/Reviewer_YfMJ"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents One-Versus-Others (OvO), a new scalable multimodal attention mechanism. The proposed formulation involves averaging the weights from each modality during training, significantly reducing the computational complexity compared to early fusion through self-attention and cross-attention methods. OvO outperformed self-attention, cross-attention, and concatenation on three diverse real-world datasets and on a simulation dataset that shows the scalability gains in an extremely multimodal setting. The results demonstrate that the proposed approach improves performance compared to state-of-the-art fusion techniques while decreasing computation costs."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+ The paper presents, OvO, a generalizable multimodal integration scheme that is domain-neutral and does not require modality alignment;\n+ OvO scales linearly with the number of modalities, while also performing competitively to self-attention and cross-attention;\n+ The paper performs robust benchmarking on new simulated and real-world multimodal integration tasks."
                },
                "weaknesses": {
                    "value": "- One of the major weaknesses of the paper is that the experiment section is not convincing. The datasets used are simulation dataset and small scale datasets or datasets which are not reported by other compared methods such as VisualBERT, VL-BERT, etc. The main argument of the paper is that, the paper proposes a scalable one-versus-others attention, which is better than cross-attention used in LXMERT and ViLBERT and self-attention used in VisualBERT and VL-BERT. Thus a fair comparison would be conducting experiments on the same datasets reported by these methods.\n\n- The multimodal fusion strategy is also explored in unified content code extraction of multimodal generation [1]. Adding related work in the reference could make the paper more smooth and have bigger impact.\n[1] Multi-Domain Image Completion for Random Missing Input Data, IEEE Transactions on Medical Imaging, 2021."
                },
                "questions": {
                    "value": "Please check Weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission348/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697824332730,
            "cdate": 1697824332730,
            "tmdate": 1699635961965,
            "mdate": 1699635961965,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5zH0Jy7UAj",
                "forum": "2nD1SvxTZc",
                "replyto": "AIJN7nTwhN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission348/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission348/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the thoughtful feedback. In what follows, we provide responses to the main weaknesses.\n\n**Weaknesses:**\n1. \u201cOne of the major weaknesses of the paper is that the experiment section is not convincing. The datasets used are simulation dataset and small scale datasets or datasets which are not reported by other compared methods such as VisualBERT, VL-BERT, etc. The main argument of the paper is that, the paper proposes a scalable one-versus-others attention, which is better than cross-attention used in LXMERT and ViLBERT and self-attention used in VisualBERT and VL-BERT. Thus a fair comparison would be conducting experiments on the same datasets reported by these methods.\u201d  \nWe thank the reviewer for their comment and understand their concerns. While models such as LXMERT, ViLBERT, VisualBERT, and VL-BERT, are popular and do train on large-scale datasets, they are limited to two modalities (images and text). Our work aims to find a general method that can be applied in multimodal settings that expand beyond two modalities and can be used in any research environment (with less computational costs). We, too, were restricted by our computational resources and thus could not use the large-scale datasets that the aforementioned models were trained on. Our method demonstrates efficiency with a growing number of modalities, which is why we focused on a wider range of modalities: 2-modality data (Hateful Memes), 3-modality data (Amazon Reviews), 5-modality data (TCGA), and 20-modality data (simulation).\n\n2. \u201cThe multimodal fusion strategy is also explored in unified content code extraction of multimodal generation [1]. Adding related work in the reference could make the paper more smooth and have bigger impact. [1] Multi-Domain Image Completion for Random Missing Input Data, IEEE Transactions on Medical Imaging, 2021.\u201d  \nWe thank the reviewer for this suggestion and add this work and reference to the Related Works."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission348/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700287742280,
                "cdate": 1700287742280,
                "tmdate": 1700287742280,
                "mdate": 1700287742280,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "vsERGfmUzU",
            "forum": "2nD1SvxTZc",
            "replyto": "2nD1SvxTZc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission348/Reviewer_UUiq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission348/Reviewer_UUiq"
            ],
            "content": {
                "summary": {
                    "value": "The current state of multimodal learning, particularly in the medical field, is confronted with challenges stemming from the diverse nature of data inputs, such as X-rays and PET scans. These varied data types necessitate a method for efficient and precise information integration. In response to this, the authors have introduced an innovative attention mechanism, termed \"one-versus-others.\" This mechanism stands out for its ability to scale linearly with the number of input modalities, presenting a significant advantage in handling multimodal data. The effectiveness of this approach has been validated through rigorous testing on three real-world datasets, where it consistently outperformed other existing fusion techniques, showcasing its potential to enhance performance in multimodal learning applications."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ The authors have introduced \"one-versus-others,\" a versatile and scalable approach for integrating multimodal data without the need for modality alignment. \n+ Despite its linear scalability with the increasing number of modalities, this method competes effectively with other attention mechanisms in terms of performance. \n+ Demonstrating robust applicability, \"one-versus-others\" has shown promising results on both simulated and real-world multimodal integration tasks, indicating its potential as a reliable tool for handling diverse data inputs."
                },
                "weaknesses": {
                    "value": "- The linear time complexity of \"one-versus-others\" is commendable; however, its storage requirements are substantial. This is evident from Equation 6, where outputs from all attention heads are concatenated and subjected to another multihead attention operation, essentially amounting to a sum of all previous module results.\n- The authors highlight the prevalent focus of existing methods on NLP applications, yet their experiments predominantly utilize NLP datasets. To bolster their argument and the generalizability of their method, the inclusion of diverse data inputs from varied domains, such as x-ray or PET scans, would be beneficial.\n- The datasets used, Amazon and hateful memes, feature a limited number of modalities. This scenario does not truly challenge or demonstrate the linearity of the proposed method.\n- The manuscript appears imbalanced, with the methodology section spanning just one page, and a disproportionate amount of content dedicated to dataset settings and experimental configurations. A recalibration of focus towards the methodological aspects of the paper is suggested.\n- The reported improvements in results are modest, with most datasets showing an enhancement of merely 1%. Such marginal gains may not sufficiently underscore the significance of the proposed method."
                },
                "questions": {
                    "value": "- The utilization of a simulated dataset is perplexing. The multimodal setting is not clearly articulated, and the necessity of simulations is questionable, especially if the proposed method is as universally applicable as suggested. The availability of multiple real-world datasets should negate the need for simulated scenarios.\n- Equation 3 outlines a weighted averaging approach to calculate attention weights, seemingly derived from prior works. This approach could potentially dilute the informative value of the inputs. It would be beneficial for the authors to delve deeper into this aspect and provide empirical or theoretical insights to address these concerns."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission348/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698765072325,
            "cdate": 1698765072325,
            "tmdate": 1699635961875,
            "mdate": 1699635961875,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Qz75XgNUHX",
                "forum": "2nD1SvxTZc",
                "replyto": "vsERGfmUzU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission348/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission348/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the thoughtful feedback. In what follows, we provide responses to the main weaknesses and questions.\n\n**Weaknesses:**\n1. \u201cOvO\u2019s storage requirements are substantial (Eq 6).\u201d  \nWe would like to clarify that Eq 6 is derived from the \"Attention is All You Need paper\" and does not add any computational burden to multiheaded attention. The equation from the original multiheaded attention is:  \n$$\n\\text{MultiHead}(Q, K, V) = \\text{concat}(\\text{head}_1, \\ldots, \\text{head}_h)W^O\n$$\nwhere \n$$\n\\text{head}_i = \\text{Attention}(QW^Q_i, KW^K_i, VW^V_i)\n$$\nOvO multiheaded attention:\n$$\n\\text{MultiheadedOvOAttention}(m_i, \\{m_j: j \\neq i\\}) = \\text{concat}(\\text{head}_1, \\ldots, \\text{head}_h)W^O \n$$\nwhere \n$$\n\\text{head}_k = \\text{OvO Attention}(m_iW_k^{m_i}, \\{m_jW_k^{m_j}: j \\neq i\\})\n$$\nStorage-wise, OvO and the baselines are similar, as all models share identical parameters except in the fusion stage. For instance, each model needs ~487 MB of storage for the Amazon Reviews data, regardless of the fusion approach.  \n2. \u201cThe experiments predominantly utilize NLP datasets.\u201d  \nWe direct the reviewer\u2019s attention to Section 4.2.3 and Table 4, which discuss the five-modality medical datasets used in our experiments. We agree with the reviewer that the generalizability of the method is stronger on diverse data; thus, we have used The Cancer Genome Atlas (TCGA) in our work. TCGA\u2019s modalities are whole-slide images, clinical data, CNV, gene expression, and DNA Methylation (Section 4.2.3 and Appendix B). Although PET scans or X-rays weren't used, whole-slide images serve a similar role in showcasing generalizability. The results on TCGA (Table 4), demonstrate OvO\u2019s success on a dataset outside the NLP domain.   \n3. \u201cThe datasets used, Amazon and hateful memes, feature a limited number of modalities.\u201d   \nWe would like to clarify that the Hateful Memes dataset is designed to demonstrate that the OvO formula does not affect the performance negatively for the simple 2-modality case. To demonstrate the linearity of OvO and the generalizability of the method, we present four multimodal scenarios: 2-modality data (Hateful Memes), 3-modality data (Amazon Reviews), 5-modality data (TCGA), and 20-modality data (simulation). We believe that this range of datasets is sufficient to demonstrate the efficiency of OvO, and we are happy to discuss these results further.  \n4. \u201cThe manuscript appears imbalanced.\u201d  \nWe will keep this suggestion in mind and rearrange sections of the paper appropriately. \n5. \u201cThe reported improvements in results are modest, with most datasets showing an enhancement of merely 1%.\u201d  \nWe recognize the reviewer's concern and emphasize that our work aims to achieve superior computational efficiency rather than surpassing performance of existing methods. Since we do observed performance gains, we use hypothesis testing to show that OvO's increase in accuracy is statistically significant. In terms of percentages, in the Amazon Reviews data, OvO's 523,520 FLOPs represent a 72.50% reduction compared to the 1,903,616 FLOPs each for cross and self-attention when isolating attention layers. In the five-modality TCGA dataset, OvO's 330,240 FLOPs amount to reductions of 93.73% and 94.96% compared to cross and self-attention's 5,260,800 and 6,556,160 FLOPs, respectively, underscoring OvO's superior efficiency.  \n\n**Questions:**\n1. \u201cThe utilization of a simulated dataset is perplexing.\u201d  \nWe demonstrate the generalizability of the proposed formulation on three real-world datasets ranging from 2 to 5 modalities. However, as the reviewer pointed out, the real-world datasets may not have enough modalities to demonstrate linearity in complexity beyond a certain number. Therefore, to systematically study the complexity of OvO attention and to show that it is indeed linear with respect to the number of modalitiles, we designed the simulation to scale the number of modalities to 20. Our results show that as the number of modalities increases, OvO grows linearly, whereas self and cross-attention grow quadratically.  \n2. \u201cEq 3 outlines a weighted averaging approach to calculate attention weights, that could potentially dilute the informative value of the inputs.\u201d  \nWe would like to clarify that Eq 3 is not a weighted average, as $m_i$ is not a scalar coefficient but the \u201cmain\u201d modality that gets multiplied by all other modalities and a neural network weight matrix (W). This dot product creates a similarity function that is the essence of every attention calculation (Section 3.1). The suspected dilution of information would be evident if there were significant decreases in performance, but we observe the opposite. Specifically, in Section 5.3 and Appendix F, we show attention heatmaps for the TCGA and Amazon Reviews data, that suggest that the strongest unimodal modalities are also the ones paid most attention to. This means that the attention scheme we created is working appropriately and not losing information."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission348/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700287509330,
                "cdate": 1700287509330,
                "tmdate": 1700287509330,
                "mdate": 1700287509330,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "NehHrd9hJr",
            "forum": "2nD1SvxTZc",
            "replyto": "2nD1SvxTZc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission348/Reviewer_sMs5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission348/Reviewer_sMs5"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a scalable attention mechanism for multimodal fusion named One-Versus-Others (OvO).  OvO averages the weights from each modality during training, reducing the computational complexity compared to early fusion through self-attention and cross-attention methods. The results demonstrate that the proposed approach outperforms self-attention, cross-attention, and concatenation on three diverse real-world datasets and a simulation dataset."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper proposed a multimodal attention fusion mechanism, scaling linearly with the number of modalities, which is more efficient than self-attention and cross-attention.\n2. The proposed OvO has the potential for a large number of modalities due to the small computational costs and compared performance."
                },
                "weaknesses": {
                    "value": "1. The novelty seems limited, as this paper's contribution is only a new design of multimodal attention that averages weights from all other modalities\u2019 encoders.\n2. The introduction of the baseline model is insufficient,  including the architecture details and the references. \n3. In the experimental results of the simulated dataset in Figure 3, it seems that the performance improvement achieved by OvO is marginal compared to concatenation with similar FLOPs.\n4. The compared method is insufficient. There are some different fusion mechanisms in  [1] such as Hierarchical Attention.\n    * [1] Xu P, Zhu X, Clifton D A. Multimodal learning with transformers: A survey[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023."
                },
                "questions": {
                    "value": "1. Please compare parameters to demonstrate the efficiency of OvO further.\n2. Is there any reference for the simulation of the 20 simulated modalities? Why is the simulation analogous to a medical setting? Please clarify.\n3. Is there any pre-trained model used for the training? The self-attention and cross-attention are both used to pretrain large models (e.g.,  ALBEF,  VisualBERT) with large datasets (e.g., COCO[1], SBU Captions[2]). However, the scale of datasets in the paper is relatively small, which is unfair. Could the OvO achieve compared performance on the big dataset? Please clarify the model details and the task application to demonstrate the advantages of the proposed method. \n5. Please check the formula  $k^P_2$ in Section 3.3.\n\n    * [1]T.-Y. Lin et al., \u201cMicrosoft COCO: Common objects in context,\u201d in Proc. Eur. Conf. Comput. Vis., 2014, pp. 740\u2013755. \n    * [2] V. Ordonez, G. Kulkarni, and T. Berg, \u201cIm2Text: Describing images using 1 million captioned photographs,\u201d in Proc. Int. Conf. Neural Inf. Process. Syst., 2011, pp. 1143\u20131151."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission348/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698802393738,
            "cdate": 1698802393738,
            "tmdate": 1699635961801,
            "mdate": 1699635961801,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "VkK9Y9JIDO",
                "forum": "2nD1SvxTZc",
                "replyto": "NehHrd9hJr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission348/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission348/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the thoughtful feedback. In what follows, we provide responses to the main weaknesses and questions.\n\n**Weaknesses:**\n1. \u201cThe contribution is only a new design of multimodal attention that averages weights from all other modalities\u2019 encoders.\u201d  \nWe appreciate the reviewer\u2019s feedback and clarify that the OvO formula is a different formulation than the one used in self and cross-attention. It is more involved than just averaging weights of all other modalities\u2019 encoders, as it learns similarity by multiplying one modality by a neural network weight matrix W, and then to the average of all other modalities. This step is crucial as W becomes a learned parameter that helps strengthen inter-modal understanding. \nWe believe that the contribution of the work should not be judged based on the simplicity of the solution but on the complexity of the problem it solves. OvO offers a new domain-neutral attention mechanism that substantially lowers computational complexity and maintains competitive performance compared to existing methods. This is increasingly relevant in diverse domains like healthcare, e-commerce, and autonomous vehicles, where efficient data integration is key.   \n2. \u201cBaseline descriptions are insufficient.\u201d  \nBaseline descriptions are in Section 4.3, with additional dataset specifics in Section 4.2. The baselines are consistent across tasks, and the only differences lie in the modality encoders, like BERT for text and ResNet for images, or a standard multilayer perceptron for medical modalities where there is no known encoder (see Section 5.2). We are happy to make further clarifications. \n3. \u201cIn simulation, the performance improvement achieved by OvO is marginal compared to concatenation.\u201d  \nThe simulation experiment aims to demonstrate the linear complexity growth with increasing modalities. Figure 3b, showing accuracy, ensures that reduced FLOPs do not mean a drop in performance. While the simulation task was designed to be straightforward for any method, including concatenation, real-world tasks, being more complex, usually favor attention-based models. Hence, in real-world datasets, our method shows a statistically significant improvement over other attention schemes and concatenation.\n4. \u201cThe compared method is insufficient (e.g., Hierarchical Attention).\u201d  \nWhile cross-attention and self-attention are two different uses of the attention formula, hierarchical attention is not a fundamental variation on attention but rather an early fusion model with self-attention in it, followed by a multi-stream scheme, which does not involve further fusion via attention. \n\n**Questions:**\n1. \u201cPlease compare parameters.\u201d  \nOvO's efficiency stems from its linear, rather than quadratic, modality fusion. The number of parameters remains consistent across fusion schemes. For example, concatenation, self-attention, cross-attention, and OvO attention, all maintain 122,027,094 parameters for the Amazon Reviews dataset. The parameter count consistency is due to the constant dimensions of input and output layers across fusion methods. In FLOPs, a key metric of computational efficiency in deep learning models, OvO's 523,520 FLOPs significantly undercut the 1,903,616 FLOPs of both cross and self-attention, marking a 72.50% reduction and highlighting OvO's efficiency.\n2. \u201cWhy is the simulation analogous to a medical setting?\u201d  \nIn medical diagnostics, providers must consider all available information. Neglecting any modality (e.g., disregarding imaging and focusing solely on genetics) can compromise patient assessment and risk misdiagnoses. Since the medical domain is where more modalities are present - imaging alone can span many types (pathology images, MRIs, PET scans, X-rays, etc.), we wanted to simulate a scenario where every simulated modality was needed to classify correctly. The simulation's main purpose is to demonstrate linear complexity growth with increasing modalities. We'll relocate this section to avoid confusion to align with the complexity analysis, not the dataset results.\n3. \u201cThe datasets in the paper are small, which is unfair as self and cross attention are pretrained on large datasets.\u201d  \nWhile models such as ALBEF and VisualBERT train on large-scale datasets, they are limited to two modalities (images and text). We aim to find a general method that can expand beyond two modalities and can be used in any research environment (with less computational costs). We, too, were restricted by our computational resources and could not use the large-scale datasets that the aforementioned models were trained on. Our method demonstrates efficiency with a growing number of modalities, which is why we focused on a wider range of modalities:  2-modality data (Hateful Memes), 3-modality data (Amazon Reviews), 5-modality data (TCGA), and 20-modality data (simulation). \n4. \u201cCheck the formula in Section 3.3.\u201d  \nWe have corrected the notation in the permutation formula."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission348/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700284693867,
                "cdate": 1700284693867,
                "tmdate": 1700284693867,
                "mdate": 1700284693867,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]