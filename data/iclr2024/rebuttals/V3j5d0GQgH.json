[
    {
        "title": "FedLoGe: Joint Local and Generic Federated Learning under Long-tailed Data"
    },
    {
        "review": {
            "id": "gn3jzbXr59",
            "forum": "V3j5d0GQgH",
            "replyto": "V3j5d0GQgH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4997/Reviewer_YQxK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4997/Reviewer_YQxK"
            ],
            "content": {
                "summary": {
                    "value": "This paper aims to address the long-tailed data in FL. The authors propose a new method called FedLoGe to improve both local and global model performance of FL under this scenario. FedLoGe introduces a Static Sparse Equiangular Tight Frame Classifier (SSE-C) to enhance data representations. It also contains feature realignment at both global and local levels."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The long-tailed problem in FL is an emerging problem that is worth studying.\n- The paper is generally well structured.\n- Figure 2 provides a good illustration of the proposed method.\n- The proposed method seems to work and the experiments demonstrate the effectiveness of the proposed method.\n- Evaluation are conducted on various backbones and different scales of datasets."
                },
                "weaknesses": {
                    "value": "- The content of the paper is not very easy to follow. For example, the message from Figure 1 is a bit hard to interpret. In the first paragraph of the third page (where Figure 1 (a) is explained), it is not quite clear what large mean and small mean imply.\n- Notations seem to be inconsistent, the notation for the number of clients is K in Section 3, but N in Section 4.\n- Probably a typo: \u201c\u2026. personalized classifier by multiple the norm of \u2026 \u201c Should it be \u201cby multiplying\u2026 \u201c?\n- The method of fixing the classifier and backbone alternatively for training is proposed in previous papers on personalized FL.\n- Some settings are not clear: e.g., the number of clients participating in training each round and the number of local epochs."
                },
                "questions": {
                    "value": "- How many clients participate in training each round?\n- Does the paper focus on cross-silo FL, cross-device FL, or both?\n- Why the proposed method can improve performance on FL with long-tailed problem?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4997/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4997/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4997/Reviewer_YQxK"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4997/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698515549090,
            "cdate": 1698515549090,
            "tmdate": 1699636487526,
            "mdate": 1699636487526,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "NCmD3kgqUI",
                "forum": "V3j5d0GQgH",
                "replyto": "gn3jzbXr59",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4997/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4997/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response for Reviewer YQxK[1/2]"
                    },
                    "comment": {
                        "value": "# Response for R4\n\nMany thanks for your detailed and valuable comments. I have addressed each of your points in the following responses.\n\nQ1. What does Figure 1 signify, and what are the implications of large and small means?\n\nThanks for the question and we are glad to clarify what Fig. 1 signifies: Compared to Fig. 1 (a), **two significant changes** are evident in Fig. 1 (b) after implementing FedLoGe: **1. Noisy features are masked.** The small mean features with bigger variances are located in the sparsely allocated area, marked by grey vertical shadows. 2. **The quality of dominant features is further enhanced**. This is evidenced by the decreased variance in features with larger means, indicating they have become more concise and effective. \n\nImplications of large and small means: Features with both **large and small means** are crucial for a network. However, training directly with a fixed ETF classifier can lead to poor quality of small mean features. This is reflected in the relative variance of smaller mean features within the same class being much higher compared to large mean features.\n\nBelow is a detailed interpretation of Fig. 1:\n\nNeural collapse reveals a principle, namely feature collapse, which shows that in a properly trained network, features should effectively collapse into the class mean (**where all relative variances in Fig. 1 (a) are expected to be zero**). Yet, when trained with a vallina fixed ETF classifier, **features, particularly those with smaller means, do not collapse effectively, namely feature degeneration. FedLoGe is designed to mitigate feature degeneration by squeezing poor-quality and noisy features into these sparsely allocated positions.** Overall, as shown in Fig. 1, after training with $\\psi_{SSE-C}$, FedLoGe inherently filters out noisy smaller mean features, making features more concise (Appendix A.4) and effective (Table 3 in the main paper), in which the variance of dominant features becomes smaller. \n\nWe give the detailed computation process for the data in Fig. 1:\nGiven that $n_c$ is the sample number of class $c$.  $\\mu_c$ is the class mean of class $c$, and $h_{i,c}$ is the feature of class $c$ on sample $i$, the features $h_{i,c}$ will collapse to the within-class mean $\\mu_c = \\frac{1}{n_c} \\sum_{i=1}^{n_c}h_{i,c}$. This indicates that the covariance $\\frac{1}{n_c} \\sum_{i=1}^{n_c}(h_{i,c}-\\mu_c)(h_{i,c}-\\mu_c)^T$ will converge to $0$. We investigate whether $h_{i,c} - \\mu_c$ tends toward zero when training with a fixed ETF classifier, as shown in Fig. 1 (a). \n\nThe steps to obtain the data in Fig. 1 are\uff1a\n\n1. Select class $c$.\n2. During global test, compute all sample features $h_{i,c}$, with dimension $D$, representing the number of features. Use $h_{i,c}[d]$ to denote the $d^{th}$ feature in $h_{i,c}$.\n\n 3.  Compute the mean and relative variance of $h_{c}$, sorting them in descending order of the mean, and accordingly adjust the variance positions.\n\nIt becomes evident that not all features, $h_{i,c}[d] - \\mu_c[d]$, converge to zero, and smaller $\\mu_c[d]$ values exhibit greater variance.\n\nThe process illustrated in **Fig.1 (b)** follows the same steps as **Fig.1 (a)**, but with grey vertical shadows for the sparsely positioned features. It can be observed that sparse ETF training effectively squeezes poor-quality and noisy features into these sparsely allocated positions.\n\n**All the calculation details have been updated in the revised appendix A.2 marked in red.**\n\nQ2. Notations and typos?\n\nThank you for the detailed feedback! We have corrected all the inconsistent usage of K and N and revised \u201c\u2026. personalized classifier by multiple the norm of \u2026 \u201c to \u201cby multiplying\u2026 \u201c in Section 3.3. and have uploaded the revised version and **marked the modifications in red color.** \n\nQ3. The method of fixing the classifier and backbone alternatively for training is proposed in previous papers on personalized FL.\n\nWe need to clarify that the alternative training framework is a well-established training paradigm, inspired by decoupling learning [4]. **However, the main contribution of FedLoGe lies in being the first to achieve joint global and local training in Federated long-tailed learning, approached from the perspective of neural collapse**, utilizing the novel methods, $\\psi_{SSE-C}$ and GLA-FR.\n\nWe detail the differences between FedLoGe and traditional alternative training methods in personalized FL: Traditional alternative training for personalized FL **ignores representation learning for backbone**, making it **challenging to obtain global model and personalized models simultaneously**, as a strong backbone is foundational for both. Moreover, other personalized FL methods are **not specifically tailored for long-tail and heterogeneous data**.\n\nFor example, FedRep [1] alternatively trains the classifier and backbone within a single local round. However, it does not specifically address imbalanced and long-tailed learning."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4997/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700222994117,
                "cdate": 1700222994117,
                "tmdate": 1700281392716,
                "mdate": 1700281392716,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ntn8ygDPMe",
                "forum": "V3j5d0GQgH",
                "replyto": "gn3jzbXr59",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4997/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4997/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response for Reviewer YQxK[2/2]"
                    },
                    "comment": {
                        "value": "CreFF (Fed-LT) in [2] involves classifier retraining but does not make advancements in representation learning, nor does it provide personalized models. In contrast, FedLoGe is capable of **simultaneously training both generic and local models**. Notably, FedLoGe **integrates strong representation learning** [8] and **feature realignment to mitigate long-tailed and heterogeneous biases** through norm calibration [9, 10], all within the framework of neural collapse. \n\n**Please refer to the detailed discussion of the differences between our work and previous work in the responses to Reviewer tSL5 (W4), or in Section A.11 of our paper.**\n\nQ4. Some settings are not clear: e.g., the number of clients participating in training each round and the number of local epochs. How many clients participate in training each round? Does the paper focus on cross-silo FL, cross-device FL, or both?\n\nThank you for detailed suggestions that really help in refining our work. FedLoGe covers diverse system scale in different dataset. For CIFAR10/100-LT datasets, we engaged 40 clients with full participation in each training round, setting the number of local epochs to 5. For ImageNet/iNat datasets, the experiments involved 20 clients, with a 40% participation rate in each training round, and the number of local epochs was set to 3.  **We have updated Section A.1, Detailed Setup, and marked the modifications in red for clarity.**\n\nTo assess FedLoGe's resilience and effectiveness in diverse settings,  we conducted experiments on CIFAR100-LT with imbalance factor of 100 and heterogeneity coefficient of 0.5, while lowering the participation rate to 30%. The results, documented in the table below, shows that our approach remains superior, particularly in terms of personalized performance. \n\nTo further explore the effectiveness of FedLoGe in a cross-device FL setting, we increased the number of clients to 100. The results demonstrate that FedLoGe significantly outperforms other algorithms.\n\n| Method | 40 clients(100% participate) | 40(30%) | 100(30%) |\n| --- | --- | --- | --- |\n| FedAvg | GM0.3818/PM0.6214 | 0.3746/0.6205 | 0.3718/0.6384 |\n| FedRoD | 0.3919/0.6919 | 0.3758/0.7156 | 0.3727/0.7665 |\n| FedETF | 0.3825/0.6421 | 0.4180/0.7083 | 0.3847/0.7822 |\n| FedLoGe | 0.4233/0.7285 | 0.4206/0.7524 | 0.4159/0.8008 |\n\n>Q5. Why the proposed method can improve performance on FL with long-tailed problem? \uff08GLA-FR\uff09\n\nThanks for the question. Here's a detailed explanation of the mechanisms involved:\n\n**The classifier's class vector norm positively correlates with its predictive power and the proportion of that class's samples in the overall training set.**  The pattern has been noted in references [4, 5, 6]. Reference [7] offers an empirical explanation for this pattern in terms of decision boundaries. Additionally, within the neural collapse framework, references [9, 10] provide theoretical justification for the relationship between the norm and data distribution, based on the peer model.\n\n**The global model trained on long-tailed often exhibits biases**, favoring predictions for head classes. GA-FR addresses this by implementing self-normalization, dividing each class vector by its own norm to make them unit vectors in order to encourage the model to make more fair judgments. \n\nAs seen in **Fig.2, the norm of $\\psi$ becomes more balanced distributed after Global Adaptive Feature Realignment (GA-FR).** For experiments, we have reported improvements in Tables 1 and 2 and there are incredible performance enhancements for few and medium classes.\n\nIt is important to note that Local Adaptive Feature Realignment (LA-FR) is designed to tailor the local model to fit local data, which is heterogeneous rather than typically long-tailed.\n\n\nMore discussion on the motivation and effectiveness of using the integrated personalized classifier for each client as well as the feature alignment can be found in the response of Reviewer Nmh4, Q2, and Reviewer YHb3, Q3.\n\n[1] Exploiting Shared Representations for Personalzied Federated Learning. PMLR 2021.\n\n[2] Federated Learning on Heterogeneous and Long-Tailed Data via Classifier Re-Training with Federated Features. IJCAI 2022.\n\n[3] Fedic:\u00a0Federated\u00a0learning on non-iid and long-tailed data via calibrated distillation. IEEE ICME 2022.\n\n[4] Decoupling representation and classifier for long-tailed recognition. ICLR 2020.\n\n[5] Equalization loss v2: A new gradient balance approach for long-tailed object detection. CVPR 2021.\n\n[6] Over-coming classifier imbalance for long-tail object detection with balanced group softmax. CPVR 2020.\n\n[7] Adjusting decision boundary for class imbalanced learning. IEEE Access 2020.\n\n[8] Prevalence of neural collapse during the terminal phase of deep learning training. PNAS 2020.\n\n[9] Neural collapse in deep linear network: From balanced to imbalanced data. ICML 2023.\n\n[10] Imbalance trouble: Revisiting neural-collapse geometry. NIPS 2022."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4997/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700223068129,
                "cdate": 1700223068129,
                "tmdate": 1700281504976,
                "mdate": 1700281504976,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DG0tdvHGhJ",
                "forum": "V3j5d0GQgH",
                "replyto": "ntn8ygDPMe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4997/Reviewer_YQxK"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4997/Reviewer_YQxK"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the rebuttal. Most of my concerns are addressed. However, I am still a bit concerned about Figure 1. The responses (also responses to other reviewers) help improve understanding, but it would be hard for a fresh reader to interpret the figure with only the description provided in the paper. \n\nAfter reviewing the revised manuscript again, it seems that not all citations are in the correct format. For example, \"We examine the effectiveness of training with fixed classifiers in Fed-LT from the perspective of neural collapse (NC) Papyan et al. (2020).\" Should the citation be like (Papyan et al., 2020)?"
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4997/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700636760059,
                "cdate": 1700636760059,
                "tmdate": 1700636760059,
                "mdate": 1700636760059,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UwpugtqnRn",
                "forum": "V3j5d0GQgH",
                "replyto": "gn3jzbXr59",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4997/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4997/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Further Response for Reviewer YQxK"
                    },
                    "comment": {
                        "value": "Thank you for your detailed suggestions for revision. We have **redrawn Fig. 1** and added a detailed explanation of its calculation process in **Appendix A.2**. Additionally, we have **rewritten the caption** of Fig. 1 for clarity and included a **footnote on Page 2** to explain the implications of Fig. 1. These revisions have been **highlighted in red** and may help fresh readers to understand.\n\nFurthermore, we have meticulously reviewed all citations and corrected their formatting.\n\nPlease refer to our revised version for all changes. We are grateful for your feedback and will continue to refine our work."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4997/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700644465692,
                "cdate": 1700644465692,
                "tmdate": 1700645178266,
                "mdate": 1700645178266,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "314JXH6t38",
                "forum": "V3j5d0GQgH",
                "replyto": "gn3jzbXr59",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4997/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4997/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Kind Reminder: Review Deadline Approaching"
                    },
                    "comment": {
                        "value": "Dear Reviewer YQxK:\n\nAs the review deadline nears, with just six hours remaining, we wish to briefly highlight our **recent latest revision**.  In the latest update, we have focused on enhancing the clarity of Fig. 1 and have meticulously revised our citations to ensure full compliance.\n\nWe hope these updates facilitate your re-evaluation. Your insights are invaluable to us, and we deeply appreciate your time and attention.\n\nWarm regards,\n\nAuthors"
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4997/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700721920427,
                "cdate": 1700721920427,
                "tmdate": 1700722049589,
                "mdate": 1700722049589,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "AsJR4apuo8",
                "forum": "V3j5d0GQgH",
                "replyto": "314JXH6t38",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4997/Reviewer_YQxK"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4997/Reviewer_YQxK"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the response, and they resolve most of my concerns. I will decide on the final rating in the next phase after discussing it with other reviewers."
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4997/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700730141170,
                "cdate": 1700730141170,
                "tmdate": 1700730141170,
                "mdate": 1700730141170,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "yvLDlkPzKY",
            "forum": "V3j5d0GQgH",
            "replyto": "V3j5d0GQgH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4997/Reviewer_tSL5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4997/Reviewer_tSL5"
            ],
            "content": {
                "summary": {
                    "value": "This paper presented a model training framework that enhances the performance of both local and generic models in Fed-LT settings in the unified perspective of neural collapse. The proposed framework is comprised of SSE-C, a component developed inspired by the feature collapse phenomenon to enhance representation learning, and GLA-FR, which enables fast adaptive feature realignment for both global and local models. As a result, the proposed method attains significant performance gains over current methods in personalized and long-tail federated learning."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "S1. This work focuses on addressing the data imbalance issue to simultaneously enhance the efficacy of the generic global model and its performance at the local level, which is an interesting topic.\n\nS2. This paper is well-written and has a good presentation.\n\nS3. The proposed SSE-C can address the problem of feature degeneration well, which is a promising finding."
                },
                "weaknesses": {
                    "value": "W1. Convergence analysis is missing, which is very important for the optimization process of federated learning. Please analyze it from the experimental and theoretical point of view.\n\nW2. Lack of discussion about privacy. Federated learning is proposed to protect the privacy of the client, but the method in this paper has the risk of gradient disclosure, please add the discussion of privacy in this work.\n\nW3. The communication overhead of the model seems to be very large, which will restrict the practical application. Please increase the experiment and analysis of the communication overhead.\n\nW4. The related work is simply a list of existing methods. Please add a discussion of the differences between this work and previous work to further clarify the contribution of this paper."
                },
                "questions": {
                    "value": "Please see the Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4997/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4997/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4997/Reviewer_tSL5"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4997/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698723844233,
            "cdate": 1698723844233,
            "tmdate": 1700638411435,
            "mdate": 1700638411435,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rDqann67Pw",
                "forum": "V3j5d0GQgH",
                "replyto": "yvLDlkPzKY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4997/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4997/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response for Reviewer tSL5 [1/2]"
                    },
                    "comment": {
                        "value": "Thank you for providing suggestions that have helped in refining and improving our work from various perspectives. \n\n> W1. Convergence analysis is missing, which is very important for the optimization process of federated learning. Please analyze it from the experimental and theoretical point of view. \n\nWe thank the reviewer for the thoughtful suggestion. We agree that the theoretical analysis would benefit the significance of the proposed method. However, given the limited time for rebuttal, we could not include a complete convergence analysis in this response. Drawing inspiration from neural collapse, a rich body of derivations about convergence proofs, based on the unconstrained features model and peel model, can be found in references [14, 15, 16, 17, 18, 19, 20, 21]. Based on this, we can extend these proofs to the federated framework and we shall provide the analysis in our revised supplementary material.\n\n\nWhile we are currently unable to provide formal proof of convergence, we have conducted experimental investigations into the convergence. Specifically, we explored the number of rounds required to reach an accuracy of 0.33 on CIFAR100 with an imbalance factor (IF) of 100 and alpha=0.5, as documented in the table below. It is evident that the convergence rate of FedLoGe is almost on par with FedAvg and surpasses the current state-of-the-art (SOTA) methods.\n\n| Methods | FedETF  | FedROD | FedLoGe (Ours) |\n| --- | --- | --- | --- |\n| Convergence Round | 83 | 68 | 62 |\n\n> W2. Lack of discussion about privacy. Federated learning is proposed to protect the privacy of the client, but the method in this paper has the risk of gradient disclosure, please add the discussion of privacy in this work.\n\nThank you for highlighting the importance of discussing privacy aspects in our work. **In the entire training process of FedLoGe, no extra information compared with FedAvg is transmitted across clients and the server:** 1. During the 'Representation Learning with SSE-C' phase, FedLoGe transmits local model weights as FedAvg necessitates. 2. In the 'Feature Realignment' phase, the local models are already downloaded for norm analysis, and there are no further information-sharing operations required.\n\nAs the norm analysis is performed on the client side or global side, there would still be a few privacy concerns. However, noteworthy that the potential privacy issue exists in the general FL frameworks rather than specific to our proposed Global and Local Adaptive Feature Realignment (GLA-FR). For instance, gradient inversion[1] can pose a threat to almost all gradient transmission-based FL methods without any privacy-preservation techniques.  Additional defense techniques such as Differential Privacy [22] and Homomorphic Encryption [23] could be considered to further enhance the performance of our work. As the privacy issue of FL framework is beyond the scope of this work, **we have included the discussion about privacy in the paper A.10, marked in red for clarity.**\n\n> W3. The communication overhead of the model seems to be very large, which will restrict the practical application. Please increase the experiment and analysis of the communication overhead.\n\n**In fact, FedLoGe ensures minimal communication overhead, as it does not require the transfer of any additional information beyond what is already exchanged in FedAvg.**\n\nFor FedLoGe's computational cost, please refer to the experimental analysis presented in **Appendix A.3**. During the backbone training phase, using frozen $\\psi_{SSE-C}$ means there\u2019s no need to compute gradients in the fixed dense linear layer. Furthermore, feature realignment is a one-shot operation that requires only basic arithmetic computations. We conducted feature realignment (GLA-FR) for 40 clients and one global model on an NVIDIA GeForce RTX 3090, which took only 1.876e-2 seconds. Additionally, we assessed the computational expense of a single round of FedLoGe training on the PyTorch platform. The results presented in the table below show that FedLoGe is a lightweight algorithm like FedAvg and FedBABU.\n\n| Method | FedAvg | FedProx | FedRep | FedBABU | FedLoGe |\n| --- | --- | --- | --- | --- | --- |\n| Time Cost/Round | 3min11s | 5min24s | 4min07s | 3min14s | 3min19s |\n\n\n> W4. The related work is simply a list of existing methods. Please add a discussion of the differences between this work and previous work to further clarify the contribution of this paper.\n\nThank you for pointing out the need for a more detailed discussion of related work comparisons. Your feedback will help us clarify the distinct contributions of our paper and **we have updated it in A.11** in red in the paper. To the best of our knowledge, w**e are the first to achieve joint global and local training in Federated long-tailed learning from the perspective of neural collapse.** In the introduction, we demonstrated the improvements of FedLoGe over some existing methods, and here we further provide the discussion:"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4997/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700222887303,
                "cdate": 1700222887303,
                "tmdate": 1700409141978,
                "mdate": 1700409141978,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "O4DZpozAE1",
                "forum": "V3j5d0GQgH",
                "replyto": "yvLDlkPzKY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4997/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4997/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response for Reviewer tSL5 [2/2]"
                    },
                    "comment": {
                        "value": "1. Most existing methods in personalized federated learning [2, 3, 4, 5, 6] utilize the global model to regularize or construct personalized models but **do not evaluate them in a generic setup**, failing to yield a strong global model without long-tail bias. This makes them unsuitable as a strong incentive for attracting new clients. In contrast, our approach learns models that **excel in both personalized (LA-FR) and generic (GA-FR) setups** without compromising either.\n2. Existing methods in federated long-tailed learning [7, 8, 9, 10, 11], focusing only on calibrating long-tail bias for a single global model, **overlook the aspect of representation learning (backbone) under federated long-tailed settings** and fail to provide personalized models. For instance, CreFF involves post-classifier retraining but does not advance in boosting the backbone and does not offer personalized models.  In contrast, Our FedLoGe **achieves strong representation learning with SSE-C** to support both global model and local models.\n3. Existing work involving fixed classifiers has not delved deeply into the analysis of the initialization of these fixed classifiers. FedBABU[12], for instance, merely employs a randomly initialized fixed linear layer. For FedETF[13], the vallina fixed ETF classifier has **suffered from feature degeneration**, characterized by significant fluctuations between features and the class mean, while **FedLoGo alleviates this issue** by filtering out noisy smaller mean features (Fig. 1), making features more concise and effective (Fig. 3 (b, c) and Fig. 6). Besides, **FedLoGe is less computationally expensive and easier to adapt to existing algorithms.** DR loss is necessary for FedETF, which means some modifications are required when integrating with existing algorithms. Besides, FedETF needs a projection layer for optimization. For ResNet18 with four blocks, a 512x512 projection layer would be approximately 1MB, almost doubling the parameter size of Block 1, which is 0.56MB.\n\n[1] Evaluating gradient inversion attacks and defenses in federated learning. NIPS 2021.\n\n[2] Exploiting shared representations for personalzied federated Learning. PMLR 2021.\n\n[3] Ditto: Fair and robust federated learning through personalization. ICML 2021.\n\n[4] Personalized federated learning with moreau envelopes. NIPS 2020.\n\n[5] Personalized federated learning via variational bayesian inference. ICML 2022.\n\n[6] Dispfl: Towards communication-efficient personalized federated learning via decentralized sparse training. PMLR 2022.\n\n[7] Federated learning on heterogeneous and long-tailed data via classifier re-training with federated features. IJCAI 2022. \n\n[8] Integrating local real data with global gradient prototypes for classifier re-balancing in federated long-tailed learning. Arvix 2023.\n\n[9] Fedic: Federated learning on noniid and long-tailed data via calibrated distillation. IEEE ICME 2022.\n\n[10] Logit calibration for non-iid and long-tailed data in federated learning. IEEE ISPA/BDCloud/SocialCom/SustainCom 2022.\n\n[11] Long-tailed federated learning via aggregated meta mapping. IEEE ICIP.\n\n[12] FedBABU: Toward Enhanced Representation for Federated Image Classification ICLR 2021.\n\n[13] No fear of classifier biases: neural collapse inspired federated learning with synthetic and fixed classifier. CVPR 2023.\n\n[14] Prevalence of neural collapse during the terminal phase of deep learning training. PNAS 2020.\n\n[15] Neural collapse in deep linear network: From balanced to imbalanced data. ICML 2023.\n\n[16] Neural Collapse: A Review on Modelling Principles and Generalization. TMLR 2023.\n\n[17] Neural collapse with unconstrained features. Arvix 2020.\n\n[18] Neural collapse with normalized features: A geometric analysis over the Riemannian Manifold. NIPS 2022.\n\n[19] Exploring deep neural networks via layer-peeled model: Minority collapse in imbalanced training. PNAS 2021.\n\n[20] Fix your features: Stationary and maximally discriminative embeddings using regular polytope. Arvix 2019.\n\n[21] A geometric analysis of neural collapse with unconstrained features. NIPS 2021.\n\n[22] On privacy and personalization in cross-silo federated learning. NIPS 2022.\n\n[23] Privacy preserving machine learning with homomorphic encryption and federated learning. Future Internet 2021."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4997/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700222913842,
                "cdate": 1700222913842,
                "tmdate": 1700281370520,
                "mdate": 1700281370520,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "69jM6Odtyf",
                "forum": "V3j5d0GQgH",
                "replyto": "O4DZpozAE1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4997/Reviewer_tSL5"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4997/Reviewer_tSL5"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the rebuttal. Most of my concerns are addressed. Although my question about convergence analysis (W1) was not well answered, I  would raise my score to 6. Please incorporate the follow-up update into your paper."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4997/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700638370647,
                "cdate": 1700638370647,
                "tmdate": 1700638370647,
                "mdate": 1700638370647,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "acsSMjGE9o",
            "forum": "V3j5d0GQgH",
            "replyto": "V3j5d0GQgH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4997/Reviewer_YHb3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4997/Reviewer_YHb3"
            ],
            "content": {
                "summary": {
                    "value": "The paper considers federated learning with long-tailed global distribution and non-iid client distributions. the authors propose a Federated Local and Generic Model Training (FedLoGe) to train a model that can perform well both on long-tailed global distribution and local client distributions. First, the proposed Static Sparse Equiangular Tight Frame Classifier fosters the acquisition of potent data representations. Second, the Global and Local Adaptive Feature Realignment (GLA-FR) is used to align global features with client preferences."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* (originality) The authors proposed a novel method by neural collapse and realignment of local and global classifiers. Though neural collapse has been used for handling data heterogeneity, the realignment is novel to address the local-generic problem.\n* (significance) The proposed method can significantly outperform baselines in the concerned settings.\n* (clarity) The figure 2 is appreciated for clarifying the method.\n* (quality) The experiments are well designed for the problem setting. Multiple baselines are compared to demonstrate the effectiveness of the proposed method both in boosting local and generic performance."
                },
                "weaknesses": {
                    "value": "* (quality) The generic-local setting looks self-contradictive to me. When the personalized federated learning targets training models personalized for each client, is it necessary to consider a model adapted for global or generic distribution? In personalized federated learning, the assumption is that each client has its own data distribution and it aims to learn a model adapted for the specific distribution. If the global or generic distribution is considered, which client will be expected to use the model?\n* (significance) Because of my concerns about the problem setting, I am afraid that the proposed method may have a limited impact on a few specific problems.\n* (clarity) The technical motivation is not clear. The neural collapse has been used for handling data heterogeneity and classifiers should be fixed by the neural collapse theorem. It is unclear why the realignment is still needed. Why the fixed classifier cannot be used for all clients?\n* (clarity) The paper is hard to follow. Specifically, the motivation experiments in the introduction are hard to understand. Why the mean and variance of class means are evaluated in Figure 1? I cannot follow the logic in the discussion quoted below.\n> However, preliminary experiments benchmarking ETF with CIFAR-100 in Fed-LT suggest that only a few features have relatively large means, while most of the small-mean features are contaminated by severe noise, as shown in Fig. 1(a). Such observations are inconsistent with the feature collapse property, and we coin it as feature degeneration.\n\n  How do the mean and variance of features relate to the feature collapse?"
                },
                "questions": {
                    "value": "* Please provide clear motivation for the generic-local setting. When the personalized federated learning targets training models personalized for each client, is it necessary to consider a model adapted for global or generic distribution? In personalized federated learning, the assumption is that each client has its own data distribution and it aims to learn a model adapted for the specific distribution. If the global or generic distribution is considered, which client will be expected to use the model?\n* How do the mean and variance of features relate to the feature collapse in the quoted content?\n> However, preliminary experiments benchmarking ETF with CIFAR-100 in Fed-LT suggest that only a few features have relatively large means, while most of the small-mean features are contaminated by severe noise, as shown in Fig. 1(a). Such observations are inconsistent with the feature collapse property, and we coin it as feature degeneration."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4997/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4997/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4997/Reviewer_YHb3"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4997/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698812825673,
            "cdate": 1698812825673,
            "tmdate": 1700622748653,
            "mdate": 1700622748653,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ga3a9R4yfj",
                "forum": "V3j5d0GQgH",
                "replyto": "acsSMjGE9o",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4997/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4997/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response for Reviewer YHb3 [1/2]:"
                    },
                    "comment": {
                        "value": "Thank you for your thoughtful review. I've carefully considered your points and respond to each as follows.\n\n> Q1. When the personalized federated learning targets training models personalized for each client, is it necessary to train a global model? Which client will be expected to use the model? Please provide clear motivation for the generic-local setting. \n\nThank you for raising this important question about the necessity and role of a global model in personalized federated learning. We shall outline our motivation for the generic-local setting as follows:\n\n1. **The global model plays a crucial role as it forms a synergistic and progressively enhancing relationship with the local models, leading to mutual improvements and advancements**. The global model becomes more robust and versatile by aggregating the learning outcomes of various local models, while the local models gain a better starting point from the updated global model, enabling them to learn the unique characteristics of their local data more effectively.\n2.  **A high-quality global model acts as a strong incentive for attracting new clients to join the federated training**. When new clients join the training, they can directly access the quick benefits of a good generic model and continuously share local knowledge in the following training. Otherwise, training from scratch could be a cost-intensive option. Without well trained global model, existing local clients are unwilling to share their own private personalized models with the new clients who make no contributions to the training process.\n3. **A high-quality global model supports faster local model adaptation**.  Taking into account scenarios where users opt to **purchase the global model for local adaptation**, rather than joining the federated training process. We conduct an experiment to investigate the local adaptation efficiency with a high-quality global model. Assuming 20 clients take a trained global model for local fast adaptation, we have documented the average accuracy of local models across various local adaptation epochs **with and without Global Adaptive Feature Realignment (GA-FR)**, as shown in the table below. It shows that employing GA-FR can achieve better performance in the same training epochs.\n4. **Other personalized federated learning papers have also considered the generic-local setting [1, 2]**, though they do not specifically designed for long-tailed and imbalanced data.\n\n|  | epoch 15 | epoch 20 | epoch 25 |\n| --- | --- | --- | --- |\n| Vallina Global Model | 0.6117 | 0.6331 | 0.6441 |\n| Global Model with GA-FR | 0.6544 | 0.6606 | 0.6642 |\n\n\n> Q2. Given my concerns about the problem setting, could it be that the proposed method will have limited impact, affecting only a few specific problems?\n\nThank you for your concern regarding the potential scope of impact of our proposed method. **Data privacy and long-tailed distribution are more common than exceptional in many real-world scenarios, and FedLoGe is applicable in these situations.** As more clients join the training process, FedLoGe not only offers excellent personalized solutions but also provides a superior global model that attracts even more clients, thereby creating a positive feedback loop.\n\nFor instance, patients\u2019 diagnosis varies substantially across medical centers but collaboratively form long-tailed distributions[3, 4]. FedLoGe can provide personalized models for specific local diagnostics, while also offering a high-quality global model that incentivizes other medical centers to participate and contribute to the training process. \n\n\n> Q3. Why the realignment is still needed. Why the fixed classifier cannot be used for all clients?\n\n**The realignment is still needed because a single fixed classifier is not suitable for all clients with diverse local data**. The fixed classifier is employed to train a robust backbone, while realignment further customizes the classifier, tailoring the model for specific local data. As illustrated in the table below, we experimented with various classifiers and the results clearly show that Local Adaptive Feature Realignment (LA-FR) is most effective.\n\nIn the table, $\\psi$ denotes the global classifier and $\\phi_k$ denotes the classifier of the $k^{th}$ client. We derive the final prediction head for the $k^{th}$ client $\\phi_k^{\\prime}$ combine with $\\phi_k$  and $\\psi$ with $\\phi'_{k,c}=\\psi_c$\n\n$*\\left\\|\\phi_{k,c}\\right\\|_2$.\n\n| $\\phi_k$ | $\\psi$ | $\\psi_{SSE-C}$ | $\\phi_k^{\\prime}$ |\n| --- | --- | --- | --- |\n| 0.6986 | 0.7128 | 0.7146 | 0.7341 |\n\nFurthermore, the table also indicates that only relying on a personalized classifier does not yield optimal performance either. The local head, lacking global knowledge, tends to overfit to local data. In contrast, the combination of global and local heads, as implemented with our LA-FR, can leverage both the extensive global knowledge and the specific data statistics of the local context."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4997/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700222362986,
                "cdate": 1700222362986,
                "tmdate": 1700282951620,
                "mdate": 1700282951620,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dmykq9SNmY",
                "forum": "V3j5d0GQgH",
                "replyto": "acsSMjGE9o",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4997/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4997/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response for Reviewer YHb3 [2/2]"
                    },
                    "comment": {
                        "value": "More discussion on the motivation for using the integrated personalized classifier for each client can be found in the response of Reviewer Nmh4, Q2.\n\n\n> Q4. Why the mean and variance of class means are evaluated in Figure 1? How do the mean and variance of features relate to the feature collapse in the quoted content? \n\nThanks for the questions. The objective of analyzing variance and means of features is to examine whether the properties revealed by neural collapse theory are maintained when using a fixed ETF classifier.\n\n**Relations bewteen feature collapse with mean and variance of features:** Neural collapse reveals a principle, namely feature collapse, which shows that in a properly trained network, features should effectively collapse into the class mean (**where all relative variances in Fig. 1 (a) are expected to be zero)**. Yet, when trained with a vallina fixed ETF classifier, **features, particularly those with smaller means, do not collapse effectively, namely feature degeneration. FedLoGe is designed to mitigate feature degeneration by squeezing poor-quality and noisy features into these sparsely allocated positions.** Overall, as shown in Fig. 1, after training with $\\psi_{SSE-C}$, FedLoGe inherently filters out noisy smaller mean features, making features more concise (Appendix A.4) and effective (Table 3 in the main paper), in which the variance of dominant features becomes smaller. \n\n**Main changes between Fig. 1 (a) and (b):** Compared to Fig. 1 (a), **two significant changes** are evident in Fig. 1 (b) after implementing FedLoGe: **1. Noisy features are masked.** The small mean features with bigger variances are located in the sparsely allocated area, marked by grey vertical shadows. 2. **The quality of dominant features is further enhanced**. This is evidenced by the decreased variance in features with larger means, indicating they have become more concise and effective.\n\nWe give the detailed computation process for the data in Fig. 1:\nGiven that $n_c$ is the sample number of class $c$.  $\\mu_c$ is the class mean of class $c$, and $h_{i,c}$ is the feature of class $c$ on sample $i$, the features $h_{i,c}$ will collapse to the within-class mean $\\mu_c = \\frac{1}{n_c} \\sum_{i=1}^{n_c}h_{i,c}$. This indicates that the covariance $\\frac{1}{n_c} \\sum_{i=1}^{n_c}(h_{i,c}-\\mu_c)(h_{i,c}-\\mu_c)^T$ will converge to $0$. We investigate whether $h_{i,c} - \\mu_c$ tends toward zero when training with a fixed ETF classifier, as shown in Fig. 1 (a). \n\nThe steps to obtain the data in Fig. 1 are\uff1a\n\n1. Select class $c$.\n2. During global test, compute all sample features $h_{i,c}$, with dimension $D$, representing the number of features. Use $h_{i,c}[d]$ to denote the $d^{th}$ feature in $h_{i,c}$.\n\n 3.  Compute the mean and relative variance of $h_{c}$, sorting them in descending order of the mean, and accordingly adjust the variance positions.\n\nIt becomes evident that not all features, $h_{i,c}[d] - \\mu_c[d]$, converge to zero, and smaller $\\mu_c[d]$ values exhibit greater variance.\n\nThe process illustrated in **Fig.1 (b)** follows the same steps as **Fig.1 (a)**, but with grey vertical shadows for the sparsely positioned features. It can be observed that sparse ETF training effectively squeezes poor-quality and noisy features into these sparsely allocated positions.\n\n**All the calculation details have been updated in the revised appendix A.2 marked in red for clarity.**\n\n[1] On Bridging Generic and Personalized Federated Learning for Image Classification. ICLR 2022.\n\n[2] Spectral Co-Distillation for Personalized Federated Learning. NeurIPS 2023.\n\n[3] lluminating the dark spaces of healthcare with ambient intelligence. Nature 2020.\n\n[4] Gender\u00a0imbalance\u00a0in\u00a0medical\u00a0imaging datasets produces biased classifiers for computer-aided diagnosis. PNAS 2020."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4997/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700222446475,
                "cdate": 1700222446475,
                "tmdate": 1700281408923,
                "mdate": 1700281408923,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "w4MINy4swh",
                "forum": "V3j5d0GQgH",
                "replyto": "acsSMjGE9o",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4997/Reviewer_YHb3"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4997/Reviewer_YHb3"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the rebuttal.\n\nQ1. Motivation for the generic-local setting\n\nI agree that a high-quality global model could speed up convergence and incentivize new users. But the problem is **how to define the quality of global models**. Currently, the quality is defined as the performance of the global model on global data distribution. However, I doubt that good performance on long-tailed global distribution could be preferred for new users or any users on the tail. Intuitively, tail (new) users prefer a global model that can work well (out-of-box) on their own distribution. Obviously, performance on global distribution can not provide such attraction.\n\nIn contrast, good local model performance for all users with a low variance is a better quality measure. New users would love to see all users gain an advantage, and so will themselves.\n\nQ2: Limited impact due to my concern in Q1.\n\nAs I am still concerned about the motivation for a generic-local setting, I keep my concern for the impact.\n\nQ3: Why the realignment is still needed.\n\nThe result is convincing for using a mixture of local and generic classifiers.\n\nPlus, I am slightly confused about why the generic-local intuition conflicts with the Neural Collapse (NC)? According to NC, shouldn't we use a fixed classifier for all clients? As the authors emphasize a lot on NC as an intuition, the conflict should be explained more intuitively.\n\nQ4. Why the mean and variance of class means are evaluated in Figure 1? \n\nThanks for the updates, which is clearer now."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4997/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700612890737,
                "cdate": 1700612890737,
                "tmdate": 1700613558853,
                "mdate": 1700613558853,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "p806aLcoF6",
                "forum": "V3j5d0GQgH",
                "replyto": "csl4wbNfmd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4997/Reviewer_YHb3"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4997/Reviewer_YHb3"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the responses. All of my concerns have been addressed. \n\nPlease incorporate the clarifications into your paper. Could you highlight the revised content to reflect the clarification on our discussed problems?"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4997/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700634282461,
                "cdate": 1700634282461,
                "tmdate": 1700634282461,
                "mdate": 1700634282461,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "AC3bPog1ya",
                "forum": "V3j5d0GQgH",
                "replyto": "acsSMjGE9o",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4997/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4997/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Further Response for Reviewer YHb3"
                    },
                    "comment": {
                        "value": "Thank you for your feedback. We will incorporate the requested clarifications into our paper. Thank you again for your valuable comments."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4997/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700634740344,
                "cdate": 1700634740344,
                "tmdate": 1700634796740,
                "mdate": 1700634796740,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "LjvBCJ1Fk1",
            "forum": "V3j5d0GQgH",
            "replyto": "V3j5d0GQgH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4997/Reviewer_Nmh4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4997/Reviewer_Nmh4"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses Federated Learning with global long-tailedness. The proposed method is inspired by the neural collapse idea and contains three steps: 1) global representation learning with SSE-C classification head, and 2) & 3) global and local feature realignment.  In step 1, a classifier head is learned with sparsity while maintaining the Equiangular Tight Frame (ETF) properties, which is then broadcast to clients for learning backbone $\\theta$ and personalized classifier head $\\psi$. In step 2 & 3, the backbone $\\theta$ and global classifier $\\psi$ and local classifier $\\phi$ is respectively updated and realigned. Empirical results on a few benchmarks show the effectiveness of the proposed approach over prior work on data heterogeneous FL or Long-tailed FL."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "\\+ This paper tackles a crucial and intriguing problem in federated learning with global long-tailedness.\n\n\\+ The key idea of using ETF to build a classifier as regularized consensus is well-motivated.\n\n\\+ Experiments and ablation studies are well-designed and comprehensively conducted."
                },
                "weaknesses": {
                    "value": "\\- Too many technical details are missing. For example, it is vague to me why performing personal classifier realignment in Eq (7) helps in tackling local data heterogeneity. \n\n\\- Authors need to provide detailed motivation of each step in the proposed algorithm, such as the motivation of keeping two classifier heads for clients: $\\phi_k$ and $\\psi_k$, and realignment of the global classifier $\\psi$ in Eq 6.\n\n\\- Writing: Certain citation formats need to be fixed; Figure 1 is difficult to comprehend.\n\n\\- Missing discussion and comparison with related work on long-tailed FL. Authors should specify the technical contributions of the proposed method over other prior work, especially FedETF, which shows much resemblance to the proposed work."
                },
                "questions": {
                    "value": "How to optimize Eq (4) that contains a maximization over $j$? \n\nWhat is the purpose of introducing sparsity to the SSE-C classifier, besides its empirical effectiveness?\n\nWhy do we need two classifier heads  $\\phi_k$ and $\\psi_k$ for each of the clients $k$ instead of just one?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4997/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699462621395,
            "cdate": 1699462621395,
            "tmdate": 1699636487276,
            "mdate": 1699636487276,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "cvMuC9mWdM",
                "forum": "V3j5d0GQgH",
                "replyto": "LjvBCJ1Fk1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4997/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4997/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response for Reviewer Nmh4 [1/3]:"
                    },
                    "comment": {
                        "value": "Thank you for your insightful feedback. I am pleased to address each of your points in the following responses.\n\n> Q1. What is the detailed motivation of each step in the proposed algorithm, such as 1. why does performing personal classifier realignment in Eq. 7 help in tackling local data heterogeneity? 2. what\u2019s the motivation of keeping two classifier heads for clients and the realignment of the global classifier\u00a0in Eq. 6?\n\nThank you for your insightful questions regarding the detailed motivations behind the steps in our proposed algorithm. I am pleased to explain the motivations point by point. FedLoGe achieves joint training of generic and local models by decoupling representation learning (backbone) and classifier learning [1, 2] in sequential order:\n\nStep 1: During the backbone training phase, we employ the fixed SSE-C classifier to address the divergence caused by data heterogeneity. A well-trained shared backbone is essential, as it lays the foundation for enhancing the potential of both the global model and local models.\n\nStep 2: In the feature realignment stage, Global and Local Adaptive Feature Realignment (GLA-FR) focuses on aligning the model's capabilities with the corresponding data distributions. Building on a robust backbone, we further customize the classifiers to fit corresponding test data. The global model is designed for fairly treating all classes (GA-LR), **highlighted in line 8 of the Revised Algorithm**. Meanwhile, the personalized models are aligned with the local statistics **(LA-FR, Eq. 7)**, seen in **Revised Algorithm, line 11.**\n\nSpecifically, we explain the motivation and objective of the feature alignment of FedLoGe. **The objective of feature realignment (FR) is to address challenges brought by the heterogeneous and imbalanced data.** In the context of long-tailed learning, it is commonly acknowledged that the classifier's class vector norm positively correlates with its predictive power and the proportion of that class's samples in the overall training set[2, 3, 4, 5, 6, 7]. In Fed-LT, the long-tailed data would lead to a highly imbalanced classifier throughout the training process. Therefore, we use FR to re-balance the norm of the classifier for a balanced representation learning with heterogeneous global/local data.\n\nGA-FR is effective because it aligns the long-tailed norm to the balanced distribution caused by a long-tailed training set, thus balancing the predictive abilities of all classifiers. Due to the inconsistency of local and global data distribution, GA-FR fails to fit the local statistic. **We design LA-FR, which adapts the global classifier to fit the local statistics by local classifier norms.** We have provided a detailed explanation of keeping two classifier heads in response to Question 2.\n\n> Q2. Why do we need two classifier heads?\u00a0\u00a0Global $\\psi$ and local $\\phi_k$ for each of the clients\u00a0$k$\u00a0instead of just one ($\\psi$ or $\\phi_k$)?\n\nNotations:\n$\\psi$ denotes global classifier and $\\phi_k$ denotes the classifier of the $k^{th}$ client. We derive the final prediction head for the $k^{th}$ client $\\phi_k^{\\prime}$ combine with $\\phi_k$ and $\\psi$ with $\\phi'_{k,c}=\\psi_c*$\n\n$\\left\\|\\phi_{k,c}\\right\\|_2$. \n\nThanks for your questions about local classifier design. We need to clarify that the reason why we do not use global $\\psi$ for each client is due to a single classifier is not suitable for clients with diverse local data. Besides, a single local head, due to its absence of global knowledge, is prone to overfitting to local data.\n**The integration/combination of the global and local heads leverages both the comprehensive knowledge from the global perspective and the data statistics specific to the local context.**\n\nThe collaboration of global $\\psi$ and local $\\phi_k$ could be regarded as knowledge transfer: **transferring** global generalized knowledge with the norm of local $\\phi_k$ **to** the new integrated local personalized head (The classifier's class vector norm positively correlates with its predictive power and the proportion of that class's samples in the overall training set. See Reviewer YQxK, Question 5 for details.).\n\nThe visualization in Figure 2, particularly the sections illustrating 'transfer' and 'decline', highlights that local models prioritize classifiers for classes more common in their local data, thereby improving accuracy for these relevant classes.\n\nAn extreme example is that for a class $c$ not present in a local model, it is reasonable to completely obliterate the predictive capability of classifier $c$. This corresponds to the combination process $\\mathbf{\\phi}^{\\prime}_{k,c}$\n\n$=\\mathbf{\\psi}_{c} $\n\n$* \\left\\| \\mathbf{\\phi}_{k,c} \\right\\|_2 $\n\n$= \\mathbf{\\psi}_{c} * 0 = 0$. \n\nWe further experimented to explore the performance of different schemes for client $k$ as illustrated in the following table. It show that scheme 3, $\\phi'_{k,c}=$\n\n$\\psi_c*\\left\\|\\phi_{k,c}\\right\\|_2$, yields the most beneficial result."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4997/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700221825557,
                "cdate": 1700221825557,
                "tmdate": 1700278025536,
                "mdate": 1700278025536,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UTlbOoll7y",
                "forum": "V3j5d0GQgH",
                "replyto": "LjvBCJ1Fk1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4997/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4997/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response for Reviewer Nmh4 [2/3]:"
                    },
                    "comment": {
                        "value": "| 1. $\\psi$ | 2. $\\phi_k$ | 3. $\\phi'_{k,c}$ | 4. $\\phi'_{k,c} + \\phi_k$ | 5. $\\psi_{SSE-C}$ |\n| --------- | ----------- | ---------------- | ------------------------- | ----------------- |\n| 0.7128    | 0.6986      | 0.7341           | 0.7321                    | 0.7146            |\n\n> Q3. Fig. 1 is difficult to comprehend.\n\nThanks for the question and we are glad to clarify what Fig. 1 implies. Compared to Fig. 1 (a), **two significant changes** are evident in Fig. 1 (b) after implementing FedLoGe: **1. Noisy features are masked.** The small mean features with bigger variances are located in the sparsely allocated area, marked by grey vertical shadows. 2. **The quality of dominant features is further enhanced**. This is evidenced by the decreased variance in features with larger means, indicating they have become more concise and effective.\n\nBelow is a detailed interpretation of Fig. 1:\n\nNeural Collapse (NC) theory highlights a key characteristic of successful neural network training, which is the convergence of all sample features (outputs of the backbone) within the same category to the class mean of that category.\n**Yet, when trained with a vallina fixed ETF classifier (Fig. 1 (a)), features with smaller means do not collapse effectively, namely feature degeneration. FedLoGe(Fig. 1 (b)) is designed to mitigate feature degeneration by squeezing poor-quality and noisy features into these sparsely allocated positions.**\n\nOverall, as shown in Fig. 1, after training with $\\psi_{SSE-C}$, FedLoGe inherently filters out noisy smaller mean features, making features more concise (Appendix A.4) and effective (Table 3 in the main paper), in which the variance of dominant features becomes smaller.\n\nWe give the detailed computation process for the data in Fig. 1:\nGiven that $n_c$ is the sample number of class $c$. $\\mu_c$ is the class mean of class $c$, and $h_{i,c}$ is the feature of class $c$ on sample $i$, the features $h_{i,c}$ will collapse to the within-class mean $\\mu_c = \\frac{1}{n_c} \\sum_{i=1}^{n_c}h_{i,c}$. This indicates that the covariance $\\frac{1}{n_c} \\sum_{i=1}^{n_c}(h_{i,c}-\\mu_c)(h_{i,c}-\\mu_c)^T$ will converge to $0$. We investigate whether $h_{i,c} - \\mu_c$ tends toward zero when training with a fixed ETF classifier, as shown in Fig. 1 (a).\n\nThe steps to obtain the data in Fig. 1 are\uff1a\n\n1. Select class $c$.\n2. During global test, compute all sample features $h_{i,c}$, with dimension $D$, representing the number of features. Use $h_{i,c}[d]$ to denote the $d^{th}$ feature in $h_{i,c}$.\n\n3. Compute the mean and relative variance of $h_{c}$, sorting them in descending order of the mean, and accordingly adjust the variance positions.\n\nIt becomes evident that not all features, $h_{i,c}[d] - \\mu_c[d]$, converge to zero, and smaller $\\mu_c[d]$ values exhibit greater variance.\n\nThe process illustrated in **Fig.1 (b)** follows the same steps as **Fig.1 (a)**, but with grey vertical shadows for the sparsely positioned features. It can be observed that sparse ETF training effectively squeezes poor-quality and noisy features into these sparsely allocated positions.\n\n**All the calculation details have been updated in the revised appendix A.2 marked in red.**\n\n> Q4. Comparison with related work on long-tailed FL, specify the technical contributions of the proposed method over other prior work, especially FedETF, which shows much resemblance to the proposed work.\n\nOther long-tailed FL approaches [8, 9, 10, 11] focus on a single global model, overlooking personalized client needs. In contrast, FedLoGe is capable of simultaneously training both generic and local models. Notably, FedLoGe integrates representation learning and feature realignment under the unified framework of neural collapse.\nBelow are the detailed differences between FedLoGe and FedETF:\n\n1. FedETF is only designed to optimize a **single global** model, while our FedLoGe optimizes **both global model and client models** which is more appealing in practice.\n2. The ETF classifier has been noted to **suffer from feature degeneration**, characterized by significant fluctuations between features and the class mean, while **FedLoGo alleviates this issue** by filtering out noisy smaller mean features, making features more concise and effective.\n3. **FedLoGe is less computationally expensive and easier to adapt to existing algorithms.** DR loss is necessary for FedETF, which means some modifications are required when integrating with existing algorithms. Besides, FedETF needs a projection layer for optimization. For ResNet18 with four blocks, a 512x512 projection layer would be approximately 1MB, almost doubling the parameter size of Block 1, which is 0.56MB. We conducted an ablation study on DR Loss and the Projection Layer, as shown in **the table below**. The results demonstrate that FedLoGe can achieve good performance even without the DR Loss and Projection Layer."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4997/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700222097020,
                "cdate": 1700222097020,
                "tmdate": 1700278046137,
                "mdate": 1700278046137,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LhMB6lqPb0",
                "forum": "V3j5d0GQgH",
                "replyto": "LjvBCJ1Fk1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4997/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4997/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response for Reviewer Nmh4 [3/3]:"
                    },
                    "comment": {
                        "value": "| Method  | DR Loss | Projection Layer | Many   | Medium | Few    | All    |\n| ------- | ------- | ---------------- | ------ | ------ | ------ | ------ |\n| CreFF   | -       | -                | 0.684  | 0.440  | 0.146  | 0.401  |\n| FedGraB | -       | -                | 0.683  | 0.553  | 0.221  | 0.411  |\n| FedETF  | \u00d7       | \u00d7                | 0.6904 | 0.4751 | 0.1820 | 0.3825 |\n| FedETF  | \u221a       | \u00d7                | 0.6882 | 0.4607 | 0.2059 | 0.3932 |\n| FedETF  | \u221a       | \u221a                | 0.6909 | 0.5011 | 0.2310 | 0.4109 |\n| FedLoGe | \u00d7       | \u00d7                | 0.7137 | 0.4989 | 0.2179 | 0.4249 |\n\n**Please refer to the detailed discussion of the differences between our work and previous work in the responses to Reviewer tSL5 (W4), or in Section A.11 of our paper.**\n\n> Q5. How to optimize Eq (4) that contains a maximization over\u00a0j?\n\nThank you for your question regarding the details of Equation (4). We select two classifier vectors $\\mathbf{\\hat{\\mathbf{\\psi}}^{\\prime}}_{:,i}$ \n\nand $\\mathbf{\\hat{\\mathbf{\\psi}}^{\\prime}}_{:,j}$ $(i \\neq j)$ which have maximum cosine similarity. Then we transfer cosine similarity to degree and optimize it. The calculation steps for Maximal Angle Loss are shown in the code below:\n\n```python\n# Constraint 2: Maximize the angle between vectors (minimize cosine similarity)\nnormalized_etf = sparse_etf / row_norms\ncos_sim = torch.mm(normalized_etf.t(), normalized_etf)\ntorch.diagonal(cos_sim).fill_(-1)\nangle_loss = -torch.acos(cos_sim.max(dim=1)[0].clamp(-0.99999, 0.99999)).mean()\n```\n\nWe shall provide a more detailed explanation of the details of the optimization process.\n\nPlease access more details about the Maximal Angle Loss in **Supplementary Material** ( ./util/etf_methods.py).\n\nQ6. What is the purpose of introducing sparsity to the SSE-C classifier, besides its empirical effectiveness?\nThank you for your insightful inquiry about the role of sparsity in the SSE-C classifier. The introduction of sparsity serves several key purposes beyond its empirical effectiveness, as outlined below:\n\n1. Non-sparse ETF results in unstable distances between features with smaller samples means and the class means, while enforcing **sparsity would mitigate feature degeneration** by squeezing poor-quality and noisy features into these sparsely allocated positions. We demosntrate this in **Fig. 1.** Please check Q3 for more details.\n2. Introducing sparsity helps **learn more concise and dominant features** for better classification, which is evident in **Figures 3(b) and 3(c)**. It shows that after training with SSE-C, pruning dominant features drastically decreases performance, while pruning negligible features barely affects performance.\n3. Sparsity can help **learn more discriminant decision boundaries**. We have conducted a T-SNE visualization of the class means before and after sparsification, detailed in **Appendix A.4**. This visualization reveals that the decision boundaries of class mean post-sparsification are more distinct.\n\n[1] Bbn: Bilateral-branch network with cumulative learning for long-tailed visual recognition. CVPR 2022.\n\n[2] Decoupling representation and classifier for long-tailed recognition. ICLR 2020.\n\n[3] Equalization loss v2: A new gradient balance approach for long-tailed object detection. CVPR 2021.\n\n[4] Over-coming classifier imbalance for long-tail object detection with balanced group softmax. CVPR 2020.\n\n[5] Adjusting decision boundary for class imbalanced learning. IEEE Access 2020.\n\n[6] Neural collapse in deep linear network: From balanced to imbalanced data. ICML 2023.\n\n[7] Imbalance trouble: Revisiting neural-collapse geometry. NIPS 2022.\n\n[8] Federated Learning on Heterogeneous and Long-Tailed Data via Classifier Re-Training with Federated Features. IJCAI 2022.\n\n[9] Fedic:\u00a0Federated\u00a0learning on non-iid and long-tailed data via calibrated distillation. IEEE ICME 2022.\n\n[10] Fed-GraB:\u00a0Federated\u00a0Long-tailed Learning with Self-Adjusting Gradient Balancer. NIPS 2023.\n\n[11] BalanceFL: Addressing Class Imbalance in Long-Tail Federated Learning. IEEE IPSN 2022."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4997/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700222158998,
                "cdate": 1700222158998,
                "tmdate": 1700223595976,
                "mdate": 1700223595976,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Y1mpQilPNk",
                "forum": "V3j5d0GQgH",
                "replyto": "LjvBCJ1Fk1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4997/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4997/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Kind Reminder: Review Deadline Approaching"
                    },
                    "comment": {
                        "value": "Dear Reviewer Nmh4:\n\nAs the review deadline is nearing, with just five hours remaining, we wish to highlight our recent efforts in addressing your concerns. We've provided clear motivations for each algorithmic step, enhanced Fig. 1's clarity, conducted a thorough comparison with related work, and clarified intricate details of our algorithm design.\n\nWe trust these clarifications will assist in your re-evaluation. Your feedback is invaluable, and we thank you for your time and attention.\n\nWarm regards,\n\nAuthors"
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4997/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700722559381,
                "cdate": 1700722559381,
                "tmdate": 1700722559381,
                "mdate": 1700722559381,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]