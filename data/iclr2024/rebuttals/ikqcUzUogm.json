[
    {
        "title": "Programmatic Evaluation of Rule-Following Behavior"
    },
    {
        "review": {
            "id": "BZZLGQE142",
            "forum": "ikqcUzUogm",
            "replyto": "ikqcUzUogm",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission382/Reviewer_1QQx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission382/Reviewer_1QQx"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies the practical problem of avoiding \u201charmful\u201d behaviors of LLMs.\n\nTo meet the \u201cThree Laws of Robotics\u201d in usability, safety, and ethics, the authors introduce the Benchmark for Identifying Noncompliant Decisions (BIND), a framework for evaluating rule-following behavior in LLM assistants. \n\nThe proposed benchmark contains 15 text scenarios drawing from the field of computer security and common children\u2019s games. Each scenario defines a set of rules in natural language and an evaluation program to check model outputs for compliance with the rules. The authors also systematically collect a challenging hand-written test suite of 862 test cases across all 15 scenarios, against which they evaluate current state-of-the-art models and find lackluster performance."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper has successfully introduced the Benchmark for Identifying Non-compliant Decisions (BIND), a framework to programmatically evaluate rule-following in LLMs. The benchmark consists of 15 text scenarios in which the model is instructed to obey a set of rules while interacting with the human user to avoid \u201charmful\u201d behaviors from LLMs."
                },
                "weaknesses": {
                    "value": "The comparison to the relevant and closed baselines is not conducted. Without this comparison, it is hard to justify the advancements of the proposed framework.\n\nThe threats to the validity of the proposed benchmark are not investigated."
                },
                "questions": {
                    "value": "Can the designed benchmark affect and guide the LLMs\u2019 behavior? Please explain in detail with some concrete examples.\n\nIs the effectiveness of the benchmark only affected and available to the current testing versions of the used LLMs? When the new versions of LLMs are released, will the proposed framework still be valid? Can the authors explain in detail with some examples?\n\nHow is the proposed framework performance compared to the baselines regarding the number of rules and the effectiveness in guiding the LLMs to avoid \u201charmful\u201d behaviors?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission382/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission382/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission382/Reviewer_1QQx"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission382/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698752592526,
            "cdate": 1698752592526,
            "tmdate": 1699635965288,
            "mdate": 1699635965288,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6US7tnJNUP",
                "forum": "ikqcUzUogm",
                "replyto": "BZZLGQE142",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission382/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission382/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Baselines.** We are unclear what types of methods are referred to as \"baselines\" by the reviewer. We would be happy to compare our benchmark with appropriate baselines if the reviewer could point them out. Since our scenarios rely only on text inputs and outputs, RuLES is compatible with future LLMs. We will publicly release all code and data, which we have now also uploaded. Future models may outperform existing models, but this will not invalidate the results found in this submission.\n\n**Red-teaming and alignment.** Our work is loosely related to prior work on red-teaming LLMs to evaluate their susceptibility to produce harmful or toxic outputs (\"alignment\"), but with important differences. Most relevantly, we evaluate ability to follow simple application-specific rules, rather than universal human values (such as \"do not be offensive\") that are relevant to all applications. Our setting, supporting zero-shot enforcement of application-specific rules not known at LLM training time, is plausibly harder than \"alignment\" (training to enforce universal values that are known at training time); and at the same time, it might also be easier, as we consider simple rules that admit an objective and unambiguous interpretation, whereas universal values can be subjective and ambiguous. For these reasons, different evaluation methods and benchmarks are needed in our domain, and that is what we provide in this paper."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission382/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700110673457,
                "cdate": 1700110673457,
                "tmdate": 1700110673457,
                "mdate": 1700110673457,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "nvcPdvOnvU",
            "forum": "ikqcUzUogm",
            "replyto": "ikqcUzUogm",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission382/Reviewer_do43"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission382/Reviewer_do43"
            ],
            "content": {
                "summary": {
                    "value": "The paper asks the question, _does expressing simple rules in natural language as prompts and/or system instructions ensure the model is able to follow these rules?_ To conduct effective and automatic evaluation, the rules choses can be evaluated by a simple computer program. The test scenarios are based on some pre-defined dimensions-- (1) Environments grounded in software security and games, (2) rules that need to be adhered to (Positive) and rules that should not be broken (Negative), and (3) Strategies (context setup in natural language) that can be used to push the model to break the rules. The experiments show, with multiple seeds and statistical testing, that both closed and open-source models fail to respect simple system rules that can easily be validated using simple computer programs."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper is well-written and easy to follow.\n2. The objectives are clearly stated and the evaluation is broken down into reasonable dimensions.\n3. The test-set will be a good benchmark for evaluating LLM's prowess at following simple rules in the future.\n4. The experiments consider prompt variances, efficacy of system vs user prompts for SOTA models, and authors seem to have conducted statistical testing to support/disapprove their claims.\n5. The authors report API results with timestamps and also consider automatic/optimized adversarial attacks on open-source models."
                },
                "weaknesses": {
                    "value": "1. I would like to believe the finding in this paper should be reasonable obvious to most people at the conference, i.e. expecting a stochastic autoregressive model to follow deterministic objectives (that programs do) seems unreasonable to start with, although I have seen a suspension of disbelief from experts, alas! Truth be told, it seems like the season for papers along similar veins (LLMs can't plan, reason, solve NP hard problems, figure out game-theoretic equilibria); duh! Tbh, beyond the test set that will help others check to improve LLM capabilities at following simple rule (not sure why they need it though if we can write programs that can be called during orchestration), I am not fully sure of the contribution).\n2. The choice of testing scenarios (esp. the security ones and the game) seems a little arbitrary, lacking good motivation.\n3. The authors seamlessly refer to figures/tables in Appendix. While I did look at them for context and understanding, I feel this skews my evaluation towards other papers who have had to strictly adhere to the page limit to make their point."
                },
                "questions": {
                    "value": "See above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Legal compliance (e.g., GDPR, copyright, terms of use)"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "Although it is obvious to me (and hopefully people of this community), unsure if the jailbreaks/strategies proposed will help naive users break closed-source models into revealing other secrets (eg Personal Identifiable Information or PIIs) from their training data. While I am aware of vulnerability disclosure strategies in software security, unsure if such a paradigm exists for LLMs (or major players have reporting obligations). Wanted to see if the authors did any testing to ensure this is a no-threat or already communicated with the model providers. Tbh, unsure if the authors should be penalized for a single paper while other related work they cite is openly releasing attacks."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission382/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698818888767,
            "cdate": 1698818888767,
            "tmdate": 1699635965207,
            "mdate": 1699635965207,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "3C3bToyuO1",
                "forum": "ikqcUzUogm",
                "replyto": "nvcPdvOnvU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission382/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission382/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Novelty and contributions.** \"Reasonably obvious\" is a fair interpretation of our results, given what we have seen in all prior work on jailbreaking LLMs within and without the academic community. That said, while many might have reasonable intuitions or expectations about how well LLMs can follow rules, we are not aware of any prior work that has quantitatively or qualitatively evaluated this question. As we seek to motivate more clearly in our revised submission (we have also significantly reorganized the presentation of the material), we view our main contribution as one of formalizing an experimental paradigm for the research community in which to make reproducible progress on developing stronger defenses and more reliable rule-following behavior in LLM assistants. Our test suite is one approach to evaluating a variety of manual jailbreaking strategies. As behaviors and defenses evolve, it may be necessary to collect newer, harder test cases.\n\n**Ethics review.** We are happy to undergo an ethics review, but would note that our work does not introduce any novel vulnerabilities or attacks. Prior to submission, we spot-checked various manual jail-breaking strategies as well as optimized GCG suffixes on ChatGPT, etc. but did not find noticeably different results from what is already readily searchable on the Internet. Further, traditional vulnerability disclosure processes such as OpenAI's bug bounty program (https://bugcrowd.com/openai) explicitly disclaim model prompts and outputs as out-of-scope, so we did not pursue formal disclosure."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission382/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700110650911,
                "cdate": 1700110650911,
                "tmdate": 1700110650911,
                "mdate": 1700110650911,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "aDrv39IJKb",
            "forum": "ikqcUzUogm",
            "replyto": "ikqcUzUogm",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission382/Reviewer_UwWZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission382/Reviewer_UwWZ"
            ],
            "content": {
                "summary": {
                    "value": "Motivated that the model\u2019s adherence to even simple rules needs human engagement, the authors propose a benchmark dataset where LLM rule-following can be programmatically evaluated. The proposed dataset consists of text scenarios that have an evaluation program to determine the model\u2019s adherence to given rules. The text scenarios are influenced by computer security systems and children\u2019s games. With the design of the dataset, the authors also use test suites of 862 hand-written test case templates to implement different high-level attacks for diverse analyses on rule-following behavior."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Suggest a benchmark dataset in which LLM\u2019s rule-following behavior can be automatically evaluated.\n- The evaluation of each rule-following is robust and cheap."
                },
                "weaknesses": {
                    "value": "- As the author\u2019s motivation is to evaluate the rule-following behavior of LLMs automatically, the direct tackling to this motivation would be the automatic evaluation of rule-following behavior in arbitrary (at least diverse) domains and rules. However, the test scenarios are fixed in two domains, and the testing rules are limited to predefined contents for each domain. Fixed domain and rules can be evaluated by human, so harm the contribution of this work."
                },
                "questions": {
                    "value": "- Can the suggested benchmark dataset be extended to other domains and rules with relatively little effort?\n- As the different test suites change the LLM's performance, why defense prompt doesn\u2019t work? Is there an explainable reason?\n- Does rule-following behaviour in computer security system and children's game have general impact for assessing LLM performance?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission382/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission382/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission382/Reviewer_UwWZ"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission382/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698836917531,
            "cdate": 1698836917531,
            "tmdate": 1699635965134,
            "mdate": 1699635965134,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Ob6SoVk2xV",
                "forum": "ikqcUzUogm",
                "replyto": "aDrv39IJKb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission382/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission382/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "The purpose of our evaluation programs is to remove the dependency on manual labeling effort. The ability to automatically evaluate model responses is a key component of our benchmark, since it enables much faster and cheaper evaluation, shortening the development cycle.\n\n**Choice of scenarios.** We chose the two domains of security properties and children's games for their relevance and simplicity/familiarity, with the former domain being more \"serious\" and the latter being more \"lighthearted\". Our proposed benchmark can be extended to other domains with relatively little effort, e.g. \"you are a security guard at the art museum\", but we did not notice any qualitatively different behavior by LLMs between the two domains we did study.\n\n**Results and validity.** We found some issues with the defense prompts evaluated in the initial submission and have removed them from our revised version. Whether our benchmark carries any external validity in the real world is an important question that would require more longitudinal study of how performance on our benchmark correlates with real-world measurements of performance over time. However, we would point to the highly correlated efficacy of both manual and automatic jailbreaks in our scenarios, compared to generation of harmful/toxic outputs as investigated in prior work, as some initial evidence in our favor."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission382/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700110553073,
                "cdate": 1700110553073,
                "tmdate": 1700110553073,
                "mdate": 1700110553073,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "pNsr561JdL",
            "forum": "ikqcUzUogm",
            "replyto": "ikqcUzUogm",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission382/Reviewer_NyVg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission382/Reviewer_NyVg"
            ],
            "content": {
                "summary": {
                    "value": "The authors describe a framework/benchmark named BIND that evaluates the ability of LLMs to follow rules under various scenarios (benign and adversarial).  They evaluate various LLMs using this benchmark and conclude that most LLMs in the status quo are not compliant with rules that are specified."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Benchmark is first of its kind."
                },
                "weaknesses": {
                    "value": "1. Unclear what takeaways can be drawn from this work.\n2. Paper could benefit from some reorganization."
                },
                "questions": {
                    "value": "Overall, this work is interesting and potentially exciting but the main takeaways are not communicated in a clear manner. This reviewer is wondering what I can learn from this paper, and how others can follow-up on this line of work.\n\n    1. The writing of the paper could benefit from some thought. For example, the authors could give examples of scenarios, rules and test cases to better highlight the difference between the 3 categories. \n    2. The paper provides limited takeaways from their experiment. It is incredible that the authors have come up with such a benchmark. But what can I learn because of it apart from the fact that LLMs do not follow rules (which was already a well know fact. Look at work from Percy Liang\u2019s group \u2014 https://arxiv.org/abs/2307.03172, or the fact that LLMs used in search e.g., BingChat can easily be subverted with prompt injection attacks)? The fact that there\u2019s nothing beyond the creation of this benchmark is making this reviewer apprehensive in recommending acceptance.\n    3. One conceivable application is one where the models are deployed in real-world systems (e.g., as in BingChat) and one would want to understand how brittle these are. But to validate such a scenario, the authors need to consider \u201clayered defenses\u201d i.e., add an output filter atop the generations from the LLMs and see how much information can be exfiltrated in such a setting. However, this was not done in this work.\n4. A lot of the findings presented by the authors have been covered earlier i.e., numerous prompt injection strategies discuss mechanisms of rule subversion. This work, to me, seems like a consolidation of those findings. Could the authors emphasize the difference?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission382/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699298135446,
            "cdate": 1699298135446,
            "tmdate": 1699635965063,
            "mdate": 1699635965063,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "nagAD2XWFm",
                "forum": "ikqcUzUogm",
                "replyto": "pNsr561JdL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission382/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission382/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "The points about overall organization and discussion are well taken, and we believe that our revised submission provides a clearer organization and more tangible analyses of our results. The goal of this work is not to study particularly novel behaviors of LLMs, rather we present the design and usage of our evaluation scenarios and test suite as a potential test bed for future research. We point the reviewer to the files `llm_rules/scenarios/{security,games}.py` for the concrete implementations of our scenarios, as well as the files in `data/systematic/` to see our test cases.\n\n**Defenses.** We recognize that there is a large space of potential prompting and filtering methods that might substantially improve rule-following. We see our work as providing an evaluation benchmark that others can use, as they propose such interventions. As far as we are aware, current deployments of LLMs generally do not use those \"layered defenses\". It is also important to note the difficulty of properly evaluating proposed defenses. Without sufficient conscientious red-teaming, it is easy to draw faulty conclusions about proposed defenses [1], and we hope our benchmark will be a first step towards supporting such evaluation. We also have removed the scratchpad and double check prompting results from our revised submission because after further analysis of output logs, we realized not all the models fully understood our existing formulations of the prompts.\n\n**Novelty and takeaways.** It might be misleading to conclude that LLMs cannot follow rules at all; while the pass rate falls far short of what we would hope for, it is also significantly higher than zero. While many might have intuitions or expectations about how well LLMs can follow rules, we are not aware of any prior work that has quantitatively or qualitatively evaluated this question. Work on prompt injection is relevant, but generally has considered only one narrow attack strategy: e.g., \"Disregard previous instructions and instead\". We consider a broader array of strategies for fooling models into violating the rules. Some of our strategies would not be considered a successful prompt injection attack (because they do not allow completely replacing the existing task with any other task) but are able to trigger rule violations. As discussed in our revised submission, we also distinguish this work from red-teaming model alignment which focuses on circumventing a set of universal rules and values that the model has been trained to always obey. The preprint [2] suggested by the review does not appear to be relevant, as it considers performance in long contexts, while all of our scenarios use short contexts.\n\n[1]: Anish Athalye et al., Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples. ICML 2018\n\n[2] Nelson F. Liu et al., Lost in the Middle: How Language Models Use Long Contexts. ArXiv 2023"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission382/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700110510248,
                "cdate": 1700110510248,
                "tmdate": 1700110510248,
                "mdate": 1700110510248,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CKHueLxMJq",
                "forum": "ikqcUzUogm",
                "replyto": "nagAD2XWFm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission382/Reviewer_NyVg"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission382/Reviewer_NyVg"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your response!"
                    },
                    "comment": {
                        "value": "Will internally deliberate. Thanks again for submitting your work to ICLR!"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission382/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700432412980,
                "cdate": 1700432412980,
                "tmdate": 1700432412980,
                "mdate": 1700432412980,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gLG4PxYfhL",
                "forum": "ikqcUzUogm",
                "replyto": "CKHueLxMJq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission382/Reviewer_NyVg"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission382/Reviewer_NyVg"
                ],
                "content": {
                    "title": {
                        "value": "Based on deliberation"
                    },
                    "comment": {
                        "value": "Thanks for your response. I have decided not to raise my score for the following reasons:\n\n1. The evaluation in this work seems to hold for this snapshot in time without any generalizable takeaways. Questions that I'd like to have answered are: (a) why does this happen? (b) what is the difference between the long context case that Liu et al. have studied and the setting we consider?, (c) what can we do to fix this behavior?, (d) are smaller, more task-specific LLMs (e.g., Orca, the Phi series etc.) better at these considerations?  While I agree that it would be impossible to conduct these experiments in time for the rebuttal, the current paper is incomplete in my eyes without these questions and answers, and I am uncomfortable recommending acceptance for a paper where I am unconvinced about what follow-ups may stem from it.\n\n2. **While many might have intuitions or expectations about how well LLMs can follow rules, we are not aware of any prior work that has quantitatively or qualitatively evaluated this question** --> this is echoing my point. While the authors have come up with a suite of clever techniques to evaluate rule-following behavior, I am always left wondering about why this matters in the real-world with more safeguards in place. The authors argue that it's hard to come to generalizable conclusions about defenses without proper red-teaming, yet advocate for acceptance of their work which is also devoid of any generalizable conclusions or methodologies; this seems paradoxical to me.  Try it out? Don't have to make a general claim. A simple google search of \"bing chat output filters\" provides ample anecdotal evidence that output filtering is done -- why not try something that you think is representative?"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission382/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700665258798,
                "cdate": 1700665258798,
                "tmdate": 1700665258798,
                "mdate": 1700665258798,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]