[
    {
        "title": "IRGen: Generative Modeling for Image Retrieval"
    },
    {
        "review": {
            "id": "mzm4zMAZXd",
            "forum": "EMCXCTsmSx",
            "replyto": "EMCXCTsmSx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2547/Reviewer_rUYC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2547/Reviewer_rUYC"
            ],
            "content": {
                "summary": {
                    "value": "The paper poses image retrieval as a form of generative modelling in order to allow the use of common transformer architectures for search in an end-to-end fashion. The proposed IRGen gets a query image as input and outputs identifiers that correspond to the nearest neighbors of the query in a given image database. This effectively turns search into a problem solved by a large transformer encoder-decoder\ncombines the two classical steps of image retrieval (feature extraction and indexing/search) into one module, that has to be trained for a given database.\n\nThey introduce a \"semantic image tokenizer\" that transforms an image into a sequence of tokens by only taking into account a global image feature from the CLS token. They use resdual quantization and alternative optimization to"
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The paper deals with an interesting formulation of image indexing as a sequence problem over quantized representations.  A differentiable index is used and therefore one can finetune the model end to end. Despite there are a number of unclear parts regarding the papers actual contribution and evaluation protocol, the fact that this approach works well is interesting."
                },
                "weaknesses": {
                    "value": "1) The paper is written in a way that is hard to follow and understand, both the method and the contributions. Figure 1 is ambiguous or wrong.\n\n2) A lot of relevant work is missing and therefore so is proper discussion on the method's relation to other works, especially in the area of deep quantization and hashing. Some example are listed below, many more can be found through the works listed.\n\n3) The paper's contributions are unclear and while there are 2 components introduced, they are only tested together and it is hard to understand the contribution of each. Specifically, The tokenizer is pretrained on top of a large visual encoder (ViT from CLIP) independently from the index via alternative optimization. This module gives a set of codes that can be used for retrieval. The performance of those codes (seen as quantization codes) is not evaluated, while a clear discussion on technical differences and contributions to other residual quantization methods or other recent works on deep quantization or hashing is missing. It is also unclear how much the fact the quantized codes are a sequence matters, versus eg using simple PQ codes that have a much easier to optimize objective. Note that the method here uses annotated data from the downstream task, therefore the fair comparisson would be vs supervised quantization methods. \n\n4) The large AR model on top acts as a differentiable index. There is no proper discussion on how this compares to other differentiable indexes from a technical perspective. The only difference mentioned to other recent differentiable indexing methods in the end of sec 4 is that the input features (document identifiers) are obtained in a different way, but this has nothing to do with the actual differentiable index modeling part. \n \n5) The training objective is not much different from pairwise objectives on top of quantized codes used in quantization/hashing works. Although one has to pass through the index, only image pair codes are used for computing the loss. This is why beam search is needed at test time,  ie there is a clear train test discrepancy here, same as training deep hashing. I cannot see how this is more \"end2end\", as the objective of the loss is not a full database ranking objective. Also note that beam search is a complex data structure that keeps multiple paths and also \"a prefix tree containing valid codes\". One can say maybe that beam search is in itself as complex as any other indexing structure.\n\n6) Experimental validation is generally lacking. As mentioned above, the contrubution of the two modules separately shoudl be studied. Also, reporting the precision and recall metrics in isolation is not the best for image retrieval. Results with mAP (or at least mAP100) for all datasets should be presented. Also results on classic retrieval datasets like Oxford and PAris [ir1] are missing. \n\n7) inference speed experiments lacking and misleading: The proposed method has a large computational overhead, ie a large transformer decoder instead of a simple index. Basically, a comparisson versus FT-CLIP + some indexing is needed, but I can only see the proposed approach to be possibly even orders of magintude slower: excluding feature extraction with CLIP (the proposed method uses CLIP as the encoder to give to the tokenizer), the proposed method needs at test timeto also 1) to tokenize, 2) pass through a large ViT decoder  (12\ntransformer decoder block with dimension 768 for the small datasets, 24 layers for IamgeNet) and 3) beam search. Instead, Ft-CLIP with a basic IVFPQ index only needs to search the index and this is really really fast.\n\n8) The model is highly task specific: a different trained AR model/index is needed for every database. Also it is unclear how such an index can handle changes in the database\n\nSome sample missing references:\n\n[dq1] YK Jang, NI Cho  Self-supervised Product Quantization for Deep Unsupervised Image Retrieval. ICCV 2021\n\n[dq2] Yue Cao, Mingsheng Long, Jianmin Wang, Han Zhu, and Qingfu Wen. Deep quantization network for efficient image retrieval. In AAAI, 2016. \n\n[dq3] Young Kyun Jang and Nam Ik Cho. Generalized product quantization network for semi-supervised image retrieval. In CVPR, 2020\n\n[dq4] Benjamin Klein and Lior Wolf. End-to-end supervised product quantization for image search and retrieval. In CVPR,\npages 5041\u20135050, 2019.\n\n[dh1]  Kamran Ghasedi Dizaji, Feng Zheng, Najmeh Sadoughi, Yanhua Yang, Cheng Deng, and Heng Huang. Unsupervised deep generative adversarial hashing network. In CVPR, pages 3664\u20133673, 2018\n\n[dh2] Jingkuan Song. Binary generative adversarial networks for image retrieval\n\n[ir1] Radenovi\u0107, Filip, Giorgos Tolias, and Ond\u0159ej Chum. \"CNN image retrieval learns from BoW: Unsupervised fine-tuning with hard examples.\" In ECCV 2016"
                },
                "questions": {
                    "value": "Please respond to the comments above.\n\nQ1: In Fig 1 shouldnt the tokenizer get the ouput of a visual encoder as input? ie f_cls? where is that? Is \"Semantic Image Tokenizer\" a separate encoder than the \"transformer encoder\" in Fig 1? \nQ2: For the same image encoder f_cls, what is the retrieval performance of the proposed tokenizer vs other PQ/hashing tokenizers?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2547/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2547/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2547/Reviewer_rUYC"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2547/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698401481506,
            "cdate": 1698401481506,
            "tmdate": 1700725908733,
            "mdate": 1700725908733,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "UtZNjB8Aac",
                "forum": "EMCXCTsmSx",
                "replyto": "mzm4zMAZXd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2547/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2547/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors, Part 1/3"
                    },
                    "comment": {
                        "value": "Thank you for taking the time to review our paper, but there exists serious misunderstanding about our paper.\n\nFirst, we address the points you mentioned in the weaknesses part.\n\n1. Figure 1 illustrates our AR training methodology that given an input image, we feed it to our transformer (encoder and decoder) and the output is supervised by the image identifier that is obtained from the query's nearest neighbor through semantic image tokenizer (which is trained before AR model). To address any misunderstandings, we kindly request more specific details regarding your concerns. \nWe itemize our contribution as follows for clarity:\n(1) To the best of our knowledge, our work represents the first effort in employing a sequence-to-sequence generative model to address image retrieval in a fully end-to-end manner that directly generates the nearest neighbor's identifier givent the query.\n(2) We introduce a semantic image tokenizer explicitly designed for image retrieval to obtain image identifiers, facilitating autoregressive training. It is essential to note that this tokenizer differs significantly from previous quantization models, as our identifier solely represents the image and is not employed for distance computation, as is the case in quantization.\n(3) Extensive experiments show that our approach exhibits substantial improvements in results, achieving new state-of-the-art results. We have demonstrated its scalability to million-scale datasets and provided throughput number, showcasing its near-real-time performance.\n\n2. We provide a more in-depth discussion comparing our approach with prior joint learning methods. In the existing literature, early methods predominantly focused on three distinct aspects: embedding learning for enhancing search accuracy, embedding compression (quantization/hashing) for reducing memory cost, and clustering (often involving quantization) or neighborhood graphs for improving search efficiency. However, as each of these components is essential for a large-scale retrieval system, integrating individual efforts may result in suboptimal solutions. Consequently, there has been a growing interest in studying joint learning. However, all these prior works (please refer to our general response for references to prior works) focus on end-to-end training of encoding and compression together while overlooking the indexing part (meaning non-exhaustive search), which is challenging to formulate as differentiable. They leverage quantization (compression index) for ANN search (with distance approximation) but still rely on linear scan in their experiments. In contrast, our approach reframes retrieval as a completely novel end-to-end task, eliminating explicit distinctions between embedding, compression, and indexing. Our method involves offline storage of image identifiers acquired from the proposed tokenizer. Notably, this differs conceptually from quantization, as we do not use the tokenized identifier for distance comparison as in quantization. Retrieving a query's nearest neighbor in our method only involves passing the query through the autoregressive (AR) model. We will incorporate this discussion into the final paper.\n\n\n\n\n\n3. There is a serious misunderstanding that needs clarification. The codes generated by our system are intended to serve as image identifiers, not quantization codes as suggested in your comment. It's important to note that these codes are not utilized for distance computation, as is the case with quantization. Moreover, our model does not rely on the codes or codebooks during the retrieval process. Instead, it dynamically generates the codes through the autoregressive decoder based on the provided query. This methodology diverges substantially from conventional deep quantization methods. \nThen we address the concerns you raised in the comment.\n(1) We have itemized our contribution in the response above.\n(2) To assess the efficacy of the semantic image tokenizer, we have provided ablations in Table 5 of the paper, which involves ablating the image identifier using random IDs or hierarchical k-means IDs. For AR decoder, we have demonstrated its superior performance compared to FT-CLIP with linear scan, as indicated in Table 1 and Table 2 of the paper. Furthermore, we add additional comparison with a baseline that directly utilizes codes from the semantic tokenizer for image retrieval, akin to how conventional quantization performs retrieval. As anticipated, given the information loss inherent in quantization, this method is naturally inferior to linear scan and, consequently, to our proposed method.\n\n| Method \\ Inshop   | Prec@1   | Prec@10  | Prec@20  | Prec@30  |\n|:-------- |:--------:| --------:| --------:| --------:|\n| IRGen     |  92.4    |  87.0    |  86.6    |  86.5    |\n| RQ-ADC|     62.5    |  48.0  |  44.8   |  43.3  |\n\n(3) The distinctions between our method and previous approaches, such as deep quantization, have been elucidated in the response above."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2547/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700653628937,
                "cdate": 1700653628937,
                "tmdate": 1700653628937,
                "mdate": 1700653628937,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FFBbfcYYCb",
                "forum": "EMCXCTsmSx",
                "replyto": "mzm4zMAZXd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2547/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2547/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors, Part 2/3"
                    },
                    "comment": {
                        "value": "(4) We add comparison to a baseline that uses PQ for tokenizer instead of RQ. The superior performance demonstrates that sequence matters in our autoregressive training.\n\n| Model \\ Inshop   | Prec@1   | Prec@10  | Prec@20  | Prec@30  |\n|:-------- |:--------:| --------:| --------:| --------:|\n| IRGen (proposed tokenizer)     |  92.4    |  87.0    |  86.6    |  86.5    |\n| IRGen (PQ tokenizer)      |  73.8    |  66.1    |  61.8    |  59.2    |\n\n(5) We compare with recent supervised deep quantization methods on ImageNet, including the state-of-the-art methods ADSVQ and DPQ. Notably, these methods use 100 classes from ImageNet, as indicated in their original papers. For fair comparison, we reran our model for 100 classes, whereas the results in our paper are based on the full 1000 categories of ImageNet. The table below illustrates the superiority of our approach.\n \n|  Method  |   mAP@1000 for 32 bits (equivalent to the length of our identifier)  |\n| --------:| ------------:|\n|   IRGen     |     93.52     |\n|DPQ [1] | 87.70 |\n|   ADSVQ [2]  |     75.21     |\n|   DTQ [3]   |     68.12     |\n|ADSH [4] | 63.36|\n|HashNet [5]| 58.71|\n\n\n[1] Klein, Benjamin, and Lior Wolf. \"End-to-end supervised product quantization for image search and retrieval.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019.\n\n[2] Zhou, Chang, Lai Man Po, and Weifeng Ou. \"Angular deep supervised vector quantization for image retrieval.\" IEEE Transactions on Neural Networks and Learning Systems 33.4 (2020): 1638-1649.\n\n[3] Liu, Bin, et al. \"Deep triplet quantization.\" Proceedings of the 26th ACM international conference on Multimedia. 2018.\n\n[4] Liu, Haomiao, et al. \"Deep supervised hashing for fast image retrieval.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.\n\n[5] Cao, Zhangjie, et al. \"Hashnet: Deep learning to hash by continuation.\" Proceedings of the IEEE international conference on computer vision. 2017.\n\n\n\n4. Differentiable indexing in our approach means non-exhaustive search, whereas deep quantization methods use differentiable index for embedding compression. Our focus on image retrieval marks the first effort. We introduce a novel semantic tokenizer specifically designed for image retrieval. We add a direct comparison using NCI's hierarchical k-means tokenizer versus ours, emphasizing the effectiveness of our proposed semantic tokenizer.\n\n| Method \\ Inshop   | Prec@1   | Prec@10  | Prec@20  | \n|:-------- |:--------:| --------:| --------:| \n| IRGen     |  92.4    |  87.0    |  86.6    | \n| NCI (HKM)  | 89.0 |81.6 |79.8 |\n\n5. There is a serious misunderstanding. Our model does not use image pair codes for computing the loss, as indicated in your comment. Instead, we aim to generate the codes for the nearest neighbor corresponding to the given input. The distinctions between our model and previous deep quantization methods have been elaborated in the response above. Our more end-to-end property stems from that we directly find the nearest neighbor through the learned generative model, compared to deep quantization that needs to calculate distance between query and each database's codes. When only the top-1 sample is required, beam search is unnecessary, and generation can be performed by straightforwardly selecting the identifier with the highest probability, ensuring no train-test discrepancy. However, for top-K retrieval, we leverage beam search with a configured beam size of K. In this scenario, a train-test discrepancy emerges. One potential solution is the further integration of beam search into the training process, a direction that is worth pursuing.\n\n6. It appears that certain experiments presented in the paper may have been overlooked. The contributions of the two modules have been examined, and please refer to point (2) in the 3rd response. We have conducted a mAP comparison on two million-scale datasets, ImageNet and Places365, as in Table 3 of the paper. Regarding the remaining three datasets in Table 1 and 2, a direct mAP or mAP@100 comparison is not reasonable due to the limited number of ground truth samples, all less than 100 for each query. It is important to note that previous works, including the compared baselines, only report recall@K metric in their experiments. To provide a comprehensive comparison, we reported precision@K metric in our paper. Furthermore, we have evaluated on three widely used benchmarks and two million-scale datasets. The classical retrieval datasets, Oxford and Paris, are relatively small scale, with 5062 images for Oxford and 6412 images for Paris. These datasets are commonly employed for zero-shot retrieval scenario, particularly for pre-training models. Notwithstanding, we add comparison to Paris, which consistently validates the effectiveness of our proposed method.\n\n\n| Method \\ RParis |   mAP for M  | mAP for H |\n| :------| ------: | -------: |\n|   IRGen   | 75.7 |40.4 |\n| FT-CLIP| 71.4 |27.1 |"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2547/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700654403200,
                "cdate": 1700654403200,
                "tmdate": 1700670425269,
                "mdate": 1700670425269,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zkedeWyK7F",
                "forum": "EMCXCTsmSx",
                "replyto": "Nx4ME4u4j1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2547/Reviewer_rUYC"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2547/Reviewer_rUYC"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for detailed answers"
                    },
                    "comment": {
                        "value": "I want to thank the reviewers for detailed answers and let me apologize for any misunderstandings on my side. I now see how there exist conceptual differences with quantization and get the novelty aspects better. \n\nI still think the paper is not clearly written, and for Figure 1 the authors did nto respond to my question: does  the tokenizer have a visual encoder inside? If so, does it  share weights with the \"transformer encoder\" in Fig 1? \n\nI also thank the authors for providing comparisons to deep quantization methods. However, it is unclear to me that they are fair - specifically: Is the tokenizer in your method _pretrained_ on top of a large visual encoder from CLIP? If so, this gives an unfair advantage to your method when you compare to other hashing/PQ methods (that I would assume only train on the ImageNet dataset - plz correct me if I am wrong). \n\nIn fact, I would appreciate if the authors list here _all the data_ that their method (tokenizer and autoencoder) was trained on. This includes any pretrained models they might have used as initialization. \n\nIt is also unclear to me if the authors uploaded an updated pdf - i believe this to be important given the fact that the paper is not clearly written and the authors added related works and comparissons in their answers. If the authors did, I  apologize and would encourage them to color all changes so that one can easily spot new additions/rephrasings/related work. \n\nI will increase my score to borderline reject; Waiting for a response from the authors to my questions above."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2547/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700725888370,
                "cdate": 1700725888370,
                "tmdate": 1700725888370,
                "mdate": 1700725888370,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "352K6xvO15",
            "forum": "EMCXCTsmSx",
            "replyto": "EMCXCTsmSx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2547/Reviewer_obPk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2547/Reviewer_obPk"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces an image retrieval method built upon semantic image tokenization. Recognizing the limitations of popular tokenization techniques like VQ-VAE and RQ-VAE for retrieval tasks, the authors propose a more efficient approach that emphasizes global feature extraction from class tokens, reducing sequence lengths and emphasizing high-level semantic information. To enhance semantic representation, they also incorporate classification loss training. In tandem, an autoregressive encoder-decoder architecture is employed that decouples input embedding from discrete code generation, focusing on understanding semantic relationships between image pairs. During the inference stage, beam search is used to efficiently find top-K matches for a given query image. The model's end-to-end design ensures efficient retrieval and offers a novel perspective compared to traditional approximate nearest neighbor search techniques."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper introduces an encoder-decoder architecture that decouples input embedding from discrete code generation. This design, focusing on understanding the semantic relationships between image pairs, offers a more flexible and adaptable framework, potentially making the model more robust and applicable across various datasets and retrieval scenarios.\n\n2. The model's design places a significant emphasis on capturing high-level semantic information. By incorporating classification loss training on both original and reconstructed embeddings, the proposed approach ensures that the retrieved images are semantically relevant to the query, bridging the gap between low-level details and meaningful content.\n\n3. The paper acknowledges the inefficiencies of traditional tokenization techniques for image retrieval and introduces an approach that reduces sequence lengths. By focusing on global feature extraction from class tokens, the model offers a more streamlined and efficient representation, especially suited for retrieval tasks."
                },
                "weaknesses": {
                    "value": "1. While emphasizing global feature extraction from class tokens might improve efficiency, there's a risk of overlooking crucial spatial information present in other parts of the image, possibly leading to incomplete or less accurate retrieval results.\n\n2. The paper proposes the use of beam search for efficient top-K retrieval, but this method can be computationally intensive, especially for large image databases. Additionally, validating each generated image identifier can be a time-consuming process, even with their proposed prefix tree optimization.\n\n3. The approach, especially the beam search with constraints, may face scalability issues when dealing with vast and diverse image datasets. As databases grow, the efficiency and accuracy of the method may be challenged.\n\n4. While the method aims to capture high-level semantics, it might not generalize well across diverse datasets with varying characteristics. The paper does not address how the model would adapt to datasets with vastly different semantic structures or image content.\n\n5. There seems to be a limited discussion on how this method performs compared to other state-of-the-art techniques. Without comprehensive comparative studies, it's challenging to ascertain the model's effectiveness and superiority in real-world scenarios."
                },
                "questions": {
                    "value": "1. How do you ensure that the emphasis on global feature extraction from class tokens doesn't compromise the finer spatial details of the image, which could be vital for certain retrieval scenarios?\n\n2. Given the complexities of beam search, especially for large image databases, what specific optimizations have you implemented to ensure real-time or near-real-time retrieval performances?\n\n3. How does the proposed model handle noisy or imperfect image datasets? Are there specific preprocessing steps or augmentations recommended to enhance retrieval accuracy?\n\n4. How adaptable is the autoregressive encoder-decoder architecture to other types of multimedia content, such as videos or audio? Are there potential modifications or extensions to the proposed method for such content?\n\n5. Given the model's dependency on class tokens, how does it handle images that may not fit neatly into predefined classes or those that belong to multiple overlapping classes?\n\n6. You have mentioned using classification loss for training. Were other loss functions explored during the development? If so, how did they impact the model's performance?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2547/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698677238083,
            "cdate": 1698677238083,
            "tmdate": 1699636191222,
            "mdate": 1699636191222,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "P3wZSVeOYu",
                "forum": "EMCXCTsmSx",
                "replyto": "352K6xvO15",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2547/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2547/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors, Part 1/2"
                    },
                    "comment": {
                        "value": "We appreciate the time you took to review our paper.\n\nFirst, we address the five points you mentioned in the weaknesses part.\n\n1. Image retrieval is a high-level vision task, akin to extreme classification problem, reliant on a global understanding of the image, where global features are pivotal. Our method proves effective not only in general retrieval, such as ImageNet, but also excels in fine-grained tasks like Inshop clothes retrieval, CUB retrieval, and car retrieval, showcasing its ability to capture necessary details for fine-grained search using global features. Recognizing scenarios where similarity is defined over small image areas requiring spatial information, we suggest a solution involving a pre-processing step that detects these important regions. For instance, person re-identification is such a case utilizing detection before search. Alternatively, incorporating both global and local features ensures a comprehensive image representation but may entail complex models with increased computational demands. Striking a balance between efficiency and efficacy is a longstanding challenge in retrieval. This paper has successfully present a novel approach that is conceptually different from previous joint learning methods, and demonstrate state-of-the-art performance on widely used benchmarks with near-real-time inference speed.\n\n2. First, the proposed prefix tree only contains valid codes and thus eliminates the need of validating each generated image identifier, largely enhancing the inference speed. We then provide a comprehensive comparison of model capacity and inference cost in the table below, focusing on the million-scale ImageNet dataset. It is crucial to emphasize that a retrieval system's storage considerations extend beyond just the model; it must also store features for all database vectors, constituting a substantial portion of storage requirements, especially in the context of a large database. While compression can alleviate this concern, the performance is severely compromised due to quantization. In terms of inference cost, our CPU inference cost is 0.30s per image, which is not orders of magnitude larger compared to FT-CLIP with linear scan. Although FT-CLIP with SPANN lowers the inference cost, it comes at the expense of a substantial drop in performance. Our GPU inference cost, tested on a Tesla P100 PCIe 16GB GPU, is 0.037s/image, which is near-real-time, considering practical retrieval times at the order of 10 milliseconds. Moreover, our approach demonstrates competitive performance compared to FT-CLIP with linear scan, and significantly reduces the memory cost. For inference speed, we anticipate that the prevalent reliance on GPUs for generative models will swiftly lead to newer GPU generations with increased processing power, more efficient memory management, and improved hardware support, thereby enhancing the inference speed of our autoregressive retrieval. Additionally, our model, being GPT-based, can leverage rapidly developed acceleration techniques designed for GPT.\n\n|  Method | Model (#Params) |  Database storage  |\n| -------:| ----------------- |  --------------:|\n|      IRGen | ViT-base encoder (86M)+ 24-layer decoder (307M)  | 4.9MB\uff1a 4 bytes (id)/image with 1281167 samples    |\n| FT-CLIP | ViT-base (86M)    | 3.67GB\uff1a  768x4 bytes (feature)/image with 1281167 samples  |\n\n\n|  Method | Search         | Inference cost | mAP@100    |\n| -------:| -------------- | --------------:| --- |\n|      IRGen | Autoregressive |   0.037s/image |   76.0  |\n| FT-CLIP | Linear scan (SPANN)   |   0.14s/image (0.007s/image) |   77.0 (65.5)  |\n\n3. Our current objective is to showcase the effectiveness of the proposed generative based model. We have conducted experiments on three widely used retrieval benchmarks, and show the scalability on other two wellknown million-scale datasets. Further scaling to handle billion-scale datasets poses a considerable challenge due to the substantially increased demand for resources, including storage and computation, which currently exceeds our available capacity.  In future work, we may address billion-scale scenario with a hybrid solution where billions of images can be clustered into millions of semantic clusters, and our proposed model IRGen can then be utilized to retrieve the relevant cluster identifiers. This approach would enable us to leverage current success in million-scale datasets and handle billion-scale datasets in a more efficient manner."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2547/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700651460866,
                "cdate": 1700651460866,
                "tmdate": 1700651460866,
                "mdate": 1700651460866,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XZMTjQi8m6",
                "forum": "EMCXCTsmSx",
                "replyto": "352K6xvO15",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2547/Reviewer_obPk"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2547/Reviewer_obPk"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the rebuttal"
                    },
                    "comment": {
                        "value": "I thank the authors for preparing the rebuttal and answering my comments. After reading authors' reply and other reviews, I am keeping my original rating. Specifically, the advantage of prefix tree and the formulation is still not clear to me. Additionally, there are other concerns requiring the paper to be updated to see how the updated manuscript would look like. Furthermore, the model diagram doesn't contain all the proposed components, which in my opinion is necessary."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2547/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700736299620,
                "cdate": 1700736299620,
                "tmdate": 1700736299620,
                "mdate": 1700736299620,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Arvoi9EcE1",
            "forum": "EMCXCTsmSx",
            "replyto": "EMCXCTsmSx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2547/Reviewer_8qfY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2547/Reviewer_8qfY"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel approach for image retrieval using generative modeling -- IRGen. IRGen is a sequence-to-sequence model that, given a provided query image, directly generates \"identifiers\" corresponding to the query\u2019s nearest neighbors. Specifically, the model takes a query image as input and autoregressively predicts discrete visual tokens, which are considered as the identifier of an image. These discrete visual tokens are learned through classification loss, the global features of an image is tokenized through residual quantization. Once the semantic image tokenizer is trained, then a decoder is learned to predict the identifier of query's nearest neighbor through autoregressive way. In summary, this paper propose a novel approach to tokenize an image into semantic identifiers and achieves state-of-the-art over conventional methods."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "originality:  the paper proposes a novel approach for image retrieval using generative modeling. The semantic image tokenizer with residual quantization is elegant and the generative modeling way is quite interesting. The whole framework looks pretty straightforward and simple.\n\nquality: the paper is technically sound. Extensive experiments were performed.\n\nclarity: the paper is well-written and well-organized.\n\nsignificance: the paper address image retrieval with generative modeling which is quite an interesting way, showing the possibility of using generative method to obtain codes for retrieval. Hence this paper can inspire many future works, such as how to learn a code that can do both generation and retrieval, how to speed up the search (e.g., with hash codes/without beam search)."
                },
                "weaknesses": {
                    "value": "- the description of how the beam search is a little bit confusing to me. I couldn't fully understand how it is done.\n- retrieval with autoregressive means the retrieval may not enjoy the benefit of maximum inner-product search\n- as the current retrieval is done with GPU, if using CPU, the decoder process will be significantly slowed down and may affect the retrieval speed"
                },
                "questions": {
                    "value": "1. does the semantic tokenizer share the weights with the encoder? \n2. seems like we can directly use the code from semantic tokenizer for image retrieval -- similar to how product quantization performs retrieval, what is the necessity of employing the decoder? What is the disadvantange/how much performance degrades if we just use the semantic tokenizer's code for retrieval?\n3. what is the retrieval speed of using IRGen on CPU compared to IVFPQ/ScaNN/SPANN?\n4. what if you train a normal classification model, then perform PQ to obtain the \"tokens\" instead of the proposed semantic tokenizer?\n5. is a two-stage training process? can they be trained end-to-end?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2547/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2547/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2547/Reviewer_8qfY"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2547/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698707640513,
            "cdate": 1698707640513,
            "tmdate": 1699636191132,
            "mdate": 1699636191132,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Y2Y8RVXGBB",
                "forum": "EMCXCTsmSx",
                "replyto": "Arvoi9EcE1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2547/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2547/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors, Part 1/2"
                    },
                    "comment": {
                        "value": "Thank you for your detailed review.\n\nFirst, we address the three points you mentioned in the weaknesses part.\n\n1. The procedure begins by defining a beam size, denoted as K in our case. Initially, the beam comprises the start-of-sequence token, and the algorithm extends the current set of candidate sequences by considering the next potential tokens in the sequence. For each candidate sequence, the algorithm outputs probabilities for multiple potential next tokens and adds them to the beam. The expanded set of sequences is then scored based on the probabilities assigned by the model. The top-k sequences with the highest scores are retained, while the others are discarded. This selection process, which can be implemented with a priority queue in linear time with the candidate list size, is repeated until the end-of-sequence token is encountered. In the case of beam search with a prefix tree, the search process is constrained to consider only valid next tokens, as determined by the prefix tree, rather than all the possible next tokens. This constraint ensures that the search process results in valid image identifiers corresponding to specific images in the dataset. We will provide a detailed explanation of the beam search algorithm in the Appendix.\n\n2. We find the noted weakness a bit perplexing, particularly in the absence of specific details regarding the particular benefit that may not be enjoyed by retrieval with an autoregressive approach. Our interpretation is based on our best understanding of the context. Maximum inner-product search (MIPS) pertains to a similarity search problem where the objective is to efficiently locate the data point in a collection that maximizes the inner product with a given query vector. Nonetheless, if the ultimate goal is to retrieve similar data, our methodology allows us to formulate and address this challenge optimally in an end-to-end manner, rather than a two-step process involving initial feature embedding followed by maximum inner-product search. We acknowledge the potential scenarios where data embedding becomes imperative, such as compact feature learning for transmission purposes. In such cases, one could also treat the embedded data as input and train an autoregressive model to identify the query's nearest neighbor, bypassing the tedious tuning over  the joint integration of compression and indexing.\n\n3. We provide a comprehensive comparison of model capacity and inference cost in the table below, focusing on the ImageNet dataset. It is crucial to emphasize that a retrieval system's storage considerations extend beyond just the model; it must also store features for all database vectors, constituting a substantial portion of storage requirements, especially in the context of a large database. In terms of inference cost, our CPU inference cost is 0.30s per image, which is not orders of magnitude larger compared to FT-CLIP with linear scan. Although FT-CLIP with SPANN lowers the inference cost, it comes at the expense of a substantial drop in performance. Our GPU inference cost, tested on a Tesla P100 PCIe 16GB GPU, is 0.037s/image, which is near-real-time, considering practical retrieval systems at the order of 10 milliseconds. Moreover, our approach demonstrates competitive performance compared to FT-CLIP with linear scan, and significantly reduces the memory cost. We anticipate that the prevalent reliance on GPUs for generative models will lead to newer GPU generations with increased processing power, more efficient memory management, and improved hardware support, thereby enhancing the inference speed of our autoregressive retrieval. Additionally, our model, being GPT-based, can leverage rapidly developed acceleration techniques designed for GPT. Our model also opens avenues for the integration of GPT in the development of more intelligent and context-aware information retrieval and generation systems.\n\n|  Method | Model (#Params) |  Database storage  |\n| -------:| ----------------- |  --------------:|\n|      IRGen | ViT-base encoder (86M)+ 24-layer decoder (307M)  | 4.9MB\uff1a 4 bytes (id)/image with 1281167 samples    |\n| FT-CLIP | ViT-base (86M)    | 3.67GB\uff1a  768x4 bytes (feature)/image with 1281167 samples  |\n\n\n|  Method | Search         | Inference cost | mAP@100    |\n| -------:| -------------- | --------------:| --- |\n|      IRGen | Autoregressive |   0.037s/image |   76.0  |\n| FT-CLIP | Linear scan (SPANN)   |   0.14s/image (0.007s/image) |   77.0 (65.5)  |"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2547/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700651268311,
                "cdate": 1700651268311,
                "tmdate": 1700651268311,
                "mdate": 1700651268311,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Ux5Oeg1OdW",
            "forum": "EMCXCTsmSx",
            "replyto": "EMCXCTsmSx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2547/Reviewer_Re4A"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2547/Reviewer_Re4A"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes an approach which learns the encoding and indexing structure jointly for improving improving image retrieval task. It relies on VIT backend for encoding and Residual Quantization (RQ) for hierarchical semantic representation learning. The proposed solution is optimized end-to-end unlike traditional approach of creating content embeddings and running approximate nearest neighbor (ANN) search independently. The solution is evaluated across multiple dataset and multiple baseline embedding models (fine-tuned on the same dataset) coupled with different ANN search algorithms. The results indicate significantly improved the image retrieval results."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Provides a simple, intuitive and technically sound approach, which borrows strong insights from the recent generative modeling literature and Residual Quantization concepts. \n- There are multiple baselines and numerous ablations to provided for evaluations. Outperforms strong baselines. \n- The paper is well motivated and written."
                },
                "weaknesses": {
                    "value": "- The concept of joint training the embedding model and index structure is not entirely novel \n- The approach scales well for million scale datasets, but not billions"
                },
                "questions": {
                    "value": "- The concept of jointly training the retrieval / embedding model and ANN index structures (or search models) is not novel. It would be great to have a more in depth review of these approaches. A quick example: \"Joint Learning of Deep Retrieval Model and Product Quantization based Embedding Index\", SIGIR'21. https://dl.acm.org/doi/10.1145/3404835.3462988 \n- Figure 4: The precision of the proposed approach increases as top-k increases. The expectation is that there should be a trade-off. \n- Table 4: The generalization capability of the approach is demonstrated by taking out 5% of the ImageNet dataset during training and used them for inference/testing. However, those 5% examples are coming from the same ImageNet dataset distribution. The model already learned the semantic classes and their visual variations. Therefore this experiment does not fully demonstrate the out-of-domain capability of the proposed method. Also, it is not clear if the same treatment applied for the other baseline model compared. Need more clarity and justification for this experiment. \n- There is a need for a table comparing the capacity and inference cost of the baseline models to the proposed solution in the Appendix (for Apples-to-Apples comparisons). A similar comparison is also needed for the retrieval stage. At million scale, even linear search is quite practical. It would be also great to have ablations where the model capacity is varied and the overall performance is evaluated. \n- Residual Vector Quantization also uses beam search and RVQ codes may be used for efficiently knn search using prefix trees (or better with Aggregating Tree). Therefore the inference time operations are quite similar to IRGen paper except that the previous paper does not train an encoding for the search task. This could also serve as a natural baseline which would demonstrate the need for joint training of encoder and indexing/search structures. OR, after training the encoder, but could we combine and use the below paper for a more scalable (billions) search? A discussion should suffice. \nLiu et al, \"Generalized Residual Vector Quantization and Aggregating Tree for Large Scale Search\", IEEE T on Multimedia, 2017\n- Sanity check: Are the reported results for IRGen the-state-of-the-art today for the target image retrieval benchmarks? This should be stated clearly in the paper. If not, we need comparison with the SOTA method."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2547/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698735728132,
            "cdate": 1698735728132,
            "tmdate": 1699636191063,
            "mdate": 1699636191063,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "HCkOqlORXC",
                "forum": "EMCXCTsmSx",
                "replyto": "Ux5Oeg1OdW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2547/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2547/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors, Part 1/2"
                    },
                    "comment": {
                        "value": "Thank you for your valuable feedback.\n\nFirst, we address the points you mentioned in the weaknesses part.\n\n\n1. We offer a detailed comparison of our approach with previous joint learning methods. Early methods in the literature primarily concentrated on three key aspects: embedding learning to enhance search accuracy, embedding compression (quantization/hashing) to reduce memory cost, and clustering (often involving quantization) or neighborhood graphs to improve search efficiency. However, integrating these essential components individually may lead to suboptimal solutions for large-scale retrieval systems. As a result, there is a growing interest in the study of joint learning. However, all prior works (please refer to our general response for references to prior works) focus on end-to-end training of encoding and compression together while overlooking the indexing part (meaning non-exhaustive search), which is challenging to formulate as differentiable. They leverage quantization (compression index) for ANN search (with distance approximation) but still rely on linear scan in their experiments. In contrast, our approach reframes retrieval as a completely novel end-to-end task, eliminating explicit distinctions between embedding, compression, and indexing. Our method involves offline storage of image identifiers acquired from the proposed tokenizer. Notably, this differs conceptually from quantization, as we do not use the tokenized identifier for distance comparison as in quantization. Retrieving a query's nearest neighbor in our method only involves passing the query through the autoregressive (AR) model.\n\n\n\n2. Thank you for acknowledging our results with million-scale datasets. Handling billion-scale datasets presents a significant challenge, given the heightened resource demands, including storage and computation, that currently surpass our available capacity. Our primary goal is to highlight the effectiveness of the proposed generative model. To demonstrate this, we conducted experiments on three retrieval benchmarks and two additional million-scale datasets. In future endeavors, we aim to address the billion-scale scenario through a hybrid solution. This involves clustering billions of images into millions of semantic clusters, allowing our model IRGen to efficiently retrieve relevant cluster identifiers. This approach would enable us to handle billion-scale datasets in a more efficient manner.\n\n\n\nThen we answer the questions below.\n\n1. Refer to the weaknesses section for an in-depth explanation. The mentioned example also involves joint learning of encoding and compression (referred to as embedding index in the paper), relying on linear scan search in their experiments. In contrast, our approach defines retrieval as a novel end-to-end task, directly retrieving a query's nearest neighbor by passing through the AR model, without explicit modeling for embedding, compression, and indexing.\n\n\n2. Traditional methods tend to witness a decline in precision with an increasing retrieval set size (K). While they initially achieve high precision by prioritizing the most relevant images in the top ranks, the precision drops rapidly as K grows due to the retrieval of irrelevant and confusing images. This indicates a struggle to retrieve harder positive images, often necessitating a reranking stage after the initial retrieval. In contrast, our AR decoder consistently retrieves relevant samples as the retrieval list lengthens. Our top-K performance even exhibits a slight increase with a larger K, showcasing its proficiency in handling challenging examples. This capability can be attributed to our approach of randomly sampling codes from images in the same class as the query image during training, enhancing the model's capacity to handle difficult samples. This advantage potentially mitigates the need for a post-reranking stage in our model to some extent.\n\n\n3. In scenarios where more new data come in and the distribution of gallery data has changed drastically, all existing data-dependent retrieval methods, including embedding model in image retrieval as well as dense retrieval methods in document retrieval, have to re-train the model and re-construct the data index. Essentially, dynamic updates only support a small number of new data that do not substantially alter the distribution. Our experiments have demonstrated the capability of handling this case through adding new, unseen data within the same class, showcasing the model's adaptability to minor distribution changes. Nevertheless, it's important to highlight that data-dependent models continue to be preferred due to their superior performance. In future work, there is a potential extension of IRGen towards an adaptive structure capable of capturing evolving distribution during training.  For our experiment, the other baseline models employed the same treatment for fair comparison, and we will provide further clarification in the revised version."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2547/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700651153637,
                "cdate": 1700651153637,
                "tmdate": 1700652986077,
                "mdate": 1700652986077,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cUxsr3aKv6",
                "forum": "EMCXCTsmSx",
                "replyto": "PEiOdKrAED",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2547/Reviewer_Re4A"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2547/Reviewer_Re4A"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks to the authors for the detailed comments. I am in favor of keeping the current rating after reading other reviews. \n\nOne side comment (which did not impact the final rating) is that I am quite surprised to see the large gap between RQ prefix-tree and IRGen comparison (MAP 56.7 vs 76). Not clear how to interpret and explain. There are also various ways to improve RQ prefix-tree solution with additional probes identified by beam search. Without these tricks, RQ prefix-tree might perform worse. For your information."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2547/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700698388053,
                "cdate": 1700698388053,
                "tmdate": 1700698388053,
                "mdate": 1700698388053,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]