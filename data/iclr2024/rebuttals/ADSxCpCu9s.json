[
    {
        "title": "LoTa-Bench: Benchmarking Language-oriented Task Planners for Embodied Agents"
    },
    {
        "review": {
            "id": "jSmYGJ1sH8",
            "forum": "ADSxCpCu9s",
            "replyto": "ADSxCpCu9s",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2254/Reviewer_mB86"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2254/Reviewer_mB86"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes LoTa-Bench, an evaluation framework for the planning capability of LLMs for embodied agents.\nThe proposed benchmark utilizes two existing benchmarks, ALFRED and WAH, that require agents to plan a sequence of actions to satisfy desired goal conditions given natural language instructions.\nIt follows the evaluation protocol of ALFRED and WAH.\nThe paper compares various large language models for planning. It conducts some experiments to show possible extensions: 1) in-context example selection, 2) replanning based on feedback, and 3) fine-tuning LLMs on downstream tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper is generally written well and easy to follow.\n- The paper provides an important framework for the evaluation of the planning ability of LLMs which have been widely used for the planning of embodied agents.\n- Focusing on planning by language by some assumptions (e.g., oracle information of objects for visual navigation) sounds reasonable.\n- The paper provides a useful extensive analysis of the planning capabilities of existing LLMs for the proposed benchmark."
                },
                "weaknesses": {
                    "value": "- I appreciate the proposed benchmark for LLMs' planning but some of the proposed possible extensions on this benchmark are somewhat expected. For example, fine-tuning a pretrained model (here, LLMs) to downstream tasks is a widely used approach. And for replanning by feedback, while a larger LLM is improved with the replanning, a smaller one is not, implying that we need some large LLMs, but it is also quite expected that a larger model may yield better performance and a smaller one.\n- The feedback for replanning is based on metadata (i.e., oracle information about what is \"wrong\") of the simulated environments but this may not be always available beyond the benchmark for other downstream tasks (e.g., some tasks might not be able to provide why agents fail at actions). Can this direction be also useful for those cases without the oracle information?\n- The authors conduct experiments based on a subset (30%) of the original datasets but do not provide some significance tests. It might be better to include them in the results (e.g., Figure 2) with different subsets.\n\n\\* Minor\n - The used font looks a bit different from the one used in the original ICLR format. It might be better to revert to the original font."
                },
                "questions": {
                    "value": "See weaknesses above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2254/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2254/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2254/Reviewer_mB86"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2254/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697701441485,
            "cdate": 1697701441485,
            "tmdate": 1699636158599,
            "mdate": 1699636158599,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ZFRpQZrFZh",
                "forum": "ADSxCpCu9s",
                "replyto": "jSmYGJ1sH8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2254/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2254/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your thorough review and constructive feedback. We are grateful for the positive remarks on the clarity of the paper and the significance of the proposed framework for evaluating language models' planning capabilities. We respond to your comments on the weaknesses.\n\n> **Some of the proposed possible extensions on this benchmark are somewhat expected.**\n\nAs you have mentioned, the two extensions we proposed do exhibit somewhat predictable outcomes. However, the significant benefit of our LoTa-Bench lies in its ability to quantitatively measure the effectiveness of fine-tuning, a widely used approach. This capability is valuable as it provides a concrete benchmark to evaluate the extent of improvement that fine-tuning can offer.\n\nIn the case of replanning by feedback, our benchmark also offers valuable insight. The performance variations when using NL feedback highlight that different types of information can significantly influence outcomes. This aspect of our work facilitates easy testing of these variables. It provides a robust test environment that can shape the direction of future research in LLM-based task planning. We believe that our LoTa-Bench will encourage the community to experiment with their ideas, and this will bring interesting results.\n\n> **The feedback for replanning is based on metadata ... but this may not be always available beyond the benchmark for other downstream tasks ... Can this direction be also useful for those cases without the oracle information?**\n\nIn Section 5.2, we conducted experiments on feedback and replanning. As you mentioned, we utilized oracle information to articulate the reasons for failure in the current step in natural language, which is then fed into an LLM-based planner for replanning. These experiments primarily investigated the impact of NL feedback on planning performance, under the assumption that reasons for failure are accessible. However, in scenarios where oracle information is unavailable, our approach could potentially infer the causes of step failures using sensor data from embodied agents. For instance, feedback like \u201c(this action failed: Robot is not holding any object)\u201d could be derived by checking if the robot\u2019s hand sensors detect an object. Similarly, \u201c(this action failed: put down failed)\u201d could be inferred using vision data by object detection. This possibility represents a separate research direction. Our paper primarily focuses on evaluating the effectiveness of NL feedback when it is available, demonstrating its potential to enhance high-level planning performance.\n\n> **The authors conduct experiments based on a subset (30%) of the original datasets but do not provide some significance tests. It might be better to include them in the results (e.g., Figure 2) with different subsets.**\n\nAlthough we randomly selected a 30% subset, it might have some biases in selection. We fully agree with you that it would be better to include results with different subsets. We conducted additional experiments with three other subsets. Due to the limited time of this discussion period, we only included results for three models of GPT-Neo, LLaMA-1, and LLaMA-2 except for the large ones having more than 60B parameters. Our code was ready for this experiment; we only changed the random seed for subset selection on line 78 in *src/alfred/alfred_evaluator.py* in the supplementary code. \n\nThe figure linked below shows the results with different subsets.\n\nhttps://figshare.com/s/557079abb37b1712d074\n\nThe bold lines are the same as the previous results in Figure 2. The thin lines represent additional results with different subsets. The general trends that 1) bigger models work better and 2) LLaMA series are better than GPT-Neo are the same. However, we found some differences in the results. For example, the success rates range widely from 7.21 to 12.5 for GPT-NeoX-20B according to the selection of subsets. And LLaMA-1 worked better than LLaMA-2 for some subsets. As we found subset selection affect results to some extent, we will also include full set (using 100\\% samples) results with all the models in the final paper (if we have the opportunity to prepare the final paper). Specifically, we will include two figures for the subset and the full set instead of Figure 2 (a).\n\nWe would like to mention that reporting results for a subset is still valuable since many researchers do not have enough computing resources, so we think it would be better to maintain a 30% subset as well so that it serves as a kind of small set for quick or affordable experiments.\n\nThis subset experiment is only for ALFRED. We already used the full set for WAH-NL."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2254/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700393697089,
                "cdate": 1700393697089,
                "tmdate": 1700393697089,
                "mdate": 1700393697089,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qyPOhhyuPf",
                "forum": "ADSxCpCu9s",
                "replyto": "jSmYGJ1sH8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2254/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2254/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "(continued)\n\n> **The used font looks a bit different from the one used in the original ICLR format. It might be better to revert to the original font.**\n\nThank you for pointing this out. We used XeLaTeX for PDF compiling, and it gave slightly different fonts than the pdfLaTeX compiler. We corrected the compiling options and will use the corrected version when we upload the revised draft. It was not intentional, and the page count was the same.\n\n---\n\nWe appreciate your valuable feedback, and we believe that addressing these points will contribute to the overall improvement of the paper. If you have any further suggestions or remaining concerns, please do not hesitate to communicate them."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2254/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700393732440,
                "cdate": 1700393732440,
                "tmdate": 1700393732440,
                "mdate": 1700393732440,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RSfwAhwZ3u",
                "forum": "ADSxCpCu9s",
                "replyto": "qyPOhhyuPf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2254/Reviewer_mB86"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2254/Reviewer_mB86"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Reviewer mB86"
                    },
                    "comment": {
                        "value": "I thank the authors for their detailed response.\nThe response addressed my concerns and thus I would like to keep my original positive rating."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2254/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700634867999,
                "cdate": 1700634867999,
                "tmdate": 1700634867999,
                "mdate": 1700634867999,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "KhfysXH4St",
            "forum": "ADSxCpCu9s",
            "replyto": "ADSxCpCu9s",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2254/Reviewer_DjjY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2254/Reviewer_DjjY"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposed the development of a benchmark to measure the effectiveness of LLM-based task planners in language-related settings (particularly where the system receives natural language instructions). The proposed benchmark consists of pairs of datasets and simulators, where the dataset includes example instructions and environment information, and the simulator allows the plan generated by the planner to be evaluated. They propose a baseline planner that selects the highest probability skill from a set of possible skills, given the relevant information. The base planner is evaluated on the two proposed dataset/simulator pairs, and they also perform additional experiments to evaluate factors like replanning, finetuning, and example selection."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The clear strength of the paper is the fact that this is a problem that is getting a lot of attention. As discussed by the authors, there aren\u2019t quite too many benchmarks of this specific quality, particularly the presence of simulators ( though I am not completely sure if this aspect is being fully utilized here). In addition to the basic evaluation, I found the additional experiments quite insightful and useful, especially the ones related to the example selection."
                },
                "weaknesses": {
                    "value": "NL to API Component: I am a little bit confused about the need for setting up an NL to API component for the current system. Why can\u2019t the skills provided to the LLM consist of directly executable API calls or actions? Is the concern here that the API calls would be too abstract and arbitrary to be connected to the actual instruction? Can\u2019t you get around it by using meaningful action names (as is the case in classical symbolic planning settings [1]) or provide a description of each action in natural language as part of the environment context?\n\nFailure Reasons: I wish there were a more detailed analysis of how the plans failed and the main results were not reported in terms of mere success rates. Some potential analyses the authors should consider here include things like checking if there were any phenomena akin to invalid preconditions, i.e., a selected low-level controller needed certain state constraints to be met before it was applicable. Similarly, how close were the failed plans to valid plans? Could you have turned them into valid plans by appending a few more actions? Were they moving the environment state closer to the goal state? Was the goal state achieved as an intermediate result and then changed by some future actions?\n\nLack of tests on GPT-4: One shortcoming of the current set of experiments is that it is missing any results that make use of GPT-4 as a planner, which is widely recognized as being one of the most powerful LLMs that are currently available. The reason cited is the unavailability of mechanisms to identify the probability associated with each possible next token. However, as I understand it, the basic planner uses a greedy approach to select the next most likely token. Each selected token is appended to the prompt, and the next token is selected. Isn\u2019t this similar to the basic autoregressive mechanism used by all LLMs, including GPT-4, especially when using low temperatures? You could have forced GPT-4 to only select tokens from the pool of potential skills by careful prompt engineering and some filtering. Even if the mechanism weren\u2019t exactly the same, this would have provided a useful point of comparison on how the state-of-the-art LLM models fair in these benchmark tasks.\n\n[1] Haslum, Patrik, et al. An introduction to the planning domain definition language. Vol. 13. San Rafael, California: Morgan & Claypool, 2019."
                },
                "questions": {
                    "value": "Please answer the questions raised under the weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2254/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698729622624,
            "cdate": 1698729622624,
            "tmdate": 1699636158513,
            "mdate": 1699636158513,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "RpOihxyNvX",
                "forum": "ADSxCpCu9s",
                "replyto": "KhfysXH4St",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2254/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2254/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your thorough review and constructive feedback. Your feedback provides valuable insights that we address in this comment.\n\n> **NL to API Component**\n\nIn both AI2THOR and VirtualHome simulators, directly executable API calls require object instances. For example, AI2THOR uses a format like {action=\u201cPickupObject\u201d, objectId=1}, and VirtualHome might use \u201c[grab] <coffeepot> (101)\u201d. We could design the skill set to include object IDs, such as S = {\u201c[grab] <coffeepot> (101)\u201d, \u2026, \u201c[open] <fridge> (10)\u201d}, enabling direct API calls. However, this method would substantially increase the skill set size, resulting in higher computational costs.\n\nTo mitigate these issues, we formulated each skill in natural language, without object instances. The NL to API component is responsible for translating these NL skills into executable API calls, selecting the nearest object ID for execution. This approach aligns with your suggestion of using \u201cmeaningful action names.\u201d\n\nFurthermore, employing NL skills also assists in avoiding simulator dependency, a crucial factor in maintaining system flexibility and generalizability. This strategy enables our system to adapt seamlessly across different simulation environments.\n\n> **Failure Reasons**\n\nWe have analyzed the failure cases in more detail based on your suggestion. We specifically examined the detailed reasons for the failures in the ALFRED results of GPT-3 (text-davinci-003) model, which showed the highest performance. For 162 failure cases, the authors manually categorized the failure reasons. We classified the failures into six categories.\n\n* 1) Failure of action planning (e.g., performing 'Pick' instead of 'Slice' at the step where a tomato need to be sliced)\n* 2) Failure of object selection (e.g., grabbing a pan instead of a pot)\n* 3) Failure due to absence of visual grounding (e.g., attempting to grab an object inside a drawer without opening it, failure to distinguish instances when multiple objects of the same kind are present)\n* 4) Lack of physical understanding (e.g., failure to put down an object because there are already other objects on the table)\n* 5) Failure in understanding user instructions (e.g., failure to differentiate whether the user-specified 'Lamp' refers to a desk lamp or a floor lamp)\n* 6) Ambiguous or incorrect user instructions (e.g., referring to 'Glass' in an instruction but the actual goal condition is 'Cup').\n\nThe results of the failure analysis are presented in the table below.\n\n| Failure category | Number of failures |\n|-------------|---------------------|\n| 1. Failure of action planning                        |  46  |\n| 2. Failure of object selection                    |  51  |\n| 3. Failure due to absence of visual grounding     |  21  |\n| 4. Lack of physical understanding                 |  15  |\n| 5. Failure in understanding user instructions     |  10  |\n| 6. Ambiguous or incorrect user instructions       |  19  |\n\n\nThis may serve as valuable information for understanding the causes of failures in the current benchmark. We found that the main reason of failure is high-level planning failures (1 and 2), which is about 60% of the total failure.\nVisual grounding and physical grounding (3 and 4) also matters and it is important future research direction of considering context grounding in high-level planning as we discussed in the conclusion and limitation section. Task instructions from human users might also be incomplete, unclear, or even incorrect (5 and 6). It would open a new research direction in which robot agents interactively request clarification of an ambiguous goal."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2254/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700563534025,
                "cdate": 1700563534025,
                "tmdate": 1700563534025,
                "mdate": 1700563534025,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xV88n6McBv",
                "forum": "ADSxCpCu9s",
                "replyto": "KhfysXH4St",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2254/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2254/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "(continued)\n\n> **Lack of tests on GPT-4**\n\nAs OpenAI provides only chat-style APIs for GPT-4 [1] unlike other base models such as GPT-3 (specifically, text-davinci-003), we were unable to directly compare GPT-4 in the same configuration. However, as you suggested, we aimed to assess GPT-4's performance, the state-of-the-art model, even with some modified experimental configurations. The following describes the configuration details and results of the GPT-4 experiments that we conducted.\n\n**GPT-4 Experiment Setting**\n\n1. Prompt\n\n    GPT-4 API has three agent roles of \u201csystem,\u201d \u201cuser,\u201d and \u201cassistant.\u201d We modified the original prompt (Listing 1 in the supplementary material) to fit these roles as follows. It worked like a prompt that gives the context of a problem and the role of the robot agent. The number of examples provided for in-context learning was the same. Examples, robot skills, and task query in square brackets [] were replaced with actual texts according to the test sample and environment.\n\n    ```\n    <System Role>\n    You are a robot operating in a home. A human user can ask you to do various tasks and you are supposed to tell the sequence of actions you would do to accomplish your task.\n    \n    <User Role>\n    Examples of human instructions and possible your (robot) answers:\n    [The same in-context-learning examples are provided]\n            \n    Now please answer the sequence of actions for the input instruction.\n    You should use one of actions of this list: [List of robot skills provided]\n    List the actions with comma separator.\n            \n    Input user instruction: [Task query]\n    ```\n\n2. Parsing a response\n\n    An assistant (robot) agent generates action sequences for the given user instruction. Due to the unavailability of Guidance\u2019s token selection mechanism [2] for GPT-4, we provided an admissible action list in the prompt to circumvent the issue.\n    \n    GPT-4 generated the answer for the given user instruction, and we extracted each step from the answer text. There was no additional post-processing tailored to GPT-4 (steps not in the provided list of the skills would not be executed correctly).\n\n\n**Results**\n\n| Model | ALFRED | WAH-NL |\n|-------------|---------------------|---------------------|\n| GPT-3 (text-davinci-003)  |  21.36%  |  40.82%  |\n| GPT-4                     |  40.38%  |   34.17%   |\n| GPT-3.5-Turbo             |  10.58%  |   28.82%   |\n\nWe tested GPT-4 and GPT-3.5-Turbo. GPT-3.5-Turbo showed lower success rates for both ALFRED and WAH-NL compared to GPT-3. GPT-4 performed well in ALFRED, showing a 40.38% success rate, a 19% improvement over GPT-3. However, in WAH-NL, GPT-4 demonstrated a lower success rate of 34.17% compared to GPT-3. \n\nDespite the limitations of the current setup, GPT-4's performance notably surpassed that of GPT-3 in ALFRED. This observation leads us to anticipate that the incorporation of Guidance\u2019s token selection mechanism [2] for GPT-4 could yield more impressive planning capabilities.\n\n\n[1] https://platform.openai.com/docs/guides/text-generation/chat-completions-api \\\n[2] https://github.com/guidance-ai/guidance\n\n---\n\nWe appreciate your valuable feedback, and we believe that addressing these points will contribute to the overall improvement of the paper. We will revise the paper to include the failure analysis and the GPT-4 results. If you have any further suggestions or remaining concerns, please do not hesitate to communicate them."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2254/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700563709891,
                "cdate": 1700563709891,
                "tmdate": 1700563709891,
                "mdate": 1700563709891,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2hRWapcA2o",
                "forum": "ADSxCpCu9s",
                "replyto": "KhfysXH4St",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2254/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2254/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking Forward to Your Feedback"
                    },
                    "comment": {
                        "value": "Dear Reviewer DjjY,\n\nThank you again for your considerable comments for our paper. We hope that our rebuttal has addressed your concerns. As the ICLR discussion deadline approaches, we kindly request your feedback on our submitted response. If you have any further questions or require additional clarification, please let us know.\n\nThank you very much for your time!\n\nBest wishes,\n\nThe Authors"
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2254/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700704003680,
                "cdate": 1700704003680,
                "tmdate": 1700704003680,
                "mdate": 1700704003680,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "eBMpiGjZ9k",
            "forum": "ADSxCpCu9s",
            "replyto": "ADSxCpCu9s",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2254/Reviewer_JeGM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2254/Reviewer_JeGM"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents LoTa-Bench, a benchmark for evaluating language task planners in home-service robots, using two datasets/simulators: ALFRED with AI2-THOR and Watch-And-Help with VirtualHome. It contributes a benchmarking suite, extensive testing of baseline planners with different models and prompts, exploration of planner enhancements, and the release of the benchmark code and an extended dataset to the public. The findings highlight the importance of model selection, in-context example strategies, replanning mechanisms, and model fine-tuning in improving planner performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. **Innovative Benchmark Development**: The paper introduces LoTa-Bench, an innovative benchmark suite for language-oriented task planners. This benchmark is critical for the field as it allows for the automatic and reproducible evaluation of task planning, which is a significant step forward in embodied AI research.\n\n2. **Comprehensive Experimental Analysis**: The authors provide extensive experimental results, which is a comprehensive demonstration of the benchmark's capabilities. They conduct experiments across various pre-trained models, demonstrating the benchmark's utility in evaluating the impact of model selection, in-context examples, replanning, and fine-tuning.\n\n3. **Resource Contribution and Future Research Facilitation**: The paper contributes valuable resources to the research community, including the benchmark code and an extended dataset. Moreover, the findings and the benchmark itself are positioned to facilitate future research, potentially accelerating advancements in the field of language-oriented task planning for embodied agents."
                },
                "weaknesses": {
                    "value": "1. **Decoupling of Planning Levels**: The research decouples high-level plans from low-level actions, focusing only on the former. This separation might not accurately reflect the complexities of end-to-end task planning where high-level decisions and low-level actions are interconnected.\n\n2. **Lack of Visual Understanding**: The benchmark does not incorporate visual understanding, which is critical for low-level actions, especially in embodied AI where egocentric views play a significant role in interpreting and interacting with the environment.\n\n3. **Domain Gap in Simulation**: There is a domain gap between the simulation environments used and the real world, leading to unrealistic assumptions. For example, the ALFRED simulator assumes an object is clean once it is placed in water, which may not always be true in real-world scenarios.\n\n4. Another limitation of the paper is the absence of an in-depth investigation into several influential factors that affect the performance of task planners. These factors include the type and size of pre-trained language models, the number and strategy for selecting in-context examples, the ability of planners to replan based on natural language feedback, and the impact of fine-tuning the models\u200b.  Addressing these factors is crucial for a thorough understanding of how to optimize task planners for real-world applications.\n\nMinor one: \nMissing references:\n- On Grounded Planning for Embodied Tasks with Language Models \n- Plan, Eliminate, and Track - Language Models are Good Teachers for Embodied Agents"
                },
                "questions": {
                    "value": "1. How does the decoupling of high-level plans from low-level actions in your benchmark reflect on the practical deployment of task planners? Could this lead to overlooking important dynamics that only emerge when both levels of planning are considered together?\n\n2. Given that AI2THOR and VirtualHome lack diversity to reflect real-world environments, could you elaborate on your plans to enhance these simulators or to incorporate additional platforms that might offer more realistic and diverse scenarios?\n\n3. How do you plan to address the domain gap between simulation and real-world application, and do you have strategies for dealing with unrealistic assumptions like the one mentioned in ALFRED about objects being cleaned once put into water?\n\n4. Can you discuss the potential development directions for evaluation frameworks that can handle the complexity of LLM-based task planning beyond simple tabletop manipulation tasks, as mentioned in the current limitation of the field?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2254/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698880227455,
            "cdate": 1698880227455,
            "tmdate": 1699636158366,
            "mdate": 1699636158366,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "LtQFjJeN8X",
                "forum": "ADSxCpCu9s",
                "replyto": "eBMpiGjZ9k",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2254/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2254/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your valuable feedback. We appreciate your recognition of the strengths in our paper, especially the innovative benchmark development, comprehensive experimental analysis, and the valuable resources contributed to the research community. We would like to respond to your comments on the weaknesses and questions.\n\n> **[W1] Decoupling of Planning Levels**\n\n> **[Q1] How does the decoupling of high-level plans from low-level actions in your benchmark reflect on the practical deployment of task planners? Could this lead to overlooking important dynamics that only emerge when both levels of planning are considered together?**\n\nDecomposing complex long-horizon tasks into multiple levels of planning is a widely used strategy. Consistent with this approach, our paper decouples high-level plans from low-level controls as you mentioned. We intentionally focused on high-level planning, particularly employing an LLM-based method. Our approach offers enhanced generalizability compared to end-to-end methods, which typically require extensive, task-specific data collection and training [1, 2, 3]. Our methodology is inherently more flexible, capable of adjusting to new environments without the need for additional data collection or exhaustive training.\n\nFurthermore, we recognize the importance of ensuring that our high-level planning seamlessly integrates with low-level controllers through natural language. Thanks to recent advancements in object navigation [4] and language-conditioned manipulation [5, 6], the NL steps generated by LLM-based task planners can be effectively executed.\n\n[1] Learning Neuro-Symbolic Skills for Bilevel Planning \\\n[2] Value Function Spaces: Skill-Centric State Abstractions for Long-Horizon Reasoning \\\n[3] Hierarchical Foresight: Self-Supervised Learning of Long-Horizon Tasks via Visual Subgoal Generation \\\n[4] A Survey of Embodied AI: From Simulators to Research Tasks \\\n[5] BC-Z: Zero-Shot Task Generalization with Robotic Imitation Learning \\\n[6] RT-1: Robotics Transformer for Real-World Control at Scale\n\n> **[W2] Lack of Visual Understanding**\n\nIn response to this concern, we wish to clarify that our LoTa-Bench is strategically designed to concentrate on high-level task planning, distinct from vision-based low-level control. The benchmarks\u2019s primary goal is to provide a clear and focused evaluation of language-oriented high-level task planning performance, thus intentionally minimizing potential confounding influences from low-level controls.\n\nHowever, we agree with your point regarding the importance of visual understanding. We are actively considering potential expansions to LoTa-Bench that would integrate visual information. This extension is designed not to exploring visual understanding for low-level control purposes but to enrich the capabilities of high-level planning. Potential directions include leveraging natural language feedback informed by visual information or integrating visual affordances into the planning process. Our objective with these future developments is to effectively bridge the gap between language-oriented task planning and visual perception.\n\n> **[W3] Domain Gap in Simulation**  \n\n> **[Q3] How do you plan to address the domain gap between simulation and real-world application, and do you have strategies for dealing with unrealistic assumptions like the one mentioned in ALFRED about objects being cleaned once put into water?**\n\nAs you pointed out, since LoTa-Bench evaluates planning performance in simulator environments, there is a domain gap with real world. The most prominent gap is ALFRED's assumption about cleaning. While other simulation tasks, such as picking and placing, heating using a microwave, and washing using a dishwasher, exhibit a smaller gap from real-world scenarios at a high-level planning stage. \n\nIn response to this, one potential approach to expand the proposed benchmark is to enhance the AI2THOR simulator used in ALFRED to reduce the domain gap. It may become more realistic by adding conditions, such as requiring the target object to be touched with a sponge, particularly addressing unrealistic gaps like cleaning, and incorporating this into the object status update logic in the simulator. Additionally, cleaning using a dishwasher, as supported in WAH-NL, could also be added. For example, instead of simply placing a fork in the water, we can add steps, placing fork inside a dishwasher, close the dishwasher, and turn on the machine."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2254/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700532008469,
                "cdate": 1700532008469,
                "tmdate": 1700532008469,
                "mdate": 1700532008469,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DpplhXFncb",
                "forum": "ADSxCpCu9s",
                "replyto": "eBMpiGjZ9k",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2254/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2254/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "(Continued)\n> **[W4] Another limitation of the paper is the absence of an in-depth investigation into several influential factors that affect the performance of task planners. ... Addressing these factors is crucial for a thorough understanding of how to optimize task planners for real-world applications.** \n\nOur paper indeed addresses each of the factors you have mentioned, using our proposed LoTa-Bench framework. The details are described in Section 5 and 6 of our manuscript.\n\n* **1) Impact of types and sizes of pre-trained language models**\n\n    In our work, we utilized the LoTa-Bench framework to evaluate and analyze the task planning performance of various pre-trained LLMs, including GPT [7], GPT-Neo series [8, 9, 10], OPT [11], MPT [12], LLaMA 1 [13], and LLaMA 2 [14], and their sizes. Our experimental results, summarized in Figure 2 of the paper, show that task planning performance generally increases with the size of the language model. Specifically, GPT-3 175B demonstrates the best performance on both ALFRED and WAH-NL. Further details are provided in:\n    * **Section 5.2**: \u201cWe evaluated the planning performance of the baseline planner described in Section 3. \u2026 Instruction- and chat-tuned models (dashed lines in Figure 2) did not perform better than their base models.\u201d and **Figure 2**\n    \n* **2) Influence of the number of in-context examples**\n\n    We explored the influence of the number of in-context examples using LLaMA 2 13B model. Our tests varied the number of examples from 0 to 30 in ALFRED and from 0 to 15 in WAH-NL. The results, depicted in Figure 3 of the paper, indicates that planning performance improves with an increased number of examples. More information can be found in:\n    * **Section 5.2**: \u201cWe have investigated the impact of the number of examples in prompt with LLaMA 2 13B model that supports a longer context length of 4096. \u2026 (with an average of 13.61 and a standard deviation of 3.22) for the LLaMA 2 13B model.\u201d and **Figure 3**\n    \n* **3) Strategy for selecting in-context examples**\n\n    We introduced three strategies (Random Sampling, Task-Specific Sampling, Semantic Similarity Sampling) for selecting in-context examples from the train set in Section 6.1 and investigated their impact on planning performance. Notably, Semantic Similarity Sampling significantly enhances planning performance. More details are available in Section 6.1 and Figure 5.\n    \n* **4) Planners\u2019 ability to replan based on natural language feedback**\n    \n    In Section 6.2, we discuss the effect of feedback and replanning. We observed that providing NL feedback about the reasons for action failure aids task planners in replanning when the size of language model is large. Further insights are available in Section 6.1 and Table 1.\n    \n* **5) Impact of fine-tuning on train set**\n\n    Our investigation into the impact of fine-tuning on the train set is discussed in Section 6.3. We employed LoRA [15] for fine-tuning LLMs, which resulted in significant performance improvements. This is elaborated upon in: Section 6.2 and Figure 6.\n\nWe would like to emphasize the critical role of our LoTa-Bench framework in facilitating these comprehensive evaluations. Without such a benchmark suite, conducting in-depth analyses of task planner performance across the factors you have highlighted would be more challenging. Our framework not only enables these detailed evaluations but also provides substantial support in advancing the field of task planning.\n\n[7] Language Models are Few-Shot Learners \\\n[8] GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow \\\n[9] GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model \\\n[10] Gpt-neox-20b: An open-source autoregressive language model \\\n[11] OPT: Open Pre-trained Transformer Language Models \\\n[12] Introducing mpt-7b: A new standard for open-source, commercially usable llms \\\n[13] LLaMA: Open and Efficient Foundation Language Models\\\n[14] Llama 2: Open Foundation and Fine-Tuned Chat Models \\\n[15] LoRA: Low-Rank Adaptation of Large Language Models\n\n> **[Q2] Given that AI2THOR and VirtualHome lack diversity to reflect real-world environments, could you elaborate on your plans to enhance these simulators or to incorporate additional platforms that might offer more realistic and diverse scenarios?**\n\nOur evaluation framework is designed to be extendable with different dataset and simulator pairs. For successful integration, the dataset must include natural language instructions and goal conditions, while the simulator should support high-level APIs. To enrich our framework with broader range of scenarios, we are actively considering the integration of additional platforms. Currently, we are examining the potential of incorporating Behavior-1K and OmniGibson simulator [16], or creating custom datasets within the NVIDIA Isaac Sim simulator.\n\n[16] BEHAVIOR-1K: A Benchmark for Embodied AI with 1,000 Everyday Activities and Realistic Simulation"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2254/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700532102768,
                "cdate": 1700532102768,
                "tmdate": 1700532102768,
                "mdate": 1700532102768,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ayZME4jrnc",
                "forum": "ADSxCpCu9s",
                "replyto": "eBMpiGjZ9k",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2254/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2254/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "(Continued)\n> **[Q4] Can you discuss the potential development directions for evaluation frameworks that can handle the complexity of LLM-based task planning beyond simple tabletop manipulation tasks, as mentioned in the current limitation of the field?**\n\nIn order to address the complexity of LLM-based task planning beyond simple tabletop manipulation tasks, it is essential to develop datasets and simulators for a variety of scenarios that focus on long-horizon tasks, similar to LoTa-Bench. The datasets we used in the present paper have 92 different home scenes in ALFRED (valid set) and 100 in WAH-NL (test set). The environment spans diverse rooms including bedrooms, kitchens, bathrooms, and living rooms. And we would like to mention that our benchmarking includes long-horizon tasks, which requires more than 25 action steps.\n\n> **Minor one: Missing references**\n\nThank you for your pointing out these references. The first paper introduced a method for encoding an environment in the form of an object table to provide context to a language model-based planner. The second paper enhances LLM-based task planner with additional modules, eliminating irrelevant objects and receptacles from observation and tracking the completion of each step. Both works contribute to the field of LLM-based task planning by focusing on integrating contextual information to improve planning performance. We plan to add both papers to the Related Work section of our manuscript as follows:\n\u201cMoreover, integrating context into LLM-based task planners has been shown to enhance planning efficacy (Huang et al., 2023; Yao et al., 2023; Chen et al., 2023; Lin et al., 2023; Wu et al., 2023)\u201d\n\n---\n\nWe thank you again for your valuable feedback, and we believe that addressing these points will contribute to clarifying the contribution of the paper and guiding the direction of our research expansion. If you have any further suggestions or concerns, feel free to communicate them."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2254/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700532135391,
                "cdate": 1700532135391,
                "tmdate": 1700532135391,
                "mdate": 1700532135391,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KnXUotfdxZ",
                "forum": "ADSxCpCu9s",
                "replyto": "eBMpiGjZ9k",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2254/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2254/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking Forward to Your Feedback"
                    },
                    "comment": {
                        "value": "Dear Reviewer JeGM,\n\nThank you again for your considerable comments for our paper. We hope that our rebuttal has addressed your concerns. As the ICLR discussion deadline approaches, we kindly request your feedback on our submitted response. If you have any further questions or require additional clarification, please let us know.\n\nThank you very much for your time!\n\nBest wishes,\n\nThe Authors"
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2254/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700703967035,
                "cdate": 1700703967035,
                "tmdate": 1700703967035,
                "mdate": 1700703967035,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "9fCL4WI3PO",
            "forum": "ADSxCpCu9s",
            "replyto": "ADSxCpCu9s",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2254/Reviewer_JjuX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2254/Reviewer_JjuX"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes LoTa-Bench, a benchmark system to automatically evaluate the performance of task planning for home-service embodied agents using large language models (LLMs) across a variety of datasets and simulators. The authors conduct extensive experiments with a baseline planner and several of its extensions within this benchmark framework."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "- The paper is well-written and structured.\n- The authors test multiple pre-trained LLMs (GPT, GPT-Neo series, LLaMA, OPT,MPT) on their benchmark.\n- The authors present results on not just a baseline planner but also other extensions (such as feedback incorporation etc) on their benchmark."
                },
                "weaknesses": {
                    "value": "The primary contribution of the paper seems limited to the integration of several existing benchmarks into a single platform. There are existing works, as cited in the Related Work section, that already employ benchmarks like ALFRED and VirtualHome to test various planning and execution techniques. The paper does not make it clear how LoTa-Bench provides additional value over these existing benchmarks. It appears that the significant novel contribution is the extension of the WAH dataset and its adaptation for autonomous agents."
                },
                "questions": {
                    "value": "- Why was there no fine-tuning conducted with the WAH-NL dataset?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2254/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2254/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2254/Reviewer_JjuX"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2254/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699520905921,
            "cdate": 1699520905921,
            "tmdate": 1700736999986,
            "mdate": 1700736999986,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "tZUjZbBj0N",
                "forum": "ADSxCpCu9s",
                "replyto": "9fCL4WI3PO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2254/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2254/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your valuable feedback. We would like to respond to your comments on the weaknesses and questions.\n\n> **The primary contribution of the paper seems limited to the integration of several existing benchmarks into a single platform. ... The paper does not make it clear how LoTa-Bench provides additional value over these existing benchmarks.**\n\nFirst, we clarify the differences between LoTa-Bench and existing benchmarks.\n\n* **1) LoTa-Bench vs. ALFRED + AI2THOR**\n\n    ALFRED [1] built on AI2THOR [2] has both high-level and low-level instructions. One example is as follows.\n      \n    ```\n    Task instruction: \"Move a towel to the sink\"\n    Step-by-step action instruction:\n    1. Move to the far left cupboard under the far left sink\n    2. Open the cupboard and remove the towel inside, shut the cupboard\n    3. Carry the towel to the left sink above the cupboard\n    4. Place the towel in the sink\n    ```\n    \n    As low-level instructions in natural language are given, the previous studies [3, 4] on ALFRED mostly focused on translating step instructions into robot actions with visual understanding, which overlooks the importance of high-level planning. Thus, we proposed LoTa-Bench designed to focus on high-level task planning by using only task instructions; no step-by-step instructions were given and no training on the dataset unlike previous studies to validate generalizability of the high-level task planner. Although we did not augment the ALFRED dataset itself, we utilized the dataset in a new way to propose a new benchmark problem to help the research community validate new task planning methods more easily and objectively.\n\n* **2) LoTa-Bench vs. ActivityPrograms \\& Watch-and-Help + VirtualHome**\n \n    The ActivityPrograms dataset [5] provides natural language instructions, but it lacks goal conditions. Some language-oriented task planning studies that used this dataset relied on human evaluation for quantitative performance evaluation [6, 7]. LoTa-Bench, however, includes datasets with goal conditions, enabling automated quantitative performance measurement.\n    \n    The Watch-and-Help dataset [8] incorporates goal conditions but not NL instructions, limiting its use for language-oriented task planning. LoTa-Bench adapts the Watch-and-Help dataset for language-oriented task planning by modifying the goal conditions for autonomous agents and enriching it with NL instructions that we collected from crowdworkers.\n\nBeyond providing benchmark suites, our comprehensive experimental analysis illuminates factors influencing LLM-based task planner performance, such as model size, type, and the number of in-context examples. We also investigate performance-enhancing extensions for LLM-based task planners within LoTa-Bench, including in-context example selection, feedback and replanning, and fine-tuning.\n\nTogether, we believe that these contributions uniquely position LoTa-Bench as a valuable asset in language-oriented task planning research.\n\n[1] Alfred: A benchmark for interpreting grounded instructions for everyday tasks \\\n[2] Ai2-thor: An interactive 3d environment for visual ai \\\n[3] Episodic Transformer for Vision-and-Language Navigation \\\n[4] Agent with the big picture: Perceiving surroundings for interactive instruction following \\\n[5] Virtualhome: Simulating household activities via programs. \\\n[6] Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents \\\n[7] Parsel : Algorithmic Reasoning with Language Models by Composing Decompositions \\\n[8] Watch-and-help: A challenge for social perception and human-ai collaboration"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2254/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700394351688,
                "cdate": 1700394351688,
                "tmdate": 1700394351688,
                "mdate": 1700394351688,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3zoH0Xtbjf",
                "forum": "ADSxCpCu9s",
                "replyto": "9fCL4WI3PO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2254/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2254/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "(countinued)\n\n> **Why was there no fine-tuning conducted with the WAH-NL dataset?**\n\nIn our initial approach, the primary goal of fine-tuning was to demonstrate the capability of our benchmark in facilitating a broad range of experiments for LLM-based planning. Consequently, we chose to present fine-tuning results solely within the ALFRED domain, considering it to be sufficiently illustrative of our benchmark's efficacy. Additionally, the WAH-NL domain offered a limited dataset of only 250 instances for fine-tuning, which we initially deemed inadequate for achieving significant performance improvements.\n\nNevertheless, inspired by your query, we have now undertaken a fine-tuning experiment using the WAH-NL dataset. This dataset comprises 250 training instances, and we maintained consistency in hyper-parameters and conditions as per our ALFRED dataset experiments. The outcomes are detailed in the table below.\n\n**Table 1. Plan execution success rates of planners fine-tuned on the WAH-NL dataset. \u03b4 denotes changes in success rates with and without fine-tuning (expressed in percentage points, pp).**\n|             | Baseline Success(%) | Fine-tuned Success (%) | \u03b4 (pp) |\n|-------------|---------------------|------------------------|-------------|\n| LLaMA 1 7B  | 28.65               | 21.36                  | -7.34       |\n| LLaMA 1 13B | 23.70               | 29.47                  | +5.77       |\n| LLaMA 1 30B | 32.22               | 33.78                  | +1.56       |\n\nAlthough the improvements were modest and not consistent across all models, fine-tuning with the LLaMA 1 13B and 30B models did exhibit some performance enhancements (5.77 and 1.56 percent points).\n\nThe entire process, excluding the time spent on fine-tuning the models, took less than 7 hours, and the evaluation was fully automated using the proposed LoTa-Bench. We are confident that our benchmark suite can significantly accelerate research on various aspects of LLM-based planners, offering an efficient and versatile evaluation tool for the research community.\n\n---\n\nWe thank you again for your valuable feedback, and we believe that addressing these points will contribute to clarifying the contribution of the paper. If you have any further suggestions or concerns, feel free to communicate them."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2254/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700394413460,
                "cdate": 1700394413460,
                "tmdate": 1700394413460,
                "mdate": 1700394413460,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zmWOabTteN",
                "forum": "ADSxCpCu9s",
                "replyto": "9fCL4WI3PO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2254/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2254/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking Forward to Your Feedback"
                    },
                    "comment": {
                        "value": "Dear Reviewer JjuX,\n\nThank you again for your considerable comments for our paper. We hope that our rebuttal has addressed your concerns. As the ICLR discussion deadline approaches, we kindly request your feedback on our submitted response. If you have any further questions or require additional clarification, please let us know.\n\nThank you very much for your time!\n\nBest wishes,\n\nThe Authors"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2254/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700703895757,
                "cdate": 1700703895757,
                "tmdate": 1700703895757,
                "mdate": 1700703895757,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BlVIcmmgV5",
                "forum": "ADSxCpCu9s",
                "replyto": "3zoH0Xtbjf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2254/Reviewer_JjuX"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2254/Reviewer_JjuX"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks a lot for your response! Most of my concerns have been addressed and I have updated the score of the paper. I hope that the authors emphasize on their contributions more clearly in the paper (for example, by juxtaposing the existing benchmarks and the additions/integrations that have been done on top of them in the introduction section itself)."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2254/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700736985763,
                "cdate": 1700736985763,
                "tmdate": 1700736985763,
                "mdate": 1700736985763,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]