[
    {
        "title": "Revisiting Knowledge Tracing: A Simple and Powerful Model"
    },
    {
        "review": {
            "id": "jHX7TAkNnc",
            "forum": "vZEgj0clDp",
            "replyto": "vZEgj0clDp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2470/Reviewer_RjCq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2470/Reviewer_RjCq"
            ],
            "content": {
                "summary": {
                    "value": "This paper aims to address the knowledge state representation and the core architecture design challenges of knowledge tracing (KT). To this end, the authors propose the ReKT model. They first take inspiration from the decision-making process of human teachers and propose the knowledge state of students from three different perspectives. Then, the authors propose a Forget-Response-Update (FRU) framework as the core architecture for the KT task. They finally demonstrate the effectiveness of their model in terms of efficiency in computing and effectiveness in score prediction through experiments on 7 public real datasets. Their experimental results show that their proposed method can reach the best performance in the question-based KT task and the best/near-best performance in the concept-based KT task, and their proposal only requires 38% computing resources compared to other KT core architectures."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper introduces a multi-perspective approach to modeling the knowledge state of students, considering questions, concepts, and domains, which is logically self-consistent.\n\n2. The FRU framework designed as the core architecture of ReKT is lightweight yet effective. According to experiments, ReKT can achieve competitive performance with significantly fewer parameters and computing resources compared to other core architectures.\n\n3. The experimental results demonstrate the superior performance of ReKT in question-based KT tasks and its competitive performance in concept-based KT tasks, showcasing the effectiveness of the proposed model."
                },
                "weaknesses": {
                    "value": "1. In terms of the methodology, the authors did not provide theoretical analysis about the spatial-temporal complexity of FRU. I hope the authors append such analysis to make the efficiency of their proposal in terms of computing resource more persuasive.\n\n2. In terms of experiment, the authors only presented results in score prediction and computational resource cost. However, as the goal of the KT task is not only to predict students\u2019 score sequences, but also to track the dynamic change of their knowledge states. Therefore, it will be helpful if the authors append such experiments (e.g., case study and visualization of student knowledge states) and use them to explain how their proposal can model student knowledge states better"
                },
                "questions": {
                    "value": "1. Can you further explain the design and workflow of the FRU framework? For example, what is the connection between FRU and human cognitive development models, what are inputs, what are learnable parameters and what are outputs? Besides, what does $I_\\alpha$ mean in Section 3.4? I cannot find it on Figure 3.\n\n2. In terms of sequence modeling, RNN and LSTM are also simple but effective. What are the advantages of FRU compared to them, especially in the context of knowledge tracing? Does your proposed FRU have the potential to be applied to other areas except for knowledge tracing (e.g., sequential recommendation)?\n\n3. There are some syntax and spelling errors need to be solved, such as the index \u201cI\u201d in the formula of loss function, $Loss_{KT}$. I guess it should be replaced with $t$."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2470/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698556114220,
            "cdate": 1698556114220,
            "tmdate": 1699636183438,
            "mdate": 1699636183438,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "OVZfeMrFNd",
                "forum": "vZEgj0clDp",
                "replyto": "jHX7TAkNnc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2470/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2470/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer RjCq"
                    },
                    "comment": {
                        "value": "Thank you for your detailed evaluation of our manuscript and for raising several insightful points. We appreciate the opportunity to address your concerns.\n\n**Q1**: \u201cIn terms of the methodology, the authors did not provide theoretical analysis about the spatial-temporal complexity of FRU. I hope the authors append such analysis to make the efficiency of their proposal in terms of computing resource more persuasive.\u201d\n\n**A1**: Thank you very much for your suggestions! In Appendix G of this paper, we have added an analysis of the time and space complexity of FRU, along with the number of its learnable parameters.\n\nOnce again, we sincerely appreciate your valuable advice!\n\n**Q2**: \u201cIn terms of experiment, the authors only presented results in score prediction and computational resource cost. However, as the goal of the KT task is not only to predict students\u2019 score sequences, but also to track the dynamic change of their knowledge states. Therefore, it will be helpful if the authors append such experiments (e.g., case study and visualization of student knowledge states) and use them to explain how their proposal can model student knowledge states better\u201d\n\n**A2**: Heartfelt thanks for your suggestions! We have added a case study comparing ReKT and other model for tracing students\u2019 knowledge state in Appendix I, and analyzed the advantages brought by ReKT.\n\nOnce again, we sincerely appreciate your valuable advice!\n\n**Q3**: \u201cCan you further explain the design and workflow of the FRU framework? For example, what is the connection between FRU and human cognitive development models, what are inputs, what are learnable parameters and what are outputs? Besides, what does $I_\\alpha$ mean in Section 3.4? I cannot find it on Figure 3.\u201d\n\n**A3**: Thank you for your more in-depth questions. We have provided a detailed response to the question about FRU in the \"General Response about FRU.\" It includes the motivation behind designing FRU, inspiration for its design, the process of FRU, and the differences and advantages of FRU compared to MLP, RNN, LSTM, and GRU.\n\nFurthermore, for FRU, its input consists of the student's historical interaction sequence (corresponding to $X_t$ in Figure 3). The learning parameters of FRU are the parameters within the Update and Forget components in Figure 3 (parameters $W_2$ and $b_2$ in Update, and parameters $W_1$ and $b_1$ in Forget). Its output is $Response_t$.\n\nThank you very much for your thorough review. $I_\\alpha$ represents the embedded representation of time interval $\\alpha$ (For example, the time interval from the last time $\\alpha=2$, assuming there is a time embedding matrix $I \\in \\mathbb{R}^{T \\times d}$, $T$ is the total time step, $d$ is the embedding dimension, then $I_\\alpha=I_2$ (that is, the $\\alpha$-th row of $I$)).\n\nWe have revised this detail in figure 3.($\\alpha -> I_\\alpha$)\n\n**Q4**: \u201cIn terms of sequence modeling, RNN and LSTM are also simple but effective. What are the advantages of FRU compared to them, especially in the context of knowledge tracing? Does your proposed FRU have the potential to be applied to other areas except for knowledge tracing (e.g., sequential recommendation)?\u201d\n\n**A4**: Thank you for your insightful inquiry. We have provided a detailed response to the question about FRU in the \"General Response about FRU.\" It includes the motivation behind designing FRU, inspiration for its design, the process of FRU, and the differences and advantages of FRU compared to MLP, RNN, LSTM, and GRU.\n\nCurrently, we have not yet explored the application of FRU in other domains. However, given that FRU is inspired by models of human cognitive development, we believe there is theoretical potential for its application in domains related to modeling implicit states in humans, such as interests, health status, and more. The field you mentioned, sequential recommendation, is one that we consider to be a promising direction for potential application. Once again, we appreciate your insightful questions.\n\n**Q5**: \u201cThere are some syntax and spelling errors need to be solved, such as the index \u201cI\u201d in the formula of loss function, $Loss_{KT}$. I guess it should be replaced with $t$\u201d \n\n**A5**: Thank you very much for your careful review. We have corrected this error in the paper. \n\nThanks again for reviewing and your valuable suggestions!"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2470/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700104451252,
                "cdate": 1700104451252,
                "tmdate": 1700109157125,
                "mdate": 1700109157125,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RlTU7We1D8",
                "forum": "vZEgj0clDp",
                "replyto": "jHX7TAkNnc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2470/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2470/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Authors"
                    },
                    "comment": {
                        "value": "Dear reviewer RjCq,\n\nAs the window for reviewer-author interaction is closing soon, I wanted to extend my sincerest gratitude for the invaluable time and effort you have dedicated to reviewing our work. To ensure that we have met your expectations, may I kindly ask if you find our responses satisfactory and if there are any remaining issues that need further clarification or improvement?"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2470/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700569499604,
                "cdate": 1700569499604,
                "tmdate": 1700569499604,
                "mdate": 1700569499604,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "iIiHBuyFI7",
                "forum": "vZEgj0clDp",
                "replyto": "jHX7TAkNnc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2470/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2470/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Appreciation and Inquiry"
                    },
                    "comment": {
                        "value": "We appreciate the valuable contributions you have made to our manuscript. As the rebuttal phase is about to close, we would like to inquire once again if we have adequately addressed your concerns and if you would be willing to consider improving our score. \n\nOnce again, we sincerely appreciate the valuable time you dedicated to evaluating our work."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2470/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700703720964,
                "cdate": 1700703720964,
                "tmdate": 1700703720964,
                "mdate": 1700703720964,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "uqXzUeTkHa",
            "forum": "vZEgj0clDp",
            "replyto": "vZEgj0clDp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2470/Reviewer_UX1Z"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2470/Reviewer_UX1Z"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a novel approach to Knowledge Tracing (KT) using the Forget-Response-Update (FRU) framework. KT is essential in online education systems for assessing and predicting student performance based on their interactions with educational content. \n\nThe FRU framework, designed based on human cognitive development models, stands out due to its lightweight nature, consisting of just two linear regression units. The proposed model, named ReKT, was extensively compared with 22 state-of-the-art KT models across 7 public datasets. Results demonstrated that ReKT consistently outperformed other methods, especially in question-based KT tasks. In concept-based KT tasks, an adapted version of ReKT, termed ReKT-concept, achieved top or near-top performance across datasets. \n\nFurthermore, despite its simplicity, the FRU framework required only about 38% of the computing resources of other architectures like Transformers or LSTMs, showcasing its efficiency. The paper underscores the effectiveness, scalability, and efficiency of the FRU design in the realm of Knowledge Tracing."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The introduction of the Forget-Response-Update (FRU) framework offers a fresh perspective in the realm of Knowledge Tracing. While many models in the literature focus on complex architectures, the FRU's simplicity, relying on just two linear regression units, stands out as a unique proposition. The research brings a blend of cognitive learning principles and machine learning, fostering a more holistic approach to Knowledge Tracing.\n\nThe empirical evaluation of the proposed ReKT model is thorough. By benchmarking against 22 state-of-the-art KT models across 7 public datasets, the authors ensure a comprehensive assessment of their model's performance. The paper's methodological rigor is evident in the detailed descriptions of the FRU framework, the equations used, and the training methodologies employed.\n\nThe paper is well-structured, with distinct sections dedicated to introducing the problem, presenting the methodology, showcasing results, and discussing implications. The inclusion of figures, tables, and illustrative examples enhances the reader's understanding and provides a visual representation of the model's performance and capabilities."
                },
                "weaknesses": {
                    "value": "The core of the proposed Forget-Response-Update (FRU) framework seems to be composed of two linear regression units. If this can be easily mirrored or replicated using two multi-layer perceptrons (MLPs), then the novelty of the FRU framework can be challenged. A deeper exploration or comparison with simple neural architectures, like MLPs, would provide clarity on the unique advantages of the FRU.\n\nThe use of terminology like \"Forget\", \"Response\", and \"Update\" in naming the modules of the FRU framework may imply distinct, targeted functionalities. However, in complex learning scenarios, such naming conventions can be misleading. In intricate neural architectures, a module named \"Forget\" might not necessarily perform a straightforward forgetting operation but might instead learn a more nuanced or intermediate representation. Over-reliance on such naming can lead to misconceptions about the actual functions and complexities of the modules, especially for those looking to adapt or build upon the framework.\n\nWhile the lightweight nature of the FRU is emphasized, there's limited exploration on how the FRU can be integrated into or combined with deeper or more complex neural network architectures."
                },
                "questions": {
                    "value": "How does the Forget-Response-Update (FRU) framework differ fundamentally from a structure consisting of two multi-layer perceptrons (MLPs)? What advantages does the FRU bring over a simple MLP setup?\n\nGiven the naming conventions like \"Forget\", \"Response\", and \"Update\", can you provide deeper insights into the exact functionalities and representations learned by each module during complex learning schedules?\n\nHow does the FRU framework integrate into more complex neural network architectures? Have there been experiments or considerations in this direction?\n\nThe paper mentioned that the FRU requires only about 38% of the computing resources compared to architectures like Transformers. Could you delve deeper into the parameter distribution within the FRU? Which module (Forget, Response, Update) consumes the most parameters?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2470/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698807180101,
            "cdate": 1698807180101,
            "tmdate": 1699636183329,
            "mdate": 1699636183329,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "k7XR2uEPYm",
                "forum": "vZEgj0clDp",
                "replyto": "uqXzUeTkHa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2470/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2470/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer UX1Z(1/2)"
                    },
                    "comment": {
                        "value": "Thank you for your detailed evaluation of our manuscript and for raising several insightful points. We appreciate the opportunity to address your concerns.\n\n**Q1**: \u201cThe core of the proposed Forget-Response-Update (FRU) framework seems to be composed of two linear regression units. If this can be easily mirrored or replicated using two multi-layer perceptrons (MLPs), then the novelty of the FRU framework can be challenged. A deeper exploration or comparison with simple neural architectures, like MLPs, would provide clarity on the unique advantages of the FRU. How does the Forget-Response-Update (FRU) framework differ fundamentally from a structure consisting of two multi-layer perceptrons (MLPs)? What advantages does the FRU bring over a simple MLP setup?\u201d\n\n**A1**: Thank you very much for your suggestion.FRU is a sequence model that involves forgetting and updating the knowledge state at each time step, similar to a recurrent neural network (RNN). On the other hand, MLP is a model for processing structured data.\n\nFRU is characterized by two linear regression units, and replacing these units with MLP or Attention, among other options, is possible (similar to replacing the four gates of an LSTM with four MLPs). However, we want to explain that the key to FRU is not the two linear regression units (it just illustrates the lightweight characteristics of FRU), but the ideas reflected in F, R, and U of FRU. We have provided a detailed response to the question about FRU in the \"General Response about FRU.\" It includes the motivation behind designing FRU, inspiration for its design, the process of FRU, and the differences and advantages of FRU compared to MLP, RNN, LSTM, and GRU.\n\nIn addition, during the experiment, we tried to replace the two linear regression units with two MLPs, and the performance is shown in the table below.\n\n| | ASSIST09| ASSIST09|ASSIST12|ASSIST12|ASSIST17|ASSIST17|Statics2011|Statics2011|EdNet|EdNet|Eedi|Eedi|\n| :----: | :----: | :----: | :----: | :----: | :----: | :----: | :----: | :----: | :----: | :----: | :----: | :----: |\n| **Evaluate**| AUC| ACC |AUC| ACC |AUC| ACC |AUC| ACC |AUC| ACC |AUC| ACC |\n| ReKT(MLP) | 0.7912| 0.7438|0.7852|0.7612|0.7818|0.7108|0.8972|0.8563|0.7768|0.7437|0.7972|0.7396|\n| ReKT    | 0.7917| 0.7449\t     |0.7852|0.7609|0.7814|0.7102|0.8967|0.8568|0.7752|0.7447|0.7971|0.7397|\n\nIt can be observed that whether using linear regression units or MLPs, the performance is nearly equivalent. This indicates that simple linear regression units are sufficient. Considering the spirit of simplifying the model as much as possible in this paper, we chose to use linear regression units.\n\nOnce again, we sincerely appreciate your advice.\n\n**Q2**: \u201cThe use of terminology like \"Forget\", \"Response\", and \"Update\" in naming the modules of the FRU framework may imply distinct, targeted functionalities. However, in complex learning scenarios, such naming conventions can be misleading. In intricate neural architectures, a module named \"Forget\" might not necessarily perform a straightforward forgetting operation but might instead learn a more nuanced or intermediate representation. Over-reliance on such naming can lead to misconceptions about the actual functions and complexities of the modules, especially for those looking to adapt or build upon the framework.\u201d\n\n**A2**: Thank you for your reminder. When naming FRU, we drew inspiration from the nomenclature of neural networks such as LSTM, GRU, as well as common module names in knowledge tracing methods like LPKT[1] and LBKT[2], such as \"forget gate\" and \"update gate.\" Therefore, we named FRU as Forget-Response-Update. The issue you pointed out is indeed meaningful and may be a common challenge that deep learning methods find difficult to address. In future research, we plan to delve into the mechanistic roles and interdependencies of Forget-Response-Update in FRU. This promises to be an intriguing and vital area of exploration. \n\nOnce again, we appreciate your insights.\n\n[1] Shuanghong Shen, Qi Liu, Enhong Chen, Zhenya Huang, Wei Huang, Yu Yin, Yu Su, and Shijin Wang. 2021. Learning process-consistent knowledge tracing. In Proceedings of the 27th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. Association for Computing Machinery, 1452\u20131460.\n\n[2] Bihan Xu, Zhenya Huang, Jiayu Liu, Shuanghong Shen, Qi Liu, Enhong Chen, Jinze Wu, and Shijin Wang. 2023. Learning behavior-oriented knowledge tracing. In Proceedings of the 29th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. Association for Computing Machinery, 2789\u20132800."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2470/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700105356855,
                "cdate": 1700105356855,
                "tmdate": 1700105356855,
                "mdate": 1700105356855,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TPWS0Hqvyc",
                "forum": "vZEgj0clDp",
                "replyto": "uqXzUeTkHa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2470/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2470/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer UX1Z(2/2)"
                    },
                    "comment": {
                        "value": "**Q3**: \u201cGiven the naming conventions like \"Forget\", \"Response\", and \"Update\", can you provide deeper insights into the exact functionalities and representations learned by each module during complex learning schedules?\u201d\n\n**A3**: Thank you for your further inquiry. In our design, for the forgetting process, we employ a linear forgetting model:\n\n$Response_t=f_t*Z_{t-\\alpha}$\n\nwhere we reduce the state of knowledge accordingly, with $f_t \\in [0,1]$. On the other hand, the updating process is described by an incremental model\uff1a\n\n$Z_t=Response_t+Tanh([Response_t \\oplus X_t ]W_2+b_2)$\n\nThis is used to model the changes in $Response_t$ after being stimulated by $X_t$. In our model design, we aimed to reference and emulate models of human cognitive development as much as possible.\n\nHowever, in the actual operational process of the model, the exact functions and representations learned by each module of FRU in complex learning scenarios may require the assistance of model interpretability methods to obtain clearer descriptions. Currently, we have not undertaken relevant work in this area, but we believe that a thorough analysis of the mechanisms and interdependencies of Forget-Response-Update in FRU will be an interesting and important topic.\n\n**Q4**: \u201cWhile the lightweight nature of the FRU is emphasized, there's limited exploration on how the FRU can be integrated into or combined with deeper or more complex neural network architectures. How does the FRU framework integrate into more complex neural network architectures? Have there been experiments or considerations in this direction?\u201d\n\n**A4**: Thank you for your more in-depth question. Currently, we have not undertaken the research of integrating FRU into more complex models. However, integrating FRU into other methods or expanding FRU is easy to implement. Specifically, FRU takes a historical sequence as input and outputs the student's current knowledge state, making it easy to use as a component in stacked models.\n\nFurthermore, expanding FRU is also simple. As mentioned in your earlier question, it is entirely feasible to replace FRU's two linear regression units with two MLPs. At the same time, FRU considers the time factor of forgetting by embedding the interval time and then transforming it. To extend FRU, one can explore modifications such as adjusting the computation method for the time factor in forgetting to incorporate exponential time decay [3].\n\n**Q5**: \u201cThe paper mentioned that the FRU requires only about 38% of the computing resources compared to architectures like Transformers. Could you delve deeper into the parameter distribution within the FRU? Which module (Forget, Response, Update) consumes the most parameters?\u201d\n\n**A5**: Thank you very much for your suggestions! In Appendix G of the paper, we have added an analysis of the time and space complexity of FRU, along with the number of its learnable parameters.\n\nFor convenience, assuming a consistent hidden layer dimension of $d$ in FRU, the number of learnable parameters for the forgetting module is $2d^2+d$, for the updating module is $2d^2+d$, and the response module has no learnable parameters.\n\n\nThanks again for reviewing and your valuable suggestions!\n\n[3] Aritra Ghosh, Neil Heffernan, and Andrew S Lan. 2020. Context-aware attentive knowledge tracing. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. Association for Computing Machinery, 2330\u20132339."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2470/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700105845542,
                "cdate": 1700105845542,
                "tmdate": 1700105845542,
                "mdate": 1700105845542,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hNeHtWKQT7",
                "forum": "vZEgj0clDp",
                "replyto": "uqXzUeTkHa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2470/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2470/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Authors"
                    },
                    "comment": {
                        "value": "Dear reviewer UX1Z,\n\nAs the window for reviewer-author interaction is closing soon, I wanted to extend my sincerest gratitude for the invaluable time and effort you have dedicated to reviewing our work. To ensure that we have met your expectations, may I kindly ask if you find our responses satisfactory and if there are any remaining issues that need further clarification or improvement?"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2470/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700569470431,
                "cdate": 1700569470431,
                "tmdate": 1700569470431,
                "mdate": 1700569470431,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4PCULNPk2z",
                "forum": "vZEgj0clDp",
                "replyto": "uqXzUeTkHa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2470/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2470/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Appreciation and Inquiry"
                    },
                    "comment": {
                        "value": "We appreciate the valuable contributions you have made to our manuscript. As the rebuttal phase is about to close, we would like to inquire once again if we have adequately addressed your concerns and if you would be willing to consider improving our score. \n\nOnce again, we sincerely appreciate the valuable time you dedicated to evaluating our work."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2470/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700703704384,
                "cdate": 1700703704384,
                "tmdate": 1700703704384,
                "mdate": 1700703704384,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "mHK0570CBz",
            "forum": "vZEgj0clDp",
            "replyto": "vZEgj0clDp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2470/Reviewer_5ixm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2470/Reviewer_5ixm"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents an improvement of the deep knowledge tracing (DKT) algorithm, ReKT. The authors revisited the DKT algorithm to design it from three perspectives.: 1)question: whether the question was attempted before, 20 concept: performance on questions with similar concepts, and 3) the entire trajectory. \nEmpirical results demonstrate the superior performance of ReKT compared to other variations of DKT."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Superior performance while 38% less resource usage"
                },
                "weaknesses": {
                    "value": "1. The paper employs similar approaches to previous DKT methods, such as RAKT [1], AKT [2], and [3]  except for the FRU unit. All of three papers also implemented the FRU unit with exponential time decay as part of the attention mechanism in the transformer architecture. The authors have used MLP units as FRU and concatenated the hidden state to the final representations.\n\n2. The authors did not provide any interpretations of the model's performance---which is very important in educational settings for both students' and teachers' perspectives. From a student's perspective, interpretation can help in recommending learning materials. From a teacher's perspective, it can be helpful to identify which questions or concepts students are struggling with.\n\n\nReferences.\n1. Pandey, S. and Srivastava, J., 2020, October. RKT: relation-aware self-attention for knowledge tracing. In Proceedings of the 29th ACM International Conference on Information & Knowledge Management (pp. 1205-1214).\n2. Ghosh A, Heffernan N, Lan AS. Context-aware attentive knowledge tracing. InProceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining 2020 Aug 23 (pp. 2330-2339).\n3. Farhana E, Rutherford T, Lynch CF. Predictive Student Modelling in an Online Reading Platform. InProceedings of the AAAI Conference on Artificial Intelligence 2022 Jun 28 (Vol. 36, No. 11, pp. 12735-12743)"
                },
                "questions": {
                    "value": "The Rasch difficulty is determined from students' question and response binary matrix.\n\nAs the authors have three different representations of question interactions, did the authors compute the Rasch difficulty from three different interaction matrices?\n\nHow did the authors handle multiple submissions for computing the Rash difficulty?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2470/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2470/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2470/Reviewer_5ixm"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2470/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698875264400,
            "cdate": 1698875264400,
            "tmdate": 1699636183237,
            "mdate": 1699636183237,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "W3ILfErwky",
                "forum": "vZEgj0clDp",
                "replyto": "mHK0570CBz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2470/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2470/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 5ixm"
                    },
                    "comment": {
                        "value": "Thank you for your detailed evaluation of our manuscript and for raising several insightful points. We appreciate the opportunity to address your concerns.\n\n**Q1**: \u201cThe paper employs similar approaches to previous DKT methods, such as RAKT [1], AKT [2], and [3] except for the FRU unit. All of three papers also implemented the FRU unit with exponential time decay as part of the attention mechanism in the transformer architecture. The authors have used MLP units as FRU and concatenated the hidden state to the final representations.\u201d\n\n**A1**: Because the goal of this paper is to make the model both simple and effective. Among the KT models based on deep learning, the DKT model as the founder can be said to be very simple. The similarity between ReKT (the model proposed in this paper) and DKT demonstrates the simplicity of ReKT. Additionally, ReKT's three different perspectives (question, concept, domain) in modeling ensure its performance advantage. \n\nAs you mentioned, the exponential decay method used in considering forgetting in the three models is indeed very effective, but this computational forgetting method is more complex [1]. This paper wants to simplify the operation as much as possible, so FRU's forgetting is not designed to have an exponential time decay effect. Of course, we believe that considering exponential decay would definitely make FRU more effective. The fact that FRU consists of two linear regression units and connects the hidden state to the final representation is to make the model as simple as possible.\n\nWe also have provided a detailed response to the question about FRU in the \"General Response about FRU\". It includes the motivation behind designing FRU, inspiration for its design, the process of FRU, and the differences and advantages of FRU compared to MLP, RNN, LSTM, and GRU.\n\n**Q2**: \u201cThe authors did not provide any interpretations of the model's performance\u201d\n\n**A2**: Thank you very much for reminding! We have added a case study of ReKT tracing students\u2019 knowledge state in Appendix I.\n\n**Q3**: \u201cAs the authors have three different representations of question interactions, did the authors compute the Rasch difficulty from three different interaction matrices? How did the authors handle multiple submissions for computing the Rash difficulty?\u201d\n\n**A3**: Thank you for your in-depth question. Since your two questions are similar, I will answer them both together.\n\nWe did not calculate Rasch difficulty from three different interaction matrices. In our design, Rasch difficulty is specific to each question (i.e. each question has its own difficulty). Therefore, we fixed the representation of the question interactions, but the range seen in different perspectives is different, this allows the question interactions have different representations.\n\nThe Rasch difficulty you pointed out should be for all questions. Then the difficulty of Rasch is different from different perspectives. This is a very interesting idea, and we have not yet considered this situation.\n\nBut I have some ideas for your reference. Because the sequences obtained from different perspectives are different, their corresponding Rasch difficulty is also different. Then we can calculate the Rasch difficulty of the corresponding sequence from the question\u3001concept\u3001domain perspective respectively.\n\nIf the inputs from the three perspectives are made consistent, then the interaction representation method needs to be modified (for example, the interaction representation can be changed to question + concept + question sequence Rasch difficulty + concept sequence Rasch difficulty + domain sequence Rasch difficulty).\n\nIf the inputs from the three perspectives are inconsistent, then the input from the question perspective can be (question + question sequence Rasch difficulty), the input from the concept perspective can be (concept + concept sequence Rasch difficulty), and the input from the domain perspective can be (question + concept + domain sequence Rasch Difficulty).\n\nWe hope our response has addressed your concerns. If you have any further questions, please feel free to let us know.\n\nThanks again for reviewing and your valuable suggestions!\n\n[1] Zitao Liu, Qiongqiong Liu, Jiahao Chen, Shuyan Huang, and Weiqi Luo. simplekt: a simple but tough-to-beat baseline for knowledge tracing. In International Conference on Learning Representations, 2023b."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2470/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700106103935,
                "cdate": 1700106103935,
                "tmdate": 1700106103935,
                "mdate": 1700106103935,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fqVhX4U5Yt",
                "forum": "vZEgj0clDp",
                "replyto": "mHK0570CBz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2470/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2470/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Authors"
                    },
                    "comment": {
                        "value": "Dear reviewer 5ixm,\n\nAs the window for reviewer-author interaction is closing soon, I wanted to extend my sincerest gratitude for the invaluable time and effort you have dedicated to reviewing our work. To ensure that we have met your expectations, may I kindly ask if you find our responses satisfactory and if there are any remaining issues that need further clarification or improvement?"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2470/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700569431926,
                "cdate": 1700569431926,
                "tmdate": 1700569431926,
                "mdate": 1700569431926,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9F3S6DxL4c",
                "forum": "vZEgj0clDp",
                "replyto": "fqVhX4U5Yt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2470/Reviewer_5ixm"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2470/Reviewer_5ixm"
                ],
                "content": {
                    "comment": {
                        "value": "I would like to thank all authors for their detailed responses. I do not have further questions at this point."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2470/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700690045270,
                "cdate": 1700690045270,
                "tmdate": 1700690045270,
                "mdate": 1700690045270,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BXEGm1mADj",
                "forum": "vZEgj0clDp",
                "replyto": "mHK0570CBz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2470/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2470/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Appreciation and Inquiry"
                    },
                    "comment": {
                        "value": "We are encouraged by the resolution of your concerns and would like to respectfully inquire if you would be willing to consider an increase in our score. \n\nOnce again, we sincerely appreciate the valuable time you dedicated to evaluating our work."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2470/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700703408511,
                "cdate": 1700703408511,
                "tmdate": 1700703849049,
                "mdate": 1700703849049,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "wkb7OTek5c",
            "forum": "vZEgj0clDp",
            "replyto": "vZEgj0clDp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2470/Reviewer_c4BU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2470/Reviewer_c4BU"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a simple yet power knowledge tracing (KT) model called ReKT. The method consists of 1) three levels of knowledge state modeling including question-, concept-, and domain-level, and 2) a forget-response update (FRU) unit. Extensive experiments show that ReKT achieves state of the art KT performance on an array of datasets comparing many baselines."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The proposed method is claimed to be simple yet powerful.\n- the evaluation appears to be comprehensive."
                },
                "weaknesses": {
                    "value": "1. I am not convinced that the FRU gate is \"simple\". It appears to me as a variant of the gated recurrent unit (GRU) without the reset gate. Compared to the GRU, FRU has a similar forget gate and the hyperbolic tangent function in the end (without the affine combination of the update gate). I think the FRU architecture design, though interesting, does not qualify it as \"very lightweight, as it consists of only two linear regression units\". Otherwise, I can make the same \"very lightweight\" statement for GRU, which only consists of three linear regression units. Why not just use GRU? GRU takes into account not just forgetting (as in FRU), but also remembering/resetting, which might make more sense and have more modeling power? What exactly in the reference article \"Toward a theory of instruction\" do the authors get the inspiration to build FRU? This is an important question that the author should answer because they claim FRU as one of their core contributions, whereas I think FRU is not much different from GRU, which diminishes the value of this contribution. The authors should also cite GRU as important alternative modeling choices to compare to FRU (in addition to LSTM).\n\n2. The proposed approach to represent knowledge at question, concept, and history level is not entirely new; methods such as learning factor analysis (https://link.springer.com/chapter/10.1007/11774303_17), performance factor analysis (https://files.eric.ed.gov/fulltext/ED506305.pdf), additive factor models (http://www.cs.cmu.edu/~ggordon/chi-etal-ifa.pdf, ), knowledge factoring machines (https://arxiv.org/pdf/1811.03388.pdf) also take into account of modeling students' knowledge at concept (sometimes called skills in these literature), question, or entire history levels. In the spirit of \"revisiting\", the authors neither mentioned nor compared to these classic knowledge tracing methods.\n\n3. Some of the results need more clarifications. For example, AKT-R (https://arxiv.org/pdf/2007.12324.pdf) can achieve an AUC of __0.8346__ on ASSIST09 (see Table 5 in the AKT paper), beating the AUC of 0.7917 by the proposed method. Several other baselines in the AKT paper also achieve AUC > 0.8."
                },
                "questions": {
                    "value": "1. How is FRU different from GRU? What motivate the differences?\n2. How is the proposed method contexualized within, and compared to, some classic literature such as LFA, PFA, AFM, KFM, and others?\n3. Why are some results different (sometimes by a large margin) to existing published results?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2470/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698995420577,
            "cdate": 1698995420577,
            "tmdate": 1699636183170,
            "mdate": 1699636183170,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "02OgO3jWrb",
                "forum": "vZEgj0clDp",
                "replyto": "wkb7OTek5c",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2470/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2470/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer c4BU(1/2)"
                    },
                    "comment": {
                        "value": "Thank you for your detailed evaluation of our manuscript and for raising several insightful points. We appreciate the opportunity to address your concerns.\n\n**Q1**: \u201cI am not convinced that the FRU gate is \"simple\". It appears to me as a variant of the gated recurrent unit (GRU) without the reset gate. Compared to the GRU, FRU has a similar forget gate and the hyperbolic tangent function in the end (without the affine combination of the update gate). I think the FRU architecture design, though interesting, does not qualify it as \"very lightweight, as it consists of only two linear regression units\". Otherwise, I can make the same \"very lightweight\" statement for GRU, which only consists of three linear regression units.\u201d\n\n**A1**: FRU is lightweight compared to most methods in the KT field (i.e. the lightweight nature of FRU is relative). FRU's design is also intended as a core framework for KT tasks. As mentioned in this paper, the current mainstream in KT research is building more complex models (such as stacked Transformers) to improve performance. In terms of being the core architecture for KT tasks, both FRU, GRU, and LSTM can be considered lightweight.\n\n**Q2**\uff1a\u201d Why not just use GRU? GRU takes into account not just forgetting (as in FRU), but also remembering/resetting, which might make more sense and have more modeling power? What exactly in the reference article \"Toward a theory of instruction\" do the authors get the inspiration to build FRU? How is FRU different from GRU? What motivate the differences?\u201d\n\n**A2**: Thank you for your in-depth questions about FRU. The purpose of designing FRU is to create a simpler (compared to LSTM, GRU, Transformer) architecture and suitable for KT, so we have not considered using GRU. \n\nThrough experimentation, we found that we can achieve comparable or even better performance without the need for more complicated operations like memory/reset. Therefore, we simplified such operations.\n\nWe have provided a detailed response to the question about FRU in the \"General Response about FRU\". It includes the motivation behind designing FRU, inspiration for its design, the process of FRU, and the differences and advantages of FRU compared to MLP, RNN, LSTM, and GRU.\n\n**Q3**: \u201cThe authors should also cite GRU as important alternative modeling choices to compare to FRU (in addition to LSTM).\u201d\n\n**A3**: Thank you very much for your suggestion. We have added a comparison of GRU as the core architecture of KT in the paper (refer to Table 6, 7, 8, 9).\n\n**Q4**: \u201cThe proposed approach to represent knowledge at question, concept, and history level is not entirely new; methods such as LFA, PFA, AFM, KTM also take into account of modeling students' knowledge at concept (sometimes called skills in these literature), question, or entire history levels.\u201d\n\n**A4**: Firstly, it needs to be clarified that classic knowledge tracing models based on machine learning, such as BKT, LFA, PFA, AFM, etc., only model student knowledge at the concept level. Knowledge tracing models based on machine learning, including KTM, only model student knowledge at the question level. They all struggle to utilize the entire interaction history of students (i.e., domain level). In contrast, mainstream approaches of deep learning-based knowledge tracing models utilize the entire interaction history of students (i.e., only domain level) to construct student knowledge (as more data helps improve the performance of deep learning models). However, very few methods exist that use deep learning to construct student knowledge from concept or question levels, which is evident as these perspectives imply a reduction in data volume (which is not beneficial to the performance of deep learning models). To the best of our knowledge, there is currently no knowledge tracing method that combines the three different perspectives of question, concept, and domain to construct student knowledge. Therefore, the proposed method in this paper is novel.\n\n**Q5**: \u201cIn the spirit of \"revisiting\", the authors neither mentioned nor compared to these classic knowledge tracing methods. How is the proposed method contexualized within, and compared to, some classic literature such as LFA, PFA, AFM, KFM, and others?\u201d\n\n**A5**: Thank you very much for your reminder. We have added an review to the classic knowledge tracing method (BKT\u3001LFA\u3001PFA\u3001AFM\u3001KTM) in RELATED WORK. In addition, we added performance comparisons with classical knowledge tracing methods BKT and KTM (refer to Table 2, 3)."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2470/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700106481945,
                "cdate": 1700106481945,
                "tmdate": 1700106481945,
                "mdate": 1700106481945,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "x5W0YERGJT",
                "forum": "vZEgj0clDp",
                "replyto": "wkb7OTek5c",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2470/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2470/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer c4BU(2/2)"
                    },
                    "comment": {
                        "value": "**Q6**: \u201cWhy are some results different (sometimes by a large margin) to existing published results\u201d\n\n**A6**: This is mainly due to the way the data is processed. Suppose there is a student answer record $(q, (c_1, c_2, c_3))$: corresponding to a question $q$ and three concepts $(c_1, c_2, c_3)$ associated with it. According to [1], the processing method of this paper is to merge these three concepts $(c_1, c_2, c_3)$ into a single concept $c$, then in question-based KT, this record becomes $(q, c)$. What AKT does is to separate these three concepts $(c_1, c_2, c_3)$ into $(c_1)$, $(c_2)$, $(c_3)$. Then in question-based KT, this record becomes $(q, c_1), (q, c_2), (q, c_3)$ three records. In a real learning environment, the concept related to a specific question should remain unchanged. The processed approach in AKT cannot guarantee this, but the processed approach used in this paper can.\n\nFurthermore, the AKT performance (and other baselines) reported in recent papers at top-level conferences (such as simpleKT[2], LBKT[3], SFKT[4]) all indicate that the performance of AKT is not as high as reported in their paper.\n\nThanks again for reviewing and your valuable suggestions!\n\n[1] Xiaolu Xiong, Siyuan Zhao, Eric G Van Inwegen, and Joseph E Beck. Going deeper with deep knowledge tracing. In Proceedings of the 9th International Conference on Educational Data Mining, pp. 545\u2013550. International Educational Data Mining Society, 2016.\n\n[2] Zitao Liu, Qiongqiong Liu, Jiahao Chen, Shuyan Huang, and Weiqi Luo. simplekt: a simple but tough-to-beat baseline for knowledge tracing. In International Conference on Learning Representations, 2023b.\n\n[3] Bihan Xu, Zhenya Huang, Jiayu Liu, Shuanghong Shen, Qi Liu, Enhong Chen, Jinze Wu, and Shijin Wang. Learning behavior-oriented knowledge tracing. In Proceedings of the 29th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 2789\u20132800. Association for Computing Machinery, 2023.\n\n[4] Zhang M, Zhu X, Zhang C, et al. No Length Left Behind: Enhancing Knowledge Tracing for Modeling Sequences of Excessive or Insufficient Lengths. In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management. 2023."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2470/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700106923067,
                "cdate": 1700106923067,
                "tmdate": 1700106923067,
                "mdate": 1700106923067,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0HqsS3qhwB",
                "forum": "vZEgj0clDp",
                "replyto": "wkb7OTek5c",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2470/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2470/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Authors"
                    },
                    "comment": {
                        "value": "Dear reviewer c4BU,\n\nAs the window for reviewer-author interaction is closing soon, I wanted to extend my sincerest gratitude for the invaluable time and effort you have dedicated to reviewing our work. To ensure that we have met your expectations, may I kindly ask if you find our responses satisfactory and if there are any remaining issues that need further clarification or improvement?"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2470/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700569395457,
                "cdate": 1700569395457,
                "tmdate": 1700569395457,
                "mdate": 1700569395457,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "o7JH7MBJaM",
                "forum": "vZEgj0clDp",
                "replyto": "wkb7OTek5c",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2470/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2470/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Appreciation and Inquiry"
                    },
                    "comment": {
                        "value": "We appreciate the valuable contributions you have made to our manuscript. As the rebuttal phase is about to close, we would like to inquire once again if we have adequately addressed your concerns and if you would be willing to consider improving our score. \n\nOnce again, we sincerely appreciate the valuable time you dedicated to evaluating our work."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2470/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700703680901,
                "cdate": 1700703680901,
                "tmdate": 1700703680901,
                "mdate": 1700703680901,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]