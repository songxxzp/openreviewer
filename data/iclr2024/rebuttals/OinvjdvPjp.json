[
    {
        "title": "xVal: A Continuous Number Encoding for Large Language Models"
    },
    {
        "review": {
            "id": "zWKA7xFnhg",
            "forum": "OinvjdvPjp",
            "replyto": "OinvjdvPjp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7921/Reviewer_aAmt"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7921/Reviewer_aAmt"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposed a two-step prediction for floating numerics embedded in the NLG tasks for the LM."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "I seldom write this but for this paper it's hard for me to find one."
                },
                "weaknesses": {
                    "value": "1. Not well motivated. The paper says it's LMs have historically struggled to solve simple arithmetic problems  but somehow many Chain-of-Thoughts paper contradicts the claim. There is no discussion and not literature review enough for this part. \n\n2. The method itself is not very interesting. \n\n3. Evaluation is rough, what are FP15, P10 and so on? No clear elaboration on this.\n\n4. Page 2 is in low-resolution. A not ready draft IMO."
                },
                "questions": {
                    "value": "I believe at least the paper need to show empirically why the task it difficult."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7921/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698612984137,
            "cdate": 1698612984137,
            "tmdate": 1699636972935,
            "mdate": 1699636972935,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "67lz6VXyqI",
                "forum": "OinvjdvPjp",
                "replyto": "zWKA7xFnhg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7921/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7921/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer aAmt"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their feedback. Here are our responses to the points raised by the reviewer. We kindly ask the reviewer to reconsider their evaluation in light of the results and the improved performance that our encoding offers.\n\n**Weaknesses**\n\n> 1. Not well motivated. The paper says it's LMs have historically struggled to solve simple arithmetic problems but somehow many Chain-of-Thoughts paper contradicts the claim. There is no discussion and not literature review enough for this part.\n\nIndeed, a number of recent work have explored methodologies that force LLMs to use Chain-of-Thought reasoning in a \u201cscratchpad\u201d setting and have proven effective for improving numerical predictions of LLMs tasks like arithmetic or mathematical word problems. We have now added this information and relevant citations to the introduction.\n\nHowever, a pre-requisite for these methods to be applicable is that Chain-of-Thought reasoning for solving the given task (or a similar task) must be available during train time. In our work, we are motivated by the need to improve numerical analysis of datasets such as those seen in many scientific contexts. For these tasks, such chain-of-thought reasoning is not known -- indeed often the goal is to discover the relationships in the first place -- and therefore a step-by-step scratchpad would not improve the results. \n\n[]()\n\n> 2. The method itself is not very interesting.\n\nWe believe that our findings and results would be of interest to the community as evidenced by the comments from other reviewers. In particular, we show state-of-the-art out-of-distribution generalization performance in a number of tasks. \n\n[]()\n\n> 3. Evaluation is rough, what are FP15, P10 and so on? No clear elaboration on this.\n\nThese tokenization schemes (FP15, P10 etc) are described in detail in Table 1 (including an example for clairty) and at the beginning of Section 3 titled **Comparison with other number encodings**. \n\n[]()\n\n> 4. Page 2 is in low-resolution. A not ready draft IMO.\n\nThank you for pointing out this accidental glitch in the rendering process. We have corrected this in our latest version.\n\n[]()\n\n**Questions**\n\n> I believe at least the paper need to show empirically why the task it difficult.\n\nIn our experiments, we compare xVal with existing text-based number encoding schemes that are currently SOTA on the type of numerical analysis tasks that we are interested in. We demonstrate that these other schemes have several notable weaknesses, especially in terms of robustness to outliers and performance in out-of-distribution generalization."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7921/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700599037847,
                "cdate": 1700599037847,
                "tmdate": 1700604001392,
                "mdate": 1700604001392,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "AF5qZe65S4",
            "forum": "OinvjdvPjp",
            "replyto": "OinvjdvPjp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7921/Reviewer_qrLU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7921/Reviewer_qrLU"
            ],
            "content": {
                "summary": {
                    "value": "Large language models rely on predefined tokenizers to process input data. While commonly used tokenizers are constructed for natural language, it is challenging to apply them to numbers. In response, the authors propose a novel method to encode number values for large language models. \n\nSpecifically, at model input, the author proposes to incorporate the numerical value of numbers as a weighted sum of token embedding and position embedding; at model output, the author proposes to construct a separate number head to decide the numerical value and use the original token head to decide whether to use this token. \n\nThe author conducts extensive experiments, which demonstrates the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The studied problem is important and may have a big impact. The proposed method is reasonable and novel. \n2. The author conducts empirical evaluations on: a) learning arithmetic; b) temperature forecasting; c) planetary orbit prediction. The proposed method demonstrates consistent performance gain."
                },
                "weaknesses": {
                    "value": "In experiment setting, the proposed method is only evaluated in the supervised training setting. It is unclear on the impact of the proposed method on pre-training tasks."
                },
                "questions": {
                    "value": "How the training hyper-parameters are configured and why different encodings have different configurations."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7921/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698711172340,
            "cdate": 1698711172340,
            "tmdate": 1699636972803,
            "mdate": 1699636972803,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qpalOJE2oB",
                "forum": "OinvjdvPjp",
                "replyto": "AF5qZe65S4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7921/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7921/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer qrLU"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their feedback. We are encouraged that they find the problem important and our method potentially impactful. Below we address the weakness and question brought up by the reviewer. We have added new experiments and clarified parts of the text that we believe have overall improved the manuscript.\n\n**Weaknesses**\n\n> In experiment setting, the proposed method is only evaluated in the supervised training setting. It is unclear on the impact of the proposed method on pre-training tasks.\n\nThank you for bringing this up. While our training regimen is self-supervised masked language modeling, we primarily evaluate the performance of the model on values that are directly present in the training corpus. We suspect that this is what the reviewer is referring to as \"evaluated in the supervised training setting\". \n\nTo more directly evaluate the model in a setting where the output is not explicitly present in the input, we performed two new tests. The first a fine-tuning task and the second a roll-out task.\n\n* We fine-tuned the temperature forecasting model on a binary downstream task, specifically we use the fine-tune the pretrained model to predict if the first reporting station is located on water or on a landmass. In principle, this problem is solvable by looking at the latitude & longitude of the station, but because we fine-tune on only 500 samples, there is not enough information to accurately learn the mapping between latitude & longitude and water coverage over the earth. Therefore, the model needs to use other information (e.g. temperature variation etc.) to make this inference. We find that xVal outperforms the other models, achieving a ROC AUC of 0.620 ( vs .600 and 0.580 for P10 and FP15 respectively). We expect that the longer context length of P10 is adversely affecting its fine-tuning behavior. We report these findings in Sec. B.1.3.\n\n* In the temperature forecast problem, we rolled-out the temperature forecast on 5 extra time-steps to predict the future development of the temperature across all sites. This is similar to text generation in language models that are only trained to predict the next time-step. We find again that xVal outperforms the other encoding schemes. This is not surprising since xVal has the best one-step prediction performance.\n\n**Questions**\n\n> How the training hyper-parameters are configured and why different encodings have different configurations.\n\nThank you for asking this clarifying question. To determine the hyper-parameters of our runs, we performed a coarse grid search for the learning rate (with a linear grid in log space). We chose the best performance on a validation set and reported the results on an unseen test set. In many cases, the validation performance of different learning rates were very close and/or within variance of each other, but we still chose the configuration with the best performance. This is one cause of differing learning rates between different experiments. Another reason for differences between the optimal hyperparameters for xVal and the text encodings is the very different statistics of the input for these different models. Whereas the input to the text models is always a text embedding, the input to xVal cares about the numbers that appear in the text.\n\nWe have added this description to the supplementary materials Sec. B."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7921/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700597479978,
                "cdate": 1700597479978,
                "tmdate": 1700597479978,
                "mdate": 1700597479978,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "rdT8ouoXNE",
            "forum": "OinvjdvPjp",
            "replyto": "OinvjdvPjp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7921/Reviewer_iRZg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7921/Reviewer_iRZg"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces an innovative numerical encoding scheme, designed to efficiently represent any real number using a single token. The encoding leverages a dedicated embedding vector, denoted as `<NUM>`, which is dynamically scaled by the numerical value. This approach significantly optimizes token usage and minimizes the vocabulary footprint.\n\nIn addition, the authors complement this encoding scheme with a novel number-inference technique, incorporating a specialized `Number Head`. This `Number Head` enables the model to generate continuous real numbers in an end-to-end manner.\n\nTo validate the effectiveness of this proposed methodology, extensive evaluations were conducted on both synthetic and real-world datasets. The results demonstrated consistently comparable or superior performance when compared to prior research in the field."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The strengths of this paper are as follows:\n\n1.  The paper introduces a deceptively simple yet novel approach to real number representation. This design not only minimizes token usage but also significantly reduces vocabulary footprint while preserving the input's value. This simplicity is an attractive feature, emphasizing efficiency without sacrificing performance.\n    \n2.  The proposed method exhibits outstanding performance, particularly in synthetic datasets used to evaluate multi-digit multiplication and multi-operand binary tree combining. The results indicate that it excels at preserving real number information, outperforming previous approaches in these scenarios.\n    \n3.  The paper is meticulously organized, presenting the idea in a coherent and transparent manner. It guides the reader through the experimental process, offering a step-by-step validation of the proposed method and effectively highlighting the distinctions from various baseline techniques.\n    \n4.  One notable advantage of this approach is its adaptability to out-of-distribution inputs. This robustness is inherent in the generation of embeddings, allowing the method to handle cases where certain real numbers are more frequently predicted due to their prevalence or distribution discrepancies between training and testing datasets."
                },
                "weaknesses": {
                    "value": "This paper, despite its strengths, has some weaknesses:\n\n1.  An issue with the rendering quality on page 2, affecting some figures, diminishes the readability of both the text and data points. The compromised legibility of axis labels and data points could potentially hinder the reader's comprehension and impact the overall impression of the paper.\n    \n2.  The paper's discussion of implicit normalization and its impact on real number embedding output is not sufficiently clear. The authors fail to provide a lucid explanation of how layer normalization influences the output of real number embeddings and why the normalization into a specific range is performed during preprocessing. Moreover, it remains unclear how these aspects might affect the performance of baseline methods. A more comprehensive and intuitive explanation is required to enhance the paper's accessibility.\n    \n3.  The experiments conducted in the paper exclusively utilize structured data in JSON format, focusing on scenarios like multi-digit multiplication and multi-operand calculations. While these experiments demonstrate the effectiveness of the proposed approach in these specific contexts, they do not adequately showcase the method's capability to understand real numbers in the broader context of natural language. This limitation may raise questions about the universal applicability and effectiveness of the proposed approach. Expanding the scope of experiments to encompass real-world language contexts would provide a more comprehensive evaluation of its capabilities."
                },
                "questions": {
                    "value": "1.  Could you please demonstrate how well the proposed method can handle situations where it needs to refer to previously mentioned real numbers in the context, ensuring these numbers remain unaltered? How does this embedding method impact a Language Model's capability to preserve real numbers in the given input?\n    \n2.  How is the capability of the proposed numerical encoding scheme affected by extremely small or extremely large numbers? Is it able to maintain representation accuracy and robustness in the presence of such numerical extremes?\n    \n3.  Can you provide an example of a use case where the proposed method demonstrates improved real number understanding capabilities, but the input data is not structured as in JSON, a binary tree, or multi-digit multiplication? This would help illustrate the method's applicability in contexts beyond structured data scenarios."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed.",
                        "Yes, Responsible research practice (e.g., human subjects, data release)"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7921/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7921/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7921/Reviewer_iRZg"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7921/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698732523322,
            "cdate": 1698732523322,
            "tmdate": 1700680659244,
            "mdate": 1700680659244,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "MR2S0kpnSe",
                "forum": "OinvjdvPjp",
                "replyto": "rdT8ouoXNE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7921/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7921/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer iRZg"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their insightful feedback. We are encouraged that they find our approach simple and well explained. We are also pleased that the reviewer finds the improved generalization of our method noteworthy. \n\nThanks to the reviewer's suggestions, we have added a new experiment and also further clarifying comments to the manuscript. We kindly ask the reviewer to reconsider their evaluation in light of these additions.\n\nIn this post, we address the weaknesses and in the next we respond to the questions.\n\n[]()\n\n## Weaknesses\n[]()\n\n> 1. An issue with the rendering quality on page 2, affecting some figures, diminishes the readability of both the text and data points. The compromised legibility of axis labels and data points could potentially hinder the reader's comprehension and impact the overall impression of the paper.\n\nThank you for pointing out this accidental glitch in the rendering process. We have corrected this in our latest version.\n\n[]()\n\n> 2. The paper's discussion of implicit normalization and its impact on real number embedding output is not sufficiently clear. The authors fail to provide a lucid explanation of how layer normalization influences the output of real number embeddings and why the normalization into a specific range is performed during preprocessing. Moreover, it remains unclear how these aspects might affect the performance of baseline methods. A more comprehensive and intuitive explanation is required to enhance the paper's accessibility.\n\n**Impact of Layer-Norm:** We have added clarifying text in Sec. 2.1 to provide more information about the effect of the layer normalization. In particular,  \n\n\u201cIn our experiments, we use additive positional encodings and therefore the result of the layer-norm is to normalize the sum of the vector associated with the [NUM] token and the positional encoding vector.\u201d\n\nHowever, as we point out in our ablation tests in Table 9 of the supplementary materials, the presence of the first layer norm does not affect the results of our experiments. (Removing all layer norms, on the other hand, causes the performance to degrade.) We believe this is because of the pre-processing rescaling, which makes the first layer-norm unnecessary. \n\n**Impact of Rescaling during Pre-Processing:** As we mention in the draft, the limited dynamic range of xVal is a drawback of the current implementation of our algorithm. Because of this, the range of the input numbers had to be rescaled such that it is compatible with this limited dynamic range. We have verified that this preprocessing rescaling step does not affect the performance of the textual baseline methods. But this step was necessary for the non-text based comparisons in the Appendix (e.g. MLPs of Sec. B.1.2). Also note that (as mentioned in the conclusion) the limited dynamic range of our method can be improved by using more complex embedding schemes that maintain the continuity of the embedding scheme. However, we decided to use the simplest continuous  encoding scheme so as not to distract the reader by the details of a higher dynamic range encoding.\n\n[]()\n> 3. The experiments conducted in the paper exclusively utilize structured data in JSON format, focusing on scenarios like multi-digit multiplication and multi-operand calculations. While these experiments demonstrate the effectiveness of the proposed approach in these specific contexts, they do not adequately showcase the method's capability to understand real numbers in the broader context of natural language. This limitation may raise questions about the universal applicability and effectiveness of the proposed approach. Expanding the scope of experiments to encompass real-world language contexts would provide a more comprehensive evaluation of its capabilities.\n\nWe agree that it would be instructive to demonstrate the use of our encoding scheme in more general settings. The goal in this current work was to demonstrate better performance when applying LLMs to the analysis of numerically dense scientific data from heterogeneous sources.\n\n**On structure:** We note that even though our samples were formatted as a JSON, this formatting is not necessary for achieving the results in the draft. Violating this strict structure, for example removing structural components of the JSON (e.g. commas and brackets) or providing information out of place (e.g. putting a key in a different part of the JSON hierarchy) does not degrade the performance, so long as these were also seen during training. (In comparison, methods that rely on the strict structure of JSON would fail when the structure is violated.)\n\n**Broadening the scope:** Demonstrating better number understanding in real-world language contexts would be the logical next step. However, training a language model with broad language understanding requires dramatically more computation time and power than the experiments run in the current paper."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7921/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700603016932,
                "cdate": 1700603016932,
                "tmdate": 1700603016932,
                "mdate": 1700603016932,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "I9MN0ZYpeq",
                "forum": "OinvjdvPjp",
                "replyto": "05voNPc5zm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7921/Reviewer_iRZg"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7921/Reviewer_iRZg"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the response"
                    },
                    "comment": {
                        "value": "Thanks for the responses, I would raise my score to 5 given my concern on layernorm is settled yet the other concerns are still there."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7921/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700680636765,
                "cdate": 1700680636765,
                "tmdate": 1700680636765,
                "mdate": 1700680636765,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "WIrktDYgQM",
            "forum": "OinvjdvPjp",
            "replyto": "OinvjdvPjp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7921/Reviewer_SWrp"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7921/Reviewer_SWrp"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a simple approach to encoding numerical values as tokenized input to a LLM. Specifically, all numbers $x$ in the input are identified and replaced with the stand-in token '[NUM]', that is then scaled by the value of $x$, i.e., $h(x) := x h(\\text{[NUM]})$. This both reduces the number of tokens per number and the vocabulary size, and leads to more efficient training. The paper also demonstrates improved performance at numerical tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The method is simple and easy to understand. It also has some computational benefits. While on simple arithmetic tasks the model performs similarly well to other good approaches (e.g., on 3-5 digit multiplication, P1000 and B1999 encoding schemes can also get near perfect performance), xVal seems to work much better on unstructured, numerical heavy experiments. There are some shortcomings (some of which the authors make a good point of noting), but in general, it seems like a straightforward and effective representation strategy for numerically-dense text."
                },
                "weaknesses": {
                    "value": "While the continuous nature of the xVal embedding can obviously be an advantage in some domains, I'm not sure how well it would work in general. For example, tasks like summarization, or question answering, where numerical values such as years/dates/account numbers are not meant to be worked with in the sense of arithmetic or other mathematical operations but simply carried about may lose performance. That said, for domain specific applications (like science), this may not be an issue. A hybrid approach may also work (e.g., representing $7.4$ as \"+ , 740, e-2, 7.4 * [NUM]\")."
                },
                "questions": {
                    "value": "- I'm not entirely sure why the runtime is so dramatically reduced. Is this due to the reduced length of each input/target and the vocab size? If the latter is a big factor, I'm surprised that what seems like a fairly small additional overhead of the softmax size for everything but FP15 setting would make that big of a difference. \n\n- I'm curious as to what kind of empirical range the xVal style network has. Is $h(\\texttt{[NUM]})$ layer normalized at the end of the network? Is the mapping from $h(\\texttt{[NUM]})$ to its numerical value by way of MSE loss minimization linear? I'd imagine that the output range would be restricted in this setup. \n\n- I'm a bit put off by the dependence on _parsing_ numerical quantities accurately, especially in messier settings. Curious if that posed any difficulties."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7921/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698888681779,
            "cdate": 1698888681779,
            "tmdate": 1699636972571,
            "mdate": 1699636972571,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8LXYOBvHzd",
                "forum": "OinvjdvPjp",
                "replyto": "WIrktDYgQM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7921/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7921/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer SWrp"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their encouraging remarks and insightful questions. In this post, we respond to the weaknesses and in the next we address the questions mentioned.\n\n**Weaknesses**\n\n> While the continuous nature of the xVal embedding can obviously be an advantage in some domains, I'm not sure how well it would work in general. For example, tasks like summarization, or question answering, where numerical values such as years/dates/account numbers are not meant to be worked with in the sense of arithmetic or other mathematical operations but simply carried about may lose performance. That said, for domain specific applications (like science), this may not be an issue. A hybrid approach may also work (e.g., representing  as \"+ , 740, e-2, 7.4 * [NUM]\").\n\nWe agree that for a general language model a hybrid approach would be useful. As the reviewer suggests, an approach combining text-based encoding with a continuous encoding to leverage advantages of each method can lead to improvements on both. One can also envision an encoding scheme that treats different number types (integers, floats, etc) differently. \n\nIn this work, we wanted to demonstrate the advantages of a continuous number encoding over text-based encodings in the setting of heterogeneous scientific data analysis. We leave the exploration of combining different tokenization schemes to future work."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7921/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700595835436,
                "cdate": 1700595835436,
                "tmdate": 1700595835436,
                "mdate": 1700595835436,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1kTrggTD5M",
                "forum": "OinvjdvPjp",
                "replyto": "WIrktDYgQM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7921/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7921/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Questions**\n\n> * I'm not entirely sure why the runtime is so dramatically reduced. Is this due to the reduced length of each input/target and the vocab size? If the latter is a big factor, I'm surprised that what seems like a fairly small additional overhead of the softmax size for everything but FP15 setting would make that big of a difference.\n\nThe reduced runtime of xVal is a combination of both fewer tokens and small vocabulary size. The bigger factor is the length of the tokenized input. Text-based encodings (other than FP15) have dramatically increased number of tokens compared to xVal and FP15  (in Sec. 3 around 8000 and 5000 tokens per sample respectively for P1000 and P10 compared to 1800 tokens for FP15 and xVal (see Table 1 for a summary of different tokenization lengths). This leads to a longer runtime for the encodings with higher number of tokens per number. In comparison, the overhead of FP15 compared to xVal is coming from the size of the vocabulary leading to large embedding and logit-head layers. Note that the relative increase in computational cost from the embedding and logit-head is larger for small transformer models. In our case, the embedding matrix of FP15, for example, is a large fraction of the total number of parameters of the model.\n\nNote that in the runs with equal number of total tokens, for efficient parallelization purposes, the number of tokens seen per batch is still different for the different methods. This also leads to a difference in run-time, even when the total number of tokens seen during training is the same.\n\n[]()\n\n> * I'm curious as to what kind of empirical range the xVal style network has. Is $h\\texttt{[Num]}$ layer normalized at the end of the network?\n\nWe use an architecture similar to GPT such that the layer-norms of the network are only placed in transformer blocks (prior to the attention module and the MLP) and the residual path through the blocks does not go through layer-norm. There are no other layer-norms in the network.\n\n[]()\n\n> * Is the mapping from to its numerical value by way of MSE loss minimization linear? I'd imagine that the output range would be restricted in this setup.\n\nIn the majority of our experiments we use a non-linear map (MLP with one hidden layer)  to extract the numbers from the final output of the transformer blocks before evaluating the MSE loss. However, in ablation tests we find that a linear readout layer performed equally well up to run-to-run variations. \n\nIf, as proposed in the conclusion, we replace the current multiplicative encoding with a different continuous encoding with higher dynamic range, we also solve the problem of the output range. For example, we can use Fourier encodings on the log space of the input numbers, leading to a vector encoding with each component taking values between -1 and 1. The dynamic range of such an encoding can be easily extended and MSE loss minimization on this embedding would also be easily implemented. We chose to implement the simplest continuous encoding in the current draft in order to highlight the advantages of continuity and leave questions regarding extended dynamic range to future work.\n\n[]()\n\n> * I'm a bit put off by the dependence on parsing numerical quantities accurately, especially in messier settings. Curious if that posed any difficulties.\n\nIn our code, the preprocessing step is implemented using a short regular expression to parse the input text and replace numerical values with the \u201c[NUM]\u201d token. This is done efficiently using the Re package and the processing adds little overhead. This simple preprocessing step would not be sufficient for a general purpose language model, especially if we want to extract the number types as suggested above. However, for such a use case, lightweight parts of speech tagging models can be used to efficiently and accurately tag numbers and number types (e.g. Jurafsky Speech and Language Processing Chapter 8)."
                    },
                    "title": {
                        "value": "Response to questions"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7921/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700595854448,
                "cdate": 1700595854448,
                "tmdate": 1700603948175,
                "mdate": 1700603948175,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uD3ouhlKdp",
                "forum": "OinvjdvPjp",
                "replyto": "WIrktDYgQM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7921/Reviewer_SWrp"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7921/Reviewer_SWrp"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for taking the time to respond to my review and questions. \n\n- I still have the same reservation about this being a good general purpose number representation (e.g., for copying numbers by value and by representing large magnitude numbers), as also shared by reviewer iRZg. Still, I agree that there are domains where this is less important, and, it would be interesting to see if a hybrid approach could bridge that gap.\n\n- I also recognize that numbers can be parsed using regular expressions and/or taggers, but these still have errors in messy situations. Again, however, I feel that it is still an OK contribution if this number representation is meant to be more domain specific.\n\n- One question about the layer norm discussion: I am assuming you are representing the vector $xu + p$ after layer normalization as \n\n$$ \\mathrm{LayerNorm}(xu + p) = \\frac{xu + p - \\mathbb{E}[xu + p]}{\\sqrt{\\mathrm{Var}[xu + p]}}= \\frac{xu + p'}{||xu + p'||}$$\n\nwhere $p' = p - \\mathbb{E}[xu + p]$? Then $u \\cdot p' \\neq 0$. Some clarity here on your approach and analysis would be good."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7921/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700674450894,
                "cdate": 1700674450894,
                "tmdate": 1700674586315,
                "mdate": 1700674586315,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]