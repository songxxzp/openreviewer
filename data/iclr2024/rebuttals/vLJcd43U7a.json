[
    {
        "title": "SYMBOL: Generating Flexible Black-Box Optimizers through Symbolic Equation Learning"
    },
    {
        "review": {
            "id": "E4rH3UU6l8",
            "forum": "vLJcd43U7a",
            "replyto": "vLJcd43U7a",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission237/Reviewer_hEPw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission237/Reviewer_hEPw"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces SYMBOL, a novel approach for BBO that leverages a neural network trained via Reinforcement Learning to dynamically predict the explicit expression of the optimization steps for a given task. \n\nSYMBOL is based on a Symbolic Equation Generator (SEG), an LSTM-based model responsible for predicting closed-form (symbolic) optimization updates. SEG can be trained via three different RL strategies, namely Exploration learning (SYMBOL-E), Guided learning (SYMBOL-G) and Synergized learning (SYMBOL-S). While SYMBOL-E does not impose any inductive bias about the sought-for optimizer,  SYMBOL-E explicitly forces the SEG to mimic the behaviour of a given black-box optimizer (teacher optimizer). SYMBOL-S integrates both approaches and regulates their relative importance via a hyperparameter $\\lambda$. \n\nThe model -- in all its three variants -- is extensively tested and compared to multiple state-of-the-art baselines. The results indicate that SYMBOL compares favourably to the considered baselines. In addition, the updates found by SEG can be directly inspected thanks to their symbolic nature."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper is well-written and the presentation is clear.\n\n- SYMBOL represents an interesting application of deep-learning-based symbolic regression, beyond the standard setting where it is usually applied.\n\n- The experiments compare SYMBOL to multiple baseline methods and show that SYMBOL archives state-of-the-art performance on BBO.\n\n- Contrarily to black box systems, SYMBOL generates interpretable update steps."
                },
                "weaknesses": {
                    "value": "- Reliance on a specific teacher in SYMBOL-G and SYMBOL-S: how to select a teacher to train the model may not be obvious in real-world applications. This makes the applicability of the aforementioned training strategies relatively limited. \n\n- Reliance on the meta-learning stage: The training of the SEG model requires the selection of a training distribution D. The authors rely on 10 BBO optimization problems to construct D. However, in practice, the obtained distribution might not be large enough to guarantee sufficient generalization to fundamentally different tasks. \n\n- Training SYMBOL is still quite computationally demanding as shown in the meta-test column in Table 1. Is training the model via RL the computational bottleneck?"
                },
                "questions": {
                    "value": "See weaknesses part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission237/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698682359281,
            "cdate": 1698682359281,
            "tmdate": 1699635949232,
            "mdate": 1699635949232,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "P7BKPQcACm",
                "forum": "vLJcd43U7a",
                "replyto": "E4rH3UU6l8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission237/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission237/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer hEPw"
                    },
                    "comment": {
                        "value": "We appreciate the reviewer for the valuable feedback and the strong support for our paper. Your recognition of the paper's clarity, the novelty of SYMBOL, its state-of-the-art performance, and its good interpretability is highly encouraging. We hope that the following response, as well as the newly added results, will clear the remaining concerns.\n\n---\n\n**[Teacher selection]** We thank the reviewer for the insightful comment. Please refer to our response **[Teacher selection in SYMBOL]** in the \u201cgeneral response\u201d above.\n\n---\n\n**[Training distribution selection]** We acknowledge the importance of task distribution $\\mathbb{D}$ in facilitating generalization during the meta-learning stage of SYMBOL. To address the concern regarding the adequacy of using only 10 synthetic functions for meta-learning a universally effective policy, we have refined the description in \u201cTraining distribution $\\mathbb{D}$\u201d part in Section 4, and included additional details in Appendix B.4 of our revised paper. Our explanation is threefold:\n\n1. The 10 synthetic functions we have selected encompass a broad spectrum of **representative** landscape features and optimisation challenges, as detailed in the newly added Table 7 in Appendix B.4. These features include uni/multi-modality, separability, smooth yet narrow ridges, and (a)symmetry, etc. In Appendix B.4, the added Figure 9 now includes visualizations of the fitness landscapes of these 10 functions. Through this diverse range and by incorporating fitness landscape analysis-based state representation (refer to Section 3.1 and Appendix A.1), SYMBOL learns not merely to solve specific tasks, but to adapt its update patterns to varying landscape features. This adaptability underpins its impressive generalization capabilities, as demonstrated in our experimental results.\n2. We have also augmented the training distribution $\\mathbb{D}$ by incorporating shifted and rotated versions of these functions, as outlined at the beginning of Section 4. This methodological augmentation enriches $\\mathbb{D}$ with a multitude of diverse landscapes, thereby expanding its scope and ensuring robust generalization.\n3. We would also like to note that these selected 10 functions have a **longstanding** history in the literature for evaluating the performance of BBO optimizers. Their proven ability to present diverse optimization properties makes them ideal for training SYMBOL, ensuring its efficacy in generalizing to a wide array of unseen, realistic tasks. \n\n---\n\n**[Is training via RL the computational bottleneck?]** We thank the reviewer for the insightful question. We agree that a detailed examination of the computational costs of SYMBOL\u2019s different sub-routines during training is essential. In SYMBOL, there are three main sub-routines:\n\n1. **Update Rule Generation**: This involves computing the contextual state representation, feeding the state into the SEG (State Encoding Generator) for generating pre-order traversals, and decoding these traversals into the corresponding update rules.\n2. **Lower-Level Optimization**: After generating the update rule, SYMBOL applies this rule to update the candidate population and evaluates the objective values of the new generation.\n3. **Meta-Level PPO Training**: Concurrently with the lower-level optimization, the collected trajectories are utilized to update the parameters in the SEG via reinforcement learning.\n\nTo quantify the computational demands, we measured the wall time for each of these sub-routines during training. The table below presents the consumed time and corresponding proportions for each subroutine, along with the total time, using SYMBOL-S as an example:\n\n| Sub-routines | Update rule generation | Lower-level optimization | Meta-level PPO training | Total time |\n| --- | --- | --- | --- | --- |\n| Time | 1.5h (14.9%) | 6.3h (62.6%) | 2.2h (22.1%) | 10h |\n\nAs the data reveals, **lower-level optimization emerges as the primary computational bottleneck in SYMBOL**. It's important to clarify that high computational costs are intrinsic to BBO (Black-Box Optimization) optimization in the MetaBBO domain. This is evident in the baseline methods we compare against in our paper (like LDE, DE-DDQN, Meta-ES, etc.).\n\nTo mitigate these costs, common practice involves parallel processing. Specifically, we simultaneously sample a batch of task instances and collect trajectories in parallel, significantly accelerating both the lower-level optimization and meta-level training processes. For more details on our approach to parallel processing, please refer to Appendix B.2.\n\nWe hope the above clarifications help, and we are happy to engage in further discussions. Thanks for taking your time and making contributions to our paper!"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission237/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700322926258,
                "cdate": 1700322926258,
                "tmdate": 1700322926258,
                "mdate": 1700322926258,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "w6DONacvnr",
            "forum": "vLJcd43U7a",
            "replyto": "vLJcd43U7a",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission237/Reviewer_ZLbt"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission237/Reviewer_ZLbt"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a novel black-box optimizer (BBO) SYMBOL featured by its symbolic equation learning via meta-learning. Different from previous MetaBBOs which directly output the updated value, SYMBOL meta-learns the updated rule as an equation which achieves a superior performance and better interpretability."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Though the idea was also explored in various domains, I think the discoveries from this paper are still interesting. It is great to see the steps for learning more interpretable optimizers.\n2. The paper is well-presented and the idea is easy to follow.\n3. The Synergized learning strategy proposed in this paper is novel and interesting, which I believe could be a promising strategy in the future.\n4 The empirical evaluation shows promising results against baselines. The generalization performance, thanks to the flexibility of its symbolic update rule, is quite impressive against previous MetaBBOs. The authors also carefully ablate the components of their method."
                },
                "weaknesses": {
                    "value": "1. Though promising, I'm not sure if this work oversimplifies the actual optimization problem in its search space, which makes it not that useful currently. There are only 3 operators and 7 operands in its basic symbol set, is the design of the symbol set relevant to the teacher algorithm?\n2. It is good to see the improvement in the experiment, however, I note SYMBOL-E does not outperform MadDe (2 out of 3 columns). This raises the question of whether the potential of SYMBOL is largely limited by the performance of MadDe. If MadDe does not work well, can the exploration reward actually help the model learn a much better update rule? The authors could try to use a relatively bad baseline as the teacher to verify this and or find a challenging task where MadDe does not work well."
                },
                "questions": {
                    "value": "I'm not directly working in this field, so I'm a bit curious about how Lion [1] is compared to the symbolic discovery in this work, which I think could be more promising in the optimizer discovery. \n\n[1] Symbolic Discovery of Optimization Algorithms. Xiangning Chen, Chen Liang, Da Huang, Esteban Real, Kaiyuan Wang, Yao Liu, Hieu Pham, Xuanyi Dong, Thang Luong, Cho-Jui Hsieh, Yifeng Lu, Quoc V. Le. https://arxiv.org/abs/2302.06675."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission237/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698793458238,
            "cdate": 1698793458238,
            "tmdate": 1699635949144,
            "mdate": 1699635949144,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "DEZsaLavrm",
                "forum": "vLJcd43U7a",
                "replyto": "w6DONacvnr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission237/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission237/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ZLbt (Part 1/2)"
                    },
                    "comment": {
                        "value": "We appreciate the reviewer for the positive and valuable feedback. Thank you for acknowledging that our paper is easy to follow and that our SYMBOL is not only **flexible**, **novel**, and **interpretable**, but also showcases **promising** **performance** and **impressive** **generalization** abilities for future usage. We hope that the following response, as well as the newly added results, will clear the remaining concerns.\n\n---\n\n**[Is the symbol set relevant to the teacher algorithm?]** We would like to clarify that the current basis symbols set is essential to cover numerous update rules (e.g., those in DEs, PSOs, GAs, etc.), and we have opted for a simple yet effective symbol set that is sufficient to generate update rules covering various searching behaviors and exhibiting different extents of exploration and exploitation abilities. Experimental results in Section 4.3, Figure 5 consistently show SYMBOL achieves new state-of-the-art performance. Meanwhile, we note that our SYMBOL can work well even if the basis symbol set is not perfectly relevant to the teacher algorithm. For example, our SYMBOL-S could outperform the teacher CMA-ES even if our symbol set could not represent the covariance update processes in CMA-ES. Please refer to our response **[Design of the symbol set]** in the \u201cgeneral response\u201d above for more discussions.\n\n---\n\n**[Whether the potential of SYMBOL is largely limited by the performance of MadDE]** Thanks for raising this concern. According to our experiments, SYMBOL-G and SYMBOL-S both outperform MadDE, clearly demonstrating the potential of SYMBOL to exceed the employed teacher optimizer. However, SYMBOL-E, designed to independently discover novel optimizers without reliance on any particular teacher optimizer, underperforms MadDE. We acknowledge that navigating the complex landscapes of BBO tasks presents significant challenges for SYMBOL-E when learning solely from scratch, which deserves further exploration in the future. So far, SYMBOL-E's strength lies in scenarios where selecting an appropriate teacher optimizer is challenging.\n\n---\n\n**[Can the exploration reward help the model learn a much better update rule?]**  Yes, it helps. Following the suggestions by the reviewer, we address this concern from the following two perspective. \n\n**i) When the teacher optimizer is relatively bad:** In Section 4.3, we explored the use of a relatively less effective teacher, vanilla DE, within SYMBOL-S. Despite the limitations of the teacher optimizer, SYMBOL-S consistently outperformed it, as evidenced by the performance curves in Figure 5 (Section 4.3) and Figure 8 (Appendix C.1). This indicates the robustness of SYMBOL-S in surpassing teacher capabilities. \n\n**ii) When the optimization task is challenging for the teacher optimizer**: In the context of more challenging tasks such as HPO-B and Protein-docking (Table 1, Meta Generalization part), SYMBOL exhibits impressive performance even when the teacher optimizer (MadDE) struggles. Both SYMBOL-E and SYMBOL-G show only limited improvements when used independently, yet SYMBOL-S significantly outperforms MadDE, underscoring that SYMBOL is not constrained by its teacher's performance. The synergy between exploration and imitation rewards in SYMBOL not only demonstrates robust generalization across diverse tasks but also ensures rapid convergence."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission237/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700322683584,
                "cdate": 1700322683584,
                "tmdate": 1700322683584,
                "mdate": 1700322683584,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wp0yUyhiub",
                "forum": "vLJcd43U7a",
                "replyto": "JdwOYGhzRR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission237/Reviewer_ZLbt"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission237/Reviewer_ZLbt"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the response"
                    },
                    "comment": {
                        "value": "I want to thank the authors for addressing my concerns. I have carefully read the response, I think my main concern is about the relationship between SYMBOL's performance and MadDE's performance. I accept the author's response and I will keep my score."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission237/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700611737319,
                "cdate": 1700611737319,
                "tmdate": 1700611737319,
                "mdate": 1700611737319,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "59nYxkx5vr",
            "forum": "vLJcd43U7a",
            "replyto": "vLJcd43U7a",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission237/Reviewer_VT1T"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission237/Reviewer_VT1T"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes SIMBOL, a framework to learn a black-box optimizer through symbolic equation learning. The paper first presents a symbolic equation generator (SEG) to generates closed-form optimization rule, where such closed-form is found through reinforcement learning. The paper argues the proposed method shows state-of-the-art results on benchmarks as well as showing  strong zero-shot generalization capabilities."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Compared with existing BBO, the proposed method seems technically novel (I am not the expert). \n- The paper is generally well-written and easy-to-follow.\n- The results seem promising compared with existing methods."
                },
                "weaknesses": {
                    "value": "- There's no strategy in choosing a teacher optimizer ($\\kappa$).\n- Since it requires training based on reinforcement learning, training may require a time for framework compared with existing BBO that does not require any training. I wonder how long it takes for training compared with other MetaBBO method?"
                },
                "questions": {
                    "value": "I have no expertise in this area, so I will adjust my score based on discussion between authors reviewers, and AC."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission237/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698816782313,
            "cdate": 1698816782313,
            "tmdate": 1699635949068,
            "mdate": 1699635949068,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Dd0lonXeoJ",
                "forum": "vLJcd43U7a",
                "replyto": "59nYxkx5vr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission237/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission237/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer VT1T"
                    },
                    "comment": {
                        "value": "We appreciate the reviewer for the positive and valuable feedback. Thank you for acknowledging that our paper is well-written and that our proposed SYMBOL is novel and promising. We hope that the following response will clear the remaining concerns.\n\n---\n\n**[Strategy in choosing a teacher optimizer]** We thank the reviewer for the insightful comment. Please refer to our response **[Teacher selection in SYMBOL]** in the \u201cgeneral response\u201d above.\n\n---\n\n**[Training cost comparison between SYMBOL and baselines]** We would like to clarify that the training time of SYMBOL and other MetaBBO baselines have been reported in Section 4.1, Table 1. For your convenience, we list the training time in the table below.\n\n| SYMBOL-E | SYMBOL-G | SYMBOL-S | LDE | DEDDQN | Meta-ES | MELBA | RNN-Opt |\n| --- | --- | --- | --- | --- | --- | --- | --- |\n| 6h | 10h | 10h | 9h | 38m | 12h | 4h | 11h |\n\nWe hope the above clarifications help, and we are happy to engage in further discussions. Thanks for taking your time and making contributions to our paper!"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission237/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700322577687,
                "cdate": 1700322577687,
                "tmdate": 1700322577687,
                "mdate": 1700322577687,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "B13i2t6vCv",
                "forum": "vLJcd43U7a",
                "replyto": "Dd0lonXeoJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission237/Reviewer_VT1T"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission237/Reviewer_VT1T"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Thanks for the response. I've read the other reviews' as well and I think this is a valuable work (though I am not en expert in this area). Thus, I retain my score."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission237/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700693028332,
                "cdate": 1700693028332,
                "tmdate": 1700693028332,
                "mdate": 1700693028332,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Z5vpEil2mt",
            "forum": "vLJcd43U7a",
            "replyto": "vLJcd43U7a",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission237/Reviewer_h4Ki"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission237/Reviewer_h4Ki"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes meta-learning for black-box optimization (BBO) methods using symbolic equation learning. The proposed method, termed SYMBOL, trains the neural network model that generates the update rule of the BBO method depending on the task and optimization situation using landscape features as the input of the model. The model for generating the update rule is trained based on the reinforcement learning algorithm. The experimental evaluation using artificial benchmark functions, HPO, and Protein docking benchmarks demonstrates that the optimizer generated by the proposed SYMBOL can beat the existing BBO and MetaBBO baselines."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The proposed SYMBOL can dynamically change the update rule of solutions depending on the search situation owing to the use of fitness landscape features, which seems to be a technical novelty. In addition, the training strategy of the optimizer generator by mimicking the existing teacher black-box optimizer is reasonable for accelerating the model training.\n- The search performance of the optimizer generated by SYMBOL outperforms other BBO and recent MetaBBO techniques."
                },
                "weaknesses": {
                    "value": "- It is somewhat unclear to me the key difference and novelty of the proposed SYMBOL because there exists a lot of MetaBBO techniques. It would be very nice if the authors clarified the advantages and key differences of the SYMBOL compared to other existing methods.\n- Although the proposed SYMBOL can generate flexible update rules of solutions, the representation ability of update rules is limited depending on the pre-defined basic symbol set. I suppose that SYMBOL cannot generate the CMA-ES type update rules."
                },
                "questions": {
                    "value": "- Many techniques regarding MetaBBO, including meta genetic algorithm and meta evolutionary algorithm, have been developed so far [i]. It might be better to mention the traditional approaches related to MetaBBO.\n- The work of [ii] might relate to this paper. Although it only tunes the step-size adaptation in CMA-ES, the concept and used techniques, such as reinforcement learning-based training and guided policy search, are somewhat similar.\n- Why does SYMBOL-G outperform MadDE in Table 1? As the teacher optimizer of SYMBOL-G is MadDE in the experiment, it seems strange that SYMBOL-G beats the teacher optimizer.\n- Could you describe the relationship between MetaBBO and automatic algorithm configurations?\n\n[i] Qi Zhao, Qiqi Duan, Bai Yan, Shi Cheng, Yuhui Shi, \"A Survey on Automated Design of Metaheuristic Algorithms,\" arXiv:2303.06532\n\n[ii] Shala, G., Biedenkapp, A., Awad, N., Adriaensen, S., Lindauer, M., Hutter, F. (2020). Learning Step-Size Adaptation in CMA-ES. In: B\u00e4ck, T., et al. Parallel Problem Solving from Nature \u2013 PPSN XVI. PPSN 2020. Lecture Notes in Computer Science(), vol 12269. Springer, Cham. https://doi.org/10.1007/978-3-030-58112-1_48\n\nMinor comments:\n- On page 2, \"At the lower lever\" should be \"At the lower level.\"\n- In equation (3), the redundant right-side parenthesis exists.\n\n\n----- After the rebuttal -----\n\nThank you for the responses. As the responses are convincing, I keep my score on the acceptance side."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission237/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission237/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission237/Reviewer_h4Ki"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission237/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699418738337,
            "cdate": 1699418738337,
            "tmdate": 1700982923559,
            "mdate": 1700982923559,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "VJhYvjBTWJ",
                "forum": "vLJcd43U7a",
                "replyto": "Z5vpEil2mt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission237/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission237/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer h4Ki (Part 1/2)"
                    },
                    "comment": {
                        "value": "We appreciate the reviewer for the positive and valuable feedback. Thank you for acknowledging that our SYMBOL is technically novel, reasonable, and could outperform other BBO and MetaBBO methods. We hope that the following response, as well as the newly added results, will clear the remaining concerns.\n\n---\n\n**[Clarify the advantages and key differences of our SYMBOL]** We thank the reviewer for the suggestion and apologize for any confusion. **To the best of our knowledge,** **SYMBOL represents the initial endeavour to promote the automated discovery of BBO update rules through symbolic equation learning**. Compared to existing MetaBBO techniques, our SYMBOL is **more flexible** (can generate novel update rules beyond hand-crafted rules dynamically depending on the search situation), **more generalizable** (owing to the leveraged fitness landscape analysis as robust state features), and **more interpretable** (the generated update rules are in closed form). Meanwhile, our SYMBOL archives new state-of-the-art performance among other BBO and recently developed MetaBBO methods. Concerning other MetaBBO works, two primary branches exist \u2014 one for auto-configuration and the other for candidate solution proposals, which are discussed as follows:\n\n1. **MetaBBO for Auto-Configuration:** This branch involves a neural policy at the meta level that dictates configurations for the **hand-crafted update rules** in the lower-level optimizer, as seen in Table 1 with baselines like LDE, MELBA, and Meta-ES. Alternatively, some other works leverage the meta-level agent to select a proper **hand-crafted update rule** from a predetermined pool of rules, as done by the DEDDQN in Table 1. However, these methods may inherit potential limitations from the hand-crafted rules within the optimizer itself. SYMBOL minimizes dependence on hand-crafted rules and flexibly generates closed-form update rules upon a symbol set, enhancing the exploration capability of efficient update rules and consequently boosting the potential performance of the generated optimizers.\n2. **MetaBBO for Candidate Solution Proposal:** This branch employs RNNs to auto-regressively propose the next generation of candidates end-to-end without explicit update rules. Specifically, the input is the position of the current candidate(s), and the output is the next candidate(s). However, these models may easily get overfitted, and their efficiency may be limited to low-dimensional BBOs only, raising generalization concerns. In contrast, SYMBOL incorporates fitness landscape analysis into the state representation, aiming to construct a generalizable model that can produce flexible update rules for various problems, ensuring a more robust optimization. Additionally, while existing works for candidate proposals are entirely \"black-box\" with limited interpretability, SYMBOL's generated rules are step-by-step transparent, allowing for easier analysis and understanding of the learned processes.\n\nCorrespondingly, we have refined our discussions in the second paragraph of the **Introduction** section and the second paragraph of the **Related Works** section.\n\n---\n\n**[Representation ability of update rules]** We thank the reviewer for the insightful comment. Please refer to our response **[Design of the symbol set]** in the \u201cgeneral response\u201d above.\n\n---\n\n**[Q1: Mention traditional MetaBBO works]** We thank the reviewer for the suggestion. We have enriched the **Related Work** section. As introduced in [i], these traditional methods also adhere to the bi-level optimization paradigm. But in their paradigm, both the meta-level and lower-level components are implemented using traditional black-box optimizers. In addition, they generate a single strategy for the entire lower-level optimization process. Our SYMBOL differs in that a) it employs a learning-based RL-Agent as the meta-level method; b) it generates flexible update rules in the lower level; and c) it dynamically adjusts its strategy throughout the lower-level optimization process, adapting to changing conditions and ensuring greater flexibility."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission237/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700322229160,
                "cdate": 1700322229160,
                "tmdate": 1700322229160,
                "mdate": 1700322229160,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]