[
    {
        "title": "Investigating the Benefits of Projection Head for Representation Learning"
    },
    {
        "review": {
            "id": "1t4N5XRVu0",
            "forum": "GgEAdqYPNA",
            "replyto": "GgEAdqYPNA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8091/Reviewer_p748"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8091/Reviewer_p748"
            ],
            "content": {
                "summary": {
                    "value": "This paper studied a very interesting question: what makes pre-projection representations better if they are not directly optimized? Based on theoretical analysis on some toy models, they proposed that the implicit bias of training algorithms makes deeper features more unequal, and hence lower layers tend to have more normalized and less specialized representations. Then they showed that lower layers are better in the following cases: (1) data augmentation disrupts useful feature; (2) downstream-relevant features are too weak/strong in pre-training. They also showed how this mechanism makes lower representations better for supervised contrastive learning and supervised learning. Finally, they conducted some experiments to verify their theoretical analyses."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper studied a very interesting question: what makes pre-projection representations better if they are not directly optimized?\n2. Based on theoretical analysis on some toy models, they proposed that the implicit bias of training algorithms makes deeper features more unequal, and hence lower layers tend to have more normalized and less specialized representations.\n3. Then they showed that lower layers are better in the following cases: (1) data augmentation disrupts useful feature; (2) downstream-relevant features are too weak/strong in pre-training.\n4. They also showed how this mechanism makes lower representations better for supervised contrastive learning and supervised learning."
                },
                "weaknesses": {
                    "value": "My main concern is that their data and model are too simple. But if similar models are commonly used, it may be okay."
                },
                "questions": {
                    "value": "1. I want to know whether their data model (Def 3.1) is commonly used in literature? Even though the authors' theoretical analysis are insightful, I am worried that this data model is too simply to be applied to practical scenarios.\n2. Most of the cases where lower representations are better arises from the inappropriate data augmentation (like Thm 4.2), namely the pre-training signal does not align with the downstream problem. In such cases, overfiting the pre-training data (what post-projection layers intend to do) may lead poor downstream performance. In addition to using pre-projection layers, can other regularization methods (weight decay, dropout, early stopping, etc.) also lead to satisfied performance even without projection head? I noticed that the author mentioned in the end of Sec 3 that the advantage of pre-projection representations diminishes when using weight decay.\n3. The author mainly focused on two-layer neural networks. When there are multiple layers, how do the depth of representations affect the result? Is there any trade-off, like deeper layers have more representation power while less diversity/robustness? How to choose the appropriate depth of projection heads?\n\nOverall, I think this is an insightful work and I am glad to raise my score if the authors can clarify their over-simple model and give a slightly deeper answers to the above questions."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8091/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698404052745,
            "cdate": 1698404052745,
            "tmdate": 1699637002544,
            "mdate": 1699637002544,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "eJXa1Qyyx8",
                "forum": "GgEAdqYPNA",
                "replyto": "1t4N5XRVu0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8091/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8091/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer p748 -- Part 1"
                    },
                    "comment": {
                        "value": "> ## Relevance of our data model\n\n\n(1) Although simple, our model encompasses a wide array of aspects, demonstrating rich results. The analytical expression derived for the sample complexity indicator reveals intricate interactions among pretraining and downstream features, data augmentation, and noise.\n\n(2) Similar data models have been utilized in numerous studies, particularly within literature exploring feature learning. The data distribution defined in 3.1 is on a hyperrectangle, akin to a simpler model employed in (Saunshi 2021) that revealed compelling insights about contrastive learning. Additionally, when coupled with the data augmentation detailed in Definition 3.2, our data distribution effectively introduces noise into the input. A special case of our distribution is the sparse signal plus dense noise setting, extensively used in theoretical studies, with variations shown in Allen-Zhu 2020, Zou 2021, Shen 2022, Chen 2023 and more. This model naturally aligns with numerous machine learning scenarios and mirrors data structures in both image and language tasks (e.g., the second paragraph in Section 2.1 of Wen 2021 gives a brief overview of the sparse coding model's relevance). Moreover, Definition 3.2 also accounts for how augmentation can discourage the learning of a feature by decorrelating it within a positive pair. Our data distribution, combined with this augmentation modeling, is akin to models used in (Wen 2021), (Liu 2021), and (Xue 2023) for investigating feature learning in self-supervised learning.\n\n\n> ## \u201cMost of the cases \u2026 also lead to satisfied performance even without projection head?.\u201d\n\nWe briefly discuss a few points: (1) weight decay. Our experiments suggest weight decay has a more negative than positive impact. Theoretically, as outlined in Section 3, weight decay encourages the model to be more \u2018concise\u2019, leading to the elimination of 'unnecessary' weights according to the training objective, thus removing additional features learned in pre-projection. Empirically, as verified in Fig 3d, increased weight decay worsens performance with inappropriate data augmentation and negates the advantages of pre-projection. (2) early stopping. Intuitively, early stopping should result in a model that is less specialized towards the training objective, potentially benefiting the downstream task when there is a misalignment. However, perhaps counter intuitively, our MNIST-on-CIFAR-10 experiments reveal that early stopping improves post-projection with good augmentation ($p_{drop}=0$), but not with bad augmentation ($p_{drop}=0.2$).The corresponding plots are included in Fig 8, where we plot the linear evaluation accuracy against number of training epochs. A deeper analysis of training dynamics during the course of training is required to fully grasp the effect of early stopping. (3) The impact of dropout might not be immediately evident, but we believe that future research could utilize the analytical framework presented in our paper to investigate the effects of dropout."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8091/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700740323812,
                "cdate": 1700740323812,
                "tmdate": 1700740323812,
                "mdate": 1700740323812,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QBwMUFQMv7",
                "forum": "GgEAdqYPNA",
                "replyto": "1t4N5XRVu0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8091/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8091/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer p748 -- Part 2"
                    },
                    "comment": {
                        "value": "> ## \u201cThe author mainly focused on two-layer neural networks\u2026\u201d\n\nThis is a great question, but providing a clear answer for deep non-linear models poses a challenge with our current understanding of deep neural networks in the community. Nonetheless, we can offer some insights through deep linear models. As mentioned in section 4.1, Theorem 3.4 can be expanded to multiple-layer models with any loss function. \n\n(1) **Similar conclusions to those in Section 3.1 can be drawn**. For a $L$-layer network, we have $W_l(t)^\\top W_l(t) = W_{l+1}(t) W_{l+1}(t)^\\top$. Using this, one can further derive the sample complexity indicator for the setting considered in Section 3 as follows \n\n\\begin{align}\n    r_l=\\frac{ \\sum_{j=1}^p c_j^{2l/L} \\hat{\\phi_j}^2 }{ c_{j^*}^{2l/L}\\hat{\\phi}_{j^*}^2 }\n\\end{align}\n\nwhere\n\n\\begin{align}\n    c_j = \\frac{(1-\\alpha_j)\\phi_j}{\\phi_j^2 + \\sigma^2} ~~~ \\text{if}~~~ j\\in  \\{j_1, \\dots, j_{\\min\\{d, p\\}}\\} ~~~\\text{else}~~~ 0 \n\\end{align}\n\nThe definitions of $j_1, \\dots, j_{\\min\\{d, p\\}}$ remain the same as in Theorem 3.5, and other quantities are consistent with the definitions provided in Section 3. Here, $c_j$ represents the weight the full model would allocate to the j-th feature. Although the expression appears complex, some intuition can be gleaned from extreme scenarios: If $c_{j^*}$ is the largest, indicating that the full model assigns the most weight to the downstream relevant feature, $r_l$ decreases with $l$. This means one should just use the final-layer representations. Conversely, if $c_{j^*}$ is the smallest among non-zero $c_j$'s, indicating that the full model assigns the least weight to the downstream relevant feature, $r_l$ increases with $l$. In this case, using the lowest layer would be preferable. Applying these observations in conjunction with the relationship between $c_j$ and $\\alpha_j, \\phi_j$, similar conclusions to those in Corollary 3.7 regarding the impact of augmentation and feature strength for multi-layer models can be drawn. \n\n(2) **The greater the mismatch between the pretraining and downstream tasks, the lower the optimal layer tends to be**. What about situations that are more intricate, occurring between the above extreme cases? By setting specific values for $c_j$ and $\\hat{\\phi}_j$\u2019s, and plotting the sample complexity indicator $r_l$ against the depth $l$, we observe instances where intermediate layers perform better (which we believe are the most common cases in practice). Please refer to Fig 6, where we see a U-shaped curve. Additionally, we conduct further exploration by fixing the weights for all other features and vary the weight for the downstream relevant features (the exact configurations are detailed in Appendix C). Fig 6 shows that the optimal layer (corresponding to the bottom of the U-shaped curve) becomes lower as the weight assigned to the relevant feature decreases, which indicates a larger mismatch between the pretraining and downstream tasks.\n\n(3) **Challenges in locating the optimal layer**: Even in this simplified scenario, we observe that the depth of the optimal layer is influenced by various factors, including the position of the downstream-relevant feature, the strength of features in the downstream task, and the weights assigned to features during pretraining. The last one is further a function of features, noise and augmentations for pretraining data. In practical settings, we acknowledge that more factors may come into play, such as the model architecture, which varies across layers. Therefore, determining the exact optimal layer for downstream tasks is very challenging and represents an intriguing and valuable avenue for future exploration. **We believe the analytical framework established in this paper, capable of expressing downstream sample complexity in closed form through various elements in pretraining and downstream tasks, and explaining several observed phenomena (e.g., those depicted in Figure 3), can significantly aid advancing research in this direction**."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8091/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700740363576,
                "cdate": 1700740363576,
                "tmdate": 1700741181538,
                "mdate": 1700741181538,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3SSuKlPsYh",
                "forum": "GgEAdqYPNA",
                "replyto": "1t4N5XRVu0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8091/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8091/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reference"
                    },
                    "comment": {
                        "value": "Saunshi, Nikunj, et al. \"Understanding contrastive learning requires incorporating inductive biases.\" International Conference on Machine Learning. PMLR, 2022.\n\nShen, Ruoqi, S\u00e9bastien Bubeck, and Suriya Gunasekar. \"Data augmentation as feature manipulation.\" International conference on machine learning. PMLR, 2022.\n\nXue, Yihao, et al. \"Which Features are Learnt by Contrastive Learning? On the Role of Simplicity Bias in Class Collapse and Feature Suppression.\" arXiv preprint arXiv:2305.16536 (2023).\n\nChen, Jinghui, Yuan Cao, and Quanquan Gu. \"Benign overfitting in adversarially robust linear classification.\" Uncertainty in Artificial Intelligence. PMLR, 2023.\n\nZou, Difan, et al. \"Understanding the generalization of adam in learning neural networks with proper regularization.\" arXiv preprint arXiv:2108.11371 (2021).\n\nAllen-Zhu, Zeyuan, and Yuanzhi Li. \"Towards understanding ensemble, knowledge distillation and self-distillation in deep learning.\" arXiv preprint arXiv:2012.09816 (2020).\n\nWen, Zixin, and Yuanzhi Li. \"Toward understanding the feature learning process of self-supervised contrastive learning.\" International Conference on Machine Learning. PMLR, 2021."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8091/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700742689757,
                "cdate": 1700742689757,
                "tmdate": 1700742689757,
                "mdate": 1700742689757,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Ips6r4KPLm",
            "forum": "GgEAdqYPNA",
            "replyto": "GgEAdqYPNA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8091/Reviewer_B9NE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8091/Reviewer_B9NE"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates the effectiveness of the projection head in self supervised contrastive learning, supervised contrastive learning and supervised learning. It provides theoretical analysis of the quality and robustness of the learned representations and their generalizability in simple linear and nonlinear models. Theoretical results are supported with experimental evaluation on several image datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Thorough theoretical analysis and interesting insights.\n- Extensive related work."
                },
                "weaknesses": {
                    "value": "- Experimental evaluation is done on very simple networks and small datasets. Although the results nicely support the theoretical results, it might be beneficial to include one more complex experiment.\n- The paper could benefit from a discussion on limitations and assumptions of the analysis."
                },
                "questions": {
                    "value": "- In Definition 3, could you add details about $\\phi_i$?\n- Theorem 3.6 essentially tells us when it is beneficial to use pre and post projection representations, with concrete guidelines given in Corollary 3.7. Do these results hold for non linear models as well? As a stretch question, I was wondering how could one infer the downstream-relevant features and the weights of features in practice? In other words, is there a way to use your results to identify what features will be relevant for the downstream task? I am aware this might be out of scope of this work. \n- What is $\\alpha_i$ in setting 2 in Sec 5.1? Could you comment on the results in Fig 1 right, where $\\phi_i$ = 0.2 and 0.4, in particular, why do we see the spikes in weights in pre-features for $\\phi_i$ = 0.2 and post-features for $\\phi_i$ = 0.4?\n- I do not understand the experiment in Fig 3b. In the text your say that \u201cFigure 3b shows the downstream accuracy against $p_{drop}$ and s=1\u201d. However, if s=1, it means that you only use MNIST images as input to the augmentation, and then you additionally drop the digit with probability $p_{drop}$. Doesn\u2019t that mean your augmented image is completely black if $p_{drop}=1$?\n- In Table 2, WaterBird SL results are almost the same, especially for no projection and pre-projection. Why is that? \n- Could you explain better what exactly is plotted in Figure 4?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8091/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698740804220,
            "cdate": 1698740804220,
            "tmdate": 1699637002385,
            "mdate": 1699637002385,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4KAdXLg7UL",
                "forum": "GgEAdqYPNA",
                "replyto": "Ips6r4KPLm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8091/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8091/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> ## \u201cExperimental evaluation is done on very simple networks and small datasets. Although the results nicely support the theoretical results, it might be beneficial to include one more complex experiment.\u201d\n\n(1) Many empirical studies have already demonstrated the effects of the projection head and data augmentation via large scale experiments. For instance, Chen et al. (2020) in Table 3, Bordes et al. (2023) in Figure 2, and Rashtchian et al. (2023) in Table 2 present experimental results for self-supervised learning, showing that: a) Information suppressed by data augmentation, such as Color, Hue, Rotation, and Spot, is more retrievable in pre-projection representations. b) Consequently, if the downstream task requires this suppressed information, utilizing pre-projection is preferable. They also provide results about supervised learning. Therefore, our objective is not to replicate these results but to provide theoretical understanding for them.\n\n(2) Additionally, we conducted another set of experiments on CIFAR-10, maintaining the same scale as our MNIST-on-CIFAR-10 experiment in the original version, but incorporating more natural methods to control the feature's strength. The experiment details can be found in Appendix D.1. In summary, we established the downstream task as categorizing CIFAR-10 images as more \u2018red,\u2019 \u2018green,\u2019 or \u2018blue.\u2019 To control the color distinguishability (the clarity in identifying a color in images), we utilized two approaches: 1) manipulating the images, or 2) selecting subsets with varying color distinguishability. This effectively controls the downstream-relevant feature\u2019s strength. Our observations in Fig 7(b)(c) revealed that, in both scenarios, the advantage of pre-projection is more pronounced when the color feature is either too weak or too strong.\n\n(3) Moreover, our last experiment in section 5 focuses on ImageNet, demonstrating how one can enhance the utility of ImageNet-pretrained model\u2019s representations under distribution shifts. This is achieved by finetuning the model with an additional projection head and subsequently removing the projection head. We believe this finding is highly practical, offering a simple and efficient way to improve the model's robustness on large datasets.\n\n\n\u201cThe paper could benefit from a discussion on limitations and assumptions of the analysis.\u201d\n\n> ## Discussion on limitations and assumptions of the analysis\n\nOne limitation arises from the assumptions made in the analysis of non-linear models, where we simplified the analysis by considering a diagonalized network rather than a fully connected one. This difference may affect training dynamics, posing challenges in characterizing the learning of additional features in a fully connected multi-layer non-linear network. Nevertheless, we maintain that insights drawn from our proof in Appendix A.4 (that ReLU function\u2019s threshold behavior filters gradients to preserve additional features in the pre-projection layer) should still apply and could assist in analyzing this more complex scenario. \nMoreover, while the experiment in Fig 4 demonstrates the benefits of the projection head amid distribution shifts, our discussion in Section 4.1 provides a general insight based on linear models, rather than characterizing this effect for nonlinear modes. The technical challenge arises from our analysis of the diagonalized nonlinear model, which relies on the coordinate-wise symmetric property of the input distribution. To study distribution shift, one often needs to consider data distributions with spurious correlations, (e.g., the data model in Sagawa 2020), where this property no longer holds due to correlations between features. Further exploration in this direction would be valuable to deepen the theoretical understanding of the observations presented in Section 4.1.\n\n> ## $\\phi_i$ in Definition 3\n\nThe explanation of the meaning of $\\phi_i$ can be found in the paragraph following Definition 3.2. In this data model, each coordinate represents a feature, and the corresponding $\\phi_i$ represents the magnitude (strength) of that feature."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8091/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700740706649,
                "cdate": 1700740706649,
                "tmdate": 1700740706649,
                "mdate": 1700740706649,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BLgGTlpu9p",
                "forum": "GgEAdqYPNA",
                "replyto": "Ips6r4KPLm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8091/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8091/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> ## \u201cTheorem 3.6 essentially tells us when it is beneficial to use pre and post projection representations, with concrete guidelines given in Corollary 3.7. Do these results hold for non linear models as well?\u201d\n\n\nEmpirically, as depicted in Fig 1, these findings also hold true for non-linear models. The model employed for Fig 1 is a two-layer fully connected neural network. In Fig 1 Left, where all features have the same strength but are disrupted differently by augmentation, we see that pre-projection treats features more equally compared to post-projection, and larger disruptions from augmentation result in more significant differences between pre-projection and post-projection weights. These would lead to the first conclusion of Corollary 3.7. In Fig 1 Right, the feature-moderating effect demonstrated aligns with Theorem 3.5, showcasing that both the strongest and weakest features are weighted the least. This observation essentially supports the second and third conclusions of Corollary 3.7. Overall, our findings in Corollary 3.7 apply to non-linear models. However, explicitly and theoretically characterizing these phenomena in non linear models remains a challenging aspect, which points toward future directions.\n\n\n> ## \u201cAs a stretch question, I was wondering how could one infer the downstream-relevant features and the weights of features in practice? In other words, is there a way to use your results to identify what features will be relevant for the downstream task? I am aware this might be out of scope of this work.\u201d\n\nAlthough beyond the scope of this paper, identifying relevant features and quantifying their assigned weights could potentially be achieved using existing analysis tools, such as visual explanation tools (Selvaraju 2016, which visualizes the regions of an image that contribute most to a neural network's decision-making) or PCA-based analysis (Jiang 2023, which defines features of an image to be the projection onto the principal components of different models\u2019 last-layer activations). In future research, if these methods can be applied and combined with the findings of our paper, they might enable us to adjust the weighting of features to replicate the effects of a projection head, essentially reducing the reliance on using a projection head.\n\n\n> ## \u201cWhat is alpha_i  in setting 2 in Sec 5.1?\u201d\n\n$\\alpha_i$ in setting 2 in Sec 5.1 is consistent with Definition 3.2 for data augmentation. For each $i$, the data augmentation randomizes the $i$-th feature with a probability of $\\alpha_i$. Therefore, larger $\\alpha_i$ means the data augmentation disrupts this feature more, consequently disencouraging the learning of this feature more. In setting 2, we set all $\\alpha_i$\u2019s to the same, meaning that the augmentation treats all features equally. This enables us to isolate and study the effect of feature strength.\n\n> ## \u201cCould you comment on the results in Fig 1 right, where phi_i = 0.2 and 0.4, in particular, why do we see the spikes in weights in pre-features for phi_i = 0.2 and post-features for phi_i = 0.4?\u201d\n\nFig 1 Right aims to demonstrate how the model moderates the features as captured in Theorem 3.5, where both the strongest and weakest features are weighted the least, while features with intermediate strengths are weighted the most. This effect is precisely depicted in the figure. The spikes observed at intermediate strengths (e.g., 0.2 and 4) signify that these features are weighted more than those that are either stronger or weaker than them, aligning exactly with our expectations based on Theorem 3.5. However, using a two-layer ReLU network may result in some differences compared to a linear model. For instance, the positions of spikes in pre-projection and post-projection do not align, unlike in a linear model. Understanding these differences might require a more detailed analysis of nonlinear models for future research. Nonetheless, our primary goal of validating the overall behavior is achieved."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8091/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700740803868,
                "cdate": 1700740803868,
                "tmdate": 1700740803868,
                "mdate": 1700740803868,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "b9rjv7LjOv",
                "forum": "GgEAdqYPNA",
                "replyto": "Ips6r4KPLm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8091/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8091/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> ## Setting of MNIST-on-CIFAR-10\n\nThe original text was not entirely accurate, and we apologize for any confusion caused. However, the sample images presented in Fig 3 (a) accurately reflect our process: We only perform a weighted sum between CIFAR-10 and MNIST on the digit's body area, while preserving the other parts of the image unchanged from the CIFAR-10 image. Essentially, we modify the transparency of the digit's body. The code used to blend these two images is roughly as follows \n\n```\nnew_image = (mnist_image <= 0) * cifar_image +  (mnist_image > 0) * (s * mnist_image + (1-s)* cifar_image)\n```\nHere after being converted to RGB using .convert('RGB') the MNIST image holds negative values for areas that are not part of the digit\u2019s body. Therefore, varying s does not change the parts of the CIFAR-10 image that do not overlay with the digit\u2019s body. \n\nWe revised the experiment description for this part in Section 5.1, highlighted in blue.\n\n> ## \u201cIn Table 2, WaterBird SL results are almost the same, especially for no projection and pre-projection. Why is that?\u201d\n\nThis interesting phenomenon might be explained as follows. SCL loss directly operates on the post-projection representations, explicitly defining their expected arrangement. In contrast, SL loss is computed on the prediction (logits), one layer after the representations, and therefore does not explicitly define the arrangement of the representations. As a result, SL is \u2018milder\u2019 than SCL, allowing the representations to be less specialized towards the training task (this is reflected in SL's higher worst-group accuracy in Table 2). This, along with the relatively simple nature of the WaterBirds dataset, allows SL\u2019s post-projection representations to already learn most features necessary for the downstream task, making pre-projection\u2019s advantage less obvious. \n\n> ## \u201cCould you explain better what exactly is plotted in Figure 4?\u201d\n\nThe plot in Figure 4 represents the OOD-vs-ID evaluation typically employed in studies on distribution shift robustness (e.g., see Taori 2020, Miller 2021, Radford 2021). We specifically compared the robustness of representations learned by different methods on ImageNet. Following Kirichenko 2022, we consider 4 different OOD benchmarks: Mixed-Rand, FG-Only, ImageNet-R, and ImageNet-A, and let the OOD accuracy be the average over the four datasets. As Mixed-Rand and FG-Only contain only nine ImageNet classes, we define ID accuracy as the accuracy on the corresponding nine classes in the original ImageNet (ImageNet-9) for comparison.\n\nOur aim is to assess how robust representations are against distribution shifts in different models trained on the original ImageNet. To achieve this, we followed Kirichenko 2022 which shows that training a linear classifier on the representations, using a mix of data from original ImageNet-9 and Mixed-Rand can lead to decent accuracy across OOD datasets. Therefore we use the OOD accuracy of this linear classifier to indicate the quality of the representations. Additionally, as suggested by Taori 2020, we also take into account the ID accuracy by plotting the OOD-ID relation to have a more comprehensive comparison. To do this, we varied the sample size for training the linear classifier to obtain different OOD-ID pairs and plotted them to observe the relationship. Our findings reveal that pre-projection (where the publicly available ImageNet pretrained model is fine-tuned on ImageNet for 50 epochs with an additional projection head which subsequently removed) not only achieves the highest overall OOD accuracy (the star in Figure 4) but also demonstrates a superior OOD-ID relationship. Specifically, (1) under the same ID distribution, it achieves better OOD distribution, and (2) compared to others, its OOD-ID slope is larger, indicating that a greater gain in OOD accuracy can be achieved by improving ID accuracy."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8091/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700740874474,
                "cdate": 1700740874474,
                "tmdate": 1700742459053,
                "mdate": 1700742459053,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "iqPrbFPX96",
                "forum": "GgEAdqYPNA",
                "replyto": "Ips6r4KPLm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8091/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8091/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reference"
                    },
                    "comment": {
                        "value": "Miller, John P., et al. \"Accuracy on the line: on the strong correlation between out-of-distribution and in-distribution generalization.\" International Conference on Machine Learning. PMLR, 2021.\n\nRadford, Alec, et al. \"Learning transferable visual models from natural language supervision.\" International conference on machine learning. PMLR, 2021.\n\nTaori, Rohan, et al. \"Measuring robustness to natural distribution shifts in image classification.\" Advances in Neural Information Processing Systems 33 (2020): 18583-18599."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8091/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700740903733,
                "cdate": 1700740903733,
                "tmdate": 1700740903733,
                "mdate": 1700740903733,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "FYGj1JN1Vx",
            "forum": "GgEAdqYPNA",
            "replyto": "GgEAdqYPNA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8091/Reviewer_EBwM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8091/Reviewer_EBwM"
            ],
            "content": {
                "summary": {
                    "value": "The paper delves into a nuanced aspect of neural network architecture design, specifically the use of a projection head during the training phase. This technique has garnered attention due to its empirical success in enhancing representation quality. The core methodology involves appending a projection head atop the encoder during training, which is subsequently discarded, favoring the pre-projection layer representations for inference tasks.\n\nDespite its proven practical effectiveness, a comprehensive theoretical understanding of why the projection head enhances representation learning remains underdeveloped. The paper aims to bridge this gap by dissecting the mechanics of the projection head and elucidating its impact on the learning dynamics of neural networks. This investigation is critical as it addresses a disconnect between empirical practices and their theoretical foundations in the field of deep learning.\n\nThe projection head's primary role is hypothesized to act as a regularization mechanism, potentially aiding in learning more generalizable and robust features. By expanding the representational capacity during training, the projection head could encourage the encoder to learn a broader set of features, some of which may be discarded during the projection phase but still contribute to a richer feature space in the pre-projection layer. Moreover, the projection head could serve to disentangle the feature space, making it easier for the network to differentiate between relevant and irrelevant features. This disentanglement might facilitate better generalization to new, unseen data by reducing overfitting to the idiosyncrasies present in the training dataset."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "**Empirical and Theoretical Integration**:\nThe paper bridges the gap between empirical success and theoretical understanding by critically investigating the role of the projection head in representation learning. By scrutinizing a technique that has demonstrated practical effectiveness without a solid theoretical foundation, the paper contributes to a more profound understanding of neural network architectures, potentially guiding future designs with a better-informed rationale.\n\n**Regularization and Feature Representation**:\nIt hypothesizes that the projection head is a regularization mechanism, allowing the encoder to explore a wider feature space during training. This could lead to more robust and generalizable representations, as the encoder is encouraged to capture a broader and more nuanced feature landscape. The paper's exploration of this aspect could elucidate how neural networks can be trained more effectively to learn generalizable features.\n\n**Feature Disentanglement**:\nThe projection head's potential to disentangle the feature space is a significant strength of the paper's hypothesis. By facilitating a clearer separation of relevant and irrelevant features, the projection head might aid in reducing overfitting and improving the model's ability to generalize to unseen data. This aspect of the paper could contribute valuable insights into how neural networks can be made more interpretable and reliable."
                },
                "weaknesses": {
                    "value": "**Potential Overfitting Risks**:\nThe introduction of a projection head could potentially lead to overfitting, especially if not properly regularized or if used in conjunction with datasets that have a high degree of noise or variability. The paper should address these risks and propose strategies to mitigate them.\n\n**Generalizability and Applicability**:\nThe projection head's effectiveness might vary across different architectures, tasks, and data modalities. The paper could benefit from a more detailed exploration of these variations to understand where the projection head is most beneficial and where it might be detrimental. This would enhance the paper's technical depth and practical applicability."
                },
                "questions": {
                    "value": "1. **How does the architecture of the projection head influence the learning dynamics and final representation quality?**\n   - The design choices within the projection head (e.g., the number of layers, types of activations, dropout rates) likely have a profound impact on its efficacy as a regularizer and feature disentangler. What are the optimal architectural configurations for different types of data and tasks? Investigating this could provide more nuanced guidelines for practitioners and lead to a deeper theoretical understanding of the projection head's role.\n\n2. **What is the impact of the projection head on the interpretability of the learned representations?**\n   - While the projection head might aid in learning more generalizable features, its impact on the interpretability of these features is unclear. Do the representations learned with a projection head offer better clarity in terms of feature importance or contribution to the final decision? Understanding this could bridge the gap between performance and explainability in neural networks.\n\n3. **Can the benefits of the projection head be replicated or enhanced by alternative or complementary techniques?**\n   - Are there other methods or architectural innovations that could either replicate the benefits of the projection head or enhance its effects? For instance, could certain types of normalization, attention mechanisms, or even different training paradigms offer similar or greater benefits regarding feature representation and generalization? Exploring this could lead to a broader set of tools for improving neural network training beyond the projection head.\n\nDelving into these questions could significantly enhance the paper's contribution, offering both a deeper theoretical understanding and more practical guidelines for employing projection heads in neural network training."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8091/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698903821703,
            "cdate": 1698903821703,
            "tmdate": 1699637002276,
            "mdate": 1699637002276,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "cPSHvR0xIl",
                "forum": "GgEAdqYPNA",
                "replyto": "FYGj1JN1Vx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8091/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8091/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer EBwM -- Part 1"
                    },
                    "comment": {
                        "value": "> ## \u201cPotential Overfitting Risks \u2026 The paper should address these risks and propose strategies to mitigate them\u201d\n\n(1) it's important to note that we're not proposing a method; rather, we're investigating the roles of the projection head. (2) We have shown that the projection head acts more as a form of regularization rather than exacerbating overfitting. It prevents the representations from being overly specialized to the pre-training task, which can potentially have some misalignment with the downstream task. Therefore, the paper precisely delves into studying how the projection head addresses these aspects mentioned by the reviewer.\n\n\n> ## \u201cGeneralizability and Applicability \u2026 where the projection head is most beneficial and where it might be detrimental.\u201d\n\n(1) Regarding the 'effectiveness across different tasks' and 'where the projection head is most beneficial and where it might be detrimental,' our paper precisely focuses on these aspects. It delves deeply into how the benefits of the projection head are reliant on the relationship between training and downstream tasks. The paper covers various cases, including the effects of data augmentation and feature strength. Additionally, we explore different types of pre-training losses, encompassing both self-supervised CL, supervised CL, and standard supervised learning.\n\n(2) Regarding architectures and data modalities, it's important to acknowledge the vast scope of these topics, which makes comprehensive coverage in a single paper unfeasible, despite their significance. Our primary objective revolves around offering the first analytical framework to study and theoretically characterize the projection head's role and its interaction with features and data augmentations. We believe this contribution merits recognition.\n\n\n> ## \u2018How does the architecture \u2026 projection head\u2019s role\u2019\n\nIn the following two points, we discuss how our paper's analysis provides insight regarding the number of layers in the projection head and activation."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8091/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700740470128,
                "cdate": 1700740470128,
                "tmdate": 1700740470128,
                "mdate": 1700740470128,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "67QxlOxlh1",
                "forum": "GgEAdqYPNA",
                "replyto": "FYGj1JN1Vx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8091/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8091/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer EBwM -- Part 2"
                    },
                    "comment": {
                        "value": "> ## Effect of the number of layers \n\n\nTo grasp the impact of the number of layers, we essentially aim to determine when it becomes beneficial to extract representations from layers preceding the penultimate layer, and, if so, how many layers prior to that point? Providing a clear answer for deep non-linear models poses a challenge with our current understanding of deep neural networks in the community. Nonetheless, we can offer some insights through deep linear models. As mentioned in section 4.1, Theorem 3.4 can be expanded to multiple-layer models with any loss function. \n(1) **Similar conclusions to those in Section 3.1 can be drawn**. For a $L$-layer network, we have $W_l(t)^\\top W_l(t) = W_{l+1}(t) W_{l+1}(t)^\\top$. Using this, one can further derive the sample complexity indicator for the setting considered in Section 3 as follows \n\n\\begin{align}\n    r_l=\\frac{ \\sum_{j=1}^p c_j^{2l/L} \\hat{\\phi_j}^2 }{ c_{j^*}^{2l/L}\\hat{\\phi}_{j^*}^2 }\n\\end{align}\n\nwhere\n\n\\begin{align}\n    c_j = \\frac{(1-\\alpha_j)\\phi_j}{\\phi_j^2 + \\sigma^2} ~~~ \\text{if}~~~ j\\in  \\{j_1, \\dots, j_{\\min\\{d, p\\}}\\} ~~~\\text{else}~~~ 0 \n\\end{align}\n\nThe definitions of $j_1, \\dots, j_{\\min\\{d, p\\}}$ remain the same as in Theorem 3.5, and other quantities are consistent with the definitions provided in Section 3. Here, $c_j$ represents the weight the full model would allocate to the j-th feature. Although the expression appears complex, some intuition can be gleaned from extreme scenarios: If $c_{j^*}$ is the largest, indicating that the full model assigns the most weight to the downstream relevant feature, $r_l$ decreases with $l$. This means one should just use the final-layer representations. Conversely, if $c_{j^*}$ is the smallest among non-zero $c_j$'s, indicating that the full model assigns the least weight to the downstream relevant feature, $r_l$ increases with $l$. In this case, using the lowest layer would be preferable. Applying these observations in conjunction with the relationship between $c_j$ and $\\alpha_j, \\phi_j$, similar conclusions to those in Corollary 3.7 regarding the impact of augmentation and feature strength for multi-layer models can be drawn. \n\n(2) **The greater the mismatch between the pretraining and downstream tasks, the lower the optimal layer tends to be**. What about situations that are more intricate, occurring between the above extreme cases? By setting specific values for $c_j$ and $\\hat{\\phi}_j$\u2019s, and plotting the sample complexity indicator $r_l$ against the depth $l$, we observe instances where intermediate layers perform better (which we believe are the most common cases in practice). Please refer to Fig 6, where we see a U-shaped curve. Additionally, we conduct further exploration by fixing the weights for all other features and vary the weight for the downstream relevant features (the exact configurations are detailed in Appendix C). Fig 6 shows that the optimal layer (corresponding to the bottom of the U-shaped curve) becomes lower as the weight assigned to the relevant feature decreases, which indicates a larger mismatch between the pretraining and downstream tasks.\n\n(3) **Challenges in locating the optimal layer**: Even in this simplified scenario, we observe that the depth of the optimal layer is influenced by various factors, including the position of the downstream-relevant feature, the strength of features in the downstream task, and the weights assigned to features during pretraining. The last one is further a function of features, noise and augmentations for pretraining data. In practical settings, we acknowledge that more factors may come into play, such as the model architecture, which varies across layers. Therefore, determining the exact optimal layer for downstream tasks is very challenging and represents an intriguing and valuable avenue for future exploration. **We believe the analytical framework established in this paper, capable of expressing downstream sample complexity in closed form through various elements in pretraining and downstream tasks, and explaining several observed phenomena (e.g., those depicted in Figure 3), can significantly aid advancing research in this direction**.\n\n> ## Role of ReLu activation\n\nIn both Thm 3.8 and 4.2, we have shown how ReLu activation enables the pre-projection layer to learn features that are not learned at all by the post-projection layer. This occurs in both self-supervised CL, supervised CL, and standard supervised learning, whereas linear models cannot exhibit this behavior. Drawing intuition from our proof in Appendix A.4, the ReLu function exhibits a threshold-like behavior, effectively filtering the gradient passed from post-projection to prevent the elimination of the additional feature in the pre-projection layer. It could be intriguing for future research to explore whether different types of nonlinearity (e.g., GeLu, Leaky ReLu) yield varying effects and potentially facilitate a more tailored design for the projection head."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8091/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700740583070,
                "cdate": 1700740583070,
                "tmdate": 1700740583070,
                "mdate": 1700740583070,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3Zl2U2wft9",
                "forum": "GgEAdqYPNA",
                "replyto": "FYGj1JN1Vx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8091/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8091/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer EBwm -- Part 3"
                    },
                    "comment": {
                        "value": "> ## impact of the projection head on the interpretability of the learned representations\n\nInterpretability isn't our primary focus in this paper. We do encourage future research to leverage our analytical framework for exploring interpretability, given that our framework offers an analytical expression of the sample complexity indicator for each layer, involving various aspects of features and data augmentation. Nevertheless, this topic falls outside the scope of our current study.\n\nIn terms of feature importance and contribution, our analysis in Thm 3.5 shows that the features are represented more equally at pre-projection, and Thm 3.8 and Thm 4.2 show that pre-projection can learn more features. Consequently, when there is a misalignment between pretraining and downstream objectives, the feature important for the downstream task is more likely to 'survive' in pre-projection.\n\n\n\n> ##  \u201cCan the benefits of the projection head be replicated \u2026\u201d\n\n\nOur study paves the way for exploring these topics. Generally, the projection head can be utilized as a black box or a more general technique to improve representations. Our findings precisely identify the reasons behind the benefits of the projection head, allowing for the implementation of more targeted techniques in various scenarios. These may include improved data augmentation strategies (to avoid disturbing the downstream relevant feature), larger representation dimensions (to learn more features), and employing early stopping techniques (to prevent the representations from being overly specialized towards the training task). We believe exploring alternative techniques is an interesting direction for future research, and our work serves the first step towards this direction by demystifying the role of the projection head."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8091/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700740633834,
                "cdate": 1700740633834,
                "tmdate": 1700740633834,
                "mdate": 1700740633834,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "XIzBecsrmQ",
            "forum": "GgEAdqYPNA",
            "replyto": "GgEAdqYPNA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8091/Reviewer_Kpzq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8091/Reviewer_Kpzq"
            ],
            "content": {
                "summary": {
                    "value": "This paper analyzed an important technique in contrastive learning: the projection head. The authors theoretically demonstrated the benefits of the projection layer via a simplified model. The theoretical analysis showed that lower layers represent features more evenly in linear networks and can represent more features in non-linear networks, which implies better generalization performance in downstream tasks. Empirically, they verify the theoretical findings on synthetic and real-world datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The projection layer is one of the most important techniques in contrastive learning and the mechanism behind it is still under-explored. Consequently, this paper addresses an important problem.\n2. The theoretical analysis in this paper looks solid and insightful. And the empirical results verify the theoretical findings."
                },
                "weaknesses": {
                    "value": "1. The theoretical analysis in this paper is based on a two-layer model. Is it possible to extend the results to multiple-layer networks? For example, should we discard other layers except for the projection head in downstream tasks in the deep networks? It would be better to provide more discussions about that.\n2. This paper demonstrates the benefits of the projection head. However, we can observe that the designs of the projector (e.g., the layers and the dimensions) also have a significant influence on the downstream performance. Is it possible to provide some insights about the design of the projector based on the theoretical analysis in this paper? \n3. As stated in this paper, the pre-projection representations are preferred in three different scenarios and the findings are verified on the synthetic datasets. However, the authors do not show similar results (e.g., the influence of data augmentations) on the real-world datasets. It would be better to provide more empirical findings on real-world datasets.\n4. I note that the abstract on the OpenReview website is different from that on the pdf file, which should be corrected.\n5. The forms of references are inconsistent. For example, some of the conferences are full titles while others are abbreviations."
                },
                "questions": {
                    "value": "see my comments above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8091/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699101880846,
            "cdate": 1699101880846,
            "tmdate": 1699637002155,
            "mdate": 1699637002155,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "T5e5EHtDqh",
                "forum": "GgEAdqYPNA",
                "replyto": "XIzBecsrmQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8091/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8091/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Kpzq -- Part 1"
                    },
                    "comment": {
                        "value": "> ## Multi-layer\n\nThis is a great question, but providing a clear answer for deep non-linear models poses a challenge with our current understanding of deep neural networks in the community. Nonetheless, we can offer some insights through deep linear models. As mentioned in section 4.1, Theorem 3.4 can be expanded to multiple-layer models with any loss function. The following discussion is also added to Appendix C.\n\n(1) **Similar conclusions to those in Section 3.1 can be drawn**. For a $L$-layer network, we have $W_l(t)^\\top W_l(t) = W_{l+1}(t) W_{l+1}(t)^\\top$. Using this, one can further derive the sample complexity indicator for the setting considered in Section 3 as follows \n\n\\begin{align}\n    r_l=\\frac{ \\sum_{j=1}^p c_j^{2l/L} \\hat{\\phi_j}^2 }{ c_{j^*}^{2l/L}\\hat{\\phi}_{j^*}^2 }\n\\end{align}\n\nwhere\n\n\\begin{align}\n    c_j = \\frac{(1-\\alpha_j)\\phi_j}{\\phi_j^2 + \\sigma^2} ~~~ \\text{if}~~~ j\\in  \\{j_1, \\dots, j_{\\min\\{d, p\\}}\\} ~~~\\text{else}~~~ 0 \n\\end{align}\n\nThe definitions of $j_1, \\dots, j_{\\min\\{d, p\\}}$ remain the same as in Theorem 3.5, and other quantities are consistent with the definitions provided in Section 3. Here, $c_j$ represents the weight the full model would allocate to the j-th feature. Although the expression appears complex, some intuition can be gleaned from extreme scenarios: If $c_{j^*}$ is the largest, indicating that the full model assigns the most weight to the downstream relevant feature, $r_l$ decreases with $l$. This means one should just use the final-layer representations. Conversely, if $c_{j^*}$ is the smallest among non-zero $c_j$'s, indicating that the full model assigns the least weight to the downstream relevant feature, $r_l$ increases with $l$. In this case, using the lowest layer would be preferable. Applying these observations in conjunction with the relationship between $c_j$ and $\\alpha_j, \\phi_j$, similar conclusions to those in Corollary 3.7 regarding the impact of augmentation and feature strength for multi-layer models can be drawn. \n\n(2) **The greater the mismatch between the pretraining and downstream tasks, the lower the optimal layer tends to be**. What about situations that are more intricate, occurring between the above extreme cases? By setting specific values for $c_j$ and $\\hat{\\phi}_j$\u2019s, and plotting the sample complexity indicator $r_l$ against the depth $l$, we observe instances where intermediate layers perform better (which we believe are the most common cases in practice). Please refer to Fig 6, where we see a U-shaped curve. Additionally, we conduct further exploration by fixing the weights for all other features and vary the weight for the downstream relevant features (the exact configurations are detailed in Appendix C). Fig 6 shows that the optimal layer (corresponding to the bottom of the U-shaped curve) becomes lower as the weight assigned to the relevant feature decreases, which indicates a larger mismatch between the pretraining and downstream tasks.\n\n(3) **Challenges in locating the optimal layer**: Even in this simplified scenario, we observe that the depth of the optimal layer is influenced by various factors, including the position of the downstream-relevant feature, the strength of features in the downstream task, and the weights assigned to features during pretraining. The last one is further a function of features, noise and augmentations for pretraining data. In practical settings, we acknowledge that more factors may come into play, such as the model architecture, which varies across layers. Therefore, determining the exact optimal layer for downstream tasks is very challenging and represents an intriguing and valuable avenue for future exploration. **We believe the analytical framework established in this paper, capable of expressing downstream sample complexity in closed form through various elements in pretraining and downstream tasks, and explaining several observed phenomena (e.g., those depicted in Figure 3), can significantly aid advancing research in this direction**."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8091/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700740084801,
                "cdate": 1700740084801,
                "tmdate": 1700740084801,
                "mdate": 1700740084801,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "slFEIneqRK",
                "forum": "GgEAdqYPNA",
                "replyto": "XIzBecsrmQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8091/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8091/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Kpzq -- Part 2"
                    },
                    "comment": {
                        "value": "> ## \u201cThis paper demonstrates \u2026 insights about the design of the projector based on the theoretical analysis in this paper?\u201d\n\n(1) **Layers**. The answer for the previous question has already discussed the effect of depth, where we see that the depth of the optimal layer is intricately dependent on many factors, therefore hard to decide. But the high level intuition based on that discussion is: with a greater mismatch between the pretraining and downstream tasks, more layers should be stacked in the projection head. (2) **Role of ReLu activation**. In both Theorems 3.8 and 4.2, we have shown how ReLu activation enables the pre-projection layer to learn features that are not learned at all by the post-projection layer. This occurs in both self-supervised CL, supervised CL, and standard supervised learning, whereas linear models cannot exhibit this behavior. Drawing intuition from our proof in Appendix A.4, the ReLu function exhibits a threshold-like behavior, effectively filtering the gradient passed from post-projection to prevent the elimination of the additional feature in the pre-projection layer. It could be intriguing for future research to explore whether different types of nonlinearity (e.g., GeLu, Leaky ReLu) yield varying effects and potentially facilitate a more tailored design for the projection head.\n\n\n\n> ## Validating theoretical findings on real-world datasets\n\n(1) **In terms of data augmentation's impact, empirical evidence from prior large-scale empirical studies strongly corroborates our theoretical findings**. For instance, Chen et al. (2020) Table 3, Bordes et al. (2023) Figure 2, and Rashtchian et al. (2023) Table 2 present experimental outcomes for SimCLR on large real datasets, showing that: a) information (such as Color, Hue, Rotation, and Spot) suppressed by data augmentation is more retrievable in pre-projection representations; b) consequently, if the downstream task requires such information, using pre-projection is preferable.\n(2) **For the other two scenarios**\u2014where the downstream-relevant features are either too strong or too weak\u2014there's been less exploration in previous research. However, **we've dedicated significant effort to provide empirical evidence**, as discussed below. It's important to note the challenge in naturally varying feature strength, as controlled modifications to images are necessary to achieve this.\n- **We've already expanded beyond purely synthetic data to include results from MNIST-on-CIFAR-10 in our original paper**. Figures 3b and 3c precisely validate our findings, including the impact of data augmentation and the non-monotonic influence of feature strength, thus taking a step closer to more realistic data representations.\n- **If the MNIST-on-CIFAR-10 dataset still seems too 'synthetic,' we conduct two new experiments on minimally modified CIFAR-10 images, which further strongly support our findings**. The experiment details can be found in Appendix D.1. In summary, we established the downstream task as categorizing CIFAR-10 images as more \u2018red,\u2019 \u2018green,\u2019 or \u2018blue.\u2019 To control the color distinguishability (the clarity in identifying a color in images), we utilized two approaches: 1) manipulating the images, or 2) selecting subsets with varying color distinguishability. This effectively controls the downstream-relevant feature\u2019s strength. Our observations in Fig 7(b)(c) revealed that, in both scenarios, the advantage of pre-projection is more pronounced when the color feature is either too weak or too strong.\n\n\u201cI note that the abstract on the OpenReview website is different from that on the pdf file, which should be corrected.\u201d\n\n> ## Fixing abstract\n\nWe apologize for the inconsistency, while we cannot fix it at this point, we will make sure it is reflected correctly in the final version, if accepted."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8091/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700740163794,
                "cdate": 1700740163794,
                "tmdate": 1700741045976,
                "mdate": 1700741045976,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]