[
    {
        "title": "Effective pruning of web-scale datasets based on complexity of concept clusters"
    },
    {
        "review": {
            "id": "gRDT3dVUvh",
            "forum": "CtOA9aN8fr",
            "replyto": "CtOA9aN8fr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5051/Reviewer_orZi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5051/Reviewer_orZi"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on improving the training and data efficiency of CLIP-style models by pushing the limits of pruning large-scale multimodal datasets. By adapting the pruning rate to the complexity of different concepts within the dataset, the authors are able to reduce training costs to a quarter of regular training. They outperform the LAION-trained OpenCLIP-ViT-B/32 model on ImageNet zero-shot accuracy by 1.1 percentage points while using only 27.7% of the data and training compute. Additionally, they achieve a new state-of-the-art ImageNet zero-shot accuracy and competitive performance on 38 evaluation tasks in the DataComp Medium benchmark. The findings demonstrate the potential of pruning methods for improving the efficiency of training multimodal models."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The method proposed in this paper exhibits a certain degree of novelty.\n2. The paper extensively validates its findings using a large-scale experimental dataset, although experiments with even larger models were not performed.\n3. The practical applicability of the approach presented in this paper suggests its value in real-world scenarios."
                },
                "weaknesses": {
                    "value": "1. We cannot determine if the current data is effective for training ViT-B-16 and ViT-L-14 models.\n2. The majority of the experimental evaluation metric is Imagenet zero-shot top1. Is there any bias in the algorithm towards Imagenet data?\n3. Detailed results for each dataset in VTAB are not provided."
                },
                "questions": {
                    "value": "In this paper\uff0c the author scales SSP-Pruning to web-scale datasets and demonstrates that the pruning criterion can also transfer to the DataComp benchmark.\nQ1: In Fig. 1, the author shows they reduced the LAION-CAT440M to 166M and improved the zero-shot performance on Imagenet, can you provide more results? such as zero-shot transfer and linear probe performance on different datasets.\nQ2: Does the robustness of model training change when the dataset size is reduced? Please provide robustness evaluation results.\nQ3: The difference between DBP and SSP-Pruning is the choice of how many examples are taken from each cluster. It seems that innovation has some shortcomings\nQ4: In Fig.3, 222M performs worse zero shot accuracy than 166M, please give a reasonable explanation.\nQ5: Why weren't models of the same size, such as CLIP ViT-L-14, OPENCLIP LAION400M ViT-L-14, EVA ViT-L-14, and DINOv2 ViT-L-14, used for clustering comparisons?\nQ6:"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5051/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698683490075,
            "cdate": 1698683490075,
            "tmdate": 1699636494983,
            "mdate": 1699636494983,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "EcaRNfY2DW",
                "forum": "CtOA9aN8fr",
                "replyto": "gRDT3dVUvh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5051/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5051/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer orZi (part 1)"
                    },
                    "comment": {
                        "value": "We thank reviewer orZi for their valuable feedback and respond to the individual concerns below.\n\n### We cannot determine if the current data is effective for training ViT-B-16 and ViT-L-14 models.\nWe have evaluated this question in our paper and would like to point the reviewer to Table 2 where we show that DBP outperforms CLIP-score filtering for different model sizes.\nWe also include the Table below for convenience.\n| | Dataset | CLIP-S/32  | CLIP-B/32 | CLIP-L/14 |\n|--|-|--|---|---|\n| Method/Model | Size    | (63M params.) | (151M params.) | (428M params.) |\n|  |   | IN top1 acc   | IN top1 acc    | IN top1 acc    |\n| CLIP score   | 30M     | 32.32  | 38.07   | 47.61  |\n| DB-Pruning   | 30M     | **39.04**   |**43.41** | **53.21**  |\n\n\n### The majority of the experimental evaluation metric is Imagenet zero-shot top1. Is there any bias in the algorithm towards Imagenet data? Detailed results for each dataset in VTAB are not provided.\nWe thank the reviewer for their comment and refer them to the newly added Tables 9 and 10 in the Appendix; we also included Table 10 in our general comment to all reviewers. In Table 9, we find that we outperform the best baseline released by DataComp on 35 out of 38 tasks. T-MARS have unfortunately neither released the numbers on all test sets nor the trained models which makes it impossible to compare to their results. In Table 10, we show results on the 38 tasks from DataComp for the models trained on LAION-CAT-440M, as well as on filtered versions of this dataset.\n\n\n### Does the robustness of model training change when the dataset size is reduced? Please provide robustness evaluation results.\nThank you for raising this important concern. We added the following paragraph to our Results section:\n\nThe performance on retrieval and ImageNet distribution shifts is relatively lower compared to ImageNet zero-shot accuracy (Fig. 3). This trend is consistent across different baselines for retrieval tasks and we hypothesize that retrieval and ImageNet dist. shift tasks need relatively longer training (in Fig. 3 we reduce the number of training iterations/cost seen to \u2264 55.4%). To study this behavior, we measure the performance gains obtained with longer training. We increase the number of iterations for training on the 166M dataset from 41.6% (Fig. 3) to 69% and measure the difference in performance on each of the validation tasks as illustrated in Fig 4(right). We observe that among the four tasks (ImageNet, ImageNet dist. shift, VTAB, retrieval), the ImageNet dist. shift and retrieval tasks benefit the most from longer training. Therefore, we conclude that the observed performance drops on ImageNet dist. shifts and retrieval tasks can be attributed to shorter training.\n\n### The difference between DBP and SSP-Pruning is the choice of how many examples are taken from each cluster. It seems that innovation has some shortcomings.\nWe would like to stress that scaling SSP-Pruning to LAION was highly untrivial in itself because 1) it required an initial step of deduplication as well as 2) the correct choice of the encoder to calculate the image / caption embeddings. We also believe that our idea to use cluster statistics to gauge the complexity of different concepts is novel and has not been proposed for dataset pruning.\n\n### In Fig.3, 222M performs worse zero shot accuracy than 166M, please give a reasonable explanation.\n\nLet\u2019s recall that data quality is different from data quantity. In this work, we reduce the dataset size while maintaining and/or improving the quality of the data by balancing the data clusters and removing easy examples. This increases the marginal information gain the model gets from every training batch. \nWe pruning a dataset we need to balance between the dataset size and the dataset quality. The 222M dataset (lower quality) and the 166M (higher quality) are good examples of this balance for the ImageNet classification task. We notice that same performance behavior when we look at the LAION-440M (lower quality) dataset and the filtered datasets (higher quality) in Fig 3. \n\nSome previous works like, [1] show theoretically and practically that balancing the data by \u201cremoving\u201d examples from the majority groups/classes can result in a better worst group/ class performance and a better model even though the dataset size gets reduced. We added a discussion on this point to our Results section.\n\n[1] Arjovsky et al. \u201cThrowing away data improves worst-class error in imbalanced classification\u201d, ICML 2023"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5051/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700172969176,
                "cdate": 1700172969176,
                "tmdate": 1700211801197,
                "mdate": 1700211801197,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Eh7f1NejHJ",
            "forum": "CtOA9aN8fr",
            "replyto": "CtOA9aN8fr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5051/Reviewer_mnGt"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5051/Reviewer_mnGt"
            ],
            "content": {
                "summary": {
                    "value": "This work scales SSP-Pruning to web-scale datasets, investigate how the complexity of different concepts within a dataset can be used for pruning, and report further improvements over regular SSP-Pruning. They evaluate it with CLIP pre-training and report the results on classification task: it shows data efficiency in terms of pre-training."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The paper is well structured and the motivation is clear. It is interesting to study the data efficiency of CLIP pre-training to reduce the computational resources."
                },
                "weaknesses": {
                    "value": "1. The biggest concern I have is about the technical novelty. The main idea is based on SSP-Pruning. Therefore, the technical contribution is very trivial. \n2. This work only reports the results on image classification. How about the zero-shot results of CLIP in retrieval tasks, such as COCO/Flickr30k image-to-text and text-to image retrieval? The zero-shot retrieval performance is also very important to measure the quality of pre-training and the effectiveness of the proposed method. The paper mentions about the retrieval task and shows some results in Figure 3 but I am not sure the implementation details: which dataset do they use? It would be helpful to report the results on standard benchmark (COCO/Flickr30k). \n3. The method involves CLIP-score filtering to curate a new dataset to train CLIP. I am unsure if such involvement is fair, as the filtering itself is similar to distilling some knowledge from a well-trained CLIP. Then, the new CLIP is somehow learning from the well-trained CLIP on large-scale datasets. Therefore, it may not be fair to claim the data efficiency for the proposed method."
                },
                "questions": {
                    "value": "See weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5051/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698739423840,
            "cdate": 1698739423840,
            "tmdate": 1699636494882,
            "mdate": 1699636494882,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "an3PcCqEEr",
                "forum": "CtOA9aN8fr",
                "replyto": "Eh7f1NejHJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5051/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5051/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer mnGt"
                    },
                    "comment": {
                        "value": "We thank reviewer mnGt for their valuable feedback and respond to the individual concerns below.\n\n### The main idea is based on SSP-Pruning. Therefore, the technical contribution is very trivial.\n\nWe would like to stress that scaling SSP-Pruning to LAION was highly untrivial in itself because 1) it required an initial step of deduplication as well as 2) the correct choice of the encoder to calculate the image / caption embeddings. We also believe that our idea to use cluster statistics to gauge the complexity of different concepts is novel and has not been proposed for dataset pruning.\n\n### This work only reports the results on image classification. How about the zero-shot results of CLIP in retrieval tasks, such as COCO/Flickr30k image-to-text and text-to image retrieval? \nWe thank the reviewer for their comment and refer them to the newly added Tables 9 and 10 in the Appendix; we also included Table 10 in our general comment to all reviewers. In Table 9, we find that we outperform the best baseline released by DataComp on 35 out of 38 tasks. T-MARS have unfortunately neither released the numbers on all test sets nor the trained models which makes it impossible to compare to their results. In Table 10, we show results on the 38 tasks from DataComp for the models trained on LAION-CAT-440M, as well as on filtered versions of this dataset.\n\n### The method involves CLIP-score filtering to curate a new dataset to train CLIP. I am unsure if such involvement is fair, as the filtering itself is similar to distilling some knowledge from a well-trained CLIP. [...]Therefore, it may not be fair to claim the data efficiency for the proposed method.\nWe thank the reviewer for raising this important concern. CLIP-score filtering has become a standard step in almost all data selection frameworks. For example, DataComp is becoming an established benchmark for testing data pruning methods at scale, and uses CLIP-score filtering as part of most of their baselines. Thus, we are comparing methods which all use CLIP-score filtering in some form. In our Table showing DataComp results and in our Figure 1 showing results on LAION, all baselines have also used CLIP-score filtering. In that sense, our approach is more efficient with respect to other methods which also use CLIP-score filtering."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5051/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700172899385,
                "cdate": 1700172899385,
                "tmdate": 1700211776439,
                "mdate": 1700211776439,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "M3yUVstU89",
            "forum": "CtOA9aN8fr",
            "replyto": "CtOA9aN8fr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5051/Reviewer_u5qD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5051/Reviewer_u5qD"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a dataset pruning method broadly applicable to web scale datasets such as LAION. The authors build upon important prior work at the ImageNet scale (SSP-Pruning, Sorscher et al., 2022), which attempts to rebalance a dataset by clustering, and then sampling datapoints inversely proportional to their cluster centroids. The authors improve upon this procedure by noting that clusters have varying degrees of importance, both as a result of the average distance of points from the centroid as well as the average distance of the centroid from other centroids, and determine the proportion of samples to be pruned proportional to the importance. The authors demonstrate the supremacy of this method, DPB, through an array of experiments that the pruned datasets jointly save on compute while increasing overall performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper's extensive analysis of their new method across multiple datasets and benchmarks makes a compelling case for the high performance of their method. The dimensions of ablations, and the high level takeaways from those ablations in the results section are clear -- DBP outperforms SSP Pruning, outperforms CLIP filtering, DBP outperforms SemDeDup alone, and Deduplicaiton is crucial to the success of the method. These takeaways make the contribution quite compelling."
                },
                "weaknesses": {
                    "value": "The core weaknesses concern clarity of the results, and the overall difficulty in connecting the many different - at times seemingly disparate - results that are thrown together in the narrative.\n\nFor one, the paper begins by proposing an improvement to SSP, but defers any result about the paper to a small table in the appendix, for a single model and single dataset -- it would help to do a more comprehensive evaluation to make clear the improvement being proposed. The authors attempt to briefly make a broader connection that their work is akin to density based pruning -- this is where the relative lack of optimization over the number of clusters -- which very much controls how the density is approximated -- is puzzling. Indeed, the 5 point plot of Figure 6-d leaves much to want, where we are led to believe 500 is the best, because it is better than 100 (?) and 10000. More ablations here would help clarify things. In general, more time assessing Density based pruning methods (e.g. for a datapoint, if its k nearest neighbors approximate it well, it can be discarded) would significantly improve the narrative of the paper. \n\nOn the other hand, the quadratic program, though interesting and unexpected in a paper of this type, seems like overkill -- assigning the maximum number of points to each cluster whose expected number of samples is above the maximum, and redistributing the remaining samples proportionally among the remaining clusters (and repeating) seems like it would achieve what the authors are seeking to do (if not exactly what the QP does) without the overhead of a QP solver, and all the space in the paper it consumes. \n\nThe discussion section would benefit from a more clear discussion of where the shortcomings of the method are relative to the other methods presented in the paper. For example, DBP does a tiny bit better than TMARS on Imagenet, but more significantly worse on Imagenet Dist and Average but a discussion of the latter point is entirely missing. Whether pruning in a density based manner affects model robustness seems like a subject at least worth touching on. Same for the retrieval plot in Figure 4 -- why is it that SemDeDup does worse than LAION-440B, but DBP catches up? An analysis of what is happening here would help understand the paper considerably. \n\nBeyond that, the many different numbers in the paper -- as a result of different filtering models (CLIP-B16 or CLIP-L/14), differing number of epochs, differing initial dataset size (LAION-50M or 440M) -- make the results very difficult to connect and piece together across tables and figures. If some consistency across variables were afforded -- or at least enough ablations such that there is an extra column in each table connecting the numbers in other tables to that table, it would make the information much easier to process."
                },
                "questions": {
                    "value": "If the intuition is that it is density based pruning, why only 500 clusters? The 5 point plot of figure 6-d suggests that clusters between 100 and 10000 should be better explored since there are sharp peaks in that range. \n\nFigure 5: Why not use concatenate CLIP's text + image embedding?\n\nHow is it that SemDeDup does worse than LAION-440M for VTAB, but DBP does better?\n\nA notable decrease in performance on ImageNet dist shifts and Retreival tasks -- does the DBP type of pruning hurt generally hurt robustness? \n\nWhy use CLIP-B16 Score for Figure 3, but CLIP-L/14 Score for Table 1?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5051/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699523018027,
            "cdate": 1699523018027,
            "tmdate": 1699636494795,
            "mdate": 1699636494795,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9KCxAATaWm",
                "forum": "CtOA9aN8fr",
                "replyto": "M3yUVstU89",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5051/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5051/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer u5qD (part 1)"
                    },
                    "comment": {
                        "value": "We thank reviewer u5qD for their valuable feedback and respond to the individual concerns below.\n\n###  Is using the QP solver necessary?\nUsing the QP solver provides a minimal overhead\u2013it requires between 5 and 15 minutes on a single CPU depending on the dataset size. We agree that the relatively simple program takes an excessive amount of text in the paper; thus, we have moved the optimization problem and the algorithm to the Appendix.\n\n### Please provide a broader ablation on the number of clusters.\nWe thank the reviewer for this suggestion and ran an experiment for a more fine-grained analysis on the number of clusters. To save compute, we trained for one epoch on our 166M dataset (Fig. 3) for a wider range of values for the number of clusters hyperparameter (50, 100, 500, 1000, 5000, 10000, 20000, 60000, 10000). \n\n| Number of Clusters |50|100|500|1000|5000|10000|20000|60000|100000|\n|---|---|---|---|---|---|---|---|---|---|\n|top1 zero-shot accuracy |50.508| 51.442 | **52.338** | 52.214 | 51.764 | 50.71 | 50.278 | 49.374 | 48.01 |\n\n### Why do you not concatenate CLIP's text + image embedding?\nWe briefly tried this during the development stage of the project and found that it doesn\u2019t work better than using the multi-modal embedding. We think that the BLIP embedding shares information between the image and caption embeddings which is missing in a simple concatenation. \n\n### Why does SemDeDup do worse than LAION-440M on VTAB, but DBP does better? Analogously, why does DBP (166M) do better than SemDeDup (280M) or LAION-440M?\n\nThank you for raising this concern. We added the following paragraph to our Results section: A smaller, more balanced dataset can lead to better models (Fig. 3). In this work, we reduce the dataset size while maintaining and/or improving the quality of the data by balancing the data clusters and removing easy examples. This increases the marginal information gain the model gets from every training batch. As a result, we observe better performance on a variety of distribution shift tasks with shorter training: The model trained on the SemDeDup+DBP-222M dataset matches or outperforms training on the full LAION-CAT-440M dataset in all categories in Fig. 3, despite using only half the compute. The work of Arjovsky et al. (2023) also shows theoretically and practically that balancing the data by \u201cremoving\u201d examples from the majority groups/classes can result in a better worst group/class performance and a better model even though the dataset size is reduced.\n\nArjovsky et al. \u201cThrowing away data improves worst-class error in imbalanced classification\u201d, ICML 2023\n\n### What are DBP\u2019s shortcomings compared to T-MARS?\nUnfortunately, the T-MARS authors have not released the numbers on the individual distribution shifts, making it impossible to perform a fine-grained comparative analysis of both methods. \n\n### Why do we see performance drops on ImageNet dist. shifts and retrieval tasks?\nPerformance on retrieval tasks is relatively lower than on the other tasks. As this trend is consistent across different baselines, we hypothesize that retrieval tasks need relatively longer training (In Fig. 3 we reduced the number of training iterations/cost seen to <=55.4\\%). \n\nTo study this behavior we conducted the following experiment: We compare the relative gains from longer training on the different tasks in Fig.3. In more detail, we compare the difference in performance gain when training for 41.6% or for 69% of the number of iterations compared to training on the full dataset. We show the difference in performance on each of the validation tasks in the table below. We observe that among the four tasks (ImageNet, ImageNet dist. shift, VTAB, retrieval), the ImageNet dist. shift and retrieval tasks benefit the most from longer training. Therefore, we conclude that the observed performance drops on ImageNet dist. shifts and retrieval tasks can be attributed to shorter training.\n\n| Dist. shift | relative performance gain |\n|-|-|\n|ImageNet dist. shifts | 0.9%|\n|Retrieval| 0.8%|\n|VTAB| 0.7%|\n|ImageNet| 0.4%|\n\nWe added this discussion and the Table (as a bar plot) to the Results section of our revised paper."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5051/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700172837223,
                "cdate": 1700172837223,
                "tmdate": 1700172837223,
                "mdate": 1700172837223,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9o7drLmyzI",
                "forum": "CtOA9aN8fr",
                "replyto": "M3yUVstU89",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5051/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5051/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer u5qD (part 2)"
                    },
                    "comment": {
                        "value": "### Why do we use different architectures for filtering on LAION (CLIP-B/16) vs DataComp (CLIP-L/14)?\n\nThe reason we use two CLIP models for computing the CLIP score (B/16 for LAION and L/14 for Datacomp) is to comply with the baselines used in the literature by using the meta-data clip score for each dataset. For example, SemDeDup uses CLIP-B/16 to deduplicate LAION; also, CLIP-B/16 was used to filter LAION-440M. On the other hand, DataComp chose to use CLIP-L/14 for filtering which forced us to also use this model to remain comparable to their reported baselines.\n\nIt is also important to note that for a given dataset (LAION or Datacomp) we use the same CLIP filtering model for all the datasets we created and that keeps the comparison between models fair and consistent for each dataset.\n\n\n### The results section is unclear due to inconsistencies in the choice of the encoder / training epochs etc.\n\nWe thank the reviewer for this important feedback. We restructured the Results section and separated the analysis experiments on the LAION dataset (main text) and the analysis experiments on how to prune the DataComp dataset (now in Appendix F) into two sections.\n\nWe thank the reviewer for providing this valuable feedback. Consequently, we have restructured the results section, segregating the analysis experiments conducted on the LAION dataset (included in the main text) from the analysis experiments on how to prune the DataComp dataset, which are now found in Appendix F.\n\nThe new structure is as follows:\n- Fig.1, Fig. 3 and Table 3: FIltering the LAION-440M\n- Table 1: Filtering the DataComp dataset\n- All the remaining figures: Method analysis and hyperparameters tuning using a small dataset (LAION50M). \n\nWe are happy to discuss and accommodate further requested changes with reviewer u5qD."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5051/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700172862221,
                "cdate": 1700172862221,
                "tmdate": 1700173610686,
                "mdate": 1700173610686,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "aH8qbuGhvN",
            "forum": "CtOA9aN8fr",
            "replyto": "CtOA9aN8fr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5051/Reviewer_7m8Z"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5051/Reviewer_7m8Z"
            ],
            "content": {
                "summary": {
                    "value": "This paper seeks to prune large-scale multimodal datasets (e.g., LAION) for training CLIP-style models for training and data efficiency. Building upon SSP-pruning, Density-Based-Pruning picks the number of samples per cluster based on the overall complexity of a particular cluster, and achieves state-of-the-art results on LAION."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.\tThis paper proposes Density-Based-Pruning to improve data efficiency on large-scale multimodal datasets.\n2.\tThis paper conducts detailed hyperparameter selection experiments, which made the experimental results more convincing."
                },
                "weaknesses": {
                    "value": "1.\tThe related work section contains excessive content, and there is a considerable amount of duplication. Besides, it is better to introduce \u201ccoreset selection[1-3]\u201d, as it is closely related to your work.\n\n2.\tMissing some baselines for comparison, including random selection and other data pruning methods [3-4].\n\n3.\tThe description in the methods section is very confusing.\n[1] Moderate coreset: A universal method of data selection for real-world data-efficient deep learning, ICLR 2023\n[2] Large-scale Dataset Pruning with Dynamic Uncertainty, arxiv23\n[3] RETRIEVE: Coreset Selection for Efficient and Robust Semi-Supervised Learning, NeurIPS 2021\n[4] Too Large; Data Reduction for Vision-Language Pre-Training, ICCV\n2023"
                },
                "questions": {
                    "value": "1.\tIn Fig. 4(right), more sample points are needed for convincing conclusions.\n2.\tIn Table 2, it would be better to include SSP-Pruning as a baseline for comparison.\n\n3.\tThe tasks are relatively simple (ImageNet zero-hot). Could we compare DBP with other baselines on different tasks?\n4.\tIn the methods section, the filtering pipeline includes deduplication, CLIP score filtering, and Density-Based Self-Supervised Prototypes pruning. However, there is no description about CLIP score filtering. The description about deduplication does not provide me with any useful information."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5051/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5051/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5051/Reviewer_7m8Z"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5051/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699675691859,
            "cdate": 1699675691859,
            "tmdate": 1699675691859,
            "mdate": 1699675691859,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "mIBCd5qeDp",
                "forum": "CtOA9aN8fr",
                "replyto": "aH8qbuGhvN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5051/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5051/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 7m8Z"
                    },
                    "comment": {
                        "value": "We thank reviewer 7m8Z for their valuable feedback and respond to the individual concerns below.\n\n### Missing baselines for comparison, including random selection and other data pruning methods [3-4].\nWe thank the reviewer for their suggestions.\n* Random selection: We excluded the random pruning baseline to save compute because it has been proven to be very weak. For example, [1] shows that random pruning consistently lags behind the SemDeDup baseline across various dataset sizes and evaluation tasks. Random selection has been tested by the DataComp creators on the \u201csmall\u201d scale where it underperformed all other baselines (Table 12, https://arxiv.org/pdf/2304.14108.pdf). [1] Abbas et.al. 2023 https://arxiv.org/abs/2303.09540\n* Comparison to [3]: \n    * **The task considered in [3] is not compatible with ours.** In [3], the authors tackle the problem of semi-supervised learning and select the coreset of the unlabeled data resulting in minimum labeled set loss when trained upon in a semi-supervised manner. In contrast, we consider the fully unsupervised setting where we train a CLIP model on a pre-chosen coreset. \n    * Further, **The dataset and model scales of [3] differ strongly from ours.** In [3], the authors perform experiments on small-scale datasets such as CIFAR10 and SVHN using WideResNets and LeNet architectures, while we perform experiments on web-scale datasets using transformer architectures. Sorscher et al. showed that methods which perform well on small-scale datasets do not scale to ImageNet, and we found scaling SSP-Prototypes up to LAION / DataComp highly untrivial. We hypothesize that scaling [3] to DataComp to enable a fair comparison will be highly involved, as well as it is not clear that the small-scale datasets are sufficiently large to train transformer models on.\n* Missing comparison to [4]: While the setting of [4] is comparable to ours since the authors of [4] also study CLIP training on large-scale datasets, **their choice of datasets is different from ours**. Our base datasets are either LAION-CAT-440M (which is a filtered version of LAION-2B) or DataComp Medium, while [4] conduct CLIP experiments on CC3M. To enable a fair comparison, we would either need to scale [4] to DataComp or implement our method on CC3M; neither of these options is trivial and cannot be done during the short rebuttal period. If the reviewer would like to see results of DBP on CC3M, we would be happy to run these experiments until the camera ready version.\n\n### The description in the methods section is confusing.\nWe rewrote and restructured the Methods section to improve clarity. In particular, we rewrote the paragraphs on deduplication and CLIP-score filtering. Following the suggestion of reviewer u5qD, we moved details on the QP program to the Appendix.\n\n### The related work section contains excessive content, duplication and there are missing works which should be discussed. \nWe thank the reviewer for their suggestions and apologise for missing these important papers. We added them to our related work section which now has a new paragraph titled \u201cData curation in supervised learning\u201d where we included several other works in addition to the reviewer\u2019s suggestions. We also removed redundancy between Introduction and Related Work.\n\n### The tasks are relatively simple (ImageNet zero-shot). Could we compare DBP with other baselines on different tasks?\nWe thank the reviewer for their comment and refer them to the newly added Tables 9 and 10 in the Appendix; we included Table 10 in our general comment to all reviewers. In Table 9, we find that we outperform the best baseline released by DataComp on 35 out of 38 tasks. T-MARS have unfortunately neither released the numbers on all test sets nor the trained models which makes it impossible to compare to their results. In Table 10, we show results on the 38 tasks from DataComp for the models trained on LAION-CAT-440M, as well as on filtered versions of this dataset."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5051/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700172754391,
                "cdate": 1700172754391,
                "tmdate": 1700211742455,
                "mdate": 1700211742455,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "97gzrdNj6G",
                "forum": "CtOA9aN8fr",
                "replyto": "mIBCd5qeDp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5051/Reviewer_7m8Z"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5051/Reviewer_7m8Z"
                ],
                "content": {
                    "title": {
                        "value": "thanks for author rebuttal"
                    },
                    "comment": {
                        "value": "thanks for the rebuttal. i have a question:\nhave the authors tried to use DBSCAN and then select a subset as a baseline for comparison?"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5051/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700457936907,
                "cdate": 1700457936907,
                "tmdate": 1700457936907,
                "mdate": 1700457936907,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "R45PQf0hwa",
                "forum": "CtOA9aN8fr",
                "replyto": "aH8qbuGhvN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5051/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5051/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response regarding DBSCAN"
                    },
                    "comment": {
                        "value": "Thank you for this interesting suggestion. There would be two ways to implement DBSCAN as a baseline: (1) as a replacement of DBP which is based on k-means clustering or (2) as an additional step before performing kmeans clustering to potentially remove outliers from the data. In the following, we will comment on both.\n\n### Considering option (1): \nWe could use DBSCAN instead of DBP which is based on k-means clustering. We could sample data points based on the density which we could estimate as the mean distance between cluster members. We see the following issues with this approach:\n- **Clusters obtained with DBSCAN do not offer a simple way to rank cluster examples.**  Our method (as well as other coreset selection methods, see our revised Related Work section) rely on some ranking metric to select examples to train the model on and most methods propose to take the most difficult examples. K-means has the advantage of spherically-shaped and prototype-based clusters which allows us to rank the examples by measuring their distance from the cluster centroid. DBSCAN does not provide us with cluster prototypes which we could use to rank examples. While we could define cluster centroids as the mean or median of all cluster members, we are doubtful how accurate or useful these estimates would be, given that the clusters could have any (potentially non-convex) shape. For example, the means of two concentric circular clusters would coincide and lie outside of the actual clusters. Nevertheless, we could test DBSCAN pruning by taking a random subset of each cluster; however, in our early experiments, we observed that randomly selecting data points using our density-based criterion strongly underperformed compared to taking the most difficult examples.\n- **K-means runs on GPU using $faiss$ and is a lot faster than DBSCAN**: The time complexity of k-means clustering is $O(nki)$ (note that $k$ (num. clusters) and $i$ (num. iterations) << $n$) while DBSCAN can have a worst-case time complexity of $O(n^2)$. For example, based on our experiments, $k * i$ can be around 100,000 while $n$ can be >100,000,000 (1000 orders of magnitude higher).  We are uncertain how feasible DBSCAN would be for our large-scale datasets containing millions of training images. The $faiss$ implementation of k-means takes around 1-4 hours depending on the dataset size; a 1000-fold increase in clustering time would not be feasible for our dataset scales.\n\n### Considering option (2): \nWe could use DBSCAN to filter out outliers from the data before clustering. This might stabilize our kmeans algorithm, but would also introduce two new hyperparameters (the maximum distance between two points and the minimum number of samples in a cluster). An additional issue is the massively increased time complexity of the whole method (see the point above)."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5051/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700502000821,
                "cdate": 1700502000821,
                "tmdate": 1700502197120,
                "mdate": 1700502197120,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]