[
    {
        "title": "Weight-Entanglement Meets Gradient-Based Neural Architecture Search"
    },
    {
        "review": {
            "id": "MkYZA13Q5g",
            "forum": "B0OwtVEejJ",
            "replyto": "B0OwtVEejJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission89/Reviewer_JeZf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission89/Reviewer_JeZf"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes the application of single-stage NAS methods to the weight-entanglement search space. The authors observe that weight-entanglement spaces are typically explored using two-stage methods, while cell-based spaces are usually explored using single-stage methods. The authors bridge the gap between them and conduct extensive experiments."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper demonstrates the feasibility of the proposed method through experiments on different datasets and search spaces.\n- The experimental results reveal interesting phenomena and observations."
                },
                "weaknesses": {
                    "value": "- The motivation behind this work is not clear, and it appears to be a simple combination of existing methods, lacking innovation. What is the necessity and advantage of using single-stage search?\n- The weight entanglement has a higher weight-sharing extent. Does the intensification of weight entanglement during the single-stage search process affect search performance?\n- The description of the method is too simplistic, resulting in a lack of overall contribution.\n- In the experimental section, the comparison with related works is not comprehensive enough, as it does not include some comparisons with methods based on weight entanglement and single-stage NAS."
                },
                "questions": {
                    "value": "- What is the specific method referred to in Figure 2b? The description is unclear.\n- What do LHS and RHS represent in Figure 2?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission89/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698743234027,
            "cdate": 1698743234027,
            "tmdate": 1699635933797,
            "mdate": 1699635933797,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "MjtG9uyf6y",
                "forum": "B0OwtVEejJ",
                "replyto": "MkYZA13Q5g",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission89/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission89/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the detailed comments and suggestions. We respond to each of your questions below:\n\n> The motivation behind this work is not clear, and it appears to be a simple combination of existing methods, lacking innovation. What is the necessity and advantage of using single-stage search?\n\nWe thank the reviewer for their question. To the best of our knowledge, our work is the first to efficiently and effectively apply single-stage methods to weight-entanglement/foundation-model macro spaces. This makes our approach novel and very useful in practice.\n\nOur work is primarily motivated by two goals:   \n\nFirstly, we aim to make single-stage NAS methods amenable to foundation model macro spaces (language models, vision transformers, mobilenets). Note that single stage methods, even recent ones like [Lambda-DARTS](https://openreview.net/forum?id=oztkQizr3kk), have only been evaluated on cell-based spaces which are not very realistic in deep learning deployment scenarios. In our opinion, it is crucial to make single-stage methods more accessible to foundation models to ensure that advancement in these methods do not remain restricted to cell-based spaces. Further, our plug-and-play framework and our publicly available code makes integration evaluation of any single-stage optimizer on these spaces easy.\n\nSecondly, two-stage methods rely on performance proxies from the supernet which might not be highly correlated with the ground truth accuracies in practice. These supernets also face interference [1] during training which exacerbate these issues. We also conduct an analysis on the CKA[2] correlation between feature representations of the best architecture in our single-stage supernet and our two-stage supernet and find that our supernet in general shows better correlations.\n\n| Model        | Inherit v/s Retrain (CIFAR10) | Inherit v/s Retrain (CIFAR100) | Fine-Tune v/s Retrain (CIFAR10) | Fine-Tune v/s Retrain (CIFAR100) |\n|--------------|--------------------------------|---------------------------------|----------------------------------|-----------------------------------|\n| TangleNAS    | **0.4630**                     | **0.5853**                      | 0.57125                          | **0.65275**                       |\n| SPOS+ES      | 0.45812                        | 0.57932                         | 0.569374                         | 0.6309                            |\n| SPOS+RS      | 0.44124                        | 0.583                           | **0.5797**                       | 0.638948                          |\n\n\n\n\n[1]  Shipard, J., Wiliem, A. and Fookes, C., 2022. Does Interference Exist When Training a Once-For-All Network?. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 3619-3628).\n\n[2] Kornblith, S., Norouzi, M., Lee, H. and Hinton, G., 2019, May. Similarity of neural network representations revisited. In International conference on machine learning (pp. 3519-3529). PMLR."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission89/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700383322259,
                "cdate": 1700383322259,
                "tmdate": 1700383340800,
                "mdate": 1700383340800,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Big6vitQWe",
            "forum": "B0OwtVEejJ",
            "replyto": "B0OwtVEejJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission89/Reviewer_2nyZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission89/Reviewer_2nyZ"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a gradient-based neural architecture search approach (TangleNAS) with a weight-entangled search space.  The main idea is to combine the memory efficiency of weight sharing (entanglement) with the search time efficiency of a differentiable (gradient-based) search space.  DrNAS, a gradient-based approach, is taken as a reference and extended to weight sharing by modifying the edge operations. All edge operations are summed after being individually weighted. The approach is evaluated on several standard benchmarks, where it often shows an improvement when combining both types of search spaces."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ The paper is well written. In particular, the related work is complete and the proposed approach is clearly positioned in relation to existing approaches. In addition, the method is mostly well presented and the main idea is easy to follow. \n\n+ The proposed idea of edge operations works well in practice. Moreover, the idea could be applied to various gradient-based NAS methods. \n\n+ The experimental section considers several standard benchmarks for evaluation, both cell-based and macro search spaces. It shows an improvement in results compared to DrNAS for most cases."
                },
                "weaknesses": {
                    "value": "- Contribution: As stated in the paper, the proposed approach is applicable to different gradient-based NAS methods. It would add value to the paper to demonstrate this. At the moment it looks like an approach to extend DrNAS, but at least DARTS and/or GDAS (or more recent TE-NAS) should have been considered.\n\n-  On the macro search space, it would make sense to consider a macro approach and then introduce the cell-based part of the proposed method. It's not clear what the reference is for measuring the improvement. Nevertheless, the comparison with existing methods is useful. \n\n- (Major limitation) This is a benchmark driven approach. It would therefore be useful to include the latest results on NAS, e.g. from Lukasik, Jovita, Steffen Jung and Margret Keuper. \"Learning where to look - generative NAS is surprisingly efficient.\" European Computer Vision Conference. Cham: Springer Nature Switzerland, 2022. Then it would also be helpful to discuss why and when the paper lacks performance compared to the latest state-of-the-art approaches. For example, on the DARTS search space (Table 4), the current SOTA is much lower than the paper's results (the cited paper or TE-NAS, for example, perform better). It is therefore important to compare with the latest approaches and possibly improve on their setup.\n\n- Clarity: The method re-defines the parts of the search space. For example, the superset search space and edge definition are missing. In general, the method would benefit from a section defining this problem."
                },
                "questions": {
                    "value": "- It would be useful to know why the latest SOTA approaches have not been used as a reference for improvement using the proposed approach."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission89/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698746888358,
            "cdate": 1698746888358,
            "tmdate": 1699635933721,
            "mdate": 1699635933721,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GFwKH521QF",
                "forum": "B0OwtVEejJ",
                "replyto": "Big6vitQWe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission89/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission89/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the detailed comments and suggestions. We respond to each of your questions below:\n\n> It would add value to the paper to demonstrate DARTS and/or GDAS\u2026\n\nIn the single-stage neural architecture search literature [1]  it is well-known that several of the single-stage optimizers like DARTS, GDAS etc.have many failure modes. We started our study on weight entanglement spaces by studying DrNAS, DARTS and GDAS on the two tiny search spaces which we create benchmarks for. We find from initial analysis that DrNAS performs competitively and is robust on both of these spaces in comparison to DARTS and GDAS. Hence we restrict ourselves to DrNAS in our large scale experiments. We have also evaluated DARTS and GDAS on the AutoFormer-T space on CIFAR-100 and presented the results below. \n\nToy conv macro (CIFAR10)\n|  Optimizer | Test-acc          |\n|------------|-------------------|\n| DrNAS      | **83.02**         |\n| SPOS+RS    | 81.2525           |\n| SPOS+RE    | 81.89             |\n| DARTS_v1   | 81.61             |\n| DARTS_v2   | 81.49             |\n| GDAS       |  10% (degenerate) |\n\nToy cell entangled (Fashion MNIST)\n\n|  Optimizer | Test-acc  |\n|------------|-----------|\n| DrNAS      |  **90.93** |\n| DARTS-V1   | 89.905    |\n| DARTS-V2   | 90.7475   |\n| GDAS       | 90.618    |\n| SPOS+RS   | 90.6875   |\n| SPOS+RE    | 90.595    |\n\nAutoFormer-T (CIFAR100)\n|  Optimizer | CIFAR100 Test-acc  |\n|------------|---------------|\n|TangleNAS-DARTS    |    82.107 \u00b1 0.392        |  \n| TangleNAS-GDAS    |     82.12 \u00b1 0.2813        | \n| TangleNAS-DrNAS   |   **82.668 \u00b1 0.161**        | \n| SPOS+ES   |       82.5175 \u00b1 0.114      | \n| SPOS+RS  |    82.210 \u00b1 0.14242          | \n\nSince our focus is mainly 2-stage gradient based NAS without the use of zero-cost proxies we restrict ourselves to approaches which operate in this realm (unlike TE-NAS, which uses proxies for pruning). \n\n> On the macro search space, it would make sense to consider a macro approach and then introduce the cell-based part of the proposed method. It's not clear what the reference is for measuring the improvement. Nevertheless, the comparison with existing methods is useful.\n\nWe would like to clarify that none of the macro-spaces in our experimental setup (i.e. AutoFormer, MobileNet, LLM space) have any cell-based components. We operate on the same search spaces as AutoFormer or OFA only modifying the NAS optimizer itself with our proposed optimizer. The baselines we use are the two-stage counterparts (usually methods which are based on SPOS with some modifications). For completeness we compare against the AutoFormer\u2019s original code, the Once-For-All\u2019s original code as baselines when comparing their two-stage method with ours. For search spaces which do not have two-stage methods available we implement the Single-Path-One-Shot (SPOS) two-stage method in these spaces. \n\n> This is a benchmark driven approach \u2026\n\nWe compare against DrNAS which improves just with the introduction of weight-entanglement. We don\u2019t compare with other baselines simply because the goal of our work is to show the effective applicability of single-stage methods on weight-entanglement spaces (transformers, mobilenet)  instead of achieving SOTA on cell-based spaces (DARTS, NB201). We hope that our flexible plug-and-play framework makes evaluation on new single-stage methods on more realistic and practical spaces like transformers, mobilenet easier. \n\n> Clarity\n\nWe thank the reviewer for this suggestion. We will provide more details about the method in our updated manuscript.\n\nIf the reviewer has any further questions we would be happy to address them."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission89/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700383180369,
                "cdate": 1700383180369,
                "tmdate": 1700390885516,
                "mdate": 1700390885516,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "oFiQUuiHLz",
                "forum": "B0OwtVEejJ",
                "replyto": "Big6vitQWe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission89/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission89/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> Evaluation on other optimizers..\n\nWe have now evaluated DARTS and GDAS on the AutoFormer-T space for CIFAR10 too. Similar to the results on CIFAR100, here too TangleNAS-DrNAS performs best out of the three choices of optimizers. This motivates our choice of gradient-based NAS optimizer in our experiments.\n\nAutoFormer-T (CIFAR10)\n\n|  Optimizer | CIFAR10 Test-acc  |\n|------------|---------------|\n|TangleNAS-DARTS    |    97.672 \u00b1 0.04    |  \n| TangleNAS-GDAS    |    97.45 \u00b1 0.096       | \n| TangleNAS-DrNAS   |   **97.872 \u00b1 0.054**        | \n| SPOS+ES   |       97.6425 \u00b1 0.023      | \n| SPOS+RS  |    97.767 \u00b1 0.024         |"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission89/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700570593786,
                "cdate": 1700570593786,
                "tmdate": 1700570618448,
                "mdate": 1700570618448,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "9LOJS3p7eL",
            "forum": "B0OwtVEejJ",
            "replyto": "B0OwtVEejJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission89/Reviewer_b7V3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission89/Reviewer_b7V3"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors introduced architectural parameters to supernet search space, where all operation choices are superposed ontothe largest one with weights. This simple process enables searching for optimal sub-network via gradient-based optimization. The authors applied the proposed method to MobileNetV3 and ViT search space and got promising performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The proposed method is simple yet effective. It inherits the good merits of DARTS-like methods (end-to-end learning with the help of architectural parameters) and supernet methods (memory efficient and supporting fine-grained search spaces)."
                },
                "weaknesses": {
                    "value": "Some experimental details are not clear:\n\n- It is unclear from the paper what are the FLOPs and parameter sizes of the searched results in the experiments. Are the comparison are done between networks?\n\n- Any reason why the performance of AutoFormer variants from that paper is not listed in Table 6? Again, there should be the FLOPs and parameter size of each model.\n\n- On Table 7, there should be comparisons with some other works which use MobileNetV3 search space (e.g., AtomNAS), together with their FLOPs and parameter sizes. Why are the results of OFA with progressive shrink not used in the table?"
                },
                "questions": {
                    "value": "My main concern is the lack of some information and comparisons in the experiments, as mentioned above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission89/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698826540027,
            "cdate": 1698826540027,
            "tmdate": 1699635933655,
            "mdate": 1699635933655,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ZCnRu1dKjv",
                "forum": "B0OwtVEejJ",
                "replyto": "9LOJS3p7eL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission89/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission89/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the detailed comments and suggestions. We respond to each of your questions below:\n\n> It is unclear from the paper what are the FLOPs and parameter sizes of the searched results in the experiments. Are the comparisons done between networks?\n\nWe have now updated the manuscript to reflect these numbers (Table 5,6,7,8). In our experimental setup, we perform unconstrained search using both two-stage and single-stage methods (i.e. TangleNAS) and then compare their evaluations. The comparisons are done between individual networks in the supernet. \n\nFurthermore now we also conduct an experiment to search for models with smaller parameter sizes. We add a differentiable parameter penalty to the loss function to enforce this. We then perform the evolutionary search of SPOS setting the parameter size of the tanglenas model as constraint. We find out that the architecture SPOS discovers in this constrained version is still worse than the tanglenas architecture discovered. This shows the ability of our search method to discover better architectures even at smaller parameter budgets in its constrained version.\n| Optimizer | CIFAR10           | Params    | CIFAR100         | Params  |\n|-----------|-------------------|-----------|------------------|---------|\n| TangleNAS | 97.254925,0.11523 | 6.647626M | 81.25467\u00b10.27151 | 7.08394 |\n| SPOS+RE   | 97.16425,0.13604  | 6.56041M   | 80.8324\u00b10.23757  | 6.8866  |\n\n> Any reason why the performance of AutoFormer variants from that paper is not listed in Table 6? Again, there should be the FLOPs and parameter size of each model.\n\nWe primarily restrict our evaluation to AutoFormer-T due to compute restrictions. However we have now evaluated our method on the AutoFormer-S space and the results (with resolution 224 images) is in Table-6 of the updated manuscript. Our approach again outperforms the architecture derived by the corresponding two-stage method. We present the table-6 below:\n| NAS Method | SuperNet-Type | ImageNet | CIFAR10 | CIFAR100 | Flowers | Pets | Cars | Params | FLOPS |\n|------------|---------------|----------|---------|----------|---------|------|------|--------|-------|\n| SPOS+ES    | AutoFormer-T  | 75.474   | 98.019  | 86.369   | **98.066** | 91.558 | 91.935 | 5.893M | 1.396G|\n| TangleNAS  | AutoFormer-T  | **78.842** | 98.249 | **88.290** | **98.066** | **92.347** | **92.396** | 8.98108M | 2.00G|\n| SPOS+ES    | AutoFormer-S  | 81.700   | 99.10   | **90.459** | 97.90  | 94.8529 | **92.5447** | 22.9M  | 5.1G  |\n| TangleNAS  | AutoFormer-S  | **81.964** | **99.12** | **90.459** | **98.3257** | **95.07** | 92.3707 | 28.806M | 6.019G|\n\n\n\n> On Table 7, there should be comparisons with some other works which use MobileNetV3 search space (e.g., AtomNAS), together with their FLOPs and parameter sizes. Why are the results of OFA with progressive shrink not used in the table?\n\nWe do use the OFA-PS supernet and report the accuracy after evolutionary search using their code. The search space variant we use is the smaller MobileNetV3 space, i.e. ofa_mbv3_d234_e346_k357_w1.0 [here](https://github.com/mit-han-lab/once-for-all/) . To the best of our knowledge AtomNAS has a very different search space compared to OFA, hence we think that AtomNAS is not really comparable to OFA or our proposed approach. \n\nIf the reviewer has any further questions we would be happy to address them."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission89/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700383011587,
                "cdate": 1700383011587,
                "tmdate": 1700383011587,
                "mdate": 1700383011587,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vbd2k5nY76",
                "forum": "B0OwtVEejJ",
                "replyto": "ZCnRu1dKjv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission89/Reviewer_b7V3"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission89/Reviewer_b7V3"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for providing the additional results. I have no more questions except a small one: in the updated table 6, it would be better if you can provide TangleNAS with similar parameter sizes and flops in your final version."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission89/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700680958642,
                "cdate": 1700680958642,
                "tmdate": 1700680958642,
                "mdate": 1700680958642,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "tymMvHS4hg",
            "forum": "B0OwtVEejJ",
            "replyto": "B0OwtVEejJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission89/Reviewer_wjLK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission89/Reviewer_wjLK"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on the integration of weight entanglement and gradient-based methods in neural architecture search (NAS). The authors propose a scheme to adapt gradient-based methods for weight-entangled spaces, enabling an in-depth comparative assessment of the performance of gradient-based NAS in weight-entangled search spaces. The findings reveal that this integration brings forth the benefits of gradient-based methods while preserving the memory efficiency of weight-entangled spaces. Additionally, the paper discusses the insights derived from the single-stage approach in designing architectures for real-world tasks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper proposes a scheme to adapt gradient-based methods for weight-entangled spaces in neural architecture search (NAS). This integration of weight-entanglement and gradient-based NAS is a new approach that has not been explored before. The paper also presents a comprehensive evaluation of the properties of single and two-stage approaches, including any-time performance, memory consumption, robustness to training fraction, and the effect of fine-tuning."
                },
                "weaknesses": {
                    "value": "1. The novelty is limited. Some works have also suggested that the weights of large kernel convolution operations can be shared in differentiable neural architecture search. For example, MergeNAS: Merge Operations into One for Differentiable Architecture Search (in IJCAI20)\n2. The performance improvements are limited compared with the baselines."
                },
                "questions": {
                    "value": "1. How to entangle non-parameter operations, such as skip or pooling, as they have no weight compared to convolution.\n2. The search cost of original DrNAS is 0.4 GPU-Days in DARTS search space. Whereas, in Table 4, the search time is 29.4 GPU-Hours. Can the authors explain the reason for the different search cost?\n3. It would be beneficial to provide a more detailed explanation of the rationale behind the selection of specific search types, optimizers, and supernet types for the comparative evaluation.\u00a0\n4. To discuss potential limitations and challenges in implementing the proposed scheme for adapting gradient-based methods for weight-entangled spaces would provide valuable insights. This could include addressing potential constraints, trade-offs, and practical considerations in real-world implementation."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission89/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699370531584,
            "cdate": 1699370531584,
            "tmdate": 1699635933547,
            "mdate": 1699635933547,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "FjFVnSIlIk",
                "forum": "B0OwtVEejJ",
                "replyto": "tymMvHS4hg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission89/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission89/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer wjLK"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the detailed comments and suggestions. We respond to each of your questions below:\n\n> The novelty is limited\u2026MergeNAS\n\nWe thank the reviewer for pointing out the \"MergeNAS\" paper, which we were unaware of at the time of writing the paper. We agree that on cell-based spaces MergeNAS works similarly to TangleNAS and we will update the manuscript to cite MergeNAS. However, MergeNAS does not apply and evaluate on MobileNet or transformer spaces ie. the weight entanglement spaces. We on the contrary extend weight entanglement to linear layers, batchnorm and layernorm layers, and attention blocks and show its effectiveness. Our work mainly aims at achieving two objectives. Firstly, making single-stage NAS methods amenable to the foundation model macro spaces (language models, vision transformers, MobileNets). This, in our opinion, is crucial to make single-stage methods more practically useful. Furthermore, our plug-and-play framework and our publicly available code makes integration and evaluation of any single-stage optimizer on these spaces easy.\n\n> Performance improvements are limited compared with the baselines\n\nWe think that our improvements on ImageNet 1-k on AutoFormer-T spaces and MobileNet spaces are quite significant. Notably, on the same AutoFormer-T space, TangleNAS finds an architecture with **~ 3.8%** higher accuracy. We have now also evaluated TangleNAS on AutoFomer-S space where it achieves an improvement of  **~ 0.264%** compared to AutoFormer. Further, we have now added evaluations on the GPT-2 search space trained on OpenWebText. We transfer the pre-trained architectures (handcrafted and searched) to the Shakespeare dataset and observe that the architectures we discover outperform the larger handcrafted architecture while being efficient in terms of the number of parameters and latency.\n\n|  Architecture | Search-Type | Loss   | Perplexity | Params  | Inference Time |\n|---------------|-------------|--------|------------|---------|----------------|\n| GPT-2         | Manual      | 3.0772 | 21.69      | 123.59M | 113.3s        |\n| TangleNAS     | Automated   | **2.9038** | **18.243**     | 116.51M | 102.5s          |\n\n\n> How to entangle non-parameter operations, such as skip or pooling, as they have no weight compared to convolution?\n\nWe acknowledge that non-parameter operations cannot be entangled. However, we would like to point out that in this work, we focus primarily on entanglement spaces which do not have skip connections as a choice of operation. These spaces, which are designed around foundation model architectures, are in fact far more realistic and practically useful than cell-based spaces (which usually have skip operations). Our goal through weight entanglement is to save as much of GPU memory as possible when applying single-stage NAS methods.\n\n> The search cost of original DrNAS is 0.4 GPU-Days in DARTS search space. Whereas, in Table 4, the search time is 29.4 GPU-Hours. Can the authors explain the reason for the different search cost?\n\nIn our configurable framework we implement DrNAS with and without weight sharing for a unified comparison of search time and performance. The reported numbers are for the DrNAS optimizer on the DARTS search space using this unified codebase. The differences in search time are likely due to the differences in the GPU hardware setup (gpu types, cpu cores) we use in comparison to the authors."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission89/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700382708268,
                "cdate": 1700382708268,
                "tmdate": 1700383478556,
                "mdate": 1700383478556,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8Y999X7K8A",
                "forum": "B0OwtVEejJ",
                "replyto": "tymMvHS4hg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission89/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission89/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> It would be beneficial to provide a more detailed explanation of the rationale behind the selection of specific search types, optimizers, and supernet types for the comparative evaluation.\n\nIn the single-stage neural architecture search literature [1],  it is well-known that several of the single-stage optimizers such as DARTS and GDAS have several failure modes. We started our study on weight entanglement spaces by studying DrNAS, DARTS and GDAS on the two tiny search spaces which we create benchmarks for. We find in our initial analysis that DrNAS performs competitively and is robust on both of these spaces in comparison to DARTS and GDAS. Hence we restrict ourselves to DrNAS in our large scale experiments. We have also evaluated DARTS and GDAS on the AutoFormer-T space on CIFAR-100 and presented the results below. \n\nToy conv macro (CIFAR10)\n\n|  Optimizer | Test-acc          |\n|------------|-------------------|\n| DrNAS      | **83.02**         |\n| SPOS+RS    | 81.2525           |\n| SPOS+RE    | 81.89             |\n| DARTS_v1   | 81.61             |\n| DARTS_v2   | 81.49             |\n| GDAS       |  10% (degenerate) |\n\nToy cell entangled (Fashion MNIST)\n\n|  Optimizer  | Test-acc  |\n|------------|-----------|\n| DrNAS      |  **90.93** |\n| DARTS-V1   | 89.905    |\n| DARTS-V2   | 90.7475   |\n| GDAS       | 90.618    |\n| SPOS+RS    |  90.6875   |\n| SPOS+RE    | 90.595    |\n\n\nAutoFormer-T (CIFAR100)\n\n|  Optimizer | CIFAR100 Test-acc  |\n|------------|---------------|\n|TangleNAS-DARTS    |    82.107 \u00b1 0.392        |  \n| TangleNAS-GDAS    |     82.12 \u00b1 0.2813        | \n| TangleNAS-DrNAS   |   **82.668 \u00b1 0.161**        | \n| SPOS+ES   |       82.5175 \u00b1 0.114      | \n| SPOS+RS  |    82.210 \u00b1 0.14242          | \n\nWe choose our search spaces with three primary factors in mind (1) Architecture Diversity: ViTs, LLMs, mobilenets i.e. convolutional space (2) Application Diversity: Classification and Language Modelling) (3) Usefulness: A focus on spaces constructed around foundation model architectures \n\n[1] White, C., Safari, M., Sukthanker, R., Ru, B., Elsken, T., Zela, A., Dey, D. and Hutter, F., 2023. Neural architecture search: Insights from 1000 papers. arXiv preprint arXiv:2301.08727.\n\n> To discuss potential limitations ...\n\nWe thank the reviewer for this question. In our opinion the primary limitation of our work is the inability to handle multiple architecture constraints like parameter-sizes, FLOPS directly. It is indeed possible to add a constrained secondary penalty to our loss. However , this modification still doesn\u2019t allow one to generate a full Pareto Front of objectives which is useful in many deep learning applications (eg: fairness, robustness, hardware constraints). We leave this extension for future work.  Further, like 2-stage methods (OFA, AutoFormer), our approach also requires some amount of engineering effort to shard and combine weights into a mixture. \n\nFurthermore now we also conduct an experiment to search for models with smaller parameter sizes. We add a differentiable parameter penalty to the loss function to enforce this. We then perform the evolutionary search of SPOS setting the parameter size of the TangleNAS model as the constraint. We find that the architecture SPOS discovers in this constrained version is still worse than the TangleNAS architecture discovered. This shows the ability of our search method to discover better architectures even at smaller parameter budgets in its constrained version.\n| Optimizer | CIFAR10           | Params    | CIFAR100         | Params  |\n|-----------|-------------------|-----------|------------------|---------|\n| TangleNAS | **97.254925\u00b10.11523** | 6.647626M | **81.25467\u00b10.27151** | 7.08394M |\n| SPOS+RE   | 97.16425\u00b10.13604  | 6.56041M   | 80.8324\u00b10.23757  | 6.8866M  |\n\nIf the reviewer has any follow-up questions we would be happy to address them."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission89/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700382782933,
                "cdate": 1700382782933,
                "tmdate": 1700390341459,
                "mdate": 1700390341459,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Uisoi6ebHJ",
                "forum": "B0OwtVEejJ",
                "replyto": "tymMvHS4hg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission89/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission89/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Followup on pending questions"
                    },
                    "comment": {
                        "value": "> The search cost of the original DrNAS is 0.4 GPU-Days\u2026\n\nWe would like to thank the reviewer for raising the concern about the total time taken by our method. In response, we investigated the matter further, re-ran our experiments, and  uncovered two important findings. Firstly, we misreported the total search time on the DARTS search space for DrNAS. Secondly, we had bottlenecks in our code which made our implementations slower than its original counterpart (such as the suboptimal number of workers in the data loader, and unnecessary iterations over all the modules of the model in every step of the optimization loop). Having fixed these, we are happy to report that our DrNAS with weight entanglement is now faster than the DrNAS baseline published by the authors. For a fair comparison against the authors\u2019 baseline, we ran our code and theirs on the same hardware (RTX-2080 with 12 CPU cores), Python environment, and using the same experimental setup. Additionally, we ran experiments with and without the bottlenecks in our code fixed. The results are summarized in the table below, where WS indicates weight-sharing and WE indicates weight-entanglement. \n\n| Optimizer                   \t| Time Taken (GPU days) |\n|---------------------------------|------------------|\n| DrNAS - WS (with bottleneck)\t| 0.45        \t|\n| DrNAS - WS (without bottleneck) | 0.38         \t|\n| DrNAS - WE (with bottleneck)\t| 0.42         \t|\n| DrNAS - WE (without bottleneck) | 0.31         \t|\n| Original DrNAS codebase             \t| 0.36         \t|\n\n\nPlease note that DrNAS - WS (with bottleneck) and DrNAS - WE (with bottleneck) are what should have been reported in the paper. We thank the reviewer for giving us the opportunity to identify and fix this error.\n\n\n\n> Evaluation on other optimizers..\n\nWe have now evaluated DARTS and GDAS on the AutoFormer-T space for CIFAR10 too. Similar to the results on CIFAR100, here too TangleNAS-DrNAS performs best out of the three choices of optimizers. This motivates our choice of gradient-based NAS optimizer in our experiments.\n\nAutoFormer-T (CIFAR10)\n\n|  Optimizer | CIFAR10 Test-acc  |\n|------------|---------------|\n|TangleNAS-DARTS    |    97.672 \u00b1 0.04    |  \n| TangleNAS-GDAS    |    97.45 \u00b1 0.096       | \n| TangleNAS-DrNAS   |   **97.872 \u00b1 0.054**        | \n| SPOS+ES   |       97.6425 \u00b1 0.023      | \n| SPOS+RS  |    97.767 \u00b1 0.024         |"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission89/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700570557781,
                "cdate": 1700570557781,
                "tmdate": 1700570640510,
                "mdate": 1700570640510,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]