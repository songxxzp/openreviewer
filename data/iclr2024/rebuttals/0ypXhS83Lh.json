[
    {
        "title": "Robust Reinforcement Learning with Structured Adversarial Ensemble"
    },
    {
        "review": {
            "id": "EAmEFevqCF",
            "forum": "0ypXhS83Lh",
            "replyto": "0ypXhS83Lh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1135/Reviewer_APuc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1135/Reviewer_APuc"
            ],
            "content": {
                "summary": {
                    "value": "This paper solves the \"robust RL\" problem by noting two major difficulties - over-optimism and over-pessimism - in the previous state-of-the-art. The robust RL considered in this paper is adversarial training using a two-player max-min game. This paper provides justifications for the proposed approach and evaluate extensively on benchmarks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "I think this work really pushes the adversarial RL community research efforts further by answering:\n\n> can we design computationally efficient adversarial RL algorithms which are not pessimistic to unrealistic adversaries?\n\nThe main contribution of game-theoretic algorithm with solutions to two concerns (over-optimism and over-pessimism) raised in the adversarial problem setting is a really nice idea worthy for publication. My score reflect the weaknesses."
                },
                "weaknesses": {
                    "value": "I have only a few weakness for this work as follows:\n\n- In summary, this approach is finding appropriate number of adversaries (resolving over-optimism) to do a domain randomization (DR) step (averaging over worst-k adversaries) on top of these many adversaries. I see this paper mentions Mehta et al 2020 in related work but due credit to more DR works are missing. I am not sure of any DR-related works, but i'll appreciate if the authors can include more related works on these methodologies and discuss their differences with the current approach.\n\n- Related work need to be better than combining all different settings into one paragraph:\n> Subsequent works generalize the objective to unknown uncertainty sets, and formulate the uncertainty as perturbations/disturbance introduced into, or intrinsically inherited from, the environments, including perturbations in the constants which are used to define environmental dynamics (e.g., gravity, friction, mass) (Abraham et al., 2020; Mankowitz et al., 2019; Mehta et al., 2020; Pinto et al., 2017; Vinitsky et al., 2020a; Tessler et al., 2019; Vinitsky et al., 2020a;b), disturbance introduced to the observations (Zhang et al., 2020) and actions (Li et al., 2021; Tessler et al., 2019).\n\n-- The current framework considers robustness against adversarial actions. Tessler et al., (2019) and thereafter are the closest to current work's setting. Of course then their algorithms need to be benchmarked against. \n\n-- Transition model perturbation can be justified in the framework mentioning the evolution of the environment depends on the adversarial actions. Model uncertainty in robust RL is defined in more generality [2-10]. So it will be better to include more detailed Related Works including [2-10] and more relevant works in the revision. I agree this work includes experiments with model uncertainty, and the baseline is M2TD3 that fits into [2-10] line of works. There also exists works on state uncertainty [1]. \n\nTo summarize, major update on the related works is required. If published as is, due credit to appropriate works will be missed and misrepresented.\nSide note: I've also stopped at '10' since you get the idea of inadequate related work discussion. \n\nI am open to discussions with the authors and reviewers to make sure the work quality matches the score, which I believe so at this point, but a potential reach to 8 definitely exists. All the best for future decisions!\n\n[1] Robust Multi-Agent Reinforcement Learning with State Uncertainty Sihong He, Songyang Han, Sanbao Su, Shuo Han, Shaofeng Zou, and Fei Miao. Transactions on Machine Learning Research, June 2023.\n\n[2] Xu. Z, Panaganti. K, Kalathil. D, Improved Sample Complexity Bounds for Distributionally Robust Reinforcement Learning. Artificial Intelligence and Statistics, 2023.\n\n[3] Nilim, A. and El Ghaoui, L. (2005). Robust control of Markov decision processes with uncertain transition matrices. Operations Research, 53(5):780\u2013798\n\n[4] Iyengar, G. N. (2005). Robust dynamic programming. Mathematics of Operations Research, 30(2):257\u2013280.\n\n[5] Panaganti, K. and Kalathil, D. (2021). Robust reinforcement learning using least squares policy iteration with provable performance guarantees. In Proceedings of the 38th International Conference on Machine Learning, pages 511\u2013520.\n\n[6] Panaganti, K. and Kalathil, D. (2022). Sample complexity of robust reinforcement learning with a generative model. In Proceedings of The 25th International Conference on Artificial Intelligence and Statistics, pages 9582\u20139602.\n\n[7] Roy, A., Xu, H., and Pokutta, S. (2017). Reinforcement learning under model mismatch. In Advances in Neural Information Processing Systems, pages 3043\u20133052.\n\n[8] Panaganti, K., Xu, Z., Kalathil, D., and Ghavamzadeh, M. (2022). Robust reinforcement learning using offline data. Advances in Neural Information Processing Systems (NeurIPS).\n\n[9] Shi, L. and Chi, Y. (2022). Distributionally robust model-based offline reinforcement learning with near-optimal sample complexity. arXiv preprint arXiv:2208.05767\n\n[10] L Shi, G Li, Y Wei, Y Chen, M Geist, Y Chi (2023) The Curious Price of Distributional Robustness in Reinforcement Learning with a Generative Model, NeurIPS 2023"
                },
                "questions": {
                    "value": "-na-"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1135/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698734791588,
            "cdate": 1698734791588,
            "tmdate": 1699636039716,
            "mdate": 1699636039716,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "MAWIAQ2cly",
                "forum": "0ypXhS83Lh",
                "replyto": "EAmEFevqCF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1135/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1135/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer APuc,\n\nWe really appreciate your detailed review. Please see below our responses to your concerns.\n\n> *Related Works.*\n\nThank you so much for referring us to these additional references.  Following your advice, we have updated the manuscript to include a more thorough discussion of the related works. For your convenience, we replicate the updated Related Work section (only the paragraphs about robust RL) below: \n\nOne research topic closely related to our method is domain randomization. Domain randomization is a technique to increase the generalization capability over a set of pre-defined environments. The set of environments are parameterized (e.g., friction and mass coefficient) to allow the agent to encode the knowledge about the deviations between training and testing scenarios. The environment parameters are commonly uniformly sampled during training [19,  21, 22, 23]. Even though ADR [20] is proposed to learn a parameter sampling strategy on the top of domain randomization, all of the aforementioned methods are not learned over the worst case scenarios. Moreover, in real life application, if not chosen carefully, the environment set can also lead to over-pessimism with a larger range while selecting a smaller range of the set will be over-optimistic. Hence, our proposed method can be readily extended into domain randomization by considering the environments as fixed adversaries. \n\nRobustness to transition models has been widely investigated. It was initially studied by robust MDPs [2,3,4] through a model-based manner by assuming the uncertainty set of environmental transitions is known, which can be solved by dynamic programming. In this approach, a base dynamic model is assumed and the uncertainty set is crafted as a ball centered around the base model with a predetermined statistical distance or divergence, e.g., KL-divergence or Wasserstein distance. Following works address scenarios where the base model is unknown but samples from the base model are available.  For example,  [6,10] propose model-based algorithms that first estimates the base model and then solve the robust MDP; [5,7] propose online model-free policy evaluation and policy iteration algorithms for robust RL with convergence guarantees; [1] proposes algorithms with polynomial guarantees for tabular cases where both the number of states and actions are finite.; [8,9] further extends the study of robust RL with only offline data. In contrast to these works, we follow the approach of RARL which does not explicitly specify the set of environments but learns a robust policy by competing with an adversary. \n\nSubsequent works generalize the objective to unknown uncertainty sets, and formulate the uncertainty as perturbations/disturbance introduced into the environments [10,11, 12, 13,14]. Notably, RARL [14] introduces an adversary with the objective to affect the environment to minimize the agent\u2019s rewards. Notably, while in this work we focus on robustness to the transition model, there are two other types of robustness: robustness to the disturbance of actions [15,16] and robustness to state/observation [17,18]. \n\n> *The current framework considers robustness against adversarial actions. Tessler et al., (2019) and thereafter are the closest to current work's setting. Of course then their algorithms need to be benchmarked against.*\n\nThank you for this suggestion. It seems our work considers a rather different setup than Tessler et al. (2019), and hope our response here could help clarify the potential misunderstanding here. Specifically, our work pursues the goal similar to the RARL work, i.e., **toward the robustness against perturbed environment by adversarial training (as part of our methodology), which would be rather different than solving the problem of robustness against adversarial actions provided by the other party (what Tessler et al. (2019) considered)**. In the framework of RARL, the adversary conducts actions to change the environment to minimize the agent\u2019s rewards. The action space of the adversarial is often different to that of the agent while Tessler et al., (2019)  assumes that the protagonist and the adversary share the same action space, which is distinct to the experimental setup in RARL and our work. \n\n*Chen Tessler, Yonathan Efroni, and Shie Mannor. Action robust reinforcement learning and applications in continuous control. In International Conference on Machine Learning, pp. 6215\u20136224. PMLR, 2019.*"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1135/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700112700051,
                "cdate": 1700112700051,
                "tmdate": 1700112700051,
                "mdate": 1700112700051,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CMwSl0dpdh",
                "forum": "0ypXhS83Lh",
                "replyto": "EAmEFevqCF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1135/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1135/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "[1] Zaiyan Xu, Kishan Panaganti, and Dileep Kalathil. Improved Sample Complexity Bounds for Distributionally Robust Reinforcement Learning. International Conference on Artificial Intelligence and Statistics, 2023\n\n[2] Iyengar, G. N. (2005). Robust dynamic programming. Mathematics of Operations Research, 30(2):257\u2013280.\n\n[3] J Andrew Bagnell, Andrew Y Ng, and Jeff G Schneider. Solving uncertain markov decision processes. Citeseer, 2001.\n\n[4] Arnab Nilim and Laurent Ghaoui. Robustness in markov decision problems with uncertain transition matrices. Advances in neural information processing systems, 16, 2003.\n\n[5] Panaganti, K. and Kalathil, D. (2021). Robust reinforcement learning using least squares policy iteration with provable performance guarantees. In Proceedings of the 38th International Conference on Machine Learning, pages 511\u2013520.\n\n[6] Panaganti, K. and Kalathil, D. (2022). Sample complexity of robust reinforcement learning with a generative model. In Proceedings of The 25th International Conference on Artificial Intelligence and Statistics, pages 9582\u20139602.\n\n[7] Roy, A., Xu, H., and Pokutta, S. (2017). Reinforcement learning under model mismatch. In Advances in Neural Information Processing Systems, pages 3043\u20133052.\n\n[8] Panaganti, K., Xu, Z., Kalathil, D., and Ghavamzadeh, M. (2022). Robust reinforcement learning using offline data. Advances in Neural Information Processing Systems (NeurIPS).\n\n[9] Shi, L. and Chi, Y. (2022). Distributionally robust model-based offline reinforcement learning with near-optimal sample complexity. arXiv preprint arXiv:2208.05767\n\n[10] \u200b\u200bL Shi, G Li, Y Wei, Y Chen, M Geist, Y Chi (2023) The Curious Price of Distributional Robustness in Reinforcement Learning with a Generative Model, NeurIPS 2023\n\n[11] Ian Abraham, Ankur Handa, Nathan Ratliff, Kendall Lowrey, Todd D Murphey, and Dieter Fox. Model-based generalization under parameter uncertainty using path integral control. IEEE Robotics and Automation Letters, 5(2):2864\u20132871, 2020.\n\n[12]Daniel J Mankowitz, Nir Levine, Rae Jeong, Yuanyuan Shi, Jackie Kay, Abbas Abdolmaleki, Jost Tobias Springenberg, Timothy Mann, Todd Hester, and Martin Riedmiller. Robust reinforcement learning for continuous control with model misspecification. arXiv preprint arXiv:1906.07516, 2019. \n\n[13]Eugene Vinitsky, Yuqing Du, Kanaad Parvate, Kathy Jang, Pieter Abbeel, and Alexandre Bayen. Robust reinforcement learning using adversarial populations, 2020a.\n\n[14]Lerrel Pinto, James Davidson, Rahul Sukthankar, and Abhinav Gupta. Robust adversarial reinforcement learning. In International Conference on Machine Learning, 2017.\n\n[15] Chen Tessler, Yonathan Efroni, and Shie Mannor. Action robust reinforcement learning and applications in continuous control. In International Conference on Machine Learning, pp. 6215\u20136224. PMLR, 2019.\n\n[16] Yutong Li, Nan Li, H Eric Tseng, Anouck Girard, Dimitar Filev, and Ilya Kolmanovsky. Safe reinforcement learning using robust action governor. In Learning for Dynamics and Control, pp. 1093\u20131104. PMLR, 2021.\n\n[17] Huan Zhang, Hongge Chen, Chaowei Xiao, Bo Li, Mingyan Liu, Duane Boning, and Cho-Jui Hsieh. Robust deep reinforcement learning against adversarial perturbations on state observations. Advances in Neural Information Processing Systems, 33:21024\u201321037, 2020.\n\n[18] Robust Multi-Agent Reinforcement Learning with State Uncertainty Sihong He, Songyang Han, Sanbao Su, Shuo Han, Shaofeng Zou, and Fei Miao. Transactions on Machine Learning Research, June 2023.\n\n[19] Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and Pieter Abbeel. Domain randomization for transferring deep neural networks from simulation to the real world. Intelligent Robots and Systems (IROS), 2017 IEEE/RSJ International Conference on. IEEE, 2017.\n\n[20] Bhairav Mehta, Manfred Diaz, Florian Golemo, Christopher J Pal, and Liam Paull. Active domain randomization. Conference on Robot Learning, pp. 1162\u20131176. PMLR, 2020.\n\n[21] X. B. Peng, M. Andrychowicz, W. Zaremba, and P. Abbeel, \u201cSim-to-Real Transfer of Robotic Control with Dynamics Randomization,\u201d in 2018 IEEE international conference on robotics and automation (ICRA), pp. 3803\u20133810, IEEE, 2018.\n\n[22]  Z. Li, X. Cheng, X. B. Peng, P. Abbeel, S. Levine, G. Berseth, and K. Sreenath, \u201cReinforcement learning for robust parameterized locomotion control of bipedal robots,\u201d in 2021 IEEE International Conference on Robotics and Automation (ICRA), pp. 2811\u20132817, IEEE, 2021.\n\n[23] J. Siekmann, Y. Godse, A. Fern, and J. Hurst, \u201cSim-to-real learning of all common bipedal gaits via periodic reward composition,\u201d in 2021 IEEE International Conference on Robotics and Automation (ICRA), pp. 7309\u20137315, IEEE, 2021."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1135/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700112740064,
                "cdate": 1700112740064,
                "tmdate": 1700112879504,
                "mdate": 1700112879504,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "93PURW3Q9u",
                "forum": "0ypXhS83Lh",
                "replyto": "EAmEFevqCF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1135/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1135/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Gentle Reminder"
                    },
                    "comment": {
                        "value": "Dear Reviewer APuc,\n\nAs the discussion period is closing to an end, we would like to know if there are any other concerns that we can help resolve. The authors are more than happy and fully committed to providing further clarification and addressing any remaining questions you may have. If our responses have effectively addressed all of your primary concerns, we sincerely ask the reviewer to kindly consider an upward adjustment of the score to reflect the improvements made.\n\nThank you once again for the precious time and effort that you have invested in reviewing this work. \n\nBest wishes,\n\nAuthors"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1135/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700506365555,
                "cdate": 1700506365555,
                "tmdate": 1700506365555,
                "mdate": 1700506365555,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2Whbff1jUB",
                "forum": "0ypXhS83Lh",
                "replyto": "EAmEFevqCF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1135/Reviewer_APuc"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1135/Reviewer_APuc"
                ],
                "content": {
                    "title": {
                        "value": "Ack"
                    },
                    "comment": {
                        "value": "Thank you for the detailed response. I think the authors have improved the manuscript compared to the pre-rebuttal stage but there are concerns from other reviewers that need further iterations. I will update my score after the author-reviewer discussion period."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1135/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700545600641,
                "cdate": 1700545600641,
                "tmdate": 1700619481613,
                "mdate": 1700619481613,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ZOtVCTxsJL",
            "forum": "0ypXhS83Lh",
            "replyto": "0ypXhS83Lh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1135/Reviewer_c2QC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1135/Reviewer_c2QC"
            ],
            "content": {
                "summary": {
                    "value": "Deep reinforcement learning (RL) has demonstrated its capability to generate optimal strategies for environments with intricate dynamics. However, there are inherent challenges: the vastness of the parameter search space and limited exploration during training can compromise the robustness and performance guarantees of resulting policies. One technique that bolsters the resilience of RL agents is robustness through adversarial training. In this method, a hostile agent (adversary) aims to minimize the RL agent's cumulative reward by causing disturbances in the environment. Although this framework has strengths, two primary issues emerge: over-optimism due to difficulties in solving inner optimization problems, and over-pessimism from broad, imprecise candidate adversary sets which may consider unrealistic disturbance scenarios. To address these challenges, this study introduces a structured adversarial ensemble where multiple adversaries operate concurrently. This ensemble approach both improves the estimation of worst-case scenarios and shifts the RL agent's objective from absolute worst-case performance to an average of the most challenging scenarios. The proposed method outperforms existing robust RL strategies, and experiments show that it consistently enhances robustness across different environmental disturbances."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper is well-written and easy-to-follow."
                },
                "weaknesses": {
                    "value": "* Limit technical novelty. RARL with an adversarial population is not a novel idea. The difference between ROSE and RAP is very incremental.\n* The theorems and lemmas fall short of providing insights into the theoretical justification of the algorithm design, for example, why to optimize the performance over the worst-$k$ adversaries, how to choose the $k$ value, etc.\n* One important assumption is that the adversaries are distinct enough from each other. However, there is no component in the algorithm that aims to improve diversity explicitly, such as in ADR (Bhairav Mehta et al., 2020). Therefore, I don't think it would be considered a 'structured ensemble'.\n* Missing related work:\n    * Shen, Macheng, and Jonathan P. How. \"Robust opponent modeling via adversarial ensemble reinforcement learning.\" Proceedings of the International Conference on Automated Planning and Scheduling. Vol. 31. 2021.\n    * Huang, Peide, et al. \"Robust reinforcement learning as a Stackelberg game via adaptively-regularized adversarial training.\" Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence (IJCAI-22).\n    * Zhai, Peng, et al. \"Robust adaptive ensemble adversary reinforcement learning.\" IEEE Robotics and Automation Letters 7.4 (2022): 12562-12568."
                },
                "questions": {
                    "value": "See weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1135/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698797766486,
            "cdate": 1698797766486,
            "tmdate": 1699636039642,
            "mdate": 1699636039642,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "no7fCyA65Z",
                "forum": "0ypXhS83Lh",
                "replyto": "ZOtVCTxsJL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1135/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1135/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer c2QC,\n\nWe greatly appreciate your comments. We have found quite a few discrepancies between the comments from the reviewer about specific parts of our methodology, and what had been introduced in the paper -- we have enriched them with more details below, and hopefully they could help clarify the misunderstandings/confusions.\n\n---\n\n> *Limit technical novelty. RARL with an adversarial population is not a novel idea.*\n\nCompared to other works, our work has the following contributions:\n\n1. We **identified and addressed two major bottlenecks at once toward robust RL through adversarial training**, the potential over-optimism and over-pessimism issues. As a result, our method, as demonstrated by our extensive empirical study (e.g., see Figure 2 and Table 1), significantly improves RARL and RAP which fail to address these problems, demonstrating the cruciality of the identified problems. \n2. Our work, to the best of our knowledge, is the first that provides thorough **theoretical understanding** to illustrate the benefit of introducing ensembles to robust RL, which **laid out the theoretical ground that future works can follow**. Moreover, our method differs from the classic average/vote-based ensemble methods, as it considers only a (self-adapted) moving subset of the ensemble that are most helpful for combating potential over-optimism/pessimism. \n3. We conducted extensive experiments and ablation studies to verify the efficacy of our method. The results have shown that our method consistently dominates baselines across **different environments, with various types of disturbance applied, over a variety (i.e., on-policy and off-policy) of backbone RL algorithms.** \n\nWhile the amount of difference between algorithms is a challenging question to settle, the difference of our proposed method to other ensemble method-based algorithms has proven to lead to significant performance improvement. This exactly underscores the technical importance of our proposed method. As the other two reviewers are satisfied with the novelty of our method, we would greatly appreciate it if the reviewer could further pinpoint from which perspectives we could further improve the novelty of our work, in order to lead to your satisfaction.\n\n> *Connection between theorem and the algorithm* \n\nWe have detailed the insights brought by the theoretical results in the paragraph **\u201cInsights from the Theoretical Results\u201d** in Section 3.1. To reiterate, our theoretical results have shown that the number of the adversaries required is of a moderate polynomial order. There are two parameters to choose: the total number of adversaries and the k-value. With the analysis corroborating that the total number of adversaries is a relatively small number and the fact that k-value is upper bounded by the total number of adversaries, the search space is significantly reduced by optimizing over the worst-k adversaries (which does not necessarily compensate for optimality either).\n\n> *Why to optimize the performance over the worst-k adversarial, how to choose the k-value, etc.*\n\nAs presented in Section 3.2, we propose to optimize over the worst-k adversaries to address the potential over-pessimism caused by the misspecification of the adversary set \u2013 among other things, this is a novelty of our work that enables our  method to significantly outperform existing (state-of-the-art) methods. In most practical real-world scenarios, it is often challenging to have a precise characterization of the adversary set, thus leaving the robust RL algorithms with an adversary set with agents mostly leading to irrelevant and infeasible outcomes, e.g., an adversary that applies extreme disturbance that is unfeasible to occur in realistic scenarios. The effectiveness of our methodology is concretely justified by extensive experiments in Section 4, because the objective of the protagonist can be significantly diverted if it optimizes against solely the worst adversary who may apply unrealistic disturbances. Hence, the choice of k-value depends on to what extent one would want to minimize the effects brought in by such unrealistic adversaries. In principle, larger $k$-value should be used for environments with straightforward dynamics, where the adversaries are more likely to discover extreme strategies easily.\n\n> *However, there is no component in the algorithm that aims to improve diversity explicitly*\n\nThank you for this question. By Occam\u2019s Razor, we believe it is beneficial to retain only necessary components of an algorithm. **As illustrated by empirical studies ( Section 6 Ablation A4), we have confirmed that as long as the adversaries are initialized differently, they will remain distinct.** And the consistently improved performance has demonstrated that the distinction from random initialization is sufficient. Hence, we choose to not induce extra components that can potentially cause stability issues and consume more computation complexity."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1135/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700112381482,
                "cdate": 1700112381482,
                "tmdate": 1700112381482,
                "mdate": 1700112381482,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WC9NWVDGUL",
                "forum": "0ypXhS83Lh",
                "replyto": "ZOtVCTxsJL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1135/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1135/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Gentle Reminder"
                    },
                    "comment": {
                        "value": "Dear Reviewer c2QC,\n\nAs the discussion period is closing to an end, we would like to know if there are any other concerns that we can help resolve. The authors are more than happy and fully committed to providing further clarification and addressing any remaining questions you may have. If our responses have effectively addressed all of your primary concerns, we sincerely ask the reviewer to kindly consider an upward adjustment of the score to reflect the improvements made.\n\nThank you once again for the precious time and effort that you have invested in reviewing this work. \n\nBest wishes,\n\nAuthors"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1135/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700506332921,
                "cdate": 1700506332921,
                "tmdate": 1700506332921,
                "mdate": 1700506332921,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "NdBbXxDz9g",
                "forum": "0ypXhS83Lh",
                "replyto": "WC9NWVDGUL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1135/Reviewer_c2QC"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1135/Reviewer_c2QC"
                ],
                "content": {
                    "title": {
                        "value": "Re"
                    },
                    "comment": {
                        "value": "Thanks to the authors for the response. However, my original concerns regarding novelty, the main theorem, and the algorithm design still remain.\n - Two major bottlenecks, namely over-optimism and over-pessimism, have been identified by numerous literature in robust RL, as evidenced by the literature review. Therefore, it is hard to count them as novelties.\n - The disjoint between the theorem and empirical results. As the authors pointed out in the **Insights from the Theoretical Results**, \"Lemma 3 implies that the true benefit brought by the adversarial ensemble lies in the optimization process instead of the final optimal solution it offers.\" In this case, why can a theorem about the approximation error and the final optimal solution justify the empirical results if the true benefit is brought by the optimization process? Maybe the authors could provide more insight into this justification.\n - The authors claim that \"as illustrated by empirical studies (Section 6 Ablation A4), as long as the adversaries are initialized differently, they will remain distinct\". There are two comments regarding this. \n   - First, ablation A4 only shows the adversaries get updated evenly and does not actually say anything about the diversity of the adversary. For example, let's say the ensemble has no diversity, e.g., output $10 \\pm \\epsilon, \\epsilon \\sim \\mathcal N (0, 0.1)$ every iteration. They will get updated evenly, but it has little to do with the diversity. \n   - Second, maintaining diversity is not an \"unnecessary component.\" As shown by a number of works such as Stein Variation Policy Gradient [1] and Active Domain Randomization [2], diversity is not going to be ensured just by initializing differently; different initialization could lead to very similar state-action visiting distributions. It is heavily dependent on the optimization landscape as well as the choice of hyperparameters. \n\nCombining all the aforementioned reasons, I think the current version does not warrant acceptance. I encourage the authors to improve the manuscripts and resubmit.\n\n**Ref:**\n[1] Liu, Yang, et al. \"Stein variational policy gradient.\" arXiv preprint arXiv:1704.02399 (2017).\n[2] Mehta, Bhairav, et al. \"Active domain randomization.\" Conference on Robot Learning. PMLR, 2020."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1135/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700613511818,
                "cdate": 1700613511818,
                "tmdate": 1700613511818,
                "mdate": 1700613511818,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JyFDDF1Ggo",
                "forum": "0ypXhS83Lh",
                "replyto": "M0v7M6EAjT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1135/Reviewer_c2QC"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1135/Reviewer_c2QC"
                ],
                "content": {
                    "title": {
                        "value": "Re:"
                    },
                    "comment": {
                        "value": "Thanks for the further response. However, some of my concerns are still not addressed adequately. Let me provide the references and reiterate some questions I have:\n\n> List of references for over-optimism and over-pessimism for adversarial training in RL.\n\n- **Overfitting to the worst case**: \n    - Vinitsky, Eugene, et al. \"Robust reinforcement learning using adversarial populations.\" arXiv preprint arXiv:2008.01825 (2020).\n    - Gleave, Adam, et al. \"Adversarial policies: Attacking deep reinforcement learning.\" arXiv preprint arXiv:1905.10615 (2019).\n    - Dennis, Michael, et al. \"Emergent complexity and zero-shot transfer via unsupervised environment design.\" Advances in neural information processing systems 33 (2020): 13049-13061.\n- **Adversarial training with worst $\\epsilon$ percentile of returns to optimize the conditional value at risk (CVaR)**\n    - Rajeswaran, Aravind, et al. \"Epopt: Learning robust neural network policies using model ensembles.\" arXiv preprint arXiv:1610.01283 (2016).\n    - Chow, Yinlam, et al. \"Risk-sensitive and robust decision-making: a cvar optimization approach.\" Advances in neural information processing systems 28 (2015).\n\nThis is a non-comprehensive list of literature. For a complete survey, please refer to Moos, Janosch, et al. \"Robust reinforcement learning: A review of foundations and recent advances.\" Machine Learning and Knowledge Extraction 4.1 (2022): 276-315.\n\n\n> Reference to SVPG and ADR.\n\nWithout explicit regularization, there is no extensive empirical or theoretical evidence for wishing ANY particle-based optimization to converge to distinct solutions just by initializing the particle differently. Actually, SVPG and the extensions from it show counter-examples for that (Figure 2 of SVPG). Again, ablation A4 offers no direct evidence for the diversity the authors claimed. Instead of the updating frequency, showing the state-action visiting distribution might be more helpful."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1135/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700689416905,
                "cdate": 1700689416905,
                "tmdate": 1700689416905,
                "mdate": 1700689416905,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "mA1yk9k9He",
            "forum": "0ypXhS83Lh",
            "replyto": "0ypXhS83Lh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1135/Reviewer_1Lc9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1135/Reviewer_1Lc9"
            ],
            "content": {
                "summary": {
                    "value": "The paper examines the limitations of reinforcement learning (RL) in robust policy design due to potential environmental disturbances. It identifies two key issues in adversarial training for RL: over-optimism from complex inner optimization and over-pessimism from the selection of adversarial scenarios. The authors propose an adversarial ensemble approach to address over-optimism and optimize average performance against the worst-k adversaries to mitigate over-pessimism. The theoretical underpinnings of this method are presented, and its efficacy is demonstrated through comprehensive experimental results."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. **Structure and Clarity**: The paper is well-structured and provides clear explanations, enhancing readability and comprehension.\n  \n2. **Clear Motivation**: The authors articulate the significance of addressing both over-optimism and over-pessimism in adversarial training, which effectively establishes the paper's purpose.\n\n3. **Theoretical Foundation**: The paper offers a solid theoretical analysis, bolstering the credibility of the proposed method.\n\n4. **Experimental Validation**: The inclusion of extensive experimental results substantiates the claims and demonstrates the practical benefits of the proposed algorithm."
                },
                "weaknesses": {
                    "value": "1. **Figure Clarity**: Figure 1, intended to aid in understanding, is unclear. A more straightforward illustration with an improved caption is needed.\n\n2. **Explanation of Solution to Over-Pessimism**: The rationale behind using the average performance over the worst-k adversaries, as presented in Section 3.2, requires further clarification to be convincing. Can the authors elaborate on how the average performance optimization directly counteracts over-pessimism?\n\nOverall, in my opinion, the paper contributes a thoughtful approach to improving the robustness of RL algorithms, which is substantiated by both theoretical analysis and experimental results. Despite some ambiguity in graphical representation and the need for additional explanations in certain sections, the paper is a good candidate for ICLR. Clarity improvements in the mentioned areas could enhance the paper's impact."
                },
                "questions": {
                    "value": "Please refer to weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1135/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698917827464,
            "cdate": 1698917827464,
            "tmdate": 1699636039581,
            "mdate": 1699636039581,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "sLZcx3OYqy",
                "forum": "0ypXhS83Lh",
                "replyto": "mA1yk9k9He",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1135/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1135/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Improved Clarify"
                    },
                    "comment": {
                        "value": "Dear Reviewer 1Lc9,\n\nThank you so much for your dedicated effort in helping improve our work! Please see below our detailed responses. \n\n---\n\n> *Figure 1. A more straightforward illustration with an improved caption is needed.*\n\nWe appreciate this advice and we have updated the manuscript to include a new Figure 1 with improved caption. \n\n> *Can the authors elaborate on how the average performance optimization directly counteracts over-pessimism?*\n\nThank you for this insightful question. In most practical real-world scenarios, it is often challenging to have a precise characterization of the adversary set, thus leaving the robust RL algorithms with an adversary set with agents mostly leading to irrelevant and infeasible outcomes, e.g., an adversary that applies extreme disturbance that is unfeasible to occur in realistic scenarios. Hence, we propose to optimize over worst-k adversaries so that **robust RL can prevent the over-pessimism without precise knowledge of the adversary set**. This approach is highly effective, as demonstrated by the compelling results from extensive experiments, because the objective of the protagonist can be significantly diverted if it optimizes against solely the worst adversary who may unfortunately be one of the erroneously chosen adversaries in the adversary set. \n\nMoreover, as the max-min problems in robust RL are normally solved by iterative updates of the protagonist and the adversaries, where in each iteration we have an adversary $\\phi$ against whom we will optimize the protagonist. However, if the adversary set is not precise, $\\phi$ may be a misspecified scenario. If the rest $k-1$ adversaries (or the majority of the worst-$k$ adversaries) are indeed in the true interested scenarios, optimizing the average over the worst-k adversaries distracts the attention of the protagonist from the single uninterested worst case to the cases of interest. However, we cannot directly optimize the average performance of all the adversaries because this can lead to too little consideration of the worst-case performance, as demonstrated by our empirical study in Figure 2. \n\nWe find this comment very helpful for improving the clarity on the motivations of this work, and we have already updated our manuscript to include this detailed explanation (in red text)."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1135/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700112196834,
                "cdate": 1700112196834,
                "tmdate": 1700118879622,
                "mdate": 1700118879622,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "09CRNqoJfi",
                "forum": "0ypXhS83Lh",
                "replyto": "mA1yk9k9He",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1135/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1135/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Gentle Reminder"
                    },
                    "comment": {
                        "value": "Dear Reviewer 1Lc9,\n\nAs the discussion period is closing to an end, we would like to know if there are any other concerns that we can help resolve. The authors are more than happy and fully committed to providing further clarification and addressing any remaining questions you may have. If our responses have effectively addressed all of your primary concerns, we sincerely ask the reviewer to kindly consider an upward adjustment of the score to reflect the improvements made.\n\nThank you once again for the precious time and effort that you have invested in reviewing this work. \n\nBest wishes,\n\nAuthors"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1135/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700506294184,
                "cdate": 1700506294184,
                "tmdate": 1700506294184,
                "mdate": 1700506294184,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]