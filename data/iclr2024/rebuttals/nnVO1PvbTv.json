[
    {
        "title": "Think-on-Graph: Deep and Responsible Reasoning of Large Language Model on Knowledge Graph"
    },
    {
        "review": {
            "id": "YYoD5qEUdD",
            "forum": "nnVO1PvbTv",
            "replyto": "nnVO1PvbTv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2328/Reviewer_LtPr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2328/Reviewer_LtPr"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents an LLM-KG integration paradigm to incorporate structural knowledge stored in KGs in LLMs reasoning, namely Think-on-Graph (ToG). ToG makes LLM serve as an agent to walk on KGs by iteratively searching and pruning relations and entities from KGs. Experiments show that ToG could enhance the LLM\u2019s reasoning capabilities and achieve SOTA on 6 datasets. ToG also exhibits knowledge traceability and correctability to improve KG quality."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1. Good written paper, very easy to read.\n2. Strong experimental performance, achieving SOTA on 6 datasets without training.\n3. The motivation is strong and clear, incorporating external knowledge (KGs) would be an important problem to enhance LLMs."
                },
                "weaknesses": {
                    "value": "1. I believe it would be interesting to see the ToG performance when encountering the knowledge conflict between external KG knowledge and parametric knowledge stored in LLMs, which is an aspect to test the robustness of ToG.\n2. I am concerned that ToG would bring too many intermediate steps and cause high latency in reasoning and expensive deployment, especially when using API-based black-box models like GPT-4.\n3. It would be interesting to see the performance of ToG when incorporating the KGs of low quality (sparsity, noisy, etc), since most KGs are sparse and out of date to some extent. So investigating the impact of KG quality would enhance the understanding of ToG method and its limitations. \n4. I would encourage authors to present experiment results on small LLMs, such as 7B and 13B."
                },
                "questions": {
                    "value": "N/A"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2328/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2328/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2328/Reviewer_LtPr"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2328/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698678960952,
            "cdate": 1698678960952,
            "tmdate": 1699636165357,
            "mdate": 1699636165357,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "hTq5zbKfQC",
                "forum": "nnVO1PvbTv",
                "replyto": "YYoD5qEUdD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2328/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2328/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2328/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2328/Reviewers",
                    "ICLR.cc/2024/Conference/Submission2328/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission2328/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2328/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer LtPr"
                    },
                    "comment": {
                        "value": "Thank you so much for your comments and inspiring suggestions. Our reply to your comments and questions are listed below. \n\n**1.\tReviewer's Comment**: I believe it would be interesting to see the ToG performance when encountering the knowledge conflict between external KG knowledge and parametric knowledge ...\n\n**Reply**: We thank the reviewer for your inspiring comment on what happens when KG conflicts with LLM. Although the intrinsic knowledge of LLMs is difficult to probe, we can use the reasoning results of LLM and the results of the KG queries to determine whether there is a knowledge conflict between LLM and KG instead. We run an experiment by analyzing the log file of ToG experiment on CWQ dataset and count KG's performance when LLM and KG output different answers. Among the questions that CoT with GPT-4's (representing LLM) answer conflicts with KG's answer, the accuracy of ToG with GPT-4 is 43.4%, a 13.7% reduction from its performance on CWQ 57.1%.\n\n**2.\tReviewer's Comment**: I am concerned that ToG would bring too many intermediate steps and cause high latency in reasoning and expensive deployment\n\n**Reply**: Please refer to our submitted official comment \"Response to All reviewers about the concerns on the Efficiency of ToG\", where we elaborate on the maximum cost of answering a question needed by our method and the average cost on CWQ. We also propose some feasible solutions on how to improve the efficiency of ToG (some have been empirically verified in experiments). In particular, we show in this official comment that the latency in inference is NOT as high as what people may imagined using improved ToG algorithms. In most cases, the number of LLM calling by ToG is only single digit (even <5 times in most cases).\n\n**3.\tReviewer's Comment**: It would be interesting to see the performance of ToG when incorporating the KGs of low quality (sparsity, noisy, etc.), since most KGs are sparse and out of date to some extent. \n\n**Reply**:\n \n(1)\tWe agree with the reviewer that quality of knowledge graphs may have significant impact on the performance of ToG. We are doing experiments to test how the qualify of a KG affect ToG's performance. Specifically, we artificially add different amount of noise/errors and/or delete different amount of triples existing in original wikidata, and evaluate how much ToG's performance drops down. Unfortunately, the experiments are still in progress due to limited computational and LLM token resource. We are trying our best to show the experimental results asap.\n \n(2)\tAnother important task we are doing is to research potential solutions for alleviating the negative effect of low-quality KG in ToG inference. Firstly, in Section 3.3, we have illustrated how ToG could correct the wrong information in KG with the help of LLMs and users (identifying potential errors in KG by LLM and determining the correctness by human). More concretely, the LLM can suggest to the users which knowledge in the reasoning paths may be wrong, and the users reply in the system on whether execute a correction action. Secondly, we are studying how to improve the quality of KG by fusing diverse external information source such as variant LLMs, search engine and many others.\n\n**4.\tReviewer's Comment**: I would encourage authors to present experiment results on small LLMs, such as 7B and 13B.\n\n**Reply**: Following the reviewer's suggestion, we tested the performance using LLaMA2-7b and LLaMA2-13b, two very small LLMs, on WebQSP, and the results are summarized below. \n\n| Method | LLaMA2-7b | LLaMA2-13b |\n| --- | --- | --- |\n| CoT | 7.1%| 8.6% |\n| ToG | 12.6% | 25.1% |\n\nUsing exact match as the evaluation metric, the CoT prompting with LLaMA-7B and LLaMA-13B can only achieve 7.1% and 8.6%, respectively, on WebQSP. Meanwhile, ToG with LLaMA2-7B and LLaMA2-13B can achieve 12.6% and 25.1%, respectively. Consistent with our expectation, ToG performs better than CoT with these two very small LLMs. However, the actual accuracies for CoT and ToG are indeed far below from our expectation. After careful investigation, we found the main factor of the bad performance is: since both LLaMA2-7B and LLaMA2-13B are base models which have not been trained with supervised fine tuning and RLHF, the format of their outputs are really difficult to manage and difficult to exactly match the format required for a \"correct\" answer. For example, we require the answers from LLM should be put between { and }, but these two small LLMs always violate this rule and make the evaluator difficult to locate the answer, leading its answer misclassified to \"wrong\". In another example, we ask LLM to prune and return top 3 relations from candidates (A, B, C, D, E), but LLaMA2-7B and LLaMA2-13B return irrelevant relations, e.g., W, S, T etc.  In some cases, LLaMA2 models restate the input questions or repeat the text over and over again without responding to the question."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2328/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700471549288,
                "cdate": 1700471549288,
                "tmdate": 1700471549288,
                "mdate": 1700471549288,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "WJZA3ZPYzn",
            "forum": "nnVO1PvbTv",
            "replyto": "nnVO1PvbTv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2328/Reviewer_K4jf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2328/Reviewer_K4jf"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a novel approach called Think-on-Graph (ToG) to synergize the LLMs and KGs for reasoning. ToG enables LLMs as agents to iteratively execute searches on KGs to discover promising reasoning paths, which are then used to guide the LLMs to generate accurate answers. ToG is a general framework that can be applied to various LLMs and KGs. Extensive experiments on several datasets show that ToG can significantly improve the reasoning performance of LLMs."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper is well-presented and easy to follow. The authors provide a clear motivation and a good introduction to the problem.\n\n2. The proposed framework can be easily plugged into existing LLMs and KGs without incurring additional training costs.\n\n3. Extensive experiments on several datasets show that ToG can significantly improve the reasoning performance of LLMs."
                },
                "weaknesses": {
                    "value": "1. The computational cost of ToG is relatively high. The searching process of ToG involves multiple LLM calls, which may be costly and limit its practical applicability in some settings. \n\n2. Some details are inconsistent in the paper. For example, in the approach introduction section, ToG selects the next step triples/relations based on current expended reasoning paths. However, in the prompt illustrated in G.3, I cannot find the current reasoning paths used for the pruning process.\n\n3. I have concerns that ToG might not understand the meaning of relations well and generalize to different KGs. The relations defined in KGs are usually in diverse formats. For example, the relations in Freebase are defined in a hierarchical format, while the relations in Yago have clearer semantics. If the relations do not reveal the clear semantics, the searching process of ToG might be misled."
                },
                "questions": {
                    "value": "1. Can the authors discuss the cost of ToG in detail? What is the average number of calls for the reasoning? What is the overall price for the ChatGPT/GPT-4 API calls?\n\n2. Can the authors explain the inconsistency I discussed above and present a clear illustration of the whole reasoning process?\n\n3. Can the authors explain how ToG can generalize to different KGs? How LLMs in ToG understand the meaning of relations in different KGs without additional training?\n\n4. Can other search algorithms be used for ToG? For example, depth-first search, breadth-first search, A* search, etc.\n\n5. Can ToG be used to solve complex reasoning problems that cannot be solved by path-based reasoning? For example, in GrailQA, there are questions like: \"How many TV programs has Bob Boyett created?\". This question needs to count the KG structures to get the answer. Besides, there are other complex questions requiring intersection, union, and negation operations. Can ToG be used to solve these kinds of questions? Maybe a limitations section can be added to the paper."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2328/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698713553927,
            "cdate": 1698713553927,
            "tmdate": 1699636165267,
            "mdate": 1699636165267,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xdOX5vbdX8",
                "forum": "nnVO1PvbTv",
                "replyto": "WJZA3ZPYzn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2328/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2328/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2328/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2328/Reviewers",
                    "ICLR.cc/2024/Conference/Submission2328/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission2328/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2328/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer K4jf"
                    },
                    "comment": {
                        "value": "Thank you so much for your comments and inspiring suggestions. Our reply to your comments and questions are listed below. \n\n**1.Reviewer's Comment**: The computational cost of ToG is relatively high. The searching process of ToG involves multiple LLM calls, which may be costly and limit its practical applicability in some settings. Can the authors discuss the cost of ToG in detail? \n\n**Reply**:  Please refer to our submitted official comment \"Response to All reviewers about the concerns on the Efficiency of ToG\", where we elaborate on the maximum cost of answering a question needed by our method and the average cost on CWQ. We also propose some feasible solutions on how to improve the efficiency of ToG (some have been empirically verified in experiments).\n\n**2.Reviewer's Comment**: Some details are inconsistent in the paper. For example, in the approach introduction section, ToG selects the next step triples/relations based on current expended reasoning paths. However, in the prompt illustrated in G.3, I cannot find the current reasoning paths used for the pruning process.\n\n**Reply**: First of all, we are sorry for the confusion about what (whole path or only triple?) ToG really gives LLM's prompt in pruning. We clarify it here: ToG selects the next relations/entities based only on the literal information of candidate entities/relations and the given question (as mentioned in Section 2.1.2), rather than the current whole expanded reasoning paths. We would revise the description in the manuscript to make it clearer for readers. In addition, in future work, we are going to study whether the pruning process might be improved by inputting current expanded reasoning paths instead.\n\n**3.Reviewer's Comment**: I have concerns that ToG might not understand the meaning of relations well and generalize to different KGs. The relations defined in KGs are usually in diverse formats. For example, the relations in Freebase are defined in a hierarchical format, while the relations in Yago have clearer semantics. \n\n**Reply**: We understand the reviewer's concern whether LLM in ToG can correctly understand complex and diverse relations in various KGs. Conceptually, although the format of relations in Freebase is different from natural language, the relation names still consist of words and phrases understandable by LLMs (e.g., people.person.occupation, sports.mascot.team, the last word shows the meaning of relation). In the case of Wikidata, most relations are quite similar to natural language (e.g., instance of, part of, founded by). Empirically, it can be seen from our experimental results that LLMs and light-weight pruning models can perform relation pruning well on both Freebase and Wikidata. In addition, the reviewer's concern leads us to study whether ToG can be improved either by cleaning the relation names in KGs before inputting them to LLMs or by incorporating the specific relation structure of a KG. We would study this problem in the future.\n\n**4.Reviewer's Comment**: Can the authors discuss the cost of ToG in detail? What is the average number of calls for the reasoning? What is the overall price for the ChatGPT/GPT-4 API calls?\n\n**Reply**: Please refer to our submitted official comment \"Response to All reviewers about the concerns on the Efficiency of ToG\", where we elaborate on the maximum cost of answering a question needed by our method and the average cost on CWQ. We also propose some feasible solutions on how to improve the efficiency of ToG (some have been empirically verified in experiments).\n\n**5.Reviewer's Comment**: Can the authors explain the inconsistency I discussed above and present a clear illustration of the whole reasoning process?\n\n**Reply**: Since this question is similar to Comment 2, please refer to the reply to Comment 2.\n\n**6.Reviewer's Comment**: Can the authors explain how ToG can generalize to different KGs? How LLMs in ToG understand the meaning of relations in different KGs without additional training?\n\n**Reply**: This question is similar to Comment 3. Please refer to the reply to reviewer's comment 3. \n \n**7.Reviewer's Comment**: Can other search algorithms be used for ToG? For example, depth-first search, breadth-first search, A* search, etc.\n\n**Reply**: Yes, theoretically, the search algorithm in ToG can be replaced to many other search algorithms, depending on the trade-off between efficiency and sufficiency. For example, beam search can be considered as a constrained breadth-first search (BFS), in order to improve search efficiency by appropriately sacrificing sufficiency. Likewise, constrained DFS and A* algorithm can also be used for searching optimal reasoning paths of ToG. In the future work, we are studying which search algorithms are better and how to automatically select the best search algorithm for ToG according to specific task and specific KG structure. In addition, we are going to study new search algorithms to optimize the performance and efficiency of ToG."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2328/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700471899693,
                "cdate": 1700471899693,
                "tmdate": 1700471998617,
                "mdate": 1700471998617,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "W34J2NrBTl",
                "forum": "nnVO1PvbTv",
                "replyto": "xdOX5vbdX8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2328/Reviewer_K4jf"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2328/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2328/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2328/Reviewers",
                    "ICLR.cc/2024/Conference/Submission2328/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission2328/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2328/Reviewer_K4jf"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for providing additional experimental results and responses to my questions. The efficiency is a concern raised by many reviewers (myself included), and I welcome the discussions on how it can be improved, albeit with some sacrifice of task performance. \n\nThe responses have addressed most of my questions, and I believe my original score is still a fair assessment of the paper. Therefore I will maintain my score."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2328/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700524753949,
                "cdate": 1700524753949,
                "tmdate": 1700524753949,
                "mdate": 1700524753949,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nmGu52H5dg",
                "forum": "nnVO1PvbTv",
                "replyto": "WJZA3ZPYzn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2328/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2328/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2328/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2328/Reviewers",
                    "ICLR.cc/2024/Conference/Submission2328/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission2328/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2328/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer K4jf"
                    },
                    "comment": {
                        "value": "We strongly recognize the need for a discussion on how to improve the efficiency of algorithms. In this paper, we have discussed some solutions such as the proposed ToG-R and the use of lightweight models instead of large language models as pruning models. We will also explore more ways to improve algorithmic efficiency afterwards, including some of the methods we mentioned in our submitted official comment **Response to all reviewers about the concerns on the efficiency of ToG**. We are also open to suggestions and comments for the efficiency issue.\n\nWe are deeply appreciative of your  recognition of the paper's contribution. And your expertise and thoughtful review have made a significant impact on our research. We look forward to potentially incorporating more of your valuable insights in our future work.\n\nThank you once again for your time and guidance."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2328/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700719166278,
                "cdate": 1700719166278,
                "tmdate": 1700719296863,
                "mdate": 1700719296863,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "dZ5muK4j3x",
            "forum": "nnVO1PvbTv",
            "replyto": "nnVO1PvbTv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2328/Reviewer_ezze"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2328/Reviewer_ezze"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes ToG (Think-on-Graph) for deep, responsible, and efficient LLM reasoning with knowledge graphs with a new paradigm of \"LLM\u00d7KG.\" The ToG is with the searching and pruning procedures to conduct deep reasoning. Empirical results on five KBQA datasets with extensive experiments justify the effectiveness of the proposed ToG method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is rather solid and detailed from a technical perspective.\n\nThe figures are clear and easy to understand.\n\nThe solution of LLM\u00d7KG is reasonable and new to the LLM community.\n\nThe depth and width of Toc are clearly investigated in the ablation study.\n\nThe limitations of the proposed method are extensively discussed in Appendix A.\n\nSeveral technical details, case studies, and evaluation results are also elaborated on in the Appendix."
                },
                "weaknesses": {
                    "value": "The writing of the paper can be largely improved. \n\nBesides, the mathematical notations and equations can be improved to be clearer.\n \nIt would be better to summarize the frequently used notations in one table or sentence.\n\nThe empirical performance of ToG is not consistently the best, which is outperformed by the \"prior finetune SOTA\" in some cases of Tab. 1. Although not requiring training is a major benefit of ToG, its reasoning power is not fully convincing enough. I would suggest the paper make a further discussion and explanation for that.\n\nBesides, the important baseline, \"Prior Prompting SOTA\" in Tab.1, is not sufficiently evaluated and compared. It would be much better and more convincing to fill in the blanks in Tab.1. \n\nThe paper mentions the hallucination problem many times and uses it as the motivation of ToG. However, the hallucination problem is not sufficiently studied. As far as I can see, there is only one preliminary analysis of error is provided in Appendix D.2.\n\nThe paper is empirically driven and lacks in-depth analysis, whether from methodological or theoretical perspectives."
                },
                "questions": {
                    "value": "What is the running-time efficiency of ToG?\n\nHow do the KGs used in the experiment part solve the limitation of out-of-date knowledge?\n\nThe beam search with pruning adopted by ToG is quite relevant to the progressive reasoning methods equipped with learnable search and pruning mechanisms, which also come from the KG areas (e.g., AdaProp [1] and Astarnet [2]). I would suggest the paper have a discussion with these relevant works. In addition, is it possible to achieve a kind of learnable pruning as an enhancement of ToG?\n\n[1] Zhang et al. AdaProp: Learning Adaptive Propagation for Graph Neural Network based Knowledge Graph Reasoning. KDD 2023.\n\n[2] Zhu et al. A*Net: A Scalable Path-based Reasoning Approach for Knowledge Graphs. NeurIPS 2023."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2328/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698738700083,
            "cdate": 1698738700083,
            "tmdate": 1699636165122,
            "mdate": 1699636165122,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "EpIyW84NW2",
                "forum": "nnVO1PvbTv",
                "replyto": "dZ5muK4j3x",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2328/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2328/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2328/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2328/Reviewers",
                    "ICLR.cc/2024/Conference/Submission2328/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission2328/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2328/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ezze"
                    },
                    "comment": {
                        "value": "We would like to appreciate your valuable time and comments. Hope our following response can answer your questions.\n\n**1.\tReviewer's Comment**: \"The writing of the paper can be largely improved. Besides, the mathematical notations and equations can be improved to be clearer. It would be better to summarize the frequently used notations in one table or sentence.\"\n\n**Reply**: We sincerely thank the reviewer's suggestions on paper writing. Firstly, we are improving the manuscript writing to make it easier to read. Secondly, we follow your suggestions to add a table explaining all notations in this paper and to refine all notations and equations to be clearer. All these revisions will be added to the final version of the manuscript. \n\n**2.\tReviewer's Comment**: \"The empirical performance of ToG is not consistently the best, which is outperformed by the \"prior finetune SOTA\" in some cases of Tab. 1. Although not requiring training is a major benefit of ToG, its reasoning power is not fully convincing enough. I would suggest the paper make a further discussion and explanation for that.\"\n\n**Reply**: ToG doesn't outperform \"prior finetune SOTA\" on SimpleQuestion and T-REx, which both are single-hop reasoning datasets. As we claimed in the end of the first paragraph of Sec 3.2.1, one of ToG's advantage is its deep reasoning ability and thus it is narually good at multi-hop reasoning tasks rather than single-hop reasoning. Specifically, answering single-hop questions does not really need long reasoning path like what ToG does with LLM on KG. It is noticeable that, although Zero-Shot RE is also a single-hop dataset, ToG (88.3%) significantly outperforms the prior FT SOTA method (74.6%), which means ToG has chance to perform good on single-hop questions. In previous version of manuscript, ToG-R (69.5%) performs slightly worse than prior FT SOTA (70.4%) on multi-hop dataset CWQ, partially because we simply set search width $N=3$ and search depth $D=3$ without any comprehensive hyper-parameter tuning. During the rebuttal period, we rerun the experiment with larger $N=4$ and $D=4$, and the accuracy of ToG-R increases from the original 69.5% to the new 72.5%, exceeding the prior FT SOTA's 70.4%. We are going to update the data in the next version of this manuscript.  \n\n**3.\tReviewer's Comment**: \"Besides, the important baseline, \"Prior Prompting SOTA\" in Tab.1, is not sufficiently evaluated and compared. It would be much better and more convincing to fill in the blanks in Tab.1.\"\n\n**Reply**\uff1aWe have the same concern with the reviewer. Although we have tried our best to search and collect \"Prior Prompting SOTA\" results from all investigated publications (incorporate external KGs into LLM reasoning), only few of them have been evaluated publicly on the datasets we used, and some of these prior works have not released their source codes publicly. That's why we only show \"Prior Prompting SOTA\" results on two datasets in Figure 1 when we submitted this manuscript. To implement a more comprehensive and sufficient comparison as you suggested, we are now reproducing the source codes of many other prompting-based methods and evaluating them on all datasets we used in this paper. The relevant experiments are still in progress, and we are sorry that new results can't be reported here due to limited time for rebuttal. We expect to be able to complete the evaluation experiments as soon as possible and update the Table 1 with the evaluation results in the later version of our paper. \n\n**4.\tReviewer's Comment**: \"The paper mentions the hallucination problem many times and uses it as the motivation of ToG. However, the hallucination problem is not sufficiently studied. As far as I can see, there is only one preliminary analysis of error is provided in Appendix D.2.\"\n\n**Reply**: In this article, we don't aim to figure out the whole hallucination problem. Rather, we aim to solve a subset of the hallucination problem: accurate deep-reasoning problem, which account for a large amount of hallucination in LLM reasoning. From the results in Table 1, Table 2 and tables in appendix, we can see ToG brings significant accuracy improvement in deep-reasoning problems compared with LLM-only methods \uff08CoT etc.).\n\n**5.\tReviewer's Comment**: What is the running-time efficiency of ToG?\n\n**Reply**: Please refer to our submitted official comment \"Response to All reviewers about the concerns on the Efficiency of ToG\"."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2328/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700465374495,
                "cdate": 1700465374495,
                "tmdate": 1700471625587,
                "mdate": 1700471625587,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FD2xSWQsMg",
                "forum": "nnVO1PvbTv",
                "replyto": "dZ5muK4j3x",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2328/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2328/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2328/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2328/Reviewers",
                    "ICLR.cc/2024/Conference/Submission2328/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission2328/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2328/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ezze, specifically about the methodological and theoretical analysis"
                    },
                    "comment": {
                        "value": "**6.\tReviewer's Comment**: \"The paper is empirically driven and lacks in-depth analysis, whether from methodological or theoretical perspectives.\"\n\n**Reply**: Along with the algorithmic research work of ToG, we are studying a theoretical framework for ToG, and even more general, for knowledge-driven LLM. We will add preliminary results of theoretical analysis in the camera-ready version of this manuscript (appendix) if this paper is accepted and a more comprehensive theoretical framework and mathematical analysis will be published in future articles.\n   \nWe briefly highlight the main results of theoretical analysis as follows. A more detailed version of theoretical description can be found at this anonymous document: https://imgbox.com/hI5D5FSu\n\n(1) We reformulate the knowledge-driven LLM problem (where ToG is a special case) as a two-level nested Markov decision process (and thus can be analyzed via RL theory), where the outer loop is the interaction between LLM and human user and the inner loop is the interaction between LLM and external knowledge base such as KG. An illustrative figure can be found at https://imgbox.com/hYXybh91. \n\n(2) In the inner loop, the reasoner LLM first checks the contents in memory buffer (which we denote as information state) and generate a reasoning action consisting of both knowledge base (KB, instantiated as KG in ToG) query (correspond to the `search` step in ToG) and information filtering (correspond to the `pruning` step in ToG). The KB, serving as the environment, takes the query action and updates the memory buffer content (correspond to transition function in Markov decision process (MDP)). The memory buffer content is also updated deterministically via `pruning`. A judge (instantiated as LLM in ToG) then takes memory buffer content and evaluate its informativeness in terms of answering user question (correspond to the `reasoning` step in ToG), serving as the reward function in MDP. The (information, action, reward) tuple is updated to the memory buffer, and the accumulated memory buffer content serves as information state for the next iteration.\n\n(3) The goal of this MDP (RL problem) is to find a reasoning policy that maximizes its value function: $$V^\\pi_\\theta(s) = \\mathbb{E}\\bigl[\\sum_{t=0}^\\infty \\gamma^t r_\\theta(s_t, a_t) | s_0=s\\bigr]$$\nwhere $\\pi$ is the reasoning policy induced by LLM inference. Notably, LLM inference can be considered as performing implicit Bayesian inference [1], where LLM infers the hidden 'concept' contained in the prompts a posteriori, and then generates output based on the concept and prompts. Formally, this process can be written as:\n$$ p(y|x) = \\int_\\theta p(y|\\theta, x) p(\\theta|x) d\\theta,$$ where $\\theta$ parametrizes the knowledge environment (i.e. $\\theta$ is the underlying knowledge).\nFor our task of LLM reasoning, the `concept` corresponds to the underlying knowledge that can be inferred from knowledge prompts. In this sense, the induced policy $\\pi$ can be considered as Bayesian model-based planning (ToG belongs to this type as a special case), where the LLM reasoner first implicitly estimates the posterior distribution of knowledge ($\\theta$) relevant to user problem (based on prior obtained via pretraining and data obtained via KB interaction), and then uses $\\theta$ to implicitly parameterize its internal world model and performs model based planning. In this sense, the `pruning` step in ToG can be regarded as a tree search planning algorithm with 0 lookahead step.\n\n(4) Assuming no bias between the LLM prior ($p(\\theta)$) and the knowledge prior ($p(\\theta^*)$), then the gap between an LLM-induced reasoning policy $\\pi$ and an optimal reasoning policy, results only from the uncertainty in posterior estimation of the model parameter, $p(\\theta|s_t)$. Since the uncertainty gradually decreases as information accumulates, we can use Bayesian regret [3], R(T), to measure the asymptotic performance of the reasoning policy $\\pi$:\n$ \\mathcal{R}(T) = \\mathbb{E}\\_{\\theta \\sim p(\\theta)} \\bigl[\\sum_{t=1}^T V^{\\pi^*}\\_\\theta (s_t) - V^{\\pi^t}\\_\\theta (s_t) \\bigr]$\nUtilizing proofs in [2] we can guarantee that this regret for LLM x KG is sublinear to the number of reasoning steps $T$ (so $\\lim_{T \\rightarrow \\infty} \\frac{R(T)}{T} = 0$), indicating that the reasoning will eventually converge to the correct information/answer due to the continuous uncertainty reduction in posterior inference $p(\\theta|s_t)$ thanks to the iterative information gain from KB interaction.\n\n(5) The analysis of the outer user feedback loop can be conducted in a similar way, and the analysis suggests that the response precision can also improve with iterative user feedback, as demonstrated in Figure. 4 of this submission."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2328/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700466234404,
                "cdate": 1700466234404,
                "tmdate": 1700466424629,
                "mdate": 1700466424629,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tJneUM9TVa",
                "forum": "nnVO1PvbTv",
                "replyto": "dZ5muK4j3x",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2328/Reviewer_ezze"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2328/Reviewer_ezze"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for providing the responses that have addressed most of my concerns. I would suggest the authors add the extra analysis and discussion to the draft and improve the presentation as promised. I will retain my score and suggest an acceptance."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2328/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700710924308,
                "cdate": 1700710924308,
                "tmdate": 1700710924308,
                "mdate": 1700710924308,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1nkAf1hlOy",
            "forum": "nnVO1PvbTv",
            "replyto": "nnVO1PvbTv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2328/Reviewer_nifq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2328/Reviewer_nifq"
            ],
            "content": {
                "summary": {
                    "value": "This work aims at improving the integration of knowledge graphs (KGs) in LLMs. The authors propose a method called Think-on-Graph (ToG) that performs beam-search on knowledge graphs, keeping track and exploring reasoning paths. Specifically, they use a \"search\" operation backed by the KG and a \"prune\" operation backed by the LLM. Through the experiments on benchmarks of KBQA/open QA/etc, the authors find an advantage of ToG in several of them compared to prior work. They also perform analyses on the effect of individual components of ToG like the selection of KG, search depth, pruning method, etc."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper proposes an intuitive and novel method in LLM-KG integration. The experiments are performed on a variety of datasets. The performance of the method is overall positive. The analyses on each component of the method is extensive. The writing of the paper is overall clear."
                },
                "weaknesses": {
                    "value": "The reliance on very strong (production-level) LLMs and the choice of baselines. The authors explored the use of Llama-2-chat (70b), ChatGPT, and GPT-4 as the LLM in the ToG method. From Table 1 and Table 2, it seems that the strength of the LLM is essential to the method (Llama worse than ChatGPT, ChatGPT much worse than GPT-4). And the performance advantage on 6 out of 9 benchmarks is only observed with GPT-4. Additionally, though the authors show GPT-4 benefits from ToG compared to non-KG prompting (e.g., CoT), stronger prompting methods targeting for compositionality may be investigated, for example, self-ask [1]. Web search and vanilla retrieval augmentation methods can also be investigated [2].\n\nEfficiency of the method. The process of beam search and pruning with LLMs can be costly. An extensive comparison on decoding time and cost across all methods should be performed and discussed. \n\n[1] Press et al. 2022. Measuring and Narrowing the Compositionality Gap in Language Models. https://arxiv.org/abs/2210.03350\n[2] Kasai et al. 2022. RealTime QA: What's the Answer Right Now? https://arxiv.org/abs/2207.13332"
                },
                "questions": {
                    "value": "Please see the weaknesses section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2328/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2328/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2328/Reviewer_nifq"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2328/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699671885619,
            "cdate": 1699671885619,
            "tmdate": 1699671885619,
            "mdate": 1699671885619,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "NaeVWMebwI",
                "forum": "nnVO1PvbTv",
                "replyto": "1nkAf1hlOy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2328/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2328/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2328/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2328/Reviewers",
                    "ICLR.cc/2024/Conference/Submission2328/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission2328/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2328/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer nifq"
                    },
                    "comment": {
                        "value": "Thank you for your insightful comments and suggestions. We appreciate the opportunity to address your concerns and questions.\n\n**1.Reviewer's Comment**: \"The reliance on very strong (production-level) LLMs and the choice of baselines. The authors explored the use of Llama-2-chat (70b), ChatGPT, and GPT-4 as the LLM in the ToG method. From Table 1 and Table 2, it seems that the strength of the LLM is essential to the method (Llama worse than ChatGPT, ChatGPT much worse than GPT-4). And the performance advantage on 6 out of 9 benchmarks is only observed with GPT-4. \".\n \n**Reply**: We agree with the reviewer that the performance of ToG relies on the foundation LLM since the LLM plays an important role in the pruning and reasoning steps of ToG. However, we would emphasize the particular advantages of ToG:\n\n(1) ToG with small and poor foundation LLM has an opportunity to perform better than large and powerful LLM-only model, as shown in Table 2 of the manuscript. We can see that, on dataset CWQ, ToG-R with LLama2-70B-Chat reaches an accuracy 57.6%, higher than the accuracy of GPT-4 (Chain-of-thought prompting) 46.0%. Similarly, on dataset WebQSP, ToG-R with LLama2-70B-Chat reaches 68.9%, higher than GPT-4 (Chain-of-thought prompting) 67.3%. This result exhibits that ToG has a significant advantage in low cost deployment. Specifically, ToG with a cheaper and faster small LLM may be good enough in many scenarios, rather than deploying expensive and slow large LLM. \n\n(2) Although ToG relies on the foundation LLM, its performance also benefits from the contribution of knowledge graph, as well as the interaction of LLM and KG. We can see from Table 2 that, for any specific LLM, ToG always outperforms the corresponding LLM-only model. E.g., on dataset CWQ, ToG with ChatGPT reaches 58.9%, and ChatGPT-only (CoT) reaches 38.8%, so there is a 20.1% accuracy gap. We would claim that, ToG has the flexibility of plug-and-play any LLMs and KGs, and this algorithm usually improve the performance of the original LLM by incorporating the structural knowledge from KGs without adding any additional training cost (training-free).\n\n(3) Please allow us to emphasize that, all prior SOTA methods belong to training-based methods, which has a natural advantage in prediction accuracy on specific task or specific dataset compared with prompting-based methods (ToG, CoT etc.). However, ToG, a prompting-based method, outperforms those training-based prior SOTA methods on 6 out of 9 datasets. Both LLM and KG contribute to the performance of ToG (from Table 2, we see GPT-4 only is not as good as prior SOTAs without the help of KG). Moreover, we would also emphasize that, prompting-based methods have their own particular advantages, such as avoid of additional training cost and better explainability.  \n\n**2.Reviewer's Comment**: \"Additionally, though the authors show GPT-4 benefits from ToG compared to non-KG prompting (e.g., CoT), stronger prompting methods targeting for compositionality may be investigated, for example, self-ask [1]. Web search and vanilla retrieval augmentation methods can also be investigated [2].\"\n\n**Reply**: We thank the reviewer's suggestion for comparing ToG with additional prompting-based method such as self-ask and with search and retrieval augmentation methods. \n|Prompting Methods w. ChatGPT | Accuracy on WebQSP|\n|--- |---|\n|CoT |62.2|\n|Self-ask|50.8|\n|Retrievel Augmentation with Wikipage| 61.6|\n|ToG|76.2|\n|ToG-R|75.8|\nDue to limited time of rebuttal, we only finish the experiment of Self-ask and Retrieval Augmentation with Wikipage on WebQSP dataset. We notice that the accuracy of self-ask method is 50.8%, and the accuracy of Retrieval Augmentation with Wikipage is 61.6%. The above experimental results are based on GPT-3.5-turbo which we use OpenAI API to call.  We would like to specifically note that the GPT3.5 turbo was updated recently. The experimental results of self-ask and RAG w. Wikipage are based on the latest GPT3.5 turbo model, while the other experimental results are based on the previous version of the GPT3.5 turbo model. We would like to evaluate Self-ask and RAG with Wikipage on all datasets used in this paper and update Table 1 with these results in the later version.\n\n**3.Reviewer's Comment**: \"Efficiency of the method. The process of beam search and pruning with LLMs can be costly. An extensive comparison on decoding time and cost across all methods should be performed and discussed.\n\n**Reply**: Please refer to the official comment we submitted, named \"**Response to all reviewers about the concerns on the efficiency of ToG**\"."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2328/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700412969600,
                "cdate": 1700412969600,
                "tmdate": 1700473558101,
                "mdate": 1700473558101,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]