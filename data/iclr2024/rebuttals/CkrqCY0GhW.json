[
    {
        "title": "Language Model Agents Suffer from Compositional Decision Making"
    },
    {
        "review": {
            "id": "U4P33a8Iw4",
            "forum": "CkrqCY0GhW",
            "replyto": "CkrqCY0GhW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1264/Reviewer_qv9D"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1264/Reviewer_qv9D"
            ],
            "content": {
                "summary": {
                    "value": "The authors study the incompetence of LLMs in dealing with compositional decision making tasks, by proposing CompWoB, a new benchmark with 50 new compositional web automation tasks, training \nHTML-T5++, a new model, with balanced data distribution across tasks, and empirically comparing with existing methods, including RCI, AdaPlanner, and Synapse."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "originality\n\nThe authors study the incompetence of LLMs in dealing with compositional decision making tasks with a new benchmark and a new model with relatively comprehensive empirical study. It is novel.  \n\nquality\n\nThe paper is basically technically sound.\n\nclarity\n\nThe paper is basically well-organized and clearly written.\n\nsignificance\n\nLanguage model-based agent becomes a buzz word, without carefully studying the capability of the foundational language models. The authors study the incompetence of LLMs in dealing with compositional decision making tasks. The community should carefully think how to make progress in language model-based agent, e.g., as recommended by the authors in the Discussion section, improving generalizable prompting methods, agent-specialized large language models, and parsing complex instructions to executable plan."
                },
                "weaknesses": {
                    "value": "See questions below."
                },
                "questions": {
                    "value": "1.\nHTML-T5++ is an important contribution, which deserves a separate section, with more details of fine-tuning HTML-T5-XL, besides balancing data distribution.\n\n2.\nCan synthetic composing of web tasks represent realist ones? Are there ways to generate realist web tasks?\n\n3.\nShould web tasks be sequential decision making problems? That is, should there be dependencies between sub-web-tasks? Or simple composition of sub-tasks? How to achieve such dependancy?\n\nIf there is no dependancy among sub-tasks, why LLMs do not perform well on compositional tasks, which may be treated as multiple separated tasks? How to measure such dependancy?\n\n4.\nLLMs do not perform well at reverse-order instructions? Why? LLMs are widely regarded as being very competent with NLP tasks.\n\n5.\n\"Figure 5 visualizes the correlation between the success rate averaged across WebGUM, HTML-T5, RCI, AdaPlanner, and Synapse (y-axis) and each statistic of compositional tasks (x-axis)\"\n\nIs such average success rate a good way?\nAverage may hide something.\nShould we study each method individually, or the one with the best performance?\n\n6. Some minor issues below\n\n2 RELATED WORKS\nWeb Automation \n\"Although prior works have solved the problems with imitation learning and reinforcement learning ...\"\n\n6 RESULTS\n\u201cOtherwise mentioned, we adopt gpt-3.5-turbo as a backbone LLM.\u201d\nsomething wrong. how about \"We adopt gpt-3.5-turbo as a backbone LLM, unless mentioned otherwise.\" \n\nFigure 2\n\"and the dark color does in CompWoB\"\nSomething wrong. How about \"and the dark color for CompWoB\"\n\nFigure 3\nRedundant info from Figure 2 \n\"The light color represents the performance in CompWoB\"\nAnd the colors are different"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1264/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1264/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1264/Reviewer_qv9D"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1264/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697848725389,
            "cdate": 1697848725389,
            "tmdate": 1699636053173,
            "mdate": 1699636053173,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "97sNeqMBWh",
                "forum": "CkrqCY0GhW",
                "replyto": "U4P33a8Iw4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1264/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1264/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer eCh6"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the thoughtful review and comments. Please let us know any remaining questions or concerns if you have.\n\n**> Questions 1**\n\nThank you for your suggestion. We separated the details of finetuned language model agents, including data-rebalancing, as an independent section in the revised paper (Section 4.5).\n\n**> Questions 2**\n\nWhile tasks are visually simplified, we believe CombWoB reflects a broader spectrum of functionalities in real-world websites without sacrificing controllability. For example, the task in Figure 1 sketches the structure of a login form with agreement checkboxes and a pop-up window, like Internet banking. We also designed tasks with page transitions, such as from the login form to the email browser. Some of the recent works utilized unsupervised auto-curricula to adaptively design new tasks from a set of primitive base tasks using a teacher/student paradigm [1, 2]. While they can be useful for designing realistic websites, they still suffer from a limited set of base tasks and training instability in a multi-agent system.\n\n\n**> Questions 3**\n\nReal-world websites have dependencies between sub-tasks. For example, Gmail requires successful login to proceed to writing an email or reading social media posts of a specific user requires navigating to the personal page of that user. In CombWoB, we implemented these kinds of dependencies in the reward function using logical AND operations. For example, while we allowed agents to continue to \u201cwrite an email\u201d subtask without successful login, the agent would get zero rewards even if the \u201cwrite an email\u201d subtask is successful. We inform agents of these dependencies using connectors in instructions such as \u201csolve Y after X\u201d or \u201csolve X then Y\u201d. Our analysis in Section 6.4 also suggests that, in addition to task compositionality, long instructions and deep HTML sub-tree can lead to challenging tasks.\n\n\n**> Questions 4**\n\nThe failure analysis in Table 3  has shown that language model agents fail to parse the instructions into the correct order. This can be because base-task demonstrations in the prompt just have \"left-to-right\" instructions from base MiniWoB tasks; strongly conditioning the LLM agents to process instructions in a linear order while compositionality could imply a non-linear processing of instructions. This mismatch between the prompt (\"left-to-right\") and inference tasks (mixing \"left-to-right\" and \"right-to-left\") could cause the performance drops.\n\n\n**> Questions 5**\n\nFollowing the recent work in deep reinforcement learning literature [3], we measured average performance as a proxy of oracle task solvability. If many kinds of agents perform poorly, such tasks are regarded as challenging. This can shed light on \"task\" or \"environment\", rather than \"language model agents\" or \"prompting methods\", while such an analysis has been overlooked so far. Additionally, we agree with the reviewer that a single statistic, such as average over the methods, might not reflect the nuances for each method. While the trend with average might be the same, distributional characteristics for each language model agent could be different. We extended our analysis by reporting the individual performances of each agent in Appendix J (Figure 8). Our results still indicate that while all the language model agents (HTML-T5++, RCI, AdaPlanner, Synapse) often show negative correlations between the success rate and instruction tokens or max subtree depth with statistical significance, the trends for other statistics may differ among the methods.\n\n**> Questions 6**\n\nWe appreciate your pointing out the grammatical errors in our manuscript. We fixed these errors based on your suggestions in the revised paper.\n\n```\n[1] Gur et al., (2022) Environment Generation for Zero-Shot Compositional Reinforcement Learning (https://arxiv.org/abs/2201.08896)\n\n[2] Sohn et al., (2022) Fast Inference and Transfer of Compositional Task Structures for Few-shot Task Generalization (https://arxiv.org/abs/2205.12648)\n\n[3] Furuta et al., (2021) Policy Information Capacity: Information-Theoretic Measure for Task Complexity in Deep Reinforcement Learning (https://arxiv.org/abs/2103.12726).\n```"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1264/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700446870475,
                "cdate": 1700446870475,
                "tmdate": 1700446870475,
                "mdate": 1700446870475,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SgL9xrViGX",
                "forum": "CkrqCY0GhW",
                "replyto": "97sNeqMBWh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1264/Reviewer_qv9D"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1264/Reviewer_qv9D"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the reviews and authors' response. I will keep my score."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1264/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700698589763,
                "cdate": 1700698589763,
                "tmdate": 1700698589763,
                "mdate": 1700698589763,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "pGXHjmbSIg",
            "forum": "CkrqCY0GhW",
            "replyto": "CkrqCY0GhW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1264/Reviewer_MfTP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1264/Reviewer_MfTP"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new benchmark, called CompWoB \u2013 50 new **compositional** web automation tasks reflecting more realistic assumptions. The authors then evaluate different LLMs to show that LLM-based agents suffer from compositional decision making. Detailed observations include: 1) while prompted gpt-3.5-turbo or gpt-4 achieve 94.0% average success rate on base tasks, their performance degrades to 24.9% success rate on compositional tasks; 2) transferred LLM-based agents (finetuned only on base tasks) show less generalization gap, dropping from 85.4% to 54.8%; 3) balancing data distribution across tasks, a finetuned model, HTML-T5++, surpasses human-level performance (95.2%) on MiniWoB, and achieves the best zero-shot performance on CompWoB (61.0%).\n\n-----\nafter rebuttal, I increased the score to weak accept."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. A noval and original study about the compositional web automation task is proposed and many insights are provided.  \n\n2. Propose a data distribution balancing method across tasks and finetune a new model to surpass human-level performance on MiniWoB.  \n\n3. Clear writting. The reviewer can follow most of this paper easily."
                },
                "weaknesses": {
                    "value": "1. The reviewer did not get why Section 4 is needed (with such a large space), since most of the introductions are baseline methods.  Also, I did not know why RCI/AdaPlanner/Synapse are used for baselines.  \n\n2. Only test on 50 compositional web automation tasks. Are the methods and evaluations/insights generalizable to other tasks?  \n\n3. A lot of details are shown in the appendix (e.g., task difficulty estimation and data balancing method)."
                },
                "questions": {
                    "value": "1. why RCI/AdaPlanner/Synapse are used for baselines?  \n\n2. Are the methods and evaluations/insights generalizable to other tasks?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1264/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1264/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1264/Reviewer_MfTP"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1264/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698665290532,
            "cdate": 1698665290532,
            "tmdate": 1700620371009,
            "mdate": 1700620371009,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "WUXtdZmaXJ",
                "forum": "CkrqCY0GhW",
                "replyto": "pGXHjmbSIg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1264/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1264/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer MfTP"
                    },
                    "comment": {
                        "value": "We appreciate your careful reading and detailed discussion of our paper. We address your concerns below and please let us know if there are remaining questions or unclear points.\n\n**> Weaknesses 1 & Questions 1**\n\nWe selected baseline LLM agents in our work based on their superior performance and novelty in using LLMs for web navigation problems. RCI [1] is the first to use prompting in a self-refinement loop, outperforming SL/RL agents on MiniWoB benchmark that requires millions of demonstrations to work. AdaPlanner [2] and Synapse [3] were the follow-up works outperforming RCI via code generation from environmental feedback or via well-designed decomposed prompts with retrieval.\n \nSince we extensively studied these three reference models, we wanted to provide a detailed and to-the-point summary of these models so that it is easy to reason about their performance, shortcomings, and practicality. To improve readability, we shortened Section 4 by moving some of the content to the Appendix.\n\n\n\n**> Weaknesses 2 & Questions 2**\n\nMiniWoB has around 104 base tasks; all the two-way and three-way combinations of these tasks would give 185,460 compositional tasks. It is infeasible to manually curate all the two-way/three-way combinations. Unfortunately, it is also nontrivial to automate this process due to the locality of the environment implementations in MiniWoB. Each environment is mostly self-contained, making it nontrivial to modularize the whole benchmark/codebase so that every combination can be automatically generated.\nWe decided to outline a set of design principles \u2013 solvability, feasibility, reality, etc. which we follow to manually curate 50 compositional tasks and 50 reverse-order instruction tasks that we believe cover a wide variety of difficulty, and compositionality. We believe these guiding principles would be applicable to other compositional generalization problems such as robotic navigation. We also believe that based on these design principles, our compositional benchmark can easily be extended in the future to study even more challenging and compositional web navigation problems.\n\n\nWe'd also like to highlight that, because previous works were tested around 50 - 60 MiniWoB tasks [1,2,3,4,5], 50 compositional tasks is a decent number of tasks to evaluate the agents. Our addition of 50 new compositional tasks doubles the number of tasks for the community to study.\n\n\n\n\n**> Weaknesses 3**\n\nDue to limited space, we had to carefully structure our paper. We wanted to make the main part to be as self-contained as possible while also giving more details in the appendix for clarification and reproducibility. We made some changes to include some of the details of data balancing and task difficulty estimation as a new section in the revised paper (Section 4.5). We explain how difficulty scores are estimated (Appendix D), or the ratio of tasks in the rebalanced dataset (Appendix E) in the appendix. Please let us know if you have any other concerns.\n\n\n\n```\n[1] Kim et al., (2023) Language Models can Solve Computer Tasks (https://arxiv.org/abs/2303.17491)\n\n[2] Sun et al., (2023) AdaPlanner: Adaptive Planning from Feedback with Language Models (https://arxiv.org/abs/2305.16653) \n\n[3] Zheng et al., (2023) Synapse: Trajectory-as-Exemplar Prompting with Memory for Computer Control (https://arxiv.org/abs/2306.07863) \n\n[4] Gur et al., (2022) Understanding HTML with Large Language Models (https://arxiv.org/abs/2210.03945) \n\n[5] Furuta et al., (2023) Multimodal Web Navigation with Instruction-Finetuned Foundation Models (https://arxiv.org/abs/2305.11854) \n```"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1264/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700446731095,
                "cdate": 1700446731095,
                "tmdate": 1700446731095,
                "mdate": 1700446731095,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "u2OXPRgb7g",
                "forum": "CkrqCY0GhW",
                "replyto": "WUXtdZmaXJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1264/Reviewer_MfTP"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1264/Reviewer_MfTP"
                ],
                "content": {
                    "title": {
                        "value": "The response addresses most of my concerns"
                    },
                    "comment": {
                        "value": "I thank the authors for detailed responses. It clasifies the reasons why the baselines and evaluation tasks are used. It also add more details in the appendix. From the reviewer perspective, these responses are reasonable and address most of my concerns, and I would like to increase my rating score."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1264/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700620290813,
                "cdate": 1700620290813,
                "tmdate": 1700620290813,
                "mdate": 1700620290813,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "rB9hl83iuD",
            "forum": "CkrqCY0GhW",
            "replyto": "CkrqCY0GhW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1264/Reviewer_G9qb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1264/Reviewer_G9qb"
            ],
            "content": {
                "summary": {
                    "value": "This paper looks at the ability of LMAs to solve compositional web-tasks. A new dataset is introduced based on the existing Mini-WoB. Models are prompted with base tasks and then asked to solve tasks that are composed of different base tasks. Experiments show that performance drops across both LMAs and fine-tuned models."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The topic of compositionally in web tasks  is extremely important given how many papers have been released in the past year showing that GPT can be used for web tasks. \n- A new dataset is introduced which can show how well LMAs actually do given a combination of tasks without any prompting. One strong aspect of the benchmark is that consists of individual tasks that LLMs already know how to solve so it is clear that the difficulty is in combining tasks. \n- The paper is well written and has a thorough analysis about the different results. In particular, section 6.4 gives insight into what makes tasks more difficult, something not usually addressed."
                },
                "weaknesses": {
                    "value": "- For LMAs, there is no discussion on how the prompt could be modified for combining tasks. For example, if a prompt shows how to perform a joint task, is the performance any better?"
                },
                "questions": {
                    "value": "None (see above)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1264/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1264/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1264/Reviewer_G9qb"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1264/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698731527373,
            "cdate": 1698731527373,
            "tmdate": 1699636052988,
            "mdate": 1699636052988,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "bx8qFyDHVG",
                "forum": "CkrqCY0GhW",
                "replyto": "rB9hl83iuD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1264/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1264/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer G9qb"
                    },
                    "comment": {
                        "value": "We appreciate the careful reading and thoughtful comments. We address your concerns below, and please let us know if there are remaining questions or unclear points.\n\n\n**> Weaknesses 1**\n\nWe provided additional results with oracle demonstrations for compositional tasks in Appendix I (Figure 7). RCI with GPT-4 and oracle exemplars achieves 82.9% success rate averaged on 20 two-way tasks, which is the best among the methods (e.g. HTML-T5++: 73.9%, RCI with gpt-3.5-turbo: 46.9%, RCI with gpt-4: 71.5). This indicates that if a prompt includes how to perform on compositional tasks, the performance gets better. However, please keep in mind that providing exemplars for every compositional problem is infeasible given the huge space of problems and the cost of collecting examples for every one of them."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1264/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700446630970,
                "cdate": 1700446630970,
                "tmdate": 1700446630970,
                "mdate": 1700446630970,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LYOHHgyEsL",
                "forum": "CkrqCY0GhW",
                "replyto": "bx8qFyDHVG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1264/Reviewer_G9qb"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1264/Reviewer_G9qb"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your comments. Based on comments and other reviews, my initial score of 8 does not change."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1264/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700695102721,
                "cdate": 1700695102721,
                "tmdate": 1700695102721,
                "mdate": 1700695102721,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "f4OU5cQCxx",
            "forum": "CkrqCY0GhW",
            "replyto": "CkrqCY0GhW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1264/Reviewer_PK52"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1264/Reviewer_PK52"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a web automation agent model and test it on a proposed \"compositional\" benchmark. They show that standard general-purpose language model agents have their performance deteriorate more on their proposed benchmark than models fine-tuned on similar tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This is solid research that asks and answers a somewhat important question. It is thorough, with a reasonable set of agent techniques and a reasonable methodology for extending MiniWoB."
                },
                "weaknesses": {
                    "value": "The contribution is relatively minor (which, in my view, is fine - obviously, not every ICLR paper needs to be revolutionary). This is especially true because \"compositionality\" is inherently somewhat arbitrary: the tasks in MiniWoB are arguably already compositional since they require a series of steps performed in the right order. By the same reasoning, arguably, all language model hierarchical/long-range planning papers, not to mention several multimodal language model approaches designed to reason over images, are performing compositional tasks. I'd also point out that there are specific strategies that have been proposed specifically for compositional action (e.g., Parsel from Zelikman et al. 2022, which uses LMs to propose a high-level plan in language and implements each subpart independently).\n\nSome nitpicks: The title makes it sound like the model itself is harmed, but that doesn't really make sense. And, in conjunction with the earlier point about MiniWoB also being somewhat compositional, the title isn't necessarily backed up by the experiments. I think this could be easily partially fixed by simply adding \"for web automation\" to the title after LMA, and web automation is probably relevant enough that with this narrower scope, it's still fine. I expect I would lower my score if the authors don't commit to making this or some other disambiguating change."
                },
                "questions": {
                    "value": "See limitations"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1264/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698815483494,
            "cdate": 1698815483494,
            "tmdate": 1699636052903,
            "mdate": 1699636052903,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KWilg4QL4g",
                "forum": "CkrqCY0GhW",
                "replyto": "f4OU5cQCxx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1264/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1264/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer PK52"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the careful reading and constructive feedback. We address your concerns below and please let us know if you have further questions.\n\n**> Weaknesses 2**\n\nThank you for your suggestions. We decided to change the title to `Language Model Agents Suffer from Compositional Generalization in Web Automation` in the final version. Since we intentionally distinguish the terminology of \"language model agents\" (prompting & pipeline) and \"large language models\" (model itself), we do not intend to argue \"the model itself is harmed\" in the title; in fact, we have compared different agent-prompting methods (RCI, AdaPlanner, Synapse) on the same LLMs (gpt-3.5-turbo/gpt-4). Please let us know if you have further concerns about the title.\n\n**> Weaknesses 1**\n\nWe agree with the reviewer that many of the real-world tasks have inherent compositionality to some degree, including MiniWoB tasks. However, these tasks are not explicitly designed for compositionality, making it difficult to systematically investigate the generalization gap. For example, existing language model agents, that we also study in our paper, already achieve human-level performance on MiniWoB \u2013 more than 90% performance on almost all the tasks in MiniWoB. However, when these tasks are combined, their performance drops significantly \u2013 allowing us to analyze this gap using the difficulty of the base tasks themselves as well as the difficulty of the task compositions. Given that these agents already solve base tasks, we found it surprising that even the most capable GPT-4-based agents still struggle with task compositions while a fine-tuned T5-based model transfers better. We also added Parsel to related work (Section 2) in the revised paper."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1264/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700446579439,
                "cdate": 1700446579439,
                "tmdate": 1700446579439,
                "mdate": 1700446579439,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "sL0EScCwHq",
                "forum": "CkrqCY0GhW",
                "replyto": "KWilg4QL4g",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1264/Reviewer_PK52"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1264/Reviewer_PK52"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response! I will keep my score as is."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1264/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700665979738,
                "cdate": 1700665979738,
                "tmdate": 1700665979738,
                "mdate": 1700665979738,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]