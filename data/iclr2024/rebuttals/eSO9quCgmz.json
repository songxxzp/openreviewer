[
    {
        "title": "Rethinking pseudo-labeling: Data-centric insights improve semi-supervised learning"
    },
    {
        "review": {
            "id": "YYFztG6r00",
            "forum": "eSO9quCgmz",
            "replyto": "eSO9quCgmz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3725/Reviewer_K9EF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3725/Reviewer_K9EF"
            ],
            "content": {
                "summary": {
                    "value": "This paper rethinks Semi-Supervised Learning from a data-centric view, that is, the labeled data may be not reliable and may contain noise in real-world applications. In this case, previous semi-supervised learning methods, which heavily rely on labeled data, are no longer applicable, showing low data efficiency. The author proposes a method for selecting high-quality data based on the average confidence and aleatoric uncertainty of historical predictions during the training process. Experimental results demonstrate that this method can identify reliable data, improve the data efficiency of semi-supervised learning, and adapt to different pseudo-labeling algorithms."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The proposal is simple and technologically reasonable.\n2. The experiments seem comprehensive, and the author has analyzed the effectiveness and practicality of the proposed method from multiple perspectives. From the results, the proposed method has consistently achieved performance improvements.\n3. Overall, this paper presents an interesting problem and provides a simple and effective solution. This could have a positive impact on the practical application of SSL in the real world."
                },
                "weaknesses": {
                    "value": "1. This paper focuses more on tabular data, which is different from the image data that the main SSL algorithms currently focus on. The experiments conducted in this article regarding images are still limited. \n2. This paper improves the robustness of SSL against noisy labeled data through a simple data selection method. However, further analysis is not provided regarding the reasons for the success of this data selection. Is there a theoretical connection between the statistical features of historical predictions (average confidence and aleatoric uncertainty in this paper) and the improvement in pseudo-label quality in SSL? Under what conditions does this method work effectively for the overall quality of labeled data, such as the proportion of label noise? More in-depth analysis can further improve this paper."
                },
                "questions": {
                    "value": "Q: Can the author provide more discussion and analysis to demonstrate the conditions under which this proposal is successful?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3725/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3725/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3725/Reviewer_K9EF"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3725/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698508363923,
            "cdate": 1698508363923,
            "tmdate": 1699636328800,
            "mdate": 1699636328800,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xXzNprYSkS",
                "forum": "eSO9quCgmz",
                "replyto": "YYFztG6r00",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3725/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3725/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer K9EF"
                    },
                    "comment": {
                        "value": "Dear Reviewer K9EF\n\nThank you for your thoughtful comments and suggestions! We give answers to each of the following points in turn and highlight the updates to the revised manuscript. In addition, we have uploaded the revised manuscript. We hope this response alleviates your concerns, but please let us know if there are any remaining concerns.\n\n- A) Additional experiments in computer vision\n- B) Analysis of conditions of success\n\n---\n\n### (A) Additional experiments in computer vision\nWe thank the reviewer for this point on applying DIPS to images datasets. To address this, we have conducted experiments on $4$ additional image datasets : satellite images from Eurosat [R1] and medical images from TissueMNIST which form part of the USB benchmark [R6]. Additionally, we include the related OrganAMNIST, and PathMNIST, which are part of the MedMNIST collection [R2].\nGiven that these datasets are well-curated, we consider a proportion of $0.2$ of symmetric label noise added to these datasets.\n\nWe report the results in the following figure available at: **https://i.imgur.com/NYmYGHs.png**\n\n\nAs is shown, DIPS consistently improves the FixMatch baseline, demonstrating its generalizability beyond tabular datasets.\n\nAdditionally, we wish to reiterate that beyond CIFAR-10N, we have also conducted additional experiments on CIFAR-100N in the Appendix.\n\n**UPDATE:** we have included these additional results in Appendix C.4.4 in our revised manuscript.\n\n---\n\n### (B) Analysis of conditions of success\nWe thank the reviewer for the question on conditions of success of DIPS. To answer it, we conduct a synthetic experiment following a similar setup as in Sec. 5.1 in our manuscript. Our setup considers the dependencies between the amount of label noise and the amount of labeled data. Note that the experiment is synthetic in order to be able to control the amount of label noise.\nWe considered the same list of label noise proportions as in Sec. 5.1, ranging from 0. to 0.45. For each label noise proportion, we consider $n_{\\mathrm{lab}} \\in$ {50,100,1000}, and fix $n_{\\mathrm{unlab}} = 1000$.\nFor each configuration, we conduct the experiment $40$ times. \nWe report the results in the following plots available at: **https://i.imgur.com/rzFeFsl.png**\n\nAs we can see on the plots, PL+DIPS consistently outperforms the supervised baselines and PL in almost all the configurations. The performance gap between DIPS and PL is remarkably noticeable for $n_{\\mathrm{lab}}=1000$, that is, when the amount of labeled samples is high, which hints at the fact that curation is made easier with more samples, and hence leads to improved performance.\nWhen the amount of labeled data is very low ($n_{lab} = 50$) and the proportion of corrupted samples is high ($0.45$), we can see that the different baselines have closer accuracies. We note, though, that DIPS consistently improves the PL baseline for reasonable amounts of label noise which we could expect in real-world settings (e.g. 0.1).\n\n**UPDATE:** We have included these results in Appendix C.5 of the revised manuscript.\n\n\nThe reviewer also asks about any \"theoretical connection\" involving \"statistical features of historical predictions\". Some previous works in the *supervised learning setting* have shown that learning dynamics emit a strong signal about the nature of samples ([R3],[R4]). For example, Theorem 3 in [R5] (Appendix A.4 in the corresponding paper) gives some intuition about mislabeled samples in the supervised setting, by stating that the probability that mislabeled examples are classified with their given (incorrect) labels tends to $0$ in a mixture setting, as the model is trained until convergence. As such, this phenomenon directly impacts the agreement between the predictions of the model and the given label of a mislabeled sample, the agreement being worse as the model converges. This agreement is directly the quantity that we capture with the learning dynamics (confidence and aleatoric uncertainty) and DIPS' selection mechanism $s$. \n\n\nWe hope this answered your points, please let us know if there are any remaining concerns."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3725/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700129895764,
                "cdate": 1700129895764,
                "tmdate": 1700131159494,
                "mdate": 1700131159494,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DX5HxP6fuA",
                "forum": "eSO9quCgmz",
                "replyto": "YYFztG6r00",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3725/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3725/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer K9EF - References"
                    },
                    "comment": {
                        "value": "### References\n\n[R1] Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 2019.\n\n[R2] Jiancheng Yang, Rui Shi, and Bingbing Ni. Medmnist classification decathlon: A lightweight automl benchmark for medical image analysis. In IEEE 18th International Symposium on Biomedical Imaging (ISBI), pages 191\u2013195, 2021.\n\n[R3] Niladri S Chatterji and Philip M Long. Finite-sample analysis of interpolating linear classifiers in the overparameterized regime. J. Mach. Learn. Res., 22:129\u20131, 2021.\n\n[R4] Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The implicit bias of gradient descent on separable data. The Journal of Machine Learning Research, 19(1):2822\u20132878, 2018.\n\n[R5] Maini, Pratyush, Saurabh Garg, Zachary Lipton, and J. Zico Kolter. \"Characterizing datapoints via second-split forgetting.\" Advances in Neural Information Processing Systems 35 (2022): 30044-30057.\n\n[R6] Wang, Yidong, et al. \"Usb: A unified semi-supervised learning benchmark for classification.\" Advances in Neural Information Processing Systems 35 (2022): 3938-3961."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3725/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700129923966,
                "cdate": 1700129923966,
                "tmdate": 1700129923966,
                "mdate": 1700129923966,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tEOIE0pfNa",
                "forum": "eSO9quCgmz",
                "replyto": "YYFztG6r00",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3725/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3725/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Dear Reviewer K9EF"
                    },
                    "comment": {
                        "value": "Dear Reviewer K9EF  \n\nWe are sincerely grateful for your time and energy in the review process.We hope that our responses and appendix/manuscript updates have been helpful. Please let us know of any leftover concerns and if there was anything else we could do to address any further questions or comments!\n\nThank you!  \nPaper 3725 Authors"
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3725/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700470663069,
                "cdate": 1700470663069,
                "tmdate": 1700470663069,
                "mdate": 1700470663069,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zdG63SHRpo",
                "forum": "eSO9quCgmz",
                "replyto": "tEOIE0pfNa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3725/Reviewer_K9EF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3725/Reviewer_K9EF"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your detailed response, which addressed my concerns. I will keep my score."
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3725/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700490268106,
                "cdate": 1700490268106,
                "tmdate": 1700490268106,
                "mdate": 1700490268106,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BoOisI2EqQ",
                "forum": "eSO9quCgmz",
                "replyto": "YYFztG6r00",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3725/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3725/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your response"
                    },
                    "comment": {
                        "value": "Thank you for your response, we are glad our additional experiments and analysis addressed your concerns. We would like to thank you for the encouragement to add further image datasets, which has improved the paper and more thoroughly illustrates the impact of DIPS and data quality across diverse settings --- which is now more extensive than other papers in this area (``see below Table``).\n\nPlease let us know if there are any remaining points that we could address that would lead you to increase your score. We are eager to do our utmost to address them!\n\n--\n| Method              | Data Modalities and Number of Datasets                                 |\n|---------------------|----------------------------------------------------------------------|\n| __DIPS (Ours)__ | __Synthetic: 1, Tabular: 12, Images: 6__ |\n| Greedy PL [R4]          | Images: 1                  |\n| UPS     [R5]               | Images: 4                  | \n| FlexMatch   [R6]       |  Images: 5                  |      \n|      SLA   [R7]                                 | Images: 3                  |\n| CSA [R8]      |  Tabular: 12, Images: 1  | \n| FixMatch  [R9] |  Images: 5  | \n|FreeMatch [R10] |  Images: 5  |\n\n---\n\n### References\n\n[R4] Dong-Hyun Lee et al. Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks. In Workshop on Challenges in Representation Learning, ICML, 2013.\n\n[R5] Mamshad Nayeem Rizve, Kevin Duarte, Yogesh S Rawat, and Mubarak Shah. In defense of pseudo-labeling: An uncertainty-aware pseudo-label selection framework for semi-supervised learning. In International Conference on Learning Representations, 2021.\n\n[R6] Bowen Zhang, Yidong Wang, Wenxin Hou, Hao Wu, Jindong Wang, Manabu Okumura, and Takahiro Shinozaki. FlexMatch: Boosting semi-supervised learning with curriculum pseudo labeling. Advances in Neural Information Processing Systems, 34, 2021\n\n[R7] Kai Sheng Tai, Peter D Bailis, and Gregory Valiant. Sinkhorn label allocation: Semi-supervised classification via annealed self-training. In International Conference on Machine Learning, pp. 10065\u201310075. PMLR, 2021.\n\n[R8] Vu-Linh Nguyen, Sachin Sudhakar Farfade, and Anton van den Hengel. Confident Sinkhorn allocation\nfor pseudo-labeling. arXiv preprint arXiv:2206.05880, 2022a\n\n[R9]Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao Zhang, Han Zhang, Colin A Raffel, Ekin Dogus Cubuk, Alexey Kurakin, and Chun-Liang Li. FixMatch: Simplifying semi-supervised learning with consistency and confidence. Advances in Neural Information Processing Systems, 33, 2020.\n\n[R10]Yidong Wang, Hao Chen, Qiang Heng, Wenxin Hou, Yue Fan, Zhen Wu, Jindong Wang, Marios Savvides, Takahiro Shinozaki, Bhiksha Raj, Bernt Schiele, and Xing Xie. Freematch: Self-adaptive thresholding for semi-supervised learning. In The Eleventh International Conference on Learning\nRepresentations, 2023"
                    }
                },
                "number": 24,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3725/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700510755111,
                "cdate": 1700510755111,
                "tmdate": 1700510854355,
                "mdate": 1700510854355,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "xXanVMRLn6",
            "forum": "eSO9quCgmz",
            "replyto": "eSO9quCgmz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3725/Reviewer_hGhE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3725/Reviewer_hGhE"
            ],
            "content": {
                "summary": {
                    "value": "The paper addresses the challenge of dealing with inaccurately labeled data in semi-supervised learning contexts, where the small amount of labeled data available may contain errors. Contrary to the common presumption that labeled data is error-free, real-world scenarios often present labeling inaccuracies due to factors like mislabeling or ambiguity. To mitigate this problem, the authors introduce a method called DIPS, which is designed to discern and select the most reliable labeled data and pseudo-labels for unlabeled data throughout the training process."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper is grounded in a well-justified research gap, addressing the often overlooked errors in labeled data from prior studies.\n2. The method proposed is both straightforward and impactful, demonstrating its efficacy across multiple tabular datasets as well as select small-scale computer vision datasets.\n3. The authors have furnished extensive experimental details to facilitate reproduction."
                },
                "weaknesses": {
                    "value": "1. While the authors assert that DIPS is intended to be a versatile tool that can be seamlessly merged with current pseudo-labeling strategies, the experiments are mainly limited to tabular datasets. It would be beneficial to extend testing to the USB[1] benchmark, encompassing datasets from computer vision, natural language processing, and speech to better demonstrate DIPS's generalizability. I will reconsider my score if more experiments are conducted.\n\n2. The sections on Notation and Methodology are challenging to interpret. Transferring the pseudo-code to the Methodology section could enhance clarity and comprehension.\n\n[1] Wang, Yidong, et al. \"Usb: A unified semi-supervised learning benchmark for classification.\" Advances in Neural Information Processing Systems 35 (2022): 3938-3961."
                },
                "questions": {
                    "value": "See weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3725/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3725/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3725/Reviewer_hGhE"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3725/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698734060652,
            "cdate": 1698734060652,
            "tmdate": 1699636328727,
            "mdate": 1699636328727,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ywJmnCHRDx",
                "forum": "eSO9quCgmz",
                "replyto": "xXanVMRLn6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3725/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3725/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer hGhE"
                    },
                    "comment": {
                        "value": "Dear Reviewer hGhE\n\nThank you for your thoughtful comments and suggestions! We give answers to each of the following points in turn and highlight the updates to the revised manuscript. In addition, we have uploaded the revised manuscript. We hope this response alleviates your concerns, but please let us know if there are any remaining concerns.\n\n- A) Additional experiments with the USB benchmark\n- B) Placement of pseudo-code\n\n---\n\n### (A) Additional experiments with the USB benchmark \nWe thank the reviewer for this good suggestion to include datasets from the suggested USB benchmark, which, we believe, will strengthen our submission. To address this point, we have conducted experiments on $4$ additional datasets: satellite images from Eurosat [R1] and medical images from TissueMNIST which form part of the USB benchmark [R3]. Additionally, we include the related OrganAMNIST, and PathMNIST, which are part of the MedMNIST collection [R2].\nGiven that these datasets are well-curated, we consider a proportion of $0.2$ of symmetric label noise added to these datasets.\n\nWe report the results in the figure available at **https://i.imgur.com/NYmYGHs.png**\n\n\nAs is shown, DIPS consistently improves the FixMatch baseline, demonstrating its generalizability beyond tabular datasets.\n\nAdditionally, we wish to reiterate that beyond CIFAR-10N, we have also conducted additional experiments on CIFAR-100N in Appendix C.4.1.\n\n**UPDATE:** we have included these additional results in Appendix C.4.4 in our revised manuscript.\n\n---\n\n### (B) Placement of pseudo-code\nThank you for your suggestion to improve the presentation of our work. We agree with you and have moved the pseudo-code presented in Algorithm 1 to Section 4.4 in the updated manuscript.\n\n**UPDATE:** Algorithm 1 moved to Section 4.4 in the revised manuscript.\n\nWe hope this answers your points, please let us know if there are any remaining concerns.\n\n---\n\n### References \n\n[R1] Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 2019.\n\n[R2] Jiancheng Yang, Rui Shi, and Bingbing Ni. Medmnist classification decathlon: A lightweight automl benchmark for medical image analysis. In IEEE 18th International Symposium on Biomedical Imaging (ISBI), pages 191\u2013195, 2021.\n\n[R3]  Wang, Yidong, et al. \u201cUsb: A unified semi-supervised learning benchmark for classification.\u201d Advances in Neural Information Processing Systems 35 (2022): 3938-3961."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3725/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700130021414,
                "cdate": 1700130021414,
                "tmdate": 1700130249115,
                "mdate": 1700130249115,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CQpoqH1b7I",
                "forum": "eSO9quCgmz",
                "replyto": "xXanVMRLn6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3725/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3725/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Dear Reviewer hGhE"
                    },
                    "comment": {
                        "value": "Dear Reviewer hGhE  \n\nWe are sincerely grateful for your time and energy in the review process.We hope that our responses and appendix/manuscript updates have been helpful. Please let us know of any leftover concerns and if there was anything else we could do to address any further questions or comments!\n\nThank you!  \nPaper 3725 Authors"
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3725/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700470631882,
                "cdate": 1700470631882,
                "tmdate": 1700470631882,
                "mdate": 1700470631882,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aea6SDFe6u",
                "forum": "eSO9quCgmz",
                "replyto": "CQpoqH1b7I",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3725/Reviewer_hGhE"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3725/Reviewer_hGhE"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response. I appreciate the provided experiments, however, I believe further experiments, particularly with semi-aves and STL-10 datasets, would significantly strengthen the evaluation of DIPS. Additionally, my query regarding the performance of DIPS on natural language and speech datasets remains unaddressed. Understanding its effectiveness in these domains is crucial for a comprehensive assessment. Due to these limitations in the scope of your experiments, I am inclined to maintain my original evaluation score."
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3725/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700474919327,
                "cdate": 1700474919327,
                "tmdate": 1700474919327,
                "mdate": 1700474919327,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FhJjoJjWe9",
                "forum": "eSO9quCgmz",
                "replyto": "xXanVMRLn6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3725/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3725/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your response"
                    },
                    "comment": {
                        "value": "Dear Reviewer hGhE,\n\nThank you once again for your insightful feedback and for acknowledging the new experiments we have added to our manuscript. We would like to take this opportunity to make the purpose of our paper clearer, which is as follows:\n\n- We identify the issue of data quality and mislabeling in the original labeled dataset and highlight the importance of this for pseudo-labeling.\n- We propose a general framework to incorporate this insight within the pseudo-labeling paradigm and introduce an approach, DIPS, that evaluates the quality of the labeling using model confidence and aleatoric uncertainty.\n- We demonstrate the application and impact of this approach in both the tabular and image modalities.\n\nYour review has been very helpful for us in disentangling these three points and we have clarified them as a result in the manuscript. \n\nIn short, we don\u2019t use experiments to demonstrate the universal superiority of our proposed approach, DIPS (point 2). Rather we seek to show the usefulness and applicability of our key insight (point 1 above) by applying DIPS to real-world cases (point 3 above), and in particular those where manual data audit might be particularly hard (e.g. tabular). We believe others will find this approach useful too and they will be able to use it and develop new approaches in other modalities, such as those you suggest.\n\nSo, our paper identifies a fundamental issue that has been overlooked previously in semi-supervised learning, introduces a new method to tackle this limitation, and then shows its usefulness with some real-world examples.\n\nWe hope this clarifies the goal of our paper and we have **updated the paper** (updates in purple) to reflect the clarity that your review has helped us to reach."
                    }
                },
                "number": 25,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3725/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700515538452,
                "cdate": 1700515538452,
                "tmdate": 1700520599832,
                "mdate": 1700520599832,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VZO8EccUux",
                "forum": "eSO9quCgmz",
                "replyto": "xXanVMRLn6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3725/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3725/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Re:our experiments"
                    },
                    "comment": {
                        "value": "Regarding our evaluation, we believe that the scope of our experiments is substantially broad to achieve our stated aims in the previous comment. We have demonstrated the impact and pervasiveness of the identified issue across multiple modalities and 19 datasets. This is substantially higher than the median of 4 datasets (mean: 4.1) employed to evaluate methods published at leading machine learning conferences (Liao et al. [R11]).\n\nWe wish to draw the Reviewer's attention to the validation of other approaches. We checked the modalities and number of datasets employed by the baseline approaches used in our manuscript, which we have included in the table below. We confirmed our experiments were more extensive than any other approach, both in terms of the number of datasets and modalities. CSA is the only other approach assessed on non-image data, and no approach conducted experiments on NLP or speech data. \n\nWe agree that NLP and speech are interesting domains for future research building on the formalism and problem introduced by DIPS, and we have updated Section 5 of our manuscript accordingly. However, as noted above, they are not the central focus of our work, which was to demonstrate that explicitly accounting for labeled data quality is an aspect that helps pseudo-labeling. \n\nWe trust that this, together with clarifying changes included in our revised manuscript, allays any concerns the Reviewer had regarding the thoroughness of our evaluation of DIPS.\n\n| Method              | Data Modalities and Number of Datasets                                 |\n|---------------------|----------------------------------------------------------------------|\n| Greedy PL [R4]          | Images: 1                  |\n| UPS     [R5]               | Images: 4                  |\n| FlexMatch   [R6]       |  Images: 5                  |        \n|      SLA   [R7]                                 | Images: 3       | \n| CSA [R8]      |  Tabular: 12, Images: 1  | \n| FixMatch  [R9] |  Images: 5  | \n|FreeMatch [R10] |  Images: 5 |\n| __DIPS (Ours)__ | __Synthetic: 1, Tabular: 12, Images: 6__ |\n\n\n### References\n[R4] Dong-Hyun Lee et al. Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks. In Workshop on Challenges in Representation Learning, ICML, 2013.\n\n[R5] Mamshad Nayeem Rizve, Kevin Duarte, Yogesh S Rawat, and Mubarak Shah. In defense of pseudo-labeling: An uncertainty-aware pseudo-label selection framework for semi-supervised learning. In International Conference on Learning Representations, 2021.\n\n[R6] Bowen Zhang, Yidong Wang, Wenxin Hou, Hao Wu, Jindong Wang, Manabu Okumura, and Takahiro Shinozaki. FlexMatch: Boosting semi-supervised learning with curriculum pseudo labeling. Advances in Neural Information Processing Systems, 34, 2021\n\n[R7] Kai Sheng Tai, Peter D Bailis, and Gregory Valiant. Sinkhorn label allocation: Semi-supervised classification via annealed self-training. In International Conference on Machine Learning, pp. 10065\u201310075. PMLR, 2021.\n\n[R8] Vu-Linh Nguyen, Sachin Sudhakar Farfade, and Anton van den Hengel. Confident Sinkhorn allocation\nfor pseudo-labeling. arXiv preprint arXiv:2206.05880, 2022a\n\n[R9]Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao Zhang, Han Zhang, Colin A Raffel, Ekin Dogus Cubuk, Alexey Kurakin, and Chun-Liang Li. FixMatch: Simplifying semi-supervised learning with consistency and confidence. Advances in Neural Information Processing Systems, 33, 2020.\n\n[R10]Yidong Wang, Hao Chen, Qiang Heng, Wenxin Hou, Yue Fan, Zhen Wu, Jindong Wang, Marios Savvides, Takahiro Shinozaki, Bhiksha Raj, Bernt Schiele, and Xing Xie. Freematch: Self-adaptive thresholding for semi-supervised learning. In The Eleventh International Conference on Learning\nRepresentations, 2023\n\n[R11] Thomas Liao, Rohan Taori, Deborah Raji, and Ludwig Schmidt. Are We Learning Yet? A Meta Review of Evaluation Failures Across Machine Learning. NeurIPS Track on Datasets and Benchmarks, 2021"
                    }
                },
                "number": 26,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3725/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700515562000,
                "cdate": 1700515562000,
                "tmdate": 1700520872183,
                "mdate": 1700520872183,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "7zICNMdrdW",
            "forum": "eSO9quCgmz",
            "replyto": "eSO9quCgmz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3725/Reviewer_hxp2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3725/Reviewer_hxp2"
            ],
            "content": {
                "summary": {
                    "value": "This paper explores an often overlooked scenario in pseudo-labels (PLs) in semi-supervised learning (SSL), where the labeled data used for training is considered perfect. This paper breaks this assumption and shows that noise in the initial labeled set can be propagated to the pseudo-labels and hurt the final performance. To tackle this issue, a Data-centric Insights for semi-supervised learning (DIPS) framework is proposed. It selectively uses useful training samples among labeled and pseudo-labeled examples for every self-training iterations based on the learning dynamics of each example. DIPs has three properties which can be practical in real use and experiments are conducted on various real world datasets across different modalities."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The motivation for considering the quality of labeled data is clearly presented, connected to real use cases. In addition, the pilot experiment in Figure 3 demonstrates that addressing the inherent noise in labeled data is necessary and that previous (standard) pseudo-labeling algorithms can fail on this setting. \n\n- Writing quality needs to be acknowledged. All sections are well organized, and easy to follow. Especially for the experiments part, dividing a section into several paragraphs and giving a short summary of the results was a good idea. In addition a supplementary material covers a lot of details including additional ablation experiments. \n\n- Although a simple data filtering method, the effectiveness of the proposed DIPS was impressive, since it achieves consistent improvements on various real-world cases."
                },
                "weaknesses": {
                    "value": "- It seems that this paper is not the first to concern the inherent label noise in the labeled data and take the data-centric approach. It is necessary to discuss and compare with [L. Schmarje et al.,2022] to more clarify the conceptual novelty.\n\n- Connection to the active learning literature is missing. Selection metric for the \u2018useful\u2019 data samples among the unlabeled data pool is of central interest in active learning. The term \u2018usefulness\u2019 can include various criterions, such as confidence and uncertainty (as in this work), coverage, diversity, and etc. In that sense, more comprehensive discussions and comparisons on selection metrics are expected. Related to this issue, how the quality and diversity (e.g., class distributions) of the selected training samples change for every generation?\n\n- Choice of the threshold parameters $\\tau_{\\text{conf}}$ and $\\tau_{\\text{al}}$ seems to be very important. For example, highly tight thresholds (i.e., high $\\tau_{\\text{conf}}$ and low $\\tau_{\\text{al}}$) will remain only a few samples for training likely to be correct and abundant samples yet include noise for the vice versa. As the proposed algorithm is not designed to make a correction on the mislabeled samples but filter potentially harmful examples, exploring such trade-offs between remaining data proportion and performance depending on the thresholds will be valuable. \n\n- Considering the computational aspects, it could be nearly free when the scale of unlabeled data is relatively small (i.e., in vision domain, CIFAR-10/100). In other words, when the scale of the unlabeled dataset grows large (i.e., million-scale samples such as ImageNet), computational overheads caused by evaluating labeled and pseudo-labeled examples with all checkpoints in previous rounds cannot be ignored.\n\n- Although the presented work mainly targets semi-supervised learning on tabular data, baselines have been taken only from the image domain and comparison with SSL methods specific to tabular data such as [J. Yoon et al., 2020] is not given."
                },
                "questions": {
                    "value": "- The proposed DIPS framework follows the iteration-based self-training scheme, while a typical pseudo-label-based SSL algorithm such as FixMatch doesn\u2019t. FixMatch takes both labeled and unlabeled data and learns from both supervision signals (i.e., labels and online pseudo-labels). But the self-training scheme makes offline pseudo-labels after an iteration phase and some selected pseudo-labeled examples considered labeled data in the self-training next iteration. It seems to be a conflict between two different mechanisms. Hence, an illustrative example of how the non-iteration-based SSL algorithms can be incorporated into the DIPS framework is expected.  \n\n---\n\nReferences\n\n[L. Schmarje et al.,2022] A data-centric approach for improving ambiguous labels with combined semi-supervised classification and clustering, in ECCV 2022. \n\n[J. Yoon et al., 2020] VIME: Extending the Success of Self- and Semi-supervised Learning to Tabular Domain, in NeurIPS 2020."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3725/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3725/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3725/Reviewer_hxp2"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3725/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698754110360,
            "cdate": 1698754110360,
            "tmdate": 1699636328604,
            "mdate": 1699636328604,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "87bKTYPmlj",
                "forum": "eSO9quCgmz",
                "replyto": "7zICNMdrdW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3725/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3725/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer hxp2 [Part 1/3]"
                    },
                    "comment": {
                        "value": "Dear Reviewer hxp2\n\nThank you for your thoughtful comments and suggestions! \n\nWe give answers to each of the following points in turn and highlight the updates to the revised manuscript. In addition, we have uploaded the revised manuscript. We hope this response alleviates your concerns, but please let us know if there are any remaining concerns.\n\n- A) Contrasting DIPS w/ Schmarje et al.,2022 **[Part 2/3]**\n- B) Connection to active learning **[Part 2/3]**\n- C) Threshold parameters & trade-offs **[Part 2/3]**\n- D) Computational trade-offs **[Part 3 /3]**\n- E) VIME baseline **[Part 3/3]**\n- F) Incorporating DIPS w/ Fixmatch **[Part 3/3]**"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3725/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700130100511,
                "cdate": 1700130100511,
                "tmdate": 1700130180118,
                "mdate": 1700130180118,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2kfZDppPB1",
                "forum": "eSO9quCgmz",
                "replyto": "7zICNMdrdW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3725/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3725/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer hxp2 [Part 2/3]"
                    },
                    "comment": {
                        "value": "### (A) Contrasting DIPS w/ Schmarje et al.,2022 [DC3]\n\n\nThank you for highlighting the paper [L. Schmarje et al.,2022]. While both DIPS and DC3 handle the data-centric issue of issues in data and share similarities in their titles, they tackle different data-centric problems which might arise in semi-supervised learning. The main differences are along 4 different dimensions.\n\n(i) **Problem setup/Type of data-centric issue**: DIPS tackles the problems of hard noisy labels where each sample has a single label assigned in the labeled set which could be incorrect. In contrast, DC3 deals with the problem of soft labeling where each sample might have multiple annotations from different annotators which may be variable.\n\n(ii) **Label noise modeling**: DIPS aims to identify the noisy labels, whereas DC3 models the inter-annotator variability to estimate label ambiguity.\n\n(iii) **Integration into SSL**: DIPS is a plug-in on top of any pseudo-labeling pipeline, selecting the labeled and pseudo-labeled data. DC3 on the other hand uses its ambiguity model (learned on the multiple annotations) to either keep the pseudo-label or use a cluster assignment.\n\n(iv) **Dataset applicability**: DIPS has lower dataset requirements as it can be applied to any dataset with labeled and unlabeled samples, even if there is only a single label per sample. It does not require multiple annotations. DC3 has higher dataset requirements as it relies on having multiple annotations per sample to estimate inter-annotator variability and label ambiguity. Without multiple labels per sample, it cannot estimate ambiguity and perform joint classification and clustering. Consequently, DIPS is applicable to the standard semi-supervised learning setup of limited labeled data and abundant unlabeled data, whereas DC3 targets the specific problem of ambiguity across multiple annotators.\n\n**UPDATE:** we have added this discussion in realtion to Schmarje et al.,2022 to Appendix A.4 in our revised manuscript.\n\n---\n\n### (B) Connection to active learning\nWe wish to clarify the difference in setting of DIPS vs active learning. We agree that the concept of 'usefulness' in the selection of data samples is a significant aspect of both active learning and our work. However, it is crucial to highlight the distinct contexts in which this term is used in both settings.\n\nActive learning primarily focuses on the iterative process of selecting data samples that, when labeled, are expected to most significantly improve the model's performance. This selection is typically based on criteria such as uncertainty sampling which focuses on **epistemic uncertainty** [R1-R4]. The primary objective is to minimize labeling effort while maximizing the model's learning efficiency.\n\nIn contrast, DIPS does both labeled and pseudo-labeled selection and employs the term 'useful' in a different sense. Here, 'usefulness' refers to the capacity of a data sample to contribute positively to the learning process based on its likelihood of being correctly labeled. Our approach, which leverages training dynamics based on **aleatoric uncertainty** and confidence, is designed to flag and exclude mislabeled data. This distinction is critical in our methodology as it directly addresses the challenge of data quality, particularly in scenarios where large volumes of unlabeled data are integrated into the training process.\n\nIn active learning, these metrics are used to identify data points that, if labeled, would yield the most significant insights for model training. In our approach, they serve to identify and exclude data points that could potentially deteriorate the model's performance due to incorrect labeling. \n\n**UPDATE:** We have included this discussion in Appendix A.5 in our revised manuscript.\n\n---\n\n### (C) Threshold parameters & trade-offs\n\nWe thank the reviewer for the question on the choice of threshold parameters. To address this point, we conducted an experiment in the synthetic setup where we varied the thresholds used for both the confidence and the aleatoric uncertainty. In addition to our choice used in our manuscript (confidence threshold = 0.8, and adaptive threshold on the aleatoric uncertainty), we consider two baselines:\n1) confidence threshold = 0.9 and uncertainty threshold = 0.1 (aggressive filtering)\n2) confidence threshold = 0.5 and uncertainty threshold = 0.2 (permissive filtering)\n\nWe show the test accuracy for these baselines in the following plot available at: https://i.imgur.com/QCYEgzm.png.\n\nAs we can see, our configuration outperforms both the aggressive filtering configuration (red line) and a permissive one (blue line), which is why we adopt it for the rest of the experiments. We empirically notice in Section 5.2 that it performs well on the 12 real-world datasets we used.\n\n**UPDATE:** we have included these experimental results in Appendix C.8 in our revised manuscript."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3725/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700130192186,
                "cdate": 1700130192186,
                "tmdate": 1700130345165,
                "mdate": 1700130345165,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "o0cLT3aZv9",
                "forum": "eSO9quCgmz",
                "replyto": "7zICNMdrdW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3725/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3725/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer hxp2 [Part 3/3]"
                    },
                    "comment": {
                        "value": "### (D) Computational trade-offs\nThank you for asking a question about the computational overheads in our method, particularly in relation to large-scale datasets. While evaluating labeled and pseudo-labeled examples to compute the learning dynamics might incur additional overhead, we want to highlight that our selection mechanism yields a direct improvement in the quality of data used for training. This effect leads to a quicker model convergence. We refer to Fig. 8) a) in our manuscript, which gives the time efficiency of DIPS for the dataset CIFAR-10N. It shows that DIPS accelerates convergence by a factor 1.5-4X, despite the computation of learning dynamics.\n\n---\n\n### (E) VIME baseline\n\nWe would like to thank the reviewer for suggesting a comparison to VIME. We believe its incorporation has helped to strengthen the paper. We report the results of VIME tested on the same setup as DIPS in the next table, where we use the same 12 datasets as in Section 5.2. These results demonstrate that DIPS outperforms VIME across multiple real-world tabular datasets.\n\n|                   | DIPS (OURS)         | VIME |\n|-------------------|---------------------|---------------------|\n| adult             | **82.66 \u00b1 0.10**    | 67.69 \u00b1 0.10        |\n| agaricus-lepiota  | 65.03 \u00b1 0.25        | **66.13 \u00b1 0.01**    |\n| blog              | **80.58 \u00b1 0.10**    | 73.52 \u00b1 0.01        |\n| credit            | **81.39 \u00b1 0.07**    | 66.91 \u00b1 0.02        |\n| covid             | **69.97 \u00b1 0.30**    | 68.28 \u00b1 0.03        |\n| compas            | **65.34 \u00b1 0.25**    | 63.41 \u00b1 0.02        |\n| cutract           | **68.60 \u00b1 0.31**    | 60.36 \u00b1 0.04        |\n| drug              | **78.16 \u00b1 0.26**    | 74.47 \u00b1 0.03        |\n| German-credit     | **69.40 \u00b1 0.46**    | 62.65 \u00b1 0.05        |\n| higgs             | **81.99 \u00b1 0.07**    | 71.34 \u00b1 0.03        |\n| maggic            | **67.60 \u00b1 0.08**    | 64.98 \u00b1 0.01        |\n| seer              | **82.74 \u00b1 0.08**    | 80.12 \u00b1 0.01        |\n\n**UPDATE:** we have included these experimental results in Appendix C.9 in our revised manuscript.\n\n---\n\n### (F) Incorporating DIPS w/ non-iteration-based SSL\nWe thank the reviewer for asking this question on the integration of DIPS in non-iteration-based SSL.\nThe DIPS framework necessitates two ingredients: a list of model checkpoints, and a label for each sample for which we want to compute the learning dynamics. In the case of non-iteration-based SSL methods, DIPS updates the learning dynamics at every training step and performs its selection of labeled and unlabeled data every $k$ training steps (for example, a training step could denote an epoch). For the unlabeled data, DIPS uses the pseudo-labels computed by the model at the beginning of every learning dynamics cycle. Then, after $k$ training steps, DIPS curates the labeled and unlabeled data using its selector function and then updates the pseudo-labels used for the computation of learning dynamics on unlabeled data for the next $k$ training steps. \n\n---\n\nWe hope this answers your points, please let us know if there are any remaining concerns.\n\n---\n\n### References\n\n\n[R1] Stephen Mussmann and Percy Liang. On the relationship between data efficiency and error for uncertainty sampling, 35th International Conference on Machine Learning, PMLR.\n\n[R2]Neil Houlsby, Ferenc Huszar, Zoubin Ghahramani, and Mate Lengyel. Bayesian active learning for classification and preference learning. arXiv preprint arXiv:1112.5745, 2011.\n\n[R3] Andreas Kirsch, Joost van Amersfoort, and Yarin Gal. Batchbald: Efficient and diverse batch acquisition for deep bayesian active learning. In Advances in Neural Information Processing Systems, pp. 7024\u20137035, 2019.\n\n[R4] Nguyen, Vu-Linh, Mohammad Hossein Shaker, and Eyke H\u00fcllermeier. \"How to measure uncertainty in uncertainty sampling for active learning.\" Machine Learning 111, no. 1 (2022): 89-122."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3725/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700130265453,
                "cdate": 1700130265453,
                "tmdate": 1700130265453,
                "mdate": 1700130265453,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3DKwYvxL9y",
                "forum": "eSO9quCgmz",
                "replyto": "7zICNMdrdW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3725/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3725/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Dear Reviewer hxp2"
                    },
                    "comment": {
                        "value": "Dear Reviewer hxp2  \n\nWe are sincerely grateful for your time and energy in the review process.We hope that our responses and appendix/manuscript updates have been helpful. Please let us know of any leftover concerns and if there was anything else we could do to address any further questions or comments!\n\nThank you!  \nPaper 3725 Authors"
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3725/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700470610886,
                "cdate": 1700470610886,
                "tmdate": 1700470610886,
                "mdate": 1700470610886,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ngiSN3ETdN",
                "forum": "eSO9quCgmz",
                "replyto": "7zICNMdrdW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3725/Reviewer_hxp2"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3725/Reviewer_hxp2"
                ],
                "content": {
                    "title": {
                        "value": "Comments on the rebuttal"
                    },
                    "comment": {
                        "value": "Thank you for your comprehensive comments, including detailed clarifications, discussions, and follow-up experiments. I have also carefully checked the discussions with other reviewers. I have a few additional comments as following.\n\n**(A) Comparison with DIPS and Schmarje et al., 2022 [DC3]**\n\nThe conceptual comparison with existing work effectively highlights the uniqueness of the DIPS framework. I recommend extending this comparison to the other work [X. Wang et al., 2022], which also focuses on selective labeling. A broader comparison would further enhance the conceptual novelty of the proposed work.\n\n\n**(E) VIME Baseline**\n\nThe experimental comparison with the Tabular-specific SSL method is appreciated. For a more comprehensive understanding, could you also include the numerical results of the supervised baseline and the vanilla PL method in the table? In addition, it would be beneficial to incorporate additional SSL methods specific to the tabular domain. While time constraints may limit this during the review period, please consider addressing this in your post-review revisions.\n\n\n**(F) Incorporation of DIPS with Non-Iteration-Based SSL**\n\nThere seems to be a fundamental difference between DIPS\u2019s approach and continuous training mechanisms like FixMatch, which generate instant pseudo-labels within the same training step. This distinction in the application and updating of pseudo-labels requires further clarification.It would be important to understand whether DIPS, when integrated with FixMatch, utilizes two distinct types of pseudo-labels: those derived from self-training and those from continuous training, similar to the way with [C. Wei et al., 2022]. A more explicit explanation of how DIPS can integrate with continuous training SSL methods, such as FixMatch, would be beneficial.\n\n\u2014\n\nReferences\n\n[X. Wang et al., 2022] Unsupervised Selective Labeling for More Effective Semi-Supervised Learning, in ECCV 2022.\n\n[C. Wei et al., 2022] CReST: A Class-Rebalancing Self-Training Framework for Imbalanced Semi-Supervised Learning, in CVPR 2022."
                    }
                },
                "number": 30,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3725/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700643354201,
                "cdate": 1700643354201,
                "tmdate": 1700643354201,
                "mdate": 1700643354201,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XTbafKVOtx",
                "forum": "eSO9quCgmz",
                "replyto": "7zICNMdrdW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3725/Reviewer_hxp2"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3725/Reviewer_hxp2"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors' comment"
                    },
                    "comment": {
                        "value": "Thank you for your feedback and further clarification. I believe there are no major issues remaining for me, and the proposed concept is both versatile and distinct from previous related literature. Therefore, I intend to maintain my favorable rating."
                    }
                },
                "number": 34,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3725/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700705447990,
                "cdate": 1700705447990,
                "tmdate": 1700705447990,
                "mdate": 1700705447990,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "nLKqoDqO8q",
            "forum": "eSO9quCgmz",
            "replyto": "eSO9quCgmz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3725/Reviewer_yF2p"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3725/Reviewer_yF2p"
            ],
            "content": {
                "summary": {
                    "value": "Pseudo-labeling technique is widespread semi-supervised learning nowadays. In most works it is assumed that labeled data have golden correct labels, while authors of the paper highlight that in real world cases labeled data comes with label noise. One of the main contributions of the paper is raising this issue with data centric AI perspective  and focusing on its properties and solutions (though label noise problem is known and tackled before for supervised training only). Authors propose simple yet effective selection algorithm, dubbed DIPS, which is plug and play and applicable to most pseudo-labeling algorithms: both labeled (assumed to be with label noise) and unlabeled data are selected based on both confidence and uncertainty for next teacher-student training. Uncertainty estimation is proposed to be based on the training dynamics: variation of predictions between different checkpoints. Authors validate necessity of proposed method on couple of domains: tabular data and image classification."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Presentation of results and overall writing is of high quality\n- Highlighting label noise problem in labeled data for semi-supervised learning and considering this problem from data centric AI point of view\n- Proposed selection / filtering of labeled data during teacher-student process\n- Results for two domains: image and tabular data with variety of datasets to show wide usage and applicability of the proposed method, as well as coverage of labeled and unlabeled data coming from different data distributions\n- Robustness in the sense that different PL algorithms with proposed selection becomes close to each other in the final performance. This is very nice property as then doesn't matter what to use in practice and this speeds up development and deployment."
                },
                "weaknesses": {
                    "value": "- Authors do not disambiguate the problem into two axes: i) amount of label noise ii) amount of data in labeled data. It is well known that with small amount of labeled data (not even relative to the unlabelled, but itself, say 1k-10k images, 10min-10h of speech) it is very problematic to train pseudo-labeling algorithms with good quality due to both weak initial teacher and other training dynamics. I suspect complicated dependency between label noise level and amount of labeled data (besides amount of unlabeled data) which authors do not investigate in depth.\n- Absence of simple basic baselines where we apply straightforward the label noise methods to labeled data and perform standard teacher-student training. \"In such situations, as shown in Fig. 1, noise propagates to the pseudo-labels, jeopardizing the accuracy of the pseudo-labeling steps\" -- if we could train on small amount of labeled data with any method of learning with noisy labels then first teacher will be strong. It is not shown in the paper that all prior methods of learning with noisy labels fail on small amount of labeled data in supervised learning and thus sec 3.2 first part is overstated.\n- \"PL methods do not update the pseudo-labels of unlabeled samples once they are incorporated in one of the $\\mathcal{D}_{train}^{(i)}$ -- authors incorrectly (check out e.g. Xie, Qizhe, et al. \"Self-training with noisy student improves imagenet classification.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2020) formulate teacher-student pseudo-labeling widely used and thus overstated issues in second part of sec 3.2. On the next iteration of teacher-student training all unlabeled data are relabeled, some selection based on confidence or/and uncertainty is applied to pseudo-labels and then labeled data and these new pseudo-labeled data are combined (or labeled data can be skipped) to train new student model. I never saw in prior works on any teacher-student training (when new student is trained from scratch on new data) for images, text and speech input data that old pseudo-labels (from older teacher) are used along with new ones (with latest teacher).\n- Absence of empirical analysis showing that confidence filtering of labeled data is not enough and aleatoric uncertainty is necessary. Also there are no baselines with widely used uncertainty-based filtering for pseudo-labeled data to be used for labeled data -- motivating usage of aleatoric uncertainty.\n- Absence of empirical justification of the proposed selection method for unlabeled data only, as then we also have mislabeling and thus it should be effective there too assuming all labeled data are correctly labeled. It could be also that maybe proposed selection is only needed for labeled data while any prior selection methods could be used for unlabeled data.\n- Paper conceptually messes up between teacher-student PL methods and the ones when one model continuously trains on data with time-to-time regenerated pseudo-labels by either EMA model or by previous model states. These two approaches have different training dynamics and problems (e.g. second one is less stable). \n  - Training of \"greedy PL\" in sec. 5.1 (which is the latter type of PL methods) is out of initial formulation of PL in sec 3.1. Moreover, there is no results in Fig 3 for the zero corrupted samples, it seems PL itself it not improving upon supervised training in this toy example which is strange (looking at x=0.1)\n  - Baselines in sec 5.2 are a mixture of both methods, which is not aligned again with formulation in sec 3.1.\n  - Algo 1 in Appendix A does not cover the second approach, e.g. greedy PL method."
                },
                "questions": {
                    "value": "- I do not agree entirely with statement \"labeled data are noisy\" as if labeled data are very limited in a lot of applications we could ask for the golden (correct) labels, as we need only small amount. \n- \"application: these works use pseudo-labeling as a tool for supervised learning, whereas DIPS extends the machinery of pseudo-labeling itself.\" I don't see really huge difference here, as noisy labels means - we don't know the correct label, so mathematically it is very close tasks and solutions.\n- why do we use checkpoints for the learning dynamic being epoch and not some parameter based on number of iterations? For very large data we will never do 1 epoch, or only 1 epoch, or only few and thus measure based on epochs will be weak. From appendix info it is not clear even what checkpoints are selected and how this selection important.\n- did authors try to exclude some first epochs from confidence and uncertainty definition as they could be not very informative?\n- I don't understand this statement \"Recall aleatoric uncertainty captures the inherent data uncertainty, hence is a principled way to capture issues such as mislabeling\". Why does aleatoric uncertainty capture mislabeling? Why def. 4.2 is aleatoric as we consider variation across checkpoints (= over learning process)? \n- What is the upper bound (when all data are labeled and when all data are labeled and correct / w/o mislabeling) in Fig. 4?\n- How many iterations are done for teacher-student baselines in Fig. 4?\n- This statement is incorrect \"Note that s is solely used to select pseudo-labeled samples, among those which have not already been pseudo-labeled at a previous iteration.\" in both types of algorithms like greedy PL and UPS we relabel and reselect pseudo-labeled data.\n- How the parameters of the baselines in Appendix B1 are found? Why it is not adopted per dataset? I see that aleatoric uncertainty is adopted per dataset, which could be unfair parameters selection for the baselines.\n- B 3.1, 3.2 what does it mean T=5 for greedy PL and for UPS? Could authors describe exactly how they do PL for both algo with 5 iterations? Does it mean that for UPS it is 5 teacher-student trainings and for greedy PL 5 teacher-student trainings with each student training based on the original prior work where we continuously train model?\n- What will happen if ablation in C2 is done only for labeled data selection and unlabeled data are selected based on PL prior works?\n- What is the percentage of selected data by every method, including authors', on every iteration?\n- What about Fig 11 with vanilla PL but w/o any data selection?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3725/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699068696511,
            "cdate": 1699068696511,
            "tmdate": 1699636328533,
            "mdate": 1699636328533,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "b0cdHfDaLV",
                "forum": "eSO9quCgmz",
                "replyto": "nLKqoDqO8q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3725/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3725/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer yF2p [Part 1/5]"
                    },
                    "comment": {
                        "value": "Dear Reviewer yF2p\n\nThank you for your thoughtful comments and suggestions! We give answers to each of the following points in turn and highlight the updates to the revised manuscript. In addition, we have uploaded the revised manuscript. We hope this response alleviates your concerns, but please let us know if there are any remaining concerns.\n\n\n- A) Dependency between label noise level and amount of labeled data **[Part 2/5]**\n- B) LNL + pseudo-labeling **[Part 2/5]**\n- C) Importance of aleatoric uncertainty **[Part 3/5]**\n- D) Selection of unlabeled data **[Part 3/5]**\n- E) Terminology of pseudo-labeling **[Part 4/5]**\n- F) Answers to the other questions **[Parts 4,5/5]**"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3725/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700129859834,
                "cdate": 1700129859834,
                "tmdate": 1700129859834,
                "mdate": 1700129859834,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VD02NnkQ9d",
                "forum": "eSO9quCgmz",
                "replyto": "nLKqoDqO8q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3725/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3725/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer yF2p [Part 2/5]"
                    },
                    "comment": {
                        "value": "### (A) Dependency between label noise level and amount of labeled data\nWe thank the reviewer for the suggestion to investigate the dependency between label noise level and the amount of labeled data. To answer this point, we conduct a synthetic experiment following a similar setup as in Section 5.1 in our manuscript. Note that the experiment is synthetic in order to be able to control the amount of label noise.\nWe considered the same list of label noise proportions, ranging from 0. to 0.45. For each label noise proportion, we consider $n_{\\mathrm{lab}} \\in$ {50,100,1000}, and fix $n_{\\mathrm{unlab}} = 1000$.\nWe conduct the experiment $40$ times for each configuration. \n\nWe report the results in the plots available at: https://i.imgur.com/rzFeFsl.png. \n\nAs we can see on the plots, PL+DIPS consistently outperforms the supervised baselines in almost all the configurations. When the amount of labeled data is low ($n_{lab} = 50$) and the proportion of corrupted samples is high ($0.45$), PL is on par with the supervised baseline. This mirrors the reviewer's intuition that pseudo-labeling is more difficult with a very low amount of labeled samples (and a high level of noise). We note, though, that DIPS consistently improves the PL baseline for reasonable amounts of label noise which we could expect in real-world settings (e.g. $0.1$). The performance gap between DIPS and PL is remarkably noticeable for $n_{\\mathrm{lab}}=1000$, i.e. when the amount of labeled samples is high.\n\n**UPDATE:** We have included these results in Appendix C.5 of the revised manuscript.\n\n---\n\n### (B) LNL + pseudo-labeling\nIn the following table, we report the results  when applying the LNL baselines to the labeled data only, and then performing standard teacher-student training. \n\n|                   | DIPS (OURS)           | Small-Loss | Fluctuation  | FINE |\n|-------------------|-----------------------|------------------------------|-----------------------------|----------------------|\n| adult             | **82.66 \u00b1 0.10**      | 80.76 \u00b1 0.20                 | 80.95 \u00b1 0.23                | 81.06 \u00b1 0.27         |\n| agaricus-lepiota  | **65.03 \u00b1 0.25**      | 64.22 \u00b1 0.27                 | 55.93 \u00b1 1.67                | 54.61 \u00b1 3.77         |\n| blog              | **80.58 \u00b1 0.10**      | 79.09 \u00b1 0.35                 | 79.16 \u00b1 0.28                | 77.98 \u00b1 0.72         |\n| credit            | **81.39 \u00b1 0.07**      | 79.57 \u00b1 0.23                 | 79.67 \u00b1 0.26                | 77.87 \u00b1 0.30         |\n| covid             | **69.97 \u00b1 0.30**      | 67.76 \u00b1 0.52                 | 67.09 \u00b1 0.59                | 66.61 \u00b1 0.60         |\n| compas            | **65.34 \u00b1 0.25**      | 59.88 \u00b1 0.46                 | 59.56 \u00b1 0.59                | 59.94 \u00b1 0.63         |\n| cutract           | **68.60 \u00b1 0.31**      | 62.83 \u00b1 1.02                 | 62.23 \u00b1 0.93                | 63.73 \u00b1 1.40         |\n| drug              | **78.16 \u00b1 0.26**      | 75.19 \u00b1 0.71                 | 74.71 \u00b1 0.69                | 74.28 \u00b1 1.00         |\n| German-credit     | 69.40 \u00b1 0.46          | **71.15 \u00b1 1.27**             | 70.95 \u00b1 1.11                | 61.10 \u00b1 4.95         |\n| higgs             | **81.99 \u00b1 0.07**      | 80.91 \u00b1 0.16                 | 80.81 \u00b1 0.19                | 79.60 \u00b1 0.19         |\n| maggic            | **67.60 \u00b1 0.08**      | 65.08 \u00b1 0.28                 | 65.18 \u00b1 0.25                | 64.43 \u00b1 0.33         |\n| seer              | **82.74 \u00b1 0.08**      | 81.57 \u00b1 0.17                 | 81.66 \u00b1 0.25                | 79.66 \u00b1 0.34         |\n\nThe results show that DIPS  outperforms these baselines (Small-loss, Fluctuation, FINE) in almost all the datasets, and suggest that that DIPS' curation method based on learning dynamics should be preferred, both on the labeled set and on top of standard teacher-student training."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3725/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700129974127,
                "cdate": 1700129974127,
                "tmdate": 1700131204065,
                "mdate": 1700131204065,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8KNTsRVKJ3",
                "forum": "eSO9quCgmz",
                "replyto": "nLKqoDqO8q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3725/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3725/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer yF2p [Part 3/5]"
                    },
                    "comment": {
                        "value": "### (C) Importance of aleatoric uncertainty\n\nWe thank the reviewer for the suggestion. We conducted an ablation study where we removed the aleatoric uncertainty and only kept a confidence-based selection (with threshold = 0.8). We term this confidence ablation to highlight if there is indeed value to the aleatoric uncertainty component of DIPS.\n\nWe report results in the following table, for the $12$ tabular datasets used in Section 5.2, which shows the benefit of the two-dimensional selection criterion of DIPS. Of course, in some cases there might not be a large difference with respect to our confidence ablation --- however we see that DIPS provides a statistically significant improvement in most of the datasets. Hence, since the computation is negligible, it is reasonable to use the 2-D approach given the benefit obtained on the noisier datasets.\n\n\n\n|                   | DIPS          |  Confidence Ablation |\n|-------------------|---------------------|---------------------|\n| adult             | **82.66 \u00b1 0.10**    | 82.13 \u00b1 0.16        |\n| agaricus-lepiota  | **65.03 \u00b1 0.25**        | 64.38 \u00b1 0.23   |\n| blog              | **80.58 \u00b1 0.10**    | 80.22 \u00b1 0.33        |\n| credit            | **81.39 \u00b1 0.07**    | 79.76 \u00b1 0.15        |\n| covid             | **69.97 \u00b1 0.30**    | 69.28 \u00b1 0.40        |\n| compas            | **65.34 \u00b1 0.25**    | 64.69 \u00b1 0.25         |\n| cutract           | **68.60 \u00b1 0.31**    | 66.32 \u00b1 0.12         |\n| drug              | **78.16 \u00b1 0.26**    | 75.37 \u00b1 0.71        |\n| higgs             | **81.99 \u00b1 0.07**    | 81.42 \u00b1 0.16        |\n| maggic            | **67.60 \u00b1 0.08**    | 66.26 \u00b1 0.18          |\n| seer              | **82.74 \u00b1 0.08**    | 82.02 \u00b1 0.15       |\n\n\n**UPDATE:** we have included these results in Appendix C.10 of our revised manuscript.\n\n\n\nWhile DIPS builds on a notion of uncertainty, we highlight that this uncertainty differs in nature from the uncertainty metrics used in the pseudo-labeling literature.\nOne key difference is that DIPS computes the aleatoric uncertainty based on some given labels (provided labels for $D_{lab}$ or pseudo-label for $D_{unlab}$), and then computes the agreement between the prediction of a model and these given labels as is captured by the quantity $[f_{e}(x)]_{y}$. This is very different in nature from the uncertainty metrics used in pseudo-labeling, which traditionally do not rely on a given label as they are applied to *unlabeled data* (as is done when using an ensemble of models and computing a standard deviation to define an uncertainty metric). This explains why these uncertainty based filtering methods only focus on unlabeled data, while ours can handle both labeled and pseudo-labeled data.\n\n---\n\n### (D) Selection of unlabeled data\n\nTo answer the Reviewer's question regarding the selection of unlabeled data in DIPS, we refer to Figure 10 in our submitted manuscript (Appendix C.1). This figure shows results when our selection mechanism is only applied to the labeled data and we only use prior selection methods for the unlabeled data (this baseline is denoted as \"A1\"). The results show DIPS outperforms this baseline by a large margin. This justifies why we apply our selection method both to labeled and unlabeled data. We have updated our manuscript (Section 4.1) to more clearly highlight this result - thank you for the suggestion!"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3725/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700130011719,
                "cdate": 1700130011719,
                "tmdate": 1700130368376,
                "mdate": 1700130368376,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "u47Cf3RQOg",
                "forum": "eSO9quCgmz",
                "replyto": "nLKqoDqO8q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3725/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3725/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer yF2p [Part 4/5]"
                    },
                    "comment": {
                        "value": "### (E) Terminology of pseudo-labeling\n\nWe thank the reviewer for the comment about the terminology of teacher-student pseudo-labeling and apologize if our description of this paradigm was not clear enough in our manuscript.\nWe decided to describe the common teacher-student pseudo-labeling methodology adopted in the **tabular setting**. As a consequence, we used the implementation provided by [R1], which grows the training set with pseudo-labels generated at the current iteration, thus keeping old pseudo-labels in the training dataset in subsequent iterations. \n\nIn addition to adopting this practice, we investigated this choice experimentally, by comparing between two versions of confidence-based pseudo-labeling: \n- Version 1): with a growing set of pseudo-labels (as followed by the implementation of [R1] and our paper) \n- Version 2): without keeping old pseudo-labels, as suggested by the reviewer.\n\nWe evaluate these two methods in the synthetic setup described in Section 5.1 of our manuscript, and report the test accuracy in the plot available at: https://i.imgur.com/VSPg1mD.png.\n\n\n\nThe red line corresponds to Version 1) (the implementation we used in the manuscript), while the green line corresponds to Version 2) (suggested by the reviewer).\nAs we can see, in this tabular setting, growing a training set by keeping the pseudo-labels generated at each iteration leads to the best results, motivating our adoption of this pseudo-labeling methodology used in the tabular setting.\n\n**UPDATE:** we have included this experiment in Appendix C.6 of our revised manuscript.\n\n\nWe apologize if the terminology of greedy-PL was not made sufficiently clear in our original manuscript. We refer to confidence-based PL as \"greedy-PL\", following the same terminology as in [R1]. As such, all the baselines in Section 5.2 fall under the umbrella of \" teacher-student PL methods\", where the models (i.e. XGBoost in Section 5.2) are trained from scratch at each iteration in a teacher/student fashion.\n\n\nFinally, to address the query about Fig 3 with zero corrupted samples, we add to the results in Fig.3 the zero corruption setting in the updated plot available at: https://i.imgur.com/EBUYDkY.png.\n\nIn the zero corruption setting, all methods almost achieve perfect test performance. \n\n**UPDATE:** we have updated Fig. 3)c) in our revised manuscript.\n\n\nWe also highlight that pseudo-labeling does always increase performance over the purely supervised approach, which is natural since PL has access to unlabeled data. However, we are interested in a setting where the labeled data is subject to **mislabeling**. Hence, this explains why the margin between PL and Supervised in Fig. 3)c) is not bigger: noise propagates to the unlabeled data, which limits the effectiveness of PL and justifies the data-centric lens that DIPS adopts, hence giving a big performance gap with the baselines.\n\n---\n\n### (F) Answers to the other questions\n- *\"I do not agree entirely with statement \"labeled data are noisy \\& if small amounts we can request the gold standard\"*\n\nThe issue of labeled data errors are prevalent in many industries from healthcare to finance. This is even the case in seemingly curated datasets as shown by Northcutt et al., where label error rates of widely-used benchmark datasets can reach up to 10\\%. As mentioned in Section 1, while it might appear possible to manually inspect the data to identify errors in the labeled set, we note that this requires domain expertise and is human-intensive, especially in modalities such as tabular data where inspecting rows in a spreadsheet can be much more challenging than reviewing an image. In other cases, updating labels is actually infeasible due to rerunning costly experiments in domains such as biology and physics, or indeed impossible due to lack of access to either the underlying sample or equipment.\n\n- *\"application: these works use pseudo-labeling as a tool for supervised learning, whereas DIPS extends the machinery of pseudo-labeling itself.\" I don't see really huge difference here, as noisy labels means - we don't know the correct label, so mathematically it is very close tasks and solutions.*\n\nWe agree that mathematically, the auditing task is similar. The main difference is how semi-supervised learning is used. In the first setting SSL is a solution to noisy labels, whereas we study and resolve the effect of noisy labels on SSL."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3725/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700130073253,
                "cdate": 1700130073253,
                "tmdate": 1700130382422,
                "mdate": 1700130382422,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kJxCHB2Sja",
                "forum": "eSO9quCgmz",
                "replyto": "nLKqoDqO8q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3725/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3725/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer yF2p [Part 5/5]"
                    },
                    "comment": {
                        "value": "- *\"why do we use checkpoints for the learning dynamic being epoch and not some parameter based on number of iterations? For very large data we will never do 1 epoch, or only 1 epoch, or only few and thus measure based on epochs will be weak.\"*\n\nWe first note that DIPS only needs models which are trained in an iterative manner. For example, in our tabular experiments in Section 5.2, we use an XGBoost backbone, where the learning dynamics are computed over the different boosting iterations. Hence what we call a checkpoint in our manuscript (Section 4.2) needs not be an epoch. For neural networks, we do not necessarily need to define checkpoints as epochs, when the dataset is large; we can instead define checkpoints after every $k$ gradient steps. \n\n- *\"did authors try to exclude some first epochs from confidence and uncertainty definition as they could be not very informative?\"*\n\nWe thank the reviewer for this suggestion. We have conducted an additional experiment to investigate the choice of the range of iterations used to compute the learning dynamics. We consider ignoring the first $25$%/$50$%/$75$% iterations and use the remaining iterations to compute the learning dynamics.\n\nThe plot available at https://i.imgur.com/QzNnHba.png\n\nIt shows the mean performance difference (on test accuracy) by using the truncated iteration windows versus using all the iterations, and averages the results over the 12 datasets used in Section 5.2. As we can see, it is better to use all the iterations window, as the initial iterations carry some informative signal about the hardness of samples.\nThis motivates our choice of computing the learning dynamics over the whole optimization trajectory, a choice which we adopt for all of our experiments.\n\n**UPDATE:** we have added these experimental results in Appendix C.7 of our revised manuscript.\n\n\n- *\"Why does aleatoric uncertainty capture mislabeling? Why def. 4.2 is aleatoric as we consider variation across checkpoints (= over learning process)?\"*\n\nDef. 4.2 of aleatoric uncertainty builds on the variability that originates from the inability to predict the correct label with high confidence over the different checkpoints. Intuitively, a sample with high aleatoric uncertainty would be a sample for which we are not confident in predicting the ground-truth label at every checkpoint (e.g. $[f_{e}(x)]_{y} = 1/2$ for all iteration $e$ in the case of binary classification). This would mean that the model is not capable of learning the given label, which happens for mislabeled points. \n\n- *\"What is the upper bound (when all data are labeled and when all data are labeled and correct / w/o mislabeling) in Fig. 4?\"*\n\nWe thank the reviewer for the question and we report in the following plot, available at https://i.imgur.com/gsLrb7r.png, the test accuracy when all the data are labeled in Section 5.2. Note that we cannot control the amount of mislabeling in this setting, as we only have access to the training labels in the datasets.\n\n\nTraining a model on all the labeled data often yields the best results, which is intuitive, as we consider the pseudo-labeling setting which uses only $10\\%$ of labeled data. We notice that in some datasets (e.g. cutract, compas, drug), DIPS improves the fully supervised baseline, which may hint at greater inherent noise in the datasets. We can't quantify exactly the level of noise as we don't have the ground-truth, but the performance gap obtained by using DIPS could be seen as a proxy for it. \n\n- *\"How many iterations are done for teacher-student baselines in Fig. 4?\"*\n*\"I see that aleatoric uncertainty is adopted per dataset.\"*\n\nWe used $T=5$ iterations for the experiment in Fig. 4. This follows the guideline of the sensitivity experiment conducted in [R1] (Appendix B.2 in the corresponding paper) and explains why we kept this parameter fixed for the datasets used in Section 5.2 in our manuscript. \nRegarding the aleatoric uncertainty threshold, it is chosen to be adaptive, and is not hand-crafted for each dataset. Furthermore, we stress that DIPS is applied on top of the baselines. \n\n\n\n- *\"What about Fig 11 with vanilla PL but w/o any data selection?*\"\n\nWe give the radial diagram showing the target dataset and how the DIPS selector and vanilla PL selector differ at the following link: https://i.imgur.com/f4VV76N.png. \n\nWe see that DIPS is closer to the test data than vanilla PL selector.\n\n**UPDATE:** we have added this plot in Appendix C.3 in our revised manuscript.\n\nWe hope this answers your points, please let us know if there are any remaining concerns."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3725/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700130158968,
                "cdate": 1700130158968,
                "tmdate": 1700131626596,
                "mdate": 1700131626596,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "L7lJSNBkb9",
                "forum": "eSO9quCgmz",
                "replyto": "nLKqoDqO8q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3725/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3725/Authors"
                ],
                "content": {
                    "title": {
                        "value": "References"
                    },
                    "comment": {
                        "value": "### References\n[R1] Vu-Linh Nguyen, Sachin Sudhakar Farfade, and Anton van den Hengel. Confident Sinkhorn allocation for pseudo-labeling. arXiv preprint arXiv:2206.05880, 2022."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3725/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700130220061,
                "cdate": 1700130220061,
                "tmdate": 1700130220061,
                "mdate": 1700130220061,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "o8vitTSbSI",
                "forum": "eSO9quCgmz",
                "replyto": "nLKqoDqO8q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3725/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3725/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Dear Reviewer yF2p"
                    },
                    "comment": {
                        "value": "Dear Reviewer yF2p  \n\nWe are sincerely grateful for your time and energy in the review process.We hope that our responses and appendix/manuscript updates have been helpful. Please let us know of any leftover concerns and if there was anything else we could do to address any further questions or comments!\n\nThank you!  \nPaper 3725 Authors"
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3725/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700470575883,
                "cdate": 1700470575883,
                "tmdate": 1700470575883,
                "mdate": 1700470575883,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6ZUS9UOEXa",
                "forum": "eSO9quCgmz",
                "replyto": "o8vitTSbSI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3725/Reviewer_yF2p"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3725/Reviewer_yF2p"
                ],
                "content": {
                    "title": {
                        "value": "Response to the authors' comments and updated manuscript"
                    },
                    "comment": {
                        "value": "Dear authors, \n\nI have read carefully all reviews and your responses as well as updated manuscript. First, thanks a lot for all detailed responses and extra ablations. Second, please find below my further comments and questions:\n\n> comparison to VIME\n\nI have looked at this ablation too in the revised manuscript. Results with VIME are worse than supervised baseline (e.g. seer, adult, credit and others) in Figure 4, why is that? Seems VIME experiments could be incorrect then. Please correct me if I missed anything here.\n\n>  (A) Dependency between label noise level and amount of labeled data\n\nThanks for conducting these experiments. For future, I think overall running 10 times is enough for getting good estimate of the std from experiments + running on real, not synthetic data, will be more helpful from the application side (e.g. you can control in CIFAR-100 how many labels you flip randomly). But ok, I am fine with the setting for the synthetic data. \n\nFrom the results in Figure 17 now I could  infer that 1) for small amount of labeled data it is less helpful (as I expected it is just very hard setting); 2) I can simply filter out noisy samples from labeled data (using any prior  method on identifying noisy samples in data), use 2x less data (1000 -> 500) and then I get the best model with classic PL / no PL at all (as here with clean labeled data we even don\u2019t need to use unlabeled data).\n\nMain question from the results I have right now is \"what if I filter out noise from labeled data first and then train standard supervised baseline and PL baseline? would they be better than DIPS?\n\n> (B) LNL + pseudo-labeling\n\nThanks for pointing to the previously done ablation. Sorry that I missed it in the initial review. I think I misunderstood that section in the Appendix, now I got what you meant. Could you also provide supervised only (w/o doing PL) for all the methods from the Table here as well as the baseline trained with standard cross-entropy loss only, as I see that results for these prior methods are worse than supervised baseline from Figure 4 (e.g. adult and seer). The latter is very suspicious that PL is hurting the results (especially in the context that supervised baselines with, say FINE, should be even better than one from Figure 4)?\n\n> (C) Importance of aleatoric uncertainty\n\nThanks for the ablation! Having these results I believe Figure 4 should be revisited in the paper, and the Vanilla runs should be done with confidence filtering included: it was shown in many prior works already that confidence filtering is needed and thus it is more fair comparison. You propose to add aleatoric uncertainty and thus exactly improvement on top of confidence filtering should be shown in Figure 4. Right now it is very misleading for results and overstating the improvements the paper brings. I believe if you redo Figure 4 with that you will have a less \u201cimpressive\u201d results as highlighted by Reviewer `R-hxp2`.\n\n> while ours can handle both labeled and pseudo-labeled data.\n\nI would argue here that uncertainty metrics focused on unlabeled data still can be applied to labeled data as they do not need any labels and we can ignore labels in labeled data.\n\n> (D) Selection of unlabeled data\n\nEither I still don\u2019t understand formulation or something is incorrectly formulated in the text. A1 is when you do selection only for training on labeled data. A2 is doing selection for both labeled and unlabeled data during next phases of training. There is no baseline when you do selection **only on labeled data** and do not do anything with unlabeled data (or do confidence filtering as in prior works). Moreover, what I proposed initially is to do experiments where you don\u2019t have any label noise in labeled data and then apply DIPS on unlabeled data only, to show that previously proposed confidence filtering is not enough for unlabeled data (even in the setting of no label noise in labeled data). This I believe should work as we can interpret pseudo-labels as data with noise."
                    }
                },
                "number": 27,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3725/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700555678490,
                "cdate": 1700555678490,
                "tmdate": 1700555678490,
                "mdate": 1700555678490,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OzAFIvUDDy",
                "forum": "eSO9quCgmz",
                "replyto": "6ZUS9UOEXa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3725/Reviewer_yF2p"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3725/Reviewer_yF2p"
                ],
                "content": {
                    "title": {
                        "value": "Response to the authors' comments and updated manuscript [continue]"
                    },
                    "comment": {
                        "value": "> (E) Terminology of pseudo-labeling\n\nProvided reference [R1] is not published in the peer-reviewed journals / conferences, thus I would suggest to use prior published papers definitions as a reference or introduce new variant of pseudo-labeling and then make connection to prior works / do proper comparisons / justify why we need another definition-algorithm. \n\nCompared baselines on FixMatch, UPS, FlexMatch, etc. do not use such definition on growing set of pseudo-labels in their experiments, moreover even 1) do continuous training with selecting new batch of unlabeled data each iteration and generating new PLs or 2) do teacher-student training. It is not clear then what authors mean exactly when they point to a particular paper baseline implementation and what is used. If they use their growing pseudo-label set \u2014 it is another algorithm than in prior works. Right now I don\u2019t have any understanding how the baseline methods are implemented. I believe in every baseline, proposed filtering based on confidence + uncertainty should be used, not another modification like expanding set of pseudo-labels (which includes older generated versions).\n\n> The red line corresponds to Version 1) (the implementation we used in the manuscript), while the green line corresponds to Version 2) (suggested by the reviewer). As we can see, in this tabular setting, growing a training set by keeping the pseudo-labels generated at each iteration leads to the best results, motivating our adoption of this pseudo-labeling methodology used in the tabular setting.\n\nI appreciate! However, this should be tested for every baseline/dataset, not only for synthetic data: it will be very dependent on the method of PL you use. Right now I see that you introduce another algorithms on PL which is non-standard in the literature (thus could be a new contribution) and is not tested e.g. on images. It maybe used in [R1] but there is no justification why it is better than prior works. Experiments on synthetic data only with on PL variant is not enough in y opinion (this even can be a separate paper).\n\n>We apologize if the terminology of greedy-PL was not made sufficiently clear in our original manuscript. We refer to confidence-based PL as \"greedy-PL\", following the same terminology as in [R1]. As such, all the baselines in Section 5.2 fall under the umbrella of \" teacher-student PL methods\", where the models (i.e. XGBoost in Section 5.2) are trained from scratch at each iteration in a teacher/student fashion. \n\nThanks for clarification. So this does not follow your definition in Section 3. All variants which are used, training from scratch or continue model training (this one can be also used in xgboost, where every new tree in the ensemble is treated as training step like in NNs), should be clearly specified in the paper because, as I said, they have different properties and dynamics in practice.\n\ngreedy-PL in the text via Lee et al., 2013 is continuous training, which is different from your reference to [R1]. I would suggest be clear in the text that for tabular data (as I understand) you use only teacher-student training and you repeat 5 times the process with increasing number of pseudo-labels for every unlabeled sample. For images, probably you do different training as FixMatch is not the training of teacher-student type, it is continuous training.\n\n> Finally, to address the query about Fig 3 with zero corrupted samples, we add to the results in Fig.3 the zero corruption setting in the updated plot available at: https://i.imgur.com/EBUYDkY.png. In the zero corruption setting, all methods almost achieve perfect test performance. \n\nI would argue that this setting seems to be unrealistic and the question could be \u201cdoes the behavior of all ablations on such synthetic data can be extrapolated to the cases where zero corruption case actually have huge gap between supervised and semi-supervised training?\u201d The effect of having labeled data corruption can be even bigger in practice as we rely a lot on unlabeled data then.. Frankly, I think this can be more strong argument in your favor if we have data where for zero corruption there is difference between supervised an semi-supervised training.\n\n> Baselines in sec 5.2 are a mixture of both methods, which is not aligned again with formulation in sec 3.1. Algo 1 in Appendix A does not cover the second approach, e.g. greedy PL method.\n\nThis is still an issue for me.\n\n> We agree that mathematically, the auditing task is similar. The main difference is how semi-supervised learning is used. In the first setting SSL is a solution to noisy labels, whereas we study and resolve the effect of noisy labels on SSL.\n\nI would suggest to smooth a bit formulation."
                    }
                },
                "number": 28,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3725/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700559873798,
                "cdate": 1700559873798,
                "tmdate": 1700559873798,
                "mdate": 1700559873798,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "a3igmn4qIV",
                "forum": "eSO9quCgmz",
                "replyto": "OzAFIvUDDy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3725/Reviewer_yF2p"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3725/Reviewer_yF2p"
                ],
                "content": {
                    "title": {
                        "value": "Response to the authors' comments and updated manuscript [continue]"
                    },
                    "comment": {
                        "value": "> We first note that DIPS only needs models which are trained in an iterative manner. For example, in our tabular experiments in Section 5.2, we use an XGBoost backbone, where the learning dynamics are computed over the different boosting iterations. Hence what we call a checkpoint in our manuscript (Section 4.2) needs not be an epoch. For neural networks, we do not necessarily need to define checkpoints as epochs, when the dataset is large; we can instead define checkpoints after every gradient steps. \n\nI see main confusion in the way you define iterative. Is it continuous training of the model (like, grad steps in NNs, adding one more tree into ensemble in the xgboost) or it is resetting model and training from scratch? I feel you mix this terminology between xgboost and NNs, and between prior works (as in most of them training is done continuously, like FixMatch). This creates a lot of interpretations what and how exactly you are doing in the paper and experiments right now. Not clear how overall proposed data selection influence both variants of PL and how the choice of the checkpoints affects the dynamics and thus the selection criteria. \n\n> We thank the reviewer for this suggestion. We have conducted an additional experiment to investigate the choice of the range of iterations used to compute the learning dynamics. [...]\n\nThanks for the ablation. So if I correctly interpret PL algorithm here \u2014 it is xgboost where each iteration is teacher-student training. Then yes, I agree it makes sense to use all, as even first iteration here is well trained already and properly showing the uncertainty of the model/data (Figure 2 then is really misleading for the xgboost then). However if we come back to FixMatch training on CIFAR where iteration will be another grad step \u2014 then I guess initial iterations will be misleading.\n\n> This would mean that the model is not capable of learning the given label, which happens for mislabeled points. \n\nWhat then happens if for samples we have memorization in the model?  \n\n> We thank the reviewer for the question and we report in the following plot, available at https://i.imgur.com/gsLrb7r.png, the test accuracy when all the data are labeled in Section 5.2. Note that we cannot control the amount of mislabeling in this setting, as we only have access to the training labels in the datasets.\n\nThanks for adding this! This is helpful! I think the plot in the paper is not updated (but it ok, I checked the link which shows results correctly).\n\n> We used iterations $T=5$ for the experiment in Fig. 4\n\nThanks for clarification! But then what about CIFAR / images experiments?\n\n> We give the radial diagram showing the target dataset and how the DIPS selector and vanilla PL selector differ at the following link: https://i.imgur.com/f4VV76N.png. \n\nThanks!"
                    }
                },
                "number": 29,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3725/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700559969210,
                "cdate": 1700559969210,
                "tmdate": 1700559969210,
                "mdate": 1700559969210,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ADnjG81BRq",
                "forum": "eSO9quCgmz",
                "replyto": "nLKqoDqO8q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3725/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3725/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your response [Part 1/2]"
                    },
                    "comment": {
                        "value": "Dear Reviewer yF2p,\n\nThank you for your time and questions! We answer your questions in what follows.\n\n**On VIME results**: In order to get these results, we used the code repository released by the authors of VIME (https://github.com/jsyoon0823/VIME) to avoid any difference in implementation. We note that VIME necessitates a neural network as it builds on pretext tasks which require training an encoder. This contrasts the framework of pseudo-labeling, which is general-purpose, and enables us to use tree-based models such as XGBoost, which are traditionally the models giving the best performance for tabular data (Grinsztajn et al., 2022), and which might explain the performance gap with the supervised baseline.\n\nGrinsztajn, Leo, Edouard Oyallon, and Gael Varoquaux. \"Why do tree-based models still outperform deep learning on typical tabular data?.\" Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2022.\n\n\n----\n\n**Filtering out noise from labeled data + training PL baseline:** We apologize if this was not clear: as mentioned in our previous response, filtering out noise from labeled data first and training the PL baseline (which is *confidence-based filtering*, and does not use DIPS) corresponds to the A1 baseline in Appendix C.1. As we show in Figure 10, DIPS outperforms A1.\n\n----\n\n**On LNL results**: The Reviewer is right that some of the results in Appendix C.2 are worse than the supervised baseline. We attribute this to the unsuitability of these selectors for the setting DIPS tackles. While these methods are part of the LNL literature, they traditionally assume access to large labeled datasets, which contrast our pseudo-labeling setting, in which none of these baselines have been used before. Hence, there are no guarantees of performance translations during the iterative procedure.\n\n---\n\n\n**On adding confidence filtering:** We want to emphasize that the baselines we consider in Figure 4 _already_ use confidence filtering in their selection of pseudo-labels. For example, greedy-PL solely uses a confidence threshold, while UPS adds to it an uncertainty threshold. We integrate DIPS on top of them in the experiment of Figure 4.\n\n\n---\n\n\n\n**Uncertainty metrics designed for unlabeled data applied to labeled data:** We wish to clarify that the uncertainty metrics focused on unlabeled data necessitate us to train a supervised model in the first place. Thereafter, we predict on the unlabeled set and apply the uncertainty metric to decide which pseudo-labels to keep. This is unsuitable for the labeled data as we would then have to use the same supervised model to predict on the very same labeled data --- yet having to \"ignore\" the label of the labeled sample.\nFor example, in tabular data, the uncertainty proxy is often obtained with an ensemble. However, training this ensemble on labeled data and evaluating it on the same labeled data would reduce diversity in the predictions of the ensemble and lead to bad calibration of this uncertainty on labeled data (with the ensemble being very overconfident). \n\n\n---\n \n\n**On ablation in Appendix C.6:** We will conduct these experiments on the datasets of Section 5.2 for our post-review revisions considering the limited time before the end of the rebuttal window.\n\n---"
                    }
                },
                "number": 31,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3725/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700700143657,
                "cdate": 1700700143657,
                "tmdate": 1700700283378,
                "mdate": 1700700283378,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dulDWnfBrs",
                "forum": "eSO9quCgmz",
                "replyto": "nLKqoDqO8q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3725/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3725/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your response [Part 2/2]"
                    },
                    "comment": {
                        "value": "**Baseline implementations:** The implementation of the baselines in our tabular experiments (Section 5.2) trains the models from scratch, while FixMatch in our images experiment (Section 5.5) is continuous model training. Hence, the methods in Section 5.2 are not implemented as a mixture of these two types of methods, they are implemented as teacher-student methods. We used their implementation provided by the authors of [R1] at the following link: https://github.com/amzn/confident-sinkhorn-allocation. We will clarify this distinction between teacher-student training and continuous model training in our revised manuscript.\n\n---\n\n\n**On iterations for learning dynamics**: Let us clarify: in Section 4.2, we define the learning dynamics by studying how the model predicts samples over different **checkpoints**. For example, for a neural network, a checkpoint can be an epoch. For an XGBoost, this can be a boosting iteration. \n\nThese checkpoints are not to be confused with the teacher-student iterations of the baselines that we use in Figure 4 and Section 5.2.  \n\n\n \n---\n\n**On ablation in Appendix C.7:** In the ablation of Appendix C.7, the \"iterations\" denote the boosting iterations of the XGBoost model. For example, 25% (on the x-axis of the plot) means that in order to compute the learning dynamics of the XGBoost model trained at _every_ teacher-student iteration, we use the last 75% of the boosting iterations of the XGBoost.\n\n----\n\n\n\n**On memorization in the model:** Memorization may impact the computation of the learning dynamics. However, with learning dynamics we compute the metrics by looking at the behavior of the model over multiple checkpoints, and not only the last one. In the latter case, memorization would indeed be a problem. Furthermore, we want to point out to theoretical results that show (in some simple scenarios) that mislabeled points are unlikely to be classified with their (incorrect) label. For example, Theorem 3 in (Maini et al., 2022) (Appendix A.4 in the corresponding paper) gives some intuition about mislabeled samples in a mixture setting, by stating that the probability that mislabeled examples are classified with their given (incorrect) labels tends to $0$, as the model is trained until convergence.\n\n\n---\n\n**FixMatch+DIPS parameters:** For the image experiments, we define one checkpoint every $1024$ gradient steps, and compute the learning dynamics over $10$ checkpoints.\n\n---\nWe thank you for your time and energy in the reviewing process, please let us know if you additional questions!\n\n---\n**References:**\n\nMaini, Pratyush, Saurabh Garg, Zachary Lipton, and J. Zico Kolter. \"Characterizing datapoints via second-split forgetting.\" Advances in Neural Information Processing Systems 35 (2022): 30044-30057."
                    }
                },
                "number": 32,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3725/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700700237514,
                "cdate": 1700700237514,
                "tmdate": 1700700255646,
                "mdate": 1700700255646,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wCtlgAhq4t",
                "forum": "eSO9quCgmz",
                "replyto": "dulDWnfBrs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3725/Reviewer_yF2p"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3725/Reviewer_yF2p"
                ],
                "content": {
                    "title": {
                        "value": "Further discussion"
                    },
                    "comment": {
                        "value": "Dear authors,\n\nThanks for additional clarifications! I need to re-read the paper again and think more about results, based on your comments and our fruitful discussion. \n\nOne thing, for sure, will be helpful for me for the final decision if you could clarify the exact setup for every baseline you run and corresponding modification when you run DIPS on top of it. Right now I am really lost in the formulations you use for pseudo-labeling. E.g. you mentioned that FixMatch for tabular data is very different from the FixMatch for images (I would argue here it is not FixMatch anymore, honestly, for tabular data then). I totally understand that particular methods maybe were designed or tested for a specific domain and are not right away applicable e.g. to tabular data and switch from NNs to XGboost. But I want you to be very specific in formulation how exactly models/baselines are trained and modified from the original papers to be sure 1) I fully understand it and then can judge on the validity of the comparisons and experiments you did; 2) we could converge to the version for the revision which will be readable for any person out of pseudo-labeling domain to use your method in practice; 3) for the sake of reproducibility.\n\nThanks in advance!"
                    }
                },
                "number": 35,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3725/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700712319568,
                "cdate": 1700712319568,
                "tmdate": 1700712319568,
                "mdate": 1700712319568,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XFXd1uG3wh",
                "forum": "eSO9quCgmz",
                "replyto": "nLKqoDqO8q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3725/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3725/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your response"
                    },
                    "comment": {
                        "value": "Dear Reviewer yF2p,\n\nThank you for your engagement during this discussion period!\nLet us clarify in this response the baselines used in our experiments.\n\n## In Sections 5.1, 5.2, 5.3, 5.4 (Tabular experiments)\n\n| Name of PL method                                   | Type of PL implementation         | Backbone model | Pseudo-label selection method                              | Number of teacher-student iterations |\n| --------------------------------------------------- | ------------------------- | ---------------| ----------------------------------------------------- | ------------------------------------- |\n| Greedy-PL (also abbreviated \"PL\" in Figure 4)       | Teacher-student method    | XGBoost         | Confidence thresholding                              | 5 iterations                         |\n| UPS                                                 | Teacher student-model     | XGBoost         | Confidence thresholding + Uncertainty thresholding (with ensembles) | 5 iterations                         |\n| FlexMatch                                           | Teacher-student method    | XGBoost | Adaptive confidence thresholding (with Curriculum learning) | 5 iterations                         |\n| SLA                                                 | Teacher-student method    | XGBoost         | Confidence-based assignment with Optimal transport         | 5 iterations                         |\n| CSA                                                 | Teacher-student method    | XGBoost           | Confidence-based assignment and uncertainty (with ensembles) | 5 iterations                                   |\n\nFor all these baselines: we use the implementation provided by https://github.com/amzn/confident-sinkhorn-allocation.\nAlso note that all these methods are implemented as **teacher-student models** in the tabular experiments of Sections 5.1, 5.2, 5.3, 5.4, and that we do _not_ use FixMatch for these tabular experiments.\n\n**DIPS integration**: DIPS is integrated as a plug-in on top of these baselines, for **both labeled samples and pseudo-labeled samples selected by these baselines, _before each_ teacher-student iteration.**\nSince DIPS is a plug-in, it means that these baselines still select pseudo-labeled samples based on their respective unlabeled data selection mechanism (e.g. \"Confidence thresholding\"). DIPS is simply applied on top of them, for both labeled and pseudo-labeled data. That is why we refer to PL+DIPS for example in Figure 5 (i.e. we use the first baseline in the above table and plug-in DIPS on top). Similarly, in Figure 4, on the grouped bar plots, DIPS is plugged on top of each baseline (giving PL+DIPS, UPS+DIPS, etc.).\n\n---\n## In Section 5.2 (Image experiment)\n|Name of PL method | Type of PL implementation | Backbone model |  Pseudo-label selection method | Number of training steps |\n|------|------|-----|-----|------|\n|FixMatch| Continuous-training method |  WideResnet-18| Confidence thresholding| $1024 \\times 10^3$ gradient steps\n\n\n**DIPS integration**: DIPS updates the learning dynamics at every checkpoint. We define one checkpoint every $1024$ gradient steps. DIPS performs its selection of labeled and unlabeled data every $10$ checkpoints. For the unlabeled data, DIPS uses the pseudo-labels computed by the model at the beginning of every learning dynamics cycle. Then, after every $10$\ncheckpoints, DIPS curates the labeled and unlabeled data using its selector function and then regenerates the pseudo-labels used for the computation of learning dynamics on unlabeled data for the next learning dynamics cycle.\n\nNote that the frozen pseudo-labels used for the learning dynamics are not to be confused with the online pseudo-labels which are used to train the WideResnet model: those are updated after every gradient step of the model.\n\n\n_We hope this clarifies your point, please let us know if you have any other questions!_"
                    }
                },
                "number": 37,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3725/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700734540654,
                "cdate": 1700734540654,
                "tmdate": 1700734636937,
                "mdate": 1700734636937,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "i9EY3uXHkT",
                "forum": "eSO9quCgmz",
                "replyto": "XFXd1uG3wh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3725/Reviewer_yF2p"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3725/Reviewer_yF2p"
                ],
                "content": {
                    "title": {
                        "value": "Further discussion"
                    },
                    "comment": {
                        "value": "Dear authors,\n\nThanks for details! One last clarification I need before I re-read the paper to make a final decision. \n\nIn all examples you described for teacher student you reuse previously generated pseudo-labels, so that you could have in the training examples repeated samples but with different pseudo-labels generated by different teacher-student iteration. Correct?\n\nFor the FixMatch on images in Sec 5.2 you do not do this as it is continuous and we always regenerate PL with the current model state for the batch we train on right now. Correct?\n\nThanks again for all clarifications!"
                    }
                },
                "number": 38,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3725/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700735431512,
                "cdate": 1700735431512,
                "tmdate": 1700735431512,
                "mdate": 1700735431512,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]