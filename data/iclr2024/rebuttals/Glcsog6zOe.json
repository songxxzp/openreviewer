[
    {
        "title": "Tree-Planner: Efficient Close-loop Task Planning with Large Language Models"
    },
    {
        "review": {
            "id": "YPG1DoTkwK",
            "forum": "Glcsog6zOe",
            "replyto": "Glcsog6zOe",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3714/Reviewer_m5sK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3714/Reviewer_m5sK"
            ],
            "content": {
                "summary": {
                    "value": "Language model planning frameworks operate iteratively by generating one action per time-step, appending it back to the prompt (along with observation for closed-loop planning), and repeating the process until the task is executed. Such approaches are token-inefficient because of their repetitive use of prompt tokens (which includes the task, action-observation history, and in-context exemplars) for each action-step generation. Moreover, in closed-loop planning, such methods may often lead to infeasible action steps, thus, requiring corrective steps/replanning (local or global). In this paper, the authors propose a way to make closed-loop planning (i) more token-efficient (i.e. requiring fewer tokens) than an iterative planner and (ii) more replanning efficient (requiring fewer corrective steps). \n\nSpecifically, the authors propose to sample multiple plans from the LLM in a non-iterative and offline fashion (Plan Sampling). These plans are then merged into a tree-like structure to avoid repetitive actions (Action Tree Construction). Finally, these executable actions are selected by prompting the LLM with the task, current observation, and the history of executed actions (Grounded Deciding). The non-iterative plan sampling approach is more token-efficient. The tree-like structure helps to backtrack and select different steps in case of action failure without having to plan from scratch, avoiding large token costs."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* Overall, the paper is well-motivated. \n* Through a number of empirical and ablation studies on VirtualHome environment, the authors show how their approach is significantly more token-efficient compared to the general iterative closed-loop planning approaches. The authors also show improved performance wrt ProgPrompt, however, ProgPrompt is more token-efficient (Table 1)\n* The framework would be of significance to the planning and decision-making audience (especially in academia) who want to use the larger models for similar works."
                },
                "weaknesses": {
                    "value": "* Originality: Placing this paper in the context of existing works (like Tree-of-Thoughts & SayCan), much of the contributions seem to be merely engineering tweaks that do not contribute (significantly) on a more fundamental level. Considering how generating and planning over tree-like structures have been explored in the past in open-loop/offline planning [A][B], where planning trees are generated iteratively, while simultaneously grounding the actions, the only novelty here is to decouple the tree construction from the grounding (Grounded deciding) for token-efficiency. Backtracking is also something that has already been introduced in Tree-of-Thoughts, although not in a proper task-planing framework. \n\n* Clarity: From what I understand, the sampled plans were generated in one go (non-iteratively) to avoid higher costs. However, iterative planning is useful in avoiding compounding errors and it is unclear as to how the authors handle such errors in their non-iterative setup (see Questions). Certain details like the definition of \"corrections\" are somewhat ambiguous.\n\n[A] Reasoning with Language Model is Planning with World Model, Hao et al., 2023 \n\n[B] SayCanPay: Heuristic Planning with Large Language Models Using Learnable Domain Knowledge, Hazra et al., 2023"
                },
                "questions": {
                    "value": "1. When sampling plans (action sequences) non-iteratively, an error in one action step (something that diverges from the list of executable actions) can lead to subsequent actions being erroneous, leading to a so-called \"compounding effect\" as highlighted in [C]. This would lead to most of the sampled plans being infeasible. How do the authors address this problem?\n\n2. \"In terms of correction efficiency, TREE-PLANNER reduces the number of corrections by 37.99% and 40.52%, respectively:\" How do you define and measure an error \"correction\": For instance, it could be (i) a new action insertion; (ii) an action deletion; (iii) replacing one action with another generated action. Is it evaluated irrespective of the success rate (i.e. is it only measured for plans that have succeded in correctly executing the task or for any plan)?\n\nMinor comments: \n\na considerable part of the prompt \"is\" less \u2026\n\nFigure 4: Leave node -> leaf node\n\nMissing citation: SayCanPay (see [B] in Weaknesses)\n\n\n[C] Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents, Huang et al., 2022"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3714/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3714/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3714/Reviewer_m5sK"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3714/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698324849986,
            "cdate": 1698324849986,
            "tmdate": 1699636327914,
            "mdate": 1699636327914,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "sj2GhtCUZM",
                "forum": "Glcsog6zOe",
                "replyto": "YPG1DoTkwK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3714/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3714/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Part 1"
                    },
                    "comment": {
                        "value": "We are deeply grateful for your recognition of our paper's motivation, performance, and potential academic impact. Your positive feedback is highly encouraging. \n\n## Weaknesses\n\n**W1. [Originality: Placing this paper in the context of existing works (like Tree-of-Thoughts & SayCan), much of the contributions seem to be merely engineering tweaks that do not contribute (significantly) on a more fundamental level.]**\n    \nThank you for highlighting the importance of differentiating our work from existing literature. I would like to clarify and further emphasize the novelty and contributions of TREE-PLANNER:\n\n1. TREE-PLANNER fundamentally differs from the mentioned works in several aspects: a) Both ToT[1] and RAP[2] leverage LLMs for tree construction. However, these approaches do not tackle decision-making in specific physical environments, a key challenge TREE-PLANNER addresses. Our method is unique in its ability to handle decision-making within constrained environments; b) SayCan[3] and its subsequent developments, like Grounded Decoding[4], utilize hidden layer representations of LLMs. This information is often inaccessible to many researchers and practitioners. TREE-PLANNER, on the other hand, demonstrates an innovative application of black-box LLMs for efficient task planning; c) Another mentioned work, SayCanPay[5], is a concurrent study (submitted to arXiv one month before our submission deadline) and focuses on a different aspect of efficiency. While SayCanPay addresses plan length, our work is centered on token efficiency and error correction. We believe this distinction does not diminish the novelty of our paper. For a more detailed discussion of these differences and their implications, please refer to Section 6 of our paper.\n\n2. TREE-PLANNER introduces a novel perspective on token efficiency: Our work is, to the best of our knowledge, the first to identify and address the issue of token efficiency in LLM-based planning, especially in complex environments. This is a critical aspect, as token consumption directly impacts the feasibility and scalability of LLM applications in real-world scenarios.\n\n[References]\n    \n[1] Yao S, Yu D, Zhao J, et al. Tree of thoughts: Deliberate problem solving with large language models[J]. arXiv preprint arXiv:2305.10601, 2023.\n\n[2] Hao S, Gu Y, Ma H, et al. Reasoning with the language model is planning with the world model[J]. arXiv preprint arXiv:2305.14992, 2023.    \n\n[3] Brohan A, Chebotar Y, Finn C, et al. Do as I can, not as I say: Grounding language in robotic affordances[C]//Conference on Robot Learning. PMLR, 2023: 287-318.\n\n[4] Huang W, Xia F, Shah D, et al. Grounded decoding: Guiding text generation with grounded models for robot control[J]. arXiv preprint arXiv:2303.00855, 2023.\n\n[5] Hazra R, Martires P Z D, De Raedt L. SayCanPay: Heuristic Planning with Large Language Models using Learnable Domain Knowledge[J]. arXiv preprint arXiv:2308.12682, 2023.\n\n**W2.1 [Clarity: From what I understand, the sampled plans were generated in one go (non-iteratively) to avoid higher costs. However, iterative planning is useful in avoiding compounding errors, and it is unclear as to how the authors handle such errors in their non-iterative setup (see Questions).]**\n    \nThanks for your valuable feedback. Detailed discussion about how our method avoids compounding error is presented in response to **Q1**.\n\n**W2.2 [Certain details like the definition of \"corrections\" are somewhat ambiguous.]**\n    \nThanks for the opportunity to include more details about our research. We have included a detailed calrification on \"correction\" in the response to **Q2**."
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3714/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700472748597,
                "cdate": 1700472748597,
                "tmdate": 1700472748597,
                "mdate": 1700472748597,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "StXSGF9qyZ",
                "forum": "Glcsog6zOe",
                "replyto": "vOLPAHdjYG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3714/Reviewer_m5sK"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3714/Reviewer_m5sK"
                ],
                "content": {
                    "title": {
                        "value": "Further clarifications requested"
                    },
                    "comment": {
                        "value": "Thank you authors for taking the time to address the questions. \n\n* **Novelty**: I'm not convinced with this statement: *Our method is unique in its ability to handle decision-making within constrained environments*. This can be done with any online (closed-loop) planning framework like SayCan, or Grounded Decoding. The novelty here (compared to SayCan) is the corrective action that you enable, which is again nullified if you consider tree search frameworks like ToT. While ToT has not been demonstrated in the classical \"planning\" domain, one could easily extrapolate the reasoning tasks of ToT into classical planning. Also, I'm utterly confused with what you mean by *b) SayCan[3] and its subsequent developments, like Grounded Decoding[4], utilize hidden layer representations of LLMs. This information is often inaccessible to many researchers and practitioners. TREE-PLANNER, on the other hand, demonstrates an innovative application of black-box LLMs for efficient task planning*. In the end, what novelty remains is the improved token efficiency, I'd reiterate seems more like an \"engineering\" problem lacking significant novelty.\n\n* Regarding **compounding effect** (Q1): Again I disagree. Compounding effect arises when an action generated by the LLM does not adhere to any action in the action set of the environment. It is only practical because it is difficult to constrain the generation of LLM to the exact action space. This leads to the subsequent actions also getting affected and can be more pronounced if the plan is generated in one go. Indeed, the grounded deciding would help to ground the plans by selecting executable actions for each step. However, from what I understand, since all the plans were pre-generated, it is likely that compounding effect would lead to most plans being already infeasible. In the Zero-shot planner, they use a simple approach to map each generated action to some action in the action set of the environment."
                    }
                },
                "number": 27,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3714/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700520301914,
                "cdate": 1700520301914,
                "tmdate": 1700520301914,
                "mdate": 1700520301914,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RHxVerRNS9",
                "forum": "Glcsog6zOe",
                "replyto": "YPG1DoTkwK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3714/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3714/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Further Clarifications Part 1"
                    },
                    "comment": {
                        "value": "Thank you for your reply and valuable feedback.\n    \n**A. [Further Clarifications on Novelty]**\n\nWe appreciate your insights and the opportunity to clarify the novelty of our work.\n\n1. Tree-Planner shares similarities with previous methods, yet it distinctly advances. Other reviewers also acknowledge this improvement, as stated by #cPrG: \"*The approach itself can be regarded as an enhancement over previous methods. The basis for such improvements is well justified, targeting specific limitations rather than merely fine-tuning pre-existing models. This approach lends a unique originality to the work.*\"\n2. In response to \"*this can be done with any online (closed-loop) planning framework like SayCan or Grounded Decoding. The novelty here (compared to SayCan) is the corrective action that you enable*\": Compared to works that focus on online LLM-based planning like SayCan, Tree-Planner effectively combines offline planning (plan sampling module) and online planning (grounded deicidng module), but not merely incorporates action correction into classical online planning framework. Ultimately, this approach surpasses the performance (success rate) of the online LLM-based planning baselines (Zero-shot Planner[1] and Iterative Planner) even under without action correction setting. Furthermore, reviewer #mZgD also regards this approach as interesting: \"*Good idea: While for model-based planning, offline planning might be weaker than integrated planning and execution, which might differ for LLM-based planning. Obtaining a policy without the distraction of the incoming observations might allow a more systematic execution.*\"\n3. We respectfully disagree that *token efficiency* is solely an engineering problem for the following aspects:\n        \n    a) Copuational efficiency has been long been studied as a research problem in NLP community, where researchers focus on how to reduce the computational cost of transformers. Previous studies have explored modifications to the architecture of transformers [2][3] as well as the adoption of alternative sampling methods [4].\n    \n    b) LLMs' black-box nature impose constraints on the study of efficiency. It is often impractical and challenging to retrain LLMs after altering their model structure in the era of LLMs. Consequently, an increasing number of researchers have begun investigating the reduction of token consumption to achieve similar objectives [5][6][7]. The goal of token efficiency is to reduce the number of tokens consumed by LLMs, which is typically associated with computational costs (in cases where the number of API calls is similar, higher token consumption means longer inference time and higher computational cost). Research on token efficiency can be considered another form of efficient transformer research in the era of LLMs and has the potential to significantly enhance the practical applications of LLMs. \n    \n4. our paper also includes a formulation in Section 5.1 to showcase the token efficiency of our approach, going beyond relying solely on empirical results for demonstration.\n\n[Reference]\n\n[1] Huang W, Abbeel P, Pathak D, et al. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents[C]//International Conference on Machine Learning. PMLR, 2022: 9118-9147.\n    \n[2] Kitaev N, Kaiser \u0141, Levskaya A. Reformer: The efficient transformer[J]. arXiv preprint arXiv:2001.04451, 2020.\n\n[3] Katharopoulos A, Vyas A, Pappas N, et al. Transformers are rnns: Fast autoregressive transformers with linear attention[C]//International conference on machine learning. PMLR, 2020: 5156-5165.\n\n[4] Chen C, Borgeaud S, Irving G, et al. Accelerating large language model decoding with speculative sampling[J]. arXiv preprint arXiv:2302.01318, 2023.\n\n[5] Kaneko M, Okazaki N. Reducing Sequence Length by Predicting Edit Operations with Large Language Models[J]. arXiv preprint arXiv:2305.11862, 2023.\n\n[6] Cheng Z, Kasai J, Yu T. Batch prompting: Efficient inference with large language model apis[J]. arXiv preprint arXiv:2301.08721, 2023.\n\n[7] ICLR 2024 Submission360 \"BatchPrompt: Accomplish more with less.\" [Link](https://openreview.net/forum?id=Agyicd577r)"
                    }
                },
                "number": 28,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3714/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700560329528,
                "cdate": 1700560329528,
                "tmdate": 1700569216572,
                "mdate": 1700569216572,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "NEmlCZT85L",
            "forum": "Glcsog6zOe",
            "replyto": "Glcsog6zOe",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3714/Reviewer_cPrG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3714/Reviewer_cPrG"
            ],
            "content": {
                "summary": {
                    "value": "This paper revisits LLM-based close-loop task planning on a classical application benchmark (VirtualHome). \nPrevious LLM use has been faced with two major limitations, which are high token consumption and redundant error correction. The proposed approach, TREE-PLANNER, aims at addressing the above limitations by reframing LLM-based task planning with LLMs into three distinct phases: plan sampling, action tree construction, and grounded deciding. \nThis decomposition of queries into a single plan-sampling call ensures token efficiency, while factoring action trees facilitates backtracking and error correction.  The system is compared to two baseline systems: Zero-Shot planner and ProgPrompt, and surpasses the state-of-the-art on multiple criteria, each time by at least a few percentage points."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper adresses a competitive topic in the field of LLM for agents and its rationale is presented in a very compelling fashion. It is technically sound, with an excellent balance of technical description and use of supplementary material for examples. The approach itself can be seen as an improvement over previous approaches, but the grounds for such improvements are well justified, and address specific limitations rather than incremental tuning of pre-existing models, conferring originality to the work.\nVery good presentation of related work by structuring it through relevant topics (to the exception of LLM for traditional Planning). \nWell-presented technical insights, in particular the formalization of token consuption, or the analysis of error types.\nDetailed results, using a panel of criteria both technical and practical (i.e. cost), which are quite convincing."
                },
                "weaknesses": {
                    "value": "Although the paper includes a fairly comprehensive list of related works, it does not address some of the LLM-based planning work. For instance, while 'Tree of Thoughts' is referenced, there is no mention of 'Faithful CoT' [Lyu et al., 2023] and its suggestion for LLM to produce PDDL representations to be passed to a regular Planner (which would still support online planning or replanning via restarting). \nIt would be good to include such a discussion, or to justify not including it (e.g. on grounds of partial observability), provided such grounds effectively preclude adoption. \n\nLyu, Q., Havaldar, S., Stein, A., Zhang, L., Rao, D., Wong, E., Apidianaki, M. and Callison-Burch, C., 2023. Faithful chain-of-thought reasoning. arXiv preprint arXiv:2301.13379."
                },
                "questions": {
                    "value": "1) What guarantees scalability to domains comprizing a larger number of actions and greater tree perplexity?\n2) Could you discuss how different LLM could be used (GPT-4, or LLaMA) with a similar approach and whether the prompting strategy would be affected?\n3) Is partial observability really a salient property of this specific set of 28 actions?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3714/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3714/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3714/Reviewer_cPrG"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3714/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698851210067,
            "cdate": 1698851210067,
            "tmdate": 1699636327842,
            "mdate": 1699636327842,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "CwnakBpZrH",
                "forum": "Glcsog6zOe",
                "replyto": "NEmlCZT85L",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3714/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3714/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Part 1"
                    },
                    "comment": {
                        "value": "We are deeply grateful for your encouraging and insightful feedback, particularly your recognition of our paper's technical soundness and originality. Your appreciation of our formalization of token efficiency and the comprehensive nature of our quantitative and qualitative results is highly valued. We are also thankful for your acknowledgment of the balance we struck between technical detail, clear examples, and the structured presentation of our related work. Such feedback motivates us to further our research in this promising field. Thank you for your thorough and constructive review.\n\n## Weaknesses\n\n**W1. [Although the paper includes a fairly comprehensive list of related works, it does not address some of the LLM-based planning work.]**\n\nThank you for your valuable feedback. Integrating LLMs with traditional task planners (like PDDL planners) is an important aspect of LLM-based planning, and we will include a discussion of these papers in the next version of our manuscript. [1][2] proposes producing the PDDL description of a given problem and then leveraging a classical planner to find an optimal plan. However, such work typically relies on the existence of Domain PDDL, which is often unavailable in many scenarios. While [3] suggests using LLMs as world models to construct domain PDDLs, this process still necessitates manual intervention due to issues like LLM hallucination. Besides, this work generally assumes a full observation environment, whereas our paper focuses on the more challenging partial observation environments.\n\n[Reference]\n    \n[1] Lyu, Q., Havaldar, S., Stein, A., Zhang, L., Rao, D., Wong, E., Apidianaki, M. and Callison-Burch, C., 2023. Faithful chain-of-thought reasoning. arXiv preprint arXiv:2301.13379.\n    \n[2] Liu B, Jiang Y, Zhang X, et al. Llm+ p: Empowering large language models with optimal planning proficiency[J]. arXiv preprint arXiv:2304.11477, 2023.\n    \n[3] Guan L, Valmeekam K, Sreedharan S, et al. Leveraging Pre-trained Large Language Models to Construct and Utilize World Models for Model-based Task Planning[J]. arXiv preprint arXiv:2305.14909, 2023.\n    \n\n## Questions\n\n**Q1. [What guarantees scalability to domains comprising a larger number of actions and greater tree perplexity?]**\n\nWe greatly appreciate the insightful feedback provided. In response to this question, we will provide explanations from two distinct perspectives:\n\n(i) The stored commonsense knowledge within LLMs plays a crucial role in maintaining the tree perplexity within manageable limits. This is due to LLMs' ability to efficiently navigate and prune decision trees based on contextual understanding.\n\n(ii) Grounded deciding can also be combined with other search algorithms to address issues of greater tree perplexity. For instance, AlphaGo[1] uses Monte Carlo Tree Search (MCTS) to handle high tree complexity. We conducted additional experiments using the Best-First Search algorithm, as detailed in Appendix C.5. \n    \n[Reference]\n    \n[1] Silver, D., Huang, A., Maddison, C. et al. Mastering the game of Go with deep neural networks and tree search. Nature 529, 484\u2013489 (2016). https://doi.org/10.1038/nature16961"
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3714/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700472595671,
                "cdate": 1700472595671,
                "tmdate": 1700472595671,
                "mdate": 1700472595671,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bM0DgmuCS5",
                "forum": "Glcsog6zOe",
                "replyto": "vJAgv8OAoa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3714/Reviewer_cPrG"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3714/Reviewer_cPrG"
                ],
                "content": {
                    "title": {
                        "value": "Acknowledgement of Authors Response"
                    },
                    "comment": {
                        "value": "Dear Authors,\nThanks for your comprehensive response.\nI was not totally convinced by the statement about common sense being embedded in LLM (Q1), as this point is still debated, in particular the extent to which common sense can support other tasks such as planning, but you have provided a reference. \nI was also marginally convinced on partial observability, although I now accept some relevance to the 28-actions scenario.\nI found your answer to Q2 a valuable extension to the paper's result. \nHowever, I found your answer to Reviewer K1gw on comparison to search-based planning somehow surprising since heuristics can be derived automatically (that is the essence of modern search-based planning). \nConsidering these various aspects I see no argument to further raise my rating which is already positive."
                    }
                },
                "number": 34,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3714/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700699821768,
                "cdate": 1700699821768,
                "tmdate": 1700699821768,
                "mdate": 1700699821768,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "W0wuAMP6hZ",
            "forum": "Glcsog6zOe",
            "replyto": "Glcsog6zOe",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3714/Reviewer_K1gw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3714/Reviewer_K1gw"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes TreePlanner, an LLM-based algorithm for task planning. TreePlanner uses the LLM more efficiently without reducing performance. The main contributions are algorithmic and empirical. The key idea is to decompose the planner into two distinct stages of LLM use. In the first stage (plan sampling), an expensive set of calls to the LLM is used to sample a number of full plans. These plans are aggregated into a tree structure which is then searched over in the second stage of the planner. The main source of efficiency (wrt LLM costs) is that the second stage (grounded deciding) can use a simpler prompt to perform action selection (instead of action generation). Experiments on tasks from the VirtualHome domain indicate the method is far more efficient without any loss of performance, compared to strong baselines."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ The paper tackles an important and popular problem of leveraging LLMs for task planning. Progress here would likely be impactful and of interest to the community.\n\n+ The main ideas in TreePlanner are intuitively clear. The two step approach for efficient use of the LLM seems novel, to my knowledge.\n\n+ The results show that compared to the baselines, TreePlanner makes very efficient use of the LLM without losing performance."
                },
                "weaknesses": {
                    "value": "- While the method clearly shows good performance on the considered domains, the experiments could be improved to address important questions about the approach, namely prompt engineering and the most efficient use of an LLM in task planning. \n  - For example, how good is the current prompt used in the plan sampling step? Given that errors made here are currently catastrophic and the prompt needs to be task-specific, a careful analysis of the quality of this prompt would be interesting. \n  - Another open question is whether it's more efficient to restrict the use of the LLM to plan sampling alone. See the questions for more details. (I'd be open to adjusting my score based on the responses to these questions.)\n\n- The set of tasks considered are from a single domain (VirtualHome). This makes it difficult to evaluate the broader utility of the proposed ideas.\n\n- Some of the the implementation details of the prompts, especially the one used in grounded deciding, are not fully described. See the questions for more details."
                },
                "questions": {
                    "value": "- I was unable to identify what error information is included in the history of the prompt used for grounded deciding. Per Figure 2 and Appendix F.3, it seems like some information about previously failed actions seems to be included in the history part of the prompt. If yes, are failed actions restricted to those on the current trajectory from the root to the current node or are they a global history over the entire tree? Which variant is better and why?\n\n- How sensitive is overall performance (e.g., success rate) to the prompt used in Plan Sampling? Since the absence of valid / optimal plans in the constructed tree leads to severely degraded performance and LLM performance is known to be very dependent on its prompt, it'd be useful to understand how much performance might be gained from a better Step 1 prompt? \n  - A related (but harder) question, how does the choice of LLM affect performance (e.g., off-the-shelf black box vs fine-tuned, etc.)?\n\n- Would it be possible to include additional tasks beyond those in VirtualHome? For example, is Toolbench relevant here?\n\n- Given the paper's emphasis is on efficient use of the LLM, what performance improvement does the LLM offer in grounded deciding over other action selection mechanisms? For example, is it feasible to replace the use of the LLM in grounded deciding with \"classical\" action selection techniques (e.g., UCB, best-first heuristic search)? How might this perform? More generally, please discuss where exactly the LLM is required / beneficial over classical search-based planners."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3714/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698858587011,
            "cdate": 1698858587011,
            "tmdate": 1699636327638,
            "mdate": 1699636327638,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qomNQzrANA",
                "forum": "Glcsog6zOe",
                "replyto": "W0wuAMP6hZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3714/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3714/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Part 1"
                    },
                    "comment": {
                        "value": "We are immensely grateful to you for recognizing the importance and novelty of our TreePlanner approach and for appreciating its clarity and efficient performance compared to existing baselines. This positive feedback is greatly encouraging.\n\n## Weaknesses\n\n**W1.1 [While the method clearly shows good performance on the considered domains, the experiments could be improved to address important questions about the approach, namely prompt engineering and the most efficient use of an LLM in task planning. How good is the current prompt used in the plan sampling step?]**\n\nThank you for pointing out the potential improvement of our methodology. We have included additional experiments on varying prompting strategies. The corresponding result is presented in response to **Q2.1**. The experiment result shows that different prompts yield varying impacts, yet the prompts we currently employ maintain a sufficiently high level of effectiveness compared to others.\n\n\n**W1.2 [Whether it's more efficient to restrict the use of the LLM to plan sampling alone.]**\n\nThanks for your suggestion. Corresponding experimental results and discussion are presented in the response to **Q4** and **Appendix C**.\n\n    \n**W2. [The set of tasks considered are from a single domain (VirtualHome). This makes it difficult to evaluate the broader utility of the proposed ideas.]**\n    \nThanks for your valuable feedback. The answer to this question is presented in response to **Q3**.\n\n**W3. [Some of the the implementation details of the prompts, especially the one used in grounded deciding, are not fully described. See the questions for more details.]**\n\nThanks for the opportunity to include more details about our research. Corresponding details are included in the response to **Q1**.\n\n## Questions\n\n**Q1 [what error information is included in the history of the prompt used for grounded deciding. If yes, are failed actions restricted to those on the current trajectory from the root to the current node, or are they a global history over the entire tree? Which variant is better and why?]**\n\nThank you for the opportunity to provide more details. Error information typically refers to situations where an action cannot be executed due to its precondition not being met (e.g., unable to operate on objects inside a fridge before opening it with [Open] <fridge>). The error information included in the prompt used for grounded deciding is in the format of [trajectory 1], [error information 1], [trajectory 2], [error information 2], and so on, indicating a global history over the entire tree.\n\nWe chose this variant because we believe LLMs can optimize themselves implicitly with multiple failed trajectories. This is consistent with the viewpoint presented in [1]. LLMs can use error information as a feedback signal to improve their decision-making process and avoid repeating the same mistakes.\n\n[Reference]\n\n[1] Shinn N, Cassano F, Gopinath A, et al. Reflexion: Language agents with verbal reinforcement learning[C]//Thirty-seventh Conference on Neural Information Processing Systems. 2023."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3714/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700472308274,
                "cdate": 1700472308274,
                "tmdate": 1700472812067,
                "mdate": 1700472812067,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "sWImQStYhZ",
            "forum": "Glcsog6zOe",
            "replyto": "Glcsog6zOe",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3714/Reviewer_mZgD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3714/Reviewer_mZgD"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents an algorithm specialized in LLMs for interacting with an environment for solving tasks that require planning. In this context, the state, the observations and the actions are represented as text. \n\nIn previous work, called iterative-planning in the submission, When an action is executed, another call to a LLM appends the observation and attempts to obtain a new action. In the planning literature, that is close to what is called integrated planning&execution, as the agent must choose actions for achieving the goal, but observations might lead the agent to change the course of action completely.\n\nIn contrast, the paper follows a more restricted mode that can be more efficient under some circumstances: offline planning, then execution of the plan. This is the more studied case in the symbolic planning literature, where offline planning has received much more attention than execution.\n\nThe proposed algorithm \u001f\u2014Tree-Planner\u2014 is relevant for domains expressed in text, where a LLM could approximate a world model, and could produce a plan using knowledge widely available in text. \n\nWhile the task can be formulated as a POMDP, the initial phase \u2013called plan sampling\u2013 does not deal with further possible observations but just the general scene. Indeed, the samples are converted that represent common prefixes of the plans. So, the policy from plan sampling can be seen as a stochastic policy. That makes sense in some cases, but it generally cannot solve POMDPs where policies might need to map state distributions into actions, and update state distributions with new observations.\n\nAlways assuming the use of a LLM, the main hypothesis mentioned in the abstract are:\n1. Tree-planner might be more effective than alternatives like iterative planning by avoiding redundant error correction.\n2. Tree-planner might have a lower token cost.\n\nThe second hypothesis is true, almost by construction, as the algorithm only needs less information about actions and objects. The first hypothesis is only studied for VirtualHome, a domain of some interest. The main claim is that backtracking allows more effective recovery than appending the error and letting the LLM recover.\n\nThe paper shows how high-quality data was refined to allow the evaluation of metrics like success rate. \n\nThe experimental setup reports a comparison with baselines including iterative-planning, all using the proprietary LLM GPT-3.5.\n\nThe results show improvements in the 35 tasks of the dataset. For N=50 samples, the best possible GCR (goal condition recall) reaches 81.2% of the tasks."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "- Good idea: While for model-based planning, offline planning might be weaker than integrated planning and execution, that might be different for LLM-based planning. Obtaining a policy without the distraction of the incoming observations might allow a more systematic execution.\n- The paper is reasonably well-written.\n- The benchmark is interesting.\n- The LLM-based baselines are reasonable enough. Some readers might want to see Chain-of-Though or Self-reflection, but I think zero-shot and iterative replanning are enough.\n- The results using the ground truth plans (oracle planner) show that more samples lead to a smaller improvement, showing the initial phase tends to find more of the correct plan.\n- Using success rate is robust with respect to alternative ways to achieve the goal."
                },
                "weaknesses": {
                    "value": "- Not clear if the policy is grounded in the environment or in the LLM implicit distribution.\n\t- Section 3.1 mentions the initial observation as part of the prompt, but the prompts in section F include a long \u201c(observation)\u201d for iterative planning and the grounded-deciding phase, but **not** for plan sampling.\n\t- If a detailed initial observation is given, then it\u2019s possible that many of the actions are grounded on them, reducing the significance of seeing the algorithm as dealing with a POMDP.\n\t- If a detailed initial observation is **not** given, then the LLM might be retrieving possible courses of action given the initial description but not grounded in the environment.\n\t- Both scenarios diminish the apparent significance of the work.\n- No discussion of the systematicity of LLM-based planning.\n\t- LLM-based planning is a popular topic, but the increasing body of work might call for a more careful examination. Each paper in this direction is an opportunity to examine the problem and the methodology.\n\t- In general, planning seems easier in the \u201chappy path,\u201d where text with high likelihood matches what might work in an environment, but things can get more complicated quickly.\n\t- Consider the task of \u201ccalling Mary using the cell phone\u201d. A good plan is not about assuming that the cell phone is on a table but exploring the space until the cell phone is found. A diverse set of plans might include the more usual places, perhaps even mentioning multiple places in a single path. But it\u2019s possible that the cell phone is in the fridge or in the supermarket bag. \n\t- While the prompt in the grounded deciding phase includes the full observation, the super-market bag would be there, prompting to choose among a few actions to pass the responsibility of examining the supermarket bag and the freezer to the plan sampling phase.\n\t- It\u2019s possible that finding a cell phone might require samples beyond a fixed N, and no alternative is offered.\n\t- So, the **key underlying problem** is that a flexible plan dealing with observations **cannot** be found using a fixed number of samples. Some tasks might require fewer samples, and some might require more. That\u2019s precisely why planning algorithms use search.\n- Experimental significance: a single environment with only 35 tasks.\n\t- The dataset subsection explains that 35 tasks were used, but the tasks were not listed. The VH has many other tasks. The 35 tasks may be biased in a way that affects the significance of the observations.\n- Experimental significance: Only VH\n\t- There are other environments for testing these ideas. For instance, the reference below. The related work section must cover what other alternatives are used in related work and why it\u2019s a good idea to select only this environment.\n\t- (Jericho) Hausknecht, Matthew, Prithviraj Ammanabrolu, Marc-Alexandre C\u00f4t\u00e9, and Xingdi Yuan. \u201cInteractive Fiction Games: A Colossal Adventure.\u201d arXiv:1909.05398 [Cs], February 25, 2020. http://arxiv.org/abs/1909.05398.\n- In general, given the absence of task descriptions and their plans, it\u2019s hard to know whether the results are significant.\n\t- Even if we have the 35 tasks and their plans, the results might be an artifact of this particular environment. Perhaps the current version of GPT 3.5 is better at those household tasks than at navigating in a store to buy groceries.\n\nSecondary issues:\n\n- Goal Condition Recall might be a misleading metric\n\t- different subgoals might be easier than others\n\t- we don\u2019t know the structure of the subgoals \n- Confusions in the theoretical emphasis on POMDPs.\n\t- The hallmark of POMDPs, being a Markov decision process, is the Markov assumption: decisions can be made just by looking at the state without the history. However, the policy in section 2 includes the history of actions but perhaps not the observation history.\n\t- The emphasis on POMDPs is over-stated as the VH has high visibility while other environments feature more partial observability  (for instance, Jericho)\n- Global replan is non-comparable with other algorithms.\n\t- Whether it\u2019s possible to teleport to the initial state or not fundamentally changes the problem.\n- The notion of inverse actions is a form of symbolic knowledge, leaving the scope of the paper.\n\t- A more purist approach would task the LLM to undo the last k actions, perhaps offering the list of actions.\n\nMinor\n\n- The best GCR in section 5.2 is interesting, but GCR is not a good predictor of SR (success rate).\n\t- Actually, in Table 2, the gap between GCR and SR is higher for Tree-Planner than for other approaches.\n\t- Related: Table 3 reports 45.5% of errors due to missing correct plan.\n- It must be clarified if the LLM is called when there is only one possible action.\n\t- Some action trees in the appendix have a few actions with no branches. This affects the overall cost but not other statistics.\n- Alg 1 doesn\u2019t add much. Any algorithm for building a \u201ctrie\u201d data structure would work.\n\n\nRecommendations:\n\n- The paper should distinguish the world model from planning. Discussing the percentage of correct plans is useful but not actionable in a new domain as the set of possible plans in a domain is heavily unbounded.\n- The directions suggested in error analysis (re-sample, chain-of-thought, self-reflection) might increase the context size, reducing one of the advantages."
                },
                "questions": {
                    "value": "- What are some other environments where this method could be tested?\n\t- Why was only VH selected?\n- Given a fixed task, is the same prompt used for the four scenes?\n- Does the observation of plan-sampling include the full description of the room or just the type of room?\n\t- I understand that in VirtualHome, the top-down view allows the agent to see everything except what\u2019s hidden in containers like drawers.\n\t- The prompts in section F include a long \u201c(observation)\u201d for iterative planning and the grounded-deciding phase, but not for plan sampling.\n- Did you consider adding the observation history?\n\t- The list of actions with the observation context where they were made might need to be more informative.\n\t- Unless the observations are monotonic, meaning that after each action, the agent observes more and more. For instance, if once the agent opens a drawer, it would still see what\u2019s there.\n\t- Are the observations monotonic?\n- GCR metric: what\u2019s the distribution of the number of goal conditions? Are the number of goal conditions the same for each of the 35 tasks?\n- Sect 5.2\n\t- Did you randomize the results for table 2? A new seed might generate another set of plans.\n\t- What would be the results for SR? Table 1 shows how GCR grows with SR, but doesn\u2019t predict SR. Actually, the gap between GCR and SR is higher for Tree-Planner than for other approaches.\n\t- Related: Table 3 reports 45.5 of errors due to missing correct plan.\n- What is the inverse of all the actions? \n\t- The supplementary material lists them, please list the corresponding inverse, say how that information was obtained, and discuss how realistic it is for that information to be available or learned.\n- Branches in action trees.\n\t- Some node action trees have no branches. See leftmost path Fig 12. Is the LLM prompt for choosing one action or are they executed? Is this accounted in the cost of the predictions?\n- Please report the average number of branches for reaching the leaves for each of the 35 tasks. This is equivalent to aggregating sequences without branching into a single action. For instance, Fig 12 has 8 leaves with [1, 1, 2, 2, 3, 3, 2, 3] branches, average 2.125 branches per leave. So, the agent should make an average of 2.125 decisions when following that branch.\n- Please discuss the number of error corrections in Table 1 compared to the number of branches. Otherwise, it\u2019s hard to know whether 3.29 vs 1.86 is a significant difference.\n- Sect 4.1: Dataset: what are the 35 unique tasks?\n\t- VirtualHome has multiple programs for some tasks, not a single gold plan.\n\t- Did you create new scenes or used scenes in VH?\n\t- Are the gold truth plans associated to a task or to (task, scene)?\n\t- Please list the 35 tasks descriptions.\n\t- What is the distribution of the number of steps for the gold truth? Even better, provide this information for each task\n- Methodological: was N=50 selected using the 35 tasks?\n\t- Is it possible that examining the tasks leads to bias in the selection of N?\n- Sect 4.1: implementation.\n\t- Was majority vote only used for grounded-deciding? Please add a reference for the method.\n- Table 3: errors for Tree-planner N 25\n\t- Is this success rate, SR?\n\t- The false negative is misleading. In the appendix, the error explanation says that the environment reports that there is no keyboard. Are keyboards part of the possible objects? The observations in the prompt mention a computer but no keyboard.\n\t- The example of the keyboard should be an environment misunderstanding.\n\t- Please fix the tables, and refine the definition of false negative. Perhaps you might to add false negative to the missing correct plan, calling it \u201csemantically correct.\u201d\n- What are the error modalities with correction?\n- Sect C.1 describes performance across plan length. \n\t- Can you provide the statistics for other approaches?\n\t- What about kind of task?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3714/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698870820539,
            "cdate": 1698870820539,
            "tmdate": 1699636327525,
            "mdate": 1699636327525,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GMncHE9NNe",
                "forum": "Glcsog6zOe",
                "replyto": "sWImQStYhZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3714/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3714/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We are grateful for the reviewer's appreciation of our novel LLM-based planning approach and the paper's clarity. Your feedback is precious to us.\n\n## Weaknesses\n\n**W1. [Not clear if the policy is grounded in the environment or the LLM implicit distribution.]**\n\nThank you for highlighting this aspect. In our experiments, detailed initial observations were included in the prompt. About \"*the LLM potentially retrieving actions based on the initial description rather than being environmentally grounded*\": Firstly, while the LLM's initial plan sampling might not be directly grounded, it provides a broad search space. In the grounded deciding stage, the LLM can select environmentally grounded actions from this set. Secondly, the LLM is prompted to generate plans conforming to commonsense logic during the plan sampling stage, as indicated in \"*The temporal relationship between subtask sequences must adhere to commonsense logic.*\" (Appendix F). This approach ensures that the LLM considers dynamic environmental changes, reducing reliance on the initial state alone and enhancing the practical relevance of our model in dealing with POMDP scenarios. We hope these perspectives will be considered, and we sincerely appreciate the opportunity to clarify these points.\n\n**W2. [No discussion of the systematicity of LLM-based planning.]**\n\nWe appreciate your insightful observations regarding the systematicity and flexibility of LLM-based planning. Your concern about the fixed number of samples (N) in planning tasks is well-founded and highlights a crucial aspect of our research.\n\nWe have conducted additional experiments using GPT-4 to dynamically adjust the sampling number (N) based on the specific task. This approach allows the model to propose a more tailored sampling N and plan length, depending on the task's complexity and context.\n\nAs detailed in our response to **Q11.1**, these experiments revealed that while the model's prediction of the number of steps has a reasonable mean error (**3.19** steps), there is significant variability in performance across different tasks, as indicated by a standard deviation of **2.73** steps. This variability underscores the challenge of predicting task complexity with high accuracy.\n\nMoreover, we observed a moderate positive correlation between task complexity (measured by the number of steps) and the suggested Sampling_N. This finding aligns with the intuitive notion that more complex tasks require more samples for exploring a more comprehensive range of potential plans.\n\nIn conclusion, dynamically adjusting the sampling N based on the task is a promising method to address the concern you raised. However, it is clear that the LLM's capability to predict the number of steps accurately still requires further improvement. This could be achieved through advanced techniques such as prompt engineering and in-context learning, which we plan to explore in our future work. Your feedback has been invaluable in guiding these efforts and ensuring our methodology is robust and adaptable to various planning scenarios."
                    },
                    "title": {
                        "value": "Part 1"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3714/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700470511164,
                "cdate": 1700470511164,
                "tmdate": 1700472838375,
                "mdate": 1700472838375,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rhp1rUa5Vv",
                "forum": "Glcsog6zOe",
                "replyto": "sWImQStYhZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3714/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3714/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Part 9"
                    },
                    "comment": {
                        "value": "**Q11.1 [Sec 4.1 - Dataset: what are the 35 unique tasks? Please list the 35 task descriptions. What is the distribution of the number of steps for the gold truth? Even better, provide this information for each task]**\n\nAs the number of plans may vary across different scenes, we have calculated the average length of the gold plans in different scenarios. As shown in the table above, our dataset encompasses a highly diverse array of household tasks.\n\n| Task                           | Number of Steps | Suggested Sampling N | Predicted No.Steps | Number of Branches | Success Rate | Number of Corrections | Number of New Objects |\n|--------------------------------|-----------------|----------------------|--------------------|--------------------|--------------|-----------------------|-----------------------|\n| Watch TV                       | 7.5             | 10                   | 8                  | None               | None         | None                  | None                   |\n| Turn on light                  | 5.8             | 10                   | 6                  | None               | None         | None                  | None                   |\n| Go to sleep                    | 5.5             | 10                   | 4                  | None               | None         | None                  | None                   |\n| Brush teeth                    | 8.8             | 50                   | 12                 | None               | None         | None                  | None                   |\n| Clean toilet                   | 6.0             | 50                   | 12                 | 9.8                | 0.00         | 6.00                  | 255                   |\n| Wash monitor                   | 16.0            | 50                   | 10                 | 11.3               | 0.00         | 10.00                 | 202                   |\n| Shave                          | 8.0             | 50                   | 12                 | 9.9                | 0.50         | 1.00                  | 223                   |\n| Hang up jacket                 | 7.8             | 50                   | 8                  | 7.6                | 0.00         | 5.67                  | 216                   |\n| Read newspaper                 | 7.3             | 50                   | 10                 | 7.5                | 0.00         | 0.33                  | 217                   |\n| Sleep                          | 4.8             | 10                   | 4                  | 5.4                | 1.00         | 0.00                  | 201                   |\n| Put alarm clock in bedroom     | 8.0             | 50                   | 12                 | 7.0                | 0.00         | 6.00                  | 195                   |\n| Sit in chair                   | 5.0             | 10                   | 4                  | 5.7                | 0.17         | 0.00                  | 260                   |\n| Open bathroom window           | 4.0             | 10                   | 4                  | 4.7                | 1.00         | 0.00                  | 189                   |\n| Wash face                      | 22.8            | 50                   | 12                 | 10.4               | 0.67         | 1.67                  | 216                   |\n| Computer work                  | 15.0            | 100                  | 10                 | 12.3               | 1.00         | 2.00                  | 198                   |\n| Take nap                       | 4.8             | 10                   | 4                  | 6.6                | 0.92         | 0.75                  | 213                   |\n| Hand washing                   | 14.0            | 100                  | 10                 | 10.0               | 0.00         | 4.22                  | 219                   |\n| Clean bathroom                 | 14.0            | 50                   | 12                 | 15.8               | 0.00         | 5.17                  | 196                   |\n| Pick up phone                  | 4.5             | 50                   | 10                 | 4.4                | 0.50         | 0.42                  | 198                   |\n| Go to toilet                   | 5.0             | 10                   | 4                  | 5.9                | 0.58         | 0.42                  | 209                   |\n| Pick up spare change on dresser| 16.0            | 50                   | 6                  | 8.2                | 0.00         | 0.17                  | 195                   |\n| Wash sink                      | 14.0            | 50                   | 12                 | 8.9                | 0.00         | 4.67                  | 180                   |\n| Put on glasses                 | 6.0             | 50                   | 12                 | 6.1                | 1.00         | 0.00                  | 213                   |"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3714/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700471184867,
                "cdate": 1700471184867,
                "tmdate": 1700589146369,
                "mdate": 1700589146369,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1ilPeyVDNI",
                "forum": "Glcsog6zOe",
                "replyto": "sWImQStYhZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3714/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3714/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Part 10"
                    },
                    "comment": {
                        "value": "| Task                        | Number of Steps | Suggested Sampling N | Predicted No.Steps | Number of Branches | Success Rate | Number of Corrections | Number of New Objects |\n|-----------------------------|-----------------|----------------------|--------------------|--------------------|--------------|-----------------------|-----------------------|\n| Shut off alarm                 | 6.0             | 10                   | 4                  | 4.9                | 1.00         | 0.00                  | 188                   |\n| Drink                          | 4.8             | 50                   | 8                  | 7.8                | 0.67         | 1.67                  | 177                   |\n| Look at mirror                 | 5.0             | 10                   | 4                  | 5.5                | 1.00         | 0.33                  | 217                   |\n| Put on your shoes           | 8.0             | 50                   | 12                 | 6.9                | 0.00         | 1.00                  | 222                   |\n| Shampoo hair                | 13.0            | 50                   | 12                 | 9.2                | 0.00         | 1.33                  | 215                   |\n| Plug in nightlight          | 5.0             | 50                   | 8                  | 5.7                | 0.00         | 0.00                  | 161                   |\n| Get dressed                 | 10.0            | 50                   | 12                 | 10.1               | 0.22         | 0.11                  | 163                   |\n| Put egg on dining table     | 8.0             | 50                   | 12                 | 7.8                | 0.50         | 0.17                  | 194                   |\n| Charge phone                | 3.0             | 50                   | 12                 | 4.7                | 1.00         | 0.50                  | 194                   |\n| Make coffee                 | 11.0            | 50                   | 10                 | 7.6                | 0.22         | 3.44                  | 194                   |\n| Read novel in bed           | 12.0            | 50                   | 10                 | 9.8                | 0.67         | 5.50                  | 194                   |\n| Sit at dining table         | 6.0             | 10                   | 3                  | 8.2                | 0.00         | 1.67                  | 250                   |\n\n\n**Q11.2 [Sec 4.1 - VirtualHome has multiple programs for some tasks, not a single gold plan.]**\n    \nYes, during the construction of our dataset, we filtered out certain plans that could not be correctly executed within the environment. We ensured that the final gold plans in our dataset achieved a success rate of 1. We annotated one gold plan for tasks not included in the VirtualHome dataset, such as Charging Phone, Making Coffee, etc..\n\n**Q11.3 [Sec 4.1 - Did you create new scenes or used scenes in VH?]**\n\nWe modified existing scenes from VirtualHome to create new scenes for our study. We made minor environmental adjustments, like changing the states of objects and adding or removing objects. This was done to generate scenes related to, yet distinct from, the original VirtualHome scenes. Below is a table mapping our newly created scenes to the original VirtualHome scenes:\n\n| Tree-Planner Scene | VirtualHome Scene |\n|---------------|------------------------------------|\n| 1             | 2                                  |\n| 2             | 4                                  |\n| 3             | 5                                  |\n| 4             | 6                                  |\n\n**Q11.4 [Sec 4.1 - Are the gold truth plans associated to a task or to (task, scene)?]**\n    \nThe gold plans are associated to (task, scene). As is discussed in Q2, plans may vary depending on the scenes."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3714/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700471297833,
                "cdate": 1700471297833,
                "tmdate": 1700589088931,
                "mdate": 1700589088931,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Jx9UZYg146",
                "forum": "Glcsog6zOe",
                "replyto": "ekBtFzLEs9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3714/Reviewer_mZgD"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3714/Reviewer_mZgD"
                ],
                "content": {
                    "title": {
                        "value": "Thank you (weakness)"
                    },
                    "comment": {
                        "value": "ICLR Tree planner\n\nThank you for your comments, both addressing the weakness and my specific questions.\n\nLet\u2019s start with the weakness:\n\n\n**W1**: [Not clear if the policy is grounded in the environment or the LLM implicit distribution.] \n\nAs a detailed initial observation is given, then the question is whether\n\n> it\u2019s possible that many of the actions are grounded on them, reducing the significance of seeing the algorithm as dealing with a POMDP.\n\nI agree that VH is a POMDP. The answer agree that VH has more observability than Jericho[1] or Alfworld[2].\n\nThe question is what\u2019s the role of the objects in the first observation used during sample vs what appears during execution, given that the plans does not account for observations.\n\nPerhaps a way to evaluate this is to report this statistics for successful traces and gold truth: how many objects were in the initial observation, and now many objects appear later? Let\u2019s called **new objects** the ones that appear later. This question is about the original setting, not the one reported in Q4 about the observation history.\n\nThe lower the number of new objects, the less the problem required partial observability. I mean, there always could be another room or another drawer full of many objects, but if they are not relevant to the goal, then it doesn\u2019t makes the problem harder.\n\nThis statistic would be crucial to defend whether the algorithm is dealing with partial observability. How? If the success is highly correlated with a low number of new objects, that means that the algorithm is not dealing well with the cases where observations are needed. Of course, one additional factor is how complex is the plan, for instance in number of steps. I\u2019d need to think about how to show that, but it might be possible to make a figure where the x-axis is the number of steps, and the y-axis is proportional of new objects in the plan, both for the gold truth and the successful plans.\n\n**W2**: [No discussion of the systematicity of LLM-based planning.]\n\nI said:\n\n> The **key underlying problem** is that a flexible plan dealing with observations **cannot** be found using a fixed number of samples. Some tasks might require fewer samples, and some might require more. That\u2019s precisely why planning algorithms use search.\n\nThe answer commented that the longer the plan, the more samples are needed. That makes sense because the required tree-search size is exponential in the length of the solution. For a binary tree, if the solution is $100$, the full tree has $2^100$, so in principle the number of samples might need to be near $2^100$. Common sense might prune that tree, but the number of leaves would increase anyway.\n\nThe answer says:\n\n> We have conducted additional experiments using GPT-4 to dynamically adjust the sampling number (N) based on the specific task. \n\nCan you elaborate on this? Determining the number of leaves of the tree is as hard as planning. Using an external more powerful tool to obtain an estimated of the number of leaves seems like an external solution to a fundamental issue.\n\nMy key concern is that the whole notion of a *fixed* number of samples is fundamentally broken. Sure, you can report a number, but it doesn\u2019t tell me how to fix it.\n\nPerhaps it\u2019d be better if you argue that the results are for a fixed number, report the results, but elaborate further on what happens with other numbers like N=75, as you mentioned, or N=25. It\u2019d be good to know if even if the hyper-parameter is set externally, the contribution still holds.\n\nI\u2019ll process later the rest of the comments.\n\nThank you!"
                    }
                },
                "number": 26,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3714/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700519631040,
                "cdate": 1700519631040,
                "tmdate": 1700519631040,
                "mdate": 1700519631040,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8IzXVFKYIT",
                "forum": "Glcsog6zOe",
                "replyto": "sWImQStYhZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3714/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3714/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Further Response Part 2"
                    },
                    "comment": {
                        "value": "**W2: [No discussion of the systematicity of LLM-based planning.]**\n    \n1. Further elaboration on \"*We have conducted additional experiments using GPT-4 to dynamically adjust the sampling number (N) based on the specific task.*\": Specifically, we **prompt** GPT-4 to generate the **a)** Predicted number of steps for a specific task; **b)** Suggested sampling N. The prompt structure looks like: [Instruction],[Description of Tree-Planner],[Global Information],[Initial Observation],[In-Context Examples]. The [Global Information], [Initial Observation], [In-Context Examples] are the same as those in Appendix F.  [[Description of Tree-Planner] introduces methodology flow. The key part in [Instruction] is \"*You need to suggest the hyper-parameter sampling_n and predict plan length based on the given information.*\". What's more, we employed prompt engineering techniques, such as the chain-of-thought approach, to enhance LLM's performance.\n\n2. Utilizing GPT-4 for the dynamic adjustment of hyperparameters is an open exploration in addressing your posed issue: \u201c*So, the key underlying problem is that a flexible plan dealing with observations cannot be found using a fixed number of samples.*  Here, the LLM's commonsense knowledge also plays a role in configuring the model's hyperparameters. Given the challenge of annotating the optimal sampling_n for various tasks, it's difficult to establish an adaptive sampling_n using a training-based approach. So we think it's a good starting point to use GPT-4 for a training-free experiment.\n\n3. Our contributions remain valid across different fixed sampling numbers. As demonstrated in the corresponding table, we maintained superior performance over the iterative planner, while preserving high token efficiency under conditions of N=25, 50, and 75.\n    \n\n|                         | **Exec** \u2191 | **SR** \u2191 | **GCR** \u2191 | **Cost** \u2193 | **No.Correction** \u2193 |\n|:-----------------------:|:-----------:|:--------:|:---------:|:------------:|:-------------------:|\n|                         |             |          |           |              |                     |\n| **_w/o correction_**    |             |          |           |              |                  |\n| Iterative Planner   | 44.54\u00b16.09  | 27.04\u00b14.65| 33.25\u00b15.32| 5.12\u00b10.14    | N/A                 |\n| Tree Planner N=25        | **55.74**\u00b10.92|**28.33**\u00b11.18|**39.96**\u00b10.16| **2.39**\u00b10.44 | N/A             |\n| Tree Planner N=50        | 49.01\u00b15.67  | 28.14\u00b12.45| 35.84\u00b14.20 | 3.48\u00b10.04    | N/A                 |\n| Tree Planner N=75        | 47.44\u00b14.40  | 28.06\u00b12.82| 34.42\u00b12.88 | 4.27\u00b10.03    | N/A                 |\n|                         |             |          |           |              |                     |\n| **_with correction_**   |             |          |           |              |                     |\n| Iterative Planner   | 79.66\u00b12.33  | 37.46\u00b11.71| 51.9\u00b10.15 | 12.88\u00b10.17   | 3.29\u00b10.46           |\n| Tree Planner N=25       | 89.13\u00b10.17| 35.30\u00b11.78| 56.65\u00b11.09| **3.30**\u00b10.01| **1.85**\u00b10.05     |\n| Tree Planner N=50        | 88.26\u00b12.47  | 41.58\u00b13.20| 59.55\u00b13.20| 4.54\u00b10.16 | 2.04\u00b10.26        |\n| Tree Planner N=75        | **89.27**\u00b10.03  | **42.15**\u00b12.79| **60.12**\u00b14.20 | 5.36\u00b10.02    | 2.24\u00b10.58                 |"
                    }
                },
                "number": 32,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3714/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700671289943,
                "cdate": 1700671289943,
                "tmdate": 1700671331384,
                "mdate": 1700671331384,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]