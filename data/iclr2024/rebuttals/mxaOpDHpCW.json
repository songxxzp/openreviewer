[
    {
        "title": "Breadth First Exploration in Grid-based Reinforcement Learning"
    },
    {
        "review": {
            "id": "UUDqQLqwUW",
            "forum": "mxaOpDHpCW",
            "replyto": "mxaOpDHpCW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3445/Reviewer_SMnh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3445/Reviewer_SMnh"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces an algorithm for hierarchical planning. To plan the solution, the goal space is divided into a grid, and the shortest path is computed. During training, the reachability statistics for the visited edges are collected, and the graph is updated to reflect which transitions are achievable and which are not. This way, both exploration and the final quality of solutions benefit."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "As far as I understand, the main idea is simple yet effective, especially in environments with complex dynamics but simple goal space, such as various ant-mazes. Because the planning is done by searching on a graph, it can deal even with an arbitrarily long horizon."
                },
                "weaknesses": {
                    "value": "The description of the algorithm itself needs to be stated more clearly. How exactly do you collect the training data? Do you always choose the shortest path (according to the current state of the graph)? When do you switch subgoals? Do you have to replan at some point? It could be handled by either adding a small section that brings all the pieces in one place and describes the algorithm step-by-step or by adding a pseudocode. Without that, it is hard to understand your approach.\n\nSince your edges have different lengths, I suppose that you use Dijkstra to find the shortest path, instead of BFS as you claim.\n\nI agree that you show a clear advantage over DHRL, although the final difference is not very large. I suggest evaluating the algorithm on even harder instances to further highlight your superiority. Perhaps instances with a lot of blockers would do since you claim dealing with it to be one of your key strengths. How far can you push it, even in a toy environment?\n\nThe _Ablation study_ section seems a little ad-hoc. I suggest extending it or merging it with other experiments."
                },
                "questions": {
                    "value": "How exactly do you collect the training data? Do you always choose the shortest path (according to the current state of the graph)? When do you switch subgoals? Do you have to replan at some point?\n\nDo you ever remove edges from the graph, or is it simply enough to assign them $\\infty$ weight?\n\nHow do you handle the initial stage of the training, when the agent has to learn the basics, e.g. how to move? In particular, I'm afraid that initially the agent will be unable to reach _any_ subgoal (barely move), which should result in labelling many goals close to the initial state as _unreachable_. This is not entirely destructive, since even if all the subgoals adjacent to the starting positions have a weight of $\\infty$, the path will go through one of them anyway. However, this initial struggle may considerably bias the graph structure, which may affect further shortest paths (that will try to avoid the falsely failed areas). Is that indeed an issue and how do you handle it?\n\nAlso, apart from the initial bias, if any edge is marked with $\\infty$ (possibly due to underfitting in a yet underexplored area), is there _any_ way of including it back to the graph?\n\nIs it of any importance that you use a grid? I suppose it can be an arbitrary structure, like random samples. Does using the grid has advantages? It seems a little rigid.\n\nDuring planning, you always select the shortest path to the goal in the graph. Does it mean that the agent is committed to this rigid grid-like movement, even if it could move diagonally? Do you see a way to adjust the structure of the graph to eventually allow for near-optimal trajectories? At least, trajectories without considerable inefficiencies?\n\nIt seems to me that this approach is useful if the goal space is low-dimensional. For instance, in 2 dimensions, like ant-maze, it is pretty efficient. However, if we consider any more complex spaces, e.g. ant-maze with the goal space equal to the observation space (which is quite natural in many scenarios), will it still work?\n\nIf the goal space is reduced, then the reachability of adjacent subgoals depends on the state in which the agent reaches each point, e.g. the exact position of its limbs. In particular, it may depend on the path that it took. How do you handle that?\n\nIn general, **I really like the high-level idea**, but in its current form, it is very hard to understand the essential details. Thus, I'm willing to increase my rating if my concerns are addressed."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3445/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3445/Reviewer_SMnh",
                        "ICLR.cc/2024/Conference/Submission3445/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3445/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698256748898,
            "cdate": 1698256748898,
            "tmdate": 1700576160013,
            "mdate": 1700576160013,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "H13kfRVHgO",
                "forum": "mxaOpDHpCW",
                "replyto": "UUDqQLqwUW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3445/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3445/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer SMnh\n\nWe would like to express our sincere gratitude for your valuable comments and insightful feedback. Your comment has greatly enriched our work and provided us with valuable perspectives. We address your comments one by one in what follows.\n___\n\n**Q1 [Weakness1 & Question1]  The description of the algorithm itself needs to be stated more clearly. How exactly do you collect the training data? Do you always choose the shortest path (according to the current state of the graph)? When do you switch subgoals? Do you have to replan at some point? It could be handled by either adding a small section that brings all the pieces in one place and describes the algorithm step-by-step or by adding a pseudocode. Without that, it is hard to understand your approach.**\n\n**A1** We appreciate your meticulous reviews on the description of the proposed method, BEAG. Based on your questions and comments, we have substantial revised the draft (particularly, Appendix A) and added a comprehensive pseudocode of our algorithm, as you suggested. We individually respond to each question for clarification in the following:\n\n&nbsp; ***Q1-1. How exactly do you collect the training data?***\n\n&nbsp; ***A1-1.*** Initially, we collect the training data from the random rollout phase where the agent proceeds with actions entirely at random. After the random rollout phase, we collect the training data using the policy network with BEAG.\n\n&nbsp; ***Q1-2. Do you always choose the shortest path (according to the current state of the graph)?***\n\n&nbsp; ***A1-2.*** Yes, we always use the shortest path.\n\n&nbsp; ***Q1-3. When do you switch subgoals?***\n\n&nbsp; ***A1-3.*** We switch subgoals when we reach the subgoal. More precisely, when a subgoal falls within $\\delta$. (i.e., $\\lVert \\phi(s\u2019)-g \\rVert < \\delta$ )\n\n&nbsp; ***Q1-4. Do you have to replan at some point?***\n\n&nbsp; ***A1-4.*** We re-plan the path when we fail to achieve the subgoal in the failure condition threshold (100) step.\n___\n**Q2. [Weakness2] Since your edges have different lengths, I suppose that you use Dijkstra to find the shortest path, instead of BFS as you claim.**\n\n**A2.** \nIt appears there might be a misunderstanding. What we mean by breadth-first \u201cexploration\u201d is not aligned with the breadth-first search algorithm to compute the shortest path. It means that BEAG prefers broadly exploring bypasses for the currently unattainable subgoals than meticulously ransacking the local of them. We will further clarify the meaning of the breadth-first exploration in the revision. We thank the reviewer for pointing out the potential confusion of our expression.\n___\n**Q3. [Weakness3] I agree that you show a clear advantage over DHRL, although the final difference is not very large. I suggest evaluating the algorithm on even harder instances to further highlight your superiority. Perhaps instances with a lot of blockers would do since you claim dealing with it to be one of your key strengths. How far can you push it, even in a toy environment?**\n\n**A3.** \nAs requested, we have added an additional experiment and discussion, showcasing a clear advantage of BEAG over DHRL (See Figure 5 in Section 5.3). Specifically, we compare BEAG and DHRL in a U-maze environment, where the initial state and goal are fixed at the bottom-left and top-left corners, respectively. This environment is dedicated to evaluating the efficiency of exploration in various algorithms. BEAG reached the goal much faster than DHRL, thanks to the breadth-first exploration. The relatively small gap between BEAG and DHRL in the random goal setting may suggest that DHRL (and others) heavily relies on random goal generation for subgoal exploration. We hope the additional experiment and discussion address your question.\n___\n**Q4. [Weakness4] The Ablation study section seems a little ad-hoc. I suggest extending it or merging it with other experiments.**\n\n**A4.** \nWe appreciate your valuable suggestions on additional experiments. As suggested, in Figure 7, we conducted ablation studies on BEAG with different values of ($\\tau_t$: [50, 100, 150, 200]) and ($\\tau_n$: [1, 3, 5]).  As expected, both $\\tau_t$ and $\\tau_n$ (determining the thresholds to avoid subgoals) need to be set at minimal values to reduce attempts at impossible subgoals. However, exceedingly small values for $\\tau_t$ and $\\tau_n$ may accidentally identify attainable subgoals as impossible ones and possibly degenerate performances in early phases, as depicted in Figure 7 (d) and (e). This observation provides a useful guideline for the hyperparameter choice of our method. We hope that these additional experiments address the reviewer's concern.\n___\n**Q5. [Question 2] Do you ever remove edges from the graph, or is it simply enough to assign them $\\infty$ weight?**\n\n**A5.** \nFor ease of implementation and presentation, we simply indicate the removed edge with infinite weight."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3445/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700382596033,
                "cdate": 1700382596033,
                "tmdate": 1700382888964,
                "mdate": 1700382888964,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TonIjGAvbR",
                "forum": "mxaOpDHpCW",
                "replyto": "UUDqQLqwUW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3445/Reviewer_SMnh"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3445/Reviewer_SMnh"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the answer. I acknowledge the clarifications by increasing my rating."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3445/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700576131599,
                "cdate": 1700576131599,
                "tmdate": 1700576131599,
                "mdate": 1700576131599,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Ku1hslc4xv",
            "forum": "mxaOpDHpCW",
            "replyto": "mxaOpDHpCW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3445/Reviewer_z2kz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3445/Reviewer_z2kz"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a method which builds grid-level graphs for better sample efficiency in subgoal HRL. When building the graph, instead of estimating weights from the Q-function. The authors set edge weights using temporal distance based on the number of successful node visits. The method is also adaptive, which means the graph can fit different grained levels while planning in complex environments."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This work has a good motivation: to address the drawbacks of current methods in subgoal-finding process. Some key issues, such as the novelty of subgoals, the accessibility of these subgoals, and sample efficiency, are considered.\n\nThe design of experiments clearly shows the sample efficiency of BERG over DHRL."
                },
                "weaknesses": {
                    "value": "1. Experiments are not enough. The authors only compared their work with DHRL, while there exists many other graph-based HRL methods that are worth to compare with, such as HIRO, HAC, ... The effect of threshold hyperparameters are not shown. Also, the ablation study can focus on more aspects of BERG.\n\n2. The method is a minor upgrade of existing Graph-based methods. Although pure empirical evidence can show the effectiveness of the method, I am expecting to see some more theoretical analysis on why it may work."
                },
                "questions": {
                    "value": "1. Could the authors try some other baseline HRL benchmarks to further show the advantage of the proposed method?\n\n2. I would like to see more experiments on failure condition and count thresholds."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3445/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3445/Reviewer_z2kz",
                        "ICLR.cc/2024/Conference/Submission3445/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3445/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698339819973,
            "cdate": 1698339819973,
            "tmdate": 1700580007782,
            "mdate": 1700580007782,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "RARl2e5KkU",
                "forum": "mxaOpDHpCW",
                "replyto": "Ku1hslc4xv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3445/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3445/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer z2kz\n\nWe would like to express our sincere gratitude for your valuable comments and insightful feedback. Your comment has greatly enriched our work and provided us with valuable perspectives. We address your comments one by one in what follows.\n___\n\n**Q1 [Weakness 1] Experiments are not enough. The authors only compared their work with DHRL, while there exists many other graph-based HRL methods that are worth to compare with, such as HIRO, HAC, \u2026 . [Question 1] Could the authors try some other baseline HRL benchmarks to further show the advantage of the proposed method?**\n\n**A1.**\nAs requested, we have expanded our evaluation by incorporating additional baselines, namely PIG [1], HIGL [2], and HIRO [3], alongside DHRL [4]. BEAG outperforms all the baselines, not only graph-based RL approaches (DHRL, PIG, HIGL) but also the other hierarchical RL one (HIRO) while DHRL is the most competitive. We note that our evaluation fairly supports the superiority of BEAG since our choice of benchmarks is standard and comprehensive, aligning with those used by other works to justify their proposals. The observed superiority of BEAG stems from its unique ability to systematically identify and avoid impossible subgoals on the adaptive grid, a factor overlooked by other baselines. For a detailed comparison, please refer to Figure 4 and Section 5.2 in the revised draft, where we present a more comprehensive analysis of the results. We are confident that this expanded comparison offers a clearer understanding of BEAG's strengths, as well as addresses your concerns.\n\n\n[1] Junsu Kim, Younggyo Seo, Sungsoo Ahn, Kyunghwan Son, and Jinwoo Shin. Imitating graph-based planning with goal-conditioned policies. ICLR 2023.\n\n[2] Junsu Kim, Younggyo Seo, and Jinwoo Shin. Landmark-guided subgoal generation in hierarchical\nreinforcement learning. Advances in Neural Information Processing Systems, 34:28336\u201328349, 2021.\n\n[3] \u200b\u200bOfir Nachum, Shixiang Shane Gu, Honglak Lee, and Sergey Levine. Data-efficient hierarchical reinforcement learning. Advances in neural information processing systems, 31, 2018.\n\n[4] Seungjae Lee, Jigang Kim, Inkyu Jang, and H Jin Kim. Dhrl: A graph-based approach for long-\nhorizon and sparse hierarchical reinforcement learning.Advances in Neural Information Process-\ning Systems, 35:13668\u201313678, 2022.\n___\n**Q2. [Weakness2] The method is a minor upgrade of existing Graph-based methods. Although pure empirical evidence can show the effectiveness of the method, I am expecting to see some more theoretical analysis on why it may work.**\n\n**A2.** Thank you for the constructive comments. One of our contributions is to address the importance of avoiding consecutive attempts at impossible subgoals, which the literature of graph-based RL (including the most competitive baseline, DHRL) have been overlooked. In addition, we report an additional experiment and discussion, demonstrating a clear advantage of BEAG over DHRL in Figure 5 and Section 5.3 of the revised draft. This result underscores the efficacy of our main idea, the breadth-first exploration.\n\nAs you may understand, it is prohibitively challenging to theoretically analyze deep RL. Instead, in response to your request for a theoretical analysis of BEAG, we can provide a guarantee on the grid-based \u201cplanner\u201d. Specifically, we can show that given 2D Euclidean goal space, the grid pattern of $\\square$ has the worst-case sub-optimality ratio $\\frac{\\sqrt{2}}{2} \\approx 0.707$, and a denser pattern of $\\boxtimes$ improves it to $\\frac{\\sqrt{5}}{1+\\sqrt{2}} \\approx 0.926$ where ratio 1 implies the optimality. Regarding this, we will include a formal statement and proof as soon as possible.\n\nWe appreciate your comment as it provides the opportunity to clarify our contributions and drive an interesting theoretical analysis. \n___\n**Q3. [Weakness1] The effect of threshold hyperparameters are not shown. Also, the ablation study can focus on more aspects of BEAG. [Question 2] I would like to see more experiments on failure conditions and count thresholds.**\n\t\n**A3.** \nWe appreciate your valuable suggestions on additional experiments. As suggested, in Figure 7, we conducted ablation studies on BEAG with different values of ($\\tau_t$: [50, 100, 150, 200]) and ($\\tau_n$: [1, 3, 5]).  As expected, both $\\tau_t$ and $\\tau_n$ (determining the thresholds to avoid subgoals) need to be set at minimal values to reduce attempts at impossible subgoals. However, exceedingly small values for $\\tau_t$ and $\\tau_n$ may accidentally identify attainable subgoals as impossible ones and possibly degenerate performances in early phases, as depicted in Figure 7 (d) and (e). This observation provides a useful guideline for the hyperparameter choice of our method. We hope that these additional experiments address the reviewer's concern."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3445/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700382393202,
                "cdate": 1700382393202,
                "tmdate": 1700382393202,
                "mdate": 1700382393202,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4AwHXIiAHO",
                "forum": "mxaOpDHpCW",
                "replyto": "Ku1hslc4xv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3445/Reviewer_z2kz"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3445/Reviewer_z2kz"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the clarification and additional experiments. They help in a better understanding of the paper. I adjusted my score based on the revised version."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3445/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700579975426,
                "cdate": 1700579975426,
                "tmdate": 1700579975426,
                "mdate": 1700579975426,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "iLC0gg3TCN",
            "forum": "mxaOpDHpCW",
            "replyto": "mxaOpDHpCW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3445/Reviewer_fvhd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3445/Reviewer_fvhd"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates the issue of inefficient exploration of graph-based methods used in goal-conditioned reinforcement learning. The authors claim that the existing research tends to record achieved goals but overlook the unattained goals, which makes the algorithms struggle to get rid of repeatedly exploring these goals and causing waste. The main idea proposed in this paper is to leverage breadth-first exploration and multi-scale graph-construction to manage achieved and unattained goals."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper concentrates on an important and challenging problem of goal-exploration in GRL, which has not been well studied in the literature. \n- At least from the lens of writing, the authors propose a simple yet effective solution incorporating a forward-looking perspective and a proxy model for value estimation. \n- This paper is easy to follow."
                },
                "weaknesses": {
                    "value": "- As admitted by the authors, the proposed method heavily relies on the task structure (distance-based, discrete) to apply the graph construction to record the achieved and unattained goals. \n- The comparative baselines are limited on some environments. Only DHRL is involved. Since RL algorithms often perform high-performance variance in different environments/tasks, I think it is necessary to make comparisons with more baselines. And also, I think it would be better if the authors make comparisons on different environments. \n- The authors did not provide clear explanations or intuitions for some of the design choices, such as why we use Euler-distance instead of others as the $w_{i,j}$. \n- The illustration in Algorithm tables is unclear (e.g., some notations lack explanation), so it is hard to and even cannot follow that. \n- It is unclear how the edge weights work for the proposed method."
                },
                "questions": {
                    "value": "- Why does the comparison in Figures 4-6 not include the other baselines introduced in the Appendix? \n- Why does the coverage in Figure 7 decrease? \n- Why does the success rate in Figure 4(d) higher than 1? \n- Why does the learning curve in Figure 8(a) not start from 0 environment step?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3445/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3445/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3445/Reviewer_fvhd"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3445/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698649527607,
            "cdate": 1698649527607,
            "tmdate": 1700624569704,
            "mdate": 1700624569704,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "cvR1i48ceb",
                "forum": "mxaOpDHpCW",
                "replyto": "iLC0gg3TCN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3445/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3445/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer fvhd\n\nWe would like to express our sincere gratitude for your valuable comments and insightful feedback. Your comment has greatly enriched our work and provided us with valuable perspectives. We address your comments one by one in what follows.\n___\n\n**Q1. [Weakness 1]The proposed method heavily relies on the task structure (distance-based, discrete) to apply the graph construction to record the achieved and unattained goals.**\n\n**A1.** \nWe appreciate the opportunity to clarify the applicability of our approach. The proposed BEAG is designed to be versatile and can be applied to a broad range of environments where other graph-based RL frameworks are applicable. In essence, the main goal of studying graph-based RL is to develop a framework that fully leverages the distance-based goal structure, a characteristic shared by various practical systems such as navigation and robot-arm manipulation.\n\nGiven such a goal structure, BEAG not only applies but also offers distinct advantages, which are twofold: (i) recording achieved and unattained subgoals to avoid consecutive attempts at impossible subgoals; and (ii) selectively densifying the grid to adapt to diverse environments.\n\nNotably, an additional experiment (Figure 8) demonstrates the effectiveness of recording achieved and unattained subgoals, even in a variant of BEAG using a random graph over the entire goal space instead of a grid, while previous works are prone to obsessively attempting impossible subgoals as they rely on SoRB, subsampling subgoals only from a set of visited subgoals. In addition to this, BEAG using grids shows a clear benefit, compared to the variant with random graphs of irregularly distributed subgoals. We refer the reviewer to Section 5.5 for a comprehensive discussion.\n\nWe hope this clarification addresses concerns about the reliance on task structure and underscores the versatility and advantages of our approach across various goal-oriented environments.\n___\n**Q2. [Weakness 2] The comparative baselines are limited on some environments. Only DHRL is involved. Since RL algorithms often perform high-performance variance in different environments/tasks, I think it is necessary to make comparisons with more baselines. And also, I think it would be better if the authors make comparisons on different environments. [Question 2.] Why does the comparison in Figures 4-6 not include the other baselines introduced in the Appendix?**\n\n**A2.** As requested, we have expanded our evaluation by incorporating additional baselines, namely PIG [1], HIGL [2], and HIRO [3], alongside DHRL [4]. BEAG outperforms all the baselines, not only graph-based RL approaches (DHRL, PIG, HIGL) but also the other hierarchical RL one (HIRO) while DHRL is the most competitive. We note that our evaluation fairly supports the superiority of BEAG since our choice of benchmarks is standard and comprehensive, aligning with those used by other works to justify their proposals. The observed superiority of BEAG stems from its unique ability to systematically identify and avoid impossible subgoals on the adaptive grid, a factor overlooked by other baselines. For a detailed comparison, please refer to Figure 4 and Section 5.2 in the revised draft, where we present a more comprehensive analysis of the results. We are confident that this expanded comparison offers a clearer understanding of BEAG's strengths, as well as addresses your concerns.\n\n\n[1] Junsu Kim, Younggyo Seo, Sungsoo Ahn, Kyunghwan Son, and Jinwoo Shin. Imitating graph-based planning with goal-conditioned policies. ICLR 2023.\n\n[2] Junsu Kim, Younggyo Seo, and Jinwoo Shin. Landmark-guided subgoal generation in hierarchical\nreinforcement learning. Advances in Neural Information Processing Systems, 34:28336\u201328349, 2021.\n\n[3] \u200b\u200bOfir Nachum, Shixiang Shane Gu, Honglak Lee, and Sergey Levine. Data-efficient hierarchical reinforcement learning. Advances in neural information processing systems, 31, 2018.\n\n[4] Seungjae Lee, Jigang Kim, Inkyu Jang, and H Jin Kim. Dhrl: A graph-based approach for long-\nhorizon and sparse hierarchical reinforcement learning.Advances in Neural Information Process-\ning Systems, 35:13668\u201313678, 2022."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3445/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700381949220,
                "cdate": 1700381949220,
                "tmdate": 1700382947637,
                "mdate": 1700382947637,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ElETilEs5C",
                "forum": "mxaOpDHpCW",
                "replyto": "iLC0gg3TCN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3445/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3445/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Q3. [Weakness 3] The authors did not provide clear explanations or intuitions for some of the design choices, such as why we use Euler-distance instead of others as the $w_{i,j}$**\n\n**A3.** \nThank you for highlighting the need for clarification in our method. In response to your request, we have added an explanation in Section 4.1  to clarify why we opted for Euclidean distance as the weights. We bring it for your convenience:\n\n>We assume that the cost of moving between two subgoals at an equal distance is similar, and thus, we use the Euclidean distance as the weights of the edges. To generalize to diverse environments, if the aforementioned assumption does not hold, the distance measure introduced in Equation 1 can be employed as the weights of the edges. However, it is noteworthy that the Q-function may exhibit instability in predictions for untried transitions, potentially hindering exploration.\n\nFurthermore, our method can adopt alternative edge weights. We are currently conducting an additional experiment using the value difference (Equation 1) for edge weight. We commit to promptly incorporating these results, while we anticipate observing nearly identical advantages of the proposed method. We thank the reviewer for the comment, providing this opportunity to explore other design options and to enhance the transparency of our design choices.\n___\n**Q4. [Weakness 4] The illustration in Algorithm tables is unclear (e.g., some notations lack explanation), so it is hard to and even cannot follow that.  [Weakness 5] It is unclear how the edge weights work for the proposed method**\n\n**A4.** \nFollowing your comments, we revised the algorithm in Appendix A and Section 4 for a detailed explanation of our method.\n- change the notation of graph $\\mathcal{G}$ into $\\mathcal{H}$ for clarification\n- change the notation $a_j, s_j$ into $n_j^a, n_j^s$ in equation (2) for clarification and align them in the algorithm.\n- add the simple explanation of notation in the algorithm.\n- add the algorithm for training and graph construction.\n- add the training details (e.g. initial random stage). \n\nAccording to your comment on the description of the edge weights in BEAG, we have revised Section 4.1. For the sake of clarity, in what follows, we elaborate on how the edge weights work for BEAG. \nEach edge weight is uniformly initialized with an interval $n$. During an episode, if an assigned subgoal remains unachieved for $\\tau_t$ steps, the edges connected to the unattained subgoal in that episode are disconnected by assigning infinite weights to encourage the exploration of new paths.\nIf such failures have been continued in the previous $\\tau_n$ episodes, the edges will remain disconnected (while the refinement procedure can revive a part of them). Otherwise, they are reconnected in the next episode. \n___\n**Q5. [Question 2] Why does the coverage in Figure 7 decrease?**\n\n**A5.** \nWe note that Figure 7 in the submitted paper has been removed in the revised one. The decrement is just temporal and the coverage is recovered in a few epochs. We present the raw data for Figure 7 in the below table. We hope these have addressed your concern.\n\n|Env steps| 129K | 219K | 309K | 399K | 489K | 579K | 669K | \n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n|Coverage| 0.154 | 0.696 | 0.821 | 0.868 | 0.775 | 0.882 | 0.954 |\n___\n**Q6. [Question 3] Why is the success rate in Figure 4(d) higher than 1?**\n\n**A6.** We note that Figure 4(d) in the submitted paper has been replaced with Figure 4(e) in the revised one. The shaded region in our plots visualizes the standard deviation centered around the average. Hence, it is possible to observe the shaded region of the success rate curve higher than 1, while there are no sample instances exceeding 1.\n___\n**Q7. [Question 4] Why does the learning curve in Figure 8(a) not start from 0 environment step?**\n\t\n**A7.**\nIn response to your question, we report the curve starting from the 0 environment step. The revised figure can be found in Figure 4(a) in the revised draft (while it was Figure 8(a) in the submitted paper). BEAG begins with a roll-out phase where the agent proceeds with actions entirely at random without training. This is to obtain a certain size of replay buffer and is also employed in DHRL. In our plots, the success rates of BEAG and DHRL are set to zero over the roll-out phase."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3445/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700382209971,
                "cdate": 1700382209971,
                "tmdate": 1700382979064,
                "mdate": 1700382979064,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "h7wtzJ7eet",
                "forum": "mxaOpDHpCW",
                "replyto": "ElETilEs5C",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3445/Reviewer_fvhd"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3445/Reviewer_fvhd"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your response"
                    },
                    "comment": {
                        "value": "I thank the authors for their detailed response, and their efforts resolve most of my doubts. However, I still have a concern about Figure 4. As illustrated, the success rate is higher than 1 or lower than zero. I know the shaded region indicates a standard deviation, but this plot surely relies on what your original data is. Thus, I want to know how the authors calculate this measurement, it would be better in formulation."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3445/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700624546875,
                "cdate": 1700624546875,
                "tmdate": 1700624546875,
                "mdate": 1700624546875,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zQTzhR2I0P",
                "forum": "mxaOpDHpCW",
                "replyto": "iLC0gg3TCN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3445/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3445/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the positive response. To adderss your remaining concern on our presentation of success rate curves (which should be bounded in [0,1], *instance-wisely*), we have made the following revisions:\n* We have included plots of all the raw data, instance-wisely, in Appendix B.\n* To reduce the potential misunderstanding, we have modified all the figures plotting success rate curves (Figure 4, 5, 6, 7, 8), where we clip the values of (average+-1 standard deviation) of success rates inbetween [0,1]. Apprantely, this cliped presentation is standard in previous works, e.g., HIGL and PIG. Additionally, we would like to emphasize that the computation of the success rate employs the same formula as used in these previous works\n \nWe believe these additional plots and modifications will address any concerns or uncertainties you may have had. Please let us know if further clarification or adjustments are needed."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3445/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700732595042,
                "cdate": 1700732595042,
                "tmdate": 1700732721812,
                "mdate": 1700732721812,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "8qlewauXjm",
            "forum": "mxaOpDHpCW",
            "replyto": "mxaOpDHpCW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3445/Reviewer_qmXA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3445/Reviewer_qmXA"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents an approach for constructing a graph to augment current graph-based planners for goal-conditioned RL, specifically, using a grid that matches the dimension of the goal space and running a shortest path algorithm on the grid graph. In addition, a local adaptive grid refinement and goal shifting are proposed to appropriately choose grid interval and explore unattempted nodes, respectively. Experiments were performed in a number of different tasks in MuJoCo environments and results were compared with state-of-the-art approaches."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper addresses a practical subproblem present in current graph-based planners for goal-conditioned RL, that is of wasting attempts to unattainable subgoals.\n\n- The proposed approach is technically sound with the use of a grid and running of shortest path algorithm, and the addition of the grid refinement and goal shifting.\n\n- The experiments include a comprehensive analysis with comparison with other state-of-the-art methods included in the appendix, as well as ablation studies, showing positive results.\n\n- The presentation is clear with a logical structure in introducing the different concepts."
                },
                "weaknesses": {
                    "value": "- while grid is a way to have a systematic representation of the space, there are other approaches in planning that are present in the literature, including sampling-based approaches that can address similar problems. It is worth discussing the rationale behind choosing grid, as that is not necessarily the primary choice for high-dimensional spaces (which the paper mentions to leave it for the future).\n\n- As Reacher3D is the scenario where results are fairly close, goals could be set at different locations to report the coverage too, unless there is a specific reason not to have different goals also for Reacher3D.\n\nSome minor comments on presentation:\n- \"embarassing\" can be substituted with \"a very low performance\" to be more formal and technical, rather than introducing a subjective judgement.\n- \"we propose graph planning method dealing with the above problem in Section 4\" -> \"we propose graph planning method dealing with the above problem in Section 4.\"\n- please ensure that all symbols are introduced, i.e., before Eq. (2), worth mentioning \" ... edge weights w_{i,j} for nodes v_i, v_j as follows\". Also the symbol for the nodes set should be introduced, when the graph is introduced. \n- \"The coverage is the averaged success rate of uniformly sampled goals, in which we sample 10 goals per unit-size cell over the\nentire goal space to make the sampling more uniform. The coverage represents the average success rate for the entire goal\" The two sentences are including redundant information, so they could be merged.\n- \"Bottleneck-maze where challenging for BEAG\" -> \"Bottleneck-maze which was challenging for BEAG\"\n- in Fig. 8, worth changing the max value on the y axis, so that the success rate is completely visible for the bottom row."
                },
                "questions": {
                    "value": "Please  see the first two points included in the Weaknesses box."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3445/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3445/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3445/Reviewer_qmXA"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3445/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698834335145,
            "cdate": 1698834335145,
            "tmdate": 1699636296598,
            "mdate": 1699636296598,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "YvnvoOpWBr",
                "forum": "mxaOpDHpCW",
                "replyto": "8qlewauXjm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3445/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3445/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer qmXA\n\nWe would like to express our sincere gratitude for your valuable comments and insightful feedback. Your comment has greatly enriched our work and provided us with valuable perspectives. We address your comments one by one in what follows.\n___\n\n**Q1. [Weakness 1] While grid is a way to have a systematic representation of the space, there are other approaches in planning that are present in the literature, including sampling-based approaches that can address similar problems. It is worth discussing the rationale behind choosing grid, as that is not necessarily the primary choice for high-dimensional spaces (which the paper mentions to leave it for the future)**\n\n**A1.**  \nThanks for the suggestion. As you mentioned, it is crucial to manage subgoals including not only visited but also unattained ones, in order for avoiding successive attempts to impossible subgoals. To support such subgoal managements, one can also consider a random sampling over the entire goal space (noticing that this random graph now contains unattained ones and thus differs from SoRB used in the previous works). However, thanks to the regularity of grid, the grid-based one is more efficient than the random one in terms of the number of subgoals covering the goal space. Indeed, in our additional experiment (Figure 8), the grid-based management (without adaptive refinement) outperforms the random-sampling. (See Section 5.5 for a comprehensive discussion.) Besides this, the regularity of grid inherently provides the ease of implementing the breadth-first exploration upon adaptive refinement, whereas it is somewhat non-trivial to devise a mechanism of adaptive refinement on the random graph due to the irregular dense, although it seems not impossible.\n\nWe note that graph-based RL algorithms share the reviewer\u2019s concern about the scalability in high-dimensional goal spaces. However, this concern can be addressed by the proposed adaptive refinement, starting from a sufficiently sparse grid and selectively densifying the grid. The grid also provides the aforementioned efficiency from the regularity. In summary, this strategic combination of adaptive refinement and grid structure is designed to offer a scalable solution to high-dimensional goal spaces, as well as the efficiency gained from the regularity of the grid.\n\nWe appreciate the opportunity to further clarify these points and invite any additional feedback or questions from the reviewer.\n___\n\n**Q2. [Weakness2] As Reacher3D is the scenario where results are fairly close, goals could be set at different locations to report the coverage too, unless there is a specific reason not to have different goals also for Reacher3D.**\n\n**A2.** We will provide the requested plot comparing the average coverage in Reacher3D, where we believe the main message drawn from the success rate is not different to that from the average coverage.\n___\n**Q3. Minor comments on the presentation**\n\n**A3.** We appreciate your valuable feedback on the presentation of our work. Specifically, we acknowledge the need for improvement regarding the provocative expression used to describe the performances of the baselines that were initially excluded. Accordingly, we have revised the manuscript to include a clear and comprehensive presentation of results, encompassing the complete set of baselines. We extend our sincere apologies for any unintended tone in the initial description. We have diligently incorporated all of your suggestions into the revised draft."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3445/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700381743437,
                "cdate": 1700381743437,
                "tmdate": 1700381743437,
                "mdate": 1700381743437,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "m3rnHTN57Z",
                "forum": "mxaOpDHpCW",
                "replyto": "YvnvoOpWBr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3445/Reviewer_qmXA"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3445/Reviewer_qmXA"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the response and for updating the paper. Overall the paper appears improved. One note: there are sampling-based approaches that includes bias sampling towards the boundaries of the obstacles, thus improving a vanilla random sampling for bottleneck scenarios."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3445/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700581386088,
                "cdate": 1700581386088,
                "tmdate": 1700581386088,
                "mdate": 1700581386088,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]