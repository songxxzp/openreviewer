[
    {
        "title": "Random Walk Diffusion For Graph Generation"
    },
    {
        "review": {
            "id": "PS87Uqq7Xt",
            "forum": "IL9o1meezQ",
            "replyto": "IL9o1meezQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5640/Reviewer_sVdq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5640/Reviewer_sVdq"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a novel graph generative model for modeling very large graphs. The main idea is to sample random walks, which generate candidate edges for the current graph t, then a GNN will run on such a noisy graph and prune out some candidate edges. Such iteration is repeated until the graph is finalized. The key intuition of the paper is to use the sparsity of the graph obtained from the random walks, which can be handled properly by the GNN. To obtain those random walks, the paper proposes to use the BERT-like model to sample node id sequences."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The idea is pretty novel and it's interesting. The model tackles the dense graph generation problem elegantly by using two sets of model (random walk and GNN refinement."
                },
                "weaknesses": {
                    "value": "1. Notation is unlcear. For example, it is not clear how equation 2 is computed, and what's the model like. When the authors say the random walks are sampled by the BERT-like model, it's not clear whether it's permutation-invariant or not since the BERT model doesn't come with the causal mask but with positional embedding.\n\n2. it's not clear how the V_start is reset using node degree information.\n\n3. In my understanding, the random walk sampling doesn't utilize any structural information of the (previous) generated graph. It must take the node feature information as input in order to work well. In this case, the proposed method can only generate graphs conditioning on node features. Since node features are not considered by NetGAN and EDGE, I suggest the author should highlight the difference.\n\n4. Based on my comment 3, the author should also add graphite as one of the baselines.\n\n5. I believe the Edge Overlap is the lower the better. If a model works well with high EO, it is simply because it memorizes the original graph structure. I don't think the comparison against the baseline is totally fair -- Arrow-diff has 50% EO while the others have ~1% EO\n\n6. Is the result reported in EDGE and NetGAN trained with node features? if so, can you provide details about how the model is modified to do so?\n\n7. Time complexity analysis is missing. It should be explicitly analyzed to demonstrate why the method is so efficient, this involves showing the detail of the used models (for random walk and GNN)."
                },
                "questions": {
                    "value": "See weakness, I'd like to change my rating if the concerns are addressed."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5640/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5640/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5640/Reviewer_sVdq"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5640/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698518781953,
            "cdate": 1698518781953,
            "tmdate": 1699636586263,
            "mdate": 1699636586263,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "U9rfTLqMlv",
                "forum": "IL9o1meezQ",
                "replyto": "PS87Uqq7Xt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5640/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5640/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the thorough feedback. We read the comments carefully and will consider them in the revised version. Following are our answers to the specific questions.\n\n**Question 1:**\n\nEquation 2 gives the log-likelihood of time step $t$ for the Order Agnostic Autoregressive Diffusion Model [1] (OA-ARDM). It is derived from Equation 1 (Section 2 \u201cBackground\u201d in our paper). The details of this derivation can be found in Hoogeboom et al. [1] in paragraph \u201cOrder Agnostic ARDMs\u201d in Section 3 of their paper. We are happy to provide more information if needed.\n\nAbout our model (see Section 5.1): We use a U-Net architecture similar to Ho et al. [2] with one ResNet block and two levels for the down- and up-sampling processes. For the graph generation process, we train a 2-layer GCN to classify edges into valid/invalid ones based on perturbed versions of the input graph. We are ready to provide more clarification if needed.\n\nPlease also check out our code in the supplementary material. It is well documented and gives the correct dimensionalities of tensors and should help to understand e.g. the computation of Equation 2.\n\nCould you please clarify whether you mean \u201cpermutation-invariant\u201d or \u201corder-agnostic\u201d here? More specifically, our model is order-agnostic and can generate random walks in any order. This is ensured by the order-agnostic training paradigm, in which for each sampled permutation of a random walk $D-t+1$ tokens are masked out and then predicted. This is what Hoogeboom et al. [1] refer to as BERT-like training.\n\n**Question 2:**\n\nThe new start nodes $V_\\text{start}$ in line 11 of Algorithm 2 are sampled from $V$ independently from each other using a Bernoulli distribution, where sampling a \u20181\u2019 corresponds to including a node $n$ to $V_\\text{start}$ and sampling a \u20180\u2019 corresponds to not including $n$ to $V_\\text{start}$. Each node $n \\in V$ has its own probability $p(n)$, which is used to sample from the Bernoulli distribution and is computed as follows:\n\n1. Compute the element-wise (or node-wise) differences in node degrees between the original graph $G$ and the current (at generation step $l$) generated graph $\\hat{G}$: $\\quad\\boldsymbol{d}_\\Delta := \\boldsymbol{d}_G - \\boldsymbol{d}\\_\\hat{G}$\n\n2. Compute the element-wise ReLU of $\\boldsymbol{d}_\\Delta$ : $\\quad \\boldsymbol{d} := \\max(0, \\boldsymbol{d}\\_\\Delta)$\n\n3. Compute the element-wise (node-wise) probabilities $p(n)$ for sampling node $n$ into $V_\\text{start}$ by dividing each entry in $\\boldsymbol{d}$ by the maximum entry in $\\boldsymbol{d}$: $\\quad p(n) = d_n / \\max(\\boldsymbol{d})$\n\nNote: The values of $p(n)$ do not sum to 1, i.e. $\\sum_{n \\in V} p(n) \\neq 1$, because each value $p(n)$ gives the probability whether to include node $n$ into $V_\\text{start}$ or not, i.e. $p(n)$ gives the probability of sampling a \u20181\u2019 from a Bernoulli distribution and $1 - p(n)$ gives the probability of sampling a \u20180\u2019 from a Bernoulli distribution.\n\nWe thank the reviewer for bringing this important point to our attention. This was not properly described in the paper. We will update the paper accordingly.\n\n**Question 3:**\n\nThere is a typo in our Algorithm 2 in reflecting this. Thank you for pointing this out. The GNN of our iterative procedure does indeed use the structure (edges) of the generated graph from the previous step $l-1$. We will change it in the paper accordingly.\n\nNode features are not considered by NetGAN and EDGE. We explain more about that in our answer to your related question 6.\n\n**Question 4:**\n\nWe plan to add the Graphite baseline model to our evaluation in the revised paper.\n\n**Question 5:**\n\n\"I believe the Edge Overlap is the lower the better.\" - This strongly depends on the type (and desired properties) of the graph to generate and thus is a matter of interpretation. There is for sure a trade-off between the edge overlap and the novelty of a generated graph. However, this trade-off is not yet well explored in the literature. Nonetheless, given the performance of our method on the rest of the metrics, we think we are not totally overfitting but rather seeing the novelty of the generated graphs.\nHere, it is worth mentioning that although we report high EO on the Cora-ML dataset, the performance on other metrics does not reflect better quality (here compared to EDGE). At the same time, we report a lower EO on DBLP but better performance on the rest of the metrics. This shows that the EO and performance are not necessarily correlated.\n\n\n---\n[1] Emiel Hoogeboom, Alexey A. Gritsenko, Jasmijn Bastings, Ben Poole, Rianne van den Berg, and Tim Salimans. Autoregressive diffusion models. In International Conference on Learning Representations, 2022.\n\n[2] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840\u20136851, 2020."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5640/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700067182002,
                "cdate": 1700067182002,
                "tmdate": 1700067182002,
                "mdate": 1700067182002,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KoeXkvx1YS",
                "forum": "IL9o1meezQ",
                "replyto": "PS87Uqq7Xt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5640/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5640/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Question 6:**\n\nNetGAN does not use node features. The method samples random walks from the training graph and trains an LSTM-based Generative Adversarial Network (GAN) for random walk generation. EDGE does not use the node features of the training graph as well.\nWe will make sure to highlight this in the paper.\n\nThe code of all baselines in the paper was not modified in a way that would change the behavior and methodology of the baselines. Nevertheless, the data loading parts in the code of the baselines were adapted for loading the datasets in our paper.\n\n**Question 7:**\n\nWe will provide a comprehensive time complexity analysis as part of the revised paper."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5640/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700068330041,
                "cdate": 1700068330041,
                "tmdate": 1700068330041,
                "mdate": 1700068330041,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Ml5gDKW1vp",
            "forum": "IL9o1meezQ",
            "replyto": "IL9o1meezQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5640/Reviewer_ynZg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5640/Reviewer_ynZg"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a novel graph generation approach by designing the diffusion models on random walks. It uses an order agnostic autoregressive diffusion model to sample random walks from a given graph, and then uses a graph neural network to predict the edge set in the final graph. The paper claims that this approach can generate high-quality graphs that are similar to real-world graphs and can scale to very large graphs efficiently."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.\tThe paper introduces a new perspective of applying diffusion models to graph generation by developing a diffusion process on the random walks.\n2.\tThe paper demonstrates the scalability and flexibility of the proposed approach, which can handle both directed and undirected graphs, and both single-graph and multi-graph datasets.\n3.\tThe paper provides extensive experiments and comparisons with several baselines on various graph metrics, showing the superiority of the proposed approach in terms of quality and speed."
                },
                "weaknesses": {
                    "value": "1.\tIt is better for the authors to provide qualitative examples or visualizations of the generated graphs, which would help to illustrate the effectiveness and diversity of the proposed approach.\n2.\tBased on my understanding, the effectiveness and efficiency of the proposed methods are both sensitive to the parameter L. The authors should provide the necessary analysis on the selection of this parameter. Intuitively, when using the large L, the inference time would increase linearly. \n3.\tIn Table 2, the performance of the proposed method seems to vary a lot on graphs with different sparsity. It is better for the authors to provide more empirical analysis."
                },
                "questions": {
                    "value": "Please provide several visualizations of the generated graphs. \nHow does the parameter L influence the performance?\nIs there a significant correlation between the performance of the proposed method and the sparsity of graphs? What is the relationship?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5640/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698573375022,
            "cdate": 1698573375022,
            "tmdate": 1699636586164,
            "mdate": 1699636586164,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "trMxm72XLA",
                "forum": "IL9o1meezQ",
                "replyto": "Ml5gDKW1vp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5640/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5640/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their feedback.\n\n**Weakness 1:**\nWe are providing a visualization of the generated graphs from our method as well as all baselines in the appendix in the revised version of our paper.\n\n**Weakness 3:**\n\nOur calculations show that the adjacency matrices of the graphs in the single graph datasets have a density percentage between 0.04% - 0.23%. However, our results (in terms of the metrics) do not seem to vary a lot. For example, the sparsity of Cora is 0.04% and the sparsity of CiteSeer is 0.21%, but the results on these two datasets do not show a lot of variation."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5640/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700654763875,
                "cdate": 1700654763875,
                "tmdate": 1700688820087,
                "mdate": 1700688820087,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "twczovNvny",
            "forum": "IL9o1meezQ",
            "replyto": "IL9o1meezQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5640/Reviewer_Yei3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5640/Reviewer_Yei3"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a generative model to generate edges of a graph. It is based on Order Agnostic AutoRegressive Diffusion Models (OA-ARDM) that capture dependencies of the input and are robust to the order in which intermediate samples are generated. The approach is an adaptation of OA-ARDM (Hoogeboom et al., 2022) to graphs where each sample at some diffusion step t is a random path of size t in the graph. The model is trained by masking those paths and predicting them."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "In terms of methodology, the proposed approach is a straigthforward adaptation of (Hoogeboom et al., 2022) to sequences made of random walks in a graph. The paper is well-written and the method seems to obtain train faster than baselines assuming that the number of iterations (i.e. sampled paths/random walks) is relatively small and their length is short."
                },
                "weaknesses": {
                    "value": "1) At inference time, the method relies on generating a sufficient number of paths with appropriate length. Assuming that the training graphs are densely connected (i.e. a large number of pairs of nodes are connected by an edge), the number of edges in the graph is quadratic in the number of nodes, and the number of paths increases significantly as their length increases. It is unclear how the proposed method is robust and representative of all the possible paths in the graphs. In particular, the method would miss a lot of edges if it does not sample enough random walks.\n\nSimilarly, during training, the method only considers local information in the graph (i.e. random edge sequences) and not its global structure. Therefore, for large dense graphs, the number of edges becomes quadratic in the number of nodes and it might be difficult to sample enough random walks to be representative of the connectivity in the graph. \n\n2) In terms of evaluation, the purpose of the task described in Section 5.2 (graph generation from a single-graph dataset) is not clear to me. The purpose of graph generation is usually to generate multiple diverse graphs that follow the same distribution as the graphs forming a dataset. If the training set contains a single graph, then it is not clear what the distribution of the training set should be other than a singleton. A naive baseline for the task would be an autoencoder whose decoder always returns the same graph as the training graph. It also seems that the proposed method corresponds to a (denoising) autoencoder that simply tries to reconstruct the edges of a single graph in the setup of Section 5.2. This would explain why the proposed method outperforms the baselines in this setup. \n\nThe evaluation with the reported baselines then seems unfair since many baselines are trained to promote novelty and non-uniqueness (i.e., generating graphs that are not in the training set, and diverse). \nThe scores reported in Table 2 seem to promote overfitting over a single sample/graph and does not really reflect the generative power of the different methods.\n\n3) The experiments in Section 5.3 and Table 3 are not convincing either. Digress and EDGE seem competitive with respect to all the evaluation metrics except training time (which may be reduced with more computing power). In generative models, an interesting metric is the inference time to generate new samples. The novelty and uniqueness score are not reported either.\n\n4) The number of nodes of the generated graph is the same as the number of nodes of the training graph."
                },
                "questions": {
                    "value": "1) What are the novelty and uniqueness scores of the different methods in Table 3?\n\n2) How important is initialization for the generation process? Assuming that the number of steps L and the number of initial sampled random walks M are both large, do different sets of random walks tend to return the same value of p(n) in Step 11 of Algorithm 2?\n\n3) The main purpose of using multiple GNN steps at inference time is to evaluate the degrees of nodes. Do you have ablation studies to see the impact of M and L for training? If M becomes large, then the training takes longer but does it improve generation scores? Same question for the length of the paths.\n\n4) How does the method deal with permutation invariance? Assuming that the dataset contains multiple graphs that are all isomorphic (e.g. the graph with the edges (1, 2) and (2, 3) is isomorphic with the graph containing (3, 1) and (1, 2), or (1, 3) and (3,2)), will the values of p(n) converge to some canonical representation or will p(n) = 1/3 for all n in this case?\n\n5) Same question for when the graph contains different nonisomorphic graphs, how does the method deal with permutation of nodes? How does the GNN differentiate the node IDs between different graphs?\n\n\nMinor detail. The paper should mention that m in step 3 of Algorithm 1 is a Boolean mask."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5640/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698697299532,
            "cdate": 1698697299532,
            "tmdate": 1699636586058,
            "mdate": 1699636586058,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "dMvlTsiSLc",
                "forum": "IL9o1meezQ",
                "replyto": "twczovNvny",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5640/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5640/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the thorough feedback. We read the comments carefully and will consider them in the revised version. Please find the answers to the specific questions below.\n\n**Weakness 1:**\n\nDuring training, the number of sampled random walks for each trained model (whether in the single or multiple-graph setting) was not fixed a priori, rather we sample a \u2018batch_size\u2019 number of random walks in every epoch and use early stopping as criterion to stop the training. During sampling, given that our iterative sampling process is guided by the change in node degree distribution, this means that the densely connected nodes will have higher probability to be chosen as start nodes to sample from for the next iteration. However, the goal of generating new graphs is not to duplicate the edges of the original graph, but rather to create graphs with a similar structure that are captured by the wide range of evaluation metrics.\n\nThe robustness of our evaluation metrics is ensured by repeating the sampling process for 10 times. We thank you for pointing this out. We will include the standard deviations of all metrics across these 10 runs in the Appendix.\n\nIt is worth mentioning that the citation graphs that we use are the largest and most dense graphs used in the graph generation literature so far [1, 2].\nAs for capturing local vs. global information: Similar to NetGAN we also use the random walk sampling strategy described in Grover & Leskovec [3], which would ensure that we also capture global structural information from the input graph.\n\n**Weakness 2:**\n\nSince obtaining a set of graphs that all come from the same distribution is hard for real-world data, the task of generating new graphs using a single training graph becomes vital, e.g. to classify real vs. fake graphs like social networks [4]. Here the task is reduced to learning the specific structural information of a graph (evaluated in terms of metrics like clustering coefficient, power-law exponent, etc.). To do this we propose to learn this structural information by modeling random walks sampled from the training graph and then using the walks that are generated via a denoising diffusion model to build the final graph in an iterative procedure. Other methods that use a single graph for learning the generative process include NetGAN and EDGE. \n\nGiven the performance of our method on the rest of the metrics, we think we are not totally overfitting, but rather seeing the novelty of the generated graphs. Here, it is worth mentioning that although we report high EO on the Cora-ML dataset, the performance on other metrics do not reflect better quality (here compared to EDGE). At the same time, we report a lower EO on DBLP but better performance on the rest of the metrics. This shows that the EO and performance are not necessarily correlated.\n\n**Weakness 4:**\n\nIndeed we have mentioned this as a limitation of our method in Section 6 of our paper.\n\n**Question 2:**\n\nAssuming we want to generate two graphs using the same training graph for the diffusion model and GNN: If for two sets of random walks $A$ and $B$ the edges inside $A$ lead to the nodes of the generated graph $G_A$ having the same node degrees as for a generated graph $G_B$, the values of $p(n)$ would be the same for every node in both graphs for different generation processes.\nIf in two different (following) steps of the same generation process the edges in the proposed random walks from the diffusion model do not lead to new edges (e.g. if all are marked as invalid by the GNN), then the values of $p(n)$ would be the same in these two iterations.\nBoth cases remain true whether $L$ and $M$ are small or large.\n\n---\n\n[1] Aleksandar Bojchevski, Oleksandr Shchur, Daniel Z\u00fcgner, and Stephan G\u00fcnnemann. NetGAN: Generating graphs via random walks. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 610\u2013619. PMLR, 10\u201315 Jul 2018.\n\n[2] Xiaohui Chen, Jiaxing He, Xu Han, and Li-Ping Liu. Efficient and degree-guided graph generation via discrete diffusion modeling. arXiv preprint arXiv:2305.04111, 2023.\n\n[3] Grover, A. and Leskovec, J. node2vec: Scalable feature learning for networks. In Proceedings of the SIGKDD international conference on Knowledge discovery and data mining, pp. 855\u2013864, 2016.\n\n[4] Chakrabarti, D. and Faloutsos, C. Graph mining: Laws, generators, and algorithms. Computing Surveys (CSUR), 38(1):2, 2006."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5640/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700234126117,
                "cdate": 1700234126117,
                "tmdate": 1700647745049,
                "mdate": 1700647745049,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nbXwmnDdCe",
                "forum": "IL9o1meezQ",
                "replyto": "twczovNvny",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5640/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5640/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Weakness 3:**\nThe reason behind the lower performance in the multiple-graph setting is due to the fact that we train one model for each graph in the training dataset as was suggested by You et al. [2]. Thus, these models do not share parameters. For a fair comparison, we do not report the sampling time but rather focus on the advantage of our method on training time in the multi-graph setting. This is because, in our current training strategy, we need to sample multiple graphs from each trained model, it is something we would like to improve in our future work. However, the superiority of our method in sampling time is reflected in the first setting of our experiments (on single large graphs). \n\n**Question 3:**\n\nThe purpose of using GNN during inference is to check for plausible edges in the generated graph based on what it had learned from the original graph.\n$L$ and $M$ are only used in the graph generation process (Algorithm 2). They are not used for training of the diffusion model or the GNN.\nWe do not sample multiple random walks per node for each training loop of the diffusion model. We control the training time (iterations) using different numbers of epochs with early stopping.\nRegarding the lengths of the random walks: We fixed $D = 16$ for the graph generation experiments on single graph datasets following the insights from Bojchevski et al. [1] and to ensure a fair comparison. In the multi-graph setting we experimented with $D \\in \\{8, 12\\}$, where $D = 12$ led to the best results.\n\n**Questions 4 and 5:**\n\nIn our multi-graph setting we train one model (diffusion model and GNN) for each training graph in the multi-graph training set as suggested by You et al. [2], i.e. when generating a new graph, $p(n)$ is independent across the models that were trained on the different graphs. Thus, also the node IDs of different graphs do not affect each other.\nThe diffusion model only predicts the node indices in the random walks, our model is not affected by the ordering of nodes (which is random, but fixed before training) and is thus permutation invariant.\n\n---\n\n[1] Aleksandar Bojchevski, Oleksandr Shchur, Daniel Z\u00fcgner, and Stephan G\u00fcnnemann. NetGAN: Generating graphs via random walks. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 610\u2013619. PMLR, 10\u201315 Jul 2018.\n\n[2] Jiaxuan You, Rex Ying, Xiang Ren, William Hamilton, and Jure Leskovec. GraphRNN: Generating realistic graphs with deep auto-regressive models. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 5708\u20135717. PMLR, 10\u201315 Jul 2018."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5640/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700234225876,
                "cdate": 1700234225876,
                "tmdate": 1700654493227,
                "mdate": 1700654493227,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "zOmVgA9YoS",
            "forum": "IL9o1meezQ",
            "replyto": "IL9o1meezQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5640/Reviewer_zEev"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5640/Reviewer_zEev"
            ],
            "content": {
                "summary": {
                    "value": "Existing diffusion-based graph generation models are designed for generating small graphs and suffer from scaling to large-scale graphs. To scale diffusion-based models to generate large graphs, this work proposes ARROW-Diff, Auto Regressive RandOm Walk Diffusion, for graph generation based on random walk diffusion. ARROW-Diff leverages order-agnostic Autoregressive Diffusion Models (OA-ARDMs) to sample random walks from the training graphs, and further encompasses an iterative procedure that generates the final graph by utilizing a Graph Neural Network (GNN) model to filter out invalid edges from sampled random walks. Experiments on both a single, large-scale graph setting and the setting of multiple, small graphs demonstrate the efficiency and scalability of ARROW-Diff."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This work scales the diffusion-based approaches for large-scale graph generation in a random walk diffusion fashion. Inspired by node2vec, ARROW-Diff utilizes OA-ARDMs to sample random walks from training graphs, and then leverages an iterative procedure to construct the final graph based on an edge classification task directed by node degrees from the sampled random walks. The idea is natural and the ARROW-Diff framework is sound.\n\n2. The authors did a great job of introducing the background and related work. The illustration of the ARROW-Diff procedure, especially Figure 1, is clear and easy to understand.\n\n3. Regarding experiments on the single, large-scale graph setting, ARROW-Diff outperforms the existing graph generation baselines by a certain margin in terms of the runtime, and also generates graphs with higher quality than baselines in terms of relatively more complex graph metrics (e.g., triangle count, edge overlap)."
                },
                "weaknesses": {
                    "value": "1. The main contribution of this work is leveraging OA-ARDMs to generate large-scale graphs efficiently in a random walk diffusion fashion. Although ARROW-Diff demonstrates its efficiency and scalability empirically, the contribution in terms of the idea is not very novel, and the authors did not give any theoretical justification (e.g., time complexity analysis) for why ARROW-Diff is more efficient than existing graph generation methods.\n\n2. Although ARROW-Diff is able to generate graphs much more efficiently than baselines in the setting of multiple, small graphs, the quality of generated graphs by ARROW-Diff does not outperform baselines in terms of all three metrics. \n\n3. (Minor) I did not find any appendix for this work in the supplementary materials. It would be great if the authors could further describe the details of the implementation of ARROW-Diff and the experiments."
                },
                "questions": {
                    "value": "1. Regarding the first point in the Weaknesses, I wonder if the authors could provide a more theoretical analysis of the complexity of ARROW-Diff.\n\n2. Regarding the second point in the Weaknesses, I wonder if the authors could discuss further whether ARROW-Diff is able to generate high-quality graphs in the single, large-scale graph setting while not outperforming baselines in the settings of multiple, small graphs.\n\n3. Recently, there is another paper related to diffusion-based graph generation [1]. With empirical analysis revealing that permutation-invariant diffusion models are harder to learn than their non-permutation-invariant counterparts, [1] proposes a non-permutation-invariant diffusion model for graph generation. I wonder if the authors could discuss about permutation-invariant vs non-permutation-invariant models for the graph generation.\n\n[1] Yan, Q., Liang, Z., Song, Y., Liao, R., \\& Wang, L. (2023). Swingnn: Rethinking permutation invariance in diffusion models for graph generation. arXiv preprint arXiv:2307.01646."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5640/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5640/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5640/Reviewer_zEev"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5640/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698775389680,
            "cdate": 1698775389680,
            "tmdate": 1699636585948,
            "mdate": 1699636585948,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "QqggoO4b0n",
                "forum": "IL9o1meezQ",
                "replyto": "zOmVgA9YoS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5640/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5640/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the thorough feedback. We read the comments carefully and will consider them in the revised version. Please find the answers to the specific questions below.\n\n**Weakness 3:**\n\nThe details of our method are described in Algorithm 2, and in Section 5.1 \u201cARROW-Diff Model Training And Sampling\u201d. The details of all experiments are described in Sections 5.2 and 5.3. The corresponding hyperparameters for our models and the baselines can be found in the supplementary material. Please let us know what kind of further details you would like to know.\n\n**Question 1 (and Weakness 1):**\n\nWe will provide a comprehensive theoretical time complexity analysis in the updated version of the manuscript.\n\n**Question 2 (and Weakness 2):**\n\nThe reason behind the lower performance in the multiple-graph setting is due to the fact that we train one model for each graph in the training dataset as was suggested by You et al. [1]. Thus, these models do not share parameters. Afterwards we sample 10 graphs from each trained model for the evaluation process. This is something we would like to explore in depth in the future. Nonetheless, we wanted to highlight the training speed of our model compared to state-of-the-art multi-graph generation methods.\n\n**Question 3:**\n\nMany existing methods for graph generation try to learn the underlying structure of graphs by modeling the adjacency matrix. Of course,  a process for handling permutation invariance is vital, but node ordering schemes that were suggested in previous methods to handle this problem become really challenging for very large graphs as is the case for GraphRNN [1] and GRAN [2]. In our proposed approach we instead learn the underlying structure of graphs by modeling the context of nodes in random walks by leveraging the order-agnostic autoregressive diffusion training. As the diffusion model only predicts the node indices in the random walks, our model is not affected by the ordering of nodes (which is random, but fixed before training) and is thus permutation invariant.\n\n---\n\n[1] Jiaxuan You, Rex Ying, Xiang Ren, William Hamilton, and Jure Leskovec. GraphRNN: Generating realistic graphs with deep auto-regressive models. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 5708\u20135717. PMLR, 10\u201315 Jul 2018.\n\n[2] Renjie Liao, Yujia Li, Yang Song, Shenlong Wang, Will Hamilton, David K Duvenaud, Raquel Urtasun, and Richard Zemel. Efficient graph generation with graph recurrent attention networks. Advances in neural information processing systems, 32, 2019."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5640/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700068607557,
                "cdate": 1700068607557,
                "tmdate": 1700068740033,
                "mdate": 1700068740033,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]