[
    {
        "title": "Leveraging characteristics of the output distribution for identifying adversarial audio examples"
    },
    {
        "review": {
            "id": "s069pYMMHY",
            "forum": "R1crLHQ4kf",
            "replyto": "R1crLHQ4kf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5285/Reviewer_DwAh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5285/Reviewer_DwAh"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a new adversarial example detection method for any automatic speech recognition (ASR) system. Relying on the characteristics of the output distribution in ASR system over the tokens from the output vocabulary, the authors use a function to compute corresponding scores and then employ a binary classifier for adversarial detection. Empirical results have demonstrated the effectiveness of the detection method. In addition, to better analyze the robustness of the proposed detection method, the authors also perform adaptive attacks with aware of the defense mechanism."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper proposes a simple and effective method for detection adversarial examples for ASR systems.\n2. The paper is presented with comprehensive experiments. The authors not only present the benign detection performance of adversarial attacks but also analysis the robustness under adaptive attacks with known detection method."
                },
                "weaknesses": {
                    "value": "1. Detection performance is only evaluated on limited adversarial attack methods. The authors only evaluate their method on C&W attack and Psychoacoustic attack. More attack methods like [1] [2] and even some black box methods like FAKEBOB [3] are still needed to be included to prove the general performance of the detection method.\n2. Lack of comparison with other audio adversarial example detection methods like [4].\n\n[1] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In International Conference on Learning Representations, 2018.\n\n[2] Alzantot, Moustafa, Bharathan Balaji, and Mani Srivastava. Did you hear that? adversarial examples against automatic speech recognition. arXiv preprint arXiv:1801.00554 (2018).\n\n[3] Guangke Chen, Zhe Zhao, Fu Song, Sen Chen, Lingling Fan, Feng Wang, and Jiashui Wang. To\u0002wards understanding and mitigating audio adversarial examples for speaker recognition. IEEE Transactions on Dependable and Secure Computing, 2022.\n\n[4] Rajaratnam, Krishan, and Jugal Kalita. Noise flooding for detecting audio adversarial examples against automatic speech recognition. In 2018 IEEE International Symposium on Signal Processing and Information Technology (ISSPIT), pp. 197-201. IEEE, 2018."
                },
                "questions": {
                    "value": "In Section5.1, four guiding principles are provided on selection process. Do such principles limit the adversarial attack implementation? For example, there should be an equal number of tokens in both the original and target transcriptions. Would there be other attack scenarios like tokens insertion or deletion?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No ethics concerns."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5285/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5285/Reviewer_DwAh",
                        "ICLR.cc/2024/Conference/Submission5285/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5285/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698651019808,
            "cdate": 1698651019808,
            "tmdate": 1700710527777,
            "mdate": 1700710527777,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "dqB3JjibD8",
                "forum": "R1crLHQ4kf",
                "replyto": "s069pYMMHY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5285/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5285/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Our response to Reviewer DwAh"
                    },
                    "comment": {
                        "value": "Many thanks for your valuable feedback. We performed further investigations based on your comments and made updates to our paper accordingly.\n\\\n\\\nQ: Detection performance is only evaluated on limited adversarial attack methods. The authors only evaluate their method on C&W attack and Psychoacoustic attack. More attack methods like [1] [2] and even some black box methods like FAKEBOB [3] are still needed to be included to prove the general performance of the detection method.\n\nA: Thanks for your suggestions, we extended our analysis to three more attacks: PDG and those proposed in [1] and [2]. We evaluated the detection performance of our classifiers across diverse attacks. Our findings reveal that our GCs utilizing mean-median characteristic and NNs exhibited successful transferability to the Psychoacoustic attack. Moreover, when exposed to untargeted attacks, they outperformed the baseline in comparison to PGD and Kenansville. For enhanced clarity, we introduced a new analysis, presented in the results section 5.3 on pages 7-8 and table 6 of our revised manuscript.\n\nWe did not include FAKEBOB in our analysis, since it is designed specifically for speaker recognition systems, which falls outside the scope of our research.\n\\\n\\\n\\\nQ: Lack of comparison with other audio adversarial example detection methods like [4].\n\nA: Thanks for your input. We included [4] as a baseline in our analysis, but found that it is less effective than our method. Please refer to section 5.3, page 8, and table 4 in our latest manuscript. \n\\\n\\\n\\\nQ: In Section5.1, four guiding principles are provided on selection process. Do such principles limit the adversarial attack implementation? Would there be other attack scenarios like tokens insertion or deletion?\n\nA: Thank you for expressing your concern. The decision to limit the audio length to up to five seconds, was a strategy compromise, balancing time and resources, as generating longer adversarial examples take much more time. \n\nThe decision to choose unique target transcriptions is primarily made to minimize bias in our results, ensuring the inclusion of the broadest possible variety of selected words.\n\nThe choice of the number of tokens for constructing a target transcription is influenced by the nature of the duration of an audio. There is always a limit of the number of tokens we can hide in an adversarial example, for shorter inputs, achieving a longer target transcription becomes more challenging, and vice versa. Even when the total number of tokens in both the original and target transcriptions are equal, the transcription length itself may vary, as shown in our Demo: https://confunknown.github.io/characteristics_demo_AEs/ \n\nThis design choice is also supported by previous research. For example, [5] conducted an evaluation on the rate of phones that could be inserted per second without compromising adversarial attack performance. Their findings indicated that as the phone rate increases, the WER also increases. They observed four phonemes per second as a reasonable choice.\n\n[5] Sch\u00f6nherr et al., Adversarial attacks against automatic speech recognition systems via psychoacoustic hiding. In Network and Distributed System Security Symposium (NDSS), 2019."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5285/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700690936842,
                "cdate": 1700690936842,
                "tmdate": 1700693740974,
                "mdate": 1700693740974,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OuDbZPabfx",
                "forum": "R1crLHQ4kf",
                "replyto": "dqB3JjibD8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5285/Reviewer_DwAh"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5285/Reviewer_DwAh"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your response. I appreciate your efforts on adding more experiments of different attack methods and detection baselines. I have increased my score."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5285/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700710509405,
                "cdate": 1700710509405,
                "tmdate": 1700710509405,
                "mdate": 1700710509405,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "rYojtn29SY",
            "forum": "R1crLHQ4kf",
            "replyto": "R1crLHQ4kf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5285/Reviewer_NsBd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5285/Reviewer_NsBd"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses the issue of adversarial attacks on automatic speech recognition (ASR) systems, where imperceptible noise can manipulate the output. The authors propose a detection strategy applicable to any ASR system, measuring various characteristics of the output distribution. By employing binary classifiers, including simple threshold-based methods and neural networks, they achieve superior performance in distinguishing adversarial examples from clean and noisy data, with AUROC scores exceeding 99% and 98%, respectively. The method's robustness is tested against adaptive attacks, showcasing its effectiveness in detecting even noisier adversarial clips, preserving the system's robustness."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper works on important issues and is written clearly."
                },
                "weaknesses": {
                    "value": "The types of attacks considered in this work appear to be limited, as it seems to primarily focus on the C&W attack. Why not consider other attack methods, such as PGD attacks or Transaudio transfer attacks[1]? Being able to defend against transferable adversarial samples would make the paper more practically significant. I appreciate the presentation of the entire work, but the limited consideration of attack types makes it hard for me to be convinced.\n\nIf the authors can reasonably address my concerns, I would consider increasing the score accordingly.\n\n\n[1] G. Qi et al., \"Transaudio: Towards the Transferable Adversarial Audio Attack Via Learning Contextualized Perturbations,\" ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Rhodes Island, Greece, 2023, pp. 1-5, doi: 10.1109/ICASSP49357.2023.10096873."
                },
                "questions": {
                    "value": "See above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5285/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698753909025,
            "cdate": 1698753909025,
            "tmdate": 1699636528694,
            "mdate": 1699636528694,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "abkVnQXptZ",
                "forum": "R1crLHQ4kf",
                "replyto": "rYojtn29SY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5285/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5285/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Our response to Reviewer NsBd"
                    },
                    "comment": {
                        "value": "Many thanks for your feedback! We run additional experiments with the suggested attacks and updated our paper accordingly.\n\\\n\\\nQ: Why not consider other attack methods, such as PGD attacks or Transaudio transfer attacks?\n\nA: We expanded our empirical analysis by incorporating additional experiments that explore three untargeted attacks: PGD, Kenansville and Genetic attacks. \nWhile we made an effort to incorporate the Transaudio attack, constraints on time prevented us from creating an implementation specifically designed for Speechbrain and discussing certain inquiries with the authors. To overcome this, we opted to assess our detectors against two additional black-box attacks\u2014Kenansville and the Genetic\u2014suggested by reviewer NsBd. \nWe evaluated the detection performance of our classifiers across diverse attacks. Our findings revealed that our GCs utilizing mean-median characteristic and NNs exhibited successful transferability to the Psychoacoustic attack. Moreover, when exposed to untargeted attacks, they outperformed the baseline in detecting PGD and Kenansville attacks. For enhanced clarity, we introduced a new analysis, presented in the results section 5.3 on pages 7-8, and table 6 of our revised manuscript."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5285/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700690504495,
                "cdate": 1700690504495,
                "tmdate": 1700693112646,
                "mdate": 1700693112646,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "wwbh3sHkne",
            "forum": "R1crLHQ4kf",
            "replyto": "R1crLHQ4kf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5285/Reviewer_jXR8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5285/Reviewer_jXR8"
            ],
            "content": {
                "summary": {
                    "value": "The paper addresses the significant issue of adversarial attacks on automatic speech recognition (ASR) systems, a relevant topic in the field of machine learning and security.  The approach is based on analyzing the probability distribution over output tokens at each time step. This involves examining statistical measures like median, maximum, minimum, entropy, and divergence (KL and JSD). Moreover, the authors claims that their detector is resilience when it comes to dealing with noisy data, meaning they can still effectively detect adversarial attempts even when the audio quality is compromised by noise."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper introduces a novel approach to identify adversarial attacks by using statistical characteristics of the output token distributions. \n\n- It has been demonstrated that specific statistical measures, like the mean of the median of probabilities, have an acceptable discriminative capabilities. This implies that the authors have done rigorous empirical analysis to identify which features are most effective.\n\n- The authors mention empirical findings, which suggests that they have tested their approaches on real-world data or experiments, providing evidence for their claims."
                },
                "weaknesses": {
                    "value": "- The proposed defense method relies on statistical features like the mean, median, maximum and minimum extracted from the output token probability distributions over time. While these aggregated metrics can efficiently summarize certain characteristics of the distributions, they may miss more subtle adversarial manipulations. For example, an attack could alter the shape of the distribution while keeping the median relatively unchanged. Or it may flip the probabilities of two unlikely tokens, barely affecting the minimum. So only looking at the summary statistics of the distributions may not be enough to detect all possible manipulations by an adaptive adversary.\n\n\n- While the proposed approach performs remarkably well empirically, it is mostly relying on simple aggregated features. Exploring more sophisticated methods to represent, compare and analyze"
                },
                "questions": {
                    "value": "- The adaptive attacks lower your detection accuracy considerably. Have you looked into ways to make the classifiers more robust? For example, by using adversarial training or adding noise to the features.\n\n- Have you evaluated the computational overhead added by extracting the distributional features and running the classifiers? Is this method efficient enough for real-time usage in production systems?\n\n- You use simple summary statistics to represent the output distributions. What prevents more sophisticated adaptive attacks that preserves these summary statistics but still fools the ASR system?\n\n- Your defense relies on statistical metrics like median and maximum probability diverging for adversarial examples. Have you explored attacks that explicitly optimize to minimize statistical distance from the benign data distribution? This could make the adversarials harder to detect.\n\n- Moreover, can the adversarial optimization problem be formulated to reduce divergence from the benign data distribution, while still fooling the ASR system? What are the challenges in constructing such \"distribution-aware\" attacks?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5285/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5285/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5285/Reviewer_jXR8"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5285/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699222775558,
            "cdate": 1699222775558,
            "tmdate": 1699636528588,
            "mdate": 1699636528588,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xhM1L8aeje",
                "forum": "R1crLHQ4kf",
                "replyto": "wwbh3sHkne",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5285/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5285/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Our response to Review jXR8"
                    },
                    "comment": {
                        "value": "We thank for your valuable feedback, we have taken your suggestions into consideration and made updates to our paper accordingly.\n\\\n\\\nQ: Have you looked into ways to make the classifiers more robust? \n\nA: During the training of all models, to improve generalization, we applied standard data augmentation techniques provided in SpeechBrain: corruption with random samples from a noise collection, removing portions of the audio, dropping frequency bands, and resampling the audio signal at a slightly different rate. We described in the paragraph \u201cASR system\u201d that is in Section 5.1, Page 5. \n\\\n\\\n\\\nQ: Have you evaluated the computational overhead added by extracting the distributional features and running the classifiers? Is this method efficient enough for real-time usage in production systems?\n\nA: Thanks for asking, we performed experiments to measure the total time the system takes to predict 100 audio clips, utilizing an NVIDIA A40 with a memory capacity of 48 GB. As a result, running the assessment with our detectors took approximately an extra 18.74 ms per sample, therefore the proposed method is suitable for real-time usage. We have incorporated this information into the paper, and provided additional details in Appendix A.3 on page 16.\n\\\n\\\n\\\nQ: Can the optimization problem for adversarial attacks be framed to minimize divergence from the benign data distribution while still deceiving the ASR system? And what challenges exist in developing adaptive attacks that maintain summary statistics while effectively fooling the system?\n\nA: Thanks for the suggestion. This is an interesting idea. Unfortunately, due to the limited amount of time, we did not manage to finalize experiments with the suggested adaptive attack, but we will continue investigating and add the results to the final paper."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5285/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700690176923,
                "cdate": 1700690176923,
                "tmdate": 1700698017442,
                "mdate": 1700698017442,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "dWUlyVIyUo",
            "forum": "R1crLHQ4kf",
            "replyto": "R1crLHQ4kf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5285/Reviewer_2Aka"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5285/Reviewer_2Aka"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a method to detect adversarial audio example by exploiting statistical features. Based on the selected features, accurate predictions can be made to differentiate adversarial audio examples and standard audio samples. An adaptive attack against proposed detection methods is also introduced, even though less effective against adversarial examples, the authors claim that the noise level of the adaptive attack is higher and the adaptive adversarial audio examples can be easily picked by the human ear."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is very well presented and easy to follow. Extensive experimental results are provided to support the claims. The results demonstrate that the proposed detection is more generally more accurate than the existing TD detection method. Results on adaptive attacks also show that if an adaptive adversarial audio example targets the proposed detection, more audible noises will be included in the adversarial example."
                },
                "weaknesses": {
                    "value": "- The reason behind the selected statistics can be further motivated. Why are these statistical features selected? A related question is why the generated adaptive adversarial audio examples are noisier when optimizing with respect to relevant feature?\n- Regarding the generalization of the proposed detection, the transferability of the detection can be further clarified. About intra-model generalization, will the detection model that is trained on one specific kind of adversarial example be generalizable to other types of adversarial examples? This point needs to be clarified since it may weaken the threat model that the detector needs to know the type of the adversarial attack beforehand. About inter-model generalization, will a detector trained on one ASR model be able to detect adversarial examples that are generated on a different ASR model? It would be great if the authors can clarify the generalization of the proposed method.\n- About the adaptive attack, have the authors considered other types of attacks that may decrease the noise level of the adversarial audio examples?  I really appreciate that the authors provide experiments on adaptive attacks, which definitely makes the claims stronger. It would be great if the authors could clarify the specific efforts that have been made to control the noise level."
                },
                "questions": {
                    "value": "See questions in the weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5285/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699575228536,
            "cdate": 1699575228536,
            "tmdate": 1699636528487,
            "mdate": 1699636528487,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "XWKanGkyiH",
                "forum": "R1crLHQ4kf",
                "replyto": "dWUlyVIyUo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5285/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5285/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Our response to Review 2Aka"
                    },
                    "comment": {
                        "value": "We value your insightful comments and queries. To address your concerns, we provide explanations in the following and updated the paper accordingly. \n\\\n\\\nQ: Why are these statistical features selected? \n\nA: Previous work indicated the high potential of statistics of the output distribution that implement uncertainty measures: on the one hand, the mean entropy was turned into a defense method against adversarial attacks on hybrid ARS system with limited vocabulary, as discussed in [1].  On the other hand, the Kullback-Leibler divergence between the distributions in consecutive steps was used to assess the reliability of ARS systems, see [2]. We therefore aimed to investigate if this also applies for state-of-the-art end-to-end ARS systems. We expanded our analysis to simple characteristics (like max, min, median) and were surprised ourselves that they lead to sometimes even better detection results. \n\n[1] D\u00e4ubener et al., Detecting adversarial examples for speech recognition via uncertainty quantification. In Proc. Interspeech 2020, pp.4661\u20134665, 2020.\n\n[2] Meyer et al., Performance monitoring for automatic speech recognition in noisy multi-channel environments. In 2016 IEEE Spoken Language Technology Workshop (SLT), pp. 50\u201356, 2016.\n\\\n\\\n\\\nQ: Why the generated adaptive adversarial audio examples are noisier when optimizing with respect to relevant feature? \n\nA: The adaptive attack contains two (potentially competing) loss functions, which makes the optimization problem harder. We also were successful in generating less noisy examples with the adaptive attack (see Appendix A.1, page 14) which however were less often fooling the detection model. We added a sentence to make this clearer in the main paper. \n\\\n\\\n\\\nQ: Will be the detection model that is trained on one specific kind of adversarial example be generalizable to other types of adversarial examples? \n\nA: Yes. First, note that the construction of Gaussian classifiers can be done only with benign examples. While we also investigated the performance based on the best characteristic picked on a validation set, we also found that some characteristics perform very well for different attack types and thus can be chosen without any adversarial data. Only benign data for picking the threshold is needed. Moreover, we conducted experiments on intra-model generalization for the neural networks, investigating if a detection model trained on the C&W attack can be applied as a defense method against the other targeted (i.e., Psychoacoustic) and three newly added untargeted attacks. We found that our GCs based on the mean-median characteristic and NNs demonstrated effective transferability to the Psychoacoustic attack. Additionally, when subjected to untargeted attacks, they performed better than the baseline against PGD and Kenansville attacks. To make this more clear, we incorporated a new analysis displayed by the results in section 5.3/pages 7-8 /Table 6 of our updated manuscript.\n\\\n\\\n\\\nQ: Will a detector trained on one ASR model be able to detect adversarial examples that are generated on a different ASR model?\n\nA: We don't expect our detectors to generalize between models, since the statistics for different models differ. Previous research has demonstrated almost no transferability of adversarial examples between speech models, as indicated in [1]. This lack of transferability, especially in the context of targeted optimization attacks, suggests that adversarial examples crafted to target a specific model lose their efficacy when applied to a new ASR model, posing a limited threat. We verified this claim with our models and obtained similar results, with almost no transferability observed between models. These results are reported in Appendix A.11 on pages 30-31.\n\n[1] Abdullah et al., Sok: The faults in our ASRs: An overview of attacks against automatic speech recognition and speaker identification systems. 2021 IEEE Symposium on Security and Privacy (SP), pp. 730\u2013747, 2020.\n\\\n\\\n\\\nQ: About the adaptive attack, have the authors considered other types of attacks that may decrease the noise level of the adversarial audio examples? \n\nA: Yes, we did. As described in details in Appendix A.1 (page 14) we experimented with several different settings that resulted in lower noise levels but also in less affective attacks. We added a sentence to make this clear in the main paper."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5285/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700689189167,
                "cdate": 1700689189167,
                "tmdate": 1700692755902,
                "mdate": 1700692755902,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]