[
    {
        "title": "Reward Collapse in Aligning Large Language Models"
    },
    {
        "review": {
            "id": "NNYDtpos4E",
            "forum": "tcx84iyqaC",
            "replyto": "tcx84iyqaC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3901/Reviewer_vqeA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3901/Reviewer_vqeA"
            ],
            "content": {
                "summary": {
                    "value": "The paper study the problem of RLHF for large language models. The paper firstly finds that the reward model trained by human rankings over different completions can not distinguish with open-ended and closed prompts, and call it as  the \"reward collapse\" phenomenon. The paper theoretically gives the reason, that is, the ranking-based objective function does not consider prompt-based factors. Some experimental analysis validates the claim."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Novelty: The claim proposed by the paper is novel and interesting.\nQuality: The paper gives enough theoretical analysis and the experimental results validates the phenomenon.\nClarity: The paper is well written.\nSignificance: The proposed claim is interesting and meaningful for the LLM community."
                },
                "weaknesses": {
                    "value": "Quality: The paper claims that we should use prompt-based utility function Eq.(3). However, all follow analysis is about the utility function without the prompt, which is inconsistent. The paper does not provide the implementation details of the prompt-aware training."
                },
                "questions": {
                    "value": "See the above section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3901/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697619676693,
            "cdate": 1697619676693,
            "tmdate": 1699636349923,
            "mdate": 1699636349923,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GbZU0h839p",
                "forum": "tcx84iyqaC",
                "replyto": "NNYDtpos4E",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3901/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3901/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer vqeA"
                    },
                    "comment": {
                        "value": "Thank you for your valuable feedback! We respond to your comments as follows:\n\n> The paper claims that we should use prompt-based utility function Eq.(3). However, all follow analysis is about the utility function without the prompt, which is inconsistent.\n\nOur theoretical analysis focuses on determining the reward distribution when a fixed utility function is chosen. Therefore, for the purposes of our analysis, we can fix a utility function.\n\n> The paper does not provide the implementation details of the prompt-aware training.\n\nIn our experiment in Section 3.2, we randomly assigned each question as either open-ended or concrete. For open-ended questions, the utility function is selected as G(x) = -1/x. For concrete questions, their utility function is chosen as G(x) = x. This choice of utility functions serves the specific purpose of demonstrating that the prompt-aware approach can effectively prevent reward collapse. Additionally, in Appendix A.2, we delve into potential practical methods for assigning prompt types."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3901/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700471592364,
                "cdate": 1700471592364,
                "tmdate": 1700471819826,
                "mdate": 1700471819826,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "29ZnOpswOn",
            "forum": "tcx84iyqaC",
            "replyto": "tcx84iyqaC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3901/Reviewer_hn6B"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3901/Reviewer_hn6B"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the reward modeling in the alignment for LLMs. Specifically, the authors investigate the phenomenon of reward collapse in LLMs and proposes a prompt-aware optimization scheme to mitigate it. The key idea of the paper is to use a prompt-dependent utility function so that the distributions can be more diverse across different prompts. Empirical results are provided to verify the theoretical and intuitive results."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1 This paper is about an important and interesting problem in RLHF, and is very relevant to the community of Neurips. The authors first demonstrate the problem with various experiments of reward modeling, and motivate the approach with sound theoretical analysis.\n\n2 The paper proposes a prompt-aware optimization scheme to overcome reward collapse and introduces utility functions that depend on the prompt to achieve prompt-dependent reward distributions. Real-world experiments on stackoverfolow and also synthetic experiments are conducted to support the findings and demonstrate a method superior to early stopping for addressing reward collapse.\n\nAs a paper for understanding some important problem in RLHF, the quality of the work is satisfactory."
                },
                "weaknesses": {
                    "value": "1 How do the conclusions change if the assumption that the LLM is sufficiently overparameterized so that it can maximize the utility for all the prompts (discussions around equation (2))?\n\n2 While the story and theoretical analysis are sound, the evidences of this paper are limited. But one thing I believe can largely improve the paper is that we can further evaluate the quality of the reward model by best-of-n policy. Specifically, we can fix a LLM, and for each prompt, we sample n responses and then take the one with the highest reward as the final output. Then, we can compare the responses by either human evaluation or GPT4 evaluation. For more details, you may check [1].\n\n------------------------------------\nUpdate in 11.11\nSorry for the late update. I just read the paper again and have a quick question about the choice of U. As I mentioned in the above review, the experiments conducted are rather limited and simple. In particular, the utility function used for the response length seems to be hard to generalize to general practical applications. Could you give an example of the choice of utility function in practice, e.g., used for the HH-RLHF dataset (whose details can be found in huggingface). \n\n[1] let's verify step by step"
                },
                "questions": {
                    "value": "see weakness"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3901/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3901/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3901/Reviewer_hn6B"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3901/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698794630777,
            "cdate": 1698794630777,
            "tmdate": 1699687915473,
            "mdate": 1699687915473,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "AM7t6bKrOc",
                "forum": "tcx84iyqaC",
                "replyto": "29ZnOpswOn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3901/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3901/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer hn6B"
                    },
                    "comment": {
                        "value": "Thank you for your support of our paper. We address the questions as follows.\n\n1.\tThe assumption that the LLM is sufficiently overparameterized is quite strong, but it\u2019s essential for our conclusions. Since if the reward does not maximize the objective function exactly, reward collapse may not exists. For example, in the experiment in Section 3 (Figure 1 and 4), reward collapse does not show up in the early stage of training. This overparameterization assumption is satisfied in most real applications. For example, InstructGPT  se a 6B reward model [1].\n\n2.\tWhile we acknowledge the potential improvement of RLHF in current LLMs, the primary focus of this paper is to document and investigate the phenomenon of reward collapse during reward model training in LLMs. We present theoretical results on reward collapse and derive the reward distribution under a specific class of utility functions. Although intuitively our approach should enhance the performance of RLHF, we believe that the impact of using a prompt-aware utility function requires thorough further research. Numerous techniques implicitly avoid the harm of reward collapse, such as early-stop or adding regularizers, which may alleviate reward collapse. Investigating the extent to which the trained reward model enhances the capabilities of large language models, such as their ability to self-calibrate uncertainty, is an interesting direction for future work beyond the scope of this paper.\n\n3.\tAassigning prompt types is fundamental in the prompt-aware approach, as discussed in Appendix A.2. A straightforward method for determining the prompt type is to gather input from human labelers, who typically rank different responses, as seen in InstructGPT. Additionally, we can request them to assess how open-ended the prompt is using a scale ranging from -1 to 1. Automated annotation processes are also possible. For instance, one approach involves assessing the variability of responses to a given prompt. If the responses exhibit high variability, the prompt can be classified as open-ended. Conversely, if the responses show low variability, the prompt may be deemed close-ended. Determining the prompt type is indeed a complex and fascinating task, providing a promising avenue for future research.\n\n[1] Ouyang, Long, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang et al. \"Training language models to follow instructions with human feedback.\" Advances in Neural Information Processing Systems 35 (2022): 27730-27744.\n\nWe sincerely value your feedback, and we hope these clarifications address your concerns. If you have any further questions or suggestions, please feel free to let us know."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3901/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700471644753,
                "cdate": 1700471644753,
                "tmdate": 1700471644753,
                "mdate": 1700471644753,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "3EahBwlsxz",
            "forum": "tcx84iyqaC",
            "replyto": "tcx84iyqaC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3901/Reviewer_C1CM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3901/Reviewer_C1CM"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents the theoretical finding of reward collapse when training reward models on ranking-based preference data. Through experiments, the authors demonstrate that the reward distributions for different prompts converge to a common prompt-independent distribution, disregarding whether prompts are open or closed-ended. To address this issue, they propose a prompt-aware utility function approach that learns distinct reward distributions based on prompt type."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "strengths:\n1. The paper clearly documents the phenomenon of reward collapse, supported by theoretical analysis and experiments. This is an important observation that will aid the development of prompt-aware reward modeling.\n2. Empirically demonstrated that the reward distributions will converge towards a prompt-independent distribution.\n3. The method is extended to handle pairwise preference data, improving applicability."
                },
                "weaknesses": {
                    "value": "1. Does not provide much detail on how the prompt-aware utility function U_prom adaptively selects between U(x) = x and U(x) = -1/x in the experiments mentioned in Section 3.2. Do you manually assign utility functions based on the question type?\n2. Experiments are done on only synthetic datasets where the word count is the ground-truth reward. Real-world ranking datasets would provide stronger validation.\n3. It is not clear how the prompt-dependent reward distribution will contribute to the performance increase for RLHF or other direct optimization methods or just the best of n sampling. It would be great to see how this reward distribution will increase performance. \n4. The evidence of the reward collapse in Figure 1: what does the dashed region represent? is it over the 128 prompt datasets? It would be clearer to present the reward collapse phenomenon on a close-end question.\n5. For the two subfigures in Figure 5, what are the differences between the two plots\u2019s settings? Could you label the y-axis?"
                },
                "questions": {
                    "value": "please see weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3901/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698820419120,
            "cdate": 1698820419120,
            "tmdate": 1699636349722,
            "mdate": 1699636349722,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9AFLNwwZyx",
                "forum": "tcx84iyqaC",
                "replyto": "3EahBwlsxz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3901/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3901/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer C1CM"
                    },
                    "comment": {
                        "value": "Thank you for taking the time to provide feedback on our work. We appreciate your thoughtful comments and would like to address each point accordingly.\n1.\t In Section 3.2, we specified that each question was randomly assigned as either open-ended or concrete. For open-ended questions, their utility function is chosen as G(x) = -1/x. For concrete questions, their utility function is chosen as G(x) = x. This is only for the purpose of our experiment. Additionally, in Appendix A.2, we delve into potential practical methods for assigning prompt types.\n\n2.\tWe understand your concern about the validation of our findings on real-world datasets. While Section 3.1 presented experiments on a real-world dataset to demonstrate the existence of reward collapse, we used synthetic datasets in Section 3.2 to illustrate how prompt-aware training can mitigate this issue. We acknowledge the potential for stronger validation with real-world ranking datasets and agree that it is an area for improvement. Nevertheless, our controlled experiments, combined with those in Section 3.1, effectively convey the key message: reward collapse can be avoided through prompt-aware training.\n\n3.\tThe primary focus of our paper is to document and investigate the phenomenon of reward collapse in the training of reward models for LLMs. We have developed theoretical results on reward collapse and derived the reward distribution under a specific class of utility functions. While we intuitively believe that our approach enhances the performance of RLHF, we recognize the need for further research on the effects of using prompt-aware utility functions. We also acknowledge the existence of other techniques that implicitly address reward collapse, such as early-stop or regularizers. Exploring how the trained reward model enhances the capabilities of large language models, including their ability to self-calibrate uncertainty, is an intriguing avenue for future research, albeit beyond the scope of this paper.\n\n4.\tYes, the dashed region signifies the randomness across 128 distinct prompts. We appreciate your suggestion, and in response, we will employ an alternative utility function to ensure a clearer presentation of the results. \n\n5.\tThe distinction lies in the utilization of two different parameters, denoted as $\\theta$ (Appendix D.2), in the BTL model. The y-axis represents the value of the empirical cumulative distribution function (ecdf) of rewards obtained from solving the optimization problem: \n$$\n\\max_{0\\le r_1,\\cdots, r_n \\le 1} S(r_1,\\cdots,r_n), \\mbox{ where }S(r_1,\\cdots,r_n) = \\sum_{1\\le i,j\\le n} U(r_i - r_j) \\sigma(\\theta_i - \\theta_j).\n$$\nWe will ensure that these details are appropriately highlighted for better clarity in our presentation.\n\nWe sincerely value your feedback, and we hope these clarifications address your concerns. If you have any further questions or suggestions, please feel free to let us know."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3901/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700471690194,
                "cdate": 1700471690194,
                "tmdate": 1700471690194,
                "mdate": 1700471690194,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]