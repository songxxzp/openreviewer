[
    {
        "title": "PETformer: Long-term Time Series Forecasting via Placeholder-enhanced Transformer"
    },
    {
        "review": {
            "id": "PdkbFJLYfV",
            "forum": "u3RJbzzBZj",
            "replyto": "u3RJbzzBZj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5028/Reviewer_27hd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5028/Reviewer_27hd"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel Transformer-based model for long-term time series forecasting (LTSF), which introduce the Placeholder-enhanced Technique (PET) to enhance the computational efficiency and predictive accuracy, and delves into different strategies related to Transformer. Experiments on multiple real-world datasets have demonstrated that PETformer achieves good performance for LTSF."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1.The idea of using \u201cPlaceholder\u201d technique in LTSF is novel.\n\n2.The experimental performance of the proposed model beats most of the baselines and shows promising results on several datasets. The ablation study seems detailed and comprehensive."
                },
                "weaknesses": {
                    "value": "1.The purpose of this paper is unclear: what problems are this paper trying to solve? To prove the effectiveness of Transformer in LSTF, or to enhance the model PatchTST? The unclear purpose causes confusions in the introduction section (for example, the introduction mentions the model Dlinear, but what is the relationship between PETformer and Dlinear)?\n\n2.The paper does not sufficiently analyze why the \"Placeholder\" technique improves the model's performance. The only sentence I find is \u201cAllowing more learnable parameters to influence the feature extraction module of the Transformer, rather than the linear prediction head, may be a key reason for PETformer outperforming PatchTST\u201d, but that\u2019s not enough. The reason why the \u201cPlaceholder\u201d technique is effective should be the core of the paper.\n\n3.The discrepancy between the results in Table 2 and Table 4 needs to be addressed with an explanation.\n\n4.The paper should provide information on the initial values in the \"Placeholder,\" and whether different values affect the model's performance.\n\n5.The experiments used the look-back window size of 720 in most datasets and claimed to achieve SOTA performance. However, this comparison may be unfair as a longer look-back window generally provides more information and potentially better results. Additionally, it is essential to investigate if PatchTST performs better with a look-back window size of 720.\n\n6.The paper mentions that \u201cThe Placeholder technique shares similarities with the currently popular Masking technique in the unsupervised pretraining domain\u201d, however, this claim requires further explanation to strengthen its validity.\n\n7.Some figures in the paper require enhancement. For example, the left sub-figure of Figure 2 should include the \u2018Placeholder\u2019 in the input, and the middle sub-figure of Figure 2 should display the \u2018Placeholder\u2019 with the same length as the output but with a different color.\nThe details of the ablation studies need to be clearly described, and there are too many abbreviations of self-defined terms (NIFA, NIHA, OFFH, SA, CA ...) in the paper, which may inconvenience readers."
                },
                "questions": {
                    "value": "Please refer to the Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5028/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5028/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5028/Reviewer_27hd"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5028/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697703357282,
            "cdate": 1697703357282,
            "tmdate": 1699636491813,
            "mdate": 1699636491813,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "1b47Esib4p",
            "forum": "u3RJbzzBZj",
            "replyto": "u3RJbzzBZj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5028/Reviewer_Gtuh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5028/Reviewer_Gtuh"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a Transformer-based model (PETformer) for Long-Term Time Series Forecasting. PETformer concatenates several learnable placeholders of future sequences and embeddings of past sequences, and inputs them into the Transformer encoder. This approach maintains the temporal continuity between past and predicted sequences, and significantly reduces the number of parameters required for the prediction head. Additionally, this paper explores the impact of multi-channel relationships and long subsequences on time series prediction tasks. Experimental validation on eight public datasets confirms the effectiveness of PETformer."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.\tThe overall writing is good. The logic is clear and easy to follow.\n2.\tThe proposed model, PETformer, achieves the SOTA results on eight public datasets. The authors compared with many advanced prediction models and reached consistent conclusions.\n3.\tThe ablation study is very comprehensive, which proves the rationality and effectiveness of the model design."
                },
                "weaknesses": {
                    "value": "1.\tMy biggest concern is the novelty of the paper. Compared to PatchTST, the main difference of PETformer is the removal of the flattened prediction head and the use of placeholder embeddings for prediction. As the authors mentioned, this idea has been explored in many other fields, such as Masked Language Modeling in NLP and Masked Image Modeling in CV. It is not uncommon to mask future tokens for prediction, as mentioned in [1]. Additionally, although the authors also explored Long Sub-sequence Division (LSD) and Multi-channel Separation and Interaction (MSI), these are not new in time series forecasting and are only experimental supplements in my view.\n2.\tWhen exploring the relationships between multiple channels, the authors only studied the relationships in the predicted sequence, without directly studying the past sequence. In fact, the interaction between multiple channels in the past sequence may also be helpful for prediction. Besides, there is another way of extracting inter-channel features that the authors did not consider. Please refer to Crossformer [2]. Therefore, these omissions may not support the argument about Multi-channel Separation and Interaction (MSI) in the paper.\n3.\tWhile the paper presents a comprehensive evaluation of the proposed approach, it does not discuss the limitations or potential drawbacks of the approach.\n[1] Gupta A, Tian S, Zhang Y, et al. MaskViT: Masked Visual Pre-Training for Video Prediction. ICLR 2023.\n[2] Zhang Y, Yan J. Crossformer: Transformer utilizing cross-dimension dependency for multivariate time series forecasting. ICLR 2023."
                },
                "questions": {
                    "value": "1.\tAlthough predicting with placeholders greatly reduces the number of parameters in the prediction head, it increases the number of tokens involved in self-attention. As we all know, the efficiency bottleneck of Transformer is self-attention. Therefore, my question is why the computational efficiency still improves after increasing the number of tokens?\n2.\tIn the study of the window size w of the sub-sequence, if the length is fixed at 720, will the performance of LTSF continue to increase as w continues to increase?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5028/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5028/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5028/Reviewer_Gtuh"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5028/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698417326583,
            "cdate": 1698417326583,
            "tmdate": 1699636491712,
            "mdate": 1699636491712,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "40AgzyzW5y",
            "forum": "u3RJbzzBZj",
            "replyto": "u3RJbzzBZj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5028/Reviewer_vDyH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5028/Reviewer_vDyH"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes Placeholder-enhanced Technique to enhance the computational efficiency and predictive accuracy of Transformer in LTSF tasks. This paper also studies the impact of larger patch strategies and channel interaction strategies on Transformer\u2019s performance. Experimental results showed the efficacy of proposed method and state-of-the-art performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1) It's novel to propose Placeholder-enhanced Technique to enhance the computational efficiency and predictive accuracy of Transformer to time series forecasting.\n\n2) Experimental results showed state-of-the-art performance."
                },
                "weaknesses": {
                    "value": "The current time series forecasting datasets are pretty small, and performance may be satuated or over-fitting."
                },
                "questions": {
                    "value": "The current time series forecasting datasets are pretty small, and performance may be satuated or over-fitting.  Could this method be used for larger datasets?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5028/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699059314466,
            "cdate": 1699059314466,
            "tmdate": 1699636491632,
            "mdate": 1699636491632,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]