[
    {
        "title": "Efficient Denoising Diffusion via Probabilistic Masking"
    },
    {
        "review": {
            "id": "o7UExaPj4b",
            "forum": "lhZEodF8Dn",
            "replyto": "lhZEodF8Dn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9106/Reviewer_jbL3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9106/Reviewer_jbL3"
            ],
            "content": {
                "summary": {
                    "value": "This work introduced an Efficient Denoising Diffusion method via Probabilistic Masking (EDDPM) to address the computational intensity issue in diffusion models. EDDPM utilizes probabilistic reparameterization to determine whether a time step should be skipped or not, thereby identifying and eliminating redundant steps during training. This approach, which jointly learns mask distribution parameters with model weights, includes a real-time sparse constraint to significantly enhance training efficiency. Remarkably, as the model reaches full proficiency, random masks converge to a sparse and deterministic form, retaining only crucial steps."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper introduces the concept of an Efficient Denoising Diffusion Model (EDDPM) to improve the sampling efficiency. This innovation addresses a significant challenge in diffusion models, which often require a large number of steps for generating a single sample. EDDPM effectively identifies and skips redundant steps, enhancing the sampling process. In my eyes, this idea is interesting."
                },
                "weaknesses": {
                    "value": "This article seems to be insufficiently prepared, containing various typos and using somewhat inappropriate notation, which can be confusing for readers. Additionally, the paper lacks intuitive explanations for some of the conclusions, and I will detail these issues in the following question."
                },
                "questions": {
                    "value": "1. As far as I know, there is typically a trade-off between sampling speed and sample quality. Having fewer sampling steps usually improves the sampling speed but often results in a decline in the quality of generated samples. This observation has been discussed in numerous studies focused on accelerating diffusion sampling. Why does Figure 1 indicate an enhancement in sample quality when the number of sampling steps is very low? Is this a consequence of randomness or an mean outcome? Is there a qualitative explanation?\n\n2. If I didn't miss it, the article doesn't employ the L0 norm. If that's the case, I recommend not introducing L0 in the notation section. Additionally, in Equation 6 and subsequent formulas, if you are using the L2 norm, I suggest explicitly writing it as $||\\cdot||_2$ not $||\\cdot||$.\n3. Under section 3, \"In this section, for the convenience of presenting our method DDPM......\", I think it should be EDDPM;\n4. Regarding the mask variable, $\\mathbf{m}_t$, I understand it to be the $t$-th entry of the vector $\\mathbf{m}$. Following tradition, a single random variable should not be represented in bold form, and it is recommended to write as $m_t$.\n5. Under Eq.(9), $\\tilde{\\boldsymbol{\\mu}}(\\mathbf{x}_t,\\mathbf{x}_0)$, $\\tilde{\\beta}_t$ should be written as $\\tilde{\\boldsymbol{\\mu}}_t(\\mathbf{x}_t,\\mathbf{x}_0,\\mathbf{m})$, $\\tilde{\\beta}_t(\\mathbf{m})$, because they depend on $\\mathbf{m}$. \n6. Under Eq.(9), in the definition of $\\tilde{\\beta}_t$ it has an extra \"t\" in the lower right corner.\n7. Due to the L1 constraint on $\\mathbf{s}$, most entries in $\\mathbf{s}$ will be pushed to 0. But why are the other entries pushed towards 1? Are there situations where some $m_t$ are around 0.5? In such cases, how should step $t$ be handled? Is there any explanation?\n8. Under eq.11, it should be $\\nabla_{\\theta}\\Phi(\\theta,s)$ and $\\nabla_{s}\\Phi(\\theta,s)$.\n9. Equation 12 is confusing because $\\gamma_e$ is not used in the algorithm. Is this a typo? Is it the case that $K=\\gamma_e*T$, and $\\gamma_e$ is expressed as in equation 12?\n10. In the context of image synthesis, the baseline comparison is limited. There are many acceleration sampling algorithms proposed for image synthesis, such as DPM-solver (Lu et al., 2022). While these works are mentioned in the related work section, they are not compared in the experiments.\n11. There are several issues with the citation format and some references are duplicated or inappropriately cited in arXiv format, even though they have been published."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9106/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9106/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9106/Reviewer_jbL3"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9106/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698297729597,
            "cdate": 1698297729597,
            "tmdate": 1700618942117,
            "mdate": 1700618942117,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "tzVMnAs34N",
                "forum": "lhZEodF8Dn",
                "replyto": "o7UExaPj4b",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9106/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9106/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer jbL3 [1]"
                    },
                    "comment": {
                        "value": "Thanks for your constructive comments.\n\n**Q1: As far as I know, there is typically a trade-off between sampling speed and sample quality. Having fewer sampling steps usually improves the sampling speed but often results in a decline in the quality of generated samples. This observation has been discussed in numerous studies focused on accelerating diffusion sampling. Why does Figure 1 indicate an enhancement in sample quality when the number of sampling steps is very low? Is this a consequence of randomness or an mean outcome? Is there a qualitative explanation?**\n\n**A1:** It is a misunderstanding. The x-axis in Figure 1 means the index of the removed step (e.g., 1-st step, 2-nd step), instead of the number of removed steps. We give this result to highlight redundant sampling steps in the denoising process. Removing these steps has a negligible effect on the quality of the samples or may even improve the sample quality.\n\n\n\n\n\n\n**Q2: If I didn't miss it, the article doesn't employ the L0 norm. If that's the case, I recommend not introducing L0 in the notation section. Additionally, in Equation 6 and subsequent formulas, if you are using the L2 norm, I suggest explicitly writing it as $||\\cdot||_2$ not $||\\cdot||$.**\n\n**A2:** Thanks. We have improved the writing accordingly in the revision.  \n\n**Q3: Under section 3, \"In this section, for the convenience of presenting our method DDPM......\", I think it should be EDDPM.**\n\n**A3:** Thanks. We have corrected it in the revision.\n\n**Q4: Regarding the mask variable, m\\_t, I understand it to be the t-th entry of the vector m. Following tradition, a single random variable should not be represented in bold form, and it is recommended to write as m\\_t.**\n\n**A4:** Thanks. We have modified these notations in the revision. \n\n**Q5: Under Eq.(9), $\\tilde{\\mathbf{u}}(\\mathbf{x}_t, \\mathbf{x}_0),\\tilde{\\beta}_t$ should be written as $\\tilde{\\mathbf{u}}_t(\\mathbf{x}_t, \\mathbf{x}_0,\\mathbf{m})$, $\\tilde{\\beta}_t(\\mathbf{m})$, because they depend on $\\mathbf{m}$.**\n\n**A5:** Thanks. We have corrected these typos in the revision.\n\n**Q6: Under Eq.(9),  in the definition of $\\tilde{\\beta}_t$, it has an extra \u201ct\" in the lower right corner.**\n\n**A6:** Thanks. We have corrected it in the revision.\n\n**Q7: Due to the L1 constraint on $\\mathbf{s}$, most entries in will be pushed to 0. But why are the other entries pushed towards 1? Are there situations where some are around 0.5? In such cases, how should step $t$ be handled? Is there any explanation?**\n\n**A7:** Actually, our constraint on $\\mathbf{s}$ enforces most of $s_i$\nconverge to either 0 or 1. The detailed explainations are given in **A3 of Reviewer MPYQ** . In Figure 3 of the revision, we present more results on the histogram of the values of $s_i$ on CIFAR-10.  In all our experiments, we observed that few  elements of $s_i$ do not converge to  0 and 1, which can hardly been seen in the histogram. The elements are always close to 0 or 1, introducing negligible randomness in our final sampled denoising steps, i.e., when fully trained our denoising trajectory is almost deterministic. \n\n**Q8: Under Eqn.(11), it should be $\\nabla_{\\mathbf{\\theta}} \\Phi(\\theta, \\mathbf{s})$ and $\\nabla_{\\mathbf{s}} \\Phi(\\theta, \\mathbf{s})$.**\n\n**A8:** Thanks. We have corrected it in the revision.\n\n\n**Q9: Eqn.(12) is confusing because $\\gamma_e$ is not used in the algorithm. Is this a typo? Is it the case that $K=\\gamma_e$ * T, and $\\gamma_e$ is expressed as in equation 12?**\n\n**A9:** A more detailed expression of $\\gamma_e$ is given in Eqn.(12) in the revision.  $\\gamma_e$ is used to define the constraint region in the current epoch $e$ and we use the projected gradient descent to update $\\mathbf{s}$ and gradient descent to train $\\theta$. That is,\n\n  $ \\theta = \\theta - \\eta \\nabla_{\\theta}\\Phi (\\theta, \\mathbf{s}) \\mbox{ and } \\mathbf{s} = proj_{\\mathcal{S}}\\left(\\mathbf{s} - \\eta \\nabla_{\\mathbf{s}} \\Phi (\\theta, \\mathbf{s})\\right),$\n\nwhere $\\mathcal{S} = \\\\{ \\mathbf{s}\\in \\mathbb{R}^T: ||\\mathbf{s}||_1\\leq K_e, \\mathbf{s}\\in [0,1]^T \\\\}$ \n with $K_e = \\gamma_e T.$\n \n$proj_{\\mathcal{S}}(\\cdot)$  is the projection on $\\mathcal{S}$. The projection can be efficiently computed with the details added in Theorem 1 in the appendix of the revision. How the constraint induces sparsity on $\\mathbf{s}$ is explained in **A3 of Reviewer MPYQ**. \n\n\n**Q10: In the context of image synthesis, the baseline comparison is limited. There are many acceleration sampling algorithms proposed for image synthesis, such as DPM-solver (Lu et al., 2022). While these works are mentioned in the related work section, they are not compared in the experiments.**\n\n**A10:** Due to the space limitation, we add 4 latest  sampling acceleration methods in Table 1 for comparison. Our EDDPM has more significant superiority  when using fewer denoising steps, especially achieves 4.89 FID with 5 steps."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9106/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700147829226,
                "cdate": 1700147829226,
                "tmdate": 1700147829226,
                "mdate": 1700147829226,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "litKzRZDCK",
                "forum": "lhZEodF8Dn",
                "replyto": "o7UExaPj4b",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9106/Reviewer_jbL3"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9106/Reviewer_jbL3"
                ],
                "content": {
                    "title": {
                        "value": "Reply to the authors' rebuttal"
                    },
                    "comment": {
                        "value": "Thanks for the authors' feedback. Most of my concerns are addressed, and I decide to increase the score to 6."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9106/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700618912021,
                "cdate": 1700618912021,
                "tmdate": 1700618912021,
                "mdate": 1700618912021,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "OCkAit54pB",
            "forum": "lhZEodF8Dn",
            "replyto": "lhZEodF8Dn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9106/Reviewer_v4Co"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9106/Reviewer_v4Co"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new skipping scheme, EDDPM, for denoising diffusion models. EDDPM uses parameterized probabilistic masking to decide whether to skip a diffusion time step for faster sampling speed. The proposed method probabilistically reparameterizes the discrete masking selection problem into a tractable continuous optimization problem. The experiment shows significant improvement in generation time (without losing much quality), and the learned masking scheme will reduce to deterministic masking."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The proposed EDDPM method is new. The idea of adding learnable probabilistic masking to the diffusion model is reasonable and seems to be new in the literature.\n2. The derivation of the method seems to be solid.\n3. The performance of EDDPM is good."
                },
                "weaknesses": {
                    "value": "1. There are some typos and citation errors to be fixed. For example: two periods appear at the end of the first paragraph, [Bao et al., 2022a] and [Bao et al., 2022b] are duplicates, etc."
                },
                "questions": {
                    "value": "1. Is the sparse masking result (like Fig. 2 (b)) pervasive across different datasets? \n2. I am wondering how you deal with the $\\ell$-1 norm constraint on $s$ during training. Is it through projection? Is the training stable with the stochastic gradient estimator in Eq. (11)?\n3. Do you have plans to release the code for open access?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9106/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9106/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9106/Reviewer_v4Co"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9106/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698736535035,
            "cdate": 1698736535035,
            "tmdate": 1699637145817,
            "mdate": 1699637145817,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jP1lQGtchl",
                "forum": "lhZEodF8Dn",
                "replyto": "OCkAit54pB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9106/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9106/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer v4Co"
                    },
                    "comment": {
                        "value": "Thanks for your constructive comments.\n\n**Q1: There are some typos and citation errors to be fixed. For example: two periods appear at the end of the first paragraph, [Bao et al., 2022a] and [Bao et al., 2022b] are duplicates, etc.**\n\n**A1:** Thanks for your detailed review. We have revised them in the new version.\n\n\n\n**Q2: Is the sparse masking result (like Fig. 2 (b)) pervasive across different datasets?**\n\n**A2:** Yes, the constraint on $\\mathbf{s}$ enforces  most of the  mask scores converge to either 0 or 1. The reasons are discussed in **A3 of Reviewer MPYQ**. We also presented the results on CIFAR-10 dataset in the appendix. In all our experiments, we observed that few  elements of $s_i$ do not converge to  0 and 1, which can hardly been seen in the histogram. The elements are always close to 0 or 1, introducing negligible randomness in our final sampled denoising steps, i.e, when fully trained our denoising trajectory is almost deterministic. \n\n\n\n**Q3: I am wondering how you deal with the l-1 norm constraint on s during training. Is it through projection? Is the training stable with the stochastic gradient estimator in Eq. (11)?**\n\n**A3:** Yes, we adopt projected gradient descent to update $\\mathbf{s}$ in each iteration. We would like to point out that the projection can be computed efficiently, whose details are given in Theorem 1 in the appendix of the revision. The training is stable since the dimension of $\\mathbf{s}$ is relatively low compared with the model weights $\\theta$. \n\n**Q4: Do you have plans to release the code for open access?**\n\n**A4:** Yes, we will release the code soon."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9106/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700146821481,
                "cdate": 1700146821481,
                "tmdate": 1700146821481,
                "mdate": 1700146821481,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "uyFu5hdnke",
            "forum": "lhZEodF8Dn",
            "replyto": "lhZEodF8Dn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9106/Reviewer_1NMC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9106/Reviewer_1NMC"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new method called EDDPM to improve the sampling efficiency of denoising diffusion models. The key idea is to assign a probabilistic binary mask to each diffusion timestep, indicating whether it should be skipped or kept. The mask probabilities are learned jointly with the diffusion model to identify and eliminate redundant steps.  EDDPM is evaluated on image synthesis and time series imputation tasks. It efficiently compresses diffusion models to 20% of steps yet achieves equal or better performance than baseline methods."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The proposed work addresses the critical challenge of inefficient sampling in diffusion models via a very clever and novel probabilistic masking approach. The formulation and inference are elegant and impressive.\n\n- The experiment settings and results are solid. It's impressive to see the proposed work can achieve state-of-the-art performance in time series imputation and image synthesis with 5-20% of steps, and enables efficient step-compression of large diffusion models through one-epoch.\n\nThe presentation is also clear and easy to follow."
                },
                "weaknesses": {
                    "value": "- The policy-gradient-based update for the prob masking is more like reinforcement learning, rather than the Bayesian variational inference. As the classical VI-based update is also feasible for inferring the Bern distribution, more discussion is encouraged on why adopting the policy-gradient-based update.  \n\n\n\n- For the constrain $K$ of the total step, can the \"Gradually Increasing Masking Rate\" trick guarantee theoretically that the final learned step is constrained by $K$? My understanding is it controls the prob $p$ of the Bern distribution and the final steps are based on the random samples. Also, it's not very clear to me why the learned $s$ is doomed to be almost 0 or 1 . The L1 constraint can do it for sure. However, it seems the training procedure doesn't include the L1 constraint explicitly, but uses the  \"Gradually Increasing Masking Rate\" trick. Clarification on these points is encouraged.     \n\n\n\n- There are some typos and missing things that should be fixed. For example:\n1. The statement under equation 11,  \"we can estimate the gradients of $\\nabla_{\\mathbf{s}} \\Phi(\\theta, \\theta)$\"- should it be $\\nabla_{\\mathbf{s}} \\Phi(\\theta, s)$\n2. The equation 12, what's the meaning of $e_1$?"
                },
                "questions": {
                    "value": "See weakness"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9106/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698793657495,
            "cdate": 1698793657495,
            "tmdate": 1699637145690,
            "mdate": 1699637145690,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "MYwcToukSW",
                "forum": "lhZEodF8Dn",
                "replyto": "uyFu5hdnke",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9106/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9106/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 1NMC"
                    },
                    "comment": {
                        "value": "Thanks for your constructive comments.\n\n**Q1: The policy-gradient-based update for the prob masking is more like reinforcement learning, rather than the Bayesian variational inference. As the classical VI-based update is also feasible for inferring the Bern distribution, more discussion is encouraged on why adopting the policy-gradient-based update.**\n\n**A1:** Our considerations in choosing the gradient estimators are as follows. \n\n(1) Bayesian method could not be a perfect choice for our training problem. The reason is that its performance is sensitive to the selection of priors. In the case of the Bernoulli distribution, the Beta distribution serves as the conjugate prior, displaying significant variation for different parameters, $\\alpha$ and $\\beta$. In the early stage, an inappropriate prior could make the model difficult to converge. \n\n(2) In contrast, for policy gradient, we design a schedule that gradually increase the masking ratio and we can update the model by projected gradient descent efficiently.\n\nThanks for your suggestion and we will conduct an in-depth exploration on the combination of Bayesian methods and our framework in the future. \n\n\n\n\n**Q2: For the constrain K of the total step, can the \"Gradually Increasing Masking Rate\" trick guarantee theoretically that the final learned step is constrained by $K$? My understanding is it controls the prob $p$ of the Bern distribution and the final steps are based on the random samples. Also, it's not very clear to me why the learned $s$ is doomed to be almost 0 or 1 . The L1 constraint can do it for sure. However, it seems the training procedure doesn't include the L1 constraint explicitly, but uses the \"Gradually Increasing Masking Rate\" trick. Clarification on these points is encouraged.**\n\n**A2:** As most elements in  $\\mathbf{s}$ converge to either 0 or 1, the length of the final learned denoising procedure is well constrained by K. The explanation of how the constraint induces sparsity on $\\mathbf{s}$ is provided in **A3 of Reviewer MPYQ**.\n\nWe impose the constrains on the training process as follows. For each epoch, we  define the following constraint region\n$$\\mathcal{S} = \\\\{ \\mathbf{s}\\in \\mathbb{R}^T: ||\\mathbf{s}||_1\\leq K_e, \\mathbf{s}\\in [0,1]^T \\\\},$$\nwhere $K_e = \\gamma_e T$ with $\\gamma_e$ defined in Eqn.(12). This allows us to control the sparsity of the mask $\\mathbf{m}$ by managing the sum of the probabilities $s_i$. In each iteration, we adopt the projected  gradient descent to update $\\mathbf{s}$ based on the constraint region above. The details are given in Eqn.(13) in the revision. As demonstrated by Theorem 1 in the appendix of the revision, this projection can be computed efficiently. As training progresses, $K_e$ gradually decreases to the targeted $K= \\gamma_f T$, leading to a reduction in the number of sampling steps involved.\n\n\n\n\n**Q3: There are some typos and missing things that should be fixed. The equation 12, what's the meaning of $e_1$?**\n\n**A3:** We have provided more detailed expression for $\\gamma_e$ in Eqn.(12). $e_1$ is a positive integer indicating that we train the entire denoising steps in the first $e_1$ epochs."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9106/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700146518445,
                "cdate": 1700146518445,
                "tmdate": 1700146518445,
                "mdate": 1700146518445,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "F5NaKc4OaO",
                "forum": "lhZEodF8Dn",
                "replyto": "MYwcToukSW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9106/Reviewer_1NMC"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9106/Reviewer_1NMC"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the detailed response. It resolves my concerns and questions. I will hold the score and still support the paper."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9106/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700497663747,
                "cdate": 1700497663747,
                "tmdate": 1700497663747,
                "mdate": 1700497663747,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "fi75pBl1if",
            "forum": "lhZEodF8Dn",
            "replyto": "lhZEodF8Dn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9106/Reviewer_MPYQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9106/Reviewer_MPYQ"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a novel accelerated diffusion model, called Efficient Denoising Diffusion (EDDPM). EDDPM eliminates the need for manually selecting denoising steps in previous sampling acceleration methods via probabilistic masking. The probabilistic masking is parameterized to be a Bernoulli random variable and thus can be efficiently learned jointly with the model parameters. After full training, most of the probabilistic masks converge to deterministic values of either 0 or 1, retaining only a small number of important steps. Empirical results demonstrated the sampling efficiency of EDDPM over state-of-the-art sampling acceleration methods on two tasks including time series imputation and image generation."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- This paper is well-structured.\n- The probabilistic masking technique is interesting and novel to the best of my knowledge.\n- Empirically, EDDPM is more sample efficient than other baselines."
                },
                "weaknesses": {
                    "value": "- The paper writing is not good for some parts.\n- The authors did not align their work correctly in the literature.\n- Some parts of the method are not clear."
                },
                "questions": {
                    "value": "- The motivation part of this paper is not persuasive. The authors said that prior works often involve manual selection or the use of handcrafted rules, such as uniform skipping, to determine denoising steps. It is not entirely true for all efficient sampling methods. The authors are referred to check this survey [r1]. The authors may need to narrow down the scope of comparison.\n  - [r1] Yang, Ling, Zhilong Zhang, Yang Song, Shenda Hong, Runsheng Xu, Yue Zhao, Wentao Zhang, Bin Cui, and Ming-Hsuan Yang. \"Diffusion models: A comprehensive survey of methods and applications.\" ACM Computing Surveys (2022).\n- The literature review part of \u201cAcceleration of DPMs\u201d is not well-written. The authors fail to position their work in the literature and summarize the issues of prior work because their chosen scope is too wide.\n- Why is it true? \u201cDue to the constraints on s, i.e., $\\lVert s \\rVert_1 \\leq K$ and $s \\in [0, 1]^T$, the optimal $s$ would be sparse and most of its components would be either 0 or 1.\u201d\n- Section 4.2: Should the masking rate decrease gradually instead? In Eqn. (12), $y_e$ also decreases as e increases from 1 to N. The smaller K is, the fewer the number of steps is. In addition, what is $e_1$?\n- Algorithm 1. \n  - What is the initialization of $s$?\n  - It is unclear how to use $y_e$.\n  - It is also unclear how the sparse constraint is enforced during training.\n- Baselines. How is DDPM with 10% or 20% denoising steps implemented?\n- Table 1. Although the algorithm is unclear, should we still expect EDDPM to become DDPM when using all the denoising steps?\n\n**Minors**:\n- Some citations are weird.\n  - \u201cHowever, this decoupled approach can lead to suboptimal performance (Song et al., 2020; Bao et al., 2022c; Liu et al., 2022; Bao et al., 2022b).\u201d \u2192 It is unclear what is the purpose of putting citations here.\n  - \u201cSohl-Dickstein et al. (Sohl-Dickstein et al., 2015) firstly introduced diffusion probabilistic models (DPMs) that they can convert one distribution into a target distribution, in which each diffusion step is tractable.\u201d and many more\u2192 There is a rule to put citations at the beginning of a sentence. Please follow it.\n- \u201cThe training of diffusion models involves a weighted variational bound derived from the connection between diffusion probabilistic models and denoising score matching with Langevin dynamics..\u201d This sentence is not correct.\n- \u201cBao et al. (Bao et al., 2022c;b) proposed to estimate the optimal covariance in each timestep of the reverse process\u201d. This sentence has a loose connection with previous sentences. It is unclear what are the benefits of the development.\n- \u201c(Luhman & Luhman, 2021) compressed the diffusion process by combining the GANs and DPMs, and the proposed model only needs one sampling step for generation.\u201d This paper uses knowledge distillation. There is no combination of GANs and DPMs.\n- The word reduced variance variational bound is confusing as reduced variance refers to another concept.\n- There are some typos and grammatical errors, please correct it. To name a few: \n  - our method DDPM \u2192 our method EDDPM\n  - thorough \u2192 through\n  - In Section 4.1: $\\tilde{\\beta}t = \\frac{1 - \\alpha{t-1}(m)}{1 - \\alpha_t(m)} \\beta_t m_t$\n  - Line 10 in Algorithm 1: $\\nabla_\\theta \\Phi(\\theta, s) \\to \\nabla_s \\Phi(\\theta, s)$\n- Eqn. (4) (and its related sentences) should be put above Eqn. (2).\n- Bulleted listings should be avoided when writing. In Section 5, some parts bulleted listing should be converted to paragraphs.\n- The caption of Table 1 lacks the notation of the second-best method."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9106/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9106/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9106/Reviewer_MPYQ"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9106/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698908092028,
            "cdate": 1698908092028,
            "tmdate": 1700513286883,
            "mdate": 1700513286883,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "YTVeuGPgPP",
                "forum": "lhZEodF8Dn",
                "replyto": "fi75pBl1if",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9106/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9106/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer MPYQ [1]"
                    },
                    "comment": {
                        "value": "Thanks for your constructive comments. \n\n**Q1: The motivation part of this paper is not persuasive. The authors said that prior works often involve manual selection or the use of handcrafted rules, such as uniform skipping, to determine denoising steps. It is not entirely true for all efficient sampling methods. The authors are referred to check this survey [r1]. The authors may need to narrow down the scope of comparison.**\n\n**A1:** When referring to learning-free efficient sampling methods, it is common for them to rely on handcrafted rules. We appreciate your reminder. In our revision, we have adhered to the survey [r1], which categorizes existing studies into learning-free and learning-based methods. Specifically, we have focused on reviewing the most relevant approaches, particularly the learning-based ones, in the related work section.\n\nIt is worth noting that learning-free methods often employ handcrafted rules, whereas learning-based methods decouple the training and inference schedules. This decoupling allows for separate learning of the training and sampling schedules. However, both approaches have the potential to result in suboptimal performance.\n\n**Q2: The literature review part of \u201cAcceleration of DPMs\u201d is not well-written. The authors fail to position their work in the literature and summarize the issues of prior work because their chosen scope is too wide.**\n\n**A2:** We appreciate your valuable suggestion. Given the extensive range of efficient sampling studies available, it is challenging to review all of them in the related work section. To ensure a comprehensive overview, we adhere to the survey [r1], which categorizes existing studies into learning-free and learning-based methods. In the revised version, we position our work within the realm of learning-based efficient sampling methods and focus on reviewing the most relevant studies in the related work section.\n\n\n**Q3: Why is it true? \u201cDue to the constraints on $\\mathbf{s}$, i.e., $\\|\\mathbf{s}\\|_1 \\leq K$ and $\\mathbf{s}\\in [0,1]^T$, the optimal would be sparse and most of its components would be either 0 or 1.\u201d**\n\n**A3:** It is true and we would like to explain this property in the following three aspects:\n\n1) Prof. Robert Tibshirani, the author of the well-known sparse learning method lasso, provides an explanation of this property from a geometric perspective in pages 10-12 (Fig.2.2) of his book titled **Statistical Learning with Sparsity: The Lasso and Generalizations**. To be precise, the optimization problem of lasso is equivalent to the following one with some $t$:\n$$\\min_{\\mathbf{\\beta}}|| \\mathbf{y}- \\mathbf{X}\\mathbf{\\beta}||^2, \\mbox{ s.t. } \\sum_{i=1}^p |\\beta_i| \\leq t. $$\nNote that the constraint region above is a diamond ($p=2$) or a rhomboid ($p>2$). The optimal solution is the point, where the elliptical contours of the loss hit this constraint region. When the dimension $p=2$, \u201cthe diamond has corners; if the solution occurs at a corner, then it has one parameter $\\beta_j$ equal to 0. When $p>2$, the diamond becomes a rhomboid, and has many corners, flat edges, and faces; there are many more opportunities for the estimated parameters to be zero\" (refer to page 12 of the above book). The situation in our problem is essentially the same with lasso, the only difference is  that our constraint region $\\sum_{t=1}^T |s_t| \\leq K, \\mathbf{s}\\in [0,1]^T$ has more corners (i.e., the coordinates are 0 or 1) than that of lasso, therefore, the optimal $s_t$ has a high probability to be either 0 or 1. \n\n2) We can also understand this property from the Complementary slackness in KKT condition, which says \u201c$=$\" has to be hold in the \u201c$\\leq$\" inequality constraints ($s_t \\leq 1$ and $-s_t \\leq 0$ in our problem) has to be hold as long as the corresponding dual variables  are nonzero. The dual variables have a high probability to be nonzero, as they are not imposed with sparse constraints. Please refer to wiki page \"https://en.wikipedia.org/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions\". for more details.   \n\n3) We empirically verified this property in Fig.2(b).\n\n\n**Q4: Section 4.2: Should the masking rate decrease gradually instead? In Eqn.(12), \n $\\gamma_e$ also decreases as $e$ increases from $e_1$ to N. The smaller K is, the fewer the number of steps is. In addition, what is $e_1$?**\n\n**A4:** 1) $\\gamma_e$ is the ratio of the remaining steps in the current epoch $e$. We have provided more detailed expression in Eqn.(12). 2) We train the full denoising sequence in the first $e_1$ epochs and then the remaining ratio $\\gamma_e$ decreases gradually to the targeted ratio $\\gamma_f$ according to Eqn.(12)."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9106/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700145036616,
                "cdate": 1700145036616,
                "tmdate": 1700578338273,
                "mdate": 1700578338273,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "047tVSXCL8",
                "forum": "lhZEodF8Dn",
                "replyto": "fi75pBl1if",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9106/Reviewer_MPYQ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9106/Reviewer_MPYQ"
                ],
                "content": {
                    "title": {
                        "value": "Respone to authors' rebuttal"
                    },
                    "comment": {
                        "value": "I thank the authors for patiently reading and responding to all my questions. I found that my concern was adequately addressed. In addition, the revised paper has improved in terms of both the related works and methodology parts. \n\nFurther note: The authors should add A3 to the appendix to ease the understanding of the paper.\n\nI would like to increase my score to 6 and incline to an acceptance."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9106/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700513098947,
                "cdate": 1700513098947,
                "tmdate": 1700513243198,
                "mdate": 1700513243198,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PXWjXwXr5V",
                "forum": "lhZEodF8Dn",
                "replyto": "fi75pBl1if",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9106/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9106/Authors"
                ],
                "content": {
                    "title": {
                        "value": "A3 is included in the new version."
                    },
                    "comment": {
                        "value": "Dear Reviewer MPYQ,\n\nAccording to your suggestion, we have included A3 into Section C of the appendix and updated the submission. Thanks.\n\nAuthors"
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9106/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700578522497,
                "cdate": 1700578522497,
                "tmdate": 1700579381092,
                "mdate": 1700579381092,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]