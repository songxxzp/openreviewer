[
    {
        "title": "Zero and Few-shot Semantic Parsing with Ambiguous Inputs"
    },
    {
        "review": {
            "id": "SUHlBStOUa",
            "forum": "qL9gogRepu",
            "replyto": "qL9gogRepu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4330/Reviewer_dZKu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4330/Reviewer_dZKu"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses an important class of tasks: evaluating semantic parsing outside of the realm of 1:1 mappings from form to meaning, potentially including an actual distribution over meanings.   They develop a dataset of templatically generated text-LF pairs where each sentence maps to two possible interpretations, use smart annotation of scalar rankings to get distributions over those ambiguities, and evaluate models on their ability to generate both possible meanings given some prior in-context-learning prompts, and ability to match the distribution of human judgements and to match inbalances of interpretion in the ICL inputs.  They find that models (most of the codegen variety) do quite bad as \"zero shot\" prediction of these LFs, somewhat well at producing a best LF in certain conditions but also quite poorly at generating well-calibrated distribution over the two choices."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- It is a well-written paper with both clear reasoning and clear explanation, enough detail to be replicable and with the promised of publication of data and code. \n- It served to target specific questions about how models handle these ambiguities (e.g. the mapping token-level confidence to LF outputs), and is properly planned out to answer those questions. \n- Both the human judgments annotation and the constrained decoding seem very rigorously done."
                },
                "weaknesses": {
                    "value": "- While just a quibble, I feel like the combination of ICL and very constrained templates makes their zero-shot setting very hard, and it's hard to draw conclusions from their findings there."
                },
                "questions": {
                    "value": "- In more real-world semantic parsing tasks there can be dramatically more than just two possible semantic parses. Would this approach (particularly use of EASL in collecting human judgements) still work in contexts with far more acceptable parses per sentence?\n-  Insofar as all models were equally poor at a number of the zero-shot tasks at ZM_5, did the authors check the effect of raising that k parameter? \n- I was curious about the focus on capturing the training data biases rather than the human annotation judgements, for the few-shot setting."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4330/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4330/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4330/Reviewer_dZKu"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4330/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698819467736,
            "cdate": 1698819467736,
            "tmdate": 1699636402519,
            "mdate": 1699636402519,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "f0fvLRxurX",
                "forum": "qL9gogRepu",
                "replyto": "SUHlBStOUa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4330/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4330/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer dZKu"
                    },
                    "comment": {
                        "value": "Thank you for your comments and questions about our work, and for your attention in reviewing it. We've attempted to address the comments and questions from your review below and in the updated paper. \n\n**Weaknesses:**\n- *\u201cI feel like the combination of ICL and very constrained templates makes their zero-shot setting very hard\u2026\u201d*\nThanks for making this point. The constrained nature of the templates is indeed a limitation of our approach \u2013 however, we would like to note that the use of constrained decoding does alleviate this concern somewhat. Because the model is constrained to use the grammar of our templates, it can only generate syntatically-correct outputs. Thus, the model has help in generating valid templates, even if the template format is not what the model would naturally generate during open generation. Nevertheless, the point that open-source models struggle with generating correct LFs of either kind in the 0-shot setting is well-taken, and is an area for further research. \n\n**Questions:**\n- *\u201cIn more real-world semantic parsing tasks there can be dramatically more than just two possible semantic parses. Would this approach (particularly use of EASL in collecting human judgements) still work in contexts with far more acceptable parses per sentence?\u201d*\nThe human judgment collection would be substantially more difficult given >2 parses, but still possible. We could in principle use the same interface, and change the confidence scale. Right now, 0 confidence is translated to p=0.5 and 1 is mapped to p = 1. The logic behind this is that if the annotator were below p=0.5, they would have chosen the other option with some confidence > 0. This could be extended to multiple options, i.e. given 3 options, 0 confidence would mean p = 0.33. Alternatively, given multiple options, we could apply ranking-based approaches for data collection instead. \n- *\u201cInsofar as all models were equally poor at a number of the zero-shot tasks at ZM_5, did the authors check the effect of raising that k parameter?\u201d*\nDue to computational constraints, we were unable to run higher batch sizes at scale. In our preliminary investigations, for a small subset of examples we found that the models we could run generally have poor beam diversity even with higher beam sizes, generally generating versions of the same output. After a certain point, the models tend to generate outputs that are structurally similar but have incorrect predicates in them, e.g. the predicate boy(x) might be used even though \u201cboy\u201d doesn\u2019t appear in the input. This suggests that increasing the beam size is unlikely to change the ZM scores by very much in practice. \n- *\u201cI was curious about the focus on capturing the training data biases rather than the human annotation judgements, for the few-shot setting.\u201d*\nThanks for this point, it suggests an interesting direction for future work. In developing the FIM/FDM scores, we did consider whether to compare against human judgements. We opted to instead compare the range of possible ratios for two reasons. Firstly, we found in our human judgements that ambiguity is highly variable between examples, meaning we would need large amounts of data to make an accuracy comparison. For example, in Fig. 6 (now Fig 5.) PP attachment ambiguity examples range from 0 to 1. Averaging this to say that annotators generally think they have P(LF0) = 0.5 would obscure this variation, so we\u2019d need instance-level annotations for all our evaluation examples. Secondly, ambiguity may not be a fixed quantity: we found that annotators differ confidently in their interpretations. Additionally, as pointed out by reviewer PRrS, there might be cultural differences in interpretation. Thus a fixed sample of interpretations might not be valid across different groups of people. We opted for metrics that subsume a metric based on estimates of judgments. Because we range the percentage of examples in the prompt from 0-100% in FDM and FIM, the true interpretation ratio should be found somewhere on this range. For example, if 30% of people interpret an example as LF0 and 70% interpret it as LF1, then this should be captured by FDM/FIM, since they test for this ratio. Marginalizing across ratios allows us to be agnostic about what the \u201ctrue\u201d ratio might be. Future work could expand this to a finer granularity \u2013 we considered increments of 10%, but given more examples we could consider finer increments."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4330/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700257587879,
                "cdate": 1700257587879,
                "tmdate": 1700257587879,
                "mdate": 1700257587879,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bGzReAHUl9",
                "forum": "qL9gogRepu",
                "replyto": "f0fvLRxurX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4330/Reviewer_dZKu"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4330/Reviewer_dZKu"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for their answers to my questions! I'm satisfied with those answers and appreciate the additional detail.  I do  appreciate some of the concerns voiced by other reviewers regarding impact, but it still seems to me to make worthwhile progress on a difficult domain, and so have decided to keep my current score."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4330/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700683381871,
                "cdate": 1700683381871,
                "tmdate": 1700683381871,
                "mdate": 1700683381871,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "dKogOi61DJ",
            "forum": "qL9gogRepu",
            "replyto": "qL9gogRepu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4330/Reviewer_49nJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4330/Reviewer_49nJ"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposed a task to convert an ambiguous sentence to its logical form to evaluate LLMs with zero-shot and few-shot evaluation. The paper studies five types of ambiguity and evaluates LLMs with a decent number of examples. They also proposed multiple metrics to interpret the model results. The dataset and experiment all together provide insights into how LLMs understand ambiguity and its difference from human ambiguity understanding."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The motivation and writing are very clear. The paper is generally easy to follow.\n2. I like the human probability vs. model probability experiments personally, and seeing that humans have certain preferences on one interpretation than the other is interesting, and model prediction somehow matches it as well is very interesting too."
                },
                "weaknesses": {
                    "value": "1. The generation task is hard, especially generating logical forms. Why not formulate this as a multi-choice problem?  Letting the model choose two from 10 possible combinations?\n2. Is there any quantitative analysis? What kind of errors does the model usually make?\n3. The evaluation metric can be improved. I have several questions about this. Why not use the same zero-shot and few-shot metric since the output format is the same? Why not use language interpretation instead of LF generations? Language interpretation is much more intuitive for the model than LF generations."
                },
                "questions": {
                    "value": "1. Why not report ZM 100 in Figure 4?\n2. In section 3.1, the zero-shot experiment, how do you get multiple predictions? Do you sample k times? Your prompt doesn\u2019t encourage the model to predict two LFs, so the model only predicts one by demonstration. The zero-shot results could get better if you encourage two predictions in your prompt.\n3. in figure 2, any intuition of why cg-2b is outperforming cg-6b on LF1?\n4. It's more like a suggestion: Do you plan to extend the ambiguity to more than two interpretations?\n\nNotes:\n\n- Section 2.1 Metrics. The last sentence is confusing: \u201cThe higher, the better. \u201c If you are talking about k values, shouldn't it be the lower, the better? If it refers to the metric, then it should be explicit.\n- Equation 1 is very confusing. Do i  and k mean the same thing here?\n- Section 3, paragraph 2, \u201cHere\u201d\u21d2 \u201cIn this setting\u201d\n- Figure 4 is a Table, not a Figure.\n- Figure 5 is a bit hard to read when it comes to different types of semantic ambiguity. You can choose to put a different text color for important tokens inside the blue circle to increase the readability.\n- In Section 4, paragraph 2, what does \u201c1 sentence at a time\u201d mean when the percentage of the LF0 sentences is from 0 to 100 in increments of 10?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4330/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4330/Reviewer_49nJ",
                        "ICLR.cc/2024/Conference/Submission4330/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4330/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698894154336,
            "cdate": 1698894154336,
            "tmdate": 1700583443257,
            "mdate": 1700583443257,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "oiS5G22Dsl",
                "forum": "qL9gogRepu",
                "replyto": "dKogOi61DJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4330/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4330/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer 49nJ (weaknesses)"
                    },
                    "comment": {
                        "value": "Thank you for your attention to our work and your thorough questions and feedback. We have made several changes to the paper, which we hope address your concerns and which we highlight below.\n\n**Weaknesses**\n1. \u201c...Why not formulate this as a multi-choice problem?...\u201d \nThis is a good question, which we have sought to address further based on your feedback. The reason to explore this problem semantic parsing (as opposed to multiple choice or open-ended language generation) is because of semantic parsing\u2019s real-world applications. Parsing is used directly in real-world systems that have real consequences (e.g. robots, digital assistants, etc.). Ideally, these systems would be able to operate in open-ended environments, i.e. environments where we cannot frame the problem as multiple choice QA (since we don\u2019t always know the choices, or there are too many to enumerate). Thus, while we\u2019re interested in model\u2019s performance on ambiguous language generally, we focus specifically on semantic parsing because of its practical applications. We agree that the results in a multiple choice setting may be very different from what we have presented, but these results would have little bearing on semantic parsing. We have added further motivation for exploring semantic parsing specifically in the introduction and in the discussion of implications of our work in the discussion (highlighted in blue).\n2. *\u201cIs there any quantitative analysis? What kind of errors does the model usually make?\u201d*\nBased on this helpful suggestion, we\u2019ve added a qualitative analysis to the appendix describing typical errors the model makes for each ambiguity type. We find that the best open-source model makes different errors depending on the ambiguity class, but many errors involve omitting key predicates or quantifiers. \n3. \n    1. *\u201c Why not use the same zero-shot and few-shot metric since the output format is the same?\u201d*\nThe expected/desired behavior in the zero and few-shot settings is different. In the zero-shot setting, we don't know what the \"true\" output distribution is, so the metric just looks at whether the model puts enough mass on both interpretations to generate them. In the few-shot setting, we're giving the model evidence of the expected distribution, so we have a reference point, and can quantify whether the model captures that distribution. Thus, we need different metrics for the two settings. \n    2. *\"Why not use language interpretation instead of LF generations? Language interpretation is much more intuitive for the model than LF generations.\"*\nThis is certainly true. However, in addition to the reason mentioned above for multiple choice (we care about semantic parsing, as opposed to general language generation) evaluation is very hard for language interpretation, and generally requires human evaluation, while automated evaluation for semantic parsing is easier since the form is constrained to a specific language."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4330/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700257402671,
                "cdate": 1700257402671,
                "tmdate": 1700257402671,
                "mdate": 1700257402671,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vDLyajBcfL",
                "forum": "qL9gogRepu",
                "replyto": "J1cLdZ1OhA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4330/Reviewer_49nJ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4330/Reviewer_49nJ"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the answers"
                    },
                    "comment": {
                        "value": "Thank you for the answers, I updated my score."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4330/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700583486087,
                "cdate": 1700583486087,
                "tmdate": 1700583486087,
                "mdate": 1700583486087,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "UkKbjonFa0",
            "forum": "qL9gogRepu",
            "replyto": "qL9gogRepu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4330/Reviewer_Qhxa"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4330/Reviewer_Qhxa"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a benchmark suite and set of corresponding metrics for measuring models' behavior in the face of ambiguity in a semantic parsing task. The task is set in an in-context learning setting where models are provided with examples of (natural language, logical form) pairs and asked to translate a new natural language sentence into a logical form (LF).\n\nThere are two test settings:\n* A \"zero-shot\" setting where the model is expected to infer the existence of an ambiguity in the input example that did not show up in its few-shot examples. For example, by Appendix A.2, the model might see the LF for `Mary with the camera` (which, side note, seems ungrammatical to me at least when interpreted in the relevant sense) and \"Galileo saw with the camera\" (which...also seems ungrammatical absent context), it's tested on whether it recovers that \"Galileo saw Mary with the camera\" is ambiguous, by checking if both possible interpretations get high enough probability (or ranking) in its output distribution.\n* A \"mixed prompting\" setting where the model is given ambiguous examples in the prompt which resolve one way or the other a given proportion of the time (e.g., attaching high or low) and it is tested on the degree to which it approximates this ambiguity resolution behavior in its outputs (either on a dataset level in what it ends up decoding on lots of examples, or on an instance level in how it allocates relative probability)."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The paper aims at an important problem (handling ambiguity in semantic parsing).\n* The setup is clever and allows for some interesting analyses. I think the looking at the token-level confidences to see how model uncertainty is reflected in ambiguity-resolution\u2013dependent choice points, as done in Figure 5, is a useful idea.\n* The comparison to human behavior (Section 3.2) is interesting and I imagine could seed future experiments."
                },
                "weaknesses": {
                    "value": "I'm worried about how we assign meaning to the various results, and I'm not sure how this result would feed into future work that helps parsers handle ambiguity better.\n\n1. Human experiments: humans were given both interpretations and asked to assign confidences to them. This seems a bit different from what the models were asked to do in the zero-shot experiments, which is implicitly pick out the ambiguity on their own. I understand it'd be hard to elicit this kind of behavior from humans \u2014 ideally you would ask them a question whose answer hinges on the ambiguity and get some measure of the uncertainty in their answer, but it's unclear what that measure would be. The measured degree of uncertainty from humans in this setting may quite exceed the actual uncertainty when reading and interpreting language in practice, as humans may simply snap to one reading or the other. So it may not be fair to compare this result to model results \u2014 though of course, there is no clear analogy between what humans and models are doing anyway, so I'm not sure what the exact criteria for a fair comparison should be. It might seem like a safer bet to compare humans to models on a similar task, where the model is given both interpretations and is asked to give a confidence score, or repeat the one that it prefers \u2014 for example, I think it'd be a strict improvement on the current paper to do the human comparison to the FIM relative confidence results instead. But of course, the confidences reported through that process may not be ecologically valid with respect to the paper's broader goal of characterizing LM behavior on ambiguous semantic parsing examples.\n2. More broadly, the paper relies on a mostly implicit assumption that the _appropriate_ behavior for an LM is to represent multiple possible readings in its top-k outputs, or for the probabilities of each possible reading to reflect the proportions of related readings in their prompts / ICL data. It isn't obvious to me that this is what we want or that it is the right way of approaching the question of whether LMs can handle ambiguity in semantic parsing appropriately. When prompted for a single output, why _should_ the model distribute its probabilities in any particular way? There's nothing that says that we have to use the top-k candidates in order to represent a set of semantically distinct alternatives, and indeed it is suboptimal for this purpose, as the paper notes that the top-k results are usually variations of the same interpretation. I can see how this may make sense if we were to, for example, use the LM probabilities as a prior to guide some kind of search through the space of parses; we then want all plausible interpretations to be discoverable. But it's not clear that's the best strategy in practice if we're going to be using LMs anyway: why not just ask it, for example, to decode all possible interpretations in sequence? There's a big space of possible things to do here if we want models to handle ambiguity, and unfortunately the results of these experiments only bear on a small subset whose promise is unclear. Not that the experiments done are bad or invalid, but I think the paper needs to make a clear argument about the nature of the construct being tested and what these results are supposed to inform.\n3. On that note, it's unclear to me from the arguments in the paper how these results can actively guide future efforts to build semantic parsing systems that handle ambiguity better. I think the burden does lie on this paper to make that case, at least in principle (I'm not saying it needs more ML experiments)."
                },
                "questions": {
                    "value": "Section 3.1:\n* I think Figure 2 would be better as a grouped bar chart? It doesn't really make sense to me to have multiple models along a line here.\n* \"we rarely observe any models predicting both LFs correctly; one exception to this is conjunction ambiguity\" \u2014 seems also true for `bound`?\n\nSection 6:\n* \"statements be ambiguous\" (typo)\n* \"Making this assumption can reduce semantic parsing to syntactic parsing\" \u2014 it's very unclear to me what this means\n* The argument in the second to last paragraph that datasets need to have multiple judgments seems wrong to me. If there is indeed annotator disagreement, this disagreement will show up implicitly spread across multiple examples. Learning to maximize the likelihood of the data should teach the model the right kind of uncertainty. Multiple annotations aren't actually necessary for learning this \u2014 though they might be necessary for _evaluating_ it."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4330/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698964163001,
            "cdate": 1698964163001,
            "tmdate": 1699636402383,
            "mdate": 1699636402383,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "OaCijEIegl",
                "forum": "qL9gogRepu",
                "replyto": "UkKbjonFa0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4330/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4330/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Qhxa"
                    },
                    "comment": {
                        "value": "Thank you for your thorough and insightful comments and questions. We have attempted to address most of the points made in our uploaded draft, where edits can be seen in blue. Below, we address each point in more detail. \n\n**Weaknesses**\n- *\u201cHuman experiments\u2026\u201d*\n\nWe are in agreement that the human results are imperfect, and have added a section on limitations of our human judgements to the appendix contrasting our study with standard methods in psycholinguistics and providing further justification for our choices. Regarding a comparison between human judgments to FIM: as-is, this judgment would not be a completely fair comparison, since human annotators were not shown any examples, while the model is shown 10 ICL examples with a particular ratio of LF1 to LF2. \n- *Assumptions about desired behavior*\n\nThank you for pointing out this assumption. We do state the desired behavior of the model at the beginning of paragraph 3, but have added clarification about what the actual \u201cmodel\u201d is. As you astutely put it, having a distribution over interpretations makes sense if we are to \u201cuse the LM probabilities as a prior to guide some kind of search through the space of parses\u201d. Since actual generations are being produced by beam search \u2013 which exactly is a search through the space of parses, guided by the LM\u2019s probabilities \u2013 I think that we are in fact in agreement on the importance of the probabilities. We have added a point to introduction, clarifying that what is considered the \u201cparser\u201d is not only the LLM but also the search procedure used to actually decode parses (in our case, beam search). In terms of why we use beam search rather than sequentially decoding parses: we are aiming here to remain fairly close to how semantic parsing models are used in practice, rather than optimizing for performance on ambiguous examples specifically. We have clarified this in our limitations section. \n- *Guiding future research*\n\nThanks for this feedback, which is shared with Q1 from reviewer PRrS. In light of this, we have updated the discussion section to include more direct recommendations for how we can improve semantic parsing under ambiguity (and why we might want to). \n\n**Questions/Typos**\n- *\u201cI think Figure 2 would be better as a grouped bar chart? It doesn't really make sense to me to have multiple models along a line here.\u201d*\nBased on your recommendation, we tried this plot style. We found that it was hard to parse which ambiguity type belonged to which model, since many of them (in fig. 2) are 0, so there are large gaps in the bar plot. These make it hard to determine which bars correspond to which model, which is clearer from the line plot. \n- *\u201c\"we rarely observe any models predicting both LFs correctly; one exception to this is conjunction ambiguity\"\u2014 seems also true for bound?\u201d*\nThanks for pointing this out \u2013 we have amended the text accordingly. \n- We have clarified the point about semantic parsing being reduced to syntactic parsing. If there is a 1-to-1 mapping between syntax and meaning, then semantic parsing can be handled by a syntactic parser (this is of course not the case for real data).\n- Re. the point on \u201cmultiple judgments\u201d: Thank you for this observation, it is well-taken. We have amended the section to clarify that redundant annotations would only be required for the evaluation data, but that the train data requires diverse annotations. (see the added footnote 8)"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4330/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700256840313,
                "cdate": 1700256840313,
                "tmdate": 1700256840313,
                "mdate": 1700256840313,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "x78TtdicYz",
            "forum": "qL9gogRepu",
            "replyto": "qL9gogRepu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4330/Reviewer_PRrS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4330/Reviewer_PRrS"
            ],
            "content": {
                "summary": {
                    "value": "The paper explores the longstanding issue of ambiguity in natural language and its implications for semantic parsing in AI. It delves into the nature of language ambiguity, highlighting how it stems from the balance of communication efficiency and interpretive flexibility, and poses challenges for AI systems that lack human-like commonsense knowledge and context. To address these challenges, the authors propose a novel framework and the Ambiguous Parsing (AMP) dataset, which includes various types of ambiguities paired with dual logical forms (LFs). This resource is aimed at enhancing the performance of large language models (LLMs) in semantic parsing tasks, especially in handling ambiguity.\n\nThe study introduces two tasks to assess how well LLMs, utilizing in-context learning (ICL), can capture multiple interpretations of an ambiguous input. These tasks are designed to evaluate model performance in both zero-shot and few-shot settings, with a series of metrics developed to quantify their ability to predict and represent ambiguity. The paper also reports on models' performance, noting that while models can sometimes mirror human preference for certain interpretations, they generally fall short in predicting all possible parses. Additionally, it is observed that some models are quite adept at reflecting the distribution of interpretations in mixed-prompt scenarios, offering insight into in-context learning amidst conflicting evidence."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1. The AMP dataset is a significant contribution, providing a resource specifically designed for investigating ambiguity in semantic parsing, which is a relatively unexplored area.\n\n2. The paper takes a comprehensive approach by addressing the challenge from the perspective of both dataset creation and model evaluation.\n\n3. The introduction of zero-shot and few-shot tasks offers a rigorous evaluation framework for future research on ambiguity in semantic parsing.\n\n4. The development of new metrics to assess the models\u2019 ability to handle ambiguity is a noteworthy contribution that can guide subsequent model development.\n\n5. The results contribute interesting insights into the capabilities and limitations of current LLMs in capturing ambiguity through zero-shot and in-context learning."
                },
                "weaknesses": {
                    "value": "1. While the paper provides a strong foundation, it could benefit from a more detailed exploration of how ambiguity affects real-world applications of semantic parsing.\n\n2. The AMP dataset, while novel, might still be limited in scope and diversity, potentially affecting the robustness of the study\u2019s conclusions.\n\n3. It is unclear how the proposed methods deal with the dynamic nature of conversational context, which can significantly affect ambiguity resolution."
                },
                "questions": {
                    "value": "1. How do you foresee the findings of this research being applied in practical AI systems, particularly in areas where ambiguity can have significant consequences, like in legal or healthcare settings?\n\n2. Is the AMP dataset extensible, and are there plans to include more complex or nuanced forms of ambiguities, such as cultural or idiomatic ones?\n\n3/ Could you elaborate on the selection process for the five types of natural language ambiguities included in your study? Were there other types of ambiguities considered but excluded?\n\n4. The use of synthetic data might not fully capture the complexity of natural language ambiguities encountered in real-world scenarios. How well do the findings translate to naturally occurring datasets?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4330/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699030498050,
            "cdate": 1699030498050,
            "tmdate": 1699636402315,
            "mdate": 1699636402315,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "P71YwnJ1dZ",
                "forum": "qL9gogRepu",
                "replyto": "x78TtdicYz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4330/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4330/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer PRrS (weaknesses)"
                    },
                    "comment": {
                        "value": "We would like to thank the reviewer for their thorough review and attention to our work. We have uploaded a new draft of the paper (revisions in blue) where we have sought to address the weaknesses pointed out. We describe our changes/reasoning in more detail below:\n\n**Weaknesses**\n1. *\u201c[the paper] could benefit from more detailed explanation of how ambiguity affects real-world applications of semantic parsing.\u201d*\nWe have added more details to the introduction, motivating why we examine semantic parsing and how misunderstandings could arise from ambiguity. \n2. *\u201cThe AMP dataset, while novel, might still be limited in scope and diversity, potentially affecting the robustness of the study\u2019s conclusions.\u201d*\nThis point is well-taken \u2013 while our study is an initial exploration into the underexplored area of ambiguity in semantic parsing, it is far from complete in terms of the range of possible ambiguities (and languages, settings, cultures, etc.) that could be explored. We have clarified our limitations subsection in Section 6 to emphasize that AMP can be extended both structurally (covering scope) and lexically (covering diversity). \n3. *\u201cIt is unclear how the proposed methods deal with the dynamic nature of conversational context, which can significantly affect ambiguity resolution.\u201d*\nThank you for pointing this out \u2013 this is definitely a limitation of our approach, since conversational context often reduces ambiguity. We have highlighted where we state this assumption in the intro and have added it to the limitations. Removing context is realistic in the \u201cdigital assistant\u201d domain, since most people would not accept a digital assistant that is constantly listening, i.e. by design, assistants have access to only parts of conversations. Note that some ambiguity is likely to persist even with context, as ambiguity commonly leads to breakdown in human conversation."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4330/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700257696360,
                "cdate": 1700257696360,
                "tmdate": 1700257696360,
                "mdate": 1700257696360,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]