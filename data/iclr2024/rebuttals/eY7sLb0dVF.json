[
    {
        "title": "Generative Modeling of Regular and Irregular Time Series Data via Koopman VAEs"
    },
    {
        "review": {
            "id": "a3kqYqyqKt",
            "forum": "eY7sLb0dVF",
            "replyto": "eY7sLb0dVF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6799/Reviewer_nGir"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6799/Reviewer_nGir"
            ],
            "content": {
                "summary": {
                    "value": "Although the existing GAN-based time-series generation showed good performance, they proposed a Koopman-based VAE model, citing problems such as unstable training or mode collapse. In addition, KVAE is a relatively simple model but it shows better performance in regular and irregular time-series than baseline models."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "Originality: There was an existing model that applied the VAE model to time-series generation, but its performance was lacking. This paper showed good performance in time-series generation by applying Koopman to VAE. In addition, while the existing GAN-based model had many hyperparameters due to unstable training, KVAE is very simple and shows good performance.\n\nQuality: In addition to various experiments conducted in previous research, the effectiveness of KAVE is clearly demonstrated through additional experiments such as physics-constrained generation.\n\nClarity: Very well written and easy to read.\n\nSignificance: Time-series generation suffered from many problems due to complex training. However, in this paper, it shows good performance with very easy learning."
                },
                "weaknesses": {
                    "value": "1. There is a lack of explanation about Koopman in the Background section. In the case of this paper, the main point is Koopman, and readers may also want to know more about Koopman. Therefore, an explanation of Koopman should be in the main paper.\n\n2. There are no results for the predictive loss term in the ablation study in Section 5.5.\n\nMinor issues\n1. For Section 5, Experiments seems to be a more appropriate word than Results.\n2. It seems that 0 should be excluded from the MuJoCo results in Table 10."
                },
                "questions": {
                    "value": "I am curious about the role of the predictive loss term. In this paper, a predictive loss term was added to the object function. Therefore, I am curious about how much the predictive loss term affects the performance that is superior to existing baseline models."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6799/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6799/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6799/Reviewer_nGir"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6799/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698635086711,
            "cdate": 1698635086711,
            "tmdate": 1700740014931,
            "mdate": 1700740014931,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xuSkegudeV",
                "forum": "eY7sLb0dVF",
                "replyto": "a3kqYqyqKt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6799/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6799/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We express our gratitude to Reviewer nGir for their favorable remarks regarding the simplicity of our model, the superior effectiveness of our approach, and the clarity of our writing. Furthermore, we appreciate their valuable contributions in posing significant questions and offering constructive suggestions for enhancing the quality of our paper. In what follows, we address the specific comments put forth by Reviewer nGir. Given the chance, we will be happy to integrate the suggested modifications outlined below into the final revision.\n\n>There is a lack of explanation about Koopman in the Background section. In the case of this paper, the main point is Koopman, and readers may also want to know more about Koopman. Therefore, an explanation of Koopman should be in the main paper.\n\nFollowing the reviewer's suggestion, we moved the appendix section that gives background information on Koopman theory and practice into Section 3 (Background) in the main paper.\n\n>There are no results for the predictive loss term in the ablation study in Section 5.5.\n\nRegrettably, there seems to be a typo in our ablation results in Table 4. Specifically, the submitted paper mentions $\\beta$, suggesting that we removed the KL divergence term that minimizes the (pseudo-)distance between the prior and posterior distributions. However, in our ablation study, we tested the effect of the predictive loss term and the obtained results without utilizing this component. Thus, all appearances of $\\beta$ in the ablation study should be replaced with $\\alpha$.\n    \n>For Section 5, Experiments seems to be a more appropriate word than Results.\n\nFollowing the reviewer's suggestion, we changed the section heading of Section 5 to read Experiments.\n\n>It seems that 0 should be excluded from the MuJoCo results in Table 10.\n\nThank you for noticing this. We fixed it.\n\n>I am curious about the role of the predictive loss term. In this paper, a predictive loss term was added to the object function. Therefore, I am curious about how much the predictive loss term affects the performance that is superior to existing baseline models.\n\nIndeed, ablating the effect of the predictive loss term is important. This ablation appears in Table 4 (after correcting the typo $\\beta \\rightarrow \\alpha$). The results indicate that the benefit of the predictive loss term can be significant. Notable examples are stocks regular, energy $30\\%$, and stocks $50\\%$. More generally, removing this loss term results in inferior error estimates in most cases."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6799/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700219577257,
                "cdate": 1700219577257,
                "tmdate": 1700219577257,
                "mdate": 1700219577257,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "32b7Vjvpdl",
                "forum": "eY7sLb0dVF",
                "replyto": "xuSkegudeV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6799/Reviewer_nGir"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6799/Reviewer_nGir"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the response to my questions. \nI also looked at other reviewers' questions and the author's responses to them.\nI have no further questions and will raise my score."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6799/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700630588372,
                "cdate": 1700630588372,
                "tmdate": 1700630588372,
                "mdate": 1700630588372,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UgSkuVNuRe",
                "forum": "eY7sLb0dVF",
                "replyto": "wn1eKE0882",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6799/Reviewer_nGir"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6799/Reviewer_nGir"
                ],
                "content": {
                    "comment": {
                        "value": "The discussions above are excellent, and I wish I could see them in the revised manuscript, which I think would help readers better understand the contributions of this article.\n\nGiven that my major concerns have already been addressed, I am now happy to update my rating to 8. Best of luck to the authors!"
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6799/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700740065970,
                "cdate": 1700740065970,
                "tmdate": 1700740065970,
                "mdate": 1700740065970,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "TDLL2R3Nli",
            "forum": "eY7sLb0dVF",
            "replyto": "eY7sLb0dVF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6799/Reviewer_HuLD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6799/Reviewer_HuLD"
            ],
            "content": {
                "summary": {
                    "value": "In this study, the authors propose a generative model using a variational auto-encoder. The variational auto-encoder employs the neural controlled differential equations to consider irregular time series and uses a GRU to march in time. The authors proposed to use a prior distribution with a linear transition function. It is shown that the proposed model outperforms previous models in the generation task."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The authors proposed to use a linear operator for the prior in the training of the variational auto-encoder. The proposed method is relatively straightforward and it is shown that the method outperforms some of the existing models for generation tasks."
                },
                "weaknesses": {
                    "value": "The novelty of the study is limited. While the authors claim their method is based on the Koopman operator, their model is, in fact, more similar to the dynamic linear model, or Kalman filter. Also, one of the claims is that the model can take care of irregular time series, the capability is simply using a pre-existing model, neural controlled differential equations. Moreover, the manuscript is not well written. The probabilistic description and the models are not clearly defined. There are a few concerns about their model. See the comments below,"
                },
                "questions": {
                    "value": "1. I know trying to optimizer both the prior and posterior in a loss function has become a trend in the deep learning community. However, theoretically speaking, trying to optimize the prior and posterior jointly in the KL loss leads to a ill-posed problem, where a unique solution does not exist. Simply put, it becomes a ping-pong game between the prior and posterior. You can easily show it by computing the parameters of the distributions in a local minima. How do you deal with this ill-posedness? \n\n2. What is $z_t$ and what is $y_t$? Are they different random variables? Based on the paper, it looks like both $y_t$ and $z_t$ denote the latent code, meaning that they are the same variable. I understand that the authors used $y_t$ and $z_t$ to distinguish between the prior and posterior latent code. But the way it is formulated now is not correct. For example, how do you define $KL[q(z)\\|p(y)]$ in eq. (3)? Shouldn't It be $KL[q(z)\\|p(z)]$ or $KL[q(y)\\|p(y)]$?\n\n3. What is the probabilistic model for $p(x_{1:t}|z_{1:t})$? Is it a parametric distribution, e.g., normal distribution? How do you compute the log likelihood function?\n\n4. If the modulus of the eigenvalues of $A$ is not strictly 1, i.e., $|\\lambda| =1$, the system either grows or decays exponentially fast. It should be a hard constraint, not a soft constraint. How do you guarantee this?\n\n5. Based on Eq (6), $y_t$ becomes deterministic once $y_{t-1}$ is observed. Then the probability distribution becomes a delta function, $p(y_t|y_{1:t}) = p(y_t|y_{t-1}) = \\delta (y_t - Ay_{t-1})$. How do you compute the KL divergence of the Dirac delta distribution?\n\n6. How do you find the correct initial condition $y_0$ to represent $x_{1:T}$. As discussed by the authors, $x_{1:T}$ is transformed to $y_{1:T}$. Then, since the model is linear, once $y_0$ is determined, the rest of the sequence is determined as $y_t = A^t y_0$. Hence, it is crucial to find $y_0$ that describes $x_{1:T}$ the best. How do you choose $y_0$ and how do you guarantee that the choice of $y_0$ is the optimal?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6799/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698635524006,
            "cdate": 1698635524006,
            "tmdate": 1699636785818,
            "mdate": 1699636785818,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "AN0NFKEJiZ",
                "forum": "eY7sLb0dVF",
                "replyto": "TDLL2R3Nli",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6799/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6799/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We would like to thank Reviewer HuLD for identifying the simplicity of our model and our strong results. Furthermore, we appreciate their valuable contributions in posing important questions and seeking clarifications that contribute to the enhancement of our paper. In the following, we provide responses to the comments raised by Reviewer HuLD. Given the chance, we would be pleased to integrate the suggested modifications outlined below into a final revision.\n\n>... their model is, in fact, more similar to the dynamic linear model, or Kalman filter.\n\nWe agree with Reviewer HuLD that the dynamic linear model and Kalman filter are related to our approach. The main difference between these techniques and Koopman-based methods is the domain where the dynamics are investigated. For the dynamic linear model and Kalman filter it is the state space, whereas in Koopman methods it is the observable (function) space. In practice, given inputs from the state space $x_{1:T}$, the dynamic linear model and Kalman filter can be viewed as studying a dynamical system $A$ respecting the equation $x_t = A x_{t-1}$. In contrast, Koopman approaches transform states to the observables domain, and study the dynamics there. Namely, $\\psi(x_t) = A \\psi(x_{t-1})$, where $\\psi$ is a nonlinear transformation. Our framework, KVAE, uses nonlinear encoder and decoder, and it employs linearity in the latent space. Therefore, we believe it aligns better and can be justified more profoundly from a Koopman perspective.\n\n>Also, one of the claims is that the model can take care of irregular time series, the capability is simply using a pre-existing model, neural controlled differential equations.\n\nIndeed, we use NCDE to handle irregularly sampled time series information, but we do not state our model's ability to handle irregular time series as a contribution. The ability to handle irregular time series is simply a feature of our model. In retrospect, we believe that utilizing NCDE is a strength of our framework as it provides strong results and it could be potentially replaced by any other module that interpolates time series data.\n\n>... How do you deal with this ill-posedness?\n\nThank you for raising this great question. In our experience, we did not observe any issues during training or inference due to the mentioned ill-posedness. Furthermore, our ablation study in the revised version Section 5.3 and Table 4 considers a model whose prior is not learned ($\\alpha=0$, no GRU). The results show a significant drop in discriminative scores in comparison to KVAE's performance. Thus, while jointly optimizing the prior and posterior may be ill-posed in theory, the strong performance we obtain outweighs this potential shortcoming. If given the opportunity, we would be happy to discuss this limitation in the final version.\n    \n>What is $z_t$ and what is $y_t$? Are they different random variables? ...\n\nRegrettably, there seems to be a typo in our notation. Thank you for pointing this out. We fixed this issue in the revised version, where we always use $z$ for the latent variable.\n\n>What is the probabilistic model for $p(x_{1:t} | z_{1:t})$? Is it a parametric distribution, e.g., normal distribution? How do you compute the log likelihood function?\n\nWe would like to thank the reviewer for pointing this out. Following a common choice in probabilistic modeling, we assume that $p$ is a Gaussian distribution whose mean is the output of the decoder. Under this assumption, the log likelihood becomes a simple MSE between $x_{1:T}$ and the output of the decoder, \n$\\tilde{x}_{1:T}$. \n\nA similar reasoning applies also for the $L_\\text{pred}$ term. We added a new discussion in the revised version Appendix C, describing technical details related to the evaluation of the loss function."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6799/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700220994545,
                "cdate": 1700220994545,
                "tmdate": 1700220994545,
                "mdate": 1700220994545,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gDxzUNjX0z",
                "forum": "eY7sLb0dVF",
                "replyto": "AN0NFKEJiZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6799/Reviewer_HuLD"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6799/Reviewer_HuLD"
                ],
                "content": {
                    "comment": {
                        "value": "I am not fully convinced with the argument on the difference between the Kalman filters and the present model. Yes. The authors are correct that in the classical Kalman filters everything is linearized to make the training analytically tractable. But recent developments of Kalman filters, for example see Bezenac et al. \"Normalizing Kalman Filters for Multivariate Time Series Analysis\" (Neurips 2020), nonlinear encoder and decoder are used for the forward and backward projections and the time evolution of the latent state is given by a linear map. The current method seems to be a restrictive case of the Normalizing KF model, as their model explicitly treats $z_t$ as a random variable and allows a time-depedent linear operator. Please, note that I am not trying to belittle the present study. But I think it is worth mentioning the similarity between the current model and the well-established classical approach."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6799/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700518575056,
                "cdate": 1700518575056,
                "tmdate": 1700518575056,
                "mdate": 1700518575056,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KzU7BZ8APG",
                "forum": "eY7sLb0dVF",
                "replyto": "TDLL2R3Nli",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6799/Reviewer_HuLD"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6799/Reviewer_HuLD"
                ],
                "content": {
                    "comment": {
                        "value": "Now, I am confused.\n\n1.  is $p(z_{1:T}) = p(\\bar{z}_{1:T}) \\prod_t \\delta (z_t - A \\bar{z}_t-1)$ a proper distribution? Does it normalize to one? Why does $\\bar{z}$ not appear in the left-hand side? I don't understand how you can approximate a Dirac delta function by one.\n\n2. I don't understand why the eigenvalues of $A$ is not so important in this method. As the authors wrote, they may compute $z_{2:t}$ by multiplying $A$ to $\\bar{z}$. But this is just a way to construct a linear system. At the end, since they are using the Koopman operator, the latent state should satisfy the relation, $z_{t+n} = A^nz_t$. Then, they end with the same problem. If one of the eigenvalues is bigger than one, $z_{t+n} \\rightarrow \\infty$, or if less than zero, the model end up with a degenerate system."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6799/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700519454258,
                "cdate": 1700519454258,
                "tmdate": 1700519473185,
                "mdate": 1700519473185,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fJ2mBu2EFF",
                "forum": "eY7sLb0dVF",
                "replyto": "TDLL2R3Nli",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6799/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6799/Authors"
                ],
                "content": {
                    "comment": {
                        "value": ">I am not fully convinced with the argument on the difference between the Kalman filters ... But I think it is worth mentioning the similarity between the current model and the well-established classical approach.\n\nThank you for sharing this reference. We totally agree with the reviewer that our approach should be also discussed and related to the Kalman filter and the above mentioned work. In the final version, we will cite this work and discuss it in the background section."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6799/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700548962607,
                "cdate": 1700548962607,
                "tmdate": 1700549021707,
                "mdate": 1700549021707,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "g1t3Qkv8G2",
            "forum": "eY7sLb0dVF",
            "replyto": "eY7sLb0dVF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6799/Reviewer_SWgv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6799/Reviewer_SWgv"
            ],
            "content": {
                "summary": {
                    "value": "A variant of VAE for time-series data is proposed. Technically a notable point of the proposed model lies in its prior model. It first samples a sequence $\\bar{y}$ based on the outputs of a GRU, thus the dynamics of $\\bar{y}$ can be nonlinear. Each $\\bar{y}$ is refined to be $y$ by the linear transformation with the DMD matrix computed on the sequence of $\\bar{y}$. Then there appears a regularization term to minimize the discrepancy between $y$ and $\\bar{y}$, which effectively imposes \"soft\" linearity on the dynamics of $y$, the final output of the prior model."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The proposed method is reasonable, and the experiments are convincing enough to see the superiority of the method especially in terms of generation. \n\nThe literature is nicely reviewed, and the paper is adequately placed in the relevant contexts.\n\nI cannot really assess the novelty and the significance in terms of time-series generation. On the other hand, in terms of Koopman-operator-based neural net architectures, the proposed model seems somewhat novel yet technically straightforward."
                },
                "weaknesses": {
                    "value": "From a purely technical point of view, the contribution might look rather incremental. So the paper should be assessed rather in the context of time-series generation models, on which I am not really an expert and thus cannot provide an accurate evaluation.\n\nThere is a GRU in the decoder part, which makes it a little difficult to assess the benefit of the Koopman-based prior model. As GRU can provide a nonlinear sequence-to-sequence transformation, it is unclear if the linear structure of $y_{1:T}$ was really beneficial when generating $x_{1:T}$. The results could be more convincing if the decoder did not have the GRU; instead, it should have had a nonlinear **pointwise** (i.e., not sequence-to-sequence) transformation such as a multilayer perceptron applied to each timestep independently. An ablation study with such a change of architecture would be highly informative.\n\n----\n\nBelow are minor points.\n\n- Why do you use two different letters, $y$ and $z$, for the prior part and the posterior part, respectively? Usually in VAE papers, the latent variable is always $z$, and we just say $p(\\cdot)$ for prior and $q(\\cdot)$ for posterior. The current notation in the paper might also be okay, but I just wondered if there could be particular intention to use the two letters.\n- Although the paper focuses on the generation capability of the models, some more experiments on the reconstruction / inference capablity could also be interesting."
                },
                "questions": {
                    "value": "(1) As stated above, the presence of GRU in the decoder makes it a little difficult to assess the real utility of the linear structure in the prior model. Do you have some observations when you did not use a nonlinear sequence-to-sequence model in the decoder?\n\n(2) In practice, how linear the sequence of $y$ is? In my understanding, the linearity of the dynamics of $y_{1:T}$ is not a hard constraint but rather is imposed in a soft manner as regularization. I am curious to what extent the $y$ could become linear with such a soft constraint."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6799/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698761207830,
            "cdate": 1698761207830,
            "tmdate": 1699636785706,
            "mdate": 1699636785706,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jVdPK8zeJA",
                "forum": "eY7sLb0dVF",
                "replyto": "g1t3Qkv8G2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6799/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6799/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We would like to thank Reviewer SWgv for acknowledging the novelty of our approach, the positioning of the paper, and the compelling experiments. We also would like to thank them for their detailed comments, questions and suggestions that help to deepen our discussion and improve the paper. Below, we address the comments raised by Reviewer SWgv. Given the opportunity, we will be happy to incorporate the modifications listed below into a final revision.\n\n>... An ablation study with such a change of architecture would be highly informative.\n\nTo assess the contribution of our Koopman-based prior, we suggested the ablation study in Table 4. There (after fixing the typo $\\beta \\rightarrow \\alpha$), we tested the behavior of the model without the predictive loss term, i.e., no linearity is imposed. Our results demonstrate that KVAE performs better in the majority of cases in comparison to the model with $\\alpha=0$. Thus, while the GRU decoder may help in obtaining good results, it is not enough, and the Koopman-based prior improves these results further.\n\nWhen designing KVAE, we opted for a GRU decoder in order to match existing generative models for a fair comparison. Modifying the decoder will have a strong impact on the performance. Naturally, using a stronger decoder may improve the results significantly, giving an unfair advantage to the approach. Similarly, employing a weaker decoder may damage the results. Thus, the decoder in all existing baselines should change to an MLP decoder for a fair and consistent comparison.\n    \n>Why do you use two different letters, $y$ and $z$, for the prior part and the posterior part, respectively? ...\n\nRegrettably, there seems to be a typo in our notation. Thank you for pointing this out. We fixed this issue in the revised version, where we always use $z$ for the latent variable.\n\n>Although the paper focuses on the generation capability of the models, some more experiments on the reconstruction / inference capablity could also be interesting.\n\nThank you for suggesting that. Following your suggestion, we added two new figures to the revised version showing several reconstruction and inference examples. Specifically, we plot in Fig. 15, the reconstructed signals in the regular setting for all datasets (dashed lines) vs. the ground-truth data (solid lines). In addition, we show in Fig. 16, the inferred signals (dashed lines) in the irregular 50\\% case. This experiment demonstrates the inference capabilities of our model since half of the samples were removed during training. We plot the complete signals (solid lines) for comparison.  In addition, we want to emphasize that our climate experiments could also be considered as inference test cases. To evaluate our model abilities in the climate setting, we omit some of the signals, which we refer to as \"points of interest\", and generate them to compare with the ground truth.\n\n>... Do you have some observations when you did not use a nonlinear sequence-to-sequence model in the decoder?\n\nIn general, the predictive loss term improves the results in comparison to models without the predictive loss term (i.e., $\\alpha=0$). This is shown in our ablation study in Table 4. In addition, our preliminary results with an MLP decoder show a similar trend. For further details regarding using an MLP decoder, please see the response above.\n    \n>In practice, how linear the sequence of $y$ is? ...\n\nIndeed, we preferred to promote linearity using a soft constraint instead of a hard constraint. We believe that it provides the learning algorithm with the flexibility to enforce linearity if possible. In addition, if the data is too complex or the encoder is not expressive enough, the predictive loss term will be high, serving as a tool for identifying potential modeling problems. In practice, the datasets we consider in this work admit a high-level of learnable linearity as we show in Table 1 below. Specifically, we report the relative predictive loss error, i.e., \n$$|z_{2:T} - A\\bar{z}_{1:T-1}|^2 / |\\bar{z}_{1:T-1}|^2$$,\nwhere $z_{2:T}, \\bar{z}_{1:T-1}$ are the latent variables and $A$ is the approximated Koopman operator. Importantly, all errors are below $6e-2$, where some errors are one- and two-orders of magnitude below that.\n\n| Data   |   | Regular   | 30\\%      | 50\\%      | 70\\%      |\n|--------|---|-----------|-----------|-----------|-----------|\n| Sines  |   | $4.14e-3$ | $2.32e-3$ | $4.29-3$  | $1.32e-2$ |\n| Stocks |   | $1.11e-3$ | $4.84e-2$ | $5.86e-2$ | $3.35e-4$ |\n| Energy |   | $5.79e-4$ | $3.88e-4$ | $2.23e-3$ | $6.73e-3$ |\n| MuJoCo |   | $1.81e-2$ | $9.22e-3$ | $1.35e-4$ | $8.20e-3$ |"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6799/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700220459512,
                "cdate": 1700220459512,
                "tmdate": 1700220459512,
                "mdate": 1700220459512,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "IIyeIm204c",
                "forum": "eY7sLb0dVF",
                "replyto": "jVdPK8zeJA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6799/Reviewer_SWgv"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6799/Reviewer_SWgv"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the response. After revisiting the paper, I would like to say that the idea of the soft constraint of linearity of latent variables sounds great. And the additional results provided about the attained linearity can also be a nice piece of information as experimental results if elaborated more.\n\nBy the way, on the decoder:\n\n> When designing KVAE, we opted for a GRU decoder in order to match existing generative models for a fair comparison. Modifying the decoder will have a strong impact on the performance. Naturally, using a stronger decoder may improve the results significantly, giving an unfair advantage to the approach. Similarly, employing a weaker decoder may damage the results. Thus, the decoder in all existing baselines should change to an MLP decoder for a fair and consistent comparison.\n\nAlthough I understand the practical issue of experimentation as the authors pointed out, I still think that the use of a nonlinear RNN in the decoder of \"Koopman (V)AE\" is somewhat misleading because we usually premise that the state, $x_t$, can be hopefully recovered from the value of the observable, $f(x_t)$. When the decoder is an RNN, the view becomes different because an RNN may use the information of $f(x_t')$ where $t' \\neq t$ to reconstruct $x_t$. This is why I commented about the use of GRU in the decoder.\n\nHowever on second thought, I now think that the use of RNN as decoder may be somewhat justified by considering that $x_t$ cannot be recovered only from $f(x_t)$.\n\nOn the other hand, the use of RNN in the encoder side is easier to justify because it can be regarded as a kind of nonlinear delay-coordinate embedding."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6799/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700575167641,
                "cdate": 1700575167641,
                "tmdate": 1700575167641,
                "mdate": 1700575167641,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Oe3GZn8gvn",
            "forum": "eY7sLb0dVF",
            "replyto": "eY7sLb0dVF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6799/Reviewer_kti2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6799/Reviewer_kti2"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a variational autoencoder, Koopman VAE (KVAE), for time series data based on Koopman theory. The idea is to use a linear map to represent the prior dynamics, alongside a nonlinear coordinate transformation (the encoder) that maps the data to a linear representation. The main features of KVAE is that (i) it can incorporate domain knowledge (in the prior) by placing constraints on the eigenvalues of the linear map; (ii) the behaviour of the system can be analysed using dynamical systems theory tools. The results in the paper are promising, showing that KVAE outperforms SOTA GANs and VAEs across synthetic and real world time series generation benchmarks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Experimental results indicate strong performance compared to GANs and VAEs\n- The use of linear latent dynamics simplifies the learning of the latent dynamics and allows for adding physical constraints, as indicated in section 4.3"
                },
                "weaknesses": {
                    "value": "- A large literature of sequential VAEs for time series data generation is omitted e.g. [1,2,3,4], despite a large number of baselines being used in the experiments section. Considering there is heavy development in this area, it would be useful to compare KVAE to these methods.\n- More discussion in the experiments section is required on the topic of analysing \"the behaviour of the system...using dynamical systems theory tools\" in order to claim this as an additional feature of KVAE\n\n\n[1] Chung et al. (2015). A Recurrent Latent Variable Model for Sequential Data\n\n[2] Rubanova et al. (2019). Latent ODEs for Irregularly-Sampled Time Series\n\n[3] Li et al. (2020). Scalable Gradients for Stochastic Differential Equations\n\n[4] Zhu et al. (2023). Markovian Gaussian Process Variational Autoencoders"
                },
                "questions": {
                    "value": "N/A"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6799/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6799/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6799/Reviewer_kti2"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6799/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698784939127,
            "cdate": 1698784939127,
            "tmdate": 1699636785588,
            "mdate": 1699636785588,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xZrueeFpQG",
                "forum": "eY7sLb0dVF",
                "replyto": "Oe3GZn8gvn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6799/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6799/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We extend our appreciation to Reviewer kti2 for recognizing the robust performance of our approach, its simplicity, and its capability to incorporate physical constraints. Moreover, we are grateful for their insightful comments and suggestions aimed at enhancing the paper and delving into a more profound discussion. Below, we provide responses to the comments raised by Reviewer kti2. If given the opportunity, we would gladly integrate the suggested modifications outlined below into the final revision.\n\n>A large literature of sequential VAEs for time series data generation is omitted e.g. [1,2,3,4], despite a large number of baselines being used in the experiments section. Considering there is heavy development in this area, it would be useful to compare KVAE to these methods.\n\nThank you for bringing these works to our attention. We agree with the reviewer that sequential VAEs are actively developed in recent years, on a broad range of applications. In our work, we focused on comparing with baseline generative approaches that target the same benchmarks and evaluation environment, for a fair and consistent evaluation of approaches. That being said, we are also happy to consider papers [1-4]. Specifically, paper [1] can be viewed as a close variant of our approach. In particular, omitting the predictive loss term in KVAE forms method [1]. The results of this method appear in our ablation study in Table 4 in the submitted paper. Please notice that there was a typo there, and all appearances of $\\beta$ should be replaced with $\\alpha$ (we have corrected this in the revised manuscript). Further, we are currently running method [2], and we will report the results as soon as we have them. As for method [3], it can be viewed as a VAE only in the time-continuous limit. Nevertheless, we are currently working on migrating the available code to fit into our training and evaluation framework. Finally, we could not find an open-source implementation of method [4], unfortunately. We started to implement this approach from scratch, however, the time constraints of the rebuttal may be too limiting to obtain preliminary results.\n\n>More discussion in the experiments section is required on the topic of analysing \"the behaviour of the system...using dynamical systems theory tools\" in order to claim this as an additional feature of KVAE\n\nFollowing the reviewer's suggestion, we extended the discussion in Section 5.2 on the physics-constrained generation example. The revised text details the particular loss term $\\mathcal{L}_\\text{eig}$ that we use. Moreover, we analyze the nonlinear pendulum system from a stability viewpoint, where Koopman eigenvalues represent growth and decay modes. The discussion is complemented by Figure 3, which shows the spectra of Koopman operators for unconstrained KVAE vs. constrained KVAE. Our results indicate that constraining the eigenvalue modulus improves generation results and positively affects the operator's spectrum. We also compute the spectra associated with the computed operators in the regular setting and further discuss the results in Appendix F."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6799/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700220676630,
                "cdate": 1700220676630,
                "tmdate": 1700220676630,
                "mdate": 1700220676630,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]