[
    {
        "title": "DisCo: Disentangled Control for Realistic Human Dance Generation"
    },
    {
        "review": {
            "id": "c3gwZ9dd7a",
            "forum": "sk7RRHFk7M",
            "replyto": "sk7RRHFk7M",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3885/Reviewer_Zusg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3885/Reviewer_Zusg"
            ],
            "content": {
                "summary": {
                    "value": "The authors introduce a disentangled representation of dance motion that separates the content and style of the motion, allowing for more control over the generated motion. They use a combination of motion capture data and a variational auto to learn this representation and generate new dance motions. The results show that their method is able to generate diverse and realistic dance motions that are controllable in terms of content and style."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Good quantitative results than baselines.\n\n- Motivation is good and sufficient. \n\n- The usage of Grounded-SAM on person and background is technical sound."
                },
                "weaknesses": {
                    "value": "Weakness:\n\n- **Ethics issues. Ethics reviewers are required to review whether double-blind regulations have been violated.**\n    - In code `config/__init__.py`, I found there exist Chinese characters and some codes like `MSRA PC Node`.  As we know, MSRA means Microsoft Research Asia. This would make the reviewer to infer the authors\u2019 nationality and possibly Microsoft affiliation.\n    - The authors did not discuss possible ethical issues with this study. Including issues such as race and gender bias.\n- About Citation\n    - It seems that authors are not clear on the difference between `\\cite` and `\\citep`. For example, \u201c\u2026 ControlNet branch with the pre-trained U-Net weight following (Zhang & Agrawala, 2023).\u201d should be\u201c\u2026 ControlNet branch with the pre-trained U-Net weight following Zhang & Agrawala (2023).\u201d There are more than one similar problem. During rebuttal, please list all of similar issues and revise them, I will check them one by one.\n    - I would like to point out serious issues with the wrong citation on Grounded-SAM. The first implementation of Grounded-SAM is the https://github.com/IDEA-Research/Grounded-Segment-Anything, which has been accepted as an ICCV demo. Please cite it correctly. Besides, if authors used it, please cite Grounding DINO.\n    - Instructpix2pix was accepted CVPR, not an arxiv paper. Please cite it correctly. There is more than one similar problem. During rebuttal, please list all similar issues and revise them, I will check them one by one.\n    - Missing comma in the equation of Section 3.1.\n    - When summarizing contributions, `To address this problem, ...` should be `To address these problems, ...`.\n- Reproduction\n    - When I try to run the code. I found it missing README and I cannot run the code correctly. I am not sure whether the codes are appropriate.\n    - Missing video demo. This is very essential for me to check the results. And the video comparison with baselines is needed.\n- Technical Design\n    - It is hard for readers to check the technical designs in Figure 2. The CLIP feature is fed into TM module (ResBlock and TransBlock). How do the lines without arrows connect to the module? How do the lines with arrows connect to the module?\n    - Do the features output by each layer need to be processed by BG ControlNet and Pose ControlNet and then added to the middle layer? If there are $X$ down layers, in the UNet middle layer, will features be added about $2X$ times?\n- Why not compare with Follow-your-pose?\n\nMy main concern comes from ethics issues and writing issues. There is something confusing that makes readers hard to follow this work. I spent about 8 hours checking this submission and tried to run the codes (but failed). I plan to rate it as 4 but the system only supports 3 or 5. Therefore, I rate it as 3 now. After the rebuttal and reviewer discussions, I will revise my rating to provide a clear rating of accept or reject."
                },
                "questions": {
                    "value": "see weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Discrimination / bias / fairness concerns",
                        "Yes, Unprofessional behaviors (e.g., unprofessional exchange between authors and reviewers)"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "- **Ethics issues. Ethics reviewers are required to review whether double-blind regulations have been violated.**\n    - In code `config/__init__.py`, I found there exist Chinese characters and some codes like `MSRA PC Node`.  As we know, MSRA means Microsoft Research Asia. This would make the reviewer to infer the authors\u2019 nationality and possibly Microsoft affiliation.\n    - The authors did not discuss possible ethical issues with this study. Including issues such as race and gender bias."
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3885/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697723191084,
            "cdate": 1697723191084,
            "tmdate": 1699636347195,
            "mdate": 1699636347195,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Hw9ijg6I2q",
                "forum": "sk7RRHFk7M",
                "replyto": "Gm2V9nTRz8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3885/Reviewer_Zusg"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3885/Reviewer_Zusg"
                ],
                "content": {
                    "title": {
                        "value": "RE: Brief Responses and Clarifications from Authors"
                    },
                    "comment": {
                        "value": "Dear authors,\n\nThanks for the clarification. In Sec 4.1, when mentioning the Grounded-SAM, authors only cite the SAM, but not Grounding DINO. I suggest it is also necessary to cite Grounding DINO here. Otherwise, it may cause misunderstandings among readers and the community. Besides, after contacting the AC/PC, my ethical concern has been resolved. \n\nWish you all the best!\n\nReviewer"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3885/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700271059325,
                "cdate": 1700271059325,
                "tmdate": 1700271059325,
                "mdate": 1700271059325,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "WWGiUQnQ9x",
            "forum": "sk7RRHFk7M",
            "replyto": "sk7RRHFk7M",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3885/Reviewer_vgGD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3885/Reviewer_vgGD"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on human motion transfer in real-world dance scenarios, and introduces a novel model called DISCO for better (i) Generalizability and (ii) Compositionality.\nSpecifically, DISCO applies the ControlNet of background and human pose to disentangle control signals.\nMoreover, it is pre-trained in a proxy task to improve the generalizability to unseen humans.\nA broadly various of evaluations demonstrate the effectiveness of proposed method."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "See summary."
                },
                "weaknesses": {
                    "value": "1. The technical contributions of this paper are limited. The proposed DISCO is mainly a combination of Image-variation Stable Diffusion, Background ControlNet, and Human pose ControlNet.\n2. The authors only provide images or frames in paper, but not videos. Therefor, the temporal consistency of synthesized videos is unconvincing."
                },
                "questions": {
                    "value": "See weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3885/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698466287917,
            "cdate": 1698466287917,
            "tmdate": 1699636347107,
            "mdate": 1699636347107,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "5cOtdLVG2l",
            "forum": "sk7RRHFk7M",
            "replyto": "sk7RRHFk7M",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3885/Reviewer_mmJ4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3885/Reviewer_mmJ4"
            ],
            "content": {
                "summary": {
                    "value": "This paper aims to generate high-fidelity dance videos given three single inputs, reference person image, target pose sketon (2D), target background. This task is similar to traditional pose transfer, while human dance is claimed to be a more challenging task. To this end, the authors propose a control-net based framework, where the foreground and background are taken as seperate inputs. To further strengthen disentanglement, a pretraining strategy is proposed which trains the model on a much larger image dataset. Extensive experiements demonstrate the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "There are several merits in this work:\n1. Using foreground and background segmentations as seperate inputs for pose transferring is new. It seems to provide a neat yet effective solution. I also appreciate the Human Attribute Pre-training. Firstly, it is simple, but effective as well. Secondly, it properly utilizes the large-scale meta data.\n2. The quantitative evaluation results are significant. The proposed model outperforms other methods by a large margin.\n3. The authors do have conducted sufficient experiments (e.g., comparisons, ablation), as well as exploring further applications such as fine-tuning on one person.\n4. Detailed implementation details, and submitted code.\n5. The image transfer results look promising."
                },
                "weaknesses": {
                    "value": "I am not an expertise in video synthesis, here are my feelings about this work. \n\nFirst of all, the authors didn't provide the video demonstration for their work, which is supposed to largely decrease the validity of this work, since the whole highlight is about dance generation. \n\n1. Though the title is about \"dance generation\", I feel the emphysis of this work, including technical design, is more on pose-based image transfer. There is little thing about \"sequence\" modeling for dance.\n2. I won't say the generated dances are realistic (as claimed in title). There are many temporal inconsistency and jittering, though I acknowledge the single image-based editting is realistic. I guess for better video modeling, we should pay more attention on the temporal consistency. (I found the video somewhere else.)\n3. Upper-body video generation is kind of limited. Is there full-body dance dataset available?\n4. Since in diffusion model, generating each image requires hundreds of steps, I am curious how long does it take to generate a dance video. Will that be a limitation of this work?\n5. For video generation, it's necessary to have comparisons with baselines.\n6. Some discussion about limitations are desirable."
                },
                "questions": {
                    "value": "Please refer to the weakness, and may respond to these questions if applicable."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3885/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3885/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3885/Reviewer_mmJ4"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3885/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698637295317,
            "cdate": 1698637295317,
            "tmdate": 1699636347033,
            "mdate": 1699636347033,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "VbtncjC30H",
            "forum": "sk7RRHFk7M",
            "replyto": "sk7RRHFk7M",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3885/Reviewer_j9YJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3885/Reviewer_j9YJ"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a method to generate dance images or videos given reference images of people and background, and poses (or pose sequences, for videos) of the desired dance. The proposed learning model generates the target images or videos by combining attention-pooled CLIP image features for the reference image foregrounds (images of people), and latent features from a ControlNet architecture for the reference image backgrounds and the target pose skeletons. To improve the plausibility of the generated images or videos, the authors also propose a human attribute pre-training strategy to reconstruct the reference images from the foreground and the background features. The authors show the benefits of their proposed method through multiple quantitative and qualitative evaluations and ablation studies."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The proposed approach of separating the foreground and the background features from the reference images to learn their individual modifications given the target pose skeletons is technically sound.\n\n2. The human attribute pre-training makes sense, particularly for the challenging scenario of images/videos with cropped or occluded humans.\n\n3. The ablation studies highlight the benefits of the proposed network and training components."
                },
                "weaknesses": {
                    "value": "1. Since the proposed task of the paper is generative, it requires a human evaluation of the perceived plausibility and overall generation quality. Quantitative metrics do not capture these aspects, and they are commonly covered through various types of user studies in related work. Without such an evaluation, it is hard to assess the impact of the proposed method fully.\n\n2. Have the authors explored or encountered any incompatibilities between the reference images and the target poses? For example, any significant differences in the relative body shapes between the reference and the target, or backgrounds that may not realistically match the target poses? A discussion of such scenarios, or other potential limitations, is important to understand the full scope of the proposed method."
                },
                "questions": {
                    "value": "1. What is the latency of the end-to-end generation pipeline during inference? Is there any component that takes significantly more time than the others, that is, becomes a bottleneck for efficiency?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3885/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698727505323,
            "cdate": 1698727505323,
            "tmdate": 1699636346954,
            "mdate": 1699636346954,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]