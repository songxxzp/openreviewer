[
    {
        "title": "Unnormalized Density Estimation with Root Sobolev Norm Regularization"
    },
    {
        "review": {
            "id": "G58CX1deZl",
            "forum": "fapHf9fmqp",
            "replyto": "fapHf9fmqp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8079/Reviewer_CL4E"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8079/Reviewer_CL4E"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the density estimation using the likelihood with the Sobolev norm regularization. Unnormalized models are employed for non-parametric estimation. The algorithm is implemented using gradient-based learning. The authors proposed sampling approximation for computing the RKHS norm. Then, they investigate the difference between the proposed method (RSR) and the standard kernel density estimator (KDE). Indeed, two estimators provide different results for the separability of the cluster structure. Some numerical experiments indicate that the proposed method outperforms most of the existing works for anomaly detection. Also, computational properties are numerically analyzed."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The formulation of the proposed method (RSR) is simple and easy to understand.\n- The authors studied some properties of the proposed estimator, e.g., the ratio of the estimated probability densities shown in Section 5."
                },
                "weaknesses": {
                    "value": "- The proposed estimator is rather straightforward and has less impact the machine learning community. \n- Once the estimator for the unnormalized model is estimated, how can the estimator be utilized?\n- Section 6.3 reports the ratio of negative values for the estimated function f. I'm not sure why the ratio of negative values is important to assess the computational properties.\n- Sobolev space is closely related to the RKHS with Matern kernel; see [1] below. Some supplementary comments on that relationship would be helpful for researchers interested in theoretical analysis of the learning algorithms.\n\n[1] Gregory E. Fasshauer and Qi Ye, Reproducing kernels of generalized Sobolev spaces via a Green function approach with distributional operators, arXiv:1204.6448."
                },
                "questions": {
                    "value": "- The estimator is similar to the one proposed in Ferraccioli's JRSS paper. Please clarify the main difference between them. \n- Once the estimator for the unnormalized model is estimated, how can the estimator be utilized? Showing an example of using an unnormalized model would be helpful for readers. \n- Section 6.3 reports the ratio of negative values for the estimated function f. I'm not sure the reason why the ratio of negative values is important to assess the computational properties. \n- Sobolev space is closely related to the RKHS with Matern kernel; see [1] below. Some supplementary comments would be helpful for researchers who are interested in theoretical analysis of the learning algorithms. \n[1] Gregory E. Fasshauer and Qi Ye, Reproducing kernels of generalized Sobolev spaces via a Green function approach with distributional operators, arXiv:1204.6448."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8079/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698132882502,
            "cdate": 1698132882502,
            "tmdate": 1699637000773,
            "mdate": 1699637000773,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "okUlTkRzLc",
                "forum": "fapHf9fmqp",
                "replyto": "G58CX1deZl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8079/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8079/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the feedback. \n\n\n\n>Sobolev space is closely related to the RKHS with Matern kernel; see [1] below. Some supplementary comments on that relationship would be helpful for researchers interested in theoretical analysis of the learning algorithms.\n\nThank you for including the reference [1]. It contains a clear relevant results on the Matern kernel in context of Sobolev space and we will include it in the paper.   \nPlease see the discussion in the global \"General Reply\" comment on the relation between the SDO kernel and Matern kernels. \n\n\n>The estimator is similar to the one proposed in Ferraccioli's JRSS paper. Please clarify the main difference between them. \n\nWe discuss the relation with the paper \n\n*Nonparametric density estimation over complicated domains, Ferraccioli et al,  JRSS 2020*\n\nIn the above paper, the authors also use a reparametrisation to ensure non-negativity of the density, and regularise using  a version of a Sobolev space. However, as we discuss in our introduction and literature review sections, \nthat general principle is known since the 1980s for one dimension. The challenge is how one can implement the principle in $d>1$.  \n\n\nThe above mentioned paper *is solely restricted to two dimensions, d=2*. (both for applications and theory). In particular, to obtain their solutions, they discretize (triangulate) the domain of interest in $R^2$. \nThis is clearly infeasible in any higher dimension. \n\n\nIn contrast, the approach we develop, including the natural gradient optimisation and the use of Fisher divergence for parameter tuning, allow us to obtain good anomaly detection performance on datasets with $d>100$. \nMoreover, we consider the fact that such methods *are* useful in high dimensional situations is a significant contribution by itself, since such methods were not considered in recent literature in high dimensions. We will emphasize this more in the discussion in the paper. \nWould the reviewer agree that this is indeed a contribution of interest?\n\n\n> Section 6.3 reports the ratio of negative values for the estimated function f. I'm not sure why the ratio of negative values is important to assess the computational properties.\n\nThis point is important and we now clarify it. \nAs we discuss in the paragraph following Eq. (5), the main optimisation objective (5) is not convex as a function of $f$. Moreover, if one nevertheless tries to optimise (5) in $f$ (i.e., in $\\alpha$, using $f_{\\alpha}$) directly, one would obtain very poor results. The gradient descent will get stuck in a local optimum with poor values. \nWe can provide an experimental demonstration of this if that would be of interest. \n\n\nHowever, we observe that (5) is convex on the non-negative cone, \ni.e., $f$ s.t. $f>0$ everywhere. (It is also sufficient to require  only $f(x_i)>0$ on data points $x_i$).    \nThus, if one restricts to such cone, one can obtain the optimal solution on that cone. We observe that such solutions are empirically much better. \nNext, we note that solving a constrained optimisation problem, requiring the solutions to be restricted to the cone is computationally difficult.  Moreover, since the cone is not closed (topologically), this is also not quite clear theoretically, since projection methods can not be applied. \n\n\nTo resolve this, we observe that  *natural gradient* preserves the cone. \nThis means that one can in fact do *unconstrained* optimisation. \nThis is true for non-negative kernels, such as the Matern kernels (discussion following Equation (5) and Proposition 10), and is approximately true for the SDO kernel.  \n\nIn the experiment in Section 6.3 we show that empirically, for the SDO kernel, natural gradient indeed tends to preserve positivity, while the regular gradient does not. This demonstrates that natural gradient is indeed better for optimisation purposes. Note that, theoretically, this is one of the very few settings were we know *why* the natural gradient is better (i.e. due to positivity). \n\nMoreover, we can also directly show that the results in Fig 2, our main empirical results, would be worse if regular gradient descent is used instead of natural. \n\nDoes this answer make sense? We would be glad to provide additional clarifications. \n\n\n\n\nContinued in the comment below."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8079/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700173064515,
                "cdate": 1700173064515,
                "tmdate": 1700173064515,
                "mdate": 1700173064515,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "okvIMM3QPY",
                "forum": "fapHf9fmqp",
                "replyto": "G58CX1deZl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8079/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8079/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer CL4E,\nThank you again for your feedback. In our earlier comments we have \nanswered the questions posed in the review and addressed the concerns raised in the Weaknesses section. \n\nHave all the concerns been addressed, or are there any issues still remaining?"
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8079/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700594530503,
                "cdate": 1700594530503,
                "tmdate": 1700594530503,
                "mdate": 1700594530503,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "NsMdrPXevM",
            "forum": "fapHf9fmqp",
            "replyto": "fapHf9fmqp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8079/Reviewer_o1rt"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8079/Reviewer_o1rt"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel framework for un-normalized kernelized density estimation that differs from common practice mainly in 2 aspects: \n\n- Kernel density estimation generally uses $ \\hat{f} = \\sum_{i=1}^N \\alpha_i k_{x_i} $ with a non-negative coefficients $\\alpha_i$ and non-negative valued-kernel $k$; the proposes $\\tilde{f} = \\left(\\sum_{i=1}^N \\beta_i k_{x_i}\\right)^2$ with un-constrained $\\beta_i$ and $k$, which trades away normalization with a different objective in regularization. The paper presents contrived example about the difference between the two density estimators and argue in favor of using the squared density estimator for spectral clustering and anomaly detection. \n- The paper proposes the SDO kernel, a stationary kernel whose RKHS norm involves derivatives of $f$ to a certain order. The paper notes that the kernel has a tractable spectral density (up to a normalization constant), and proposes a random Fourier feature approximation based on MCMC sampling of the spectral density."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Soundness: the paper establishes a sound theoretical framework for un-normalized density estimation. \n- Novelty: (i) the authors propose a novel SDO kernel, which furthers our understanding on the relation between Sobolev spaces and the spectral densities of kernels; (ii) the argument how RSR differs from KDE is interesting and presents a good motivation to use this framework for density estimation. \n- Experiments: the paper mainly focuses on anomaly detection with overall good empirical results."
                },
                "weaknesses": {
                    "value": "- Motivation: I admit my lack of familiarity in un-normalized density estimation but I hope the authors can help clarify on a few issues: (i) I think the paper makes a sound argument about using the squared version of a linear combination of kernels by comparing it against KDE, but the decision to use a _single_ derivative order lacks motivation in my view. I pose this question in the next section, and I believe that a well-written section on this topic should be included in the paper. (ii) the paper mentions on the top section of p. 2 that regularizing over $ |f^*|_{L_2} $ is a desirable property: what does it mean?\n- Presentation: the paper is well-written overall, but the latter parts of the manuscript seems in a draft form: (i) the manuscript lacks a final summary section, and the Fisher divergence and hyperparameter tuning section seems better placed in an early section when the authors present the methodology; (ii) The paper mentions the consistency of the estimator in the main manuscript, but delays all discussions about consistency to the supplementary materials - I believe that it would be helpful to bring up the main theorems for consistency in the main text."
                },
                "questions": {
                    "value": "I have one main question about the objective of the SDO kernel: the paper proposes an RKHS norm that involves the L2 norm of the function, and also its derivatives _at_ a certain degree $m$, leading to a kernel with no analytical expression, but has a tractable spectral density (up to a normalization constant). The type of Sobolev norm $ \\sum_{|\\kappa|_1=m} ||D^{\\kappa} f||^2 $ seems quite unusual. \n\nWe know that the original Sobolev norm involves derivatives _up_ to degree $m$: $ \\sum_{|\\kappa|_1\\leq m} ||D^{\\kappa} f||^2 $, and the Sobolev spaces are norm-equivalent to the RKHS of a Mat\u00e9rn kernel (in closed form and well-studied). Could the authors explain why their choice of the Sobolev norm is useful, and what will the results be like if one uses the Mat\u00e9rn kernel in replacement of the SDO kernel? I think a good justification for this model choice is a useful addition to the paper, as the Mat\u00e9rn kernel is sufficiently close to a regularization on Sobolev norm, and the SDO kernel seems more difficult because of the reliance on using Fourier features to approximate its kernel values. \n\n- The paper's main method is marked as \"INER\" in Figures 1 and 2, but the text makes no mention of what it stands for: is this a mistake?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8079/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8079/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8079/Reviewer_o1rt"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8079/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698604775902,
            "cdate": 1698604775902,
            "tmdate": 1699637000620,
            "mdate": 1699637000620,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ryIrm0PHIY",
                "forum": "fapHf9fmqp",
                "replyto": "NsMdrPXevM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8079/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8079/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the feedback! \n\nWe agree with the point raised in this review that the specific choice of the SDO kernel requires more motivation. Please see a detailed discussion of our motivation, and of relations of SDO with other kernels, in the General Reply comment common to all reviews. In addition, in a few days we will include experiments that evaluate our algorithms with the Laplacian and Gaussian kernels, which correspond to a representative range of Sobolev related kernels and have analytic expressions. As we mention in the above discussion, all contributions of the paper besides the SDO kernel are relevant for these kernels too. \n\n\nWe comment on an additional point on equivalence of Sobolev spaces, which was mentioned in the review.  It is true for instance that the spaces corresponding to the Laplacian and SDO kernels are equivalent for all values of the Laplacian parameter $\\sigma$ and SDO parameter $a$ (see the discussion above). However, this only means that the spaces *contain* the same set of functions. This in turn implies that the norms are equivalent, but the equivalence constants diverge with the dimension. Since for regularization we are interested in an actual value of the norm, the resulting algorithms will be different between such spaces. Consider for instance that $\\|\\cdot \\|_1$ and $\\|\\cdot\\|_2$ norms on $R^d$ are equivalent norms, but with drastically different regularisation properties. Thus just norm equivalence is too coarse a notion to distinguish (or unify) between different regularisation algorithms.\n\n\nDo these comments address the issue with the SDO choice?\n\n\n> (ii) the paper mentions on the top section of p. 2 that regularizing over  $\\|f\\|_{L_2}^2$ is a desirable property: what does it mean?\n\nOur intention here was to say by using the Sobolev type kernel, we automatically deal with functions that satisfy $\\inf f^2(x)dx < \\infty$. Such functions are densities up to a constant. Non integrable functions on the other hand can not represent densities (for the purposes of sampling from them, for instance). This is not a trivial condition, as there is no reason why an \"arbitrary\" function should be integrable.\n\n \n\n>The paper's main method is marked as \"INER\" in Figures 1 and 2, but the text makes no mention of what it stands for: is this a mistake?\n\nYes, it should be RSR. Will be fixed. \n\n\n>The paper mentions the consistency of the estimator in the main manuscript, but delays all discussions about consistency to the supplementary materials - I believe that it would be helpful to bring up the main theorems for consistency in the main text.\n\nWe will definitely consider this. The challenge here is that there seems to be no other part of the paper which is non crucial enough to be moved to the supplementary instead."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8079/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700172865994,
                "cdate": 1700172865994,
                "tmdate": 1700172865994,
                "mdate": 1700172865994,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "BcEfnrulmB",
            "forum": "fapHf9fmqp",
            "replyto": "fapHf9fmqp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8079/Reviewer_oZU9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8079/Reviewer_oZU9"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new nonparametric density estimator based on regularized maximum likelihood estimation. The estimate is represented as the square of its square root, ensuring non-negativity at the cost of not having unit mass, which the paper argues is sufficient for applications, such as anomaly detection or sampling, that can utilize such an unnormalized density estimate."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The proposed method is fairly clearly well-explained. The empirical results on the anomaly detection benchmark are quite impressive and show the proposed method is useful in a number of real-world problems."
                },
                "weaknesses": {
                    "value": "**Major**\n\n1. Page 1, Paragraph 1:\n> *While there is recent work for low dimensional (one or two dimensional) data... there still are very few non-parametric methods applicable in higher dimensions.*\n\nI'm not convinced by this motivation, for a two reasons. First, many nonparametric methods have been described for data of arbitrary dimension (including in some of the cited papers). However, standard minimax lower bounds show that high-dimensional non-parametric density estimation is statistically intractable, in terms of many performance metrics. Thus, I'm not convinced that a new method will perform well in high dimensions unless it makes some more explicit assumptions on the density being estimated. Second, while recent neural network based density estimation methods are technically parametric, the complexity of these models is so large that they often behave more like non-parametric methods; i.e., they can fit quite complex densities. So it's not clear that a new nonparametric method should outperform these models in practical applications.\n\n2. Eq. (1): This objective is unbounded if the order of the derivative $D$ is $\\leq d/2$ (since the Sobolev $W^{k,2}(R^d)$ contains singularities, which can be centered on the samples.) The assumption $m > d/2$ in Theorem 2 addresses this, but, when first seeing Eq. (1), I was a bit confused by this. So perhaps it is worth mentioning this assumption earlier.\n\n3. Section 1, Last Paragraph, and Section 2, Last Paragraph: Both of these briefly mention consistency, but no details are provided here regarding the type of consistency (in $L_2$ in probability) or the assumptions made.\n\n4. Section 3.1, just after Eq. (5)\n> \"there seems to be no computationally affordable way to restrict the optimization to the positive cone $\\mathcal{C}$... We resolve this issue in two steps...\"\n\nAlthough it seems reasonable in practice, the solution proposed here is *ad hoc*, and it's not clear how this relates to the consistency guarantee (Theorem 11). Ultimately, in Appendix K, the paper assumes \"positivity on the data points $x_i$\", and it's not clear whether this condition is likely to hold as $n \\to \\infty$. On one hand, the estimate might approach the true (non-negative) density, but, on the other hand, the number of points $x_i$ is increasing. So, I think this is a big hole in the consistency guarantee. To fix this, the paper should analyze whether the \"positivity on the data points $x_i$\" condition holds as $n \\to \\infty$, and whether any additional conditions are necessary to ensure this (e.g., it might suffice if the true density is bounded away from $0$?).\n\n5. While the experiments demonstrate impressive performance on an anomaly detection benchmark, it's not clear from the paper where this advantage comes from or whether it is statistically significant or simply due to chance. One way to strengthen this would be an experiment on synthetic data where one can clearly (i.e., in an intuitive and unambiguous manner) see the advantage of RSR. The synthetic experiment in Section 5 almost does this, but it doesn't go as far as quantifying the advantage of RSR.\n\n6. I didn't really understand the purpose of Section 5. The two curves in Figure 2(b) seem almost identical, up to a constant scaling factor. Since the $y$-axis log likelihood, this just looks like INER=(KDE)$^2$, up to a constant factor. Related to this, I didn't understand why \"the gap between the values on the first and second cluster [being] larger for the RSR model\" explains why RSR is better for anomaly detection -- what matters for anomaly detection is probably the *ratio of the between-cluster variation to the within-cluster variation*, but the within-cluster variation is also much bigger for RSR (INER) than for KDE.\n\n**Minor**\n\n1. Usually $\\mathcal{H}^a$ denotes the Sobolev space $W^{a, 2}$ (i.e., the space of $L_2$ functions with $a^{th}$ derivatives in $L_2$). I suggest aligning the use of $\\mathcal{H}^a$ in this paper with this more standard notation. In particular, it is strange to me that the derivative order is not explicitly denoted in this paper's usage of $\\mathcal{H}^a$.\n\n2. Page 2, Paragraph 1, Last Sentence: Should \"|f^*|_{L_2}$\" be \"||f^*||_{L_2}$\"? I believe this is a proper norm...\n\n3. Section 4, \"Single Derivative Order kernel approximation\": Although I realize this is not the intent, the name \"Single Derivative Order kernel\" makes me think \"first\" derivative, rather than a single derivative of arbitrary order. I suggest a more explicit name like \"$m$-order Derivative kernel\".\n\n4. The paper ends a bit abruptly. I think it would help the reader to end with a summary of the paper's key contributions and perhaps a discussion of the limitations and weaknesses of the proposed method."
                },
                "questions": {
                    "value": "**Major**\n\n1. A central idea of this paper is to estimate a square root of the target density function, in order to ensure non-negativity of the estimated density (after squaring). This is closely related to the proposal of [MFBR20], who propose a general framework for estimating non-negative quantities built on this idea. In the particular case of nonparametric density estimation, I think their proposal is very similar to the present paper's (namely, maximum likelihood with Sobolev norm regularization). They also propose various mechanisms to enforce the constraint that the density estimate integrates to $1$. How exactly does the present paper's proposal differ from that of [MFBR20], and what, if any, are the advantages?\n\n2. Regarding the \"positivity on the data points $x_i$\" assumption in Appendix K: Does this assumption hold (for large $n$, with high probability) as $n \\to \\infty$?\n\n**Minor**\n\n1. Is there a motivation for explicitly enforcing the $L_2$ penalty $||f||_2$, as opposed to simply enforcing the Sobolev pseudo-norm penalty $||D f||_2$?\n\n2. Equation (9): I think there is an extra square in the definition of this inner product (i.e., $\\langle D^\\kappa f, D^\\kappa g \\rangle^2$ should be $\\langle D^\\kappa f, D^\\kappa g \\rangle$).\n\n3. Figure 1: What is \"INER\"? Should this be \"RSR\"?\n\n4. Figure 2: What do the error bars indicate? Quantiles?\n\n**References**\n\n[MFBR20] Marteau-Ferey, U., Bach, F., & Rudi, A. (2020). Non-parametric models for non-negative functions. Advances in neural information processing systems, 33, 12816-12826."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8079/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8079/Reviewer_oZU9",
                        "ICLR.cc/2024/Conference/Submission8079/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8079/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698937753449,
            "cdate": 1698937753449,
            "tmdate": 1700749671380,
            "mdate": 1700749671380,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "MNsXwgGHnD",
                "forum": "fapHf9fmqp",
                "replyto": "BcEfnrulmB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8079/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8079/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the detailed feedback. We were glad to learn that the empirical results were found to be quite impressive. \n\nIn the following we discuss all points raised in the review. \nIn particular, we discuss the technical issue of density positivity, the relation to the work [MFBR20] (thanks for bringing it to our attention!), and the purpose of Section 5.  Please let us know whether some issues remain. \n\nIn addition, we believe the reviewer may find the material in the global \"General Reply\" comment above to be of interest. There, we attempt to clarify the relation between various Sobolev space related kernels.\n\n\nFirst, we address the \"positivity on the data points\" question in the consistency result. This was raised in point 4 of the review and in the related Question 2. \n\n**positivity on the data points**:\n\nThe reason we only look for solutions in the cone (104) with strictly positive $f(x_i)>0$ is that the gradient of the loss is not defined otherwise (Eq. (6) or (106)).  Note that the functions in the cone satisfy \n$f(x_i)>0$ simply because we defined them to satisfy this, thus, there is no contradiction here.   \nIn addition, we do want to evaluate the gradient of the *true* density $v$ at the sample points. We believe this was the source of the issue in the review, when $n\\rightarrow \\infty$.  However, this also does not constitute a problem. \nIndeed, note that we do not require a lower bound on $f(x_i)$, we just need to know the gradient is well defined, i.e., $f(x_i)\\neq 0$.  To this end, \nfor any *continuous*  density $v^2$, we have the following: \n\nLet $\\{x_i\\}$ be an infinite sequence of iid samples from $v^2$. Then $v(x_i)>0$ for all $i$ with probability 1. (since choosing $x$ with $f(x) =0$ is a 0 probability event). \n\nAs an example, let $v^2$ be either a Gaussain, or a uniform density on a sphere. In both cases samples will have $v^2(x_i)>0$. The infimum of such values will  certainly be 0 for the Gaussian but there is no contradiction in that. Such points, approaching 0,  will not be optimal, and again, we only need the gradient to be well defined.   \n\nDoes this resolve the issue? We would be glad to discuss it further.\n\n\n\nNext we address the other major question, i.e., the relation to work [MFBR20]. \n\n**relation to [MFBR20]**:\n\n\nAt a very high level, the approach of [MFBR20] is indeed similar to ours, since they consider quadratic functions and regularisation with a version of an RKHS norm. \nThe differences are both in scope and in details of the construction that enable computation.  From a computational viewpoint, they optimise in the space on non-negative matrices. This makes the problem convex without any need for positivity, but it requires $N^2$ parameters to encode such a matrix (their Theorem 1), and $O(N^3)$ computational cost per step (their top of page 6). Further, the optimisation is a constrained one, which is also significantly more difficult. It is unlikely their methods can work on something like ADBench benchmark, and indeed, they only test it on a 1 dim Gaussian with N=50. \n\nSecond, while [MFBR20] is concerned with other applications as well, we \ninvestigate the density estimation problem much more closely. \nWe prove consistency, which is completely orthogonal to all their results, we prove the method is different from KDE, and we do provide evaluation on a state of the art benchmark of datasets. \n\n\nFinally, they do treat the normalisation (their Prop 4, and paragraph below it). \nHowever, this is simply by a straight forward opening of the brackets. \nThat is (in our case), if $f(x) = \\sum_{i} \\alpha_i k(x_i,x)$ then \n$\\int f^2 (x)dx = \\sum_{i,j} \\alpha_i \\alpha_j k(x_i,x)k(x_j,x)$. \n\nThus, if one knows $\\int k(x_i,x)k(x_j,x) dx$ for every $i,j$ one can compute  theoretically the normalisation constant. The issues with this are as follows: (a) This would involve summing $N^2$ floating point numbers, which would be inherently unstable for large N. (b) Computing \n$\\int k(x_i,x)k(x_j,x) dx$ analytically is easy *only* for the Gaussian kernel. Even for exponential (Laplacian) kernel in $d>1$, we do not see any way to evaluate this analytically. Of course, for the SDO kernel, this also does not seem to be computable. Due to this reason, we have not pursued normalisation. \n\nIt is worth noting however, that while the work of [MFBR20] advocates for using the computation above as a constraint, if one uses the Sobolev norms as we do, \nthe integral is guaranteed to be finite, and it is thus sufficient to normalise only once, when the optimisation is finished. \n\n\n\nContinued in the comment below."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8079/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700172460244,
                "cdate": 1700172460244,
                "tmdate": 1700172460244,
                "mdate": 1700172460244,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1IU7LZUDHW",
                "forum": "fapHf9fmqp",
                "replyto": "BcEfnrulmB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8079/Reviewer_oZU9"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8079/Reviewer_oZU9"
                ],
                "content": {
                    "title": {
                        "value": "\"positivity on the data points\" condition"
                    },
                    "comment": {
                        "value": "Thanks to the authors for their responses to my questions. The discussion of [MFBR20] was helpful to me; in particular, it seems that the optimization iterations of the proposed approach are $O(n^2)$, in contrast to the $O(n^3)$ iterations of [MFBR20], allowing the proposed method to scale to larger datasets.\n\nHowever, I didn't understand the authors' respose to my question about the \"positivity on the data points\" condition. Please let me know if I'm missing something here.\n\n> In addition, we do want to evaluate the gradient of the true density\n\nThe parameters of the *true density* are fixed; what does it mean to evaluate \"the gradient of the true density\"? Doesn't (natural or standard) gradient descent actually require taking the gradient of $\\alpha$ *at the current iterate*, not at the true value?\n\nTo be explicit, let's consider the initial value $\\alpha_0$. I believe computing the natural gradient (Eq. (6)) requires that the *estimated* density is positive on the data points, i.e., $f_0 := \\min_{i = 1,...,N} \\sum_{j = 1}^N \\alpha_{0,j} k_{x_j}(x_i) > 0$. I agree that this is satisfied if $f_0$ lies in the positive cone. However, the paper claims that enforcing this cone constraint is computationally intractable and therefore suggests an alternative approach in practice, namely initializing each $\\alpha_{0,j} \\geq 0$. As noted in the paper, for a potentially negative kernel, such as the SDO kernel, $\\sum_{i = 1}^N \\alpha_{0,i} k_{x_i}(x)$ may still take negative values at some $x$. So it seems necessary to me to justify that, for $\\alpha_{0,j} \\geq 0$, $\\min_{i = 1,...,N} \\sum_{j = 1}^N \\alpha_{0,j} k_{x_j}(x_i) > 0$.\n\nThis seems unlikely to be true in general. In fact, it's easy to construct a counterexample when $N = 2$ (let $\\alpha_{0,1} \\gg \\alpha_{0,2}$, and let $x_2$ be such that $k_{x_1}(x_2) < 0$). I think this creates a hole in the consistency argument. I was wondering, however, if there is a way to choose $\\alpha$ such that this is true with probability approaching 1 for large $N$? If so, then this might not be a problem in practice, and showing this would solidify the consistency argument.\n\nEDIT: Another possibility that just occured to me is that the consistency guarantee is intended to be for the \"ideal\" algorithm, which exactly enforces the cone constraint. If this is the case, then I guess the guarantee stands, but there is a bit of a gap between the theory and the practice. Is this the intent?"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8079/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700569151094,
                "cdate": 1700569151094,
                "tmdate": 1700652403655,
                "mdate": 1700652403655,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jbsfDiylLE",
                "forum": "fapHf9fmqp",
                "replyto": "BcEfnrulmB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8079/Reviewer_oZU9"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8079/Reviewer_oZU9"
                ],
                "content": {
                    "title": {
                        "value": "Clarifying my comment about the paper's motivation"
                    },
                    "comment": {
                        "value": "Regarding my point about the paper's motivation\n> I'm not convinced by this motivation, for a two reasons...\n\nI fully agree with the authors' claim that\n> all methods have a certain bias, and there [is] no \"best\" method.\n\nIn fact, I did not mean to question the motivation underlying the search for a new method (sorry it came across in this way!). Rather, I am commenting on the specific sentences\n> While there is recent work for low dimensional (one or two dimensional) data... there still are very few non-parametric methods applicable in higher dimensions.\n\nThese sentences don't seem accurate or convincing to me, and I am suggesting that the authors re-write this paragraph to identify more specific strengths or motivations of their proposed method."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8079/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700569946036,
                "cdate": 1700569946036,
                "tmdate": 1700570039355,
                "mdate": 1700570039355,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PYM9zyXvxk",
                "forum": "fapHf9fmqp",
                "replyto": "0r5GGVgB2q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8079/Reviewer_oZU9"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8079/Reviewer_oZU9"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the responses.\n\nI understand now that the consistency result assumes that the optimization is performed exactly, rather than using the heuristic initialization $\\alpha \\geq 0$. While this creates a bit of a gap between theory and practice, I am ok with this simplification, as long this is clarified in the paper. I think it would help significantly to put a formal statement of the consistency guarantee in the main paper rather than just the brief high-level description in Section 2.\n\nRegarding the motivations paragraph, I also agree, I think this is a more convincing motivation.\n\nRegarding the purpose of Section 5 (my original point 6.): I guess understand the intent of this section, but, the results here don't seem illuminating enough to devote 1.5 pages in the main paper. Although it shows that there is a difference between RSR and KDE in a particular example, it doesn't give a very clear or general idea of what this difference is. So, I think most of this could go in an appendix, with a few sentences in the main paper refering to this. This would create more space to describe the consistency result in detail.\n\nThese responses are enough for me to raise my score, but I have to read through the other reviews and responses more carefully before deciding how much."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8079/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700654230451,
                "cdate": 1700654230451,
                "tmdate": 1700654230451,
                "mdate": 1700654230451,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]