[
    {
        "title": "Balancing Information Preservation and Computational Efficiency: L2 Normalization and Geodesic Distance in Manifold Learning"
    },
    {
        "review": {
            "id": "2CoG32iH6f",
            "forum": "2Kf1AIdeyt",
            "replyto": "2Kf1AIdeyt",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7472/Reviewer_mtLk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7472/Reviewer_mtLk"
            ],
            "content": {
                "summary": {
                    "value": "In their paper \"Balancing information presevration and computational efficiency: L2 normalization and geodesic distance in manifold learning\", the authors suggest a t-SNE modification called HS-SNE that does two things: (1) uses L2 normalization of the input data to project it to the hypersphere; (2) replaces Gaussian distribution with von Mises distribution to compute high-dimensional pairwise affinities between points. Using simulated as well as real biological data, the authors argue that their approach outperforms L1 normalization and standard t-SNE commonly used to analyze single-cell RNA-seq data."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "I am interested in manifold learning and scRNA-seq data, and think that exploring the effect of data normalization is an interesting and under-studied topic. I think the paper is on-topic, suggests a somewhat novel approach (HS-SNE), and uses quantitative evaluation."
                },
                "weaknesses": {
                    "value": "That said, I also think that the paper does not convincingly show benefits of HS-SNE, does not present the necessary ablation experiments, and does not show all the relevant controls. In fact, the modification (2) does not do anything at all, because the way authors defined it, von Mises distribution is equivalent to Gaussian. I am afraid the paper does not rise to the ICLR level of general interest."
                },
                "questions": {
                    "value": "MAJOR COMMENTS\n\n* Once the points are projected onto the hypersphere and have length 1, cosine of the geodesic distance is simply cosine similarity, which is 1 minus one half of the squared Euclidean distance. So the expression exp(kappa * cos(geo_dist) - kappa) used in section 2 to define p_{ij} values in t-SNE is exactly equivalent to exp(-eucl_dist^2 / (2/kappa)), which is *exactly* the expression used by standard t-SNE. If kappa is tuned like sigma is tuned in t-SNE (to achieve a given perplexity), this is an absolutely identical procedure and will make no difference.\n\n* In the experiment in Section 3.1, HS-SNE is compared to t-SNE after L1 normalization. What is crucially missing, is standard t-SNE after L2 normalization. As explained above, it must be equivalent to HS-SNE. \n\n* As a sanity check, I would also like to see t-SNE of the raw data (without any normalization) in Figure 4. On the other hand, t-SNE with Manhattan and with Chebyshev distances are not really needed there, in my opinion.\n\n* In Figure 4, I don't see any difference between t-SNE(L1) and HS-SNE: blue vs red. Why do the authors conclude that HS-SNE \"surpasses\" t-SNE?? I am really confused by that intepretation.\n\n* Figure 5 and Figure 6c are more convincing, and do show a noticeable difference between t-SNE(L1) and HS-SNE. Again, what is missing here is a comparison to t-SNE(L2), which should be equivalent to HS-SNE. Then the message of the paper would reduce to \"For scRNA-seq data, L2 normalization is better than L1 normalization\", which may be interesting in itself, but probably better suited for a bioinformatics journal. \n\n* kNN accuracy used in Figure 6 is a good metric, but why use kNN accuracy in 2D after t-SNE, instead of using kNN accuracy directly in the high-dimensional space? That would be a more direct measure of the normalization quality. \n\n\nMINOR COMMENTS\n\n* IDD formula on page 3 -- why does the sum go until 0.995?\n\n* Section 3.2: standard practice in scRNA-seq literature is to apply PCA first, keeping 50-100 dimensions, before doing t-SNE. It would be interesting to know if this alleviates the problem of L1 normalization or not. I.e. it would be interesting to add t-SNE(PCA(L1)) pipeline to the comparison."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7472/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698317865033,
            "cdate": 1698317865033,
            "tmdate": 1699636901304,
            "mdate": 1699636901304,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "ca8cplKKRx",
            "forum": "2Kf1AIdeyt",
            "replyto": "2Kf1AIdeyt",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7472/Reviewer_FzWv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7472/Reviewer_FzWv"
            ],
            "content": {
                "summary": {
                    "value": "The primary objective of this paper is to refine the normalization process employed in existing unsupervised learning algorithms. In a more detailed sense, the paper presents the novel idea of utilizing distance distribution and its related information to measure information loss, thereby indicating that earlier methods might lead to unidentifiable distances. To mitigate this, the paper recommends replacing the Gaussian distribution kernel with a Von Mises distribution kernel when implementing the t-SNE method."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.As the dimensionality increases, the tendency of previous methods to result in indistinguishable distance distributions becomes more pronounced.\n\n2.Empirical experiments demonstrate the effectiveness of the proposed method in comparison to the original t-SNE."
                },
                "weaknesses": {
                    "value": "1.I question whether the information from distance distribution could serve as an effective measure of information loss, given the problem background.\n\n2.The dataset used in section 3.2 needs a more detailed introduction. The phrase \"some biological meaningful\" could potentially lead to ambiguity."
                },
                "questions": {
                    "value": "Why is the information from distance distribution effective in representing the information? Wouldn't metrics considering inter or intra-class distances, which account for the \"correct distance for clustering,\" be a more appropriate choice for representing distance information? Is it possible to have a highly distinguishable distance distribution but still end up with significantly incorrect clustering results?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7472/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698656979589,
            "cdate": 1698656979589,
            "tmdate": 1699636901133,
            "mdate": 1699636901133,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "vgIz1JPP3e",
            "forum": "2Kf1AIdeyt",
            "replyto": "2Kf1AIdeyt",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7472/Reviewer_2wH8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7472/Reviewer_2wH8"
            ],
            "content": {
                "summary": {
                    "value": "1.\tThe paper establishes a robust mathematical foundation, highlighting the limitations of using Euclidean distance after the L1-norm for distinguishing data points with varying degrees of affinity.\n2.\tWhile not the most information-captive method, the paper demonstrates the effectiveness of L2-normalization and geodesic distance in preserving inter-observation differences, especially with lower p values in p-norm.\n3.\tThe introduction of HS-SNE (HyperSphere-SNE) presents an innovative approach to data visualization. By projecting observations onto a hypersphere and employing geodesic distances, it significantly improves cluster boundaries in visualizations, all while reducing computational costs in single-cell RNA-seq datasets."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.\tThe paper establishes a robust mathematical foundation, highlighting the limitations of using Euclidean distance after the L1-norm for distinguishing data points with varying degrees of affinity.\n2.\tWhile not the most information-captive method, the paper demonstrates the effectiveness of L2-normalization and geodesic distance in preserving inter-observation differences, especially with lower p values in p-norm.\n3.\tThe introduction of HS-SNE (HyperSphere-SNE) presents an innovative approach to data visualization. By projecting observations onto a hypersphere and employing geodesic distances, it significantly improves cluster boundaries in visualizations, all while reducing computational costs in single-cell RNA-seq datasets."
                },
                "weaknesses": {
                    "value": "1.\tThe contribution of the paper is limited, considering that the authors simply replace the original L1-normalization term with an L2-normalization term to solve the dimensional catastrophe problem, with no innovative paradigm contribution presented. Moreover, this approach can only alleviate the problem to a certain extent without a milestone breakthrough.\n2.\tAs claimed in the article INTRODUCTION, the advantage of L2-normalization is maintaining sufficient information in high-dimensional spaces, which lacks subsequent real datasets\u2019 experimental validation. In addition, the dataset used lacked description in the paper, and I had difficulty confirming that the authors used a high-dimensional real dataset.\n3.\tThe experimental part is not sufficient to verify the validity of the method. As far as the experiment in Fig. 5 is concerned, there are too few comparison methods and only one classical t-SNE algorithm, which is not convincing. Moreover, more models are expected for the experiment in Fig. 7 to demonstrate the effectiveness of the L2 -normalization term."
                },
                "questions": {
                    "value": "1.\tThe contribution of the paper is limited, considering that the authors simply replace the original L1-normalization term with an L2-normalization term to solve the dimensional catastrophe problem, with no innovative paradigm contribution presented. Moreover, this approach can only alleviate the problem to a certain extent without a milestone breakthrough.\n2.\tAs claimed in the article INTRODUCTION, the advantage of L2-normalization is maintaining sufficient information in high-dimensional spaces, which lacks subsequent real datasets\u2019 experimental validation. In addition, the dataset used lacked description in the paper, and I had difficulty confirming that the authors used a high-dimensional real dataset.\n3.\tThe experimental part is not sufficient to verify the validity of the method. As far as the experiment in Fig. 5 is concerned, there are too few comparison methods and only one classical t-SNE algorithm, which is not convincing. Moreover, more models are expected for the experiment in Fig. 7 to demonstrate the effectiveness of the L2 -normalization term."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7472/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699315651909,
            "cdate": 1699315651909,
            "tmdate": 1699636901019,
            "mdate": 1699636901019,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]