[
    {
        "title": "On the generalization capacity of neural networks during generic multimodal reasoning"
    },
    {
        "review": {
            "id": "c4bD4kpXHW",
            "forum": "zyBJodMrn5",
            "replyto": "zyBJodMrn5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1924/Reviewer_skmj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1924/Reviewer_skmj"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies multi-modal generalization in neural networks such as transformer-based models and recurrent networks. To do so, the authors propose Genertic COG, a modular benchmark with multi-modal splits to test for 3 types of generalization: 1) distractor (generalization to different noise distribution), 2) systemic compositional (generalization to new permutation of task structures) and 3) productive compositional (generalization to tasks of greater complexity) generalization. Experiments conducted by the authors showed that while cross-attention based transformers (e.g. CrossAttn and Perceiver) outperform other models and perform well on distractor and systemic compositional generalization, they fail at productive generalization when the depth of the task tree goes to out-of-distribution (>3). Representational analysis is done to show that cross-attention based transformers (e.g. CrossAttn and Perceiver) superior performance on distractor generalization might be due to their ability to better retain task-relevant (e.g. stimulus and response) information at the penultimate layer."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+The paper studies a timely and critical question about the generalization capability of multimodal transformer-based models\n\n+The proposed benchmark dataset uncovers a limitation of current multimodal transformer-based models: productive generalization which can facilitate the development of more generalizable transformers/LLMs. \n\n+The paper is generally well-written and easy to follow"
                },
                "weaknesses": {
                    "value": "-While the paper\u2019s studies show that certain designs (e.g. cross-attention) seem to confer multi-modal generalization, there are still some key questions that can be more thoroughly studied to uncover the reasons why this is the case.\n\n-Similarly, important discussions such as why the (cross-attention) transformers might fail at productive generalization is lacking."
                },
                "questions": {
                    "value": "What is the key architectural difference between dual stream transformer and transformers with cross attn that can explain their generalization performance? Is it only the lack of a cross attention between the different modalities?\n\nPossible typo:\n\u201cFinally, we included a Perceiver-like model (Jaegle et al., 2021), an architecture designed to generically process multimodal inputs (Fig. 2f).\u201d:  (Fig. 2f) > (Fig. 2e)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1924/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698632081062,
            "cdate": 1698632081062,
            "tmdate": 1699636123237,
            "mdate": 1699636123237,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "T8BAbxS3ru",
                "forum": "zyBJodMrn5",
                "replyto": "c4bD4kpXHW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1924/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1924/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer skmj (1/N)"
                    },
                    "comment": {
                        "value": "We thank the Reviewer for providing feedback on our manuscript, and the overall positive assessment. Below we address the weaknesses and specific questions the Reviewer raised. \n\n **Weaknesses**\n\n*While the paper\u2019s studies show that certain designs (e.g. cross-attention) seem to confer multi-modal generalization, there are still some key questions that can be more thoroughly studied to uncover the reasons why this is the case.* \n\nIn response to the Reviewer\u2019s concerns (and the related comment by Reviewer a4Su), we have now performed additional experiments that focus on how model scale and complexity can influence multimodal generalization. While the original manuscript was focused on understanding how a class of base neural architectures would fare on multimodal generalization, we agree that it is important to understand how choice of hyperparameters, such as number of attention heads and layer depth, can influence generalization. \n\nAs such, we have performed a systematic experiment on a standard encoder-only transformer (with the same architecture as BERT; Devlin et al., 2019). We manipulated the number of layers (1, 2, 3, 4 layers), and number of attention heads (1, 4, 8 heads) and assessed the corresponding generalization performance across all splits. (Note that 4 transformer encoder layers and 8 attention heads match the BERT-small architecture.) Indeed, we found that increasing encoder depth significantly improved distractor and systematic generalization. Increasing attention heads also improved distractor and systematic generalization, though to a lesser extent. Nevertheless, neither of these modifications influenced productive generalization. Our conclusion from this is that some tasks (e.g., systematic generalization) require more abstractions; a single layer of attention that handles multimodal inputs is insufficient. Cross-attention mechanisms offer a more targeted and efficient solution (I.e., fewer number of parameters and faster to train) that explicitly integrate visual stimuli (keys and values) with the instructions (queries). However, simply adding encoder/attention layers and parameters can suffice for some types of generalization. \n\nWe have now included discussion of some of these experiments and insights into the manuscript, with a few key revisions copied below. The new results are depicted in Figure 8 (Appendix). Below is the caption for Figure 8; please see the updated PDF submission for the actual figure: \u201cFigure 8: Evaluating generalization splits on BERT-like single-stream transformer models with varying layers and attention heads. We manipulate a generic encoder-only transformer based on the BERT architecture, evaluating the influence of the number of encoder layers (1, 2, 3, and 4 layers), and the number of attention heads per encoder layer (1, 4, and 8 heads). Overall, increasing layers improves generalization across distractor and systematic generalization, but not productive generalization. Increasing attention heads also marginally improves distractor and systematic generalization, but to a lesser extent than adding depth. A) Evaluation on distractor generalization across all model parameters. B) The effect of adding additional encoder layers on distractor generalization performance (averaged across all attention head configurations). C) The effect of adding attention heads on distractor generalization performance (averaged across all layer depth configurations. D-F) Evaluation on systematicity for depth 1 tasks (identical to generalization split in Fig. 4a). G-I) Evaluation on systematicity for depth 3 tasks (identical to generalization split in Fig. 4d). J-L) Evaluation on productivity split (identical to generalization split in Fig. 5a).\u201d \n\nUpdates to Contributions Section (1.2): \u201c2. A comprehensive evaluation of commonly-used base neural models (RNNs, GRUs, Transformers, Perceivers) on distractor, systematic, and productive generalization splits. We find that for distractor and systematic generalization, including a cross-attention mechanism across input modalities is important. However, all models fail on the productivity split. In addition, we include experiments demonstrating the impact of transformer depth and attention heads on all generalization splits in an encoder-only Transformer model.\""
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1924/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700536612979,
                "cdate": 1700536612979,
                "tmdate": 1700536612979,
                "mdate": 1700536612979,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dKxA6Y2VhI",
                "forum": "zyBJodMrn5",
                "replyto": "c4bD4kpXHW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1924/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1924/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer skmj (N/N)"
                    },
                    "comment": {
                        "value": "*Similarly, important discussions such as why the (cross-attention) transformers might fail at productive generalization is lacking.* \n\nThis is a challenging question to tackle. Our ongoing hypothesis is that productive generalization is a fundamentally distinct type of generalization relative to systematic compositional generalization. We have now included a brief discussion in the Results section of Productive Compositional Generalization (Section 3.3) that addresses these challenges: \u201cOne possible explanation for the disparity between systematic and productive generalization in neural models is that systematicity requires the ability to exchange semantics (or tokens) from a known syntactic structure (e.g., a tree of certain depth). In contrast, productive generalization requires generalizing to an entirely new syntactic structure (e.g., a task tree of different size or depth). This requires understanding the syntax -- how to piece together syntactic structures on-the-fly -- requiring another level of abstraction. To our knowledge, there is nothing in the current set of mechanisms in the Transformer that would enable this. Thus, productive compositional generalization remains a difficult capability for purely neural models to achieve.\u201d \n\n**Questions** \n\n*What is the key architectural difference between dual stream transformer and transformers with cross attn that can explain their generalization performance? Is it only the lack of a cross attention between the different modalities?* \n\nThe short answer is yes. When comparing the Dual Stream Transformer with the models with cross attention, indeed, the only distinction is the lack of an attention mechanism to explicitly integrate outputs from the two input streams.  \n\nThe longer answer, which became clear after performing the new experiments (that scaled up to the BERT-small architecture), is that at minimum, a second attention layer is required to systematically abstract token information from each of the inputs. While applying a large self-attention matrix twice to simultaneously presented visual and language instructions is computationally inefficient (our cross-attention models show that it is unnecessary), it provides the necessary base structure to allow for good systematic generalization.  \n\n*Possible typo: \u201cFinally, we included a Perceiver-like model (Jaegle et al., 2021), an architecture designed to generically process multimodal inputs (Fig. 2f).\u201d: (Fig. 2f) > (Fig. 2e).*\n\nWe thank the Reviewer for spotting this error. The manuscript has now been updated."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1924/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700536720484,
                "cdate": 1700536720484,
                "tmdate": 1700536732592,
                "mdate": 1700536732592,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "WaCqOkvd4I",
            "forum": "zyBJodMrn5",
            "replyto": "zyBJodMrn5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1924/Reviewer_a4Su"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1924/Reviewer_a4Su"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new benchmark for assessing various forms of generalization in a multimodal setting named gCOG. The dataset includes several different splits intended to measure different aspects of generalization. The paper also compares several different model architectures on the dataset."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* The paper introduces a new dataset, gCOG. While the dataset is conceptually similar to those from prior work, such as gSCAN, it supports different types of contexts and instruction types, including more compositional instructions. I'm aware of some prior work (e.g. [1], [2]) that studied compositional generalization in natural language tasks and found that gains on one synthetic task did not always transfer to other tasks, so increasing the diversity of such benchmarks for assessing compositional generalization and related challenges in the multimodal setting could be a potentially valuable contribution.\n\n[1] https://arxiv.org/abs/2007.08970\n[2] https://aclanthology.org/2021.acl-long.75/"
                },
                "weaknesses": {
                    "value": "* I'm concerned about the strength of the baselines used in the paper (see my related questions below). While the primary contribution of the paper is the dataset, it is also important to establish strong baselines for this new dataset and to ensure that the conclusions from the empirical results are valid. The appendix states that only a *single Transformer layer* with a *single attention head* was used. This is almost certainly not an optimal depth and number of attention heads. Relatedly, it looks like some models are potentially underfit, according to the figures. With >5M training examples and a relatively simple input space, I would have expected a reasonably sized Transformer model to achieve low training loss and reasonable IID generalization. If these models could have been applied to similar tasks such as gSCAN (even using symbolic tokens to represent the scene context), where they could be compared with comparable baselines from prior work, this would have helped establish that these are indeed reasonably strong baselines that have been well tuned.\n* The qualitative difference between gCOG and datasets from prior work such as gSCAN was not very clearly described. For example, one of the key claims seemed to be gCOG \"employs generic feature sets that are not tied to any specific modality\". However, it seems like it is a useful property for a multimodal dataset to have a clear relation to real-world multimodal tasks. Indeed, the authors provide interpretations of their tasks in the form of natural language instructions and visual scenes (e.g. in Figure 1), and these are very useful for understanding the task. Representing this dataset using familiar modalities (e.g. vision, natural language) could enable future work to study different research questions, e.g. the impact of pre-training. The ability to alternatively represent the task input as a sequence of tokens is also reasonable for studying certain research questions, but this also seems possible for datasets from prior work. For example, I understand that gSCAN includes both symbolic descriptions as well as visual renderings. Anyways, I think clarifying the motivation for this dataset (e.g. increasing diversity of available benchmarks, focusing on different generalization challenges, etc.) separately from how inputs are represented for the experiments in this paper (e.g. token sequence vs. images and natural language) would be useful.\n* Some of the main empirical conclusions (e.g. that generalization to greater \"depth\" is challenging for models such as Transformers) are generally known from prior work.\n\nnits:\n* Introduction paragraph 1 - \"on a carefully controlled generic multimodal reasoning tasks\" -> \"on carefully...\" or \"...task\"\n* Appendix A.2.1 - Maybe reference Tables 8 and 9 where you discuss different positional embeddings.\n* Consider discussing [3] in related work. [3] demonstrated the importance of cross-modal attention for gSCAN, and similarly studied the relative difficulty of various aspects of generalization, including distractors.\n\n[3] https://aclanthology.org/2021.emnlp-main.166/"
                },
                "questions": {
                    "value": "* Why not try more layers and attention heads, e.g. following a standard hyperparameter setting for model size such as those of BERT-Base? Or even BERT-Small?\n* In Figure 2 (F) why does the single-stream Transformer have almost double the parameters of the double stream Transformer? For the other Transformers, do the encoder blocks used for the task vector and stimulus vector share parameters? \n* What optimizer and hyperparameters (e.g. learning rate) were used for training? How were these chosen? I didn't see these details in Appendix A.2. \n* Position embeddings - Since you are representing 10x10 grids as 1D sequences, 1D relative positions may not capture this structure well. On the other hand, absolute position embeddings seem potentially problematic in the case of the SSTrfmr model, since they will not be consistently assigned to the same grid position if the text sequence is first and has varying length. Mitigating this may be important to provide for a fairer comparison with the SSTrfmr model.\n* To what do you attribute the periodic loss spikes during training that are shown in Figure 4 (E)?\n* I found the usage of \"cross-attention\" a bit confusing. For example, the single stream Transformer features cross-modal attention as an implicit consequence of self-attention over the concatenated sequence. I thought this would commonly be referred to as an instance of \"cross-attention\" between modalities. \n* Does the dataset also contain visual renderings and natural language instructions to enable future work to study these tasks using familiar modalities?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1924/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699400405601,
            "cdate": 1699400405601,
            "tmdate": 1699636123172,
            "mdate": 1699636123172,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7lMFAUHysn",
                "forum": "zyBJodMrn5",
                "replyto": "WaCqOkvd4I",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1924/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1924/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer a4Su (1/N)"
                    },
                    "comment": {
                        "value": "We thank the Reviewer for their thorough and thoughtful feedback. Below, we have worked to address some of the weaknesses the Reviewer raised, particularly the strength of the baselines. We have included new experiments to directly address these concerns, taking into consideration this Reviewer\u2019s suggestion. Below, we also address all questions directly.\n\nHowever, before addressing the Reviewer\u2019s specific questions, we first wanted to address the two weaknesses the Reviewer raised. The first weakness was the lack of evaluation of deeper networks with more attention heads. We have now directly addressed this by performing additional experiments on a range of transformer layer depths (1, 2, 3, 4 layers) and attention heads (1, 4, and 8 heads), up to the size of BERT-small (4 layers, 8 heads; 12 model experiments in total). We have now included figures that detail the performance across this parameter sweep, with the primary conclusion that adding layers indeed aids with distractor and systematic generalization, but not productive generalization. We provide additional details below, and have incorporated the results for these new experiments in Figure 8 (in the Appendix), and have referenced these results in the manuscript's main text. In addition, we wanted to address another comment the Reviewer raised: \u201c*I would have expected a reasonably sized Transformer model to achieve low training loss and reasonable IID generalization.*\u201d  \n\nWe thank the Reviewer for their suggestion on expanding our evaluations. The Reviewer was correct in their intuition \u2013 deeper models help standard single stream transformers achieve improved IID generalization across all generalization splits (Fig. 8). We have now included the following text to the main manuscript (Results section 3.2) to reference these results: \"(Note, however, that increasing depth (encoder layers) to Transformers improves IID generalization on these splits; Fig. 8).\"\n\nThe second weakness was the lack of clear distinction between our presented task, gCOG, and prior tasks such as the gSCAN task. We thank the Reviewer for this comment, and have now emphasized the primary distinctions between the two tasks. In brief, the two tasks require different neural network architectures. In terms of transformers, gSCAN is a generation task (requiring a decoder architecture) and gCOG is a classification task, which requires only an encoder Transformer. While models evaluated on gSCAN can incorporate encoder models to the architecture, as illustrated in one of the articles the Reviewer cites (Qiu et al., 2021), generating navigation instructions autoregressively adds complexity to studying compositional productivity, since autoregressive models are susceptible to exposure bias (Wang & Sennrich, 2020). Additionally, gCOG includes a distractor generalization split; our dataloaders are organized such that distractor generalization splits can interact with either systematic and/or productivity splits. We have now emphasized these differences in the manuscript, and have included citations to the studies mentioned by the Reviewer in the Related work sections. \n\nRelated work revisions: \n\n\u201cOur approach to constructing arbitrarily complex compositions of simple tasks is similar to (Ruis et al., 2020). However, it differs in three key ways. First, we focus on question-answer tasks (which require encoder-only architectures), rather than sequence-to-sequence learning tasks (which require decoder architectures, e.g., Qiu et al., 2021). Sequence decoding tasks introduce the added complexity of requiring autoregressive responses, which are susceptible to fundamental statistical challenges, such as exposure bias (Wang & Senn, 2021). Second, gCOG includes a distractor generalization split, in addition to systematic and productive compositional generalization splits. Finally, we methodically characterize different forms of generalization using simpler underlying abstractions (i.e., without the explicit use of image pixels).\u201d \n\nWe thank the reviewer for bringing to our attention Furer et al. (2021), and Shaw et al, (2021). References to these two studies are now included in the preceding paragraph of the above quoted passage."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1924/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700535318890,
                "cdate": 1700535318890,
                "tmdate": 1700582050202,
                "mdate": 1700582050202,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bgPc95Mj4f",
                "forum": "zyBJodMrn5",
                "replyto": "zpmebzXNAb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1924/Reviewer_a4Su"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1924/Reviewer_a4Su"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you to the authors for including the new experiments on Transformers with >1 layer and >1 attention head, up to approximately \"BERT small\" sizes. And also thank you for clarifying the details of the available input formats released with the dataset. I think these changes have the potential to significantly improve the contribution of this work.\n\nHowever, I am a bit unsure about whether to increase my score. I still have issues with the current submission. I have no doubt that this paper could be revised in such a way that I would vote for acceptance, and that the dataset could be a nice contribution. But I think this requires significant enough changes to the experiments and paper that it would be more realistic to revise and resubmit this work to a future conference. Therefore, I will leave my score the same and let the AC take this into consideration.\n\nMy main concern continues to be about the empirical experiments. I think the main results should be based on Transformers of a reasonable size and capacity, e.g. at least the \"BERT small\" size. The single layer Transformer results risk being misleading, since the expressiveness of such a small Transformer model is severely limited and may not correlate with the results of most Transformers used in practice. \n\nI also have some more minor concerns about the technical details: \n\n> Given that attention is an n^2 calculation on all pairs of tokens, this reason alone nearly doubles the number of parameters in the SSTfmr relative to the DSTfmr. \n\nI understand why this increases the amount of *computation*, but I don't understand why this increases the number of *parameters*, as attention parameters are shared along the sequence dimension.\n\nI also recommend better understanding the loss spikes observed during training, and tuning of the learning rate if models continue to underfit the training data."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1924/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700583784680,
                "cdate": 1700583784680,
                "tmdate": 1700583784680,
                "mdate": 1700583784680,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "umBGrmnYm6",
            "forum": "zyBJodMrn5",
            "replyto": "zyBJodMrn5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1924/Reviewer_DJb6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1924/Reviewer_DJb6"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a new multimodal question answering benchmark for out-of-distribution generalization, specifically covering task compositionality, robustness to distractors and combinatorial generalization. It uses this benchmark to evaluate various models and analyze their performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- **Topic**: The paper studies an important topic which in my opinion is underexplored in current deep learning research. Especially given the tendency these days to scale training up to vast amounts of data, I believe it is particularly important to design carefully controlled benchmarks that can: evaluate the model\u2019s performance from a critical and cautious standpoint, point to their fundamental limitations (e.g. systematic generalization), and support further research about ways to overcome these.  \n- **Evaluation**: The paper offers both extensive extrinsic evaluation, with performance comparison of various models on the different generalization skills, as well as intrinsic analysis of their internal representations\u2019 degree of alignment to the stimuli.\n- **Clarity**: The writing quality is good and the paper is clear and easy to follow. The paper is well-organized, claims and findings are clearly stated, and useful figures and diagrams are provided.\n- **Related Works**: It does a good job in providing the relevant context, motivation and related works. \n- **Contribution**: The empirical findings of the paper on the benefits and limitations of different inductive biases such as recurrent and attention-based are important and may be of broad interest to the community."
                },
                "weaknesses": {
                    "value": "- **Pre-trained models** The paper focuses on models trained from scratch rather than pre-trained. This could be a strength and a weakness. On the one hand, it allows for isolating the contribution of the architectural choices from other factors of optimization, and training data. On the other hand, it has been observed that by training models at large enough scales enables the emergence of generalization capabilities, which we don\u2019t see in smaller scales. I think it will be critical to also analyze the performance of pretrained models on the benchmark, in order to strengthen the paper.\n- **Visual Simplicity**: The visual side of the benchmark is quite rudimentary, featuring colorful letters. Extending it to a larger range of visual tokens/objects, that could have more than one property (color), and a broader set of elements and variations (than 26 letters), could be a straightforward extension that could help make it a bit more challenging visually."
                },
                "questions": {
                    "value": "- **COG task**: It will be useful to discuss the COG task (rather than just mentioning it) before describing the new gCOG one, so that it will be clearer to the reader what are new contributions of the new benchmark compared to COG and the degree of their importance. In the overview diagram I would also recommend showing a sample also from COG to make the differences clearer. \n- **Grid size / generalization**: It could be interesting to vary the size of the grid in training/evaluation and study its impact on model\u2019s performance. \n- **Terminology**: I recommend changing the phrase \u201cDistractor generalization\u201d to one that better conveys it\u2019s about changing the answer distribution. Maybe e.g. answer distribution shift. I also recommend changing the name \u201cSystematic compositional generalization\u201d to \u201ccombinatorial generalization\u201d, to emphasize that the main point is the generalization to permutation, and also to better contrast it with the following \u201cProductive generalization\u201d (which could also be systematic).\n- **Figures**: Would be good to increase the size of the plots in Figure 3b. It will also be good the increase the distance and visual separation between the sub-figures in each figure throughout the paper. \n- In the introduction: \u201cmultimodal question-answer\u201d -> \u201canswering\u201d.\n- \u201cThis design allowed us\u201d -> \u201cThis design allow us\u201d."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1924/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699470958350,
            "cdate": 1699470958350,
            "tmdate": 1699636122858,
            "mdate": 1699636122858,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6yiMbjGKRA",
                "forum": "zyBJodMrn5",
                "replyto": "umBGrmnYm6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1924/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1924/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Djb6 (1/N)"
                    },
                    "comment": {
                        "value": "We thank Reviewer Djb6 for their thoughtful review, and their positive assessment of our manuscript. Below we address the weaknesses the Reviewer raised, and directly respond to questions. \n\nWeakness 1: Lack of evaluation using pre-trained models. We agree with the Reviewer that there is utility in assessing how a pretrained model performs on a new task to the literature. However, when trying to address this question regarding our specific task, we realized that most models would not be able to perform this task out-of-the-box without any fine-tuning, since tokens in the current task set up have no intrinsic meaning. Since we could not directly test mainstream pretrained transformer models (without devising a specific way of first aligning domains through finetuning), we addressed a related question raised by the Reviewer: How would mainstream architectures (such as BERT-like models; Devlin et al., 2019) fare on our benchmark? Moreover, much of the motivation for this manuscript was to focus on questions such as: What architectural components are essential for distractor, systematic, and productive generalization? To this end, we performed additional experiments to evaluate how standard architectures \u2013 such as BERT-like architectures -- generalize appropriately. These experiments focused on investigating how increasing encoder-layer depth, and increasing attention heads in each encoder layer, influenced generalization performance. These results have now been included as Figure 8 in the Appendix. Nevertheless, we agree that evaluating the out-of-the-box performance of pretrained models on a pixel and natural language variant of this task is important for future work to explore (once a sensible fine-tuning procedure to align the two is agreed upon). \n\nWeakness 2: Visual Simplicity. We agree with this sentiment; many of the visual stimuli are indeed rudimentary. However, note that in this task, we will deploy a config file that allows one to extend the number and choice of symbols that can be included. Moreover, the mapping from categorical tokens to image pixels implies that any symbol with a UTF-8 encoding can be easily incorporated (e.g., letters, numbers, symbols). (In principle emojis can also be included, but note that the mapping emojis to a specific color attribute is not straightforward.) Release of the dataset will include the standard splits that we have included with this manuscript, in addition to datasets that map categorical tokens to image pixels and natural language. We will also provide the raw dataloaders that can be customized as desired. We have clarified this in the Experimental Design section (2.1): \u201cThe total number of unique individual tasks (i.e., task trees of depth 1) is 8 operators \u2217 26 shapes \u2217 10 colors = 2080 unique individual task commands, but can be straightforwardly extended with modifications to the configuration file.\u201d \n\n## Specific questions. (*Italics* indicates Reviewer comment/question.)\n\n*COG task: It will be useful to discuss the COG task (rather than just mentioning it) before describing the new gCOG one, so that it will be clearer to the reader what are new contributions of the new benchmark compared to COG and the degree of their importance. In the overview diagram I would also recommend showing a sample also from COG to make the differences clearer.*\n\nWe thank the Reviewer for the suggestion. We have now clarified the explicit differences between COG and gCOG in the Experimental Design section (2.1): \u201cgCOG is a configurable question-answer dataset, originally inspired from COG (Yang et al., 2018), that programmatically composes task instructions, and then generates synthetic stimuli to satisfy those instructions on-the-fly (Fig. 1). The primary modifications in gCOG are 1) differences in the set of task operators, 2) the ability to use categorical tokens to allow for generic testing of multimodal reasoning, and 3) the ability to allow for arbitrarily long task trees to assess productive compositional generalization, in addition to distractor and systematic generalization (e.g., see Appendix Fig. 7). Importantly, the original COG task did not allow for tasks with more than a single conditional statement, e.g., a task tree of depth 3, making it ill-suited to evaluate productive compositional generalization... We additionally provide functionality in the dataset that allows the choice to load samples using either a categorical task encoding, or a task encoding with image pixels and natural language task instructions.\"\n\nDue to copyright and the license of the original COG paper, we were unable to include a figure from the original COG task/paper (Springer publishing). However, we have included sample questions/queries from the COG task in the Appendix: \u201cA few example queries from the original COG task include: `What is the color of the latest triangle? Point to the latest red object. If a square exists, then point to the current x, otherwise point to the last b.\u2019\u201d"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1924/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700534592248,
                "cdate": 1700534592248,
                "tmdate": 1700534674409,
                "mdate": 1700534674409,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "E1iHQYM4dO",
                "forum": "zyBJodMrn5",
                "replyto": "umBGrmnYm6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1924/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1924/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Djb6 (N/N)"
                    },
                    "comment": {
                        "value": "*Grid size / generalization: It could be interesting to vary the size of the grid in training/evaluation and study its impact on model\u2019s performance.* \n\nThis is a good suggestion that we originally considered. We will provide a configuration file that specifies the dimensions of the task grid, so altering the specific grid size can be straightforwardly implemented in future studies. \n\n*Terminology: I recommend changing the phrase \u201cDistractor generalization\u201d to one that better conveys it\u2019s about changing the answer distribution. Maybe e.g. answer distribution shift. I also recommend changing the name \u201cSystematic compositional generalization\u201d to \u201ccombinatorial generalization\u201d, to emphasize that the main point is the generalization to permutation, and also to better contrast it with the following \u201cProductive generalization\u201d (which could also be systematic).* \n\nOn distractor generalization: Initially, we planned to call it noise generalization, as distractors can be viewed from the perspective of noise. However, we settled on \u2018distractor\u2019 generalization, as it is similar to how distractors are commonly referred to in psychology tasks. In this scenario, distractors serve to make it harder to evaluate the task instruction due to additional visual distractors. Adding/subtracting distractors would not change the distribution of the answer. \n\nOn systematic and productive compositional generalization: We have opted to keep the terms systematic and productive compositional generalization, given the established precedence in the machine learning literature for those terms (see Hupkes et al., 2020, JAIR, for a comprehensive review defining systematic and productive compositionality). However, the Reviewer is correct that systematic compositional generalization is intuitively similar to combinatorial generalization, since a novel combination of task instructions is employed. We have therefore made this link in the Introduction, when we first introduce the term systematic compositional generalization. Revisions to the Introduction: \u201cThis design allowed us to comprehensively evaluate a variety of neural network architectures on tasks that test for three different forms of OOD generalization: 1) Distractor generalization (generalization in the presence of a different noise distribution), 2) Systematic compositional generalization (generalization to new permutations of task structures, **i.e., combinatorial generalization)**, and 3) Productive compositional generalization (generalization to task structures of greater complexity).\u201d \n\n*Figures: Would be good to increase the size of the plots in Figure 3b. It will also be good the increase the distance and visual separation between the sub-figures in each figure throughout the paper.* \n\nWe have now increased the size of the plots in Figure 3b, splitting panel 3b in to 3b and 3c. We have also worked to increase the amount of visual separation between panels in Figure 2, but note for some figures this was difficult due to space constraints."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1924/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700534846753,
                "cdate": 1700534846753,
                "tmdate": 1700534862221,
                "mdate": 1700534862221,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]