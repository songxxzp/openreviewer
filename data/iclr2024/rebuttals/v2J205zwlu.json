[
    {
        "title": "UniPose: Detecting Any Keypoints"
    },
    {
        "review": {
            "id": "Hv4fZiSPBa",
            "forum": "v2J205zwlu",
            "replyto": "v2J205zwlu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1029/Reviewer_Di8P"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1029/Reviewer_Di8P"
            ],
            "content": {
                "summary": {
                    "value": "The authors proposed a unified framework named UniPose to estimate keypoints for any object with visual and tuxtual prompts. The trained model could generalize to cross-instance and cross-keypoint classes, as well as various styles, scales, and poses.  The authors unified 13 keypoint detection datasets with 338 keypoints across 1,237 categories over 400K instances, and employed such large-scale dataset to train the UniPose model, and obtained a generalist keypoint detector."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1) This work arranges a large scale keypoint detection dataset.\n2) The paper proposes a large keypoint detection model with visual and tuxtual prompts.\n3) The proposed model shows good performance in generalization.\n4) The experiments are numerous."
                },
                "weaknesses": {
                    "value": "Overall, the current version of paper does not provide sufficient explanations and in-depth analyses on major points. There are lots of experiments on comparison, but the in-depth experimental analyses (e.g., on prompts) are more necessary and meaningful.\n\n- The most important one weakness is that there is no clear explanation for the textual prompts of keypoints. For examples, clothing or table could have ambiguous keypoint names, how to employ textual prompts to detect their keypoints? \n\n- Another important issue is that all the visualizations are shown without prompts and thus difficult to analyse. It is critical for understanding the effects of various prompts.\n\n- In Section 4.1, why not apply UniPose in the setting of class-agnostic pose estimation as in CapeFormer? The comparison with adapted CapeFormer is unfair and unclear for indicating the performance of UniPose on detecting unseen objects.\n\n- Lacking analysis for Section 4.3. The performance gaps are trivial, considering the differences on data and tasks.\n\n- In Section 4.4, how to evaluate the SOTA open-vocabulary object detector on keypoint-level detection? Is is feasible to represent each keypoint with a small bbox in fine-tuning?\n\n- Why does CapeFormer drop a lot in Tab. 12? Are the training and test resolutions of CapeFormer consistent? \n\n- What are UniPose-T and UniPose-V in Tab. 4?\n\n- In Section 4.4, what is CLIP score? How to analyse the CLIP scores of UniPose and CLIP in Fig. 7?"
                },
                "questions": {
                    "value": "See Weaknesses*"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1029/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698250676389,
            "cdate": 1698250676389,
            "tmdate": 1699636029227,
            "mdate": 1699636029227,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "3ilh49U7Su",
                "forum": "v2J205zwlu",
                "replyto": "Hv4fZiSPBa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1029/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1029/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors (Part 1)"
                    },
                    "comment": {
                        "value": "**1. The most important one weakness is that there is no clear explanation for the textual prompts of keypoints. For examples, clothing or table could have ambiguous keypoint names, how to employ textual prompts to detect their keypoints?**\n\nDuring the construction of our UniKPT dataset, one of the great efforts is to address the issues of ambiguous keypoint names in existing datasets. We merge and unify all the keypoints from 13 datasets into 338 keypoints in the UniKPT dataset. And we take great care in annotating each keypoint with comprehensive textual descriptions, incorporating rich directional information such as 'upper,' 'lower,' 'left,' and 'right,' and assigning each keypoint a unique name to add the fine-grained semantic meaning and prevent any conflicts. We will provide a comprehensive list of 338 textual prompts for keypoints in the Appendix of the revised version.\n\nFor instance, in the DeepFashion2 dataset, keypoints are identified solely by numerical labels (e.g., 1, 2, 3, 4) for clothing items. In contrast, our annotation process involves the detailed labeling of specific keypoints. For example, when annotating the neckline, we distinguish between six distinct keypoints: 'upper center neckline,' 'upper right neckline,' 'lower right neckline,' 'lower center neckline,' 'lower left neckline,' and 'upper left neckline.' This enables our model to achieve fine-grained text-to-keypoint alignment and utilize corresponding keypoint prompts for precise detection.\n\n**2. Another important issue is that all the visualizations are shown without prompts and thus difficult to analyse. It is critical for understanding the effects of various prompts.**\n\nThanks for the suggestions. We will supplement the corresponding prompts along with visualizations in the revised version.\nFor in-the-wild images presented in Figure 2, we employ textual prompts during testing, where these textual prompts are user-friendly and are particularly suitable for large-scale image testing. \nWe default to using the keypoints description defined by AP-10K for animal categories. For the human category, we opt for the keypoints description defined by COCO. The results show that UniPose demonstrates exceptional fine-grained localization capabilities and its ability to generalize effectively across different image styles, instances, and poses. \n\n\n**3. In Section 4.1, why not apply UniPose in the setting of class-agnostic pose estimation as in CapeFormer? The comparison with adapted CapeFormer is unfair and unclear for indicating the performance of UniPose on detecting unseen objects.**\n\nIn Table. 2, we indeed have conducted a fair comparison with CapeFormer and existing methods in the same class-agnostic pose estimation setting using the MP-100 dataset for training and testing. CapeFormer is a top-down-based method that utilizes ground-truth bounding boxes to crop images into individual object images for performing class-agnostic pose estimation. However, UniPose is an end-to-end method with a significant advantage in its ability to generalize to different objects and detect new ones. To maintain consistency with CapeFormer's settings, we also use ground-truth bounding boxes, foregoing object generalization. The results show that UniPose, as an end-to-end framework, could surpass existing top-down-based approaches with a superior inference time.\n\nFurthermore, it is essential to highlight UniPose's competence in conducting detection and pose estimation in multi-object, multi-class scenarios\u2014an achievement CapeFormer cannot attain. As demonstrated in Table. 3 and 12, we present results without ground-truth bounding boxes and test CapeFormer in single-/multi-object scenarios. These results underscore a notable performance decrease for CapeFormer in single-object scenarios (where disguising the background and object is challenging), and it demonstrates an inability to effectively handle scenes with multiple objects.\n\n\n**4. Lacking analysis for Section 4.3. The performance gaps are trivial, considering the differences on data and tasks.**\n\nExisting general models focus on training models to handle multiple vision tasks. However, in the field of pose estimation, their performance is not optimal due to the gap involved in handling multiple tasks, even when they employ large models. Also, due to the limited scope for pose estimation, they all tend to focus solely on tasks like person keypoint detection (e.g., COCO) or animal keypoint detection (e.g., AP-10K).\n\nIn comparison, our focus is to unify the keypoint detection tasks and target a keypoint generalist that achieves effectiveness and generality in keypoint detection across any articulated (e.g., human, animal, and detailed face and fingers), rigid (e.g., vehicle), and soft objects (e.g., clothing). \nIn section 4.3, we compare UniPose with existing general models to demonstrate the effectiveness of UniPose as a powerful generalist model in the field of pose estimation."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1029/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700011659379,
                "cdate": 1700011659379,
                "tmdate": 1700011659379,
                "mdate": 1700011659379,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "bkwh9Gyco7",
            "forum": "v2J205zwlu",
            "replyto": "v2J205zwlu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1029/Reviewer_Ldy8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1029/Reviewer_Ldy8"
            ],
            "content": {
                "summary": {
                    "value": "This paper makes the first attempt to propose an end-to-end coarse-to-fine keypoints detection framework named UniPose trained on a unified keypoint dataset, which can detect the keypoints of any object from the instance to the keypoint levels via either visual prompts or textual prompts and has remarkable generalization capabilities for unseen objects and keypoints detection. This work unifies 13 keypoint detection datasets containing 338 keypoints detection over 400K instances to train a generic keypoint detection model. Compared to the state-of-the-art CAPE method, this method exhibits a notable 42.8% improvement in PCK performance and far outperforms CLIP in discerning various image styles."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* Originality\uff1a\nThe first attempt of an end-to-end keypoints detection framework is developed by combining visual and textual prompts. Through the mutual enhancement of textual and visual prompts, this method can have strong fine-grained localization and generalization abilities for class-agnostic pose estimation and multi-class keypoints detection tasks.\n\n* Quality\uff1a\nThe method description is detailed, and it is illustrated in the accompanying figures. \n\nThe experiment is fully configured, taking into consideration unseen objects, expert model, general model, and open-vocabulary model. Comparative tests are conducted.\n\n* Clarity\nThe paper exhibits a well-structured logical flow, accompanied by an appendix that provides an extensive overview of the work, including the introduction of the dataset, supplementary experiments, algorithmic limitations, and more.\n\n* Significance\nThis paper Unifies 13 datasets to build a unified keypoints detection dataset named UniKPT. And the authors say that each keypoint has its own text prompts, and each category has its default set of structured key points. This unified dataset with visual and textual prompts can provide data support for point detection tasks in future work."
                },
                "weaknesses": {
                    "value": "There are no complete examples of textual prompts.\n\nIn 4.4, the Fig.7 should be Tab.7."
                },
                "questions": {
                    "value": "For the Inference Pipeline, whether only one prompt is used as input, and whether it will work better if two prompts are used?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1029/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1029/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1029/Reviewer_Ldy8"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1029/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698677142023,
            "cdate": 1698677142023,
            "tmdate": 1699636029138,
            "mdate": 1699636029138,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "JqOOU3QUgy",
            "forum": "v2J205zwlu",
            "replyto": "v2J205zwlu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1029/Reviewer_SvA3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1029/Reviewer_SvA3"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a so-called UniPose to detect various types of keypoints. The UniPose takes text or image with keypoint annotations as prompts, in order to detect the corresponding keypoints in query image. In order to support both modalities of prompts, the visual prompt encoder, textual prompt encoder, and two decoders are developped. The model is trained on 13 keypoint detection datasets with 338 keypoints, and the results show it can detect varying types of keypoints to some extent."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1) A model which supports textual or visual prompts are proposed.\n\n2) The visualization shows the effectiveness to some extent."
                },
                "weaknesses": {
                    "value": "1) Reading through the paper, it lets reviewer feel that the writing requires significant improvement and the math symbols are in mess. Moreover, there are already many works in literature using visual prompts such few-shot keypoint detection [1,2,3,4], and also the works regarding textual prompts such as [5,6]. The paper should fully discuss these two types of related works. From the technical perspective, it shows little advance compared to existing works. The simple combination of both types of prompts make the contribution and novelty of this work weak. \n\n    [1] Metacloth: Learning unseen tasks of dense fashion landmark detection from a few samples @ TIP21\n\n    [2] Few-shot keypoint detection with uncertainty learning for unseen species @ CVPR'22\n\n    [3] Pose for everything+Towards category-agnostic pose estimation @ ECCV'22\n\n    [4] Few-shot Geometry-Aware Keypoint Localization @ CVPR'23\n\n    [5] Clamp: Prompt-based contrastive learning for connecting language and animal pose @ CVPR'23\n\n    [6] Language-driven Open-Vocabulary Keypoint Detection for Animal Body and Face @ arxiv'23\n\n2) Some claims may not be true. For example, ``Xu et al. (2022a) first proposed the task of category-agnostic...\". Work [3] may not be the first work as [1-2] are earlier. Moreover, what is the meaning of \"the keypoint to keypoint matching schemes without instance-to-instance matching are not effective\"? The keypoint representation can also aggregate the global information or context.\n\n3) Given the existence of a similar approach such as CLAMP [5], it's essential to evaluate the performance of your method when compared to CLAMP. This evaluation should be conducted on the Animal pose dataset within the context of a five-leave-one-out setup. In this setting, the model is trained on four different species while being tested on the remaining one species. The paper should include the results of PCK@0.1 to facilitate meaningful comparisons.\n\n3) Coarse-to-fine strategy already appears in paper [2023-ICLR-Explicit box detection unifies end-to-end multi-person pose estimation]; while the ideas of using prompts based keypoint detection already appears in FSKD, CLAMP, etc.\n\n4) Some details are missing. This work uses CLIP as image and text encoder. The CLIP generally takes an image with size of 224 as input, while in table 2 and 3, the UniPose takes original image or image with size of 800 as input. Will the high-resolution input slow down the speed? What is the size of feature map after passing image (e.g. 800) through CLIP? A step forward, will the CLIP retain its prior knowledge after using such a high-resolution input?\n\n    Moreover, how to select the visual object feature and textual object feature to produce $Q_{obj}$? How to select if both exist?\n\n5) In table 1, some of datasets are already included in MP-100. For example, COCO, AP-10K, etc. So what is the meaning of counting it again to build the dataset of UniKPT?"
                },
                "questions": {
                    "value": "See weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1029/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698765858498,
            "cdate": 1698765858498,
            "tmdate": 1699636029053,
            "mdate": 1699636029053,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "nUxUOdKt7j",
                "forum": "v2J205zwlu",
                "replyto": "JqOOU3QUgy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1029/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1029/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors (Part 1)"
                    },
                    "comment": {
                        "value": "**1. Reading through the paper, it lets reviewer feel that the writing requires significant improvement and the math symbols are in mess.**\n\nThanks for the suggestions. Our main article has aligned mathematical symbols with our implementation details. We will carefully refine this version and improve the notation in the revised edition. If you have specific examples in mind, we would be grateful if you could share them, and we can focus on those for better revisions.\n\n**2. There are already many works in literature using visual prompts such few-shot keypoint detection [1,2,3,4], and also the works regarding textual prompts such as [5,6]. The paper should fully discuss these two types of related works. From the technical perspective, it shows little advance compared to existing works. The simple combination of both types of prompts make the contribution and novelty of this work weak.**\n\n\nFirst and foremost, we would like to emphasize that UniPose is the first end-to-end framework that can generalize to unseen objects and keypoints in multi-object, multi-class scenes. In comparison, **all the methods listed above [1-6] are top-down approaches, as they rely on ground-truth bounding boxes to crop a multi-object image into several single-object images and subsequently employ visual or textual prompts for the following single-object keypoint detection.** Thus, these methods are unable to address multi-class multi-object scenarios without known instance-level object detection, particularly situations where an image contains numerous objects of different categories with varying keypoint definitions. \n\n\nSecondly, UniPose is the first multi-modal prompt-based pose estimator. However, **the methods [1-6] only support a single modal prompt**. Only supporting visual prompts makes the user interaction unfriendly and inefficient, while only supporting textual prompts lack fine-grained low-level visual information and make it hard to localize the indescribable positions. UniPose jointly leverages visual and textual prompts for training via cross-modality contrastive learning to boost the generalization and effectiveness of any keypoint detection. We have demonstrated the mutual benefit between the two kinds of prompts in Table. 10. \n\n\n\nThirdly, UniPose targets a keypoint generalist that achieves effectiveness and generality in keypoint detection across any articulated (e.g., human, animal, and detailed face and fingers), rigid (e.g., vehicle), and soft objects (e.g., clothing), which is trained on the proposed UniKPT dataset.\nIn contrast, references [1] or [2] primarily focus on single-object keypoint detection, specifically within the clothing or animal super-categories. Reference [3,4], despite handling more objects, exhibits limitations in terms of generalizability and effectiveness due to relatively small-scale training data.\nAs for reference [5], as detailed in our related work section, CLAMP concentrates solely on single-object animal keypoint detection. It leverages CLIP with language guidance to prompt animal keypoints containing a fixed keypoint set (e.g., 20) and doesn't consider keypoint-level open vocabulary. The primary emphasis of CLAMP lies in cross-species generalization within a predefined skeleton structure, a domain where UniPose has made substantial advancements.\nRegarding reference [6], it's noteworthy that this work was submitted to Arxiv in October 2023, coinciding with our work. Also, it focuses on single objects, with a specific focus on animal body and facial parts.\n\n**3. Some claims may not be true. For example, ``Xu et al. (2022a) first proposed the task of category-agnostic...\". Work [3] may not be the first work as [1-2] are earlier.**\n\nIn fact, we follow the definition of the category-agnostic pose estimation (CAPE) task by [3]. This task requires the pose estimator to detect keypoints of arbitrary categories, such as animals, clothing, furniture, and persons, given the keypoint definitions.\n\n[1] and [2] primarily concentrate on a single super category like clothing or animals. We will clarify this distinction and explain it in the revised version."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1029/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700010728727,
                "cdate": 1700010728727,
                "tmdate": 1700010728727,
                "mdate": 1700010728727,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "0Z321V3FSC",
            "forum": "v2J205zwlu",
            "replyto": "v2J205zwlu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1029/Reviewer_WBby"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1029/Reviewer_WBby"
            ],
            "content": {
                "summary": {
                    "value": "The paper discusses a new framework called UniPose, which aims to detect keypoints in various objects, including articulated, rigid, and soft ones, using visual or textual prompts. Keypoints are pixel-level representations of objects, especially articulated ones. Current fine-grained promptable tasks focus on object instance detection and segmentation but struggle with identifying detailed structured information, such as eyes, legs, or paws. The UniPose framework is the first attempt to create an end-to-end prompt-based keypoint detection system that can be applied to any object. It unifies various keypoint detection tasks and leverages multiple datasets to train a generic keypoint detection model. UniPose aligns textual and visual prompts through cross-modality contrastive learning optimization, resulting in strong fine-grained localization and generalization capabilities across different image styles, categories, and poses. The framework is expected to enhance fine-grained visual perception, understanding, and generation."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper is well-written and easy to follow.\n2. This article provides a comprehensive summary of previous keypoint research, highlighting both its strengths and weaknesses, and uses this as motivation to propose its own approach.\n3. The methodology in this article is well-designed, combining both text and image prompts for keypoint detection ."
                },
                "weaknesses": {
                    "value": "This article combines two approaches to prompts, but lacks in-depth analysis of the strengths and weaknesses of both modalities for this task:\n\n1.What are the advantages and disadvantages of each prompt individually, and can you provide some visual results?\n2. Can the strengths and weaknesses of the two prompts complement each other?\n3. Is it possible to dynamically weight the two prompts? For example, can text be prioritized when there are no suitable images to serve as prompts?"
                },
                "questions": {
                    "value": "Please see the weakness. If you address my concerns, I am willing to improve my score. Thanks."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1029/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1029/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1029/Reviewer_WBby"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1029/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699339007539,
            "cdate": 1699339007539,
            "tmdate": 1699636028989,
            "mdate": 1699636028989,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]