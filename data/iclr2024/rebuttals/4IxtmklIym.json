[
    {
        "title": "FruitBin: A tunable large-scale dataset for advancing 6D Pose estimation in fruit bin picking automation"
    },
    {
        "review": {
            "id": "fIMo8HxZ9u",
            "forum": "4IxtmklIym",
            "replyto": "4IxtmklIym",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9289/Reviewer_bGzb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9289/Reviewer_bGzb"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a large-scale PickSim-based synthetic dataset FruitBin for 6D object pose estimation in fruit bin picking. The dataset features comprehensive challenges and devised benchmarks for scene and camera view generalization as well as occlusion."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This is the first 6D object pose estimation dataset tailored for fruit bin picking although it is synthetic."
                },
                "weaknesses": {
                    "value": "-- One drawback of Gazebo is that it can not do photorealistic rendering for objects and scenes with PBR textures. Although the generated dataset is large, without photorealistic textures, the transfer ability to real world is limited compared with other simulators such as BlenderProc and Kubric even the domain randomization techniques have been leveraged.\n\n-- For real-world fruits, the size and shape of different instances of the same category vary to different degrees. However, it seems for FruitBin, these factors are not taken into consideration.\n\n-- There is no real test set for the dataset, which is essential for sim2real and real-world applications.\n\n-- The benchmarking methods are a bit outdated. PVNet and DenseFusion are from 2018-2019, but it is 2023 now. \n\n-- It would be better to showcase some robotic applications like bin picking using this dataset, since it is targeted for fruit bin picking.\n\n-- It would be better to mark symmetric objects with \"*\" in Table 2.\n\n-- Table 1 is too wide. \n\n-- There are some minor issues in the writing, more thorough proofreading is required."
                },
                "questions": {
                    "value": "1) In the experiments, does PVNet use GT bboxes for cropping the objects in order to handle multiple instances of the same object class?\n\n2) How does the diffusion generated backgrounds contribute to the performance?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9289/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9289/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9289/Reviewer_bGzb"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9289/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698075294291,
            "cdate": 1698075294291,
            "tmdate": 1699637170318,
            "mdate": 1699637170318,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wDUIauCh4y",
                "forum": "4IxtmklIym",
                "replyto": "fIMo8HxZ9u",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9289/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9289/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Answer to the review"
                    },
                    "comment": {
                        "value": "Dear Reviewer,\n\nWe are grateful for your comprehensive review and constructive feedback on our manuscript. We have taken the time to address each of your comments and concerns as follows:\n\n**Photorealistic Rendering:**\n\nWe concur with your observation regarding the limitations of Gazebo in terms of photorealistic rendering. While state-of-the-art generators such as Blenderproc and Kubric offer some advantages, they lack crucial occlusion annotation. Furthermore, we emphasize that models trained on our dataset can be integrated and tested for robotic simulation, a feature not guaranteed with Kubric and Blenderproc (we are not certain that the PBR rendering can be integrated on a camera in Pybullet, for example).\n\nWe are cognizant of these limitations and believe that domain randomization is sufficient to reduce the sim2real gap. Moreover, as a general way of addressing the recurrent sim2real gap, data augmentation or domain adaptation [4] can further reduce the sim2real gap, and we demonstrated that a simple diffusion model can generate a good variety of realistic backgrounds.\n\n**Variation in Fruit Size and Shape:** \n\nWhile our current focus is on 6D pose estimation, we acknowledge the lack of variation in size and shape. Although not part of the benchmark yet, we leveraged PickSim to generate testing data where random scale variation over the three directions is applied to vary the shapes. Additionally, to answer real-world scenarios of industry and supermarkets, we generated bin picking for only one type of object. This comes with two variations: one where the mesh is unique and the second where mesh modification is applied.\n\n**Real Test Set:** \n\nWe agree with your assertion on the importance of a real test set for sim2real and real-world applications. To this end, we intend to augment our dataset with real-world data to create a test set for evaluating sim2real performance.\n\n**Benchmarking Methods:** \n\nWe concur that our benchmarks should include more recent methods. Although Pvnet and Densefusion are getting outperformed by recent methods, we believe that they remain representative baselines. It is worth noting that even with the recent result of the 6D pose BOP challenge 2023 [3], the difficulties are still the same. We can highlight non-negligible performance differences for occluded or cluttered datasets (LM-O, IC-BIN, ITODD) with scores below 0.8 and better than 0.9 for less occluded and cluttered ones ( T-LESS, YCB-V). This strengthens the need for occlusion studies in 6D pose estimation and benchmarks.\n\nWe understand the importance of strong baselines in demonstrating the effectiveness of our approach, and **we are working to integrate GRD-NET**[1][2], which has been \u201cThe Best Open-Source Method\u201d for the benchmarks in the BOP challenge in 2022 and 2023[3] to provide a more precise evaluation of our dataset.\n\n**Robotic Applications:** \n\nWe appreciate your suggestion to demonstrate robotic applications such as bin picking using our dataset. For this rebuttal, we haven\u2019t yet included grasping with the real robot. However, as an early proof of concept, a grasping simulation pipeline has been set up as follows:\n\n- we automatically generate a grasping list associated with the mesh,\n- we evaluate the 6D pose with our trained Densefusion model\n- we used free obstacle path planning libraries such MoveIt to grasp the object with the list of grasp and the 6D pose of the object\n\n**Symmetric Objects in Table 2:** We thank you for your suggestion and denoted symmetric objects with an asterisk (*) in Table 2 in the revised version of the paper.\n\n**Writing Issues:** We will address the writing issues you pointed out, including reducing the size of Table 1, separating additional work from the main paper, and improving the marking of symmetric objects in Table 2.\n\n**Questions:** \n\n**PVNet and GT Bboxes:** In all our generated benchmarks, we have only considered 1 instance of a category in the data. The filtering parameters for the benchmarks\"\n\n**Diffusion-Generated Backgrounds:** The diffusion backgrounds have only been tested to showcase the feasibility of improving the sim2real gap. However, we haven\u2019t explored how it would impact the model for 6D pose estimation. \nWe hope that our responses address your concerns.\n\n[1]Wang, Gu et al. \u201cGDR-Net: Geometry-Guided Direct Regression Network for Monocular 6D Object Pose Estimation.\u201d 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2021): 16606-16616.\n\n[2] https://github.com/shanice-l/gdrnpp_bop2022\n\n[3] https://bop.felk.cvut.cz/challenges/bop-challenge-2023/\n\n[4] Truong, J., Chernova, S., & Batra, D. (2021). Bi-directional domain adaptation for sim2real transfer of embodied navigation agents. IEEE Robotics and Automation Letters, 6(2), 2634-2641."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9289/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700692003656,
                "cdate": 1700692003656,
                "tmdate": 1700692003656,
                "mdate": 1700692003656,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "AQCFnL54QB",
            "forum": "4IxtmklIym",
            "replyto": "4IxtmklIym",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9289/Reviewer_gFnj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9289/Reviewer_gFnj"
            ],
            "content": {
                "summary": {
                    "value": "* This paper tackles novel research direction of fruits (or generalized any grocery item) using a robo-arm. \n* Dataset uses RGB and depth cameras for curating and annotatings the dataset."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* This industry really needs a good dataset to further explore the problem, this paper just targeted that. \n* This paper generalizes scenes as well as camera position for wider acceptability of it. \n* Good reference to prior work on datasets."
                },
                "weaknesses": {
                    "value": "* I would have preferred to see even more robust baselines."
                },
                "questions": {
                    "value": "NA"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9289/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698419715088,
            "cdate": 1698419715088,
            "tmdate": 1699637170210,
            "mdate": 1699637170210,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Fn31lhREVx",
                "forum": "4IxtmklIym",
                "replyto": "AQCFnL54QB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9289/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9289/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Answer to the review"
                    },
                    "comment": {
                        "value": "Dear Reviewer,\n\nThank you for your review and positive feedback on our paper. We are pleased that you found our research direction and dataset valuable for the industry.\n\nWe appreciate your suggestion regarding the need for more robust baselines. Although Pvnet and Densefusion are getting outperformed by recent methods, we believe that they remain representative baselines. It is worth noting that even with the recent results of the 6D pose BOP challenge 2023 [3], the difficulties are still the same. We can highlight non-negligible performance differences for occluded or cluttered datasets (LM-O, IC-BIN, ITODD) with scores below 0.8 and better than 0.9 for less occluded and cluttered ones (T-LESS, YCB-V). This strengthens the need for occlusion study in 6D pose estimation and benchmarks.\n\nWe understand the importance of strong baselines in demonstrating the effectiveness of our approach. **We are working to integrate GRD-NET**[1][2], which has been \u201cThe Best Open-Source Method\u201d for the benchmarks in the BOP challenge in 2022 and 2023[3], to provide a more precise evaluation of our dataset.\nOnce again, we thank you for your time and constructive feedback. We look forward to incorporating your suggestions to improve our work.\n\n[1]Wang, Gu et al. \u201cGDR-Net: Geometry-Guided Direct Regression Network for Monocular 6D Object Pose Estimation.\u201d 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2021): 16606-16616.\n\n[2] https://github.com/shanice-l/gdrnpp_bop2022\n\n[3] https://bop.felk.cvut.cz/challenges/bop-challenge-2023/"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9289/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700691359911,
                "cdate": 1700691359911,
                "tmdate": 1700691359911,
                "mdate": 1700691359911,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "POpqW93t2O",
            "forum": "4IxtmklIym",
            "replyto": "4IxtmklIym",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9289/Reviewer_J2Gb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9289/Reviewer_J2Gb"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents FruitBin, a 6D pose estimation dataset for fruit bin picking with benchmarking over scene generalization, camera generalization and occlusion robustness. It contains over a million images and 40 million instance-level 6D poses."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "- This paper proposes a large-scale dataset, which may facilitate future research for bin-picking tasks.\n- The technical details are clearly presented."
                },
                "weaknesses": {
                    "value": "- Limited Contribution\n    - It seems that the technical contributions of this paper is just replacing the assets in PickSim with fruits. I don't think this contribution is sufficient for an ICLR paper.\n    - All the data are collected in the simulator. It seems that no data is collected in the real world.\n- Inconvient Platform\n    - This paper uses ROS+Gazebo as its simulator platform, and claims it's for \"seamless robot learning\". However, I would think mujoco, PyBullet, or Isaac Gym are some more popular options in the robot learning community.\n- Format Issues\n    - Table 1 and the references are with format issues.\n    - The supplementary materials should not be attached to the main paper."
                },
                "questions": {
                    "value": "- Will the dataset include more samples collected in the real world?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9289/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9289/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9289/Reviewer_J2Gb"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9289/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698567553527,
            "cdate": 1698567553527,
            "tmdate": 1699637170037,
            "mdate": 1699637170037,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "pDOimQO52T",
                "forum": "4IxtmklIym",
                "replyto": "POpqW93t2O",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9289/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9289/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Answer to the review"
                    },
                    "comment": {
                        "value": "Dear Reviewer,\n\nWe are grateful for the time and effort you have invested in reviewing our paper. We appreciate your feedback and would like to address your concerns.\n\n**Limited Contribution:** \n\nWe acknowledge your concern regarding the technical contributions of our paper. However, we believe that our datasets, specifically designed for 6D pose estimation in bin picking, could significantly advance the development of 6D pose estimation models. Our unique dataset brings together major challenges in 6D pose estimation, and its large scale allows for the creation of specific benchmarks. This is not proposed by the current existing datasets. The recent 2023 BOP challenge results highlight that state-of-the-art 6D pose estimation models are still sensitive to complex scenes and occlusion, as evidenced by a drop of 0.1/0.2 for datasets with occlusion and bin picking, indicating room for improvement.\n\nRegarding the lack of real-world data, we agree that this is a significant limitation. We are currently collecting data from physical setups to enhance our dataset, allowing for a better evaluation of the sim2real gap and the performance of models under real-world conditions.\n\n**Inconvenient Platform:** We selected ROS+Gazebo due to its widespread adoption in the robotics community and its compatibility with various hardware. While Mujoco, PyBullet, and Isaac Gym are gaining popularity in the robot learning community, we maintain that ROS+Gazebo remains one of the most utilized platforms for robotic simulation [1]. To our knowledge, Mujoco, Pybullet, and Isaac Gym may be more low-level robot learning-oriented than Gazebo+ROS. The latter offers the advantages of a large community and integrates popular high-level libraries such as MoveIt. The default use of ROS enables users to easily apply their developed pipeline to real robots. Specifically to robot learning, even if maybe less popular, Gazebo is still a reliable choice [2,3].\n\n**Format Issues:** We apologize for the formatting issues in Table 1 and the references. These will be corrected in the revised version of the paper, along with the supplementary materials detached from the main paper.\n\n**Questions:** In response to your query, we do indeed plan to include samples collected in the real world in the dataset. We believe this enhancement will increase the dataset\u2019s value for the community.\n\n[1] Collins, J., Chand, S., Vanderkop, A., & Howard, D. (2021). A review of physics simulators for robotic applications. IEEE Access, 9, 51416-51431.\n\n[2] Zamora, Iker, et al. \"Extending the openai gym for robotics: a toolkit for reinforcement learning using ros and gazebo.\" arXiv preprint arXiv:1608.05742 (2016).\n\n[3] Ferigo, D., Traversaro, S., Metta, G., & Pucci, D. (2020, January). Gym-ignition: Reproducible robotic simulations for reinforcement learning. In 2020 IEEE/SICE International Symposium on System Integration (SII) (pp. 885-890). IEEE."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9289/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700691239696,
                "cdate": 1700691239696,
                "tmdate": 1700691239696,
                "mdate": 1700691239696,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "kLxVSQjN3B",
            "forum": "4IxtmklIym",
            "replyto": "4IxtmklIym",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9289/Reviewer_Ksan"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9289/Reviewer_Ksan"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a novel and extensive dataset designed for the task of fruit bin picking. This dataset is entirely synthetic and comprises 3D meshes of eight distinct fruits arranged in randomized configurations within bins, with varying lighting conditions and camera perspectives. The research employs this dataset to train two distinct models, one utilizing RGB data and the other incorporating RGB-D information, to serve as exemplary methods for 6-DOF pose estimation."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is well-written and easy to understand. It explains its ideas clearly, making it accessible to a broad audience.\n2. The dataset is extensive regarding images, configurations, and annotations. \n3. The paper also offers detailed insights into the dataset, providing readers with a comprehensive understanding of its composition. This helps other researchers in utilizing the dataset effectively.\n4. The synthetic nature of the dataset allows for the extraction of highly detailed annotations, which can be challenging to obtain in real-world scenarios."
                },
                "weaknesses": {
                    "value": "1. One primary concern regarding the paper pertains to its real-world applicability. While the synthetic dataset's ability to provide detailed annotations is a strength, it also raises questions about the practical utility of algorithms trained on it in real-world scenarios. The paper should delve into the broader implications and limitations of applying such models to real-world fruit-picking scenarios.\n\n2. A related concern is the limited variety of objects in the dataset. With only 8 types of fruits, and a significant majority of them being spherical (75%), the need for 6DOF pose estimation for these objects may be questionable. The paper should address the relevance of 6DOF pose estimation for objects that might not require such detailed positioning information.\n\n3. The paper should explore the broader question of whether 6DOF pose estimation is necessary for fruit picking, particularly when considering that many real-world fruit-picking applications rely on suction grippers, making pose estimation less critical.\n\n4. It is important to clarify the specific scenarios that the dataset targets. Random mixing of different fruits in bins may not represent common real-world scenarios, where fruits are typically harvested in monocultures and packed separately. The paper should outline the dataset's intended use cases and their alignment with real-world applications.\n\n5. While the paper claims diversity in the dataset, I would argue that diversity should be measured by the variety of objects rather than the sheer number of images and annotations. The paper should address these concerns and clarify how the dataset's diversity aligns with its practical usefulness.\n\n6. In my opinion, the representative images in the paper all look similar, and the lighting variations are synthetic without showing real-world visual phenomena (shadows, reflection). The paper should discuss how these factors affect the dataset's applicability to real-world scenarios and consider potential improvements.\n\n7. Some of the language choices throughout the paper, such as the use of \"comprehensive\" to describe the evaluation using two models, are overly grandiose, in my opinion. The paper should adopt more precise and measured language to accurately represent the extent of the evaluation and avoid overstating its findings.\n\n8. As this dataset targets robotic grasping of fruits, I would have liked to see a comparison of using the dataset on 6DOF grasping with a robotic gripper, not only pose estimation."
                },
                "questions": {
                    "value": "1. In the intro, the paper mentions that the dataset contains delicate fruits like bananas and apricots that require haptic feedback for grasping, yet it is not mentioned how this is modeled and incorporated in the benchmark. Is this only in reference to exact pose estimation?\n2. In Table 1. how does the presented dataset compare to other 6DOF datasets regarding object diversity?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9289/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698691264508,
            "cdate": 1698691264508,
            "tmdate": 1699637169923,
            "mdate": 1699637169923,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "T5o9Gf4G2s",
                "forum": "4IxtmklIym",
                "replyto": "kLxVSQjN3B",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9289/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9289/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Answer to the review - Part1"
                    },
                    "comment": {
                        "value": "Dear Reviewer,\n\nThank you for your insightful comments and constructive feedback. We appreciate the time and effort you\u2019ve put into reviewing our paper. We agree with your concerns and have addressed them as follows:\n\n**Real-world applicability:** We acknowledge the limitations of synthetic datasets in replicating real-world conditions. Precise evaluation of our dataset for real fruits is an important question, given that we only have synthetic data. **We are currently scanning 3D real fruits and adding 6D pose real-world data to extend our dataset.** This dataset will be used for evaluating the sim2real gap of our dataset.\n\n**Variety of objects:** We agree that by targeting fruit bin picking, we inevitably limit the variety of shapes. Fruits are indeed mostly smooth compared to artificially created objects, such as industrial objects. However, fruit bin picking is a delicate task that requires careful handling to avoid damaging the fruits. We believe that knowing the semantics of the fruit and its position in the scene is of major importance during the grasping process to avoid any damage.\n\n**Necessity of 6DOF pose estimation:** We believe that 6DOF pose estimation isn\u2019t incompatible with simple objects, and it can provide more precise control for robotic arms, even when using suction grippers. However, suction grasping requires fruits to be smooth, like apples, pears, or maybe bananas. In practical use, it will not be suitable for rugous textures such as lemons, oranges, or kiwis.\n\n**Intended use cases:** The dataset\u2019s intended uses are multiple, with a goal of making it general for multiple purposes:\n\n- The first intended use is benchmark making as it gathers major challenges in 6D pose. It comes as a challenge for the community to make improvements in 6D pose estimation as it is commonly used with the popular BOP challenge.\n- In addition to these existing datasets for benchmarking, ours offers a useful and practical scenario that could easily occur in daily scenarios: fruit industry, house fruit bin, or even supermarket.\n- Mixing fruits in a bin is the most general and difficult scene we could create for 6D pose estimation purposes. However, it is right to note that real-world scenarios, such as industry or supermarkets, usually deal with only one category. In order to address this, we extended our dataset with data with bins of only one category of object.\n\n**Diversity of the dataset**: We understand your concern about the diversity of the dataset. We have included 8 fruits that we believe are the most common (apple, apricot, banana, kiwi, lemon, orange, peach, and pear). However, it remains in the range of the number of objects used by 6D pose datasets.\n\n**Lighting variations:** We agree that real-world visual phenomena like shadows and reflections are important. We are cognizant of these limitations and believe that domain randomization is sufficient to reduce the sim2real gap. Moreover, as a general way of addressing the recurrent sim2real gap, data augmentation or domain adaptation can further reduce the sim2real gap, and we demonstrated that a simple diffusion model could generate a good variety of realistic backgrounds and can create shadows or reflections.\n\n**Language choices:** We have revisited the language by removing some unnecessary adjectives.\n\n**Comparison with robotic gripper:** We appreciate your suggestion to demonstrate robotic applications such as bin picking using our dataset. For this rebuttal, we haven\u2019t yet included grasping with the real robot. However, as an early proof of concept, a grasping simulation pipeline has been set up as follows:\n\n- We automatically generate a grasping list associated with the mesh.\n- We evaluate the 6D pose with our trained Densefusion model.\n- We used free obstacle path planning libraries such as MoveIt to grasp the object with the list of grasp and the 6D pose of the object."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9289/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700690814854,
                "cdate": 1700690814854,
                "tmdate": 1700690814854,
                "mdate": 1700690814854,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PiTbQlfP6D",
                "forum": "4IxtmklIym",
                "replyto": "kLxVSQjN3B",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9289/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9289/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Answer to the review - Part2"
                    },
                    "comment": {
                        "value": "**Question:**\n\nIn response to your query about modeling haptic feedback for delicate fruits, we currently do not incorporate this aspect into our dataset, as our primary focus is on pose estimation. We have clarified this point in the paper.\n\nRegarding object diversity, other benchmark datasets typically include between 8 (occluded linemod) and 39 objects (excluding GraspNet-1B, which primarily focuses on 6DOF grasping rather than 6D pose estimation). This is generally more than our dataset, as we focus solely on fruits. However, it\u2019s important to note that even if the number of objects is low, datasets can pose a significant challenge for the 6D pose estimation community, particularly with occlusion. For instance, the popular dataset \u201coccluded-Linemod\u201d also has only 8 objects but remains one of the most challenging datasets. Indeed, in the recent BOP challenge 2023 [4], this dataset was one of the most difficult, with a score of 0.794 achieved by the winner of the challenge. This is compared to 0.928 for \u201cYCB-video\u201d which has 21 objects, or even \u201cHomebrewedDB\u201d with a score of 0.950 with 33 objects.\n\nWe hope this addresses your concerns.\n\n[1] Zhongkui Wang, Shinichi Hirai, and Sadao Kawamura. Challenges and Opportunities in Robotic Food Handling: A Review, jan 2022. ISSN 22969144.\n\n[2] Tobin, Josh, et al. \"Domain randomization for transferring deep neural networks from simulation to the real world.\" 2017 IEEE/RSJ international conference on intelligent robots and systems (IROS). IEEE, 2017.\n\n[3] Truong, J., Chernova, S., & Batra, D. (2021). Bi-directional domain adaptation for sim2real transfer of embodied navigation agents. IEEE Robotics and Automation Letters, 6(2), 2634-2641.\n\n[4] https://bop.felk.cvut.cz/challenges/bop-challenge-2023/"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9289/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700690956545,
                "cdate": 1700690956545,
                "tmdate": 1700704222328,
                "mdate": 1700704222328,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]