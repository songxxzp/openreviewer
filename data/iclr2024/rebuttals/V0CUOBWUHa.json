[
    {
        "title": "Scaling Sentence Embeddings with Large Language Models"
    },
    {
        "review": {
            "id": "RSKudrzs3F",
            "forum": "V0CUOBWUHa",
            "replyto": "V0CUOBWUHa",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission596/Reviewer_ViaW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission596/Reviewer_ViaW"
            ],
            "content": {
                "summary": {
                    "value": "This paper works on generating sentence embeddings using large language models. First,  the authors design a specific prompt  to compress the semantic of an input sentence into a single word.  Then, this paper investigates zero-shot,  in-context and fine-tuning settings of sentence embedding learning.  For in-context learning,  this paper proposes a demonstration selection method for inducing good sentence representations.  For fine-tuning, to solve the large memory issue,  the authors use QLoRA to perform contrastive learning.  Empirical results on common sentence embedding evaluation benchmarks with both OPT and LLaMA series models show that the proposed method can match (or even exceed) the performance of pretrained language models (such as BERT)."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The writing is easy to follow and the idea is well presented. \n2. The proposed prompt, in-context demonstration and fine-tuning method solve the specific issues of scaling large language models for sentence embedding learning. \n3. The experimental results are effective on both the SentEval and Transfer settings compared to BERT-base based contrastive learning method."
                },
                "weaknesses": {
                    "value": "1. In Table 1,  only the results based on OPT are presented.  Why not also including the results based on LLaMA? \n2. In Table 1,  the best configuration PromptEOL+ICL+ OPT (6.7B) does not show clear advantages than PromptRoBERTa (123M). \n3. For the in-context setting,  why only use one demonstration?   In Table 1,  comparing PromptEOL+ICL + OPT with baselines models  is not fair since the baseline models do not use the development set. \n4. When the model size increases, the performance does always not increase.  Especially, the 13B, 30B, and 60B models do not perform better than smaller models such as 1.3B and 6B models. \n5. The overall method is a little bit heavy. It is worth to discuss whether we should improve the sentence embeddings using large language models."
                },
                "questions": {
                    "value": "1. Do you also try LLaMA 2? \n2. In Equation 1, why using the last token hidden state as the sentence representation instead of the representation vector of the last generated token using the explicit one word prompt? \n3. For the fine-tuning, do you also try including in-context demonstration for the fine-tuning? \n\nMinors: \n\nThe citation format is not correct. Please correct all of them.  Try to use the cite command in a correct way."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission596/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698602302196,
            "cdate": 1698602302196,
            "tmdate": 1699635987119,
            "mdate": 1699635987119,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "cyITnSZ4qm",
                "forum": "V0CUOBWUHa",
                "replyto": "RSKudrzs3F",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission596/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission596/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ViaW (Part 1/2)"
                    },
                    "comment": {
                        "value": "We sincerely thank you for your helpful feedback and insightful comments. We address your comments and questions below.\n\n> **W1:** In Table 1, only the results based on OPT are presented. Why not also including the results based on LLaMA?\n\nIn this work, the main focus is on scaling up sentence embeddings with various model sizes. While LLaMA only has four model sizes, we use OPT as the primary model to showcase the results.\n\nTo address the concern about the robustness of our method, we evaluated the performance of PromptEOL and PromptEOL+ICL on current popular LLMs, varying in size from 3B to 70B. The evaluated LLMs include LLaMA, LLaMA-2, open-LLaMA, MPT, and Mistral.\nOur method shows significant improvement across above 13 models. PromptEOL achieves an average Spearman correlation score of 70.27 on STS tasks among the above models, while PromptEOL+ICL achieves an average score of 77.14. Comparing to the average pooling baseline, PromptEOL and PromptEOL+ICL achieve average improvement of 22.27 and 29.14, respectively. Furthermore, when compared to the previous prompt representation method, PromptEOL and PromptEOL+ICL exhibit an average improvement of 15.55 and 22.42. We have added results in Appendix F of our paper.\n\n\n> **W2:** In Table 1, the best configuration PromptEOL+ICL+ OPT (6.7B) does not show clear advantages than PromptRoBERTa (123M)\n\nPromptRoBERTa is fine-tuned using contrastive learning, whereas PromptEOL+ICL does not require any fine-tuning. It may not be fair to directly compare these two methods. However, PromptEOL+ICL also achieves similar performance to PromptRoBERTa, which demonstrates that a general language model can achieve such performance without relying on contrastive learning or fine-tuning.\nWe also introduce other advantages benefit from PromptEOL+ICL in **W5**.\n\n> **W3:** For the in-context setting, why only use one demonstration? In Table 1, comparing PromptEOL+ICL + OPT with baselines models is not fair since the baseline models do not use the development set.\n\nWe found that one demonstration is sufficient for LLMs to generate good sentence embeddings. Increasing the number of demonstrations does not further improve the performance.\nWe believe the comparison is fair. The ICL examples used in PromptEOL+ICL are taken from STS-B training set and Oxford dictionary. We only use development set to evaluate the performance of ICL examples. In comparison, other baselines such as SimCSE and PromptBERT also utilize STS-B development set to select the best checkpoint during training. Moreover, BERT prompt also utilizes the STS-B development set for prompt searching.\n\n> **W4:** When the model size increases, the performance does always not increase. Especially, the 13B, 30B, and 60B models do not perform better than smaller models such as 1.3B and 6B models.\n\nIn this work, we aim to investigate the capability of LLMs on sentence embeddings by scaling up the model size. As we mention in our paper, we find that continuing to scale up to over ten billion parameters does not always improve the performance of sentence embeddings for STS tasks. We believe that our work can provide valuable insights for future research on leveraging LLMs for sentence embeddings.\n\n> **W5:** The overall method is a little bit heavy. It is worth to discuss whether we should improve the sentence embeddings using large language models.\n\nFirst, our work not only works for LLMs (7B, 13B, 60B models), but also smaller models like OPT 1.3B or 2.7B. Furthermore, we significantly reduce the computation cost of fine-tuning high-quality sentence embeddings compared to the previous SOTA methods like ST5. We can exceed the previous 4.8B ST5 with 2.7B OPT fine-tuned on less training data. Compared to 4.8B ST5,  it uses two-stage training with 2 billion extra question-answers pairs and 275k NLI datasets by fine-tuning all parameters, PromptEOL+CSE can leverage efficient fine-tuning method QLoRA on 275k NLI datasets with one epoch. We can train 2.7B OPT with PromptEOL+CSE for around one hour on four 3090 GPUs. However, it will take more than hundreds of hours with the same hardware environment to train 4.8B ST5.\n\nSecond, PromptEOL+ICL also provides a lightweight method that allows LLMs to generate sentence embeddings without the need of fine-tuning. We can use LLMs themselves to embedding sentence to perform retrieval augmented generation, without additional embedding models. Moreover, we can control the embedding behaviors by modifying prompt and ICL example to suit specific application, such as cross-lingual representation. For instance, we can utilize an ICL example with a Chinese sentence and an English summary word to project the semantics of Chinese sentences onto an English word. This can help bridge the language gap in sentence embeddings, which we leave for future work."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission596/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700117655727,
                "cdate": 1700117655727,
                "tmdate": 1700117655727,
                "mdate": 1700117655727,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "x6ncS8HehX",
                "forum": "V0CUOBWUHa",
                "replyto": "uw305C8JdZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission596/Reviewer_ViaW"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission596/Reviewer_ViaW"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the reminder and the rebuttal.   I can increase my score to 7 but since there is no such an option, I just write it down to  let the AC and the other reviewers know. \n\nFor the rebuttals,  I can fully accept W1 and W3.  Especially thanks for the clarifications of W3. \n\nThe reasons I did not improve my score to 8 are based on two points.  \n\nFirst,  at the high-level, it is not always easy to compress a sentence into a single word.  It can work to some extent for short sentences but the limitation is also very heavy for long sentences. \n\nSecond,  the model performances do not well scale up for model sizes.  This raises concerns why we need large language models for sentence embedding models.  If we aim for small embedding models,  fully finetuning using contrastive learning (can be assisted with in-context examples) is not so expensive, which still can be a good option.  But if we aim for large embedding models, the performances do not improve, which might not be cost-effective.  These concerns put the proposed method in an little embarrassing position.  \n\nIn conclusion,  I believe the proposed method is novel and this is first work in this direction.  However, whether the designed method can fully activate the abilities of LLMs for sentence embedding models is still worth to discuss."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission596/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700585566790,
                "cdate": 1700585566790,
                "tmdate": 1700585566790,
                "mdate": 1700585566790,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pzxg7Tqqvm",
                "forum": "V0CUOBWUHa",
                "replyto": "RSKudrzs3F",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission596/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission596/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for responses. We greatly appreciate your positive comments. \n\nHowever, we would like to clarify above points:\n\n* First, our method does not compress a sentence directly into a single word, but rather uses hidden states as sentence embeddings. This reflects the probability of words that best summarize the sentence, rather than a single word. This ensures that our method also works on long sentence tasks like SUBJ in transfer tasks.\n\n* Second, scaling up works well on transfer tasks and STS tasks with fine-tuning (PromptEOL+CSE). We noticed that the model performance doesn't scale up as effectively only on STS tasks without fine-tuning (PromptEOL+ICL). We also discussed the reason for this issue in **Q1** for reviewer [dT5b](https://openreview.net/forum?id=V0CUOBWUHa&noteId=0USsLQ9H9a).\nRegarding the performance concerns of our method:\n  * Our method with LLMs achieves the state-of-the-art results on transfer tasks and STS tasks, demonstrating the potential of using LLMs to generate sentence embeddings. Our method significantly improves the performance over small models on Table 2 and 3.\n  * Since contrastive learning can alleviate the anisotropy problem of sentence embeddings[1], comparing a fine-tuned model with PromptEOL+ICL might seem unfair. Nonetheless, PromptEOL+ICL still achieves comparable performance. When considering baselines without fine-tuning, LLMs can benefit from in-context learning with PromptEOL+ICL, showing strong advantages over other methods. We also discuss the benefits of in-context learning in the second point of **W5**.\n  * We provide PromptEOL+CSE for the fine-tuning setting, which outperforms small models by more than 3 points on the Spearman correlation, even with limited computational resources. For instance, we can efficiently fine-tune a 2.7B OPT with PromptEOL+CSE for approximately one hour on four 3090 GPUs.\n\nThank you again for your time and effort. Please let us know if you have further questions.\n\n**Reference**\n\n[1] Gao T, Yao X, Chen D. SimCSE: Simple Contrastive Learning of Sentence Embeddings (EMNLP 2021)"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission596/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700597499606,
                "cdate": 1700597499606,
                "tmdate": 1700645441767,
                "mdate": 1700645441767,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "AKcdyF51L1",
                "forum": "V0CUOBWUHa",
                "replyto": "RSKudrzs3F",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission596/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission596/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Waiting for further discussion"
                    },
                    "comment": {
                        "value": "Dear reviewer ViaW,\n\nThanks very much for your responses. As the discussion phase is going to end today, could you review our recent responses adequately addressed your concerns? \n\n We sincerely thank you for your positive feedback.\n\nBest regards\n\nAuthors of 596"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission596/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700710061103,
                "cdate": 1700710061103,
                "tmdate": 1700710061103,
                "mdate": 1700710061103,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nobT0uP4Y0",
                "forum": "V0CUOBWUHa",
                "replyto": "RSKudrzs3F",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission596/Reviewer_ViaW"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission596/Reviewer_ViaW"
                ],
                "content": {
                    "comment": {
                        "value": "Your prompt is ```This sentence: \u201cxi\u201d means in one word:```, which intends to compress the whole semantics into a single word. The hidden state vectors represent the intention of your prompt.  I do not know why you are now claiming you are not compressing it.   Ideally, this prompt will compress any input sentences into a class in the output vocab, which might be too aggressive.   \n\nSUBJ is a simple task for identifying subjectivities of user's movie review.  The performance on SUBJ can not directly and fully show the embedding quality.  \n\nFor scale,  I mean your 66B model give much lower performance than your 1.3B model in Table 1 for the PromptEOL\nOPT setting. Similarly, for the PromptEOL+ICL+OPT setting,  the 66B model does not show clear advantage of the 13B model. \n\nI have clearly expressed my score will be 7. And this is my final score.  Thanks."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission596/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700722914346,
                "cdate": 1700722914346,
                "tmdate": 1700722925801,
                "mdate": 1700722925801,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ZQGewhNaTX",
            "forum": "V0CUOBWUHa",
            "replyto": "V0CUOBWUHa",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission596/Reviewer_oEkP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission596/Reviewer_oEkP"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a set of methods leveraging LLMs for sentence embeddings.\n\n* It introduces a prompting strategy with explicit one word limitation, pushing the model to condense as much information as possible into the last hidden representation. This method is an adaptation of PromptBERT's approach for autoregression models.\n* It leverages in-context-learning in order to improve the quality of sentence embeddings. To this end, it relies on two approaches: 1) It generates one-word summaries of sentences from the STS training set using GPT-3.5. 2) It leverages entries from the Oxford dictionary. The concatenation of samples from these two sources is then incorporated into the LLM's prompt.\n* It leverages fine-tuning with contrastive learning to further improve the quality of sentence embeddings. It does so by leveraging qLORA and training on supervised datasets such as SNLI and MNLI.\n\n\nThe paper's conclusions are as follow:\n* The explicit one-word limitation prompt improves the quality of sentence embeddings derived from OPT on STS benchmarks.\n* In-context learning and supervised fine-tuning improve the performance on STS benchmarks, allowing the proposed solution to beat the state of the art. However, the resulting embeddings do not transfer as well to other tasks.\n* In the paper's proposed setup, the largest base models do not have a clear performance advantage: best results on STS without fine-tuning are obtained with OPT's variants ranging between 1.3 and 6.7B parameters."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* Originality: the proposed PromptEOL is a novel adaptation of the BERT prompting paradigm for sentence representations. The prompting strategy combining GPT-augmented STS sentences and oxford definitions is novel as well, and the use of qLORA to make contrastive fine-tuning feasible for larger models shows creativity in putting together existing solutions.\n\n* Quality: The experiments are well-devised and executed. The proposed methods are simple and beat the state of the art on semantic textual similarity benchmarks.\n\n* Clarity: The paper articulates very clearly its methodology. It is easy to read and describes well the corresponding pre-existing work. It motivates very well the choice of an explicit one-word prompt, the value of in-context learning and the need for quantization in order to fine-tune the largest models in a contrastive learning setup.\n\n* Significance: while the results on STS benchmarks look good, both in-context learning and contrastive fine-tuning do not show incremental value on transfer tasks. The relatively low generalisation capabilities of these methods limit greatly the appeal of such techniques for the average practitioner, as most real-life applications of sentence embeddings are not for semantic textual similarity."
                },
                "weaknesses": {
                    "value": "* While it is helpful to the reader to see the entire distribution of Spearman correlations, it may be relevant to give more details on how the two sources for ICL data impact the quality of downstream representations. The 1/3-2/3 mix of STS sentences vs Oxford definitions would benefit from an explicit ablation.\n\n* The value of ICL and CSE is demonstrated only for OPT. Indeed, table 1 is missing results that would demonstrate the added value of ICL and CSE on Llama.\n\n* The proposed methods (in-context learning and possibly fine-tuning) are performing worse than simple explicit-one-word-limit prompting on transfer tasks.\n\n* It is not clear what section 5.2 demonstrates:\n  * first, the text mentions \"in-context learning examples that were obtained from each model on the STS-B development set\", while the table caption reads \"In-context learning examples used in various model size\". The paper states clearly in section 3.2 that the in-context learning examples (1) come from the STS-B training set and (2) are not generated / obtained from the model itself.\n  * second, the method used to sample the data from table 4 is not described, and the meaning of the \"Improve\" column is not clear: does it correspond to the improvement coming from one additional sentence in the prompt? Or from the addition of the 100s of sentences in the ICL prompt? In any case, it seems premature to draw generic conclusions such as \"related examples are usually more implicit\" from a sample size of 1 from each model. Appending 5-10 random samples of each category in the appendix would give more compelling evidence for this.\n  * finally, it is not clear how to relate the findings of section 5.2 to the overall quality of the sentence embeddings introduced by this work.\n\n* Typos and errors:\n  * table 6, the first group of data rows should mention (16-bit)\n  * table 8, the ordering is not the same between the \"Without fine-tuning\" and \"Fine-tuning on unsupervised datasets\" groups of rows."
                },
                "questions": {
                    "value": "1. It would seem that some experiments have been run only on OPT, while others have been run on OPT and Llama. It would be helpful to have all experiments run on both models to show that the conclusions are robust to the choice of base LLM.\n\n2. How was the data mix for ICL (1/3 STS sentences, 2/3 Oxford definitions) devised? Are they both necessary to achieve good performance? A proper ablation of this setup would be helpful."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission596/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission596/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission596/Reviewer_oEkP"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission596/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698664430369,
            "cdate": 1698664430369,
            "tmdate": 1699635987040,
            "mdate": 1699635987040,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "SLtxfcQQge",
                "forum": "V0CUOBWUHa",
                "replyto": "ZQGewhNaTX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission596/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission596/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer oEkP (Part 1/2)"
                    },
                    "comment": {
                        "value": "We sincerely thank you for your helpful feedback and insightful comments. We address your comments and questions below.\n\n> **W1:** While it is helpful to the reader to see the entire distribution of Spearman correlations, it may be relevant to give more details on how the two sources for ICL data impact the quality of downstream representations. The 1/3-2/3 mix of STS sentences vs Oxford definitions would benefit from an explicit ablation.\n\nWe would like to clarify that in our method, only one example from the ICL data is used to perform sentence embeddings. To select this example, we evaluate the performance of each example on the STS-B development set and choose the one that performs the best. This process is described in detail in Section 3.2 of our paper. The proportion of STS-B data and the Oxford dictionary in our method is not a crucial factor; we use a 1/3-2/3 mix simply because obtaining data from the Oxford dictionary is much easier compared to generating words from ChatGPT.\n\n> **W2:** The value of ICL and CSE is demonstrated only for OPT. Indeed, table 1 is missing results that would demonstrate the added value of ICL and CSE on Llama.\n\nIn this work, we focus on scaling up sentence embeddings with various model sizes. Compared to OPT, LLaMA only has four model sizes. We use OPT as the main model to show the results. Due to the page limit, we only present the ICL results on OPT. However, we have included the CSE on LLaMA in Table 2.\n\nTo address the concern about the robustness of our method, we evaluated the performance of PromptEOL and PromptEOL+ICL on current popular LLMs, varying in size from 3B to 70B. The evaluated LLMs include LLaMA, LLaMA-2, open-LLaMA, MPT, and Mistral.\nOur method shows significant improvement across above 13 models. PromptEOL achieves an average Spearman correlation score of 70.27 on STS tasks among the above models, while PromptEOL+ICL achieves an average score of 77.14. Comparing to the average pooling baseline, PromptEOL and PromptEOL+ICL achieve average improvement of 22.27 and 29.14, respectively. Furthermore, when compared to the previous prompt representation method, PromptEOL and PromptEOL+ICL exhibit an average improvement of 15.55 and 22.42. We have included detailed results in Appendix F of our paper.\n\n> **W3:** The proposed methods (in-context learning and possibly fine-tuning) are performing worse than simple explicit-one-word-limit prompting on transfer tasks.\n\nBenefiting from our PromptEOL method, LLMs can capture the semantics of sentences well for transfer tasks. In Figure 4(c), we demonstrate that PromptEOL significantly outperforms other representation methods like prompt and average pooling on transfer tasks, achieving state-of-the-art results without fine-tuning.\nFor in-context learning, we can flexibility modify the prompt to generate embeddings that fit the task following [1]. For example, by using a sentiment classification example, we can improve the results of PromptEOL 6.7B OPT from 91.34 to 91.78 average accuracy on seven transfer tasks.\nFor fine-tuning, due to limited computational resources, we only use efficient fine-tuning LLMs with QLoRA, which may harm the original transfer task performance through 4-bit quantization.\n\n**References:**\n\n[1] Su H, Kasai J, Wang Y, et al. One embedder, any task: Instruction-finetuned text embeddings (ACL findings 2023)"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission596/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700116481098,
                "cdate": 1700116481098,
                "tmdate": 1700116481098,
                "mdate": 1700116481098,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7fUuKRq9z7",
                "forum": "V0CUOBWUHa",
                "replyto": "ZQGewhNaTX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission596/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission596/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Does our response address your concerns?"
                    },
                    "comment": {
                        "value": "Dear reviewer oEkP,\n\nThanks very much for your time and effort. As the discussion phase is going to end today, could you review our responses to ensure they have addressed your concerns? We would also appreciate any additional feedback you might have for improvements.\n\nBest regards,\n\nAuthors of 596"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission596/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700709751957,
                "cdate": 1700709751957,
                "tmdate": 1700709751957,
                "mdate": 1700709751957,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "iEC7dgyYjo",
            "forum": "V0CUOBWUHa",
            "replyto": "V0CUOBWUHa",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission596/Reviewer_dT5b"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission596/Reviewer_dT5b"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates how to better leverage large language models (LLMs) for generating sentence representations, traditionally obtained from smaller encoder-based models like BERT variants. \nIt introduces two approaches. \nInitially, it adopts in-context learning, similar to the utilization of LLMs in other tasks. \nEmploying the \"Explicit One word Limitation (EOL)\"\u2014which posits that decoder-based models can produce viable sentence-level representations when prompted to summarize a sentence in a single word\u2014sentence-to-word pair contexts are used to enhance representation derivation. \nDecoder models, without any fine-tuning, showed performance on par with existing contrastive learning approaches. \nAdditionally, the authors explored fine-tuning decoder models using the prevalent contrastive learning framework in sentence representation research, employing the parameter-efficient technique known as QLoRA. \nThe findings reveal that fine-tuning with contrastive learning notably benefits larger decoder models, surpassing smaller encoder models in both Semantic Textual Similarity (STS) benchmarks and transfer tasks for classification."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Suggested a variety of plausible methods for utilizing Large Language Models (LLMs) to compute sentence representations.\n- Explored both in-context learning and fine-tuning approaches with LLMs, encompassing a broad spectrum of potential applications for these models.\n- Introduced a straightforward yet insightful technique for integrating in-context learning into the sentence representation learning paradigm."
                },
                "weaknesses": {
                    "value": "- While the methods proposed are sound, they consist of previously suggested and widely implemented techniques, which diminishes the novelty aspect of the work.\n- Contrary to SimCSE, the in-context learning approach depends on the use of the STS-B dataset, including its training and validation components, which could potentially confer an unfair advantage to the method.\n- There appears to be no direct link between the two proposed methods; that is, the approach based on in-context learning and the one utilizing contrastive learning."
                },
                "questions": {
                    "value": "- I'm curious whether the authors have any insights or hypotheses as to why (much) larger models (over 10B) do not excel as expected in computing sentence representations, which contrasts with their effectiveness in other standard applications."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission596/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698739476461,
            "cdate": 1698739476461,
            "tmdate": 1699635986956,
            "mdate": 1699635986956,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0USsLQ9H9a",
                "forum": "V0CUOBWUHa",
                "replyto": "iEC7dgyYjo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission596/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission596/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer dT5b"
                    },
                    "comment": {
                        "value": "We sincerely thank you for your helpful feedback and insightful comments. We address your comments and questions below.\n\n> **W1:** While the methods proposed are sound, they consist of previously suggested and widely implemented techniques, which diminishes the novelty aspect of the work.\n\nThe primary focus of our research is the scaling up of sentence embeddings using LLMs, which remains unexplored. We also proposed three methods to leverage the capacity of LLMs for sentence embeddings: PromptEOL, PromptEOL+ICL, and PromptEOL+CSE.\n\nWhile our work is inspired by earlier studies such as PromptBERT, we have identified that PromptBERT does not extend its applicability to LLMs for sentence embeddings. To address this shortcoming, we have proposed a solution named PromptEOL. This method introduces an implicit single-word limitation to the prompt, thereby facilitating the generation of sentence embeddings. It is necessary to mention that, as the output of sentence embeddings results in a singular vector, in-context learning cannot be applied directly. To overcome this challenge, we propose PromptEOL+ICL. To the best of our knowledge, we are the first to show that in-context learning can be applied to sentence embeddings without fine-tuning.\n\n> **W2:** Contrary to SimCSE, the in-context learning approach depends on the use of the STS-B dataset, including its training and validation components, which could potentially confer an unfair advantage to the method.\n\nWe think the comparison with SimCSE and other similar methods is fair. First, SimCSE also uses STS-B validation set for selecting the best checkpoint during training. Second, we do not use any label information from the STS-B training set. Instead, we only utilize 100 sentences from the STS-B training set, ensuring that none of these sentences come from the same sentence pairs. In fact, our approach outperforms SimCSE even when solely leveraging the Oxford dictionary. Finally, our ICL method does not rely on fine-tuning or contrastive learning, which are considered essential for achieving better representation in current sentence embeddings methods[1,2].\n\n> **W3:** There appears to be no direct link between the two proposed methods; that is, the approach based on in-context learning and the one utilizing contrastive learning.\n\nBoth in-context learning and contrastive learning methods are built upon our proposed representation method, namely PromptEOL. This method focuses on two settings in sentence embeddings: with and without fine-tuning. In the case of in-context learning, we demonstrate that combining PromptEOL with in-context learning yields high-quality sentence representations without the need for fine-tuning. Additionally, in the context of contrastive learning, we provide evidence in Figure 4(b) that PromptEOL can benefit from contrastive learning, outperforming other representation methods.\n\n> **Q1:** I'm curious whether the authors have any insights or hypotheses as to why (much) larger models (over 10B) do not excel as expected in computing sentence representations, which contrasts with their effectiveness in other standard applications.\n\nWe have also observed that the scaling law performs well in transfer learning tasks, as shown in Table 3, but does not yield the same success in STS tasks, as shown in Table 1. We think that the main reason for this discrepancy may be related to the issue of anisotropy in sentence representation [1,3], which harms the performance on STS tasks. To validate this, we measure the anisotropy of sentence embeddings across different model sizes following [4].\n\n|                    | sentence anisotropy |\n| ---------- | :--------: |\n| PromptEOL OPT-1.3b |               0.565 |\n| PromptEOL OPT-2.7b |               0.589 |\n| PromptEOL OPT-6.7b |               0.694 |\n| PromptEOL OPT-13b  |               0.719 |\n| PromptEOL OPT-30b  |               0.751 |\n| PromptEOL OPT-66b  |               0.715 |\n\nThrough our analysis, we have observed that the anisotropy of sentence embeddings increases as the model size increases. While larger models may possess greater overall capacity, the issue of anisotropy may limit their performance.\n\n\n**References:**\n\n[1] Gao T, Yao X, Chen D. SimCSE: Simple Contrastive Learning of Sentence Embeddings (EMNLP 2021)\n\n[2] Yan Y, Li R, Wang S, et al. ConSERT: A Contrastive Framework for Self-Supervised Sentence Representation Transfer (ACL 2021)\n\n[3] Ethayarajh K. How Contextual are Contextualized Word Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings (EMNLP 2019)\n\n[4] Jiang T, Jiao J, Huang S, et al. PromptBERT: Improving BERT Sentence Embeddings with Prompts (EMNLP 2022)"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission596/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700115532190,
                "cdate": 1700115532190,
                "tmdate": 1700116097659,
                "mdate": 1700116097659,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "R9O6JRNps4",
                "forum": "V0CUOBWUHa",
                "replyto": "iEC7dgyYjo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission596/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission596/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Does our response address your concerns?"
                    },
                    "comment": {
                        "value": "Dear reviewer dT5b,\n\nThanks very much for your time and effort. As the discussion phase is going to end today, could you review our responses to ensure they have addressed your concerns? We would also appreciate any additional feedback you might have for improvements.\n\nBest regards,\n\nAuthors of 596"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission596/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700709708327,
                "cdate": 1700709708327,
                "tmdate": 1700709708327,
                "mdate": 1700709708327,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]