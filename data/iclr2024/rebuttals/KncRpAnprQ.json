[
    {
        "title": "A Novel Approach For Adversarial Robustness"
    },
    {
        "review": {
            "id": "qnsiunOcXM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission798/Reviewer_88kq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission798/Reviewer_88kq"
            ],
            "forum": "KncRpAnprQ",
            "replyto": "KncRpAnprQ",
            "content": {
                "summary": {
                    "value": "In this paper, the authors propose a new plug-in module, which can help the model against adversarial attacks. After using such a model, only standard training can fulfill the robust requirement."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The authors propose a random module to defend against various adversarial attacks.\n\n2. The proposed module is lightweight without introducing too many parameters.\n\n3. The authors consider different model structures cooperating with the proposed method.\n\n4. There are EOT experiment results, which is important for a randomization-based defense."
                },
                "weaknesses": {
                    "value": "1. Stochastic Neural Networks (SNNs) have been proposed for many years, which are not a novel approach to defend against adversarial attacks. Therefore, I think the authors overclaim their contribution.\n\n2. The authors do not mention any related works under the topic of SNNs or other stochastic methods. I am not sure whether the authors are on purpose or not. But the Related Works should mainly discuss the most related papers.\n\n3. The authors do not compare any SNN baselines, which causes unfairness in the experiments. For example, I can simply find a paper [1] from Google Scholar, which discusses SNN in adversarial defense. In experiments, they compared various stochastic methods, which I cannot find in this paper. This unfair comparison causes a false contribution.\n\n\n[1] Yang, H., Wang, M., Yu, Z., & Zhou, Y. (2022). Rethinking feature uncertainty in stochastic neural networks for adversarial robustness. arXiv preprint arXiv:2201.00148."
                },
                "questions": {
                    "value": "Please see weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission798/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697181976948,
            "cdate": 1697181976948,
            "tmdate": 1699636007117,
            "mdate": 1699636007117,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "uzyVIuEnUi",
                "forum": "KncRpAnprQ",
                "replyto": "qnsiunOcXM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission798/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission798/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for pointing out some SNN works. Actually, we only look for and compare with the top-ranking methods in the robustbench leaderboard https://robustbench.github.io/, where there are no SNNs. It appears that SNN is far behind the mainstream performance of adversarial defense. Of course, we will add more related work of SNN.\n\nOur approaches are very different from all SNN works, as we essentially do the random feature squeezing. \n\nJust for a quick reference, here we make some comparisons with [1,2], where, unfortunately, there is no AA evaluation and moreover no experiment with ImageNet.\n\nSo we only compare with square for cifar10, In table 5 of [1], the square at 16/255 is 55.2 and 48.8 in table 4 of [2], far below ours  78.31.\n\n[1] Yang, H., Wang, M., Yu, Z., & Zhou, Y. (2022). Rethinking feature uncertainty in stochastic neural networks for adversarial robustness. arXiv preprint arXiv:2201.00148.\n\n[2] Weight-covariance alignment for adversarially robust neural networks Panagiotis Eustratiadis and Henry Gouk and Da Li and Timothy M. Hospedales ICML 2021"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission798/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700015192710,
                "cdate": 1700015192710,
                "tmdate": 1700015192710,
                "mdate": 1700015192710,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "xJzbuOaGk0",
            "forum": "KncRpAnprQ",
            "replyto": "KncRpAnprQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission798/Reviewer_S7uL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission798/Reviewer_S7uL"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses the challenge of adversarial attacks on deep learning models. The authors propose an approach that enhances network robustness without knowledge of the attack strategy. They introduce an input layer that reduces the impact of malicious perturbations, achieving robustness against various attack types through standard training with clean images on datasets like CIFAR-10 and ImageNet."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "- The authors focus on an important topic, namely adversarial training to robustify deep neural networks. \n- The idea of an approach that requires no prior knowledge of attacks is interesting. \n- The approach is straightforward. \n- The authors evaluate their approach on ImageNet."
                },
                "weaknesses": {
                    "value": "- Trivial technical contribution: The manipulation of early or other intermediate features has been proposed various times in the literature. Most of these defenses have also been defeated. \n- The authors do not provide any ablation studies to justify their design choices. \n- The title is chosen too general and is vague. \n- The writing is poor: \n\t- Some parts of the paper are incomprehensible. \n\t- The introduction is written like a related work section\n\t- The contributions are not clearly explained and distinguished to previous works\n\t- A proper explanation of the proposed approach  \n- I can't grasp what Figure 2 is supposed to show. \n- While the authors evaluated with AutoAttack I am wondering how this method performs against the PGD-attack. \n- The authors might encounter obfuscated gradients here. Hence the authors should follow the guidelines in [1] and [2] and evaluate with BPDA. \n- Did the authors try if their approach also works for vision transformer models? \n\n[1] Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples; ICML 2018   \n[2] On Evaluating Adversarial Robustness; ArXiv 2019"
                },
                "questions": {
                    "value": "Please address the points in my weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission798/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697620181878,
            "cdate": 1697620181878,
            "tmdate": 1699636007049,
            "mdate": 1699636007049,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5pHZxzCsXd",
                "forum": "KncRpAnprQ",
                "replyto": "xJzbuOaGk0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission798/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission798/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Trivial technical contribution\n\nWe don\u2019t agree. Ours is based on random feature squeezing, which is novel and different from all other approaches. \n\nablation studies:\n\nEssentially, our specially designed input layer is coupled with loss functions. In other words, they should be treated as a whole. So ablation studies are not applicable here.\n\nThe title is chosen too general and is vague.\n\nTrue. At this moment, we are considering changing it to Adversarial Robustness through Random Feature Squeezing \n\nThe writing is poor:\n\nWe will correct it. Thanks.\n\nFigure 2:\n\nWe are trying to show how each channel of input images is processed in our specially designed input layer. \n\nWhile the authors evaluated with AutoAttack I am wondering how this method performs against the PGD-attack.\n\nAutoAttack includes the APGD-ce and APGD -dlr, which are presumably more powerful than PGD.\n\nThe authors might encounter obfuscated gradients here. Hence the authors should follow the guidelines in [1] and [2] and evaluate with BPDA.\n\nEvery unit in our specially designed input layer is differentiable, so BPDA is not applicable. We do evaluate EOT.\n\nDid the authors try if their approach also works for vision transformer models?\n\nWe have not tried."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission798/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700015185912,
                "cdate": 1700015185912,
                "tmdate": 1700015185912,
                "mdate": 1700015185912,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "S2dBh50vYJ",
            "forum": "KncRpAnprQ",
            "replyto": "KncRpAnprQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission798/Reviewer_CbH6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission798/Reviewer_CbH6"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a input preprocessing method for defending against adversarial examples. The method works by _squeezing_ the input into the range $[0, 1]$ by first performing a simple (linear + thresholding) transformation followed by a random shift and a random scaling. Finally a sigmoid activation is used to squeeze the input to $[0, 1]$. With this simple preprocessing function, this paper reports stunning performances against strong adversarial attacks."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "This paper proposes a preprocessor that obtains very strong performance on adversarial examples while not training on adversarial examples at all. This is a very surprising result if it is able to withstand thorough empirical evaluation."
                },
                "weaknesses": {
                    "value": "1.\tGradient Masking and Suppression: The paper has many well-known signs of gradient masking, which is a phenomenon where estimating gradients of a classifier might be error-prone. In this paper, there are two components that might be causing this: (1) the random shift and scaling, (2) sigmoid squeezing. Further, Table 1 shows that the performance of the proposed method barely decreases under the strong auto attack ($\\ell_infty, \\ell_1$) as well as square attacks. This typically indicates some issues with the underlying evaluation [1].\u2028\n\n2.\tEvaluation on Black Box Attacks: Interestingly, the paper does provide an evaluation on the black box Square attack, which is considered to be a strong black box attack. However, the evaluation is subject to concern, as the performance barely dips below the benign performance after attack. A simple test to check any problems with the evaluation would be to intentionally inject adversarial examples and check if the performance is still retained [2].\u2028\n\n3.\tCode for the method and evaluations: Since the authors report stunning performance increases, in light of the above concerns, it would be easier to believe the claims if well documented code would be provided for each of the evaluations.\u2028\n\n4.\tWriting: The writing is loose and informal in some parts of the paper.\n\t1.\tP3: Step 1: What is mean, std?\n\t2.\t\u201c$\\delta$ is a uniform one\u201d -> \u201c$\\delta \\sim {\\rm Unif}([0, 1])$\", etc.\n\t3.\tSec 4.3: It seems that the adversarial examples are generated for one realization of the network, and tested on another. This is not standard practice. \n\t4.\tP5: EOT \u2014 it would be useful to mention the exact parameters over EOT is performed, and how those parameters were chosen. \n\t5.\tFigure 2: Please show the RGB in the first column, all rows \u2014 it is hard to understand what is going on by looking at R,G,B channels separately. Even then, what are we supposed to take away from this figure?\n\n[1]: On Adaptive Attacks to Adversarial Example Defenses, Florian Tramer,\u00a0Nicholas Carlini,\u00a0Wieland Brendel,\u00a0Aleksander Madry\n\n[2]: Increasing confidence in adversarial robustness evaluations. Roland S Zimmermann, Wieland Brendel, Florian Tramer, Nicholas Carlini."
                },
                "questions": {
                    "value": "In addition to the concerns raised above, \n\n1.\tWhat is the robustness vs accuracy curve? When does it dip below the benign performance, for each of the attacks tested? At what perturbation does it go to zero? At this perturbation, how does a human perform?\u2028\n\n2.\tWhat is the role of each of the components of the preprocessor towards the final robustness, in that what happens when each of them are replaced by an identity transformation? (1) Sigmoid, (2) Random scaling, (3) Random Shift"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission798/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission798/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission798/Reviewer_CbH6"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission798/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698813942623,
            "cdate": 1698813942623,
            "tmdate": 1699636006978,
            "mdate": 1699636006978,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "MAlRSuAhiH",
                "forum": "KncRpAnprQ",
                "replyto": "S2dBh50vYJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission798/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission798/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "P3: Step 1: What is mean, std?\n\nMean and std are the mean and standard deviation of the training set, the parameters needed for input normalization. We just want to make everything clear, so delve into such detail which is usually ignored by many other papers.\n\n\u201c\u03b4 is a uniform one\u201d -> \u201c\u03b4\u223cUnif([0,1])\", etc\n\nThanks!\n\nSec 4.3: It seems that the adversarial examples are generated for one realization of the network, and tested on another. This is not standard practice.\n\nAs our approach is random, the same sample could be classified with different logits when executed multiple times. Actually, for any input clean sample, a very simple approach to defeat ours is just feeding this perhaps thousands of times, and it is highly possible one of them will fool our net. Of course, one cannot conclude that ours is useless at all. So there is nothing wrong with the last move. In fact,  other works, for example[1], also adopt this under different contexts.\n\n[1] Fighting Gradients with Gradients: Dynamic Defenses against Adversarial Attacks Dequan Wang and An Ju and Evan Shelhamer and David A. Wagner and Trevor Darrell\n\nP5: EOT \u2014 it would be useful to mention the exact parameters over EOT is performed, and how those parameters were chosen.\n\nEOT is provided by the AutoAttack toolbox, with default iterations of 20.\n\nFigure 2: Please show the RGB in the first column, all rows \u2014 it is hard to understand what is going on by looking at R,G,B channels separately. Even then, what are we supposed to take away from this figure?\n\nWe want to show how each channel of input images is processed in our specially designed input layer. The first row is the input R, G, and B, the second row shows the sampling matrix with very small values due to the loss function, and the third is the output. \n\nYes, we have tried to show the RGB in the first column, all rows when preparing the paper. But it is not very meaningful, since actually they are not the usual RGB channels of color images. But we will take into further consideration this.\n\nWhat is the robustness vs accuracy curve? When does it dip below the benign performance, for each of the attacks tested? At what perturbation does it go to zero? At this perturbation, how does a human perform?\u2028\n\nThis is an interesting question. We will take it as our future work. In fact, EOT appears to be \nthe only effective attack although at a high computation cost.\n\nWhat is the role of each of the components of the preprocessor towards the final robustness, in that what happens when each of them are replaced by an identity transformation? (1) Sigmoid, (2) Random scaling, (3) Random Shift\n\nWhen any component is missing, the method can not work. Our specially designed input layer is also coupled with loss functions.  The sampling matrix should be small such that the response of Sigmoid will be mostly on the saturated region, namely \u201crandom feature squeezing\u201d. In other words, this is the key to the robustness."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission798/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700015174507,
                "cdate": 1700015174507,
                "tmdate": 1700015174507,
                "mdate": 1700015174507,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aFdhYLHPk4",
                "forum": "KncRpAnprQ",
                "replyto": "MAlRSuAhiH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission798/Reviewer_CbH6"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission798/Reviewer_CbH6"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the response. In light of the deficiencies in evaluation and writing, I have decided to keep my score."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission798/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700566122849,
                "cdate": 1700566122849,
                "tmdate": 1700566122849,
                "mdate": 1700566122849,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "g53kmPZSz5",
            "forum": "KncRpAnprQ",
            "replyto": "KncRpAnprQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission798/Reviewer_S5kz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission798/Reviewer_S5kz"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a specialized input layer to improve the adversarial robustness of deep neural networks. Each pixel in the normalized images goes through a perturbation and multiplication process, and then are fed into a Sigmoid function before proceeding with the rest of the network. Evaluations on CIFAR10 and Imagenet-1k in the white-box setting demonstrate that the resulting networks are robust to various $\\ell_p$ bounded perturbations generated using AutoAttack."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The proposed input layer design introduces virtually no computation overhead while improving the adversarial robustness of the network in the while-box setting, against gradient-based attacks. Empirical evaluations are performed on CIFAR10 as well as larger dataset such as Imagenet. Visualizations presented in Figure 2 are helpful in understanding the effect of the proposed input layer."
                },
                "weaknesses": {
                    "value": "Figure 2 shows that the output of proposed input layers are mostly 0's and 1's, which are towards the saturation range in the sigmoid function. This means that the robustness improvement comes mostly from obfuscated gradients [1]. In other words, the network is having trouble finding effective adversarial perturbations, rather than being truly more adversarial robust compared to the baselines. \n\n[1] Athalye et al, Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples ICML 2018"
                },
                "questions": {
                    "value": "A simple test to verify the obfuscating gradient behaviour is to measure whether the perturbation, found based on the attack methods in the paper, indeed reaches the specified radius of the $\\ell_p$ ball."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission798/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission798/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission798/Reviewer_S5kz"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission798/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698814544932,
            "cdate": 1698814544932,
            "tmdate": 1699636006903,
            "mdate": 1699636006903,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]