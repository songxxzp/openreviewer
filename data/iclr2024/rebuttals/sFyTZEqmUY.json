[
    {
        "title": "Learning Interactive Real-World Simulators"
    },
    {
        "review": {
            "id": "neyFV0WvE8",
            "forum": "sFyTZEqmUY",
            "replyto": "sFyTZEqmUY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2698/Reviewer_ihUZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2698/Reviewer_ihUZ"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a universal simulator (UniSim) that aims to simulate how humans and agents interact with the world. The proposed framework combines various types of datasets, including internet text-image pairs and robotics data, with the motivation that existing datasets are useful along different axes. The paper uses a video diffusion model as an interactive simulator of the world. UniSim can simulate both high-level instructions and low-level control, which show zero-shot transferability to real-world scenarios, addressing the sim-to-real transferability problem. The authors highlight the potential for UniSim to be used in broader applications, such as video captioning and rare event detection."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "- This is an interesting paper that presents some exciting results.\n- The paper is well organized and well-written."
                },
                "weaknesses": {
                    "value": "- It would be nice if the paper delved more into the limitations of the models. The paper has shown that exciting results can be obtained, but it's useful for the community to know the limits of the generalization capabilities, especially if people want to use this in the future for various applications. \n- For reproducibility, it would be helpful if the authors could release the code and some example pre-trained checkpoints."
                },
                "questions": {
                    "value": "See weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2698/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698732580272,
            "cdate": 1698732580272,
            "tmdate": 1699636211375,
            "mdate": 1699636211375,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Ug8hbrLLuT",
                "forum": "sFyTZEqmUY",
                "replyto": "neyFV0WvE8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2698/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2698/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response"
                    },
                    "comment": {
                        "value": "Thank you for the positive feedback! We have expanded the discussion on the limitations of this work and included the following in the paper (conclusion section):\n\n- Hallucination. When an action is unrealistic given the scene (e.g., \u201cwash hands\u201d is given to a tabletop robot), we observe hallucinations (e.g., the table turns into a sink or the view turning away from the tabletop robot and a sink shows up). Ideally, we want UniSim to detect actions that are not possible to simulate as opposed to Hallucinate unrealistic outcomes.\n- Limited memory. UniSim conditioned on a few frames of the recent history cannot capture long-term memory (e.g., an apple in a drawer could disappear when the drawer is opened if putting the apple in the drawer is not a part of the history for conditioning). How much history to condition on depends on the application of UniSim (e.g., whether UniSim will be used for policy learning in a near-Markov setting or question answering that requires long-term memory).\n- Limited out-of-domain generalization. This is especially true for domains that are not represented in the training data. For instance, UniSim is mostly trained on 4 robot morphologies, and its ability to generalize to an unseen robot is limited. Further scaling up training data could help, as UniSim\u2019s training data is nowhere near all the video data available on the internet.\n- Visual simulation only. UniSim is not suitable for environments where actions do not cause visual observation change (e.g., different forces in grasping a static cup). A true universal simulator should capture all aspects of the world beyond visual experience (e.g., sound, sensory, etc)\nWe will work on releasing a code for doing inference with UniSim. We will try to open-source the UniSim model, but since the model is pretrained on private videos, we foresee challenges in this process."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2698/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700501240270,
                "cdate": 1700501240270,
                "tmdate": 1700501240270,
                "mdate": 1700501240270,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "DBZbFbd302",
            "forum": "sFyTZEqmUY",
            "replyto": "sFyTZEqmUY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2698/Reviewer_b2Jg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2698/Reviewer_b2Jg"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents UniSim, a video prediction and generative model aiming for serving as a universal simulator of diverse scenarios conditioned on input language-described actions. It devotes a big effort in combining dataset with different modalities and information axes, trained a unified generative model, and shows the trained model can be used for downstream policy learning."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "- Very cool and impressive research direction and proposed method\n- Huge effort devoted in unifying multiple large scale datasets\n- Experiments demonstrated effectiveness for downstream policy learning"
                },
                "weaknesses": {
                    "value": "I think the paper presents a very important step towards learning a universal video predictive world model. One of my questions is, the shown demo looks like generally still in distribution, in terms of generalization across different embodiment: the generated video contaiing robot are very similar to robotic dataset, and in more complex scenes training using human videos the model seems only handling human hands. How does it work in those complex scenes when the model is commanded to predict outcomes given a robot action input?\nAlso, when it comes to low level control input, the paper seems only handling delta motion in the cartesion space. Does it handle more general end-effector action in SE3 space? (joint space seems out of reach for this family of method since it's not observable) Is it true that for predicting outcomes conditioned on robot action, the robot arm needs to be visible in the first place?"
                },
                "questions": {
                    "value": "See above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2698/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2698/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2698/Reviewer_b2Jg"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2698/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698782698771,
            "cdate": 1698782698771,
            "tmdate": 1699636211287,
            "mdate": 1699636211287,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Tx5jYciPAB",
                "forum": "sFyTZEqmUY",
                "replyto": "DBZbFbd302",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2698/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2698/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response"
                    },
                    "comment": {
                        "value": "Thank you for noting the significance of this work! We provide answers to your questions below.\n\n> Out-of-domain generalization for robotics.\n\nWe note that because UniSim is only trained on 4 types of robots in fixed settings (e.g., in front of a cabinet, a tabletop with colored blocks), the out-of-domain generalization of UniSim is limited. We observe both success and failures (https://drive.google.com/file/d/1tpInX4KUywXLt971Tr15_RsW8I_ytpvu/view?usp=sharing) applying UniSim to out-of-domain settings. We believe that training UniSim on more diverse robotic datasets (e.g., RT-X [1]) can potentially mitigate this limitation. We made note of this limitation in the conclusion section of the paper.\n\n> More general end-effector action in SE3 space.\n\nWe finetuned the UniSim model on the RT-X dataset [1] using low-level control actions (e.g., 7 DoF end-effector). We provide examples (https://drive.google.com/file/d/1AiMFfCEPIL2GXIYXu2qDThvtXzV2I4-o/view?usp=sharing) where UniSim can visually simulate the end-effector actions reasonably well. We note that these low-level control action conditioned models require more rigorous evaluation such as being used to train policies. We also hypothesize that UniSim can potentially be effective as long as there is an action interface shared by robots from which abundant data can be combined. Texts, cartesian coordinates, and end-effectors are all examples of such shared action spaces.\n\n> Conditioned on robot action, the robot arm needs to be visible in the first place?\n\nThe robot arm does not need to be visible to simulate, for instance, the ego-centric view from a robot's wrist view, which is quite similar to ego-centric videos with camera settings converted to actions. Here is an example (https://drive.google.com/file/d/1EBqTpONGMyF9ZZCHA9kEAxnR_Yu9ewGY/view?usp=sharing) simulation of a robot\u2019s wrist view while stacking blocks.\n\n[1] Open X-Embodiment: Robotic Learning Datasets and RT-X Models."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2698/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700501191670,
                "cdate": 1700501191670,
                "tmdate": 1700501191670,
                "mdate": 1700501191670,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "IgXEAYwQ15",
                "forum": "sFyTZEqmUY",
                "replyto": "Tx5jYciPAB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2698/Reviewer_b2Jg"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2698/Reviewer_b2Jg"
                ],
                "content": {
                    "title": {
                        "value": "Reviewer Response"
                    },
                    "comment": {
                        "value": "Thank you for your reply! I will keep my positive score."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2698/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700690335827,
                "cdate": 1700690335827,
                "tmdate": 1700690335827,
                "mdate": 1700690335827,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "2qaG3iXNkU",
            "forum": "sFyTZEqmUY",
            "replyto": "sFyTZEqmUY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2698/Reviewer_kkjZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2698/Reviewer_kkjZ"
            ],
            "content": {
                "summary": {
                    "value": "- The paper presents a video diffusion model that does conditional next (few) frame prediction. It conditions on previous frames and either a text description of the video, or more granular actions (robotic movements, or camera angles). The focus is on its use in robotics contexts.\n- The novelty is in the mix of data trained on. Rather than focusing on a single environment or even single action space, the model (UniSim) is trained jointly on 14 common datasets, from the text-image LAION dataset (often used for image generation), to the Something-somethingV2 video dataset (often used for video classification). Significant compute is used (512x TPUs)\n- A limited ablation is conducted on how previous observations should be conditioned upon.\n- Three use cases are explored:\n1) A separate vision-language model is first trained to predict language instruction and actions, given a start and end observation, on a robotics control task in the Language Table environment. It is then finetuned using simulated trajectories from data synthetically generated by UniSim (longer to those in the original dataset).\n2) A separate policy model is first trained via BC on the Language Table environment, then finetuned with RL using simulated trajectories from data synthetically generated by UniSim (itself trained on Language Table data).\n3) A separate video-to-caption model is found to benefit when finetuned on data synthetically generated by UniSim, for producing captions on ActivityNet.\n\n___\nFollowing the rebuttal, I upgrade my ratings as follows: soundness 2$\\to$3, overall rating 5$\\to$8, and confidence 4$\\to$5. The main remaining weakness is that the mixture of datasets and modalities (a key contribution of the work) appears to be of limited benefit on the tasks assessed by the paper. But there are enough positives in the paper for me to downweight this issue."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper will undoubtedly draw a lot of attention and excitement from researchers working in several areas, including RL, robotics, large models, and diffusion models.\n- It represents a major effort in training a cross-domain model, with emphasis on its use for robotic control.\n- I welcome this kind of larger scale-up work being submitted to an academic conference, since a recent trend has seen similar works restricted to industry-lab preprints.\n- The first few pages motivating the work are quite inspiring.\n- Effort has been made to explore a range of use cases.\n- Overall it represents a very promising direction towards foundational models for control."
                },
                "weaknesses": {
                    "value": "I expect that this paper will comfortably clear the bar for acceptance. However, there are two main issues I believe should first be addressed. I've set my score relatively low because of these, but anticipate increasing it following a revised version.\n\n1) Whilst it's difficult to accuse the paper of overclaiming in any specific place, the writing and framing risk feeling a little showy. The title is very general, and applies to any paper on model-based RL for the real-world rather than something specific to this paper, and naming the method a \"universal simulator\" feels grandiose. (Happy to collect other revewiers' opinions on this.) The connection between POMDP's and action-conditioned video generation is more-or-less assumed by any world model paper (e.g. [1]), and shouldn't be highlighted as a main contribution of the paper.\n2) One of the recurring claims throughout the paper is that the major novelty is UniSim's \"orchestration of diverse datasets, each providing a different aspect of the overall experience\" into a single model. Yet no hard evidence is given for this combination being important -- aside from two vague figures in Appendix E. At a minimum, it would be important to train a version of UniSim on say, datasets from the Language Table environment _only_, and report numbers for when synthetic data was generated from this, in Table 2 and 3. This would help support the claim that dataset diversity is valuable.\n\nOther issues (in decreasing priority)\n- I think it'd be useful to investigate how entwined the effect of actions is with the dataset distribution. For example, could camera commands (zoom in etc) successfully be applied to kitchen scenes as in Figure 3? The fact that the name of the dataset had to be included as part of the action during training, makes me suspect actions may not be able to generalise well to new kinds of video. This would not be a dealbreaker for the paper's acceptance, but is important to show readers how general this data mixing is.\n- A lack of strong baselines might be expected for this kind of scale-up work. But in their absence, ablations become more important, to verify that the various components of the model were all necessary. The paper only presents a brief study of which frames to condition on.\n- The model section is poorly written. The use of $\\mathcal{T}$ is (I think) slightly misleading -- usually the transition fn of a POMDP is defined as operating on the states, $\\mathcal{T}(s_t,a_t) \\to s_{t+1}$, and there is a separate emission function producing the observations, $p(o_t|s_t)$. Eq. 1 implicitly combines these -- I might recommend renaming it $f$ or $g$ or whatever. I didn't follow why $o_l$ notation needed to be introduced, since it's immediately unrolled into $[o_t, o_{t+1}]$ and never referred to again. I also didn't understand why the model conditions on the noised, rather than clean, previous observations. It's said the last four frames are concatenated from $o_{t-1}$, which confused me -- does $o_{t-1}$ represent four frames, or should it read $o_{t-1:t-4}$ or similar?\n- It's a shame to give the model details only in the Appendix C, as I believe many readers would be interested in them. I hope some of these can be shifted to the main body, particularly key details around the diffusion architecture (such as the core and super-resolution modules) and the amount of compute required.\n- Any algorithmic or model novelty is light (more or less straightforward video diffusion).\n- The two main experiments were conducted on environments that were within the training distribution of UniSim. It would have been more impressive to investigate the performance on new environments.\n- The wordy description of all datasets in 2.1, I felt was much better summarized by Table 5 in the Appendix (perhaps with the addition of a column explaining how an action space is defined and handled), and might be swapped. (Optional!)\n\nMinor issues/questions\n- Appendix says 1M steps on 512 TPUs with batchsize 256 -- this seemed a low ratio of training updates to available compute. Did performance saturate beyond this?\n- What was the wall clock time of the model training?\n- How many parameters were in the model?\n- Will the model weights be open-sourced?\n\n[1] Transformers are Sample-Efficient World Models"
                },
                "questions": {
                    "value": "See weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2698/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2698/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2698/Reviewer_kkjZ"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2698/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698947816457,
            "cdate": 1698947816457,
            "tmdate": 1700754985809,
            "mdate": 1700754985809,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "UZaHBBQCd0",
                "forum": "sFyTZEqmUY",
                "replyto": "2qaG3iXNkU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2698/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2698/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response"
                    },
                    "comment": {
                        "value": "Thank you for the detailed and constructive feedback! We answer your questions below and in the updated paper. Please let us know if you have additional questions.\n\n> 1.1 Novelty in relation to prior model-based RL work.\n\nWhile UniSim shares similarities with model based RL, model-based RL generally focuses on single-task settings and often do not use rollouts in the pixel space for policy optimization. UniSim focuses on learning a general purpose simulator that can be used for a variety of purposes such as serving as environments for manipulation and navigation, supporting human-AI interaction, generating data for VLMs, etc. More importantly, existing model-based methods have yet to achieve good real-world simulation results in the pixel space, so it is hard to call them \u201cInteractive Real-World Simulators\u201d as in our title.\n\n> 1.2 Concerns around method name.\n\nWe were aware that our naming choice of 'universal simulator' could lead to concerns about overclaiming, similar to some other naming choices such as 'generalist agent.' However, we wanted to take a forward-looking stance when we chose this name, as UniSim demonstrates considerable potential in using internet data to simulate the universe, indicating that such an ambitious goal is achievable. We are open to renaming the method if reviewers generally agree that a less ambitious name is more favorable.\n\n> 1.3 Novelty in establishing the connection between video generation and POMDPs.\n\nThank you for noting that Micheli et al. have previously established this connection. We have reformulated UniSim as an observation prediction model that approximates sampling in a POMDP. This formulation allows us to use multi-frame history conditioning to produce consistent long-horizon videos. We have also updated the second contribution bullet in the introduction.\n\n> 2. Effect of diverse datasets.\n\nThank you for pointing out the lack of dataset analysis in the current version of the paper. We conducted ablation studies on the effect of diverse datasets, and included the following table in Appendix E.1:\n| Dataset | FVD | CLIP |\n| ----------- | ----------- | ----------- |\n| Internet only | 219.62 | 22.27 |\n| UniSim without internet | 307.80 | 21.99 |\n| UniSim   |   **211.30**     | **22.63** |\n\nWe found combining internet data with various human activity and robot data results in the best FVD and CLIP scores on a held-out test split, whereas only using internet data (last row in Table 5) or not including internet data (all except for last row in Table 5) leads to worse performance. We further provide more qualitative examples (https://drive.google.com/file/d/174qUvW8fUxvKBVv3UV4m2yZ8y3v6gMW6/view?usp=sharing) where training on entire UniSim data is better than using internet-only data or without internet data. In terms of using downstream RL and planning to evaluate the effect of diverse data, we trained a video model only on Language Table as suggested by the reviewer, but we did not observe significant performance differences between UniSim and this task-specific video model in Table 2 and Table 3. Our hypothesis is that Language Table is a simple domain and already has a lot of data, so training a domain-specific video model is sufficient, but this is not true in general for harder environments.\n\n> 3.1 Entwined actions and videos.\n\nWe found that actions are generally not entwined with videos in the training distribution; we can apply diverse actions to scenes as long as such actions are reasonable. As the reviewer requested, we generate various egocentric movements in the kitchen scene (https://drive.google.com/file/d/1HhvJIOlA2TAotZiRrt0CHVkgjc0kEQH2/view?usp=sharing) from Figure 3, including look up, look down, zoom in, zoom out, etc. We see that the generated videos generally follow the actions, but the videos might look less realistic when the actions are less reasonable (e.g., zooming out from a person\u2019s view looking down at the kitchen table). We also generated some diverse interactions with objects (https://drive.google.com/file/d/1MWzWOHpeC9etJsW6gZXHm2q2E8DKyR24/view?usp=sharing).\n\n> 3.2 Dataset name as a part of action label.\n\nNote that we only used dataset name for dataset with very few videos (habitat), as it helped the model identify the domain better. We think this may not be needed if we tune the data mixture ratio during pretraining. For all the other datasets, we don\u2019t use dataset name, so that language actions from different datasets can generalize across domains."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2698/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700501101051,
                "cdate": 1700501101051,
                "tmdate": 1700501101051,
                "mdate": 1700501101051,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jO4eTjsW2u",
                "forum": "sFyTZEqmUY",
                "replyto": "2qaG3iXNkU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2698/Reviewer_kkjZ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2698/Reviewer_kkjZ"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal response"
                    },
                    "comment": {
                        "value": "I appreciate the hardwork put into the rebuttal by the authors, both to my own and other reviewers' feedback, which has been managed excellently. I believe the paper is signficantly improved and will be increasing my score. \n\nI share several reactions to the rebuttal.\n- 1.1/1.2 -- Fair points. I will follow up with other reviewers in discussion about naming.\n- 1.3 -- Great. And POMDP section seems much improved.\n- 2 -- Thanks for adding the additional experiments, though they seem light on details for now (assume these will be added in a later version). E.g. what does internet only data comprise? what were the metrics computed over? \n- 2 -- It's disappointing that the Language Table-only model performed as well as full UniSim (did I read that correctly?). This is a critical point since the dataset mix is a main contribution of the paper. I take the authors' point that more benefit might be expected in harder tasks, but the onus is on the paper to demonstrate this concretely. Nevertheless there is enough creativity and positives in the paper that I couldn't recommend rejecting it on these grounds. I would however push for these results to be honestly reported in the main paper.\n- 3.1/3.2 -- Brilliant, appreciate the videos. Looks to generalize better than I'd expected.\n- 4 -- These seem good to me. I think it's a little harsh on the model to say the scaling behavior is disappointing -- the improvements are still significant for the 5.6B model, and there is no reason to expect these metrics would improve in the smooth power-law patterns seen in cross-entropy loss.\n- Remaining points look good."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2698/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700665519647,
                "cdate": 1700665519647,
                "tmdate": 1700665519647,
                "mdate": 1700665519647,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ihrdy34PtF",
            "forum": "sFyTZEqmUY",
            "replyto": "sFyTZEqmUY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2698/Reviewer_ZLpe"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2698/Reviewer_ZLpe"
            ],
            "content": {
                "summary": {
                    "value": "In this work, the authors propose to learn a universal simulator (UniSim) of real-world interaction through generative modeling (a diffusion model for outputting the next frame given the previous frame and the input actions). They achieve so by careful orchestration of diverse datasets, which are rich along completely different axes (e.g., some videos have object-level diversity, some have densely labeled language instructions, and some have scene-level diversity). They show applications of the proposed simulator such as training long-horizon embodied planners and low-level object manipulators."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ Reasonably scalable approach to collect training data for the proposed simulator \n+ The use of diffusion models to fuse different aspects of the diverse datasets with decent results is impressive\n+ Particularly the sim-to-real transfer is a promising direction for using the proposed real-world simulator."
                },
                "weaknesses": {
                    "value": "While this work shows great promise in a range of downstream applications. I believe it might need more experimental evidence to support the claim that it can simulate low-level actions well. Specifically, section 4.2 only shows results for a relatively simple object (mostly blocks) re-arrangement (without grasping, e.g.) on a table. What about grasping objects, pulling objects (e.g., opening a drawer), etc? It will give us insights as to how fine-grained the controls are supported by the proposed simulator, even if it cannot simulate low-level actions perfectly."
                },
                "questions": {
                    "value": "See \u201cweaknesses\u201d"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2698/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2698/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2698/Reviewer_ZLpe"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2698/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698999340626,
            "cdate": 1698999340626,
            "tmdate": 1699636211128,
            "mdate": 1699636211128,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "UbErnT7kfL",
                "forum": "sFyTZEqmUY",
                "replyto": "ihrdy34PtF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2698/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2698/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response"
                    },
                    "comment": {
                        "value": "Thank you for the feedback. We finetuned the UniSim model on the RT-X dataset [1] using low-level control actions (e.g., 7 DoF end-effector). We provide examples (https://drive.google.com/file/d/1AiMFfCEPIL2GXIYXu2qDThvtXzV2I4-o/view?usp=sharing) where UniSim can visually simulate the end-effector actions reasonably well. We note that these low-level control action conditioned models require more rigorous evaluation such as being used to train policies. We also hypothesize that UniSim can potentially be effective as long as there is an action interface shared by robots from which abundant data can be combined. Texts, cartesian coordinates, and end-effectors are all examples of such shared action spaces.\n\n[1] Open X-Embodiment: Robotic Learning Datasets and RT-X Models"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2698/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700500926426,
                "cdate": 1700500926426,
                "tmdate": 1700500926426,
                "mdate": 1700500926426,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]