[
    {
        "title": "A Joint Spectro-Temporal Relational Thinking Based Acoustic Modeling Framework"
    },
    {
        "review": {
            "id": "zh2uFJgSAL",
            "forum": "ufvwhR3XmN",
            "replyto": "ufvwhR3XmN",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7013/Reviewer_jL9N"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7013/Reviewer_jL9N"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates injection of relational thinking on the frame-level phoneme recegnition task on TIMIT. The main proposal of the paper is that instead of focusing on time-only or frequency-only relations between consecutive frames, they should be jointly modeled. The acoustic model uses wav2vec2 features from audio and concatenates them with the features extracted from the relational thinking based graph embeddings before applying a classification layer. The model parameters are trained using a variational lower bound based approach. Experimental results show that joint time-frequency relations are important and the proposed method can outperform the wav2vec2 based baseline in phone recognition task on TIMIT. Analysis of the results show that the model is more effective on vowels as compared to the consonants."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Originality:\n\n1. The joint modeling of time-frequency seems to be effective on the phoneme recognition task. Additional analyses on the learned graphs show that the model can learn the vowel patterns more consistently. \n\n2. It is nice to see the parallelism between human perception of vowels and the model\u2019s results. \n\n- Quality:\n1. The paper has shown the model's effectiveness on the TIMIT task. The paper investigated various aspects of the model and design choices (even though they are sometimes limited). \n\n- Clarity:\nClearly written\n\n- Significance: \n1. Even though the acoustic and graph embedding combination is performed in a rather straightforward way, the formulation of the learning objective can provide an opportunity for further extensions of the graph parameters."
                },
                "weaknesses": {
                    "value": "1. More parameter settings and comparisons could have been investigated to strengthen the conclusions from the results. \n2. Some additional analysis of the results could have been useful. \n\nPlease refer to the Questions below for details."
                },
                "questions": {
                    "value": "1. Are the baselines trained with cross-entropy objective?\n\n2. Would it make sense to Impose left to right constraint between the time steps for causality?\n\n3. PER Analysis at the speaker level may give further intuition on how the model performs as compared to human perceptions.\n\n4. It would be good to see Fig.5 repeated with t1f8 and t8f1 models.\n\n5. Does feature mapping (Eq. 1) involve mixing of features within frequency bins? Is $  \\Lambda $ diagonal or not? \n\n6. Have you considered other types of spectra-temporal features for comparison? One example could be from, https://engineering.jhu.edu/lcap/data/uploads/pdfs/interspeech2012_carlin.pdf\n\n7. Have you considered a comparison between spectro-temporal HMM based recognition and the proposed approach?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7013/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698608051623,
            "cdate": 1698608051623,
            "tmdate": 1699636822101,
            "mdate": 1699636822101,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "mhIEYURM6U",
                "forum": "ufvwhR3XmN",
                "replyto": "zh2uFJgSAL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7013/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7013/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We value the reviewer's insightful queries and the valuable feedback provided. Below are our responses addressing the questions and concerns raised by the reviewer.\n\n> 1. Are the baselines trained with cross-entropy objective?\n\n**AR:** No, the baselines used in our experiments were not trained with the cross-entropy objective. Instead, when we fine-tuned the baselines, we followed Baevski et al. (2020) and optimized the standard connectionist temporal classification (CTC) loss function (Graves et al., 2006). CTC is a commonly employed method for end-to-end sequence modeling tasks in the field of speech processing.\n\n\n> 2. Would it make sense to Impose left to right constraint between the time steps for causality?\n\n**AR:** We agree that it is essential to consider causality when modeling sequences. In our proposed relational thinking modeling, we have already taken causality into account. As stated at the beginning of Section 3.1, for each time step $t$, we constructed the feature map $\\\\mathbf{C}\\_t$ by incorporating the acoustic feature vectors from the current time step and the previous $w - 1$ time steps as\n$$\n\\\\mathbf{C}\\_t = [\\\\mathbf{c}\\_{t - w + 1}, \\\\ldots, \\\\mathbf{c}\\_t]. \\\\quad \\\\quad \\\\quad \\text{(e)}\n$$\nThis formulation (e) explicitly excluded acoustic feature vectors from future time steps. Essentially, this aligned with the concept of a left-to-right constraint by considering only past and current information and not incorporating information from future time steps.\n\n\n> 3. PER Analysis at the speaker level may give further intuition on how the model performs as compared to human perceptions.\n\n**AR:** We appreciate the reviewer's suggestion regarding PER analysis at the speaker level. It is indeed a valuable perspective for understanding how the model's performance compares to human perceptions. In this particular study, we adhered to the established protocol of reporting PER over the entire split to maintain consistency with previous research in phoneme recognition, ensuring a fair comparison. However, we acknowledge the significance of speaker-level analysis and plan to explore this approach in our future work.\n\n\n> 4. It would be good to see Fig.5 repeated with t1f8 and t8f1 models.\n\n**AR:** As indicated in Table 1, the two joint spectro-temporal models, w20-t4f2 and w20-t2f4, demonstrated superior performance compared to the purely temporal and spectral models, w20-t8f1 and w20-t1f8. This suggested that the spectro-temporal models had a better grasp of the inherent relational information involved in speech. Therefore, we believed it would be more beneficial to conduct further analyses on these joint spectro-temporal models rather than the purely temporal or spectral model to gain more accurate and valuable insights.\n\n\n> 5. Does feature mapping (Eq. 1) involve mixing of features within frequency bins? Is $\\\\mathbf{\\\\Lambda}$ diagonal or not?\n\n**AR:** As mentioned in Section 3.1, in our proposed framework, we utilized a temporal convolution to implement the filtering operator $\\\\Xi$ in (1). Therefore, it is possible that this operation may involve linear combinations of features within frequency bins. However, we can also choose to implement $\\\\Xi$ with various other methods (e.g., pooling), which may not mix features within frequency bins.\n\n$\\\\mathbf{\\\\Lambda}\\_{t, d^{(f)}, d^{(t)}} \\\\in \\\\mathbb{R}^{D\\_s \\\\times \\\\check{w}\\_s}, \\forall d^{(f)}, d^{(t)}$ are sub feature maps of $\\\\check{\\\\mathbf{C}}\\_t$, which consist of the spectro and temporal domain feature information, and they are not necessarily diagonal.\n\n\n> 6. Have you considered other types of spectra-temporal features for comparison? One example could be from, https://engineering.jhu.edu/lcap/data/uploads/pdfs/interspeech2012\\_carlin.pdf\n\n**AR:**  We thank the reviewer for directing us to the reference. Given that our paper primarily aimed to explore relational thinking modeling in recognition tasks, we anticipate that such modeling should be agnostic to acoustic feature types. Consequently, we utilized the state-of-the-art acoustic features extracted by wav2vec2. However, we acknowledge and appreciate the significance of considering various types of acoustic features, which can greatly influence system performance. To this end, we have performed additional experiments using MFCCs as an alternate acoustic feature type (as detailed in **A5: Generalization to Other Acoustic Features** in Section 5.1). As highlighted by the reviewer's reference, it is clear that their proposed spectro-temporal features exhibited similar performance to MFCCs in clean speech scenarios, which aligned with our study. These experiments with MFCCs yielded consistent results and further validated the generalizability of our proposed framework."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7013/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700451701753,
                "cdate": 1700451701753,
                "tmdate": 1700451701753,
                "mdate": 1700451701753,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ylLx0FAuf6",
                "forum": "ufvwhR3XmN",
                "replyto": "8DWSlm1y6f",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7013/Reviewer_jL9N"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7013/Reviewer_jL9N"
                ],
                "content": {
                    "title": {
                        "value": "Read the Response, No Update to the Score"
                    },
                    "comment": {
                        "value": "I would like to thank the authors for their response. I would like to keep my score as is."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7013/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700608765316,
                "cdate": 1700608765316,
                "tmdate": 1700608765316,
                "mdate": 1700608765316,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "DM9ZqoZJEU",
            "forum": "ufvwhR3XmN",
            "replyto": "ufvwhR3XmN",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7013/Reviewer_KrqG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7013/Reviewer_KrqG"
            ],
            "content": {
                "summary": {
                    "value": "This work proposed to use relational thinking-based acoustic model to learn the spectro-temporal correlation for automtic speech recognition task. Specficially, the proposed method is applied on the speech features extracted by a pre-trained wav2vec module. In the experiment, two tasks are performed including phenome recognition and automatic speech recognition. The results show the performance gain compared to these baseline systems.  These baseline systems are mostly the pre-training based methods which output the speech features."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper attempts to solve a valuable problem for acoustic modeling. The motivation of the work is clear and reasonable."
                },
                "weaknesses": {
                    "value": "1, The innovation is not clear. The paper claims their innovation of using relational thinking based modeling method on spectro-temporal domain for acoustic modeling. However, from the description in the paper, there is no distinction between relational thinking based modeling and self-attention based modeling. For example, with several self-attention modeling layers stacked, it's equivalent to the so called relational thinking based model that pair-wised relation will be learnt among the transformed forms of each node (each time step), rather than the single step embedding. Therefore, theoretically, there is no difference between self-attention and relational thinking based method. \n2, The experiment part is not complete. In order to demonstrate the superiority of the proposed relational thinking based method compared to the self-attention based method, the results of the self-attention should be included as one of the baseline results. However, the results of the paper only includes these feature extraction based method without extra modeling. In addition, the paper should also list the model size of each compared method to have a more fair comparision.  \n3, The tradeoff study between temporal context and spectral context is not able to lead such conclusion that higher frequency domain resolution provideds more benefits compared higher time domain resolution, as the results of these two setting are very close in the test set (20.80 vs. 20.66)."
                },
                "questions": {
                    "value": "1, Have you  done such experiment that replace the relational thinking based model with Transfomer/Conformer type of module? If so, what is the performance?\n2, Could you please explain why the proposed method cannot perform well in non-vower recognition?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7013/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7013/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7013/Reviewer_KrqG"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7013/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698648497549,
            "cdate": 1698648497549,
            "tmdate": 1699636821993,
            "mdate": 1699636821993,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "vbFlYLYGVB",
                "forum": "ufvwhR3XmN",
                "replyto": "DM9ZqoZJEU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7013/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7013/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We are grateful for the reviewer\u2019s considerate questions and valuable feedback. Please find our responses to the questions and concerns raised by the reviewer below.\n\n> 1. The innovation is not clear. The paper claims their innovation of using relational thinking based modeling method on spectro-temporal domain for acoustic modeling. However, from the description in the paper, there is no distinction between relational thinking based modeling and self-attention based modeling. For example, with several self-attention modeling layers stacked, it's equivalent to the so called relational thinking based model that pair-wised relation will be learnt among the transformed forms of each node (each time step), rather than the single step embedding. Therefore, theoretically, there is no difference between self-attention and relational thinking based method.\n\n**AR:** Relational thinking and self-attention are two distinct mechanisms designed to capture different types of information. To illustrate the distinction, let us consider a simplified 2\\-layer self-attention network w.l.o.g., where each layer $l$ comprises two nodes $\\\\mathbf{h}\\_1^{(l)}$ and $\\\\mathbf{h}\\_2^{(l)}$. According to (13), the calculation of nodes in the subsequent layer $l + 1$ is as follows:\n$$\n\\\\left [ \\\\mathbf{h}\\_1^{(l + 1)}, \\\\mathbf{h}\\_2^{(l + 1)} \\\\right ] = \\\\mathbf{W}\\_v^{(l)} \\\\left [ \\\\mathbf{h}\\_1^{(l)}, \\\\mathbf{h}\\_2^{(l)} \\\\right ] \\\\left [ \n\\\\begin{array}{cc}\n\\\\alpha\\_{1, 1}^{(l)} & \\\\alpha\\_{2, 1}^{(l)} \\\\\\\\\n\\\\alpha\\_{1, 2}^{(l)} & \\\\alpha\\_{2, 2}^{(l)} \n\\\\end{array}\n\\\\right ]. \\\\quad \\\\quad \\\\quad \\\\text{(b)}\n$$\nAs a result, the state of a node $\\\\mathbf{h}\\_2^{(3)}$ after undergoing two layers of self-attention calculations is \n$$\n\\\\begin{align}\n\\\\mathbf{h}\\_2^{(3)} & = \\\\alpha\\_{2, 1}^{(2)} \\\\mathbf{W}\\_v^{(2)} \\\\left ( \\\\alpha\\_{1, 1}^{(1)} \\\\mathbf{W}\\_v^{(1)} \\\\mathbf{h}\\_1^{(1)} + \\\\alpha\\_{1, 2}^{(1)} \\\\mathbf{W}\\_v^{(1)} \\\\mathbf{h}\\_2^{(1)} \\\\right ) + \\\\alpha\\_{2, 2}^{(2)} \\\\mathbf{W}\\_v^{(2)} \\\\left ( \\\\alpha\\_{2, 1}^{(1)} \\\\mathbf{W}\\_v^{(1)} \\\\mathbf{h}\\_1^{(1)} + \\\\alpha\\_{2, 2}^{(1)} \\\\mathbf{W}\\_v^{(1)} \\\\mathbf{h}\\_2^{(1)} \\right ) \\\\quad \\\\quad \\\\quad \\\\text{(c-1)} \\\\\\\\\n& = \\\\left ( \\\\alpha\\_{1, 1}^{(1)} \\\\alpha\\_{2, 1}^{(2)} + \\\\alpha\\_{2, 1}^{(1)} \\\\alpha\\_{2, 2}^{(2)} \\\\right ) \\\\mathbf{W}\\_v^{(2)} \\\\mathbf{W}\\_v^{(1)} \\\\mathbf{h}\\_1^{(1)} + \\\\left ( \\\\alpha\\_{1, 2}^{(1)} \\\\alpha\\_{2, 1}^{(2)} + \\\\alpha\\_{2, 2}^{(1)} \\\\alpha\\_{2, 2}^{(2)} \\\\right ) \\\\mathbf{W}\\_v^{(2)} \\\\mathbf{W}\\_v^{(1)} \\\\mathbf{h}\\_2^{(1)}. \\\\quad \\\\quad \\\\quad \\\\text{(c-2)}\n\\end{align}\n$$\nEven though (c\\-1) may exhibit a similar form to (12) (i.e., the graph embedding obtained by relational thinking, also as shown below),\n$$\n\\\\mathbf{r} = \\\\sum\\_{(i, j) \\\\in \\\\{ (i, j) | i < j, (i, j) \\\\in \\\\bar{\\\\mathcal{E}} \\\\}} \\\\bar{\\\\alpha}\\_{i, j} \\\\bar{f}\\_{\\\\theta}(\\\\mathbf{h}\\_{i}, \\mathbf{h}\\_{j}), \\\\quad \\\\quad \\\\quad \\\\text{(d)}\n$$\nparticularly when we view $\\\\mathbf{W}\\_v^{(2)} \\\\left ( \\\\alpha\\_{1, 1}^{(1)} \\\\mathbf{W}\\_v^{(1)} \\\\mathbf{h}\\_1^{(1)} + \\\\alpha\\_{1, 2}^{(1)} \\\\mathbf{W}\\_v^{(1)} \\\\mathbf{h}\\_2^{(1)} \\\\right )$ and $\\\\mathbf{W}\\_v^{(2)} \\\\left ( \\\\alpha\\_{2, 1}^{(1)} \\\\mathbf{W}\\_v^{(1)} \\\\mathbf{h}\\_1^{(1)} + \\\\alpha\\_{2, 2}^{(1)} \\\\mathbf{W}\\_v^{(1)} \\\\mathbf{h}\\_2^{(1)} \\\\right )$ as node pair embedding functions from a linear family $\\\\mathcal{F} \\\\left ( \\\\mathbf{h}\\_1^{(1)}, \\\\mathbf{h}\\_2^{(1)} \\\\right )$, it is crucial to note that $\\\\mathbf{h}\\_2^{(3)}$ is fundamentally still a weighted sum of node embeddings (as revealed by (c-2)) rather than a weighted sum of node pair embeddings as obtained by relational thinking (d), where $\\\\bar{f}\\_{\\\\theta}(\\\\mathbf{h}\\_{i}, \\\\mathbf{h}\\_{j})$ is an arbitrary node pair embedding function. Therefore, the stacked self-attention mechanism and relational thinking are not entirely equivalent, and models cannot solely rely on the self-attention mechanism to effectively assess the importance of a pair of nodes, as discussed in Appendix D.\n\nFurthermore, it is also important to highlight that our proposed relational thinking modeling is a variational approach, whereas the self-attention mechanism is a deterministic approach. As elucidated in Nan et al. (2023), deterministic models can produce incorrect predictions when confronted with unseen data, particularly due to the discontinuous and sparse nature of the latent space. In contrast, variational models address this challenge by mapping the input data to collections of distributions in the latent space rather than singular points. By capturing uncertainty in the latent space, variational modeling enhances model generalization, rendering the model more adaptable to diverse and unseen data. Also, we have further clarified our novelty; please refer to our responses to Reviewer Q5NM for more details."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7013/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700450735987,
                "cdate": 1700450735987,
                "tmdate": 1700450735987,
                "mdate": 1700450735987,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "T2Rejc73f5",
            "forum": "ufvwhR3XmN",
            "replyto": "ufvwhR3XmN",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7013/Reviewer_Q5NM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7013/Reviewer_Q5NM"
            ],
            "content": {
                "summary": {
                    "value": "The authors present a  spectrotemporal relational thinking-based framework for acoustic modeling. The proposed framework improves upon the original relational thinking-based frame by extending the probabilistic graph modeling from the temporal domain to the frequency-temporal domain. The paper reports a 7.82% improvement in phoneme recognition over the state-of-the-art for TIMIT phoneme recognition task."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Biological Inspiration and Acoustic Modeling: The exploration of biologically-inspired algorithms, such as relational thinking, in acoustic modeling is noteworthy. Given humans' inherent ability to process audio signals across both frequency and temporal domains, the extension of the original relational thinking network to a temporal-frequency domain seems  reasonable.\n\n- Promising Results on TIMIT: Experimental results on the TIMIT dataset, though small, show promise against various baselines. Additionally, the detailed analysis and visualization of the generated graph and its relationship with different phoneme categories provide valuable insights."
                },
                "weaknesses": {
                    "value": "- Incremental Technical Contribution: The technical developments in this work appear to be an incremental advancement from Huang et al. (2020). The main modification is the extension of the input from one dimension to two dimensions, followed by a direct application of the relational thinking network proposed by Huang et al.\n\n- Dataset Limitations: The experiments rely heavily on the TIMIT dataset, which is relatively small in size. To firmly establish the proposed method's efficacy and robustness, it is imperative to test it on larger, more diverse datasets and under complex conditions."
                },
                "questions": {
                    "value": "See weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7013/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698734823027,
            "cdate": 1698734823027,
            "tmdate": 1699636821810,
            "mdate": 1699636821810,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "tPJr0Bctth",
                "forum": "ufvwhR3XmN",
                "replyto": "T2Rejc73f5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7013/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7013/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> Incremental Technical Contribution: The technical developments in this work appear to be an incremental advancement from Huang et al. (2020). The main modification is the extension of the input from one dimension to two dimensions, followed by a direct application of the relational thinking network proposed by Huang et al.\n\n**AR:** In addition to extending the input from one dimension to two dimensions, our work differs from that of Huang et al. (2020) in three perspectives:\n\na) Our focus differs from Huang et al. (2020) as we examined a distinct application, one that readily extends to scenarios where the input and output sequences are of different lengths. This is a common scenario in sequence modeling tasks like speech recognition.  To accommodate these real-world applications, we developed a novel loss function that can handle such cases. This stands in contrast to the loss function proposed by Huang et al. (2020), which is applicable only to cases where input and output sequences have equal lengths. This paper addressed the challenges that arise when dealing with input and output sequences of different lengths, a common scenario in sequence modeling tasks like speech recognition. To accommodate real-world applications, we developed a novel loss function that can handle such cases. This stands in contrast to the loss function proposed by Huang et al. (2020), which is applicable only to cases where input and output sequences have equal lengths. \n\nb) Our paper included a comprehensive series of analyses toward the recognition results and the learned relational information, providing valuable insights into human speech perception and comprehension. We uncovered the patterns involved in the captured relations (i.e., the edges $\\bar{\\alpha}^{(t)}_{i, j}$ of the task-specific graph), which exhibited more similarities for phoneme classes within the same sub-group, while showed significant variations between phoneme classes from different sub-groups. We also conveyed that relational thinking modeling primarily enhanced the system by improving its performance in recognizing vowels. These insights represent a novel and previously unexplored aspect of research.\n\nc) We conducted a theoretical analysis that explained the distinctions between relational thinking and the self-attention mechanism (in Appendix~D). Our analysis underscored that relational thinking captured a new type of (co-occurrence or pair-wise) information that could offer additional advantages beyond what the attention mechanism has achieved.\n\n\n> Dataset Limitations: The experiments rely heavily on the TIMIT dataset, which is relatively small in size. To firmly establish the proposed method's efficacy and robustness, it is imperative to test it on larger, more diverse datasets and under complex conditions.\n\n**AR:** As clarified in Section 4, we used the TIMIT dataset because this study primarily aimed to delve deeper into the concepts of relational thinking with speech understanding and recognition tasks at the fine-grained level, serving as an initial step toward a comprehensive understanding of how this can be applied to speech. Such an analysis is feasible exclusively with the TIMIT dataset, which stands out as one of the very few datasets that provide precise information about the start and end instants of each phoneme. This unique attribute allowed us to conduct more in-depth analyses of the recognition results. For instance, it allowed us to uncover the patterns involved in the captured relations (i.e., the edges $\\bar{\\alpha}^{(t)}_{i, j}$ of the task-specific graph). These patterns exhibited more similarities for phoneme classes within the same sub-group, while they showed significant variations between phoneme classes from different sub-groups (as detailed in Section 5.2 and Appendix G.2). It also facilitated our ability to convey that relational thinking modeling primarily enhanced the system by improving its performance in recognizing vowels (as detailed in Section 5.3 and Appendix G.1). Without the manually annotated labels in the TIMIT dataset, we would not have been able to attain these insights. With the refined knowledge and deep understanding, we would like to extend and corroborate our findings on larger datasets beyond phoneme recognition in the future."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7013/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700448525604,
                "cdate": 1700448525604,
                "tmdate": 1700448525604,
                "mdate": 1700448525604,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OSQWY6Z705",
                "forum": "ufvwhR3XmN",
                "replyto": "tPJr0Bctth",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7013/Reviewer_Q5NM"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7013/Reviewer_Q5NM"
                ],
                "content": {
                    "title": {
                        "value": "Thanks"
                    },
                    "comment": {
                        "value": "Thanks for your efforts.  I agree with the authors that this work introduces a new perspective to applying DGP( et al. (2020)) to two-dimensional settings. However, the technical complexity of tackling such a setting seems straightforward and trivial. Extension to larger datasets is indeed essential and interesting to the processing community. Due to the above concerns, I decided to keep my original ratings."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7013/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700738831652,
                "cdate": 1700738831652,
                "tmdate": 1700738831652,
                "mdate": 1700738831652,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "NHXrMzxT0t",
            "forum": "ufvwhR3XmN",
            "replyto": "ufvwhR3XmN",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7013/Reviewer_4GN3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7013/Reviewer_4GN3"
            ],
            "content": {
                "summary": {
                    "value": "This paper describes an approach to representing smoothed spectrograms using a graph formulation where features are computed from pairwise interactions between spectrogram chunks. These features are then used for phoneme classification in TIMIT, where they show good performance, achieving 9.2% phoneme error rate on the TIMIT test set."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The experiments seem to show that the approach works well."
                },
                "weaknesses": {
                    "value": "This paper was very difficult to read and understand. It uses many words with very suggestive connotations like \"Thinking\" in the title, \"unconscious\", \"mental impressions\", etc. without the necessary strong justification for invoking them in the setting of a machine learning paper. These words obscure what is actually going on in the approach and are not necessary.\n\nThe task of phoneme classification on TIMIT is very old and is a reasonable first step in demonstrating the promise of an approach, but is definitely not sufficient to show that a model is learning a reasonable representation. Additionally, while the proposed system's results are good on the task (9.2% PER on the test set from Table 2), the reported wav2vec 2.0 baseline numbers (9.98% PER on the test set) are not the numbers that are reported in that paper (8.3% PER on the test set). It is not clear where the 9.98% number comes from.\n\nFigure 5 visualizes four relational graphs that show hard to interpret spectrogram pieces without axis labels conected by lines of varying weights. It is not clear which weights we should expect to be strong or weak, although some are strong and some are weak. \n\nThere is also an analysis of the proportion of frames in which each phoneme is predicted in figure 6, showing that the proposed system predicts phonemes with closer proportions to the ground truth than the baseline system of wav2vec 2.0, although it does not show error rates or accuracies for these predictions. It is not clear which phonemes are more accurately predicted, just which ones are more frequently predicted.\n\nThere are 13 pages of appendices and reading through all of it still does not explain all of the necessary details like explicitly stating the loss that is optimized."
                },
                "questions": {
                    "value": "Where do the numbers in table 2 for wav2vec 2.0 come from?\n\nWhat is the loss that is actually optimized and what parameters are adjusted to optimize that loss?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7013/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698805237767,
            "cdate": 1698805237767,
            "tmdate": 1699636821699,
            "mdate": 1699636821699,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "K7iUp0rfOv",
                "forum": "ufvwhR3XmN",
                "replyto": "NHXrMzxT0t",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7013/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7013/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for his insightful questions and feedback. Please see below for our responses to the reviewer's questions and concerns.\n\n> This paper was very difficult to read and understand. It uses many words with very suggestive connotations like \"Thinking\" in the title, \"unconscious\", \"mental impressions\", etc. without the necessary strong justification for invoking them in the setting of a machine learning paper. These words obscure what is actually going on in the approach and are not necessary.\n\n**AR:** The term \"relational thinking\" in neuroscience is commonly used to describe the human ability to handle abstract mental representations of the relationships among objects, attributes, and events (Alexander, 2016). This encompasses concepts such as \"unconscious\" and \"mental impressions\". While this is an established area of study in neuroscience, it is a relatively novel field of inquiry within the machine learning community, particularly concerning speech understanding and recognition tasks (Huang et al., 2020). We recognize the importance of striving for clarity and precision in scientific writing as highlighted by the reviewer. However, it may not be possible to clearly convey the essence of this concept without relying on the terminologies borrowed from neuroscience.\n\n\n> The task of phoneme classification on TIMIT is very old and is a reasonable first step in demonstrating the promise of an approach, but is definitely not sufficient to show that a model is learning a reasonable representation. Additionally, while the proposed system's results are good on the task (9.2% PER on the test set from Table 2), the reported wav2vec 2.0 baseline numbers (9.98% PER on the test set) are not the numbers that are reported in that paper (8.3% PER on the test set). It is not clear where the 9.98% number comes from.\n\n**AR:** In Baevski et al. (2020), the authors explicitly mentioned that two versions of the wav2vec2 models, wav2vec2 BASE and wav2vec2 LARGE, were implemented. The authors reported a PER of 8.3\\% using wav2vec2 LARGE (in Table 3), while we adopted wav2vec2 BASE in our experiments (as highlighted in Appendix F.1.1 of our paper). The rationale behind this choice is that the TIMIT dataset is not particularly large, and the wav2vec2 BASE, which has the same architecture but fewer parameters, is considered sufficient for the task at hand. While Baevski et al. (2020) did not provide the performance of wav2vec2 BASE, we fine-tuned it for our experiments, with the corresponding PER being 9.98\\%.\n\nAs clarified in Section 4, we used the TIMIT dataset because this study primarily aimed to delve deeper into the concepts of relational thinking with speech understanding and recognition tasks at the fine-grained level, serving as an initial step toward a comprehensive understanding of how this can be applied to speech. Such an analysis is feasible exclusively with the TIMIT dataset, which stands out as one of the very few datasets that provide precise information about the start and end instants of each phoneme. This unique attribute allowed us to conduct more in-depth analyses of the recognition results. For instance, it allowed us to uncover the patterns involved in the captured relations (i.e., the edges $\\bar{\\alpha}^{(t)}_{i, j}$ of the task-specific graph). These patterns exhibited more similarities for phoneme classes within the same sub-group, while they showed significant variations between phoneme classes from different sub-groups (as detailed in Section 5.2 and Appendix G.2). It also facilitated our ability to convey that relational thinking modeling primarily enhanced the system by improving its performance in recognizing vowels (as detailed in Section 5.3 and Appendix G.1). Without the manually annotated labels in the TIMIT dataset, we would not have been able to attain these insights. With the refined knowledge and deep understanding, we would like to extend and corroborate our findings on larger datasets beyond phoneme recognition in the future."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7013/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700448868934,
                "cdate": 1700448868934,
                "tmdate": 1700448868934,
                "mdate": 1700448868934,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]