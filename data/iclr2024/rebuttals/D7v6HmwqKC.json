[
    {
        "title": "Can LLMs Effectively Leverage Graph Structural Information: When and Why"
    },
    {
        "review": {
            "id": "AfgUHRG1cg",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8633/Reviewer_vRFw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8633/Reviewer_vRFw"
            ],
            "forum": "D7v6HmwqKC",
            "replyto": "D7v6HmwqKC",
            "content": {
                "summary": {
                    "value": "This paper offers valuable insights into the utilization of graph structural information by large language models (LLMs), focusing on the context of node classification on text attributed graphs. In addressing the \"when\" aspect, the authors investigate the impact of different prompt designs and the richness of textual data. Regarding the \"why\" aspect, the study delves into the effects of label leakage and homogeneity."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Significance: The paper's exploration of the \"when\" and \"why\" behind LLMs' effectiveness in graph-related tasks holds immense significance. Given the increasing prominence of Graph+LLMs in various applications, the insights provided by this work are timely and valuable.\n\n- Quality. The authors dissect the problem into two crucial aspects: \"when\" and \"why.\" They provide a clear and well-reasoned argument, accompanied by rigorous experimental and analytical support, such as the introduction of the new dataset arxiv-2023.\n\n- Clarity. The paper is  well-structured and clear in its presentation"
                },
                "weaknesses": {
                    "value": "- **Overlap with Previous Work.** Several experiments and observations made in the paper have already been addressed in previous or concurrent works, such as [1], [2], and [3]. These prior works have explored topics like the effect of different prompt designs and settings, including zero-shot, few-shot, with k-hop neighbor information, and with title or title and abstract. It is acknowledged that the paper does introduce some unique elements, such as prompt designs with attention analogous to Graph Attention Networks (GAT) and providing more in-depth analysis. However, the experiments appear to have significant overlaps with existing research, leading to a perception of a relatively trivial contribution.\n- **Spurious Analysis.**\n- Q1: \"If data leakage is a major contributor of performance on OGBN-ARXIV, we would expect prompting methods based on LLMs to perform worse than MPNNs on ARXIV-2023.\"\nThis argument regarding data leakage and the expected performance of LLMs compared to MPNNs on a new dataset is not convincingly made. We would expect LLMs to perform worse than MPNNs on ARXIV-2023 because there might be no causality between MPNN doing good/bad and LLM also doing good/bad. \n- Q2: The statement, \"our findings underline the critical role of homophily in influencing LLM\u2019s node classification performance,\" is made, but it may oversimplify the issue.  While homophily's influence is acknowledged, it's essential to delve deeper into why LLMs are affected by homophily. It is suggested that LLMs tend to employ a simple majority vote approach when provided with neighbor information, regardless of the design of prompts, due to not being directly trained on structured (graph) data. \n\nReference:\n\n[1] Harnessing Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed Graph Representation Learning (Table 8) [arxiv link](https://arxiv.org/abs/2305.19523) \n\n[2] Exploring the Potential of Large Language Models (LLMs) in Learning on Graphs (Table 10, Table 16) [arxiv link](https://arxiv.org/abs/2307.03393)\n\n[3] GraphText: Graph Reasoning in Text Space (Table 1) [arxiv link](https://arxiv.org/abs/2310.01089)"
                },
                "questions": {
                    "value": "For Q1 and Q2, see weakness above.\n\nQ3: In the new arXiv-2023 datasets, you mention, \"Specifically, we first sample test nodes from arXiv CS papers published in 2023, and then gather papers within a 2-hop of these test nodes to create a citation network.\" Could you please provide the exact number of nodes that belong to the year 2023 in this dataset?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8633/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697353749430,
            "cdate": 1697353749430,
            "tmdate": 1699637080791,
            "mdate": 1699637080791,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "plqhm3XLN2",
                "forum": "D7v6HmwqKC",
                "replyto": "AfgUHRG1cg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8633/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8633/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer vRFw"
                    },
                    "comment": {
                        "value": "Thank you for your thorough review and insightful feedback on our paper. We are grateful for your recognition of the paper's significance on timely topics on \u201cincreasing prominence of Graph+LLMs in various applications\u201d, quality of \u201crigorous experimental and analytical support\u201d, and clarity. \n\nWe address your concerns in detail below.\n\n\n> \u201cOverlap with Previous Work. Several experiments and observations made in the paper have already been addressed in previous or concurrent works, such as [1], [2], and [3]. These prior works have explored topics like the effect of different prompt designs and settings, including zero-shot, few-shot, with k-hop neighbor information, and with title or title and abstract. It is acknowledged that the paper does introduce some unique elements, such as prompt designs with attention analogous to Graph Attention Networks (GAT) and providing more in-depth analysis. However, the experiments appear to have significant overlaps with existing research, leading to a perception of a relatively trivial contribution.\u201d\n\nWhile we acknowledge these papers, **developing new prompts is not our main focus**. For example, one of the major questions that we want to answer here is **how does the richness of target nodes\u2019 textual features affect LLMs\u2019 benefit from structural information**. And the take-away is that LLMs can benefit more from structural information when textual node features are scarce. In our experiment, we compared two settings: Rich textual context (e.g. paper title and abstract are given) and Scarce textual context (only title is given), to show this (Page 6, Table 2).\n\n**The above question is not overlapping with previous works mentioned.**\n1. The prompts designed in [1] always use title and abstract (Table 7,8), lacking comparison in terms of node-level feature richness.\n2. [2] also regards the textual features of the target node as a whole, without any control over the richness of the features. \n3. Although [3] includes a comparison between \u201cfeature+label\u201d and \u201clabel\u201d in Table. 1, there is no control over the **richness** of the feature. Moreover, [3] is published on arXiv on 10/02/23, which is later than the submission deadline of ICLR 2024 (09/28/23).\n\n> \u201cQ1: \"If data leakage is a major contributor of performance on OGBN-ARXIV, we would expect prompting methods based on LLMs to perform worse than MPNNs on ARXIV-2023.\" This argument regarding data leakage and the expected performance of LLMs compared to MPNNs on a new dataset is not convincingly made. We would expect LLMs to perform worse than MPNNs on ARXIV-2023 because there might be no causality between MPNN doing good/bad and LLM also doing good/bad.\u201d\n\nSorry for the confusion in L 255, we are not assuming there is a strict relationship between MPNNs performance and LLMs performance. Our logic to support the argument that \u201cthere is no substantial evidence indicating that the performance of LLMs is significantly attributed to data leakage\u201d is elaborated in the common response to all authors. \n\n> \u201cQ2: The statement, \"our findings underline the critical role of homophily in influencing LLM\u2019s node classification performance,\" is made, but it may oversimplify the issue. While homophily's influence is acknowledged, it's essential to delve deeper into why LLMs are affected by homophily. It is suggested that LLMs tend to employ a simple majority vote approach when provided with neighbor information, regardless of the design of prompts, due to not being directly trained on structured (graph) data.\u201d\n\nWe acknowledge your suggestion to delve deeper into why homophily affects LLMs' performance in node classification. To address the concern that \u201cLLMs tend to employ a simple majority vote approach when provided with neighbor information\u201d, we develop a new neighbor dropping experiment with three new setting with different prompt for neighbors: i) 1-hop title+label, (ii) 1-hop title and (iii) 1-hop label. \u201c1-hop label\u201d means that we only include the label of the neighboring papers. This special design is to **gauge whether LLM is performing a majority vote with extra neighborhood information**. **Please refer to Page 17, Figure. 7,8 in the update paper.**\n\nIf LLMs do rely on a majority vote to determine its prediction. We would expect that the \u201cdrop different\u201d curve with 1-hop label goes up to 100% accuracy. However, we are not observing this, and the 1-hop label curve is lower than 1-hop title+label curve. **This observation refutes the hypothesis that LLMs rely on simple majority vote for prediction**. Instead, including more context information will help LLMs to make more accurate predictions as 1-hop title+label curve is higher than 1-hop label curve on both cora and arxiv-2023 datasets.\nReference:\n\n[1] Harnessing Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed Graph Representation Learning \n\n[2] Exploring the Potential of Large Language Models (LLMs) in Learning on Graphs \n\n[3] GraphText: Graph Reasoning in Text Space"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8633/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700284259807,
                "cdate": 1700284259807,
                "tmdate": 1700284259807,
                "mdate": 1700284259807,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1mj4DGpiwr",
                "forum": "D7v6HmwqKC",
                "replyto": "plqhm3XLN2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8633/Reviewer_vRFw"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8633/Reviewer_vRFw"
                ],
                "content": {
                    "comment": {
                        "value": "I would like to thank the authors for the detailed response and the added experiments.\n\n**Overlap with Previous Work.** I fully understand that developing new prompts is not your main focus, and I concur with the key takeaway message the authors aim to convey regarding the relationship between the richness of semantics of the target node and the performance of LLM. However, concerning the paper itself, regarding the \"when\" aspect, the takeaway message is that:\n- [Semantic Richness] LLMs can benefit from structural information when textual node features are scarce.\n\nRegarding the \"why\" aspect, the message is that:\n- [Homophily] the performance of LLMs on a target node is strongly positively related to the local homophily ratio of the node. As mentioned by Reviewer Ayoh and Reviewer 2d38, the effect of homophily is well-established and not surprising.\n\n- [Label Leakage] there is no substantial evidence indicating that the performance of LLMs is significantly attributed to data leakage; While the new dataset and experiments can rule out potential reasons why LLM works well, they do not provide a definitive answer to the \"why\" question.\n\nIn summary, regarding [Semantic Richness], I agree that previous work might not have delved into it. Concerning [Homophily], the message seems trivial and not surprising. Regarding [Label Leakage], it does not essentially answer the \"why\" question.\n\n\n\n**Q1.** Thanks for the clarification for Q1.\n\n**Q2.** Thanks for the added experiment for Q2. I have a quick question, how did you deal with the nodes with no neighbours after dropping neighbours? Did you include them or filter out them? Because if they are still included, it can justify why the accuracy is not 100% for the 'drop different' curve with 1-hop label\n\n\nThanks for the authors' efforts in addressing my concern. But I still tend to maintain my score, mainly due to concerns about the overall contribution, i.e., the limited message conveyed as a benchmark/survey/investigation paper."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8633/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700538264247,
                "cdate": 1700538264247,
                "tmdate": 1700538264247,
                "mdate": 1700538264247,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "7qecLmyiKs",
            "forum": "D7v6HmwqKC",
            "replyto": "D7v6HmwqKC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8633/Reviewer_Ayoh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8633/Reviewer_Ayoh"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents an experimental study on leveraging structured information (specifically graphs) in LLMs and analyzed the impact of structured information on the performance of LLM, specifically on the node classification task.  The general idea is to augment the textual features of a given node in the prompt with the structured information from the graph by describing the list of k-hop neighbors (grouped by hop levels) along with their textual features. The paper conducts their experimental study using ChatGPT 3.5 turbo on  our popular public graph-based datasets (OGBN-ARXIV, CORA, PUBMED, and OGBN-PRODUCT). Based on their experimental observations, following major conclusions were drawn from the paper: i) LLMs can potentially benefit from structural information, especially when textual features on the individual nodes  is scarce, ii) LLMs performance improves further on graphs that have higher local homphily ratio, i.e. graphs where nodes with similar features are more likely to be connected by an edge, and iii) No conclusive evidence was observed to attribute improvement of performance in LLMs to data leakage."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. The paper presents a  detailed experimental study on the impact of structural information on LLM's performance on node classification tasks while exploring several aspects of the problem, including: i) the impact of local homophily ratio on LLM performance, ii) altering textual information on the node, iii) data-leakage impact (i.e. possibility of test dataset being used in LLM training )\n2. The insights seem to be logical and intuitive and I see fair experimental setup being used to backup the claims."
                },
                "weaknesses": {
                    "value": "1. Lack of significant contributions. My biggest concern about this paper is that the conclusions derived from the paper are quite trivial and unsurprising. For instance, the observation that stronger improvements in the performance on data with scarce textual features have been made even in the context of language models prior to LMs. I don't see why this wouldn't have hold for LLMs? Similarly, the observation regarding higher accuracies of LLM predictions on  nodes with higher local homophilic ratio is also a common observation in the graph-mining literature. Finally, in the third insight of this paper, there is no concrete evidence on whether data-leakage could potentially boost capabilities LLMs in leveraging structured information. The lack of evidence is only applicable to prompt styles explored in this paper, but there is no evidence to make any general claims (as also recognized by the authors).\n\n2. The scope of the study is limited to the task of node classification. What about leveraging graph-based information in LLMs in other node tasks such as forecasting or regression problems? Similarly, how could graph based information help LLMs in link prediction problems and its applications in recommendation systems (e.g. predicting if a user will like a movie)?\n\nSome other specific comment: \n1. Page 4 line 174: It seems like there is another strategy called k-hop attention strategy which is not described here, but somehow being reffered as \"second strategy\" in the next paragraph."
                },
                "questions": {
                    "value": "None."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8633/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698391546695,
            "cdate": 1698391546695,
            "tmdate": 1699637080659,
            "mdate": 1699637080659,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "DRetjpg2PF",
                "forum": "D7v6HmwqKC",
                "replyto": "7qecLmyiKs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8633/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8633/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Ayoh (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for your thoughtful review. We appreciate your acknowledgment of our paper's detailed experimental study, which delves into the impact of structural information on LLMs in node classification tasks.\n\nWe address your concerns in detail below.\n\n\n> W1.1: \u201cLack of significant contributions. My biggest concern about this paper is that the conclusions derived from the paper are quite trivial and unsurprising. For instance, the observation that stronger improvements in the performance on data with scarce textual features have been made even in the context of language models prior to LMs. I don't see why this wouldn't have hold for LLMs? Similarly, the observation regarding higher accuracies of LLM predictions on nodes with higher local homophilic ratio is also a common observation in the graph-mining literature.\u201d\n\n\nOur intention is to investigate if LLMs can be augmented with appropriate structural information for text classification tasks, which is not reiterating known concepts from graph-mining.\n\nSome other interesting/non-trivial observations:\n1. **Paper with high homophily is easier for LLMs to classify even without any structural information** (Page 9, Table 5, row 2). With zero-shot prompts (only the title of the paper is given), we observe a significant positive correlation between local homophily ratio and prediction correctness on 4 datasets. This is actually very interesting.\n2. **The benefit of structural information saturates earlier on LLMs than MPNNs.** To show this, we add an experiment to compare the performance increase from incorporating structural information for LLMs and MPNNs respectively. The results are shown in below table and included in Appendix D.2, Page 16, Table 9.\n\nThe average increase from structural data of **LLMs** on 4 datasets is 2.78% (rich context) and 5.44% (scarce context). But the increase from structural data of **MPNNs** is 6.98% (rich context) and 14.07% (scarce context), which is significantly higher than the gain of LLMs. It means that The benefit of structural information saturates earlier on LLMs than MPNNs. This may be explained by LLMs have potential advantages in handling rich text features compared to MPNNs.\n\n\nTable 9: Classification accuracy for the OGBN-ARXIV, CORA, ARXIV-2023, PUBMED on ChatGPT as well as GCN, SAGE and MLP. \u2191 (LLMs) denotes the improvements of best prompt style that leverages structural information over zero-shot method. \u2191 (MPNNs) denotes the improvements of the best MPNNs over MLP (without structural information).\n| Textual Context | Prompt Style               | Arxiv  | Cora   | Arxiv 2023 | Pubmed |\n|-----------------|----------------------------|--------|--------|------------|--------|\n| Rich            | Zero-shot                  | 74.0   | 66.1   | 73.5       | 88.6   |\n|                 | 1-hop title+label          | 75.1   | 72.5   | 73.8       | 89.1   |\n|                 | 2-hop title+label          | 74.5   | 74.7   | 73.2       | 89.7   |\n|                 | 1-hop attention            | 74.7   | 72.5   | 73.6       | 88.8   |\n|                 | $\\uparrow$ (LLMs)          | 1.1    | 8.6    | 0.3        | 1.1    |\n|-----------------|----------------------------|--------|--------|------------|--------|\n|                 | MLP                        | 69.9   | 65.4   | 69.7       | 86.2   |\n|                 | GCN                        | 75.4   | 83.0   | 70.3       | 88.4   |\n|                 | SAGE                       | 75.0   | 83.2   | 70.9       | 90.0   |\n|                 | $\\uparrow$ (MPNNs)         | 5.5    | 17.8   | 1.3        | 3.8    |\n|-----------------|----------------------------|--------|--------|------------|--------|\n| Scarce          | Zero-shot                  | 69.8   | 61.8   | 66.6       | 85.9   |\n|                 | 1-hop title                | 72.3   | 69.6   | 70.7       | 80.8   |\n|                 | 1-hop title+label          | 74.3   | 73.9   | 70.4       | 84.7   |\n|                 | 2-hop title                | 71.3   | 69.9   | 68.9       | 83.5   |\n|                 | 2-hop title+label          | 74.2   | 74.5   | 68.5       | 86.4   |\n|                 | $\\uparrow$  (LLMs)         | 4.5    | 12.7   | 4.1        | 0.5    |\n|-----------------|----------------------------|--------|--------|------------|--------|\n|                 | MLP                        | 61.9   | 55.7   | 58.5       | 82.0   |\n|                 | GCN                        | 74.8   | 81.2   | 70.3       | 87.1   |\n|                 | SAGE                       | 74.4   | 78.8   | 69.1       | 87.9   |\n|                 | $\\uparrow$ (MPNNs)         | 13.0   | 25.6   | 11.8       | 6.0    |"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8633/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700284115421,
                "cdate": 1700284115421,
                "tmdate": 1700284115421,
                "mdate": 1700284115421,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2IuUjWgzW3",
                "forum": "D7v6HmwqKC",
                "replyto": "dVnQdMmDuC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8633/Reviewer_Ayoh"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8633/Reviewer_Ayoh"
                ],
                "content": {
                    "title": {
                        "value": "Response to Author's comments"
                    },
                    "comment": {
                        "value": "I thank authors for their responses and providing more details. However, I am afraid  that I am still not convinced regarding the significance of the contributions. The major key message of this paper is that  LLMs can benefit more from structural information when textual node features are scarce. The benefit is further enhanced in case of homophilic nodes. These both observations are expected to be seen for any textual-feature based models and I don't see why LLMs would have behaved differently. More generally, If you provide more information to a model, it is certainly going to boost the performance. The boost in the performance is expected to be more for less complex models and less for more complex models like LLMs. As of now I am inclined to keep my rating as it is as of now. \n\nI will read author's responses to other reviewers before making my final decision."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8633/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700517932098,
                "cdate": 1700517932098,
                "tmdate": 1700517932098,
                "mdate": 1700517932098,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "xLAceh4qjH",
            "forum": "D7v6HmwqKC",
            "replyto": "D7v6HmwqKC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8633/Reviewer_PJPi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8633/Reviewer_PJPi"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the problem of enabling Large Language Models (LLMs) to process graph-structured data. Specifically, this paper studies the potential effects on node classification accuracy with different prompting methods. The paper also studies the effect of data leakage and homophily."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "S1. This paper is easy to follow.\n\nS2. The proposed methods work reasonably well.\n\nS3. The topic combining Graph and LLM is interesting."
                },
                "weaknesses": {
                    "value": "W1. The review feels the motivation of this paper is not clear. Although LLM is popular, why we need LLMs for node classification tasks is not well-explained. The authors only focus on one node classification task. Given that a simple GNN can already yield satisfactory results for this task, the rationale for introducing LLM, which may be slower in inference, is unclear.\n\nW2. This paper lacks solid explanations and analysis of the experimental results. For example, why do different prompts result in different performance? Is there any theoretical analysis or intuitions? Why can LLM outperform GNN on ARXIV-2023 while not on OGBN-ARXIV? The current version of this paper only provides an experiment report without solid analysis.\n\nW3. The experimental analysis is also weak. The experiments in section 3.4 cannot really show the impact of homophily on LLM's classification accuracy. It may show that incorporating the context information with the same label can improve performance. The authors should compare more ablations with context samples which have the same label.\n\nW4. The authors study the problem of incorporating structural information (such as graphs) to improve the predictive accuracy of LLMs but only conduct experiments in node classification. Some experimental results might provide better understanding. For example, checking the performance of other competitive LLM methods on graphs and downstream tasks."
                },
                "questions": {
                    "value": "Please see the above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8633/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698803613826,
            "cdate": 1698803613826,
            "tmdate": 1699637080502,
            "mdate": 1699637080502,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "dX3ugOxEwG",
                "forum": "D7v6HmwqKC",
                "replyto": "xLAceh4qjH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8633/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8633/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer PJPi"
                    },
                    "comment": {
                        "value": "Thank you for your thoughtful feedback! We appreciate your recognition of the paper's clarity and the novelty of combining graphs and LLMs. We address your concerns in detail below. \n\n> W1: \u201cThe review feels the motivation of this paper is not clear. Although LLM is popular, why we need LLMs for node classification tasks is not well-explained. The authors only focus on one node classification task. Given that a simple GNN can already yield satisfactory results for this task, the rationale for introducing LLM, which may be slower in inference, is unclear.\u201d\n\nOur intention is to investigate if **LLMs**, known for their versatility and robustness in text processing, **can be augmented with appropriate structural information**, instead of **simply applying LLMs to solve graph tasks**. Our motivation can be summarized as follows:\n1. LLMs can serve as a zero-shot or few-shot classifier without any training, compared to GNNs, which is advantageous in some real-life scenarios such as online recommendations.\n2. While GNNs are efficient for graph-structured data, LLMs offer potential advantages in handling rich text features within nodes, which might not be fully leveraged by GNNs.\n3. The conditions under which LLMs can benefit from structural information is underexplored in current literature.\n\nWe refined the introduction and discussion sections to better clarify this motivation (L5, L47-48, L135, L142).\n\n> W2.1: \u201cThis paper lacks solid explanations and analysis of the experimental results. For example, why do different prompts result in different performance? Is there any theoretical analysis or intuitions?\u201d\n\nIt may be explained by that different prompts may encapsulate varying degrees of contextual relevance to the target node, impacting the LLM's ability to utilize the provided information effectively. This is further discussed in Section 3.4: Impact of Homophily on LLMs Classification Accuracy.\n\n> W2.2: \"Why can LLM outperform GNN on ARXIV-2023 while not on OGBN-ARXIV?\"\n\nThe relationship of performance of GNNs and LLMs are dependent on datasets and prompts. The observation that \"LLM outperforms GNN on ARXIV-2023 while not on OGBN-ARXIV\" may be explained by that LLMs are better at classifying certain types of papers. And the two datasets have different label distribution.\n\n> W3. \u201cThe experimental analysis is also weak. The experiments in section 3.4 cannot really show the impact of homophily on LLM's classification accuracy. It may show that incorporating the context information with the same label can improve performance. The authors should compare more ablations with context samples which have the same label.\u201d\n\nIn Section 3.4, two key experiments were conducted to investigate the impact of homophily on LLM classification accuracy. \n1. The first experiment involved dropping neighbors of a target node in three ways: those with the same label, different labels, and randomly. This demonstrated that a higher local homophily ratio, where more neighbors share the same label as the target node, positively influences LLM's prediction accuracy. \n2. The second correlation study **revealed a positive correlation** between **local homophily ratio** and **prediction correctness in all 5 datasets** with p value less than 0.001 (Page 9, Table 6). **Notice this experiment does not involve any dropping of neighbors and is calculated in the original dataset**. This further emphasizes the significance of homophily in improving the accuracy of LLM classifications.\n\nWhile we believe the results of the two experiments are fairly strong evidence of the impact of homophily, we still appreciate it if you could elaborate on the details of the ablation setting suggested in your review, so that we can further improve our experiments. Thanks!\n\n> W4. \u201cThe authors study the problem of incorporating structural information (such as graphs) to improve the predictive accuracy of LLMs but only conduct experiments in node classification. Some experimental results might provide better understanding. For example, checking the performance of other competitive LLM methods on graphs and downstream tasks.\u201d\n\nThank you for your advice on checking \u201cother competitive LLM methods on graphs and downstream tasks\u201d. We acknowledge that other graph-related problems are important to understand how LLMs can benefit from structural information. However, as an early study on this topic, we aim to focus more on the depth of the analysis (answering the why question) rather than the breadth. We believe narrowing down the type of prediction tasks could provide us a better testbed for our analysis. That being said, we plan to extend our research to encompass a broader range of graph-related tasks, including link prediction, in future work."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8633/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700284047229,
                "cdate": 1700284047229,
                "tmdate": 1700284047229,
                "mdate": 1700284047229,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "90A8JLDaL0",
            "forum": "D7v6HmwqKC",
            "replyto": "D7v6HmwqKC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8633/Reviewer_ytsy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8633/Reviewer_ytsy"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates how Large Language Models (LLMs) can leverage graph structural information for node classification tasks with textual features.  The authors investigate the \"when\" and \"why\" of incorporating structured data, particularly graphs, into LLMs. They examine different prompting methods and factors that affect the performance of LLMs, such as data leakage, homophily (nodes with similar characteristics). Through controlled experiments and correlational analyses, the authors  establish a positive relationship between the local homophily ratio and the prediction accuracy of LLMs, i.e., more homophily incorporated in the prompt, the more accurate the prediction becomes. While data leakage is a potential concern , the authors observe that LLMs' consistent performance across different datasets suggests that they are robust and can perform well across varying distribution domains. Finally, the authors provide insights and suggestions for future research on LLMs and graph data."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The paper is well-written, clear, and organized. The authors present their motivation, approach, methodology, results, and contributions in a logical and coherent manner.\n* The paper addresses an interesting topic of integrating LLMs with graph data, which is a crucial data modality that can provide additional information to LLMs.\n* The evaluation results provide valuable insights into the potential and limitations of LLMs in leveraging graph structural information, as well as the challenges and opportunities for future research on LLMs and graph data."
                },
                "weaknesses": {
                    "value": "One of the limitations is that the study only focuses on node classification tasks and does not explore other graph-related tasks. It is unclear whether these techniques can be generalized to other tasks."
                },
                "questions": {
                    "value": "Do you think other Large language models such as LLaMA or Falcon robust against data leakage?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8633/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698919570045,
            "cdate": 1698919570045,
            "tmdate": 1699637080350,
            "mdate": 1699637080350,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "A9vpqLRGBH",
                "forum": "D7v6HmwqKC",
                "replyto": "90A8JLDaL0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8633/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8633/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ytsy"
                    },
                    "comment": {
                        "value": "Thank you for recognizing the \u201cvaluable insights\u201d in our work and clear writing. We address your detailed concerns below. \n\n\n> One of the limitations is that the study only focuses on node classification tasks and does not explore other graph-related tasks. It is unclear whether these techniques can be generalized to other tasks.\n\nSee common response.\n\n> Question: \"Do you think other Large language models such as LLaMA or Falcon are robust against data leakage?\"\n\nSimilar to ChatGPT, There is no significant evidence that the performance of LLaMA-2 is significantly attributed to data leakage.\n\nWe run the classification experiment on LLaMA-2-7B-chat [1] and include the results below as well as in Appendix D, Page 16, Table 8. Notice that the pretraining data of  LLaMA-2 has a cutoff of September 2022 (Page 77) and the test data in arxiv-2023 are arXiv papers published in 2023. \n\nIf LLaMA-2-7B-chat rely on data leakage to make accurate prediction, the performance drop of LLaMA-2-7B-chat between ogbn-arxiv (may have leakage problem) and arxiv-2023 (leakage-free) should **be significantly greater** than the drop on MPNNs on two datasets. This is because LLMs may benefit from their memory on ogbn-arxiv from data leakage to have higher accuracy. But this advantage is not likely on arxiv-2023. However, in Table 8 (below), we **do not observe this**: the performance drop of LLMs between ogbn-arxiv and arxiv-2023 **is less than** the drop on MPNNs in scarce context (6.7% vs 4.5%), and slightly higher in rich context (4.6% vs 5.1%). From this, we argue that there is no significant evidence that the performance of LLaMA-2 is significantly attributed to data leakage. \n\n\n| Textual Context | Prompt Style       | Ogbnarxiv | Arxiv |\n|-----------------|--------------------|-----------|-------|\n| Rich            | Zero-shot          | 45.1      | 45.1  |\n|                 | 1-hop title        | 51.6      | 50.0  |\n|                 | 1-hop title+label  | **66.9**  | **60.2**|\n|                 | GCN                | 75.4      | 70.3  |\n|                 | SAGE               | 75.0      | 70.9  |\n| Scarce          | Zero-shot          | 38.8      | 38.2  |\n|                 | 1-hop title        | 51.5      | 45.5  |\n|                 | 1-hop title+label* | **58.0**  | **53.4** |\n|                 | GCN                | 74.8      | 70.3  |\n|                 | SAGE               | 74.4      | 69.1  |\n\n\n\n\n[1] Llama 2: Open Foundation and Fine-Tuned Chat Models, https://arxiv.org/abs/2307.09288"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8633/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700283996621,
                "cdate": 1700283996621,
                "tmdate": 1700283996621,
                "mdate": 1700283996621,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "beazmEgG2Z",
                "forum": "D7v6HmwqKC",
                "replyto": "8pfqSwtn2T",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8633/Reviewer_ytsy"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8633/Reviewer_ytsy"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Authors, thank you for your detailed rebuttal and additional experiments. I appreciate your effort to clarify some concerns raised by other reviewers. I think you've provided some evidence that LLaMA-2 isn't much affected by data leakage, and that your method can perform well on node classification tasks with textual features. I still have some doubts about how your approach can generalize well for other graph-related tasks and other datasets, therefore I keep my recommendation as marginally Accepted."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8633/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700730622570,
                "cdate": 1700730622570,
                "tmdate": 1700730622570,
                "mdate": 1700730622570,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "HYomIa6iuW",
            "forum": "D7v6HmwqKC",
            "replyto": "D7v6HmwqKC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8633/Reviewer_2d38"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8633/Reviewer_2d38"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents an empirical study on how LLMs can capture information contained in graph structures. Specifically, the authors consider the task of node classification with textual features as node features. The authors verify that with neighbor information to complement textual features, the performances of LLMs can be improved, especially under the scenario where scarce textural information is given. The authors then analyze where the LLMs performance on graphs come from, and come to two conclusions: the performance of LLMs on graphs is not related to data leakage, and homophily contributes to the performance of LLMs on graph-structured data."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Timely topic. It is interesting to explore the potentials of LLM on a wide range of tasks, and graph node classification task with rich textual information is a suitable topic for LLMs, in that it is closely related to texts and incorporates different data structures. \n2. Good organization. The paper is well-organized and easy to follow. It is easy for me to understand the main ideas and conclusions/arguments of this paper. \n3. The 'data leakage' aspect of this paper is interesting and a somewhat unique aspect in the combination of LLMs and graphs. Indeed, in the area of traditional graph learning, data leakage is not an issue. Therefore, I appreciate the authors' efforts in identifying such a potential cause and perform experiments."
                },
                "weaknesses": {
                    "value": "1. The conclusions of this paper are somewhat not surprising. As LLMs work on texts, it is intuitive that with rich texts, LLMs can already well understand the node, and when there are scarce texts, some additional related texts can further boost the performance of LLM. Also, the conclusion about homophily is also not surprising: bringing heterophilous nodes to the context will distract LLM and thus compromise the accuracy. I am not saying that the contribution of this paper is limited --- **it is a solid paper with good contributions**, but the conclusions are just not that surprising, which would slightly reduce my rating. \n2. Studying the behavior of ChatGPT indeed leads to better understanding of LLMs on graphs. However, as ChatGPT is not open source, it would be better if the authors can also perform experiments on open-source models, such as LLaMA. In that way, the insights may better lead to more concrete efforts in combining LLMs with graph learning, as LLaMA can be more easily fine-tuned to fit the need of graph data understanding. Also, as LLMs are sensitive to prompts,  there is no guarantee that the conclusions on ChatGPT can generalize to other LLMs. In this sense, it is always better to do experiments on a wider range of LLMs to ensure that the conclusion is general enough."
                },
                "questions": {
                    "value": "1. Do you observe cases where ChatGPT fails to follow the instructions and generates stuff that cannot be automatically parsed? How do you deal with these samples?\n2. LLMs may be sensitive to the order of contexts. Have you tried to modify the order of 1-hop, 2-hop neighbors in the 1/2-hop title+label setting? Does it lead to significant performance changes?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "Not required."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8633/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698973213640,
            "cdate": 1698973213640,
            "tmdate": 1699637080240,
            "mdate": 1699637080240,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2ceGOutTU6",
                "forum": "D7v6HmwqKC",
                "replyto": "HYomIa6iuW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8633/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8633/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 2d38"
                    },
                    "comment": {
                        "value": "Thank you for recognizing the timeliness and good organization of our work, and the novel \naspect of data leakage. We address your detailed concerns below. \n\n> it would be better if the authors can also perform experiments on open-source models, such as LLaMA.\n\nSee common response. We added experiments on LLaMA-2-7B-chat [1].\n\n> Question1: \"Do you observe cases where ChatGPT fails to follow the instructions and generates stuff that cannot be automatically parsed? How do you deal with these samples?\"\n\nWe did not observe that ChatGPT failed to follow the instructions. For example on dataset ogbn-arxiv, our instruction is \"Please predict the most appropriate arXiv Computer Science (CS) sub-category for the paper. The predicted sub-category should be in the format \u2019cs.XX\u2019.\". ChatGPT always prints out 'cs.XX' as its answer without extra text. \nOn the other hand, LLaMA-2-7B-chat sometimes provides extra explanation text in addition to its prediction. In this case, we matched the last appeared category as its prediction. This parsing strategy worked throughout our experiment.\n\n> Question 2: \"LLMs may be sensitive to the order of contexts. Have you tried to modify the order of 1-hop, 2-hop neighbors in the 1/2-hop title+label setting? Does it lead to significant performance changes?\"\n\nShuffling the order of neighbors in prompt has a negligible effect on prediction performance. We have conducted the shuffling experiment on Cora, and the results show small standard deviations after shuffling the order of neighbors over 3 runs: $0.7281 \\pm 0.0031$, (1-hop title+label), $0.6359 \\pm 0.0008$ (1-hop title), $0.7577 \\pm 0.0031$ (2-hop title+label). This observation also holds for other datasets in our experiments.\n\n\n[1] Llama 2: Open Foundation and Fine-Tuned Chat Models, https://arxiv.org/abs/2307.09288"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8633/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700283963052,
                "cdate": 1700283963052,
                "tmdate": 1700283963052,
                "mdate": 1700283963052,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4XwtDit0A5",
                "forum": "D7v6HmwqKC",
                "replyto": "2ceGOutTU6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8633/Reviewer_2d38"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8633/Reviewer_2d38"
                ],
                "content": {
                    "title": {
                        "value": "Response acknowledged."
                    },
                    "comment": {
                        "value": "I have read the author response. I appreciate the authors providing new results on LLaMA. I will maintain a weak accept for now and will consider raise the scores in future reviewer discussions."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8633/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700516580608,
                "cdate": 1700516580608,
                "tmdate": 1700516580608,
                "mdate": 1700516580608,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]