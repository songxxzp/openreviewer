[
    {
        "title": "Boosting Temporal Graph Learning From Global and Local Perspectives"
    },
    {
        "review": {
            "id": "3GOAxfAR7n",
            "forum": "auguNUCto5",
            "replyto": "auguNUCto5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1741/Reviewer_u41A"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1741/Reviewer_u41A"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on temporal graph representation learning. Different from exiting works that utilize GNN-RNN frameworks or local subgraph information to learn effective node representations, this paper proposes to generate node embeddings by considering both global and local perspectives. Specifically, GCN-TCN is utilized to encode global graph information; TGN is used to model local graph information; Then, self-attention mechanism is employed to aggregate node representations from both global and local embeddings. Experimental results on seven datasets demonstrate that the proposed model can achieve satisfied performance on both link prediction and node classification tasks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1)\tThis paper investigates temporal graph representation learning, which is an important topic in the graph community.\n2)\tVarious ablation studies are given to show the effectiveness of the proposed components.\n3)\tTime complexity analysis is given, and model efficiency analyses are also presented in the experiment section."
                },
                "weaknesses": {
                    "value": "1)\tThe technical contribution of this paper is quite limited. The three components in this paper are all from existing works. The global module is from TCN, and the local module is from TGN. The cross-perspective fusion is a self-attention module. No new module is proposed.\n2)\tIn the cross-perspective fusion module, it is not clear why query $Q_i$ is the linear projection of $Z^{Global}$. What if we use $Z^{local}$ to generate $Q_i$? In this case, $\\tilde{z}$ is the weighted combination of $Z^{global}$ and $z_u = FFN(\\tilde{z}_u \\Vert z_u^{local})$. More ablation studies should be conducted.\n3)\tIn Table 2, it is not clear why the performance of GraphMixer and TIGER are missing in the setting of historical and inductive negative sampling. \n4)\tThere are lots of other manners to combine both the global and local perspectives. For instance, $z_u$ is directly generated by concatenating $Z^{local}$ and $Z^{global}$. $z_u$ can also be aggregated with simple attention mechanism instead of self-attention.\n5)\tIn Equation 10, what if some of the nodes do not have $|\\mathcal{N}|$ neighbors? Are there any strategies to handle this situation? \n\nThere are lots of typos in the paper. For example,\n\nIn Figure 1, \u201cwhether nodes B and C will interact at t3\u201d should be \u201cwhether nodes B and D will interact at t3\u201d\n\n\u201cwe focus on further improve the model performance\u201d should be \u201cwe focus on further improving the model performance\u201d\n\n\u201cWheras, GLEN can keep\u201d should be \u201cWhereas, GLEN can keep\u201d"
                },
                "questions": {
                    "value": "1)\tIn Table 2, it is not clear why the performance of GraphMixer and TIGER are missing in the setting of historical and inductive negative sampling. \n2)\tThere are lots of other manners to combine both the global and local perspectives. For instance, $z_u$ is directly generated by concatenating $Z^{local}$ and $Z^{global}$. $z_u$ can also be aggregated with simple attention mechanism instead of self-attention.\n3)\tIn Equation 10, what if some of the nodes do not have $|\\mathcal{N}|$ neighbors? Are there any strategies to handle this situation?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No ethics review needed."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1741/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698584396278,
            "cdate": 1698584396278,
            "tmdate": 1699636103357,
            "mdate": 1699636103357,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "bMVBfnhfTk",
                "forum": "auguNUCto5",
                "replyto": "3GOAxfAR7n",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1741/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1741/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Part 1 for Reviewer u41A"
                    },
                    "comment": {
                        "value": "Thanks a lot for your insightful comments and detailed suggestions, which are very helpful for us to further improve this paper. We would like to thank you for bringing up the typos in our paper. We have corrected them and further improved our paper. Next, we'll answer your questions one by one. We hope our following answers will address the points you have raised and improve your view of our work.\n\n**W1:**\n\nThanks for your comments. \n\nTo the best of our knowledge, we are the first in the field of temporal graph learning to propose a method that simultaneously models the graph structure from an entire global perspective and a local subgraph perspective, and fuses all node embeddings across views. Our method fills the gap in existing methods that only focus on one perspective and highlights the importance of considering both views. Our concept and related analysis of the idea that modeling temporal graphs from both global and local perspectives is novel and has research significance in this field. And we propose a completely new model based on this motivation.\n\nTherefore, our novelty lies in our contributed insights, motivation as well as the innovatively proposed method. The novelty of each module is specified below:\n\n- The novelty of the global embedding module is that there is no work that combines GNN and TCN for learning on temporal graphs to obtain more efficient and effective results, to the best of our knowledge.\n- In the local embedding module, we designed a new time interval weighting algorithm that is more concise and efficient, which is different from TGN.\n- The Cross-Perspective Fusion Module is designed to better fuse global and local embeddings, and its novelty lies in the motivation for integration based on the semantic correlation among all node embeddings, while the attention mechanism is just a way for us to implement the module.\n\nIt is also of research significance to construct effective and efficient methods that work well in experiments and applications using basic, simple components. This inspires us that it may not be necessary to adopt a complex approach to temporal graph learning. Often basic models with novel and solid insights can also make good gains and achieve a better balance between inference precision and efficiency.\n\nWe hope our answers address your concerns. \n\n\n\n**W2:**\n\nThanks for pointing this out. Due to space constraints, we did not explain this in the paper. We use the global embedding as the query and the local embedding as the key and value for 2 main reasons:\n\n- In terms of modeling temporal information, local embeddings are obtained by encoding specific timestamp information, whereas global module relies more on evolution patterns across time slices by modeling sequencial information at a coarse-grained level using TCN. Therefore, in order to better utilize the fine-grained timestamp information, the hidden state of local embeddings need to be utilized.\n\n- In terms of topological information, although both local and global modules aggregate neighborhood information for nodes, the local module can aggregate neighbor information more effectively than the graph convolution of the global module due to the use of our devised  weighted sum algorithm based on time interval. And as we mentioned in the introduction, local and global are oriented to graph structures with different degrees of integrity (local subgraphs or entire graph snapshots), so the operations of the local module are more fine-grained.\n\nTo better validate the soundness of our considerations and address your concern, we further compare the performance (Average Precision, %) of the model before and after (denoted as GLEN-swap) swapping $Z^{Global}$ and $Z^{Local}$ in the attention, as shown in the following tables:"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1741/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699977487837,
                "cdate": 1699977487837,
                "tmdate": 1699977487837,
                "mdate": 1699977487837,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4Qa9NmUU0g",
                "forum": "auguNUCto5",
                "replyto": "3GOAxfAR7n",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1741/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1741/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Part 2 for Reviewer u41A"
                    },
                    "comment": {
                        "value": "| Dataset                      |           | Wikipedia  | Wikipedia      | Wikipedia     | Reddit     | Reddit         | Reddit        | Enron      | Enron          | Enron         | UCI        | UCI            | UCI           |\n| ---------------------------- | --------- | ---------- | -------------- | ------------- | ---------- | -------------- | ------------- | ---------- | -------------- | ------------- | ---------- | -------------- | ------------- |\n| **NS Strategy**              |           | **Random** | **Historical** | **Inductive** | **Random** | **Historical** | **Inductive** | **Random** | **Historical** | **Inductive** | **Random** | **Historical** | **Inductive** |\n| Transductive link prediction | GLEN      | 99.99\u00b10.01 | 97.67\u00b10.16     | 95.20\u00b10.36    | 99.99\u00b10.01 | 98.74\u00b11.25     | 99.24\u00b10.82    | 93.67\u00b10.48 | 94.35\u00b12.24     | 94.99\u00b11.91    | 99.55\u00b10.90 | 98.24\u00b11.13     | 93.30\u00b13.31    |\n| Transductive link prediction | GLEN-swap | 96.79\u00b10.69 | 89.97\u00b13.45     | 84.38\u00b111.43   | 98.19\u00b10.10 | 86.60\u00b12.23     | 85.13\u00b16.27    | 88.46\u00b10.84 | 77.79\u00b16.31     | 71.76\u00b12.56    | 82.56\u00b15.08 | 77.36\u00b11.13     | 76.80\u00b10.62    |\n|                              | Reduction | \u2b073.2%      | \u2b077.88%         | \u2b0711.37%       | \u2b071.8%      | \u2b0712.29%        | \u2b0714.20%       | \u2b074.86%     | \u2b0717.55%        | \u2b0724.46%       | \u2b0717.07%    | \u2b0731.43%        | \u2b0728.4%        |\n| Inductive link prediction    | GLEN      | 99.95\u00b10.05 | 96.25\u00b10.27     | 96.13\u00b10.29    | 99.85\u00b10.28 | 97.31\u00b12.46     | 97.28\u00b12.48    | 96.15\u00b11.61 | 97.28\u00b10.53     | 97.38\u00b10.46    | 99.11\u00b10.30 | 95.94\u00b12.00     | 95.43\u00b12.63    |\n| Inductive link prediction    | GLEN-swap | 96.82\u00b10.46 | 85.72\u00b111.98    | 85.86\u00b111.81   | 96.18\u00b10.17 | 79.84\u00b11.07     | 81.81\u00b12.90    | 88.48\u00b12.68 | 62.86\u00b13.14     | 63.52\u00b13.14    | 89.91\u00b11.99 | 78.98\u00b10.83     | 79.03\u00b10.65    |\n|                              | Reduction | \u2b073.13%     | \u2b0710.94%        | \u2b0710.68%       | \u2b073.68%     | \u2b0717.93%        | \u2b0715.9%        | \u2b0718.38%    | \u2b0735.38%        | \u2b0734.77%       | \u2b0719.37%    | \u2b0728.1%         | \u2b0727.66%       |\n\n| Dataset                      |           | UN Trade    | UN Trade       | UN Trade      | MOOC       | MOOC           | MOOC          | Flights    | Flights        | Flights       |\n| ---------------------------- | --------- | ----------- | -------------- | ------------- | ---------- | -------------- | ------------- | ---------- | -------------- | ------------- |\n| **NS Strategy**              |           | **Random**  | **Historical** | **Inductive** | **Random** | **Historical** | **Inductive** | **Random** | **Historical** | **Inductive** |\n| Transductive link prediction | GLEN      | 97.83\u00b10.15  | 97.65\u00b10.08     | 97.45\u00b12.11    | 98.39\u00b12.52 | 99.89\u00b10.11     | 99.86\u00b10.19    | 99.96\u00b10.01 | 85.46\u00b10.39     | 89.83\u00b10.77    |\n| Transductive link prediction | GLEN-swap | 83.43\u00b111.32 | 72.62\u00b110.57    | 72.72\u00b110.48   | 97.87\u00b13.92 | 85.26\u00b13.49     | 88.27\u00b15.24    | 78.88\u00b13.92 | 79.92\u00b15.33     | 79.61\u00b114.67   |\n|                              | Reduction | \u2b0714.72%     | \u2b0725.63%        | \u2b0725.38%       | \u2b070.53%     | \u2b0714.65%        | \u2b0711.61%       | \u2b0721.09%    | \u2b076.48%         | \u2b0711.49%       |\n| Inductive link prediction    | GLEN      | 96.09\u00b10.12  | 95.78\u00b12.32     | 95.76\u00b12.32    | 96.48\u00b14.02 | 99.53\u00b10.93     | 99.54\u00b10.91    | 99.36\u00b10.17 | 76.96\u00b10.54     | 77.23\u00b10.61    |\n| Inductive link prediction    | GLEN-swap | 80.16\u00b18.71  | 69.59\u00b17.56     | 69.58\u00b17.51    | 95.50\u00b13.54 | 85.98\u00b13.84     | 85.88\u00b13.69    | 77.50\u00b13.54 | 75.43\u00b14.39     | 75.58\u00b15.25    |\n|                              | Reduction | \u2b0716.58%     | \u2b0727.34%        | \u2b0727.34%       | \u2b071.02%     | \u2b0713.61%        | \u2b0713.72%       | \u2b0722%       | \u2b071.99%         | \u2b072.14%        |\n\nThe performance decreases on almost all datasets after swapping $Z^{Global}$ and $Z^{Local}$ . The experimental results somewhat justify our setting of the query, key and value."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1741/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699977528634,
                "cdate": 1699977528634,
                "tmdate": 1699977528634,
                "mdate": 1699977528634,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BYyvq5ycLJ",
                "forum": "auguNUCto5",
                "replyto": "3GOAxfAR7n",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1741/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1741/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Part 3 for Reviewer u41A"
                    },
                    "comment": {
                        "value": "**W3&Q1:**\n\nThanks for pointing this out. Since these two methods were presented after the benchmark [1] that proposes the historical and inductive negative sampling, and due to the coupling and complex frameworks of the codes, it is difficult to evaluate these two methods using historical and inductive negative sampling. \n\nWhen we submitted this paper, we did not have time to successfully apply both strategies to these methods. However, we have now been able to evaluate GraphMixer with both strategies and the results are presented in the table below. We have updated the results in our paper.\n\n| NS  Strategy | Task         | Method     | Wikipedia      | Reddit         | Enron          | UCI            | Untrade        | MOOC           | Flights        |\n| ------------ | ------------ | ---------- | -------------- | -------------- | -------------- | -------------- | -------------- | -------------- | -------------- |\n| Historical   | Transductive | GraphMixer | 90.83\u00b10.25     | 77.96\u00b10.40     | 78.89\u00b10.96     | 85.63\u00b10.35     | 59.64\u00b12.71     | 77.56\u00b11.05     | 71.55\u00b10.23     |\n| Historical   | Transductive | GLEN       | **97.67\u00b10.16** | **98.74\u00b11.25** | **94.35\u00b12.24** | **98.24\u00b11.13** | **97.65\u00b10.08** | **99.89\u00b10.11** | **85.46\u00b10.39** |\n| Historical   | Inductive    | GraphMixer | 88.02\u00b10.39     | 64.48\u00b10.36     | 73.18\u00b11.20     | 80.29\u00b10.31     | 58.92\u00b12.67     | 74.07\u00b10.73     | 65.23\u00b10.23     |\n| Historical   | Inductive    | GLEN       | **96.25\u00b10.27** | **97.31\u00b12.46** | **97.28\u00b10.53** | **95.94\u00b12.00** | **95.78\u00b12.32** | **99.53\u00b10.93** | **76.96\u00b10.54** |\n| Inductive    | Transductive | GraphMixer | 88.43\u00b10.40     | 85.04\u00b10.17     | 75.14\u00b10.93     | 77.97\u00b10.31     | 62.86\u00b13.21     | 74.34\u00b10.38     | 74.84\u00b10.20     |\n| Inductive    | Transductive | GLEN       | **95.20\u00b10.36** | **99.24\u00b10.82** | **94.99\u00b11.91** | **93.30\u00b13.31** | **97.45\u00b12.11** | **99.86\u00b10.19** | **89.83\u00b10.77** |\n| Inductive    | Inductive    | GraphMixer | 83.91\u00b10.54     | 63.96\u00b10.26     | 73.19\u00b11.19     | 80.33\u00b10.31     | 58.89\u00b12.66     | 74.08\u00b10.73     | 63.13\u00b10.15     |\n| Inductive    | Inductive    | GLEN       | **96.13\u00b10.29** | **97.28\u00b12.48** | **97.38\u00b10.46** | **95.43\u00b12.63** | **95.76\u00b12.32** | **99.54\u00b10.91** | **77.23\u00b10.61** |\n\nImplementing historical and inductive strategies requires recording the historical interaction edges of all nodes and sampling accordingly during the testing phase. Since TIGER focuses primarily on model restarting and designs a restarter to output the warm initialization for node memory, the memory is very different from the other baselines. The restarter re-initializes the memory using only a small portion of the recent events. Therefore, the historical and inductive negative sampling strategies are not applicable to TIGER. We realize that this may be confusing to readers who are less familiar with the field. We will add relevant explanations in the future version of our paper.\n\n[1] Poursafaei, Farimah , et al. \"Towards Better Evaluation for Dynamic Link Prediction.\" (NIPS, 2022). URL https://openreview.net/forum?id=1GVpwr2Tfdg."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1741/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699977560298,
                "cdate": 1699977560298,
                "tmdate": 1700208478776,
                "mdate": 1700208478776,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Aq35RA60GP",
                "forum": "auguNUCto5",
                "replyto": "3GOAxfAR7n",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1741/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1741/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Part 4 for Reviewer u41A"
                    },
                    "comment": {
                        "value": "**W4&Q2:**\n\nWhat we used in the cross-perspective fusion module is not self-attention. As for other methods of combination, as we said in Appendix C.6, the **w/o. Fusion** variant in the ablation experiments is to simply add $Z^{local}$ and $Z^{local}$, which leads to degradation in performance. As for the concatenation approach and simple attention mechanism (We think you may be referring to weighted summation using learned weights.) you mentioned, we add experiments to verify that the module we designed is more effective.\n\n**Experimental results of GLEN using concatenation for fusion:**\n\n| Dataset                      |                          | Wikipedia  |   Wikipedia    |   Wikipedia   |   Reddit   |     Reddit     |    Reddit     |   Enron    |     Enron      |     Enron     |    UCI     |      UCI       |      UCI      |\n| ---------------------------- | ------------------------ | :--------: | :------------: | :-----------: | :--------: | :------------: | :-----------: | :--------: | :------------: | :-----------: | :--------: | :------------: | :-----------: |\n| **NS Strategy**              |                          | **Random** | **Historical** | **Inductive** | **Random** | **Historical** | **Inductive** | **Random** | **Historical** | **Inductive** | **Random** | **Historical** | **Inductive** |\n| Transductive link prediction | GLEN                     | 99.99\u00b10.01 |   97.67\u00b10.16   |  95.20\u00b10.36   | 99.99\u00b10.01 |   98.74\u00b11.25   |  99.24\u00b10.82   | 93.67\u00b10.48 |   94.35\u00b12.24   |  94.99\u00b11.91   | 99.55\u00b10.90 |   98.24\u00b11.13   |  93.30\u00b13.31   |\n|                              | GLEN-using concatenation | 97.13\u00b10.23 |   71.10\u00b12.06   |  77.14\u00b14.42   | 96.35\u00b10.25 |   68.28\u00b10.65   |  62.25\u00b10.54   | 79.11\u00b15.48 |   66.95\u00b12.22   |  65.45\u00b13.68   | 85.34\u00b12.99 |   69.06\u00b12.66   |  68.71\u00b11.08   |\n|                              | Reduction                |   \u2b072.86%   |     \u2b0727.2%     |    \u2b0718.97%    |   \u2b073.64%   |    \u2b0730.85%     |    \u2b0737.27%    |  \u2b0715.54%   |    \u2b0729.04%     |    \u2b0731.1%     |  \u2b0714.27%   |     \u2b0729.7%     |    \u2b0726.36%    |\n| Inductive link prediction    | GLEN                     | 99.95\u00b10.05 |   96.25\u00b10.27   |  96.13\u00b10.29   | 99.85\u00b10.28 |   97.31\u00b12.46   |  97.28\u00b12.48   | 96.15\u00b11.61 |   97.28\u00b10.53   |  97.38\u00b10.46   | 99.11\u00b10.30 |   95.94\u00b12.00   |  95.43\u00b12.63   |\n|                              | GLEN-using concatenation | 96.61\u00b10.18 |   77.48\u00b14.04   |  77.27\u00b13.61   | 96.14\u00b10.14 |   59.10\u00b10.39   |   59.29\u00b1051   | 75.85\u00b17.33 |   62.24\u00b12.83   |  62.31\u00b12.83   | 81.42\u00b11.33 |   70.98\u00b10.70   |  71.15\u00b10.65   |\n|                              | Reduction                |   \u2b073.34%   |     \u2b0719.5%     |    \u2b0719.62%    |   \u2b073.72%   |    \u2b0739.27%     |    \u2b0737.99%    |  \u2b0721.11%   |    \u2b0736.02%     |    \u2b0736.01%    |  \u2b0717.85%   |    \u2b0726.02%     |    \u2b0725.44%    |\n\n| Dataset                      |                          |  UN Trade  |    UN Trade    | UN Trade      |    MOOC    |      MOOC      | MOOC          | Flights    | Flights        | Flights       |\n| ---------------------------- | ------------------------ | :--------: | :------------: | ------------- | :--------: | :------------: | ------------- | ---------- | -------------- | ------------- |\n| **NS Strategy**              |                          | **Random** | **Historical** | **Inductive** | **Random** | **Historical** | **Inductive** | **Random** | **Historical** | **Inductive** |\n| Transductive link prediction | GLEN                     | 97.83\u00b10.15 |   97.65\u00b10.08   | 97.45\u00b12.11    | 98.39\u00b12.52 |   99.89\u00b10.11   | 99.86\u00b10.19    | 99.96\u00b10.01 | 85.46\u00b10.39     | 89.83\u00b10.77    |\n|                              | GLEN-using concatenation | 94.97\u00b10.06 |   89.67\u00b10.21   | 89.96\u00b10.03    | 82.04\u00b13.06 |   68.31\u00b14.95   | 73.63\u00b11.27    | 92.03\u00b10.52 | 68.28\u00b10.65     | 65.62\u00b12.43    |\n|                              | Reduction                |   \u2b072.92%   |     \u2b078.17%     | \u2b077.69%        |  \u2b0716.62%   |    \u2b0731.61%     | \u2b0726.27%       | \u2b077.93%     | \u2b0720.1%         | \u2b0726.95%       |\n| Inductive link prediction    | GLEN                     | 96.09\u00b10.12 |   95.78\u00b12.32   | 95.76\u00b12.32    | 96.48\u00b14.02 |   99.53\u00b10.93   | 99.54\u00b10.91    | 99.36\u00b10.17 | 76.96\u00b10.54     | 77.23\u00b10.61    |\n|                              | GLEN-using concatenation | 92.87\u00b10.01 |   89.66\u00b10.05   | 89.68\u00b10.14    | 81.20\u00b13.97 |   72.20\u00b12.64   | 72.08\u00b12.95    | 84.50\u00b10.52 | 59.19\u00b10.39     | 59.32\u00b11.91    |\n|                              | Reduction                |   \u2b073.35%   |     \u2b076.39%     | \u2b076.35%        |  \u2b0715.84%   |    \u2b0727.46%     | \u2b0727.59%       | \u2b0714.96%    | \u2b0723.09%        | \u2b0723.19%       |"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1741/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699977599758,
                "cdate": 1699977599758,
                "tmdate": 1699977599758,
                "mdate": 1699977599758,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9tMxji7vmw",
                "forum": "auguNUCto5",
                "replyto": "3GOAxfAR7n",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1741/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1741/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Part 5 for Reviewer u41A"
                    },
                    "comment": {
                        "value": "**Experimental results of GLEN using simple attention mechanism for fusion:**\n\n| Dataset                      |                             | Wikipedia  | Wikipedia  | Wikipedia  |   Reddit   |   Reddit   |   Reddit   |   Enron    |   Enron    |   Enron    |    UCI     |    UCI     |    UCI     |\n| ---------------------------- | --------------------------- | :--------: | :--------: | :--------: | :--------: | :--------: | :--------: | :--------: | :--------: | :--------: | :--------: | :--------: | :--------: |\n| NS Strategy                  |                             |   Random   | Historical | Inductive  |   Random   | Historical | Inductive  |   Random   | Historical | Inductive  |   Random   | Historical | Inductive  |\n| Transductive link prediction | GLEN                        | 99.99\u00b10.01 | 97.67\u00b10.16 | 95.20\u00b10.36 | 99.99\u00b10.01 | 98.74\u00b11.25 | 99.24\u00b10.82 | 93.67\u00b10.48 | 94.35\u00b12.24 | 94.99\u00b11.91 | 99.55\u00b10.90 | 98.24\u00b11.13 | 93.30\u00b13.31 |\n|                              | GLEN-using simple attention | 95.04\u00b10.24 | 77.20\u00b13.41 | 83.26\u00b10.30 | 98.77\u00b10.23 | 62.20\u00b12.45 | 64.81\u00b12.23 | 92.62\u00b10.66 | 94.24\u00b10.85 | 93.47\u00b10.74 | 85.19\u00b10.78 | 68.60\u00b10.71 | 68.72\u00b10.95 |\n|                              | Reduction                   |   \u2b074.95%   |  \u2b0720.96%   |  \u2b0712.54%   |   \u2b071.22%   |  \u2b0737.01%   |  \u2b0734.69%   |   \u2b071.12%   |   \u2b070.12%   |   \u2b071.60%   |  \u2b0714.42%   |  \u2b0730.17%   |  \u2b0726.35%   |\n| Inductive link prediction    | GLEN                        | 99.95\u00b10.05 | 96.25\u00b10.27 | 96.13\u00b10.29 | 99.85\u00b10.28 | 97.31\u00b12.46 | 97.28\u00b12.48 | 96.15\u00b11.61 | 97.28\u00b10.53 | 97.38\u00b10.46 | 99.11\u00b10.30 | 95.94\u00b12.00 | 95.43\u00b12.63 |\n|                              | GLEN-using simple attention | 95.68\u00b10.08 | 84.12\u00b10.55 | 84.40\u00b10.48 | 95.17\u00b10.39 | 68.10\u00b10.88 | 68.13\u00b10.90 | 95.20\u00b10.13 | 94.63\u00b11.78 | 94.76\u00b11.58 | 81.03\u00b10.67 | 70.91\u00b10.88 | 70.97\u00b10.98 |\n|                              | Reduction                   |   \u2b074.27%   |  \u2b0712.60%   |   \u2b0712.2%   |   \u2b074.69%   |  \u2b0730.02%   |  \u2b0729.97%   |   \u2b070.98%   |   \u2b072.72%   |   \u2b072.69%   |  \u2b0718.24%   |  \u2b0726.09%   |  \u2b0725.63%   |\n\n| Dataset                      |                             |  UN Trade  |  UN Trade  |  UN Trade  |    MOOC    |    MOOC    |    MOOC    |  Flights   |  Flights   |  Flights   |\n| ---------------------------- | --------------------------- | :--------: | :--------: | :--------: | :--------: | :--------: | :--------: | :--------: | :--------: | :--------: |\n| NS Strategy                  |                             |   Random   | Historical | Inductive  |   Random   | Historical | Inductive  |   Random   | Historical | Inductive  |\n| Transductive link prediction | GLEN                        | 97.83\u00b10.15 | 97.65\u00b10.08 | 97.45\u00b12.11 | 98.39\u00b12.52 | 99.89\u00b10.11 | 99.86\u00b10.19 | 99.96\u00b10.01 | 85.46\u00b10.39 | 89.83\u00b10.77 |\n|                              | GLEN-using simple attention | 69.87\u00b10.29 | 62.44\u00b10.38 | 66.17\u00b10.98 | 76.33\u00b13.65 | 66.46\u00b14.31 | 69.81\u00b12.18 | 91.63\u00b11.06 | 66.65\u00b10.79 | 67.78\u00b10.58 |\n|                              | Reduction                   |  \u2b0728.58%   |  \u2b0736.06%   |  \u2b0732.10%   |  \u2b0722.42%   |  \u2b0733.47%   |  \u2b0730.09%   |   \u2b078.33%   |  \u2b0722.01%   |  \u2b0724.55%   |\n| Inductive link prediction    | GLEN                        | 96.09\u00b10.12 | 95.78\u00b12.32 | 95.76\u00b12.32 | 96.48\u00b14.02 | 99.53\u00b10.93 | 99.54\u00b10.91 | 99.36\u00b10.17 | 76.96\u00b10.54 | 77.23\u00b10.61 |\n|                              | GLEN-using simple attention | 63.67\u00b10.64 | 55.13\u00b10.07 | 55.18\u00b10.25 | 72.64\u00b14.69 | 65.26\u00b13.40 | 65.10\u00b13.26 | 85.71\u00b11.60 | 60.95\u00b11.10 | 60.82\u00b11.14 |\n|                              | Reduction                   |  \u2b0733.74%   |  \u2b0742.44%   |  \u2b0742.38%   |  \u2b0724.71%   |  \u2b0734.43%   |  \u2b0734.60%   |  \u2b0713.74%   |   20.80%   |  \u2b0721.34%   |\n\nActually, our main motivations for proposing the use of attention in the cross-perspective fusion module are the following points:\n\n- To better capture the semantic correlations between global embeddings and local embeddings of all nodes, which cannot be achieved by simple addition or concatenation.\n- The use of attentional aggregation allows the consideration of the eigenvectors of all nodes jointly, whereas addition or concatenation only operates on the global and local embeddings of the same node.\n\nAs we mentioned in Appendix A, global and local embeddings can capture some correlations between linked nodes, but also ignore some. The strong correlations between all linked nodes can be reflected in fused embeddings based on the devised attention mechanism."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1741/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699977641211,
                "cdate": 1699977641211,
                "tmdate": 1699977641211,
                "mdate": 1699977641211,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gO2JDM46le",
                "forum": "auguNUCto5",
                "replyto": "3GOAxfAR7n",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1741/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1741/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Part 6 for Reviewer u41A"
                    },
                    "comment": {
                        "value": "**W5&Q3:**\n\nConsistent with baselines, when there are not enough |N| neighbors, only available neighbors are used for information aggregation. When a node does not have enough temporal neighbors, it indicates that there are few events currently associated with this node, which is also an evolution pattern that needs to be considered without additional operations.\n\nThe purpose of setting |N|is to utilize only the most recently interacted neighbors, otherwise all neighbors in the history will be considered. Therfore, not restricting the number of neighbors may lead to an excessive number of neighbors and excessive computation, which is more serious for dense datasets like Flights and Reddit. In addition, not limiting the number of neighbors may lead to unstable training.\n\n**Weakness about typos:** Thanks again for your comments and the typos you pointed out. We have corrected them and further improved our paper. We hope our answers address your concerns and can improve your view of our work."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1741/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699977659614,
                "cdate": 1699977659614,
                "tmdate": 1699977659614,
                "mdate": 1699977659614,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "M8rGEUgEbK",
            "forum": "auguNUCto5",
            "replyto": "auguNUCto5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1741/Reviewer_farU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1741/Reviewer_farU"
            ],
            "content": {
                "summary": {
                    "value": "This paper propose GLEN, an adventurous method for effective and efficient temporal graph representation learning. GLEN can generates embeddings for graph nodes by considering both global and local perspectives. Sufficient experimental results demonstrate that GLEN outperforms other baselines in both link prediction and dynamic node classification tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.\tThe paper clearly explain the motivation of the idea combining global and local perspectives in temporal graph representation learning, and the reason to use RNN-TCN and TGN correspondingly.\n2.\tThe experiments in this paper are comprehensive, and they provide ample evidence of the model's superiority in terms of performance and efficiency."
                },
                "weaknesses": {
                    "value": "1.\tIn the specific components of the model, many aspects are not novel. For example, RNN-TCN and TGN are both derived from previous works. In Cross-Perspective Fusion Module, this module is a common transformer.\n\n2.\tI still have some doubts regarding the use of RNN-TCN for extracting global information. Both GCN and TGN employ similar information aggregation approaches, aggregating nodes up to n-hops away. Why is RNN-TCN considered to be more effective in representing global information?"
                },
                "questions": {
                    "value": "See \u201cWeaknesses\u201d"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1741/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1741/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1741/Reviewer_farU"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1741/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698751840809,
            "cdate": 1698751840809,
            "tmdate": 1699636102865,
            "mdate": 1699636102865,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "S2NNJDlhuB",
                "forum": "auguNUCto5",
                "replyto": "M8rGEUgEbK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1741/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1741/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks a lot for your very considerate feedback, which is very helpful for us to further improve this paper. We hope our following answers will address the points you have raised and improve your view of our work.\n\n**W1:**\n\nThanks for your comments. In fact, we don't use any RNN in our method. As mentioned in our paper, RNNs generally suffer from inefficiency and unstable training. To avoid the problems, we innovatively adopt TCN to model the sequential effect across snapshots. Our fusion module is also not a transfomer, but a designed attention mechanism.\n\nTo the best of our knowledge, we are the first in the field of temporal graph learning to propose a method that simultaneously models the graph structure from an entire global perspective and a local subgraph perspective, and fuses all node embeddings across views. Our method fills the gap in existing methods that only focus on one perspective and highlights the importance of considering both views. Our concept and related analysis of the idea that modeling temporal graphs from both global and local perspectives is novel and has research significance in this field. And we propose a completely new model based on this motivation.\n\nTherefore, our novelty lies in our contributed insights, motivation as well as the innovatively proposed method. The novelty of each module is specified below:\n\n- The novelty of the global embedding module is that there is no work that combines GNN and TCN for learning on temporal graphs to obtain more efficient and effective results, to the best of our knowledge.\n- In the local embedding module, we designed a new time interval weighting algorithm that is more concise and efficient.\n- The Cross-Perspective Fusion Module is designed to better fuse global and local embeddings, and its novelty lies in the motivation for integration based on the semantic correlation among all node embeddings, while the attention mechanism is just a way for us to implement the module.\n\nIt is also of research significance to construct effective and efficient methods that work well in experiments and applications using basic, simple components. This inspires us that it may not be necessary to adopt a complex approach to temporal graph learning. Often basic models with novel and solid insights can also make good gains and achieve a better balance between inference precision and efficiency.\n\nWe hope our answers address your concerns.\n\n\n\n**W2:**\n\nThank you for your insightful suggestion.\n\nGlobal and local are defined according to the completeness of the graph structure being processed. Global modeling means that a snapshot of the entire graph is fed into the model when learning node representations. Graph learning operations are performed on the complete graph structure, which is done without filtering the neighbors of each node at the current time slice. Local modeling refers to processing each event of the event stream as the object and aggregating neighborhood information for the nodes involved in the events. In addition, the two perspectives model temporal patterns differently, and more elaboration can be found specifically in our introduction.\n\nAlthough both of these approaches aggregate information for each node within a certain range of neighborhood, TGN focuses on message passing for each node involved in interactions and use a time encoding function to fuse temporal information. This approach is more like fine-grained event-driven learning. GCN-TCN, on the other hand, captures topological information at each time slice and models sequential information across time, which is more like exploring the evolution of the graph through the model itself. Therefore, there is still some difference in the neighbor information aggregated in the two ways. As we mentioned in Appendix A, the information aggregated in the two ways is indeed going to be different. Global and local embeddings can capture some correlations between linked nodes, but also ignore some. The strong correlations between all linked nodes can be reflected in fused embeddings.\n\nIn addition, our cross-perspective fusion module can break through the n-hop range restriction since it takes into account the embeddings of all nodes in the graph. We hope our answers can address your concerns."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1741/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699977358792,
                "cdate": 1699977358792,
                "tmdate": 1699977358792,
                "mdate": 1699977358792,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4eam1GIbAm",
                "forum": "auguNUCto5",
                "replyto": "S2NNJDlhuB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1741/Reviewer_farU"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1741/Reviewer_farU"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the authors\u2018 reply, which resolved most of my questions. I would like to maintain my rating."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1741/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700582633445,
                "cdate": 1700582633445,
                "tmdate": 1700582633445,
                "mdate": 1700582633445,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "dmw1YrgyYm",
            "forum": "auguNUCto5",
            "replyto": "auguNUCto5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1741/Reviewer_K85V"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1741/Reviewer_K85V"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a novel Global and Local embedding Network (GLEN) for temporal graph representation learning, which captures both local and global information. Specifically, GLEN first generates both local and global embeddings, and then combine these embeddings via cross-perspective fusion module. The proposed GLEN is evaluated on several real-world datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The idea of local embedding and global embedding is novel and interesting. For a given window of the temporal graph, GCN is used to extract node embeddings for each time stamp. TCN is used to capture global node embeddings, and a temporal interval weighting module is used over a restricted neighborhood to capture the local embeddings. In the end, the local and global embeddings are combined via a cross-perspective fusion module.\n2. The proposed method could significantly outperform SOTA temporal graph embedding methods on several benchmark datasets, and the ablation study demonstrates that each of the proposed component is crucial for model's performance.\n3. The writing of the paper is clear in general."
                },
                "weaknesses": {
                    "value": "1. The motivation of the cross-perspective fusion module needs further clarification. Why do you use the global embedding as the query but not the local embedding? Why not (1) use global embedding as query to obtain z1, (2) use local embedding as query to obtain z2 and (3) combine z1 and z2? \n2. What will happen if you only use the local embedding module but without restricting the size of neighbors?\n3. Some details need further improvement.\n(1). What are $\\hat{y}_0, \\hat{y}_1,\\dots$? (Between Eq. (8) and Eq. (9))\n(2). $\\mathbf{h}_v^{(0)} = \\mathbf{s}_v^{(t)}+\\mathbf{x}_v^{(t)}$?"
                },
                "questions": {
                    "value": "Please refer to weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1741/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698758438186,
            "cdate": 1698758438186,
            "tmdate": 1699636102785,
            "mdate": 1699636102785,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KCNOGnfq4C",
                "forum": "auguNUCto5",
                "replyto": "dmw1YrgyYm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1741/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1741/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Part 1 for Reviewer K85V"
                    },
                    "comment": {
                        "value": "Thanks a lot for your insightful comments and detailed suggestions, which are very helpful for us to further improve this paper. We hope our following answers will address the points you have raised and improve your view of our work.\n\n**W1:**\n\nThanks for pointing this out. Due to space constraints, we did not explain this in the paper. We use the global embedding as the query and the local embedding as the key and value for two main reasons:\n\n- In terms of modeling temporal information, local embeddings are obtained by encoding specific timestamp information, whereas global module relies more on evolution patterns across time slices by modeling sequencial information at a coarse-grained level using TCN. Therefore, in order to better utilize the fine-grained timestamp information, the hidden state of local embeddings need to be utilized.\n\n- In terms of topological information, although both local and global modules aggregate neighborhood information for nodes, the local module can aggregate neighbor information more effectively than the graph convolution of the global module due to the use of our devised  weighted sum algorithm based on time interval. And as we mentioned in the Introduction, local and global are oriented to graph structures with different degrees of integrity (local subgraphs or entire graph snapshots), so the operations of the local module are more fine-grained.\n\nTo better validate the soundness of our considerations and address your concern, we further compare the performance (Average Precision, %)  of the model before and after (denoted as GLEN-swap) swapping $Z^{Global}$ and $Z^{Local}$ in the attention, as shown in the following tables:\n\n| Dataset                      |           | Wikipedia  | Wikipedia      | Wikipedia     | Reddit     | Reddit         | Reddit        | Enron      | Enron          | Enron         | UCI        | UCI            | UCI           |\n| ---------------------------- | --------- | ---------- | -------------- | ------------- | ---------- | -------------- | ------------- | ---------- | -------------- | ------------- | ---------- | -------------- | ------------- |\n| **NS Strategy**              |           | **Random** | **Historical** | **Inductive** | **Random** | **Historical** | **Inductive** | **Random** | **Historical** | **Inductive** | **Random** | **Historical** | **Inductive** |\n| Transductive link prediction | GLEN      | 99.99\u00b10.01 | 97.67\u00b10.16     | 95.20\u00b10.36    | 99.99\u00b10.01 | 98.74\u00b11.25     | 99.24\u00b10.82    | 93.67\u00b10.48 | 94.35\u00b12.24     | 94.99\u00b11.91    | 99.55\u00b10.90 | 98.24\u00b11.13     | 93.30\u00b13.31    |\n| Transductive link prediction | GLEN-swap | 96.79\u00b10.69 | 89.97\u00b13.45     | 84.38\u00b111.43   | 98.19\u00b10.10 | 86.60\u00b12.23     | 85.13\u00b16.27    | 88.46\u00b10.84 | 77.79\u00b16.31     | 71.76\u00b12.56    | 82.56\u00b15.08 | 77.36\u00b11.13     | 76.80\u00b10.62    |\n|                              | Reduction | \u2b073.2%      | \u2b077.88%         | \u2b0711.37%       | \u2b071.8%      | \u2b0712.29%        | \u2b0714.20%       | \u2b074.86%     | \u2b0717.55%        | \u2b0724.46%       | \u2b0717.07%    | \u2b0731.43%        | \u2b0728.4%        |\n| Inductive link prediction    | GLEN      | 99.95\u00b10.05 | 96.25\u00b10.27     | 96.13\u00b10.29    | 99.85\u00b10.28 | 97.31\u00b12.46     | 97.28\u00b12.48    | 96.15\u00b11.61 | 97.28\u00b10.53     | 97.38\u00b10.46    | 99.11\u00b10.30 | 95.94\u00b12.00     | 95.43\u00b12.63    |\n| Inductive link prediction    | GLEN-swap | 96.82\u00b10.46 | 85.72\u00b111.98    | 85.86\u00b111.81   | 96.18\u00b10.17 | 79.84\u00b11.07     | 81.81\u00b12.90    | 88.48\u00b12.68 | 62.86\u00b13.14     | 63.52\u00b13.14    | 89.91\u00b11.99 | 78.98\u00b10.83     | 79.03\u00b10.65    |\n|                              | Reduction | \u2b073.13%     | \u2b0710.94%        | \u2b0710.68%       | \u2b073.68%     | \u2b0717.93%        | \u2b0715.9%        | \u2b0718.38%    | \u2b0735.38%        | \u2b0734.77%       | \u2b0719.37%    | \u2b0728.1%         | \u2b0727.66%       |"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1741/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699976497175,
                "cdate": 1699976497175,
                "tmdate": 1699976497175,
                "mdate": 1699976497175,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wkigjBNF3x",
                "forum": "auguNUCto5",
                "replyto": "dmw1YrgyYm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1741/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1741/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Part 2 for Reviewer K85V"
                    },
                    "comment": {
                        "value": "| Dataset                      |           | UN Trade    | UN Trade       | UN Trade      | MOOC       | MOOC           | MOOC          | Flights    | Flights        | Flights       |\n| ---------------------------- | --------- | ----------- | -------------- | ------------- | ---------- | -------------- | ------------- | ---------- | -------------- | ------------- |\n| **NS Strategy**              |           | **Random**  | **Historical** | **Inductive** | **Random** | **Historical** | **Inductive** | **Random** | **Historical** | **Inductive** |\n| Transductive link prediction | GLEN      | 97.83\u00b10.15  | 97.65\u00b10.08     | 97.45\u00b12.11    | 98.39\u00b12.52 | 99.89\u00b10.11     | 99.86\u00b10.19    | 99.96\u00b10.01 | 85.46\u00b10.39     | 89.83\u00b10.77    |\n| Transductive link prediction | GLEN-swap | 83.43\u00b111.32 | 72.62\u00b110.57    | 72.72\u00b110.48   | 97.87\u00b13.92 | 85.26\u00b13.49     | 88.27\u00b15.24    | 78.88\u00b13.92 | 79.92\u00b15.33     | 79.61\u00b114.67   |\n|                               | Reduction | \u2b0714.72%     | \u2b0725.63%        | \u2b0725.38%       | \u2b070.53%     | \u2b0714.65%        | \u2b0711.61%       | \u2b0721.09%    | \u2b076.48%         | \u2b0711.49%       |\n| Inductive link prediction    | GLEN      | 96.09\u00b10.12  | 95.78\u00b12.32     | 95.76\u00b12.32    | 96.48\u00b14.02 | 99.53\u00b10.93     | 99.54\u00b10.91    | 99.36\u00b10.17 | 76.96\u00b10.54     | 77.23\u00b10.61    |\n| Inductive link prediction    | GLEN-swap | 80.16\u00b18.71  | 69.59\u00b17.56     | 69.58\u00b17.51    | 95.50\u00b13.54 | 85.98\u00b13.84     | 85.88\u00b13.69    | 77.50\u00b13.54 | 75.43\u00b14.39     | 75.58\u00b15.25    |\n|                               | Reduction | \u2b0716.58%     | \u2b0727.34%        | \u2b0727.34%       | \u2b071.02%     | \u2b0713.61%        | \u2b0713.72%       | \u2b0722%       | \u2b071.99%         | \u2b072.14%        |\n\nThe performance decreases on all datasets after swapping $Z^{Global}$ and $Z^{Local}$ . The experimental results somewhat justify our setting of the query, key and value."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1741/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699977223388,
                "cdate": 1699977223388,
                "tmdate": 1699977223388,
                "mdate": 1699977223388,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UKyGU8gx2N",
                "forum": "auguNUCto5",
                "replyto": "dmw1YrgyYm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1741/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1741/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Part 3 for Reviewer K85V"
                    },
                    "comment": {
                        "value": "**W2:**\n\nThanks for catching this. Limiting the number of temporal neighbors is to utilize only the most recently interacted neighbors, otherwise all neighbors in the history will be considered. Therfore, not restricting the number of neighbors may lead to an **excessive number of neighbors and excessive computation**, which is more serious for dense datasets like Flights and Reddit. In addition, not limiting the number of neighbors may lead to **unstable training**.\n\nExisting works on temporal graphs all have imposed a limit on the neighborhood size, so we do so as well. The main reasons for doing so are summarized as follows:\n\n- To simplify the problem and fix the computation pattern. To avoid too many neighbors of some nodes leading to excessive computational overhead and time cost. A large amount of existing works demonstrate that limiting the number of neighbors can strike a good balance between accuracy and efficiency.\n- For the sake of stable training. As mentioned in the paper TGAT [1], sampling from neighborhood, or known as neighborhood dropout, may speed up and stabilize model training. For temporal graphs, neighborhood dropout can be carried uniformly or weighted by the inverse timespan such that more recent interactions has higher probability of being sampled.\n- For the sake of comparison fairness. All baselines limit the neighborhood size and investigate it as a hyperparameter. In order to allow a fair comparison between our method and baselines, it is inappropriate to change this operation.\n\nIn addition, the global module processes the graph structure at each time slice. If only use the local embedding module but without restricting the size of neighbors, all interactions in the history are considered by the local module, which is inconsistent with the role of the global embedding module. The concept of \"global\" emphasizes the integrity of the graph structure at each time slice rather than considering the full historical information.\n\n**W3:**\n\n(1)The sequence of $\\hat{y}$ is the output sequence after the TCN performs the causal convolution operation on the input sequence $\\{x_0,x_1,...\\}$. As drawn in Figure 2, the TCN outputs an output sequence with the same length as the input sequence. For $d$-dimensional eigenvectors, we consider the values at different timestamps for each dimension as a time series. So the convolution operation is performed in $d$ channels, each using the last output element $\\hat{y}_{\\Gamma-1}$ as the predictive value.\n\n(2)What you have written may refer to the sentence \"$h^{(0)}_v(t)$ is the sum of $s_v(t)$ and temporal node features.\" Differently from the  original baseline TGAT [1] where no node-wise temporal features were used, in our local module the input representation of each node $v$ is $h^{(0)}_v(t)=s_v(t)+x_v(t)$ and as such it allows the model to exploit both the current memory $s_v(t)$ and the temporal node features $x_v(t)$.\n\nThanks again for your advice. Due to space constraints, some of our statements may not be clear. We will consider adding relevant explanations to the paper to make it clearer.\n\n[1] Da Xu, Chuanwei Ruan, Evren Korpeoglu, Sushant Kumar, and Kannan Achan. Inductive representation learning on temporal graphs. In ICLR, 2020. URL https://openreview.net/forum?id=rJeW1yHYwH."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1741/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699977283983,
                "cdate": 1699977283983,
                "tmdate": 1699977283983,
                "mdate": 1699977283983,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZrJmShYIth",
                "forum": "auguNUCto5",
                "replyto": "M8rGEUgEbK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1741/Reviewer_K85V"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1741/Reviewer_K85V"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your responses. After reading all the comments, I'd like to keep my rating."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1741/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700645524588,
                "cdate": 1700645524588,
                "tmdate": 1700645524588,
                "mdate": 1700645524588,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "PXhDGnz45s",
            "forum": "auguNUCto5",
            "replyto": "auguNUCto5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1741/Reviewer_L1Kz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1741/Reviewer_L1Kz"
            ],
            "content": {
                "summary": {
                    "value": "The paper discusses the limitations of existing methods in temporal graph learning, which either focus on global or local perspectives but not both. To overcome this, the Global and Local Embedding Network (GLEN) is proposed. GLEN dynamically generates node embeddings by considering both global and local information. These embeddings are then fused using a cross-perspective module to capture high-order semantic relations. GLEN has been evaluated on multiple datasets and outperforms baselines in tasks like link prediction and dynamic node classification."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "S1. Temporal graph is an important problem to address in practical world, yet majority of the research deals with static graphs only.\n\nS2. The analysis of existing RNN-based work and random walk/message passing models provides useful insights.\n\nS3. The writing/organization of the paper is generally clear, although some parts need more clarification. (See W2)"
                },
                "weaknesses": {
                    "value": "W1. The proposed model, while technically valid and sound, is not sufficiently novel or exciting. Combining local and global perspectives are common ideas in graphs. Even on temporal graph, point process based modeling aims to capture the graph-wide evolution pattern from  a global perspective, such as (Lu et al., 2019) and the below paper [a]. A detailed discussion on temporal point processes for temporal graph is warranted, potentially with additional experimental comparison.\n\n[a] Trend: Temporal event and node dynamics for graph representation learning. WWW 2022.\n\nW2. Certain parts in the motivation of the paper are not clearly explained. For example, the following sentences:\n\"Pairwise interactions observed in different graphs or even the same temporal graph typically have different temporal properties.\"\n\"Since the endogenous and exogenous factors driving the generative process ...\" \nI'm not exactly sure how they directly connect to or motivate the proposed method.\n\nW3. In Table 2, Random tends to perform the best compared to historical/inductive strategies. It is surprising and more discussion is needed. (Also, I'm not confident of the results in Table 2, as it has some discrepancy with the results in Table 3 -- e.g. for UCI, the results in Table 2 and Table 3 are different."
                },
                "questions": {
                    "value": "Please see Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1741/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698829362853,
            "cdate": 1698829362853,
            "tmdate": 1699636102685,
            "mdate": 1699636102685,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "o6moNgXtAB",
                "forum": "auguNUCto5",
                "replyto": "PXhDGnz45s",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1741/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1741/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Part 1 for Reviewer L1Kz"
                    },
                    "comment": {
                        "value": "Thanks a lot for your insightful comments and detailed suggestions, which are very helpful for us to further improve this paper. We hope our following answers will address the points you have raised and improve your view of our work.\n\n**W1:**\n\nThank you for your valuable comments. We provide relevant discussions about the two methods using temporal point processes as follows. We will add relevant discussions of these methods in the future versions of our paper.\n\n**Discussions about the paper (Lu et al., 2019):**\n\nThe method for macro- and micro-dynamics proposed in the paper (Lu et al., 2019) are quite different from our devised modules of global and local perspectives.\n\n- For micro-dynamics, they \"regard the establishments of edges as the occurrences of chronological events and propose a temporal attention point process to capture the formation process of network structures\". Therefore, the temporal point process they use doesn't aims to capture the graph-wide evolution pattern from a global perspective. Time point processes are often based on assumptions such as events occurring as a random process based on conditional intensity, and the effect of historical events on the intensity function is cumulative. These assumptions are not generally applicable to temporal graphs. Our devised local embedding module based on time interval weighting fully considers the temporal nature to aggregate information about neighbors and events more effectively.\n- For macro-dynamics, they use \"a general dynamics equation parameterized with network embeddings\". While our global perspective modeling uses graph neural networks and TCN, which are better suited for modeling the graph structures and sequential information. \n- In addition, they set the loss function terms for macro and micro dynamics respectively, which makes predictions more dependent on the model's capability itself. Whereas, our method fuses global and local embeddings through a devised attention mechanism, which helps adaptively capture the semantic correlations between global embeddings and local embeddings of all nodes.\n\n**Discussions about TREND:**\n\nThe primary motivation of TREND is to explicitly capture the exciting effects between events through the temporal point processes, most notably the Hawkes process. A Hawkes process is a stochastic process that can be understood as counting the number of events up to time \ud835\udc61. Its behavior is typically modeled by a conditional intensity function \ud835\udf06(\ud835\udc61), the rate of event occurring at time \ud835\udc61 given the past events. The paper mentions: \"integrating the node dynamics provides a regularizing mechanism beyond individual events, to ensure that **the events from a node, as a collection**, conform to the continuous evolution of the node.\" Thus, methods like TREND focus mainly on processing the collection of events about each node. \n\nHere \"collection\" corresponds to the set of events related with temporal neighbors of a single node, which is different from our global perspective that processes the entire graph structure. This operation is more like the local perspective we defined, since it is oriented to a local subgraph of each node. Our global module focuses on graph representation learning on the complete graph structure at each time slice, which has **a larger scope compared to approaches that model the event collection of each node**. \n\nTo indicate the difference between our method and methods using temporal point processes, we show here the inductive link prediction performance (in percent, with the random negative sampling strategy) comparison between our approach GLEN and TREND, as TREND is the SOTA method that uses the temporal point processes:\n\n| Dataset | Wikipedia  | Wikipedia  |   Reddit   |   Reddit   |    UCI     |    UCI     |   Enron    |   Enron    |  UN Trade  |  UN Trade  |    MOOC    |    MOOC    |  Flights   |  Flights   |\n| ------- | :--------: | :--------: | :--------: | :--------: | :--------: | :--------: | :--------: | :--------: | :--------: | :--------: | :--------: | :--------: | :--------: | :--------: |\n|         |  Accuracy  |     F1     |  Accuracy  |     F1     |  Accuracy  |     F1     |  Accuracy  |     F1     |  Accuracy  |     F1     |  Accuracy  |     F1     |  Accuracy  |     F1     |\n| TREND   | 83.75\u00b11.19 | 83.86\u00b11.24 | 83.06\u00b10.24 | 83.65\u00b10.27 | 65.05\u00b10.92 | 63.15\u00b12.54 | 73.58\u00b12.49 | 69.79\u00b12.42 | 75.40\u00b12.11 | 74.18\u00b11.65 | 76.36\u00b12.55 | 79.20\u00b13.40 | 73.26\u00b10.02 | 73.52\u00b10.32 |\n| GLEN    | 97.44\u00b10.97 | 96.14\u00b11.04 | 98.26\u00b10.21 | 96.86\u00b10.39 | 76.80\u00b15.52 | 82.03\u00b11.60 | 84.02\u00b13.29 | 85.55\u00b11.62 | 89.13\u00b11.55 | 91.96\u00b11.09 | 87.77\u00b13.09 | 91.13\u00b11.74 | 85.32\u00b10.75 | 85.65\u00b11.78 |\n| Improv. |  \u2b0616.35%   |  \u2b0614.64%   |  \u2b0618.31%   |  \u2b0615.79%   |  \u2b0618.06%   |  \u2b0623.02%   |  \u2b0614.19%   |  \u2b0622.58%   |  \u2b0618.21%   |  \u2b0623.97%   |  \u2b0614.94%   |  \u2b0615.06%   |  \u2b0616.46%   |  \u2b0616.50%   |"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1741/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699976338948,
                "cdate": 1699976338948,
                "tmdate": 1699976338948,
                "mdate": 1699976338948,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Cx8CB8YD67",
                "forum": "auguNUCto5",
                "replyto": "PXhDGnz45s",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1741/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1741/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Part 2 for Reviewer L1Kz"
                    },
                    "comment": {
                        "value": "The experimental results above somewhat demonstrate the advantages of our approach over methods that use temporal point processes. In addition, the training efficiency of our model is much higher than TREND. Due to the high dependence on the number of historical events and node features, methods using temporal point processes may perform poorly on sparse or absent node features datasets (such as UCI and Enron).\n\n**W2:**\n\nThanks for catching this. \n\nThese sentences are elaborated mainly to **explain our motivation for introducing information from both global and local perspectives**. \n\n- The first sentence indicates that graph topologies across domains or across time may have quite different temporal properties. For example, it is clear that the evolution patterns of the temporal graphs in the economics (UN Trade), social interaction (Reddit) and transport (Flights) domains are quite different. Due to the regularity and abruptness of events, the pattern of events can vary across time. Therefore, modeling at different time granularities have to be taken into account, implying the necessity for us to utilize both global and local modules.\n- The second sentence demonstrates the diversity of graph evolution. The complexity of endogenous and exogenous factors that drive changes in graph structure can make temporal graphs both regular and mutant. Thus this also indicates the need to introduce both global and local perspective modeling for adaptively capturing different graph natures. \n\nWe will consider revising the statement here to make it clearer in the future version of our paper. We hope our answers address your concerns. \n\n**W3:**\n\nThanks a lot for pointing this out. We did have a data recording error in Table 3. The actual data about the UCI dataset should be consistent with Table 2 and we have corrected it in the paper. \n\nThe historical and inductive negative sampling strategies are first proposed in this benchmark paper [1]. Before this benchmark was proposed, the evaluation of link prediction for temporal graph methods was evaluated only using the random negative sampling strategy. **This benchmark paper points out the following problems with the random strategy :**\n\n- No Collision Checking: It is possible for the same edge to be both positive and negative. This collision is more likely to happen in denser datasets, such as UN Trade.\n- No Reoccurring Edges: The probability of sampling an edge which was observed before is often very low due to the sparsity of the graph. However, in many real-world tasks such as flight prediction, correct prediction of the same edge for different time steps is particularly important.\n\nThus, the random strategy may exaggerate the efficacy of current models on real-world tasks and hinder researchers\u2019 ability to evaluate if new models are superior. **Historical and Inductive strategies are more stringent and challenging evaluation procedures for link prediction specific to temporal graphs, which reflect real-world considerations** and better compare the strengths and weaknesses of methods. We introduced these negative strategies in Appendix 4.5 and highlight them here for your convenience:\n\n- Historical Negative Sampling: Historical negative sampling strategy samples negative edges from the set of edges that have been observed during previous timestamps but are absent in the current step in order to evaluate whether a given method is able to correctly predict an observed training edge would reoccur.\n- Inductive Negative Sampling: Unlike historical negative sampling, the objective of inductive negative sampling is to evaluate whether a given method can model the reoccurrence pattern of edges only seen during test time. Thus, this strategy samples negative edges from the test instances that were not observed during training and are also absent currently.\n\n**The benchmark paper also points out that existing methods have significant performance degradation with the evaluation of both historical and inductive strategies.** Therefore, it's normal that random tends to perform the best compared to historical/inductive strategies.\n\n[1] Poursafaei, Farimah , et al. \"Towards Better Evaluation for Dynamic Link Prediction.\" (NIPS, 2022). URL https://openreview.net/forum?id=1GVpwr2Tfdg."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1741/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699976379499,
                "cdate": 1699976379499,
                "tmdate": 1700541100679,
                "mdate": 1700541100679,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9QzToLV2Ui",
                "forum": "auguNUCto5",
                "replyto": "Cx8CB8YD67",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1741/Reviewer_L1Kz"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1741/Reviewer_L1Kz"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the detailed responses. I will weigh them carefully."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1741/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700315561117,
                "cdate": 1700315561117,
                "tmdate": 1700315561117,
                "mdate": 1700315561117,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "inQMl2WCwU",
                "forum": "auguNUCto5",
                "replyto": "PXhDGnz45s",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1741/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1741/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks a lot for your response and discretion. We welcome any other questions and further discussions with us. We sincerely hope that our replies can improve your perception of our work."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1741/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700318083657,
                "cdate": 1700318083657,
                "tmdate": 1700318676332,
                "mdate": 1700318676332,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]