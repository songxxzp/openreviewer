[
    {
        "title": "GENERATIVE TIME SERIES LEARNING WITH TIME-FREQUENCY FUSED ENERGY-BASED MODEL"
    },
    {
        "review": {
            "id": "mWg2BIZPFx",
            "forum": "rGdEM131Ht",
            "replyto": "rGdEM131Ht",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3526/Reviewer_DVHZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3526/Reviewer_DVHZ"
            ],
            "content": {
                "summary": {
                    "value": "They proposed an energy-based model capable of both time-series forecasting and imputation. The model consists of a residual dilated convolutional network and considers both time and frequency features. They show good performance in long-term time series forecasting and imputation."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Originality: Models that apply energy-based models to time-series tasks already exist, but the proposed model (TB-EBM) differs in that it maintains long-term coherence better.\n\nQuality: They structured the introduction, related works, and proposed method well to show the differences between TB-EBM and existing models.\n\nClarity: The paper is expressed well.\n\nSignificance: It is very interesting that it can handle long-term time-series forecasting and imputation at the same time."
                },
                "weaknesses": {
                    "value": "1. They explained their model well, but the placement of Figures and Tables is not appropriate.\n2. Figure 1 is not intuitive. I don't know what the \"+\" mark next to the Time-Freq Block part means. Also, the inference part in Figure 1 is more difficult to understand.\n3. The table mentioned in the ablation study is difficult to see because it is not in the main paper.\n4. The overall structure of the paper is not friendly to readers."
                },
                "questions": {
                    "value": "1. Isn't there a process in Equation 6 to invert from the fequency domain back to the time domain? If so, I don't understand how the fequency domain feature and time domain feature are added in Equation 7.\n\n2. The residual dilated convolutional network part in the 4.4 ablation study does not seem to have any meaning. There is no doubt that performance increases as the number of layers increases because the capacity of the model increases. Rather, it seems meaningful to use the same number of layers but subtract the residual part.\n\n3. In [1], only one linear layer shows better performance than all existing models in long-term time-series forecasting. A model comparison with [1] seems necessary.\n\n[1] Li, Zhe, et al. \"Revisiting Long-term Time Series Forecasting: An Investigation on Linear Mapping.\" arXiv preprint arXiv:2305.10721 (2023)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3526/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698312178471,
            "cdate": 1698312178471,
            "tmdate": 1699636306561,
            "mdate": 1699636306561,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "V02RAc80hz",
                "forum": "rGdEM131Ht",
                "replyto": "mWg2BIZPFx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3526/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3526/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer DVHZ(Part1)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for offering the valuable feedback. We have addressed each of the concerns raised by the reviewer as below.\n\nW1.They explained their model well, but the placement of Figures and Tables is not appropriate.\n\n\u25cf We rearranged some tables and figures in the updated paper .\n\nW2.Figure 1 is not intuitive. I don't know what the \"+\" mark next to the Time-Freq Block part means. Also, the inference part in Figure 1 is more difficult to understand.\n\n\u25cf We updated both panels of Figure 1 to make them clearer in the revised paper.\n\n\u25cf The \"+\" mark means we add the time and freq feature as indicated in Equation (7).\n\n\u25cf The Inference part at the bottom panel in Figure 1 is just a standard Langevin MCMC process, which shows how to sample one time step from $y_{1:T}$to $y_{T+1:T+\\tau}$with 59 steps of Langevin sampling.\n\nW3.The table mentioned in the ablation study is difficult to see because it is not in the main paper.\n\n\u25cf This is unfortunate due to the space limit of the main paper. \n\nW4.The overall structure of the paper is not friendly to readers.\n\n\u25cf We rearranged some tables and figures to make the updated paper more reader friendly. However, we were unable to include all tables and figures to the main paper due to the space limit imposed by ICLR. \n\nQ1.Isn't there a process in Equation 6 to invert from the fequency domain back to the time domain? If so, I don't understand how the fequency domain feature and time domain feature are added in Equation 7.\n\n\u25cf No, we do not invert the frequency domain back to the time domain. As the goal of the training the energy based model is to get well estimation of the unnormalized probatlity density for temoral path,  we extract the time domain and freqency domain feature for each layer, then add the time domain and frequency domain features, which yields better results in our experiments that concating them together.\n\nQ2.The residual dilated convolutional network part in the 4.4 ablation study does not seem to have any meaning. There is no doubt that performance increases as the number of layers increases because the capacity of the model increases. Rather, it seems meaningful to use the same number of layers but subtract the residual part.\n\n\u25cf We try to show in this ablation study that the multi-layer rather than a single-layer dilated convolutional neural network is necessary to filter out the noise in the time series. It is well-known that some high frequency modes could be noise instead of valuable signal. Since our TF-EBM model keeps all the frequency modes out of the FFT, which is different from some other works (FedFormer, Fast Fourier Convolution, etc. ) that keeps a portion of them, we reply on such multi-layer dilated CNN to automatically extract useful modes."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3526/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700327704763,
                "cdate": 1700327704763,
                "tmdate": 1700327704763,
                "mdate": 1700327704763,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Hc3kcDf8yL",
            "forum": "rGdEM131Ht",
            "replyto": "rGdEM131Ht",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3526/Reviewer_op54"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3526/Reviewer_op54"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a generative model for time series forecasting (and imputation). The proposed energy based model makes use of both temporal and frequency based features through a neural network architecture the paper calls Time-Frequency block. The two sub-parts that make up this block are derived from previous works but the combination allows for integrating local and global patterns."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper makes clever use of existing work in time series feature extraction to propose the Time-Frequency block as an original neural network building block for time series data. It would in fact be curious to see how an NN based on these blocks works for other more general tasks around time-series data such as prediction, unsupervised learning etc.\n\nThe paper is overall well written and is clear in its descriptions and in providing relevant background. The intro and related works section does a good job at summarizing the paper as well as how the proposed method relates to and improves upon existing approaches.\n\nThe proposed method is directed towards the significant task of time series forecasting, and the results seem promising."
                },
                "weaknesses": {
                    "value": "This is overall a good paper. I do however have some concerns around the experiments section which are detailed in Questions below:"
                },
                "questions": {
                    "value": "1: It's unclear how significant the resulting improvements are, the numbers. Can the authors quantify if the improvements are significant? (instead of just reporting means, report the errors too). Also, this might be a typo but Table 3 first row FEDformer MSE is the lowest.\n\n2: It could be worth discussing why certain differences in results arise, e.g. Table 2 DeepAR seems to perform basically identical to the proposed method for exhange rate and electricity datasets, but performs noticeably worse on the other two datasets. What's the reason? This could provide insight into where the proposed method can provide maximum improvement (and why).\n\n3: The paper mentions that deterministic methods generally don't help with uncertainty quantification, that sets the reader up to seeing uncertainty quantification being addressed by the paper, but it doesn't seem like it has been addressed in text or in experiments.\n\nI'll be happy to revisit my score if the above points are meaningfully addressed."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3526/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698611476675,
            "cdate": 1698611476675,
            "tmdate": 1699636306479,
            "mdate": 1699636306479,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Vvbry0iHtS",
                "forum": "rGdEM131Ht",
                "replyto": "Hc3kcDf8yL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3526/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3526/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer op54"
                    },
                    "comment": {
                        "value": "We thank the reviewer for offering the valuable feedback. We have addressed each of the concerns raised by the reviewer as below.\n\nQ1: It's unclear how significant the resulting improvements are, the numbers. Can the authors quantify if the improvements are significant? (instead of just reporting means, report the errors too). Also, this might be a typo but Table 3 first row FEDformer MSE is the lowest.\n\n\u25cf Our TF-EBM model, being a probabilistic time series model, is evaluated using the CRPS metric to assess its distribution modeling capabilities. To strengthen the comparison, we have incorporated the TimeGrad model, another prominent EBM-based time series model, into Table 1. Additionally, we have included extended CRPS results for longer forecast horizons of 336 and 720 time steps in Table 8 of the Appendix. The improvements over state-of-the-art probabilistic models can be substantial, with a 30% reduction in CRPS for traffic data. Furthermore, we present both MAE and MSE metrics based on the error between the mean value of the forecasted distribution and the ground truth. Employing the mean value as the point estimate allows for a fair comparison with recent transformer-based models.\n\n\u25cf The typo in the MSE field inTable 3 for FEDformer is fixed in the updated paper.\n\nQ2: It could be worth discussing why certain differences in results arise, e.g. Table 2 DeepAR seems to perform basically identical to the proposed method for exhange rate and electricity datasets, but performs noticeably worse on the other two datasets. What's the reason? This could provide insight into where the proposed method can provide maximum improvement (and why).\n\n\u25cf The exchange rate and electricity datasets contain 8 and 370 time series, respectively, while the traffic and Wikipedia datasets have 963 and 2000 series, significantly larger than the other two datasets. The exchange rate and electricity datasets have relatively high autocorrelations (>90% as shown in Table 6) which could make them suitable for autoregressive models such as DeepAR, especially in short-horizon forecasting. \n\n\u25cf We performed additional experiments by extending the forecast horizon to 336 and 720 time steps, using the CRPS metric to evaluate performance. As shown in Table 8 in the Appendix of the updated paper, our TF-EBM outperforms DeepAR by a substantial margin for the electricity and exchange rate datasets, except for the electricity dataset with a 336 forecast horizon. This demonstrates TF-EBM's superior long-term forecasting capability, while DeepAR struggles with error accumulation\n\nQ3: The paper mentions that deterministic methods generally don't help with uncertainty quantification, that sets the reader up to seeing uncertainty quantification being addressed by the paper, but it doesn't seem like it has been addressed in text or in experiments.\n\n\u25cf Our TF-EBM model's uncertainty quantification capability stems from its probabilistic nature. As a probabilistic time series model, TF-EBM generates probability distributions for future values, allowing us to sample from these distributions to obtain the empirical quantiles of the uncertainty of our predictions. This probabilistic approach enables us to readily quantify the uncertainty associated with our predictions. For example, we can output the 70% confidence interval along with the mean value as shown in Figure 3 in the appendix. In contrast, point forecast models like TimesNet and AutoFormer only provide single-valued predictions for each time step, lacking the ability to capture the inherent uncertainty in time series data. \n\n\u25cf To further demonstrate the uncertainty quantification capability, we performed an experiment in Section G in the appendix of the updated paper. The synthetic ground truth is injected with different levels of noise. We output the 99% confidence interval along with the mean forecasts. One can observe that the model forecasts in the time steps with larger noise variance show wider confidence interval than those in the low variance time stpe. The uncertainty of the forecasts is consisitent with the level of the noise."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3526/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700327928032,
                "cdate": 1700327928032,
                "tmdate": 1700327928032,
                "mdate": 1700327928032,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "CdOi6AXZfi",
            "forum": "rGdEM131Ht",
            "replyto": "rGdEM131Ht",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3526/Reviewer_yqBZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3526/Reviewer_yqBZ"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a novel generative model called Time-Frequency fused Energy-Based Model (TF-EBM) for long-term probabilistic time series forecasting and imputation.\nTF-EBM is an encoder-only model that employs energy-based learning to construct an unnormalized probability density over temporal paths. This allows coherent long-term forecasting."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Originality:\n\nProposes a novel architecture combining energy-based models and time-frequency modeling for time series, which is an original contribution.\n\nLeverages energy-based learning in a new way for coherent long-term time series forecasting.\n\nPre-training approach for time series using TF-EBM is an original idea inspired by NLP models.\n\nQuality:\n\nComprehensive experiments across various forecasting and imputation tasks on multiple datasets.\n\nComparison to many strong baselines like DeepAR, Autoformer, etc. demonstrates quality.\n\nStrong performance especially on long-term forecasting shows effectiveness of the approach.\n\nAblation studies analyze the model components like dilated CNN and time-frequency modules.\n\nClarity:\n\nThe method and architecture are clearly explained with useful diagrams.\n\nThe related work section covers relevant literature on energy-based models, transformers, etc.\n\nExperiments are well-organized and different tasks nicely showcase model capabilities."
                },
                "weaknesses": {
                    "value": "The motivation for using energy-based learning specifically is not clearly articulated. References connecting energy-based models to time series properties could help.\n\nMore analysis on why the time-frequency modeling outperforms just time or just frequency features could strengthen this contribution.\n\nThe pre-training evaluation is limited. More exhaustive experiments on the transfer learning capabilities could be done.\n\nOnly univariate time series are evaluated. Comparing with PatchTST is needed in this setting.\n\nThe comparison to autoregressive models like LSTMs is missing. This could reveal advantages over common recurrent approaches.\n\nThe synthetic ablation study focuses on noise removal. Ablations on modeling long-term dependencies could be more insightful.\n\nAll datasets are regular time series. Applying to irregularly sampled data from healthcare etc. could reveal robustness.\n\nUncertainty estimation and calibration are not evaluated for the probabilistic forecasting. This could be an issue.\n\nHyperparameter tuning details are not provided clearly. It's unclear if suboptimal settings affect comparisons.\n\nThe advantages over previous energy-based time series methods like TimeGrad are not fully fleshed out."
                },
                "questions": {
                    "value": "See the weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "-"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3526/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3526/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3526/Reviewer_yqBZ"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3526/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698978911211,
            "cdate": 1698978911211,
            "tmdate": 1699636306409,
            "mdate": 1699636306409,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "s6EoYXF1Wo",
                "forum": "rGdEM131Ht",
                "replyto": "CdOi6AXZfi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3526/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3526/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer yqBZ(Part1)"
                    },
                    "comment": {
                        "value": "W1.The motivation for using energy-based learning specifically is not clearly articulated. References connecting energy-based models to time series properties could help.\n\n\u25cf Energy based model (EBM) has been widely applied in the image generation because EBM models high-dimentional joint distribution via a flexible unnormalized probability density benefitting deep model design. As for the time series, forecasting task, especailly for long-term forecast needs to model the long-term temporal coherence, and EBM can naturally serve as a powerful tool to model the joint distribution of the long-term correlation. There are limited number of papers applying EBM on time series forecasting works to the best of our knowledge. Previous EBM models used in time series forecasting including Time Grad/Score grad cited in the Related work section focus on modeling the join distribution across time series at each time step, while we utilize the EBM to model the joint distribution along the temporal path of the time series, which can guarantee the maximum likelihood of the whole sequence.\n\nW2.More analysis on why the time-frequency modeling outperforms just time or just frequency features could strengthen this contribution.\n\n\u25cf We consider the particularity of time series as seqence data which can be descirbed in the time and freqency demension. As we laid out in Section 3.5 and references therein, the time and frequency dimension mostly characterize the local and global patterns of the time series, respectivley, that are both crucial for long-term time series forecasting. Therefore, it is a natural choice to combine both patters to enhance the model performance.\n\n\u25cf We added additinoal ablation study on the exchange rate and traffic datasets with results included in Table 6 in the Appendix. The results on the full datasets show that combing both time and frequency features contribute better forecasting ability in our model design.\n\nW3.The pre-training evaluation is limited. More exhaustive experiments on the transfer learning capabilities could be done.\n\n\u25cf We evaluated the transfer learning capabilities of our model by pre-training it on the electricity dataset and fine-tuning it on the traffic and four ETT datasets. The results, summarized in Table 9, demonstrate that the fine-tuned model outperforms or matches the performance of other models.\n\nW4.Only univariate time series are evaluated. Comparing with PatchTST is needed in this setting.\n\n\u25cf As we mentioned in both the Introduction and the model framework section, unlike PatchTST, TF-EBM is an univaraite time-series model, and we will extend it to the multivaraite case as we mentioned in the Future work section. We wil compare with PathTST by that time. \n\nW5.The comparison to autoregressive models like LSTMs is missing. This could reveal advantages over common recurrent approaches.\n\n\u25cf We showed DeepAR model results in Table 1 for the CRPS metric. DeepAR is a strong autoregressive model based on LSTM. Additionally, in both Table 1 and 8, we added results from the TimeGrad model, which is a strong autoregressive EBM model.\n\n\u25cf We appended additional column in Table 3 for the MAE/MSE metric obtained using LSTM model. They are also taken directly from TimesNet paper.\n\n\u25cf TF-EBM outperforms both DeepAR and TimeGrad on most of the datasets."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3526/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700328089154,
                "cdate": 1700328089154,
                "tmdate": 1700328196039,
                "mdate": 1700328196039,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "qfoJZOY8i6",
            "forum": "rGdEM131Ht",
            "replyto": "rGdEM131Ht",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3526/Reviewer_5STj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3526/Reviewer_5STj"
            ],
            "content": {
                "summary": {
                    "value": "This manuscript describes a generative model that can be used for imputing or forecasting a univariate time series. The model's encoder block consists of a sequence of \"time-freq blocks\" (TFB). Within a single TFB, convolutional and MLP layers act on both the original input in the time domain, as well as the concatenated real and imaginary parts of a Fourier transform of those inputs. TFB are connected with residual connections. The decoder block consists of a series of convolutional layers followed by MLP layers. Model estimation was done via maximum likelihood by minimizing a measure of divergence between observed data samples and samples generated from the model -- though I will say that I am not familiar with the specific estimation methods used, and did not read them carefully. Experiments with forecasting and imputation tasks indicate that the model has performance that is generally comparable to or better than several recently published methods. An ablation study demonstrates that both the time and frequency features are useful."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "My understanding is that the primary contribution of this article is the proposal of time-frequency blocks as described in the summary, which allow the model to use information from both the time and frequency domain. Previous modeling approaches have also incorporated mechanisms for incorporating time and frequency domain features, but the specific architecture proposed here may be novel. I do not possess sufficient knowledge of the related literature to fully assess the significance of this contribution, but it has the feeling of an interesting idea that is worth being published. The article was fairly clear overall; I could implement a similar model based on the descriptions given, though I could not exactly reproduce it. I am confident that man of my questions along these lines could be addressed in a revision."
                },
                "weaknesses": {
                    "value": "The main weakness I see in the paper is that clear procedures for separating model development from the experiments evaluating model skill were not described. This leaves the reader with the impression that a clear \"model development\" and \"model evaluation\" data split was not made, and that claims about matching or exceeding state-of-the-art performance may not generalize to novel data sets. If careful procedures for evaluating the model on data that were not used for model development were indeed in place, it would be beneficial to describe them. If not, perhaps results could be given for new data sets?"
                },
                "questions": {
                    "value": "1. I found equations 6 and 7 to be helpful, but they did not seem to align precisely with Figure 1 and did not completely fill out all details of the model. A few specific questions are below:\n\n    a) The text just above Eq 6 defines the operator $g = MLP(SiLU(Conv1D(\\cdot)))$, while Figure 1 and earlier text indicate that Dilated Conv and Conv1d layers were used. Am I understanding correctly that the Dilated Conv and Conv layers in the figure represent the $g$ operator? If so, can this inconsistency be resolved? Otherwise, can the figure or text be amended to address this point of confusion?\n\n    b) I find the layout of the sub-blocks within the TFB on the left side of Figure 1 confusing. The inputs to the DC -> SiLU -> Conv1D block are the time feats or frequency feats, right? This is not apparent to me from the organization of the figure, which has arrows pointing from the DC -> SiLU -> Conv1D block to the time/freq feat modules. And why is there an arrow pointing up from the Conv1D block?\n\n    c) I believe from the figure that $y^l = y^{(l-1)} + TFB(y^{(l-1)} = y^{(l-1)} + f^l_{freq} + f^l_{time}$, but I am not fully confident in this and didn't see a clear statement of this in the text. It would be helpful to add a line to Eq 6 clearly stating how the output of a TFB is calculated.\n\n2. How were missing data values (e.g., as in imputation settings) handled in computation of the FFT?\n\n3. I don't understand the motivation for using a probabilistic score CRPS for a distributional forecast in the short-term forecasting problem, and the MAE/MSE for a point prediction in the long-term forecasting and imputation problems. It seems to me that it would be valuable to examine measures of point forecast skill and distributional forecast skill in both forecasting settings. This is particularly salient since the second sentence of the abstract reads, \"However, existing models primarily focus on deterministic point forecasts, neglecting generative long-term probabilistic forecasting and pre-training models across diverse time series analytic tasks, which are essential for uncertainty quantification and computational efficiency.\" The abstract states that long-term probabilistic forecasting is an important problem, but we do not see CRPS results for the long-term forecasting example. Could a more complete suite of results be added?\n\n4. The ablation study looking at the contribution of time and frequency extraction only looks at a subset of the data sets. I think it would be interesting to provide results for all data sets used in the paper, and to provide some examination to indicate settings in which it is more or less helpful to use both feature sets.  For example, could columns be added to Table 6 giving something like the lag 1 autocorrelation of the series and the power of the first few dominant frequencies? With the hypothesis being that the relative value of using both feature groups may depend on characteristics of the timer series being modeled?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3526/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3526/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3526/Reviewer_5STj"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3526/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698979603913,
            "cdate": 1698979603913,
            "tmdate": 1699636306296,
            "mdate": 1699636306296,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Ehl17DZje3",
                "forum": "rGdEM131Ht",
                "replyto": "qfoJZOY8i6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3526/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3526/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 5STj"
                    },
                    "comment": {
                        "value": "We thank the reviewer for offering the valuable feedback. We have addressed each of the concerns raised by the reviewer as below.\n\nW1.The main weakness I see in the paper is that clear procedures for separating model development from the experiments evaluating model skill were not described. \n\n\u25cf The datasets we used to evaluate our model performance are all public datasets that are widely available and heavily tested in other time series forecasting literature. We used the same dataset splitting ratio as in other published papers for training and testing data. \n\n\u25cf For the CRPS resutls in Table 2 and 8, we followed the same splitting ratio as in [1].\n\n\u25cf For all the MAE/MSE results in Table 3, 4, 5, 6, and 9, we used the same data preparation code released by the authors of the TimesNet paper: https://github.com/thuml/Time-Series-Library/blob/main/data_provider/data_loader.py\n\n\u25cf In order to evaluate model's performan on new datasets, we included the transfer learning experiments in the Appendix of the revised paper. To be specific, we pre-train our model on the electricity dataset, and fine tune it on traffic and four ETT datasets using just three epochs. The results are summarized in Table 9. The fine-tuning performance is still better than or comparable to other models. \n\n[1]Richard Kurle, Syama Sundar Rangapuram, Emmanuel de B\u00e9zenac, Stephan G\u00fcnnemann, and Jan Gasthaus. Deep rao-blackwellised particle filters for time series forecasting.Advances in Neural Information Processing Systems, 33:15371\u201315382, 2020.\n\nQ1.I found equations 6 and 7 to be helpful, but they did not seem to align precisely with Figure 1 and did not completely fill out all details of the model. A few specific questions are below:\n\n\u25cf a)The Dildated Conv and Conv layers shown inside the Time-Freq Block (TFB) in Figure 1 do not represent the g operator. Note that we named it as DSC (DilatedConv -> SiLU -> Conv1d) network in the updated paper to make it clearer. The g operator (g_time and g_freq) is only applied in the Time Feat Moudule and Frequency Feat Module in Figure 1 which is formalized in the Eq 6.  The Time-Freq Block shown in Figure 1 comprises two compents: one is the DSC network, and the other is the time-frequency (TF) domain features extracting network. The Figure 1, Eq.6 and Section 3.5 are all updated in the revised paper. \n\n\u25cf b)The inputs to the DSC network shown in the sub-blocks within TFB on the left side of Figure 1 are not the time feats or frequency feats. The inputs to the DSC network of the L-th layer are the sum of the inputs and outputs of DSC of the  (L-1)th layer (defined by Eq 5 in the updated paper). The inputs to the time and frequency feats are the outputs of the DSC network (red arrow in Figure 1). The outputs of the time and frequency feats are added together to produce the ultimate energy scalar defined by Eq 7. \n\n\u25cf c) We have named DilatedConv -> SiLU -> Conv1d as DSC in updated paper, so Eq 5 has been updated as \n$y^l=y^{l-1}+DSC(y^{l-1})$\nto clarify the statement.\n\nQ2.How were missing data values (e.g., as in imputation settings) handled in computation of the FFT?\n\n\u25cf For both imputation and forecasting tasks, missing values are replaced with random Gaussian noise prior to the FFT transform, followed by Langevin sampling to generate model-derived samples.\n\nQ3.The abstract states that long-term probabilistic forecasting is an important problem, but we do not see CRPS results for the long-term forecasting example. Could a more complete suite of results be added?\n\n\u25cf We included CRPS results for the long-term (forecast horizon equals 336 and 720) forecasting in Table 8 in the Appendix. We compared against three models: DeepAR, DeepState, and TImeGrad, but not the RSGLS-ISSM or ARSGLS because their code implementations were not publicly available. We did not test the Wiki dataset as it only has 792 data points that are not enough for the long-term forecast. \n\nQ4.The ablation study looking at the contribution of time and frequency extraction only looks at a subset of the data sets. I think it would be interesting to provide results for all data sets used in the paper.\n\n\u25cf We performed the same ablation study on the exchange rate and traffic datasets as well. The results are included in Table 6 in the Appendix, which also includes the average of the lag-1 autocorrelation, standard deviation, and four dominant power spectrums. However, such characteristics of the timer series does not seem to have strong correlation with the relative value of using both feature groups.  Actually,the model equipped with time and frequency feature extracting module performs the best on almost all datasets and forecasting ranges compared with only equipped with either time or frequency extracting module. The three exceptions on the ETTm1 and traffic datasets do not seem to have strong statistical significance."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3526/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700328994618,
                "cdate": 1700328994618,
                "tmdate": 1700328994618,
                "mdate": 1700328994618,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "kY5iFORfxG",
            "forum": "rGdEM131Ht",
            "replyto": "rGdEM131Ht",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3526/Reviewer_F51B"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3526/Reviewer_F51B"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a time-frequency fused parameterization of EBM for long-term time series modeling. The EBM defines a bottom-up mapping which is a encoder architecture as mentioned in this paper. The design of this encoder can be tricky. The author(s) leveraged a novel residual time-frequency block fuse information from time and frequency domain and hence got pretty good results in several benchmarks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The proposed method is simple and clear.\n\n2. The overall performance is good.\n\n3. The masked pretraining method seems interesting and may be used to scale-up time series training."
                },
                "weaknesses": {
                    "value": "The paper got good results in several benchmarks but lack of novelty in two ways. If the author claims to improve EBM based learning method in time series, the author should discuss more results about MCMC sampling results, e.g. the chain mixing problem. MCMC chain mixing is a well-known issue in non-sequential signals, it could be worse in time series. If the author claim the novel parameterization of EBM and specially designed fusion block (i.e. the important inductive bias for time series), I would suggest add more insightful ablation studies. There are many design choices for fusion. The author should clarify the benefits of current design choice.\n\nMeanwhile, the author missed some pioneering literature about basic EBM, such as,\n\n[1] \"A theory of generative convnet.\" International Conference on Machine Learning. PMLR, 2016.\n\n[2] \"Implicit generation and modeling with energy based models.\" Advances in Neural Information Processing Systems 32 (2019).\n\n[3] \"Cooperative training of descriptor and generator networks.\" IEEE transactions on pattern analysis and machine intelligence 42.1 (2018): 27-45.\n\nFor short-run Langevin dynamics learning of EBM, [4] is also an important work.\n\n[4] \"A tale of two flows: Cooperative learning of langevin flow and normalizing flow toward energy-based model.\" ICLR (2022)."
                },
                "questions": {
                    "value": "See weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3526/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3526/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3526/Reviewer_F51B"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3526/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699159307568,
            "cdate": 1699159307568,
            "tmdate": 1699636306216,
            "mdate": 1699636306216,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xoSLslinqs",
                "forum": "rGdEM131Ht",
                "replyto": "kY5iFORfxG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3526/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3526/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer F51B"
                    },
                    "comment": {
                        "value": "We thank the reviewer for offering the valuable feedback. We have addressed each of the concerns raised by the reviewer as below.\n\nW1.The paper got good results in several benchmarks but lack of novelty in two ways. If the author claims to improve EBM based learning method in time series, the author should discuss more results about MCMC sampling results, e.g. the chain mixing problem. MCMC chain mixing is a well-known issue in non-sequential signals, it could be worse in time series. If the author claim the novel parameterization of EBM and specially designed fusion block (i.e. the important inductive bias for time series), I would suggest add more insightful ablation studies. There are many design choices for fusion. The author should clarify the benefits of current design choice.\n\n\u25cf As explained in Section 3.2, we opted for the short-run MCMC technique due to its efficiency in accelerating the Langevin sampling process. Through our experimentation, we observed that even non-mixing chains have the capability to produce favorable outcomes, as demonstrated by the experiment results. \n\n\u25cf Our main novelty is choosing EBM as the backbone to model the long-term temporal dependence of time series. The existing energy-based models applied in time series fields like Time Grad/Score Grad model the joint distribution of multiple time series at each time step, and use an autoregressive structure like RNN to model the temporal dependence which can suffer from error accumulation. However, we use EBM to model the joint distribution of the whole temporal path of each time series ( $y_{1:T+\\tau}$) innovatively. During training and predicting process, we do not perform gradient update on the context$y_{1:T}$, but we only update the predicted values $y_{T+1:T+\\tau}$following the Langevin dynamics with calculated gradient. Therefore, the forecasts $y_{T+1:T+\\tau}$can be sampled at once benefiting the long-term forecasting, which is demonstrated in the extensive experiments.\n\n\u25cf The main goal to applying EBM to time series modeling is to get a accurate estimator for the probability density of the entire whole temporal path, in which the time and frequency fusion architecture plays an important role. In the image generation work, the density estimation network is directly applied on the raw pixel, however, it is different in the time series field because time series can be described in both time and frequency dimension. Our novelty here is to use the DSC network defined in Section 3.4 of the updated paper to extract different levels of information of the time series. Specifically, we apply a time and frequency feat extracting network to extract local and global features, respectively, which are then added together to generate the ultimate energy scalar. Additionally, the ablation study conducted in Section 4.4 and Appendix C further demonstrates the effectiveness of our chosen modeling approach. It is worth noting that alternative fusion methods like concatenation or gating may potentially yield even better results, and therefore, in our future work, we plan to thoroughly investigate and explore these alternatives.\n\n\u25cf In summary, our paper introduces a novel EBM-based encoder-only generative time series model that leverages fused time and frequency features. \n\n  \u25cb Our main novelty is to choose EBM as the backbone due to its flexibility as a general distribution modeling tool, as the energy function can be any scalar-returning function and does not need to integrate to 1, making it particularly well-suited for neural networks. Moreover, to our knowledge, there is a limited body of work applying EBM to time series analysis tasks. \n\n  \u25cb The model generates forecasts in one forward run unlike autoregressive models which suffers from inefficiency and error accumulation especially for long-term forecasts. \n\n  \u25cb The proposed versatile model demonstrates its effectiveness in various tasks, including short-term and long-term forecasting, imputation, pre-training, transfer learning (Appendix F), and uncertainty quantification (Appendix G). These extensive experiments validate the model's performance using the current model design. \n\n  \u25cb The model's pre-training capability makes TF-EBM a promising candidate for a time series foundation model to scale-up time series training.\n\nW2.Meanwhile, the author missed some pioneering literature about basic EBM\n\n\u25cf We have added the pioneering literature mentioned about basic EBM in the updated paper."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3526/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700328371206,
                "cdate": 1700328371206,
                "tmdate": 1700460950405,
                "mdate": 1700460950405,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]