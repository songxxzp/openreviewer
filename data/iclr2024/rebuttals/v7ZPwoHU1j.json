[
    {
        "title": "Statistically Optimal $K$-means Clustering via Nonnegative Low-rank Semidefinite Programming"
    },
    {
        "review": {
            "id": "BShDfuhdTy",
            "forum": "v7ZPwoHU1j",
            "replyto": "v7ZPwoHU1j",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission400/Reviewer_3o8f"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission400/Reviewer_3o8f"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes an NMF-like algorithm for the problem of k-means clustering. The benefit of an NMF algorithm is its simplicity and scalability, but at the same time achieve the same statistical optimality as proven for the SDP. \nThis is a clean, strong, and interesting contribution."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The problem is of course classical in machine learning and very important. \nThe authors propose a simple and strong algorithm for this problem, and prove statistical guarantees. \nThe paper is well-written and the proof seem clean to me."
                },
                "weaknesses": {
                    "value": "No evident weakness except the separation assumption, but I acknowledge that overcoming this assumption is difficult mathematically, and even with this assumption the derivations and ideas are not trivial."
                },
                "questions": {
                    "value": "I do not have important questions, but I wonder whether one can prove similar results for other distributions in (14)? I guess things will work out for sub-Gaussian distributions, but can you say something about the assumptions that are needed in your analysis in this sense?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission400/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698649347454,
            "cdate": 1698649347454,
            "tmdate": 1699635967005,
            "mdate": 1699635967005,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "hA1RY2gmo4",
                "forum": "v7ZPwoHU1j",
                "replyto": "BShDfuhdTy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission400/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission400/Authors"
                ],
                "content": {
                    "title": {
                        "value": "We thank you for your positive comments on our work and appreciate all your precious advice."
                    },
                    "comment": {
                        "value": "> Regarding the separation assumption:\n\nThank you for pointing that out! Our derivation relies on the fact that the optimum solutions of SDP and BM coincide under the separation assumption, which acts as the sufficient condition. Interestingly, in practice, we also observe linear convergence of BM even with small separation. Hence, we anticipate a similar derivation of convergence analysis under a weak separation assumption, which will be a focus of our future research. One main difficulty, however, lies in analyzing the SDP under partial recovery, for which a sharp bound does not currently exist in the literature to the best of our knowledge. We have incorporated this discussion in Appendix C of the revised document.\n\n> Regarding the theoretical results for other distributions:\n\nThat is a good point. The results of convergence analysis can be derived similarly for sub-Gaussian distributions, except that the separation condition would become weaker. The threshold of \nexact recovery for Gaussian distributions is based on the high dimensional concentration bounds __AND__ the rotation-invariance property of Gaussian. Therefore, similar threshold of \nexact recovery for sub-Gaussian distributions could be derived accordingly. However, we anticipate a larger separation condition for sub-Gaussian distributions as the high dimensional concentration bounds for sub-Gaussians might not as tight as the bounds for Gaussians. On the other hand, the noise level of sub-Gaussian distributions is higher than Gaussian distributions. Therefore, we need larger signal (separation) to cluster the distributions successfully."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission400/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700435609056,
                "cdate": 1700435609056,
                "tmdate": 1700435609056,
                "mdate": 1700435609056,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "nVdPDsRN19",
            "forum": "v7ZPwoHU1j",
            "replyto": "v7ZPwoHU1j",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission400/Reviewer_hAG7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission400/Reviewer_hAG7"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents an iterative method to (approximately) solve the k-means clustering problem. This method is obtained by applying existing methods (\"Burer\u2013Monteiro factorization approach\") to a particular reformulation of (a relaxed version) of k-means. \nThe authors claim that the resulting algorithm blends favorable computational and statistical guarantees of different existing methods for solving k-means."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Computationally efficient methods for solving k-means clustering, which is a core task of data analysis, are welcome."
                },
                "weaknesses": {
                    "value": "* It is unclear how Theorem 1 allows to verify the claims about computational and statistical superiority of Algorithm 1 compared to existing clustering methods. In particular, how does Theorem 1 and the numerical experiments imply the claims \"..simple and scalable as state-of-the- art NMF algorithms, while also enjoying the same strong statistical optimality..\" in the abstract?\n\n* there should be more discussion or comparison of computational complexity and clustering error incurred by Algorithm 1 compared to existing clustering methods for the Gaussian mixture model Eq. (14). \n\n* the connection between theoretical analysis in Section 4 and the num. exp. in Section 5 could be made more explicit. For example, there are not many references to the theoretical results in the current Section 5. How do the numerical results confirm Theorem 1? How did the theoretical analysis guide the design choices (datasets, hyperparams of Algorithm 1) of the numerical experiments. \n\n* the use of Algorithm 1 needs more discussion: How to choose beta, alpha and r in practice? How does Algorithm 1 deliver a cluster assignment that approximately solves k-means?\n\n* use of language can be improved, e.g.,\n--  \"..is the sharp threshold defined..\" what is a \"sharp\" threshold ?; \n-- \"..can be converted to the an equality-constrained..\"\n-- what is a \"manifold-like\" subset ? \n-- what is a \"is a nonconvex approach\" ? \n-- \"...by simultaneously leverage the implicit psd structure\" \n-- \".. that achieves the statistical optimality \" \n-- \"..which reparameterizes the assignment matrix .. as the psd membership matrix..\"\n--  what is a \".. one-shot encoding \"?"
                },
                "questions": {
                    "value": "see above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission400/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698775264023,
            "cdate": 1698775264023,
            "tmdate": 1699635966911,
            "mdate": 1699635966911,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "QFDp7d1StV",
                "forum": "v7ZPwoHU1j",
                "replyto": "nVdPDsRN19",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission400/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission400/Authors"
                ],
                "content": {
                    "title": {
                        "value": "We thank you for your comments on our work and appreciate all your helpful feedback."
                    },
                    "comment": {
                        "value": "> Regarding the question ``It is unclear how Theorem 1 allows to verify the claims...\":\n\nIn previous works, the SDP formulation of $K$-means was shown to have strong statistical guarantees, but it was solved using non-scalable algorithms like the primal-dual interior-point method. At a high level, our approach computes a solution for the same SDP formulation of $K$-means using a new scalable algorithm reminiscent of NMF. On one hand, our approach ''enjoys the same strong statistical optimality\" because it computes the same SDP solution as prior SDP approaches. On the other hand, each iteration of our proposed algorithm is essentially identical to NMF. Theorem 1 and our experiments establish that the algorithm rapidly converges to the SDP solution. Therefore, our proposed algorithm is indeed ``simple and scalable as state-of-the-art NMF\" for solving the SDP.\n\nAs mentioned in the last paragraph on page 6, the goal of our main theorem (Theorem 1) is ``to demonstrate that projected GD can efficiently solve the primal subproblem $\\min_{U\\in\\Omega} \\mathcal{L}_\\beta (U,y)$\", which exhibits rapid linear convergence locally. This fact, in conjunction with Prop. 1 (Existence and Quality of Primal Minimizer) and Prop. 2 (Linear Convergence of Dual Multipliers), indicates the local linear convergence of Algorithm 1. Here, our condition on initialization requires only a constant element-wise relative error bound.\n\nMoreover, the third plot in Figure 2 also indicates the linear convergence of our algorithm, as justified by our theoretical analysis. In this setting of large separation, the global optimums of the BM and SDP formulations align. Therefore, BM is superior compared to existing clustering methods in that it can achieve exact recovery like the SDP, while maintaining linear time complexity similar to KM, NMF, and SC. This fact is summarized in Figure 1 of the paper, where our algorithm achieves significantly smaller mis-clustering errors compared to existing state-of-the-art methods given the same CPU running time. Hence, we state in the abstract, ``The resulting algorithm is just as simple and scalable as state-of-the-art NMF algorithms, while also enjoying the same strong statistical optimality guarantees as the SDP\", a claim supported by both theoretical analysis and numerical results.\n\n> Regarding the question ``there should be more discussion or comparison of computational complexity...\":\n\nIn Section 5, we compared the computational complexity and mis-clustering errors of our algorithm with existing clustering methods for Gaussian mixture models, as detailed in the paragraph \"Performance for BM for GMM\". This analysis considers small separation with sample sizes ranging from $n=400$ to $n=57,600$. The results are summarized in the first two plots of Figure 2. In the last few sentences of the discussion and comparison, we note that the mis-clustering error of SDP and BM coincides, while NMF, KM, and SC show large variance and are far from achieving exact recovery, as evident from the first plot of Figure 2. The second plot of Figure 2, as mentioned, ``indicates that SDP and SC have super-linear time complexity, while the log-scale curves of our BM approach, KM, and NMF are nearly parallel, indicating that they all achieve linear time complexity\".\n\nFurthermore, in our revised version (see Appendix B), we have added comparisons of CPU time costs and mis-clustering errors with increasing dimension $p$ or with increasing cluster number $K$ across different methods. For more details, please refer to \"Reply to Reviewer v53M\".\n\n>Regarding the question ``the connection between theoretical analysis in Section 4 and the num. exp. in Section 5...\":\n\nThe convergence of Algorithm 1, accompanied by empirical results, is analyzed in the second experiment (Linear Convergence of BM) in Section 5. From the third plot in Figure 2, we observe that our BM approach achieves linear convergence in this setting. Additionally, the curves for $r = K$, $r = 2K$, and $r = 20K$ are closely aligned, suggesting that the choice of $r$ does not significantly impact the convergence rate under large (cluster) separation. Conversely, the theorem recommends a smaller step size $\\alpha$ and a larger augmentation parameter $\\beta$ for large sample sizes $n$. This guidance allows for the tuning of parameters for simple GMMs with small sample sizes, which can be adapted for applying BM to real datasets with varying sample sizes $n$."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission400/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700435257015,
                "cdate": 1700435257015,
                "tmdate": 1700435257015,
                "mdate": 1700435257015,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "oVuDaZLmJ5",
            "forum": "v7ZPwoHU1j",
            "replyto": "v7ZPwoHU1j",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission400/Reviewer_cacp"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission400/Reviewer_cacp"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a new approach to solving the K-means clustering problem, addressing the limitations of existing methods. The authors propose an efficient nonnegative matrix factorization-like algorithm, incorporating semidefinite programming (SDP) relaxations within an augmented Lagrangian framework. The algorithm optimizes a nonnegative factor matrix using primal-dual gradient descent ascent, ensuring rapid convergence and precise solutions to the challenging primal update problem. The method demonstrates strong statistical optimality guarantees comparable to SDP while being scalable and simple, similar to state-of-the-art NMF algorithms. Experimental results confirm significantly reduced mis-clustering errors compared to existing methods, marking a significant advancement in large-scale K-means clustering."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "(1) $\\textbf{New algorithm design}$: The paper introduces a novel algorithm for solving the K-means clustering problem, leveraging a combination of nonnegative matrix factorization (NMF) techniques and semidefinite programming (SDP) relaxations. The proposed algorithm addresses the challenges faced by prior methods and offers a unique solution approach by integrating concepts from different areas of machine learning.\n\n(2) $\\textbf{Theoretical grounding and guarantees}$: The authors provide a strong theoretical foundation for their algorithm, demonstrating local linear convergence within a primal-dual neighborhood of the SDP solution. The paper also offers rigorous proofs, such as the ability to solve the primal update problem at a rapid linear rate to machine precision. These theoretical insights establish the reliability and efficiency of the proposed method.\n\n(3) $\\textbf{Empirical validation}$: The paper supports its claims with empirical evidence, showcasing the effectiveness of the proposed algorithm through extensive experiments. The results demonstrate substantial improvements in terms of mis-clustering errors when compared to existing state-of-the-art methods. This empirical validation strengthens the credibility of the proposed approach and highlights its practical utility in real-world applications."
                },
                "weaknesses": {
                    "value": "$\\textbf{Insufficient discussion of practical limitations}$: The paper might not thoroughly address the practical limitations or challenges that users might face when applying the proposed algorithm in real-world scenarios. Understanding the algorithm's limitations in terms of computational resources, scalability, or specific data types is crucial for potential users and researchers.\n\n$\\textbf{Initialization condition}$: The authors base their proof of Theorem 1 on the assumption that the initialization meets a specific condition. While this assumption is discussed in the paper, it would significantly enhance the rigor and credibility of their work if the authors were to provide a rigorous proof for this initialization criterion."
                },
                "questions": {
                    "value": "1. Given the theoretical grounding and experimental results presented in the paper, how does the proposed algorithm compare to other state-of-the-art techniques in terms of computational efficiency and scalability, especially when dealing with large-scale datasets? \n\n2. It is a little abstract to understand Propositions 1 & 2. The authors should improve their presentation here."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission400/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission400/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission400/Reviewer_cacp"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission400/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698857689021,
            "cdate": 1698857689021,
            "tmdate": 1699635966837,
            "mdate": 1699635966837,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "DC6h1l5wRJ",
                "forum": "v7ZPwoHU1j",
                "replyto": "oVuDaZLmJ5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission400/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission400/Authors"
                ],
                "content": {
                    "title": {
                        "value": "We thank you for your positive comments on our work and appreciate all your valuable comments."
                    },
                    "comment": {
                        "value": "> Regarding the discussion of practical limitations:\n\nThank you for pointing that out! In practice, when applying our algorithm to datasets with very small separations (signal-to-noise ratio), our algorithm, like SDP and NMF, may fail to yield informative clustering results. We anticipate that this issue could be addressed with the application of different rounding procedures. Specifically, for cases with small separation where the solutions to BM, SDP, and NMF no longer represent exact membership assignments, it becomes crucial to consider and compare different rounding processes to extract informative membership assignments. We intend to pursue this as a future research goal. Due to space limitations, we have included a discussion on this topic in Appendix C of the revised document.  \nRegarding computational aspects, unlike SDP, our algorithm (BM) can be solved in linear time with respect to sample size $n,$ where parallel computing can be applied to further enhance the computational efficiency. On the other hand, BM formulation (eq.(6)) keeps the matrix $A$ (contains the information of dataset) from the SDP formulation (eq.(4)), which indicates that BM can deal with the same data types as those for SDP. Therefore we can adapt our BM formulation to manifold data, measure-valued data, or data with heterogeneous covariance in the GMM context.\n\n> Regarding the initialization conditions:\n\nThanks for your advice. The initialization criteria mentioned in Theorem 1 is derived from Theorem 3 in Appendix D.3., which provides a more rigorous form of the initialization criteria. Following your suggestion, we have added details in Appendix D.3 of the revised document to highlight the relationship between these two expressions of the initialization criteria.\n\n> Regarding the question ``Given the theoretical grounding and experimental results presented in the paper...\":\n\nAs detailed in Section 5, we compared the computational complexity and mis-clustering errors of our algorithm with existing clustering methods for Gaussian mixture models in the paragraph ``Performance for BM for GMM\". This comparison considers small separation with sample sizes ranging from $n=400$ to $n=57,600$. The results, summarized in the first two plots of Figure 2, reveal that the mis-clustering error of SDP and BM coincides, while NMF, KM ($K$-means++, the fastest clustering algorithm scalable to large datasets), and SC show large variance and fall short of exact recovery, as depicted in the first plot of Figure 2. The second plot in Figure 2, as mentioned, indicates that SDP and SC exhibit super-linear time complexity, while the log-scale curves of our BM approach, KM, and NMF are nearly parallel, suggesting that they all achieve linear time complexity.\n\nFurthermore, we have added comparisons of CPU time costs and mis-clustering errors with increasing dimensions $p$ or increasing cluster numbers $K$ across different methods in Appendix B of the revised document. For more details, please refer to ``Reply to Reviewer v53M\".\n\nOverall, our proposed algorithm achieves linear time complexity with respect to both sample size $n$ and dimension $p$, (as well as super-linear complexity with respect to \n$K$) which is comparable to other state-of-the-art algorithms such as $K$-means++ and NMF, while maintaining the same superior statistical performance as SDP.\n\n> Regarding the question ``It is a little abstract to understand Propositions 1 and 2...\":\n\nThank you for your advice. We have incorporated more details about Prop. 1 and Prop. 2 in the revised version. Due to space constraints, we have included additional explanations of Prop. 1 and Prop. 2 in Appendix A in the revised paper."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission400/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700434830514,
                "cdate": 1700434830514,
                "tmdate": 1700434830514,
                "mdate": 1700434830514,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "3YYnxmi2kl",
            "forum": "v7ZPwoHU1j",
            "replyto": "v7ZPwoHU1j",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission400/Reviewer_v53M"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission400/Reviewer_v53M"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a new algorithm for $k$-means problem in the Gaussian mixture model setting. The new algorithm overcomes some of the key limitations of prior algorithms for the same problem. Concretely, the SDP-based algorithm is not practical in settings where the datasets are large, and the NMF-based algorithm, despite its scalability, does not have theoretical guarantees. The new algorithm is inspired by these two methods and it is designed a way that it enjoys desirable properties of both the SDP and NMF-based methods. The authors show convergence guarantees in the exact recovery regime as well as the general setting for the algorithm and provide extensive numerical experiments that compare the new algorithm with prior approaches and demonstrate its performance."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "**Originality**\n\nThe clever use of projected gradient descent to solve the primal of augmented Lagrangian efficiently shows the originality of the algorithm. I also appreciate the way authors cast the relaxed SDP formulation as a non-convex optimization problem so that a method like Burer-Monteiro can be used to find the low-rank solution. Derivations of the problem and the proof techniques are non-trivial.\n\n**Quality**\n\nSolid theoretical results on problem formulation and the convergence of the algorithm. Numerical experiments are on point and demonstrate the theoretical guarantees.\n\n**Significance**\n\n$k$-means problem in GMM setting is an important problem for the community. A scalable solution for this problem that enjoys good theoretical guarantees is significant.\n\n**Clarity**\n\nThe paper is easy to follow. The contributions are clearly stated and the content is well organized."
                },
                "weaknesses": {
                    "value": "**Weaknesses**\n\nThe time complexity of solving the primal-dual algorithm is $O(K^6nr)$ and this becomes prohibitively large when $K$  is large. The experiments show small $K$ values(eg: $4$). Even for $K=10$, the time for convergence can grow very quickly. In some applications such as document deduplication, and entity resolution, the value of $K$ can be significantly larger than what is used in the experiments.  \n\n**Typos**\n\n1. On page 3, in the paragraph after equation $2$, \"one-shot encoding\" $\\rightarrow$ \"one-hot encoding\"."
                },
                "questions": {
                    "value": "1. What is the criterion to select the rank $r$ in the algorithm? Is it arbitrary or is there are heuristic for this?\n\n2. In experiments, I am noticing that the misclustering error of SDP is higher than this algorithm in general. Is it supposed to be like this? My understanding was SDP should have comparable or better accuracy than BM.  \n\n3. Can the authors compare the dependency of time complexity on $K$  for this algorithm and prior methods? Perhaps it is not clear for the NMF-based method but for the other methods discussed in the paper, it maybe possible. It is helpful to understand in what regimes this algorithm can be applied instead of others. I believe the dependency of Lloyd's initialized with $k$-means++ on $K$ is not as severe as this algorithm.\n\n4. Nowadays it is normal to see datasets with high dimensions. Numerical experiments in the paper use rather small values for dimension $p$(eg: $20$). Are there experiments done with higher dimensions, perhaps in the range $p=100, 500, 1000$? This will be helpful to determine how this algorithm performs in high dimensions compared to others."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission400/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699506349317,
            "cdate": 1699506349317,
            "tmdate": 1699635966757,
            "mdate": 1699635966757,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "r8td8KkMUI",
                "forum": "v7ZPwoHU1j",
                "replyto": "3YYnxmi2kl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission400/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission400/Authors"
                ],
                "content": {
                    "title": {
                        "value": "We thank you for your positive comments on our work and appreciate all your useful comments."
                    },
                    "comment": {
                        "value": "> Regarding the time complexity with respect to number of clusters $K$:\n\nWe acknowledge that the dependence on $K$ in our theoretical analysis is very likely not sharp. Our added simulation results suggest that the dependence of our runtime on $K$ appears to be of linear order $O(K)$, which is much better than $O(K^6)$ in our theoretical guarantee. To compare the CPU time costs and mis-clustering errors when the number of clusters increases across different methods, we did the comparisons of CPU time cost across different methods when $K=5,10,20,40,50$ in the revision. Under the same setting as our second experiment (``Performance for BM for GMM\") in Section 5, except that now we consider fixed sample size $n=1000$, dimension $p=50$. SC is not considered as it would fail for large $K$ in our experiments. The results are summarized in the first plot of Figure 6 in Appendix B in the revision, where we can observe that the log-scale curve of BM is nearly parallel to the log-scale curve of KM and NMF, indicating that the growth of CPU time cost for BM with respect to $K$ is reasonable (nearly $O(K)$) and would not achieve the loose upper bound $O(K^6)$ derived from the analysis. The curve of computational time cost for SDP is relatively stable for different $K$ since the dominant term of computational complexity for SDP is the sample size $n$, which is as large as the order $O(n^{3.5})$. More details can be found in Appendix B in the revision.\n\nOn the theoretical side, it is common to treat the cluster number $K$ as constant. Indeed, the best known theoretical guarantee for exact recovery via SDP necessarily requires a small value of $K=O(\\log(n)/\\log\\log(n))$. So the time complexity bound for our BM is at most $\\text{polylog}(n)$. It is a widely open conjecture whether a sharp threshold exists without a statistical--computational gap when $K \\gg \\log(n)$. For these reasons, we did not attempt to optimize the dependence on $K$ in the theory in our paper.\n\n> Regarding ``one-shot encoding\":\n\nThanks for pointing out. We have corrected it in the revision. \n\n> Regarding the selection of the rank $r$ in the algorithm:\n\nThe second numerical experiment in the paper demonstrates that the choice of $r$ does not significantly impact the convergence rate. Therefore, ideally, we would choose the rank $r$ as small as $r=K$. However, in practice, we found that $r=K$ results in many local minima for small separation. Consequently, we slightly increase $r$ and choose $r=2K$ for all applications, which provided overall good performance. More details regarding the choices of parameters have been added to Appendix A in the revision.\n\nIn real applications, the number of clusters $K$ might be unknown as well. In these situations, we can adopt some common methods recommended in the literature for $K$-means to choose $K$. For example, we may use the popular elbow method, which first runs fast clustering algorithms such as the $K$-means++ for a range of values of $k$ (number of clusters) and then plots the total within-cluster sum of square against $k$. After that, we may choose a smallest value of $k$ that maintains a relatively low sum of square of the distances.\n\n> Regarding the question ``In experiments, I am noticing that the misclustering error of SDP...\":\n\nThat is a valid point. On one hand, Table 1 shows that BM yields slightly better mis-clustering errors compared to SDP. However, the differences are not significant since the error bars for BM and SDP overlap. On the other hand, it is noteworthy that BM imposes a more stringent constraint, $U \\ge 0$, compared to SDP's constraint of $UU^T \\ge 0$. This makes BM a tighter formulation relative to the original K-means formulation. Consequently, it is possible for BM to provide a better approximation to the $K$-means formulation, especially when the separation is small."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission400/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700434267793,
                "cdate": 1700434267793,
                "tmdate": 1700434267793,
                "mdate": 1700434267793,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aFRe69ClzP",
                "forum": "v7ZPwoHU1j",
                "replyto": "cSEm5J7Kpu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission400/Reviewer_v53M"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission400/Reviewer_v53M"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for the clarifications and more details. I agree with the points you made on $K$. \nI maintain my score after reading authors comments."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission400/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700683288351,
                "cdate": 1700683288351,
                "tmdate": 1700683288351,
                "mdate": 1700683288351,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]