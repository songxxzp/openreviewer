[
    {
        "title": "Diffusion-TS: Interpretable Diffusion for General Time Series Generation"
    },
    {
        "review": {
            "id": "oIYe60v7f0",
            "forum": "4h1apFjO99",
            "replyto": "4h1apFjO99",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2899/Reviewer_iyrP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2899/Reviewer_iyrP"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose Diffusion-TS, a diffusion method for multivariate time series generation. The method involves an interpretable decomposition into trend and seasonality components. The diffusion model is trained using a joint L2 and Fourier loss, and the authors apply conditional generation techniques to apply the method to forecasting and imputation directly without changes in the model. Experiments show Diffusion-TS achieves state-of-the-art performance on time series datasets, including in an irregularly sampled setting."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Originality:\n- As far as the reviewer is aware, the authors are the first to combine a trend-seasonality decomposition technique with diffusion for time series generation. The authors mention this choice leads to improved performance across the various evaluation metrics.\n\nQuality:\n- The experiments are generally well-constructed. A variety of datasets and evaluation metrics are used for standard unconditional generation, and several ablation studies are included in the appendix.\n- The authors show qualitative results showing the trend and seasonality components predicted by the model generally behave as intended.\n\nClarity:\n- The paper is generally well-written, and the experiments are described clearly.\n\nSignificance:\n- The experimental results are quite compelling. In the unconditional time series generation settings (Table 1), Diffusion-TS outperforms existing methods. The performance gap is especially clear for higher-dimensional datasets and long-term generation (Table 2)."
                },
                "weaknesses": {
                    "value": "- Generally, it's a bit unclear to the reviewer which design choices are crucial to the improvement in performance, especially the trend-seasonality decomposition. Two detailed techniques are involved in implementing the decomposition, the polynomial regressor and the Fourier synthetic layers, but it's unclear to what extent these techniques contribute to the model performance.\n- It's a bit unclear how compelling the qualitative results are. For example, in Figure 3, the trend components seem quite uninformative compared to the season/error components. In Figure 4, it's unclear whether the t-SNE visualizations support the claim that the Diffusion-TS distribution better aligns with the data distribution than TimeGAN distribution.\n- Experimental metrics reported are inconsistent. For example, correlational and context-FID scores are reported in Table 2, and discriminative and predictive scores are reported in Figure 7 and Appendix C.1/C.2."
                },
                "questions": {
                    "value": "Main:\n- In Appendix C.2, what is the \"w/o interpretability\" model? Is this equivalent to $x_0(x_t, t, \\theta) = R$ in Eqn 7, setting the $V_{tr}^t$ and $S_{i, t}$ terms to $0$? How crucial are the polynomial regressor and Fourier synthetic layer techniques?\n- How well does the trend-seasonality decomposition method of Diffusion-TS achieve what's intended for the toy model (Eqn 1), for example on synthetic data?\n- How do discriminative and predictive scores compare on the long-term time series generation task (Table 2)?\n- For the long-term generation task (Table 2), why is it that Diffusion-TS seems to perform generally better with longer time series? It seems like longer time series generation should be a more difficult task.\n- In Figure 6, is MAE computed only over missing data (imputation targets) or over the full time series including existing data? For Diffusion-TS-G, since a soft constraint is used to enforce the conditional generation, how closely do the generated time series for imputation/forecasting match the existing data?\n- How does the number of parameters compare between Diffusion-TS and its competitors?\n\nClarifications:\n- How is the data in Figure 3 generated? Is column b the ground truth and column a the result of adding noise?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2899/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2899/Reviewer_iyrP",
                        "ICLR.cc/2024/Conference/Submission2899/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2899/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698677208319,
            "cdate": 1698677208319,
            "tmdate": 1700710485974,
            "mdate": 1700710485974,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5e30XSmnb6",
                "forum": "4h1apFjO99",
                "replyto": "oIYe60v7f0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2899/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2899/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reponse to Reviewer iyrP (Part 1)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the comments. We address individual concerns below.\n\n> Generally, it's a bit unclear to the reviewer which design choices are crucial to the improvement in performance, especially the trend-seasonality decomposition. How crucial are the polynomial regressor and Fourier synthetic layer techniques?\n\nTable 9 in Appendix C.7 has reported the ablations for the components of trend & seasonality and Fourier regularization on 24-length time series. When the dataset, i.e. fMRI, has a high frequency and dimension, a network with interpretable design achieves the most significant performance improvement. For validating a more explict improvement, we added the ablation study in long sequence infilling on MuJoCo dataset to demonstrate that each disentanglement plays an important role in improving the performance on generative tasks. (This has been included as Appendix C.7 of the revised paper)\n\n|Model|$70 \\%$ Missing|$80 \\%$ Missing|$90 \\%$ Missing|\n|:--:|:--:|:--:|:--:|\n|Residual|0.51(1)|0.59(7)|0.85(10)|\n|Residual+Season|0.45(5)|0.52(3)|0.77(9)|\n|Residual+Trend|0.46(2)|0.50(5)|0.80(7)|\n|Season+Trend|0.63(3)|1.05(6)|1.42(10)|\n|Diffusion-TS|__0.37(3)__|__0.43(3)__|__0.73(12)__|\n\n> In Figure 3, the trend components seem quite uninformative compared to the season/error components. \n\nFirst, the constraints imposed by polynomial regressor are very strong to ensure slow-varying trend components. The same phenomenon can be found in the experimental part of NBEATS [1]. Second, juggling various noisy data in diffusion models exacerbates the information deficit. In our early experiments, we also tried to use moving average to model trends like Autoformer [2] and Fedformer [3], but this leads to worse quality of generated samples. However, the ''uninformative'' curve does not hurt since we are only considering coarse-grained trends.\n\n> How well does the trend-seasonality decomposition method of Diffusion-TS achieve what's intended for the toy model (Eqn 1), for example on synthetic data?\n\nThank you very much for your comments. To better validate the toy model, we have added an extra interpretability study on synthetic data in Appendix C.5 of the revised paper. Through experiments, we can clearly find that the learned disentangled components are very similar to the ground truth. This verifies the interpretability of our proposed method.\n\n> In Figure 4, it's unclear whether the t-SNE visualizations support the claim that the Diffusion-TS distribution better aligns with the data distribution than TimeGAN distribution.\n\nActually in Figure 4, we can see that the 2-dimensional points from our generated samples are distributed more evenly than that of TimeGAN\u2019s samples. That means the samples generated by our method more coincide with the ground truth. To better visualize the superiority of our method, we  also added the PCA and t-SNE visualizations of long-term series data on ETTh data set in the Appendix C.2 of the revised paper. In these figures, the synthetic samples generated by Diffusion-TS show significant superiority than TimeGAN.\n\n> Experimental metrics reported are inconsistent. \n\nIn the revised paper, we added the results on the complete metrics in the Appendix C.1. We also supplemented the 24-sequence ablation experiment in the Appendix C.7 to ensure the consistency of experimental metrics.\n\n> In Appendix C.2, what is the \"w/o interpretability\" model? Is this equivalent to $R$, setting the $V_{tr}^t$ and $S_{i,t}$?\n\nYes, the \"w/o interpretability\" model remove both the polynomial regressor and Fourier synthetic layer.\n\n> For the long-term generation task (Table 2), why is it that Diffusion-TS seems to perform generally better with longer time series? It seems like longer time series generation should be a more difficult task.\n\nThank you for raising this question. First of all, the self-attention mechanism of Transformer naturally supports long sequence modeling. The similar results that the attention-based models perform better in longer time series generation were also reported in PSA-GAN [4]. In addition, the trend-seasonal decomposition applied in our framework is more suitable for processing long time series, as both trend and seasonality are long-term time properties.\n\nreferences:\n\n[1] N-beats: Neural basis expansion analysis for interpretable time series forecasting, 2020.\n\n[2] Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting. Advances in Neural Information Processing Systems, 34:22419\u201322430, 2021.\n\n[3] Fedformer: Frequency enhanced decomposed transformer for long-term series forecasting, 2022.\n\n[4] Psa-gan: Progressive self attention gans for synthetic time series, 2022."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2899/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700468315638,
                "cdate": 1700468315638,
                "tmdate": 1700468315638,
                "mdate": 1700468315638,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UYOdmProUq",
                "forum": "4h1apFjO99",
                "replyto": "LmVnnhE6Vd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2899/Reviewer_iyrP"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2899/Reviewer_iyrP"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the detailed response. The additional details and experiments are compelling and help clarify the role of the season/trend components. Therefore I'm willing to raise my score to 6."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2899/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700710459364,
                "cdate": 1700710459364,
                "tmdate": 1700710459364,
                "mdate": 1700710459364,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "J7GX7qryNo",
            "forum": "4h1apFjO99",
            "replyto": "4h1apFjO99",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2899/Reviewer_Prro"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2899/Reviewer_Prro"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a diffusion model for time series generation. The model involves a custom architecture with components that help synthesizing the trend and seasonal components of the time series. The model is trained to directly estimate the observation from arbitrary diffusion steps. The objective function of the diffusion model is also modified to incorporate a loss based on fourier transform of the time series. Experiments show improved performance over existing generative models in terms of different generative metrics."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The model explores both unconditional and conditional generation using a single model.\n- The model performs better than baseline models in terms of time series generation."
                },
                "weaknesses": {
                    "value": "- The claim \"little attention has been given to leveraging the powerful generative ability for general time series production\" is not entirely correct. See [1] and [2], for example. Furthermore, in the light of [2], the claim \"... **first** DDPM-based framework for both unconditional and conditional time series synthesis\" is not correct either. [2] proposes a self-guidance mechanism to use unconditional diffusion models for conditional time series tasks and also studies the unconditional generative properties of the model. This limits the novelty of the paper and Sec. 3.4, in particular.\n- The paper is poorly written and does not tell a coherent story. Sec. 3.2 reads like an arbitrary combination of ideas. The individual components are also not analyzed later via ablations. The manuscript also has many typographical errors (synthetic instead of synthesis, DSDI instead of CSDI).\n- The evaluation on the conditional tasks is limited. The model is only compared against Diffwave and CSDI (which is fairly close to Diffwave) and baselines from time series forecasting literature are missing. It is also unclear how these CSDI and Diffwave baselines were trained. \n\n\n\n[1] Lim, Haksoo, et al. \"Regular Time-series Generation using SGM.\" arXiv preprint arXiv:2301.08518 (2023).      \n[2] Kollovieh, Marcel, et al. \"Predict, refine, synthesize: Self-guiding diffusion models for probabilistic time series forecasting.\" arXiv preprint arXiv:2307.11494 (2023)."
                },
                "questions": {
                    "value": "- What is the empirical significance of the trend & seasonality synthesis blocks and the fourier regularization? Did the authors conduct ablations for these components? (Check the results in [1] which doesn't use any of these components)\n- Why does Diffwave perform so much worse in the case of long time series generation? The sequence lengths considered in this work are smaller than those used in audio synthesis. Is it possible that there is a bug in the experiments setup? \n    - Why did not authors not compare against a CSDI-like model for unconditional generation? That would probably be a better comparison and ablation for the components proposed in this work. \n\n[1] Lim, Haksoo, et al. \"Regular Time-series Generation using SGM.\" arXiv preprint arXiv:2301.08518 (2023)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2899/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2899/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2899/Reviewer_Prro"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2899/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699045774373,
            "cdate": 1699045774373,
            "tmdate": 1700735752829,
            "mdate": 1700735752829,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "UzQT1BHafE",
                "forum": "4h1apFjO99",
                "replyto": "J7GX7qryNo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2899/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2899/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reponse to Reviewer Prro (Part 1)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the comments. We address individual concerns below.\n\n> The claim \"little attention has been given to leveraging the powerful generative ability for general time series production\" is not entirely correct. The claim \"... first DDPM-based framework for both unconditional and conditional time series synthesis\" is not correct either.\n\nWe apologize for the unprecise expression. Thanks for the reviewer for providing more concurrent work. We modified the introduction and related work in the revised paper to embrace these work, and highlighted the differences between the recent proposed DDPM-based framework and the work proposed in our paper. Particularly, our method is expressly focused on the problem of __general__ time series generation, which not only represents (un)conditional time series synthesis, but also represents high (low) dimension and long (short) time series synthesis. For the framework proposed in [1], it focused on univariate time series using a modification of DiffWave with S4 layers, which was designed for single-channel audio data. While the framework proposed in [2] was only able to generate unconditional time series of length 24, and it is hardly to extended to long-term series generation Besides, it was quite resource-intensive. For example, it requires 3318.99s (255 times of ours: 13s) for sampling 1000 Stock sequences and 1620.84s (49 times of ours: 33s) for Energy sequences.\n\n> The paper is poorly written and does not tell a coherent story. Sec. 3.2 reads like an arbitrary combination of ideas. The manuscript also has many typographical errors (synthetic instead of synthesis, DSDI instead of CSDI).\n\nThanks very much for pointing out this issue.. We have improved the organization and fixed typographical errors to enhance the readability of our paper. In the introduction, we present more motivations and highlight several aspects in which our decomposition architecture performs more favorably in terms of interpretability and effectiveness We reorganized Section 3, especially Section 3.2, to emphasize the rationality of our ideas.\n\n> The evaluation on the conditional tasks is limited. \n\nWe agree that comparing our method to additional baselines on the conditional tasks will further confirm that our approach is relevant in practice. To demonstrate that the performance of Diffusion-TS, we repeat time series imputation and forecasting experiments in SSSD [3]. We report an averaged MSE for a single imputation per sample on the MuJoCo data set of length 100. The results are shown in table below.\n\n|Model|$70 \\%$ Missing|$80 \\%$ Missing|$90 \\%$ Missing|\n|:--:|:--:|:--:|:--:|\n|RNN GRU-D|11.34|14.21|19.68|\n|ODE-RNN|9.86|12.09|16.47|\n|NeuralCDE|8.35|10.71|13.52|\n|Latent-ODE|3|2.95|3.6|\n|NAOMI|1.46|2.32|4.42|\n|NRTSI|0.63|1.22|4.06|\n|CSDI|__0.24(3)__|0.61(10)|4.84(2)|\n|SSSD|0.59(8)|1.00(5)|1.90(3)|\n|Diffusion-TS|0.37(3)|__0.43(3)__|__0.73(12)__|\n\nThen we test on the Solar data set collected from GluonTS [4], a forecasting task where the conditional values and forecast horizon are 168 and 24 time steps respectively. The results are shown in table below. We can see our model still performs well as it has the best performance against the baselines. (This has been included as Appendix C.3 of the revised paper).\n\n|Model|MSE|\n|:--:|:--:|\n|GP-copula|9.8e2\u00b15.2e1|\n|TransMAF|9.30e2|\n|TLAE|6.8e2\u00b17.5e1|\n|CSDI|9.0e2\u00b16.1e1|\n|SSSD|5.03e2\u00b11.06e1|\n|Diffusion-TS|__3.75e2\u00b13.6e1__|\n\nReferences:\n\n[1] Predict, refine, synthesize: Self-guiding diffusion models for probabilistic\ntime series forecasting. arXiv preprint arXiv:2307.11494, 2023.\n\n[2] Regular time-series generation using sgm. arXiv preprint arXiv:2301.08518, 2023.\n\n[3] Diffusion-based time series imputation and forecasting with structured state space models. arXiv preprint arXiv:2208.09399, 2022.\n\n[4] Gluonts: Probabilistic and neural time series modeling in python. The Journal of\nMachine Learning Research, 21(1):4629\u20134634, 2020."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2899/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700467695139,
                "cdate": 1700467695139,
                "tmdate": 1700468100065,
                "mdate": 1700468100065,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "778OdbakNV",
                "forum": "4h1apFjO99",
                "replyto": "UzQT1BHafE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2899/Reviewer_Prro"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2899/Reviewer_Prro"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for updating the claims in the introduction. However, I do feel that the writing is still imprecise. For instance, \"Meanwhile, the rare work on unconditional time-related synthesis with diffusion models (Kong et al., 2021; Kollovieh et al., 2023; Lim et al., 2023) still struggle in synthesizing high-dimension or long time series.\" lumps univariate and multivariate models together and \"We propose Diffusion-TS, the first generative framework to combine seasonal-trend decomposition techniques with diffusion models\" is too specific of a thing to claim. I hope these things can be improved in the revision.\n\nThank you for the new results on the conditional tasks. They look promising."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2899/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700662382116,
                "cdate": 1700662382116,
                "tmdate": 1700662382116,
                "mdate": 1700662382116,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ffzqlCq4Gf",
                "forum": "4h1apFjO99",
                "replyto": "nK0ueuHpHO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2899/Reviewer_Prro"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2899/Reviewer_Prro"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for pointing out the table in the Appendix. It would be great if the discussion on the components can be improved in the main text. \n\nThank you for results on CSDI. I don't think it's an unfair comparison. It's just another diffusion model that can also be trained unconditionally. The performance in terms on the predictive score is pretty close which begs the point about the contributions of this paper, e.g., in terms of the decomposition and the Fourier loss."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2899/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700662689369,
                "cdate": 1700662689369,
                "tmdate": 1700662689369,
                "mdate": 1700662689369,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7LizzYuZxe",
                "forum": "4h1apFjO99",
                "replyto": "J7GX7qryNo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2899/Reviewer_Prro"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2899/Reviewer_Prro"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the update. I am raising my score to 5.\n\nMinor comment: What I meant by imprecise is saying \"the rare work on unconditional time-related synthesis with diffusion models still **struggle in synthesizing high-dimension** (Kong et al., 2021; Kollovieh et al., 2023)\" when these papers actually do not claim to be be applicable on high-dimenisonal time series. It would be more precise to say something like \"focus on low-dimensional or univariate time series\"."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2899/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700735726180,
                "cdate": 1700735726180,
                "tmdate": 1700735737592,
                "mdate": 1700735737592,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "sBP16wrtlQ",
            "forum": "4h1apFjO99",
            "replyto": "4h1apFjO99",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2899/Reviewer_bNeL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2899/Reviewer_bNeL"
            ],
            "content": {
                "summary": {
                    "value": "A transformer based denoising diffusion probabilistic model, Diffusion-TS, is proposed for generating multivariate time series. \nRepresentations from Oreshkin 2020, Desai 2021, De Livera 2011 and Woo 2022 are adopted for trend and seasonality. Under time series model (1) and Fourier-based loss term following similar work as Ho 2020 and Fons 2022, the \\hat{x}_{0} is estimated directly for unconditional time series. \nThe proposed method Diffusion-TS is compared with four other models TimeGAN 2019, TimeVAE 2021, Diffwave 2021, and Cot-Gan 2020 under 6 datasets (stock price, electricity transformer, etc. The proposed method works better than the other four methods most frequently under the evaluation rules suggested by Yoon 2019, Paul 2022 and Ni 2020 especially under high-dimensional datasets.\nExtension to conditional model by adding gradients, as well as simulation results, are presented."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This idea and algorithm of the proposed method are generally well presented. It\u2019s compared with multiple recent time series generation methods and simulation results outperform these in most cases."
                },
                "weaknesses": {
                    "value": "Potentially the proposed method works well or even better than other existing methods under typical seasonal time series data. Pending further exploration/confirmation how well the performance the proposed method can be under times series of real data with different unique features like big jump in stock price. Also wondering the hyper-parameter selection impact on the result and convergence speed."
                },
                "questions": {
                    "value": "P4, Please define similarly as in Woo (2022) your \\omega_seas or at least add definition by English. \nP4, Equation (4), please either split the A and \\Phi definition in two rows or write as a vector, so would not be confused as compound operators. \nP5, Could you clarify the Figure 2, \u2018Decoder Block 1\u2019, ..., \u2018Decoder Block N\u2019 each refers to under the N samples (i=1, ... ,N) of time-series signals on P2 or something else considering the squares/arrows in that figure. \nP17, Can the dataset names or links be more specific? There\u2019re multiple datasets available under some links.  \nP6 and P18, Not questioning the vitality of your result, just want to have a better understanding of the performance to be expected under your method for other potential new runs: \n\uf06cHow different Table 1 results can be if you did try other hyper-parameters?    \nPrediction under jump model can be hard. How well the performance of your model can be when there\u2019s big jump in observed data? (The Google stock data used is from 2004 to 2019, the big changes in 2020 and 2022 not included. The electricity transformer temperature and blood oxygen related data might be typical stable time series. I\u2019m not familiar with MujoCo data.)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2899/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699252436723,
            "cdate": 1699252436723,
            "tmdate": 1699636233269,
            "mdate": 1699636233269,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "yr4c8kEePG",
                "forum": "4h1apFjO99",
                "replyto": "sBP16wrtlQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2899/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2899/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reponse to Reviewer bNeL"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the comments. We address individual concerns below.\n\n> Please define similarly as in Woo (2022) your \\omega_seas or at least add definition by English. Equation (4), please either split the A and \\Phi definition in two rows or write as a vector, so would not be confused as compound operators. \n\nWe thank the reviewer for the suggestion. We have moved the definition of $w_{(seas)}^{i,t}$ and $w_{(tr)}^{i,t}$ before the discussion in Section 3.2, and split Equation 4 to two equations.\n\n> Could you clarify the Figure 2, \u2018Decoder Block 1\u2019, ..., \u2018Decoder Block N\u2019 each refers to under the N samples (i=1, ... ,N) of time-series signals on P2 or something else considering the squares/arrows in that figure.\n\nWe apologize for the lack of clarity in the description of the generative method. Here, N in Figure 2 only represents the number of residual blocks of deep decomposition architecture in the decoder. Therefore, there is no relationship between it and the number of samples appearing on P2. To avoid ambiguity, we change N to another letter K in the figure to avoid the confusion.\n\n> Can the dataset names or links be more specific? There\u2019re multiple datasets available under some links.\n\nSince Sines and MuJoCo are synthetic data sets, we only provide the link to the source code. Additionally, we corrected the dataset link for ETTh data set.\n\n> Wondering the hyper-parameter selection impact on the result and convergence speed.\n\nWe did limited hyperparameter tuning in this study to find default hyperparemters that perform\nwell across datasets. Due to the time limitation, we were not able to repeat all experiments multiple times. To better illustrate, we added an extra hyperparameter tuning study in Appendix C.6 of the revised paper and will release more experimental results upon the acception. Furthermore, we have also added the impact of the scale parameter $\\gamma$ in that subsection:\n\n|$\\gamma$|$70 \\%$ Missing|$80 \\%$ Missing|$90 \\%$ Missing|\n|:--:|:--:|:--:|:--:|\n|1.|2.8(13)|4.1(10)|6.8(17)|\n|1e-1|__0.37(4)__|0.45(0)|0.82(9)|\n|5e-2|__0.37(3)__|__0.43(3)__|__0.73(12)__|\n|1e-2|0.60(5)|0.70(10)|1.07(14)|\n|1e-3|3.1(8)|7.2(20)|19.6(22)|\n\nWith perceptual qualities superior to GANs while avoiding the optimization challenges of adversarial training, the convergence speed of the diffusion model is not greatly affected by the hyper-parameters. And we compare the time to train Diffusion-TS with TimeGAN in the table below.\n\n|Model|Sines|Stocks|Energy|\n|:--:|:--:|:--:|:--:|\n|TimeGAN|176(min)|179(min)|217(min)|\n|Diffusion-TS|17(min)|15(min)|60(min)|\n\n> How well the performance of your model can be when there\u2019s big jump in observed data?\n\nWe thank the reviewer for this comment. We agree that further exploration/confirmation how well the performance the method can be under times series of real data with different unique features is a meaningful direction of work. We collected Google stock data from 2004 to 2022, and used data from 2004 to 2019 as the training set. Then we conducted predictions on the remaining data set (See Figure 11 in Appendix C.3). Although the accuracy of the results is relatively good, what we mainly discuss in this article is addressing the gap between time series generation (decomposition) and diffusion models. Thus we do not think this issue is the focus of our work and will treat it as a work in the near future."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2899/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700467431468,
                "cdate": 1700467431468,
                "tmdate": 1700467431468,
                "mdate": 1700467431468,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]