[
    {
        "title": "MMD Graph Kernel: Effective Metric Learning for Graphs via Maximum Mean Discrepancy"
    },
    {
        "review": {
            "id": "AaNayDAUzb",
            "forum": "GZ6AcZwA8r",
            "replyto": "GZ6AcZwA8r",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3490/Reviewer_CP9G"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3490/Reviewer_CP9G"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduced a class of MMD based graph kernels. It also introduced a class of deep MMD graph kernels that have learnable parameters. Some theoretical analysis such as robustness have been done. The experiments of graph clustering and classification showed the effectiveness of the proposed methods."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1. Measuring the similarity between graphs is a practical and challenging problem.\n2. The proposed shallow and deep MMD graph kernels are novel and interesting. The deep MMD graph kernel is learnable in both supervised and unsupervised ways. \n3. The paper studied the robustness and generalization of the graph kernels. \n4. In the experiments, the proposed deep MMD graph kernel is more effective than classical graph kernels and graph neural networks. The superior performance in clustering is quite impressive.\nIn sum, this is a good work with solid theoretical analysis and numerical evaluation. The unsupervised deep MMD graph kernel makes a breakthrough in the area."
                },
                "weaknesses": {
                    "value": "The paper has the following weaknesses.. \n1. The motivation of the two unsupervised loss functions (eq.10 and eq.11) in Section 4 haven\u2019t been sufficiently explained. \n2. For graph clustering, GNN-based clustering method should be compared in Table 1."
                },
                "questions": {
                    "value": "It would be great if the authors could improve the aforementioned weaknesses and answer the following questions.\n1. In Definition 5, for the generalized MMD, how to efficiently determine $k$ in practice? It may have infinite choices.\n2. In Theorem 1, it seems that if $h$ is close to zero, the bound is loose. How to avoid small $h$ in practice?\n3. In unsupervised learning with loss functions (11) and (12), will model collapse (all $s_{ij}$ are zero or one) happen? This may be caused by zero or infinity weights in the deep model.\n4. More explanation about the usefulness of (11) and (12) are required.\n5. In Section 4.2, suppose $\\beta$ can be computed, how to obtain the stability parameter $\\omega$?\n6. In Table 1, the NMIs and ARIs given by all methods on datasets DHFR and PTC are very low. What is the possible reason?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3490/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3490/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3490/Reviewer_CP9G"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3490/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698570344678,
            "cdate": 1698570344678,
            "tmdate": 1700754278657,
            "mdate": 1700754278657,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "V1925sED7h",
                "forum": "GZ6AcZwA8r",
                "replyto": "AaNayDAUzb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3490/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3490/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal for Reviewer CP9G"
                    },
                    "comment": {
                        "value": "**W1:** The motivation of the two unsupervised loss functions (eq.10 and eq.11) in Section 4 haven\u2019t been sufficiently explained.\n\n**Response:** Thanks for your suggestion. We would further explain the motive behind the unsupervised loss function.\n- Supervised Contrastive Loss (SCL) equation is designed to enhance the model's ability to learn from graph labels when they are available. By adopting the supervised version of the InfoNEC loss function SCL focuses on maximizing the agreement between embeddings of graphs that share the same label (indicated by $C_i = C_j$). The equation reflects this by encouraging similarity (high $s\\_{ij}$) between graphs of the same class while penalizing similarity between graphs of different classes. The component $\\alpha \\sum\\_{k}\\mathbb{I}{[C\\_i \\ne C\\_i]}s_{ik}$ in the denominator acts as a regularizing term to control the influence of dissimilar pairs.\n- Unsupervised Contrastive Loss (UCL) equation provides a mechanism for learning the MMD graph kernel when graph labels are not available. This loss function is a modification of SCL where the distinction between similar and dissimilar graph pairs is made based on their similarity rather than their labels. UCL focuses on selecting the most similar pairs ($S\\_+$) and the least similar pairs ($S\\_-$) within each training epoch. The loss function encourages the model to distinguish between these two sets in the absence of explicit labels.\n\n**W2:** For graph clustering, GNN-based clustering method should be compared in Table 1.\n\n**Response:** Thanks for your suggestion. Following your recommendation, we have updated the table to incorporate results from two notable GNN methods [1][2] for clustering. We use the two methods to obtain graph representations and then conduct spectral clustering. Our methods outperformed these two baselines.\n\n[1] You, Yuning, et al. \"Graph contrastive learning with augmentations.\" NeurIPS 2020\n[2] Sun, Fan-Yun, et al. \"InfoGraph: Unsupervised and Semi-supervised Graph-Level Representation Learning via Mutual Information Maximization.\" ICLR 2020.\n\n---\n\n**Q1:** In Definition 5, for the generalized MMD, how to efficiently determine $k$ in practice? It may have infinite choices.\n\n**Response:** Thanks for your inquiry. A common way to determine the $k$ is to set the parameter range with limited sparse choices such as using an RBF kernel family with bandwidths like $[1e-1, 1e0, 1e1]$. This method provides a balance between computational efficiency and the ability to capture different scales of features in the data.\n\n**Q2:** In Theorem 1, it seems that if $h$ is close to zero, the bound is loose. How to avoid small $h$ in practice? \n\n**Response:** Thanks for bringing up this point. To address this issue in practice, the most straightforward way is avoid including a minimal (e.g. smaller than $1e-3$) bandwidth in the kernel family.\n\n**Q3:** In unsupervised learning with loss functions (11) and (12), will model collapse (all $s_{ij}$ are zero or one) happen? This may be caused by zero or infinity weights in the deep model.\n\n**Response:** Thanks for your question. In the case of UCL, employing a diverse set of negative samples (i.e., non-similar pairs) can prevent the model from trivially maximizing similarity scores. In practice, we found the non-similar pairs could ensure a representative sample of dissimilar pairs discourages the model from collapsing towards uniform similarity scores. For KL loss, we have implemented regularization techniques to prevent extreme values in the model weights.\n\n**Q4:** More explanation about the usefulness of (11) and (12) are required.\n\n**Response:** Thanks for your suggestions. Both UCL and KL serve crucial roles in our approach to unsupervised learning from unlabeled data. The primary benefit of UCL is that it facilitates the learning of discriminative features from a set of graphs. By minimizing the KL Divergence, the model learns to adjust its embeddings to better reflect the natural clustering of the data. We have added the explanation to Section 4 in the manuscript. \n\n**Q5:** In Section 4.2, suppose $\\beta$ can be computed, how to obtain the stability parameter $w$ ?\n\n**Response:** Thanks for your query. We provide the explicit formulation for the stability parameter $\\omega$ in Appendix F."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3490/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700619377819,
                "cdate": 1700619377819,
                "tmdate": 1700619527009,
                "mdate": 1700619527009,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "oQaH1NYfPq",
                "forum": "GZ6AcZwA8r",
                "replyto": "V1925sED7h",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3490/Reviewer_CP9G"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3490/Reviewer_CP9G"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks a lot for the responses\uff0cespecially the derivation of $\\omega$ and results of new baselines. My concerns have been well addressed. I keep my score and raise the confidence level to 5."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3490/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700667119089,
                "cdate": 1700667119089,
                "tmdate": 1700667119089,
                "mdate": 1700667119089,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "SkaB5pJvvZ",
            "forum": "GZ6AcZwA8r",
            "replyto": "GZ6AcZwA8r",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3490/Reviewer_avjd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3490/Reviewer_avjd"
            ],
            "content": {
                "summary": {
                    "value": "This work proposed a class of (deep) MMD based graph kernels, coupled with theoretical results. The proposed kernels are applied to graph classification and graph clustering on a few benchmark datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The paper proposed a class of (deep) MMD based graph kernels (MMD-GKs). The novelty is high. Different from classical graph kernels that have fixed features, the proposed Deep MMD-GKs are learnable, with or without labels.\n* The authors provided some theoretical analysis, which are useful and explainable. \n* The deep MMD-GKs outperformed the baselines in supervised learning and unsupervised learning on several graph datasets."
                },
                "weaknesses": {
                    "value": "* The time cost comparison is missing. \n* Deep MMD-GK is more effective than the vanilla MMD-GK but the corresponding algorithm (Algorithm 2) is in the appendix.\n* The analysis of the experimental results should be strengthened."
                },
                "questions": {
                    "value": "* Section 3.1 can be made more compact to provide more space for Section 4 and Section 5.\n* What is the advantage of the general MMD compared to the single-kernel MMD?\n* In Algorithm 1, for the input, the authors mentioned bandwidth. Does this mean the algorithm only use RBF kernels?\n* In formula (11), how to determine $S_+$ and $S_-$?\n* In Theorem 2, the influence of $n$ and $h$ hasn\u2019t been discussed.\n* What is the labeling rate in Table 2?\n* In Table 2, on BZR, the values given by supervised and unsupervised Deep MMD-GKs are the same. Is it a typo?\n* In Appendix G, are there any explanation for the additional results?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3490/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3490/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3490/Reviewer_avjd"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3490/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698637434088,
            "cdate": 1698637434088,
            "tmdate": 1700732713521,
            "mdate": 1700732713521,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xyE2znB9l0",
                "forum": "GZ6AcZwA8r",
                "replyto": "SkaB5pJvvZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3490/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3490/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal for Reviewer avjd"
                    },
                    "comment": {
                        "value": "**W1:** The time cost comparison is missing.\n\n**Response:** \nThanks for your comments. We have added the complexity analysis in Section 4.3 of the paper with proof in Appendix G.1. We also show them below. \n\n**Theorem 3.** *The MMD-GK using a kernels $\\mathcal{K}$ on a pair of graphs $G$ and $G'$, both with $L$-level of $d$-dimensional node features, can be computed in $O(Lmd+\\kappa n^2d)$, where $n$ is the number of nodes, $m$ is the number of edges, and $\\kappa$ is the size of the kernel family, i.e. $\\kappa=|\\mathcal{K}|$. For $N$ graphs, all pairwise MMD-GKs are computed in $O(NLmd+N^2\\kappa n^2d)$.*\n\n\n**Theorem 4.** *Suppose the widths of the hidden layers of the neural network are $O(d)$. The Deep MMD-GK using a kernels $\\mathcal{K}$ on a pair of graphs $G$ and $G'$, both with $L$-level of $d$-dimensional node features, can be computed in $O(Lnd^2+Lmd+\\kappa n^2d)$, where $n$ is the number of nodes, $m$ is the number of edges and $\\kappa$ is the size of the kernel family, i.e. $\\kappa=|\\mathcal{K}|$. For $N$ graphs, all pairwise Deep MMD-GKs are computed in $O(Nnd^2+NLmd+N^2\\kappa n^2d)$.*\n\nWe compare the theoretical efficiency of the proposed kernel with other baselines in Table 3. Our MMD-GK is competitive with kernels, such as the Shortest Path and Graphlet with a $k>2$. Generally, we set $\\kappa$ such that $\\kappa d<n$, so the MMD-GK is better in complexity than the Wasserstein Weisfeiler-Lehman graph kernel, when holding $L$ equal to $h$. We note that the Weisfeiler-Lehman Subtree graph kernels scale linearly with the number of nodes; therefore, this method is faster than our approach.\n\n| Graph Kernel                  | Node Labels | Node Attributes |         Complexity         |\n|-------------------------------|:-----------:|:---------------:|:--------------------------:|\n| Graphlet                      |      F      |        F        |          $O(n^k)$          |\n| Shortest Path                 |      T      |        T        |          $O(n^4)$          |\n| Weisfeiler-Lehman Subtree     |      T      |        F        |         $O(hm+hn)$         |\n| Wasserstein Weisfeiler-Lehman |      T      |        T        |    $O(hm + n^3\\log(n))$    |\n| MMD-GK                        |      T      |        T        |    $O(Lmd+\\kappa n^2d)$    |\n| Deep MMD-GK                   |      T      |        T        | $O(Lnd^2+Lmd+\\kappa n^2d)$ |\n\nAbove is the summary of selected graph kernels regarding support for node-labeled and node-attributed graphs, and computational complexity. Notations not been introduced yet: $k$: size of largest subgraph considered; $h$: maximum distance between root of neighborhood subgraph/subtree pattern and its nodes.\n\n\nWe also add runtimes analysis in Appendix G.2. Following the setting of Togninalli et al. [1], we conducted a simulation using a consistent number of graphs while varying the average node number per graph, to assess the performance of our MMD-GK approach with respect to the average number of nodes. Specifically, we generated 1000 synthetic graphs with the same node feature's dimension $d=20$. For each graph, we generate the number of nodes based on a normal distribution centered on a average node number. For each level of node features $L\\in\\{1, 2, 3, 4, 5\\}$, we applied the MMD-GK with a single Radial Basis Function (RBF) kernel (i.e. $|\\mathcal{K}|=1$) to measure and compare runtimes over different average number of nodes $n$.\nAs shown by Figure 3 in the Appendix F of the manuscript, the runtime increases with the square of the number of nodes $n^2$, which is modulated by the number of levels $L$. \n\nWe also compare our approaches to four graph kernels on the synthetic graphs with node features $d=1$. For the Graphlet kernel, $k=5$; for the Weisfeiler-Lehman Subtree kernel, $h=5$; for our MMD-GK and Deep MMD-GK, $L=h$, $\\kappa=1$ and $d=1$. $m$ is held constant across the kerenls. All methods are performed ten times. Refer to the Table 4, we report the total computation time for 10 runs on 1000 synthetic graphs with average of 20 nodes 50 edges, which indicates our approaches are competitive with the existing graph kernel methods.\n\n| Graph Kernel                  | Total Runtime |\n|-------------------------------|:------------:|\n| Graphlet                      |    120.4s    |\n| Shortest Path                 |    30.3.s    |\n| Weisfeiler-Lehman Subtree     |    26.4 s    |\n| Wasserstein Weisfeiler-Lehman |    211.0s    |\n| MMD-GK                        |     29.2s    |\n| Deep MMD-GK                   |     35.2s    |\n\n\n[1] Togninalli, Matteo, et al. \"Wasserstein weisfeiler-lehman graph kernels.\" NeurIPS 2019\n\n---\n\n**W2:** Deep MMD-GK is more effective than the vanilla MMD-GK but the corresponding algorithm (Algorithm 2) is in the appendix.\n\n**Response:** Thanks for your suggestions. We certainly incorporate your feedback into our manuscript, ensuring that Algorithm of Deep MMD-GK is more accessible by including it in the main body of the text."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3490/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700619815259,
                "cdate": 1700619815259,
                "tmdate": 1700619815259,
                "mdate": 1700619815259,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vzQiwa81ME",
                "forum": "GZ6AcZwA8r",
                "replyto": "Ah491IHTSM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3490/Reviewer_avjd"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3490/Reviewer_avjd"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your response"
                    },
                    "comment": {
                        "value": "Thanks for your response! It addressed my concerns, so I'd like to increase my score."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3490/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700732694186,
                "cdate": 1700732694186,
                "tmdate": 1700732694186,
                "mdate": 1700732694186,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "t1nTXT1NOA",
            "forum": "GZ6AcZwA8r",
            "replyto": "GZ6AcZwA8r",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3490/Reviewer_a9u9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3490/Reviewer_a9u9"
            ],
            "content": {
                "summary": {
                    "value": "This paper discusses graph kernels based on maximum mean discrepancy (MMD) on the (empirical) distribution of embedded nodes. An extension of this MMD graph kernel similar to the deep graph kernel (Yanardag and Vishwanathan 2015) is proposed. For both graph kernel variants the authors provide a theoretical robustness/perturbation analysis. Finally, first experimental results on standard benchmark datasets are presented."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "* First empirical results on (arguably simple) benchmark datasets are promising.\n* Theoretical robustness analysis."
                },
                "weaknesses": {
                    "value": "While the paper is interesting, I vote for rejection in its current form, due the following issues.\n\nBadly written and hard to follow paper:\n* The reader is not guided well through the paper. \n* While the robustness analyses might be interesting, they are completely unmotivated. The assumptions made and the achieved bounds are hard to interpret and set into context. It is not clear why the authors decided to include them and what the significance of them is. Please guide the reader more and put the results into context, e.g. discuss similar bounds. \n* The architecture overview in Fig. 1 is rather confusing and not explained well. Also the \"GCN\" probably stands for the GCN of Kipf&Welling, however it's not clear what it is doing in the Figure and what is meant e.g. with \"vanilla: done. GCN: updating\". \n* The central definition 6 and Assumption 1 are difficult to understand.\n\n\nDiscussion of related work is insufficient\n* Most of the actually related work is not discussed. E.g. many papers exist discussing MMD in the context of graph kernels and GNNs. E.g. [1, 2, Borgwardt's thesis, ... ] Similarly many related work exist that perform graph matching / optimal assignment [Kriege et al., survey, etc.], or modify WL using Wasserstein distances [3-5]. This makes the novelty difficult to assess. Please discuss.\n\nMisleading and wrong claims:\n* The authors claim that the proposed kernel has \"much lower computational cost\" then e.g. the WL kernel. This seems to be wrong. The WL kernel can be computed in near-linear time if the height is fixed. This is similar to the parameter $l$ here in $X^l$. In fact, the computation of WL up to height $l$ should be essentially the same as computing the $X^l$s. \n* two of the three main benefits of the proposed approach are (1) dealing with unequal sizes and (2) permutation-invariance. This is however the case for almost all graph kernels, including the popular ones like the WL-kernel, walk kernels, pattern-based kernels etc. \n* the claims about degree + power law distributions are unclear. Why should non-power law distributed degrees lead to \"inaccurate substructure representations\".  Also, the authors claim that GNNs resolve the previously mentioned issue and \"capture intricate higher-level interactions\". This is however rather misleading as most common GNNs (MPNNs) are bounded in their expressivity by WL and hence cannot capture \"more\" than the WL kernel (Xu et al., 2019, Morris et al 2019).\n* The cited \"MMD GAN\" paper is mentioned as \"MMD also aids in the training of GNN\", while the paper actually discussed training of *GANs* and does not deal with graphs at all.\n\nA lot of sentences are vague, handwavy, and unnecessarily wordy, e.g.\n* what should \"substructures derived from graphs are intertwined\" even mean exactly.\n* $X^l$ and $X^{(l)}$ are probably the same thing, but the latter is not defined.\n\nTypos and minor issues:\n* $\\mu_P$ is not defined. Similarly, most readers probably don't know what a characteristic kernel is in the context of MMD. Please make the paper self-contained.\n* \"to learn graph metric*s*\"\n* \"(2012) . \" the space before the dot \".\"\n\nI am happy to change my score if my concerns are addressed and the contribution/novelty is made clearer.\n\n[1] Ghanem, Hashem et al. \"Fast graph kernel with optical random features.\" ICASSP 2021\n\n[2] O'Bray, Leslie, et al. \"Evaluation metrics for graph generative models: Problems, pitfalls, and practical solutions. ICLR 2022\n\n[3] Togninalli, Matteo, et al. \"Wasserstein weisfeiler-lehman graph kernels.\" NeurIPS 2019\n\n[4] Samantha Chen et al. \"Weisfeiler-Lehman Meets Gromov-Wasserstein\" ICML 2022\n\n[5] Till Schulz et al. \"A generalized Weisfeiler-Lehman graph kernel\" Machine Learning 2022"
                },
                "questions": {
                    "value": "Why do the authors not use the standard unbiased MMD estimator e.g. in Eq. (4)? I.e. the one normalized with $(n\\cdot(n-1))$ in the in-distribution sums, cf. Gretton et al. 2012."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3490/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3490/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3490/Reviewer_a9u9"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3490/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698771684458,
            "cdate": 1698771684458,
            "tmdate": 1700729658808,
            "mdate": 1700729658808,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "nxHwxaRY4k",
                "forum": "GZ6AcZwA8r",
                "replyto": "t1nTXT1NOA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3490/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3490/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal for Reviewer a9u9"
                    },
                    "comment": {
                        "value": "**Overview of Responses:** \n\nDear Reviewer,\n\nThank you for your insightful feedback. In response to your comments, we have implemented the following enhancements to our manuscript:\n\n1. **Enhanced Related Work:** We have expanded the discussion in the related work section to better position our study within the existing body of research.\n\n2. **Elaboration of Motivation and Implications:** We have enriched the Section 3.3 and 4.1 on the motivation behind our robustness analysis. \n\n3. **Improved Clarity and Readability:** We have carefully revised the manuscript refining the language, streamlining the narrative, and ensuring a more coherent presentation of our research.\n\nWe believe these improvements  strengthen the manuscript and look forward to your further feedback.\n\n---\n**W1:** The reader is not guided well through the paper.\n\n**Response:** Thanks for your comments. In response, we have undertaken a thorough revision to improve the flow and structure.\n\n**W2:** While the robustness analyses might be interesting, they are completely unmotivated. The assumptions made and the achieved bounds are hard to interpret and set into context. It is not clear why the authors decided to include them and what the significance of them is. Please guide the reader more and put the results into context, e.g. discuss similar bounds.\n\n**Response:** Thank you for pointing this out. We have revised this section to explicitly state why these analyses are crucial.\n\nWe conduct the robust analysis for the following reasons.\n* First, since real data are often noisy, we would like to theoretically show the upper bound of the gap between the distance of clean graphs and the distance of corrupted graphs. By the way, the robustness is also useful when considering differential privacy, i.e., adding noises to the graphs to protect the data or user privacy in some scenarios.\n* Second, the robust analysis is useful for quantifying the generalization ability of the model. For instance, given a test pair $\\tilde{G}_1$ and $\\tilde{G}_2$, suppose their closest graphs in the training dataset $\\mathcal{G}$ are $G_1, G_2$ respectively. Then according to Theorem 1, $\\hat{d}_K^2(\\tilde{G}_1^{(l)}, \\tilde{G}_2^{(l)}) \\leq \\hat{d}_K^2(G_1^{(l)}, G_2^{(l)})+\\bar{\\Delta}$, where $\\bar{\\Delta}$ is small provided that $G_1, G_2$ are similar to $\\tilde{G}_1, \\tilde{G}_2$ in terms of the adjacency matrices and node features. A smaller $\\bar{\\Delta}$ implies a stronger generalization ability. \n\nThese discussions have been added to Section 3.2 and Section 4.1. \n\nIn the revised paper, we provide more explanation about the assumption (e.g. the norm upper bounds and $\\Delta_{A_i}$) in the theorems. They can be found in Section 3.3.\n\n**W3:** The architecture overview in Fig. 1 is rather confusing and not explained well. Also the \"GCN\" probably stands for the GCN of Kipf&Welling, however it's not clear what it is doing in the Figure and what is meant e.g. with \"vanilla: done. GCN: updating\".\n\n**Response:** Thanks for your feedback. What we mean by \"GCN: updating\" is that the deep model requires training and updating parameters, while the vanilla version only goes through the process one time. We changed this figure and its caption to clarify the meaning of terms like \"GCN\" and \"vanilla: done. GCN: updating\". Additionally, we will reference the work of [1] to provide context.\n\n[1] Kipf, Thomas N., and Max Welling. \"Semi-Supervised Classification with Graph Convolutional Networks.\" ICLR. 2016."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3490/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700625750495,
                "cdate": 1700625750495,
                "tmdate": 1700626167382,
                "mdate": 1700626167382,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FI20XyVxyc",
                "forum": "GZ6AcZwA8r",
                "replyto": "EHoMo9Tmco",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3490/Reviewer_a9u9"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3490/Reviewer_a9u9"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for these very thorough clarifications! I raised my score."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3490/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700729632531,
                "cdate": 1700729632531,
                "tmdate": 1700729632531,
                "mdate": 1700729632531,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "uGjyPc9QS6",
            "forum": "GZ6AcZwA8r",
            "replyto": "GZ6AcZwA8r",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3490/Reviewer_9SJd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3490/Reviewer_9SJd"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new graph kernel to calculate the maximum mean discrepancy (MMD) which measures the similarity of two graphs. The main idea is to regard each graph as a sampling distribution from a latent metric space where a similarity kernel can be designed for efficiency. Based on such an MMD-based graph kernel, two methods are designed for unsupervised and supervised graph metric learning, respectively."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "+ The paper is well-written, and I find it a joy to read. The authors present a nice and coherent story to introduce the current gap and motivation of the idea.\n\n+ The proposed MMD-based kernels are technically sound and will have a significant impact on the graph metric learning domain.\n\n+ In-depth and well-organized theoretical analysis is provided to justify the approach in a principled manner."
                },
                "weaknesses": {
                    "value": "- The major limitation is the lack of theoretical and empirical study of the efficiency of the proposed kernel. As highlighted in the introduction the proposed method aims to address the computation and memory cost of existing methods, no evidence shows this goal is achieved."
                },
                "questions": {
                    "value": "See weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3490/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699038971972,
            "cdate": 1699038971972,
            "tmdate": 1699636301999,
            "mdate": 1699636301999,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "sTmdUMlhwE",
                "forum": "GZ6AcZwA8r",
                "replyto": "uGjyPc9QS6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3490/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3490/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal for Reviewer 9SJd"
                    },
                    "comment": {
                        "value": "**W1:** The major limitation is the lack of theoretical and empirical study of the efficiency of the proposed kernel. As highlighted in the introduction the proposed method aims to address the computation and memory cost of existing methods, no evidence shows this goal is achieved.\n \n**Response:** \nThanks for your comments. We have added the complexity analysis in Section 4.3 of the paper with proof in Appendix G.1. We also show them below. \n\n**Theorem 3.** *The MMD-GK using a kernels $\\mathcal{K}$ on a pair of graphs $G$ and $G'$, both with $L$-level of $d$-dimensional node features, can be computed in $O(Lmd+\\kappa n^2d)$, where $n$ is the number of nodes, $m$ is the number of edges, and $\\kappa$ is the size of the kernel family, i.e. $\\kappa=|\\mathcal{K}|$. For $N$ graphs, all pairwise MMD-GKs are computed in $O(NLmd+N^2\\kappa n^2d)$.*\n\n\n**Theorem 4.** *Suppose the widths of the hidden layers of the neural network are $O(d)$. The Deep MMD-GK using a kernels $\\mathcal{K}$ on a pair of graphs $G$ and $G'$, both with $L$-level of $d$-dimensional node features, can be computed in $O(Lnd^2+Lmd+\\kappa n^2d)$, where $n$ is the number of nodes, $m$ is the number of edges and $\\kappa$ is the size of the kernel family, i.e. $\\kappa=|\\mathcal{K}|$. For $N$ graphs, all pairwise Deep MMD-GKs are computed in $O(Nnd^2+NLmd+N^2\\kappa n^2d)$.*\n\nWe compare the theoretical efficiency of the proposed kernel with other baselines in Table 3. Our MMD-GK is competitive with kernels, such as the Shortest Path and Graphlet with a $k>2$. Generally, we set $\\kappa$ such that $\\kappa d<n$, so the MMD-GK is better in complexity than the Wasserstein Weisfeiler-Lehman graph kernel, when holding $L$ equal to $h$. We note that the Weisfeiler-Lehman Subtree graph kernels scale linearly with the number of nodes; therefore, this method is faster than our approach.\n\n| Graph Kernel                  | Node Labels | Node Attributes |         Complexity         |\n|-------------------------------|:-----------:|:---------------:|:--------------------------:|\n| Graphlet                      |      F      |        F        |          $O(n^k)$          |\n| Shortest Path                 |      T      |        T        |          $O(n^4)$          |\n| Weisfeiler-Lehman Subtree     |      T      |        F        |         $O(hm+hn)$         |\n| Wasserstein Weisfeiler-Lehman |      T      |        T        |    $O(hm + n^3\\log(n))$    |\n| MMD-GK                        |      T      |        T        |    $O(Lmd+\\kappa n^2d)$    |\n| Deep MMD-GK                   |      T      |        T        | $O(Lnd^2+Lmd+\\kappa n^2d)$ |\n\nAbove is the summary of selected graph kernels regarding support for node-labeled and node-attributed graphs, and computational complexity. Notations not been introduced yet: $k$: size of largest subgraph considered; $h$: maximum distance between root of neighborhood subgraph/subtree pattern and its nodes.\n\n\nIn addition, we add runtimes analysis in Appendix G.2. Following the setting of Togninalli et al. [1], we conducted a simulation using a consistent number of graphs while varying the average node number per graph, to assess the performance of our MMD-GK approach with respect to the average number of nodes. Specifically, we generated 1000 synthetic graphs with the same node feature's dimension $d=20$. For each graph, we generate the number of nodes based on a normal distribution centered on a average node number. For each level of node features $L\\in\\{1, 2, 3, 4, 5\\}$, we applied the MMD-GK with a single Radial Basis Function (RBF) kernel (i.e. $|\\mathcal{K}|=1$) to measure and compare runtimes over different average number of nodes $n$.\nAs shown by Figure 3 in the Appendix F of the manuscript, the runtime increases with the square of the number of nodes $n^2$, which is modulated by the number of levels $L$. \n\nWe also compare our approaches to four graph kernels on the synthetic graphs with node features $d=1$. For the Graphlet kernel, $k=5$; for the Weisfeiler-Lehman Subtree kernel, $h=5$; for our MMD-GK and Deep MMD-GK, $L=h$, $\\kappa=1$ and $d=1$. $m$ is held constant across the kerenls. All methods are performed ten times. Refer to the Table 4, we report the total computation time for 10 runs on 1000 synthetic graphs with average of 20 nodes 50 edges, which indicates our approaches are competitive with the existing graph kernel methods.\n\n| Graph Kernel                  | Total Runtime |\n|-------------------------------|:------------:|\n| Graphlet                      |    120.4s    |\n| Shortest Path                 |    30.3.s    |\n| Weisfeiler-Lehman Subtree     |    26.4 s    |\n| Wasserstein Weisfeiler-Lehman |    211.0s    |\n| MMD-GK                        |     29.2s    |\n| Deep MMD-GK                   |     35.2s    |\n\n[1] Togninalli, Matteo, et al. \"Wasserstein weisfeiler-lehman graph kernels.\" NeurIPS 2019\n\n**Hope this response can solve your concerns. We thank the reviewer again for recognizing our work.**"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3490/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700618603305,
                "cdate": 1700618603305,
                "tmdate": 1700618995718,
                "mdate": 1700618995718,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]