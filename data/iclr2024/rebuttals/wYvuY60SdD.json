[
    {
        "title": "Mixture of Weak and Strong Experts on Graphs"
    },
    {
        "review": {
            "id": "6DS4dfHwNt",
            "forum": "wYvuY60SdD",
            "replyto": "wYvuY60SdD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6162/Reviewer_jetB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6162/Reviewer_jetB"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors leverage the idea of using mixture-of-experts models to improve the model capacity for graph neural networks (GNNs). Since the GNNs are both expensive and hard to optimize, the authors propose mixing a light-weight multi-layer perceptron (MLP), with an off-the-shelf GNN rather than modeling each expert as a GNN. Here, the MLP specializes in extracting the rich self-features of nodes and it is referred to a weak expert, while the GNN exploits the structure of neighborhood and is called a strong expert. To aggregate these two models, the authors introduce a biased gating function towards the GNN predictions based on a novel \"confidence\" mechanism, and name this model \"Mowst\"\n\nFor inference, the authors run the weak expert first and obtain a prediction. If the confidence score of that prediction exceeds some random threshold, then it is selected as the final prediction. Otherwise, they continue to run the strong expert, and use the prediction of that expert.\n\nFor training, they propose the loss function $L_{Mowst}$ by firstly computing the losses of two experts separately, and then combining them via the confidence function $C$.\n\nIn addition, the authors also introduce a variant of the Mowst model in which they use the confidence function $C$ to combine the predictions of the two experts, and then calculate a single loss.\n\nThe authors demonstrate that both models are at least as expressive as the MLP or GNN alone but with a comparable computional cost. Finally, they empirically show that these models have significant accuracy improvement on 6 standard node classification benchmarks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Originality: the idea of using mixture of weak and strong experts with the confidence mechanism is novel.\n2. Quality: the authors provide both theoretical and empirical results to demonstrate the effectiveness of the proposed model \"Mowst\"."
                },
                "weaknesses": {
                    "value": "1. Clarity: the paper is not well-written. \n- The authors should emphasize the problem they would like to solve more clearly in the introduction section. \n- The presentation of all propositions and theorems is informal. Additionally, the statements of Propositions 2.5, 2.6, 2.7 are so confusing (see Question section).\n- The notations used in this paper are difficult to digest (see Theorem 2.4 and Corollary 2.4.1).\n\n2. As far as I understand, with the confidence gating function, we can only leverage two experts in the Mowst without being able to use multiple experts as in previous work. Therefore, the ability to scale up the model capacity is quite limited.\n\n3. The experiments in Section 4 do not show the ability to scale up the model capacity of the Mowst model and its variant."
                },
                "questions": {
                    "value": "1. Below Proposition 2.2, the authors should either present formal formutions of variance and negative entropy functions or give references for these functions.\n\n2. In Section 2.3, are indistinguishable self-features are necessarily the same? The authors should explain more about the term 'indistinguishable'.\n\n3. In Proposition 2.5, does the loss function $L_{Mowst}$ upper bound its counterpart $L^*_{Mowst}$ for any choice of confidence function $C$?\n\n4. In Proposition 2.6 and Theoremm 2.7, the authors should illustrate the concept of expressiveness mathematically. \n\n5. At the end of page 1, the authors claim that the sparse gating function may make the optimization harder due to its discontinuity. I would like to emphasize that not all sparse gating functions are discontinuous. For instance, a temperature softmax gating function in [1] is a sparse yet continuous gating function.\n\n6. Could the authors please explain more clearly why the MLP denoises for the GNN during training?\n\n7. In Algorithm 2, what methods do the authors use to learn the MLP weights and the GNN weights? And what are the convergence rates of those parameters?\n\n8. Why do the authors need the confidence $C$ to be quasiconvex? What happens if $C$ is not quasiconvex?\n\n9. What are the main challenges of using the Mowst model?\n\n10. In Section 3, the authors should cite more relevant papers regarding symmetric gating functions in mixture-of-experts models, namely [1], [2], [3], [4]. \n\n**Minor issues**:\n\n1. The abbreviation 'GCN' in page 6 has not been introduced.\n2. Grammatical errors: 'the number graph convolution' (Section 1).\n3. After the statement of each result, the authors should give references to location (within the paper) of the corresponding proofs.\n\n**References**\n\n[1] X. Nie. Dense-to-Sparse Gate for Mixture-of-Experts.\n[2] H. Nguyen. A General Theory for Softmax Gating Multinomial Logistic Mixture of Experts.\n[3] H. Nguyen. Statistical Perspective of Top-K Sparse Softmax Gating Mixture of Experts.\n[4] H. Nguyen. Demystifying Softmax Gating Function in Gaussian Mixture of Experts."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6162/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6162/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6162/Reviewer_jetB"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6162/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698558257663,
            "cdate": 1698558257663,
            "tmdate": 1699636669309,
            "mdate": 1699636669309,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "OKp3F8GO7N",
                "forum": "wYvuY60SdD",
                "replyto": "6DS4dfHwNt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6162/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6162/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors' Response (Part 1)"
                    },
                    "comment": {
                        "value": "We greatly appreciate the valuable feedback from the reviewer. We are actively preparing a revision to address the questions and include the suggested citations. Detailed responses to each question / concern are presented in the following. \n\n## Weakness 1\n\n> The authors should emphasize the problem they would like to solve more clearly in the introduction section.\n\nThanks for the suggestion. We will update the presentation accordingly in our revision. \n\nAs summarized in the first sentence of paragraph 3 in *Introduction*, our goal is \u201c*to improve [GNN] model capacity without significant trade-offs in computation overhead and optimization difficulty*\u201d. The two challenges of \u201ccomputation overhead\u201d and \u201coptimization difficulty\u201d are illustrated in paragraph 2 of *Introduction*. \n\n> The presentation of all propositions and theorems is informal. \n\n> The notations used in this paper are difficult to digest\n\nOur theoretical statements are formal and all results have been **rigorously proven** in *Appendix B*. Nevertheless, we appreciate the feedback that some terms could be more clearly described. See our response to the \"Questions\" section below. \n\nWe will also adjust the structure of *Section 2.3* to facilitate readers' better understand. \n\n## Weakness 2\n\n> with the confidence gating function, we can only leverage two experts in the Mowst without being able to use multiple experts as in previous work. \n\n> Therefore, the ability to scale up the model capacity is quite limited.\n\nThe confidence function is **not** an issue. In *Section 2.7*, we have described the detailed methodology to generalize Mowst to more than 2 experts (mathematical derivation is in *Appendix C.2*). In this case, we have a series of \u201cprogressively stronger\u201d experts rather than just a weak and a strong one. \n\nTo summarize the idea: the generalization to multiple experts follows a **recursive formulation**, where the 2-expert case is the fundamental building block. We use a 3-expert example for illustration (expert 1 weaker than expert 2, and expert 2 weaker than expert 3). We first combine experts 2 and 3 according to the 2-expert Mowst design. Then we regard the sub-system of experts 2 and 3 as a \u201cmeta-model\u201d. Finally, we combine expert 1 with the \u201cmeta-model\u201d again according to the 2-expert Mowst design. Under such recursion, the **confidence mechanism still applies**: we first compute expert 1\u2019s confidence to decide if we want to proceed to experts 2 & 3. If so, we compute expert 2\u2019s confidence to decide between the predictions of experts 2 & 3. \n\nThe exact **Mowst loss function under any number of experts** is shown in *Equation 40, Appendix C.2*. Finally, the analysis on the 2-expert case (*Section 2.3* to *2.6*) can be automatically generalized to the multi-expert case. Thus, we **already have a comprehensive design for the multi-expert Mowst**. \n\n*Side note*: In addition to the above recursive formulation, there is another straightforward way to integrate many experts. We can use the 2-expert Mowst to construct a hierarchical mixture. The strong expert of the 2-expert Mowst can be an existing MoE model. Thus, our strong expert now contains many \u201csub-experts\u201d which can be controlled by existing symmetric gating modules. \n\n\n## Weakness 3\n\n> The experiments in Section 4 do not show the ability to scale up the model capacity of the Mowst model and its variant.\n\nDue to resource and time constraints, we have only empirically evaluated the 2-expert case. Even without the many-expert results, we believe the 2-expert Mowst has already demonstrated a significant enhancement to the existing literature. Currently, there lacks a good MoE design in the graph learning domain, state-of-the-art GNN-based MoE models (e.g., GraphMoE in *Table 1*) or GNN-based ensemble models (e.g., AdaGCN in *Table 1*) do not show significant accuracy improvements on realistic graphs, and may even significantly increase the computation complexity compared with the vanilla GNNs. We have shown that even the basic 2-expert Mowst can significantly and consistently improve the accuracy of state-of-the-art, with computation cost comparable to a single vanilla GNN. Thus, the current experiments have provided **solid evidence on significantly improved model capacity**. \n\nNevertheless, we thank the reviewer for mentioning the evaluation on multi-expert Mowst. We agree that it is a promising direction given the good performance already achieved by the 2-expert version. We will leave such evaluation as future work if we don\u2019t have enough time or resources during the rebuttal."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6162/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700122063436,
                "cdate": 1700122063436,
                "tmdate": 1700122063436,
                "mdate": 1700122063436,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7gmqTPcpx1",
                "forum": "wYvuY60SdD",
                "replyto": "6DS4dfHwNt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6162/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6162/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors' Response (Part 2)"
                    },
                    "comment": {
                        "value": "## Q1\n\n> \u200b\u200bformal formulations of variance and negative entropy\n\nPlease refer to *Appendix B.1.1* for the exact mathematical expression of variance and negative entropy. We omitted the equations in the main text because none of our analysis in *Section 2* relies on a specific form of $D$ (*Definition 2.1* has specified all requirements on $D$). \n\n## Q2\n\n> are indistinguishable self-features necessarily the same?\n\n*Theoretically*, the self-features being indistinguishable means that they are exactly the same. This is because, theoretically, MLPs are universal approximators (Hornik et al., 1989 in original submission), capable of approximating any function to an arbitrary precision. Thus, even if two nodes have slightly different self features, the MLP can in theory distinguish them and classify them as having *any* two labels. \n\nTherefore, we will clarify in *Section 2.3* that here we refer to identical self-features. \n\nAs a side node (*not affecting theoretical analysis*), in practice, an MLP tends to generate similar predictions for similar features (due to the generalization of neural networks). So practically, \u201cindistinguishable\u201d refers to features that are so similar that the MLP generates the same predictions. Thus, the insights from *Theorem 2.4* and *Corollary 2.4.1* help us understand the practical behaviors of the two experts on clusters of nodes with similar self-features. \n\n## Q3\n\n> In Proposition 2.5, does the loss function $L_{Mowst}$ upper bound its counterpart $L_{Mowst}^*$ for any choice of confidence function $C$?\n\nYes. The upper bound holds as long as the output of $C$ is between 0 and 1. The proof is in *Appendix B.1.4*. In short, $C$ and $1-C$ lead to a convex combination of the experts\u2019 loss (for Mowst) or prediction logits (for Mowst*). The upper bound is thus a property of the cross-entropy loss function (which is a convex function). \n\nSince $C$ defined by *Definition 2.1* satisfies the above condition, we didn\u2019t specify any additional condition on $C$ in the statement of *Proposition 2.5*. \n\n## Q4 (Updated from authors' original response)\n> In Proposition 2.6 and Theorem 2.7, the authors should illustrate the concept of expressiveness mathematically.\n\nThanks for the suggestion. We will clarify the following definition in our revision: \n\nThe \"expressive power\" **follows the standard definition** in the literature (e.g., [a]) to describe *the neural network's ability to approximate functions*. \n\nIn our case (*Propositions 2.6, 2.7, 2.8*):\n* If model A is as expressive as model B, then for any model B applied on any graph, we can find a corresponding model A that generates *the same predictions* as model B. \n* If model A is more expressive than model B, then 1) model A is at least as expressive as model B, and 2) there *exists* a graph such that we can find a model A whose predictions are different from the predictions of any model B. \n\nIn the proof (*Appendix B*), to show \"as expressive as\", we construct confidence functions such that Mowst generates exactly the same outputs as any of its experts. Then to show \"more expressive than\", we construct a graph and a corresponding Mowst-GCN such that the Mowst-GCN can classify more nodes correctly than any GCN alone. \n\nHere GCN refers to \"Graph Convolution Network\" (Kipf et al., 2016 in original submission). \n\nReference\n\n[a] Lu et al. The Expressive Power of Neural Networks: A View from the Width. In NeurIPS 2017. \n\n## Q5\n\n> not all sparse gating functions are discontinuous. For instance, a temperature softmax gating function in [1] is a sparse yet continuous gating function.\n\nThanks for mentioning the related work [1]. We will cite it in our revision and clarify the statement regarding sparsity. [1] addresses the discontinuity in sparse gating by implementing a novel dense gate whose output adaptively evolves to a one-hot distribution when training progresses. \n\nHowever, we would like to clarify that the discontinuity and optimization difficulty are still **prevailing issues in popular MoE designs**. For example, see the discussion in a recent review paper by Google Brain: \u201cA review of sparse expert models in deep learning\u201d (cited as Fedus et al., 2022 in the original submission)."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6162/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700123797831,
                "cdate": 1700123797831,
                "tmdate": 1700443789803,
                "mdate": 1700443789803,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CNMAqcoYyx",
                "forum": "wYvuY60SdD",
                "replyto": "6DS4dfHwNt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6162/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6162/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors' Response (Part 3)"
                    },
                    "comment": {
                        "value": "## Q6\n\n> Could the authors please explain more clearly why the MLP denoises for the GNN during training?\n\nThe denoising process is described in *Section 2.4*, and we are happy to explain this interesting behavior from another perspective. \n\nLet\u2019s start from the fundamentals. According to theoretical analysis in *Section 2.3*, under the confidence-based loss (*Equation 1*), the MLP expert will be assigned to nodes on which it can outperform the GNN. i.e., such nodes contain rich self-feature information but may also have structural noises. The remaining nodes are assigned to the GNN and they would contain useful structural information. Let the sets of nodes assigned to the MLP and GNN be $V_1$ and $V_2$, respectively. There are two potential benefits provided by Mowst:\n* **Specialization**: We match different parts of the data to the best-fit expert model. So naturally, $V_1$ is matched with MLP and $V_2$ with GNN. \n* **Denoising**: Without Mixture-of-Experts, a single GNN model will be shared between both $V_1$ and $V_2$. The noises in $V_1$ will negatively affect the shared GNN model (e.g., generating harmful gradients as described in *Section 2.4*), resulting in degraded performance even on the clean data $V_2$. When the MLP filters out $V_1$ from the GNN training set, the GNN can improve its performance on $V_2$ since it is no longer affected by the harmful gradients from $V_1$. \n\nIn the normal case, we would expect that $V_1$ and $V_2$ both contain a large number of nodes and thus they each capture a meaningful distribution of the training data. In this case, both \u201cspecialization\u201d and \u201cdenoising\u201d contribute to the model performance boost. \n\n**In our case, we observe another possibility**. Due to the weak-strong combination, it can be much harder for the MLP to take charge of a node than the GNN. Consequently, $V_1$ may possibly only consist of a very small amount of data -- e.g., a few outliers or noisy nodes in the training set. Thus, $V_1$ does **not** really represent a meaningful data distribution. Optimizing an MLP on $V_1$ just results in **overfitting** (mentioned in *Section 2.4*). In other words, the MLP does not truly \u201cspecialize\u201d. Even so, the interesting part is that we still observe significant accuracy improvement comparing Mowst with the baseline GNN. This means **\u201cdenoising\u201d by MLP plays a critical role to improve the model quality of the GNN expert**. \n\n*Remark*: Whether $V_1$ consists of a significant portion of the training data or not, the MLP-GNN interaction is governed by the same mechanism theoretically described in *Section 2.3*. When MLP overfits a small $V_1$ set, the GNN should have already converged to a reasonable model (otherwise, the GNN would not be powerful enough to dominate on almost the entire training set). Therefore, we use \u201c**fine-tuning**\u201d in *Section 2.4* to describe the further GNN model improvement due to denoising. \n\n\n## Q7\n> In Algorithm 2, what methods do the authors use to learn the MLP weights and the GNN weights? And what are the convergence rates of those parameters?\n\nSince the loss defined by *Equation 1* is end-to-end differentiable, we use the standard gradient-based optimizer to learn the MLP and GNN weights. Specifically, in our experiments, we use the Adam optimizer with dropout. See *Appendix A.2 & A.3* for the parameter search methodology. \n\nIn practice, the MLP training converges much faster than the GNN training (e.g., on Flickr, the MLP takes 134 iterations to converge while the GNN takes 280, both averaged by 10 runs). The difference in the convergence speed is the main motivation that we use the \u201cin-turn\u201d training strategy in *Algorithm 2*. \n\n## Q8\n> Why do the authors need the confidence C to be quasiconvex? What happens if C is not quasiconvex?\n\nWe have the \u201cquasiconvexity\u201d requirement mainly for ease of theoretical analysis. If $C$ is not quasiconvex, the bound in *Theorem 2.4* does not necessarily hold. The experts will still interact in a meaningful & collaborative way, but less explainable. \n\nWithout quasiconvexity, the following theoretical results still hold: the bound of *Proposition 2.5*, and the conclusions on expressive power by *Proposition 2.6*, *Theorem 2.7* and *Proposition 2.8*. \n\nQuasiconvexity is easy to satisfy. Following *Section 2.2*, $C$ is decomposed as the $D$ and $G$ functions. Popular dispersion functions such as variance and negative entropy can satisfy the convexity (and thus quasiconvexity) requirement of $D$. So to make $C$ quasiconvex, we only need $G$ to be a monotonically non-decreasing scalar function (which is very intuitive & reasonable -- model should be more confident if its prediction has higher dispersion $D$)."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6162/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700124673266,
                "cdate": 1700124673266,
                "tmdate": 1700124673266,
                "mdate": 1700124673266,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KvknNcMF16",
                "forum": "wYvuY60SdD",
                "replyto": "6DS4dfHwNt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6162/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6162/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors' Response (Part 4)"
                    },
                    "comment": {
                        "value": "## Q9\n\n> What are the main challenges of using the Mowst model?\n\nThere is no apparent tradeoff to apply Mowst. Due to the low computation complexity of the weak MLP expert, the overall complexity of Mowst is close to that of a single GNN model. Thus, the increased model capability does not come at the cost of more computation. Further, since our confidence mechanism is applied after executing the entire expert model, we have very little restriction on the experts\u2019 model architecture. For example, on a very large graph, we can easily apply techniques to scale up the Mowst computation (e.g., neighborhood or subgraph sampling commonly seen in scalable GNN designs). \n\nOne question worth exploring is, on what types of graphs would Mowst work most effectively. For example, what are the properties of the graph that determines the experts\u2019 specialization, and how to measure the relative importance of features and structures. Another question is, what are the suitable choices for weak & strong experts in non-graph domains (e.g., time-series analysis, computer vision, etc. ). These are interesting questions to explore as future work. \n\n\n## Q10\n> In Section 3, the authors should cite more relevant papers regarding symmetric gating functions in mixture-of-experts models, namely [1], [2], [3], [4].\n\nWe are happy to include the mentioned citations in our revision. We have discussed [1] in the response to Q5. [2,3,4] all present deep theoretical understanding on the symmetric softmax gating. [2] proposes a novel class of modified softmax gating functions to address the slow parameter estimation rate under the vanishing experts\u2019 parameters scenario. [3] performs novel theoretical analysis on the top-k sparse gating, which shows no tradeoff between model capacity and model performance (computation cost & convergence rate), efficacy of the top-1 gate on parameter estimation and the implication on estimating true number of experts. [4] proposes a novel aspect to analyze the convergence rates under Gaussian MoE and proposes a novel Voronoi loss function. \n\nThe above citations are all about symmetric gating. They are clearly different from Mowst because of Mowst\u2019s unique weak-strong combination."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6162/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700124960399,
                "cdate": 1700124960399,
                "tmdate": 1700124960399,
                "mdate": 1700124960399,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YRlSB0OZMv",
                "forum": "wYvuY60SdD",
                "replyto": "6DS4dfHwNt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6162/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6162/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Follow-up from Authors: New Revision"
                    },
                    "comment": {
                        "value": "Dear reviewer,\n\nWe would like to follow up with you regarding our new revision. \n\nWe have significantly improved our paper according to you suggestion. We have:\n* revised Introduction to state the target problem and main motivation more clearly\n* improved description throughout the technical section (Section 2) to make the theoretical statements more readable and easier to understand\n* added missing definitions for theoretical statements\n* rewritten the description on the \"denoising\" process\n\nWe have also included all the suggested references to make our \"Related Work\" more complete. \n\nPlease do not hesitate to let us know of any further comments or questions. \n\nThanks!\n\nAuthors"
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6162/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700471713824,
                "cdate": 1700471713824,
                "tmdate": 1700471713824,
                "mdate": 1700471713824,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "48z2642NVy",
                "forum": "wYvuY60SdD",
                "replyto": "YRlSB0OZMv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6162/Reviewer_jetB"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6162/Reviewer_jetB"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Authors,\n\nThanks for your detailed response, which have addressed many of my concerns. The revision of your paper looks better than the original version. However, based on your response, I am still not convinced about the ability to scale up the model capacity of the proposed model, which is the main goal of this work. I agree that the accuracies for the 2-expert Mowst model and its variant are slightly improved compared to previous methods, but we cannot guarantee that these accuracies will be significantly improved when the number of experts increases. Therefore, I suggest that the authors should study the multi-expert Mowst more extensively, and conduct more experiments on that model to strengthen the paper. For those reasons, I decide to keep my score unchanged. \n\nThank you,\n\nReviewer jetB"
                    }
                },
                "number": 39,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6162/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700583607557,
                "cdate": 1700583607557,
                "tmdate": 1700583607557,
                "mdate": 1700583607557,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "wxve1lfL1i",
            "forum": "wYvuY60SdD",
            "replyto": "wYvuY60SdD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6162/Reviewer_SJRD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6162/Reviewer_SJRD"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a novel prediction strategy designed for scenarios where the model inputs are graphs.  The authors try to improve the model's capacity in graph neural networks (GNN) without raising the computational costs. For making predictions, they introduce two experts: a basic Multi-Layer Perceptron (MLP) and an off-the-shelf GNN. The collaboration between these experts follows a system known as the mixture of experts (MoE), which is referred to as the confidence mechanism.  This gating scheme determines which expert's prediction should be chosen. The proposed solution employs two algorithms for both inference and training. During inference, a prediction is generated for each node, while during training, the MLP and GNN models are updated. The authors demonstrate the superiority of their proposed solution on node classification benchmarks for both homophilous and heterophilous graphs."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper addresses a significant and challenging problem in graph neural networks: model capacity. It demonstrates good quality, offering clear explanations for mathematical aspects, and the appendix provides helpful content. The technical details of the proposed solution are well-explained. The idea of converting the prediction task into an ensemble technique (here mixture of experts, MoE) is impressive. Although the conventional MoE is not a novel method, model ensemble on graphs has received much attention in recent years."
                },
                "weaknesses": {
                    "value": "The paper, in its current state, exhibits several deficiencies that require attention from the authors. The most significant issue with the paper is the absence of a clear presentation of the details of the proposed solution.\n\n- A Minor Issue: A minor issue I've observed in the paper is that on two pages, a significant portion of the content is dedicated to citations and references rather than the core content. On page 1, approximately 40 percent of the introduction is occupied by references, and a similar issue is present on page 6, where content presentation heavily relies on citations. While I understand that this issue may be related to the template used, it can inconvenience readers who may not wish to review all the references. It would greatly benefit the paper if the authors could pay attention to this issue to enhance the reader's understanding.\n\n- An important source of ambiguity arises in Algorithms 1 and 2. Algorithm 1 is responsible for generating predictions using the MLP or GNN, whereas Algorithm 2 focuses on estimating the parameters of the networks. The paper lacks clarity regarding the process when the MLP and GNN have not been trained, and Algorithm 1 does not contain an initialization step for their parameters or hyperparameters. It is imperative that the paper defines the interaction and collaboration between these two algorithms to address this issue. In particular, it is crucial to explain how Algorithm 1 can produce predictions when the experts have not yet been trained.\n\n- The loss functions in (Eq.1) and (Eq.3) play a crucial role in the training process, as they are meant to be minimized. However, an issue arises in these loss functions because all elements within them are known. Specifically, the confidence, and predictions of both experts and the true labels are all given. Algorithm 1 is responsible for generating confidence and predictions, while the true labels are known. This results in the loss function having a fixed value, and it remains unclear with respect to which parameters the minimization task is intended. As a result, the loss function appears to be independent of the experts' parameters. The paper should address and clarify this issue to ensure a proper understanding of the optimization process.\n\n- Confidence mechanism C: The paper introduces a novel gating variable, referred to as confidence, which holds promise for providing appropriate weightings in the Mixture of Experts (MoE) framework. However, a significant concern arises with the use of the random value q. The strategy resembles sampling techniques such as Metropolis-Hastings, where an acceptance ratio is compared with a generated uniform random number. Nevertheless, the fundamental problem here differs as the model's goal is to choose between experts. While the paper presents an innovative confidence mechanism, it ultimately compares it with a completely random value. This approach may not be a precise method for selecting one of the experts, as the confidence mechanism is inherently tied to the experts' predictions, while the ratio q is an independent value. The paper should address this issue to ensure a more reliable method for expert selection.\n\n- Limitations of the proposed solution based on MLP and GNN: It's important to note that available baselines often face challenges when dealing with graph inputs and have their own limitations. The paper introduces two experts, MLP and GNN. However, the use of MLP as an expert should be explored more thoroughly in the paper, particularly due to its potentially weak prediction quality. For example, if Algorithm 1 consistently generates small values for q, resulting in MLP being selected in most cases, questions arise about the guarantee for the prediction quality of the $M_{owst}$ method. Additionally, the paper should delve into the main advantages of using MLP compared to other potential candidates. The current discussion in the paper does not adequately address this scenario. On the other hand, the limitations of using an off-the-shelf GNN have not been sufficiently discussed. The paper should elaborate on how employing GNN as an expert can effectively address its known limitations. A more comprehensive discussion in these areas is necessary to provide a well-rounded understanding of the model's capabilities and potential challenges. \n\n- Computational complexity: While the paper briefly touches upon the complexity of the proposed model, it is imperative to provide a more comprehensive analysis of its computational efficiency, especially within the experimental context. The Abstract highlights the efficiency of the $M_{owst}$ method, making it essential for the authors to include sensitivity analyses related to the model's complexity in the experiments. This would help in quantifying the trade-offs between model performance and computational resources. Furthermore, the cost associated with the confidence mechanism, even if estimated using a simple MLP, should be clearly explained in the paper. Precise details on the computational cost will aid readers in understanding the practical implications of implementing this mechanism. Providing a more in-depth discussion and analysis of these aspects will enhance the paper's completeness and help readers assess the practical feasibility of the proposed approach."
                },
                "questions": {
                    "value": "See discussions in the weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6162/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6162/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6162/Reviewer_SJRD"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6162/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698771275450,
            "cdate": 1698771275450,
            "tmdate": 1700657065060,
            "mdate": 1700657065060,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "R9smN0LblA",
                "forum": "wYvuY60SdD",
                "replyto": "wxve1lfL1i",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6162/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6162/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors' Response (Part 1)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for carefully going through our paper and we greatly appreciate the valuable feedback. We cherish this opportunity to resolve the misunderstanding on our algorithms and to clarify some missing context. \n\n## Citation formatting issue\n\n> on two pages, a significant portion of the content is dedicated to citations and references rather than the core content.\n\nWe thank the reviewer for carefully thinking about how to make our paper more readable. We agree that this is partially an issue with the template. We are working on a revision accordingly. \n\n## Relation between Algorithms 1 & 2\n\n> The paper lacks clarity regarding the process when the MLP and GNN have not been trained, and Algorithm 1 does not contain an initialization step for their parameters or hyperparameters. \n\n> It is imperative that the paper defines the interaction and collaboration between these two algorithms to address this issue.\n\n> it is crucial to explain how Algorithm 1 can produce predictions when the experts have not yet been trained.\n\nWe thank the reviewer for raising this clarity issue. We see that the *numbering* of the two algorithms may have caused this confusion. We will revise this in the revision. \n\n*Algorithm 1* will only be applied **after** the Mowst system has been trained via *Algorithm 2*. And *Algorithm 1* will only be applied on the test set for prediction generation, while *Algorithm 2* will only be applied on the training set for model parameter update. In other words, *Algorithm 1* does **not need an initialization step**. It just executes the forward path of a **trained MLP**, obtains its confidence, and then optionally executes the forward path of a **trained GNN**. \n\nThe relation between *Algorithms 1 & 2* are summarized as follows:\n* *Algorithms 1 & 2* do not interact with each other. Execution of *Algorithm 2* (training) does not involve execution of *Algorithm 1* (inference), and vice versa. \n* *Algorithm 2* is mathematically consistent with *Algorithm 1*, in the sense that the training of *Algorithm 2* optimizes the **expected loss** incurred during the inference of *Algorithm 1*. See more details from our response to the \"Confidence $C$\" question. \n\n*Another aspect of understanding*: *Algorithm 1* defines how to combine the predictions of the MLP and GNN, in order to generate **one single** final prediction for each target node. On the other hand, for training in *Algorithm 2* to proceed, the predictions separately generated by the MLP and GNN are sufficient to compute the loss (as well as the confidence) defined by *Equation 1*, and we don\u2019t need to combine the two predictions into a single one via *Algorithm 1* to execute gradient descent. \n\n*Presentation clarity*: We appreciate that the reviewer raised this question. We will update the description in *Section 2.1* to avoid confusion. Our rationale to describe the inference *Algorithm 1* before the training *Algorithm 2* is that the training objective is designed according to the inference procedure (see response to \"Confidence $C$\").\n\n*Typo correction*: in *Algorithm 2*: $L$ should be $L_{Mowst}$ defined in *Equation 1*."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6162/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700197884787,
                "cdate": 1700197884787,
                "tmdate": 1700197884787,
                "mdate": 1700197884787,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6fVLwgjI6w",
                "forum": "wYvuY60SdD",
                "replyto": "wxve1lfL1i",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6162/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6162/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors' Response (Part 2)"
                    },
                    "comment": {
                        "value": "## Training procedure & parameters\n\n> an issue arises in these loss functions because all elements within them are known. Specifically, the confidence, and predictions of both experts and the true labels are all given. \n\n> Algorithm 1 is responsible for generating confidence and predictions, while the true labels are known. \n\n> This results in the loss function having a fixed value, and it remains unclear with respect to which parameters the minimization task is intended. \n\nWe hope that the response in the above section has helped clarify the confusion here. In summary, *Algorithm 1* is not involved in the training process. The training is to optimize the loss in *Equation 1*, where $p_{v}$ and $p_{v}'$ are generated by the 2 experts. See the description in the \"Training\" paragraph of *Section 2.1*: \"$p_{v} = MLP(x_{v}; \\theta)$ and $p_{v}' = GNN(x_{v}; \\theta')$\" and \"*$\\theta$ and $\\theta'$ are the experts\u2019 model parameters*\". \n\nThus, **$\\theta$ and $\\theta'$ are our learnable parameters**. This is consistent with our statement in *Algorithm 2* that we \"*update MLP (or GNN) weights to $\\theta_{r}$ (or $\\theta_{r}'$)*\". The **loss is not fixed**: update on $\\theta$ and $\\theta'$ will change the predictions $p_{v}$ and $p_{v}'$, and will subsequently change the confidence $C(p_{v})$. Eventually, it will change the loss $L_{Mowst}$. \n\n*Remark*: We can optionally implement a learnable confidence function $C$ (see the end of *Section 2.2*). In this case, the learnable parameters will additionally include the model weights for the confidence neural network. At the end of the \"Training\" paragraph in *Section 2.1*, we described that \"*if $C$ is learnable, we update its parameters together with MLP\u2019s $\\theta$*\". \n\n## Confidence $C$: Sampling via an independent random number is precise\n\n> a significant concern arises with the use of the random value q.\n\n> While the paper presents an innovative confidence mechanism, it ultimately compares it with a completely random value. This approach may not be a precise method for selecting one of the experts, as the confidence mechanism is inherently tied to the experts' predictions, while the ratio q is an independent value. \n\nWe thank the reviewer for asking this question and making a connection with Metropolis-Hastings. We would like to clarify that the goal is **not** to estimate a probability distribution. The functionality of *Algorithm 1* is very straightforward: with probability $C$, we accept the MLP\u2019s prediction. With probability $(1-C)$, we reject MLP\u2019s prediction and accept GNN\u2019s prediction. Thus, the independent random variable $q$ under uniform distribution is used to **exactly** realize such functionality, and so *Algorithm 1* is **precise**. \n\nWe will clarify in the revision that the random number is used so that MLP is activated with probability $C$. \n\nFinally, we would like to provide some additional context regarding why **the training *Algorithm 2* aligns with the inference *Algorithm 1*.** As stated in *Section 2.1*, the fundamental objective of training is to \"*minimize the **expected** loss of inference*\". During inference, since the MLP\u2019s prediction is used with probability $C(p_{v})$, the loss incurred by the MLP is **on expectation** $C(p_{v}) L(p_{v}, y_{v})$. Similarly, the expected loss incurred from the GNN side is $(1 - C(p_{v})) L(p_{v}', y_{v})$. This leads to the total expected loss of Mowst being *Equation 1*."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6162/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700198832701,
                "cdate": 1700198832701,
                "tmdate": 1700198832701,
                "mdate": 1700198832701,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xctKWp3q3B",
                "forum": "wYvuY60SdD",
                "replyto": "wxve1lfL1i",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6162/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6162/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors' Response (Part 3)"
                    },
                    "comment": {
                        "value": "## Mowst discourages the weak MLP from making poor predictions\n\n> if Algorithm 1 consistently generates small values for q, resulting in MLP being selected in most cases, questions arise about the guarantee for the prediction quality of the method. \n\nSuch an extreme case is theoretically possible, but **the probability is so low that it does not actually have practical implications**. \n\nSince $q$ is sampled under a uniform random distribution, the probability of $q < C$ is exactly $C$. In addition, $q$ is independently sampled for each node. So if we consider two nodes $u$ and $v$, the probability that MLP is selected on both nodes is equal to the probability that $q_{u} < C_{u}$ **and** $q_{v} < C_{v}$, which is exactly $C_{u} \\cdot C_{v}$. This means it is **extremely unlikely** that we select MLP simultaneously on these two nodes if both have low confidence. For example, $C_{u} \\cdot C_{v} = 0.01$ if $C_{u} = C_{v} = 0.1$. \n\n**Empirical evidence**: In Table 1, we can compare the variance of the model accuracy under 10 runs. It should be clear that the variance of Mowst-GCN and Mowst-SAGE is not larger than that of the baseline GCN and GraphSAGE. Thus, **the randomness introduced by $q$ is not of practical concern**. \n\n\n> the use of MLP as an expert should be explored more thoroughly in the paper, particularly due to its potentially weak prediction quality. \n\nOur confidence mechanism **discourages the poor MLP predictions from being accepted** by Mowst. In fact, one of our fundamental design principles is that \u201c*the weak expert should only be cautiously activated to avoid accuracy degradation*\u201d (paragraph 1, *Section 2*). \n\nGiven some target nodes, the MLP predictions being poor means that the MLP has higher loss than the GNN. Thus, Mowst has the incentive to decrease the confidence so that the bad MLP contributes less and the overall loss $L_{Mowst}$ (*Equation 1*) is reduced. Such intuition has **rigorous theoretical guarantee**. According to the analysis in *Section 2.3*, on the part of data where GNN achieves lower loss, the MLP will have 0 confidence -- i.e., the MLP takes no effect. On the part of data where MLP achieves lower loss (e.g., on nodes with noisy neighborhood information), the MLP will have positive confidence, but its value may still be less than 1 (exact $C$ value depends on the learnt confidence function) so that the GNN can still play some role. Thus, our gating is not fair between the two experts, and **inherently favors the GNN** (described as \u201c*desirable bias*\u201d in *Section 2.1 & 2.3*). \n\n\n## MLP as a suitable weak expert\n\n> the paper should delve into the main advantages of using MLP compared to other potential candidates. \n\nThere are several reasons for choosing the MLP as the weak expert:\n\n**MLP is a simplified version of GNN**. Each GNN layer generally performs two main steps: *neighbor aggregation* and *feature transformation*. The *neighbor aggregation* combines the feature vectors from the neighbor nodes into a single embedding vector. Afterwards, such an embedding vector goes through a \u201c*transformation*\u201d module which often just performs linear transformation. If we discard the neighbor information, the \u201csimplified\u201d GNN layer only performs a linear transformation on self-embedding from the previous layer, which is exactly the same as the operation of an MLP layer. More concretely, let\u2019s see how a GCN and a GraphSAGE layer reduces to a standard MLP layer:\n* **GCN**: based on the equation here https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GCNConv.html, without neighbors, the term related to the graph adjacency matrix, $\\hat{D}^{-1/2}\\hat{A} \\hat{D}^{-1/2}$ becomes the identity matrix, and so the layer simply performs $X\u2019=X \\Theta$, which is exactly the MLP layer operation before non-linear activation. \n* **GraphSAGE**: based on the equation here https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.SAGEConv.html, without neighbors, the $W_2$ term becomes 0, and the layer performs $x_i\u2019 = W_1 x_i$, which is again exactly the MLP layer operation before activation. \n\nSimilar reduction can be applied to many other state-of-the-art GNN architectures, such as **GIN**. \n\n**MLP is efficient to compute**. Among other choices of a weak expert (e.g., a shallow GNN), MLP is probably the most efficient one to compute, since an MLP does not consider neighbors at all. More details on computation complexity can be seen in the reply in the \u201c*Computation complexity*\u201d section. \n\n**MLP is easy to optimize**. Due to its simple architecture, the training convergence of an MLP is both fast and stable. On the contrary, optimizing a GNN is more challenging due to issues such as oversmoothing (Chen et al., 2020a; Li et al., 2018 in the original submission) and over-squashing (Topping et al., 2022; Alon & Yahav, 2021 in the original submission). Such challenges may still exist even if we use a simplified GNN model as a weak model."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6162/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700207709003,
                "cdate": 1700207709003,
                "tmdate": 1700207879412,
                "mdate": 1700207879412,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "NibgY9Qhlm",
                "forum": "wYvuY60SdD",
                "replyto": "wxve1lfL1i",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6162/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6162/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors' Response (Part 4)"
                    },
                    "comment": {
                        "value": "## Clarification on GNN expert\n\n> The paper should elaborate on how employing GNN as an expert can effectively address its known limitations.\n\nThere are numerous GNN designs in the literature. Different GNN architectures handle different types of information in the neighborhood. For example, GCN performs low-path filtering on the graph signals to smooth out the neighbor features, and GIN simulates graph isomorphism tests to identify meaningful subgraph structures in the neighborhood. \n\nNo matter what specific GNN architecture we choose, the following observations generally hold:\n* Noises in the neighborhood, whether they are noisy edge structures (for GIN) or noisy neighbor features (for GCN), cannot be avoided. \n* Some nodes contain sufficiently rich self-features for the classification task, and thus GNN's neighbor aggregation does not provide obvious advantage. \n\nMotivated by the above, Mowst provides a **model agnostic** solution that can generally benefit many different GNN architectures. In other words, **we do not provide a new way to aggregate neighbor information. Instead, we make the existing neighbor aggregation functionality more robust and of higher-quality**. \n\n## More details on computation complexity\n\n> it is imperative to provide a more comprehensive analysis of its computational efficiency, especially within the experimental context.\n\n\nEmpirically, we observe that \n* The computation of a GNN model is much slower than an MLP. \n* The computation of Mowst is as fast as the baseline GNN. \n\nwhich is consistent with our theoretical analysis. \n\nWe will follow up on the exact numbers on execution time, and include them in the revision. We will also include the following details on complexity analysis in the revision. \n\nTheoretical computation complexity analysis is presented in the last paragraph of *Section 2.6*. We provide some more context here. We consider the forward path of the neural network, as the complexity analysis on backward propagation is similar. For a GNN model, as mentioned in the above \u201c*MLP is a simplified version of GNN*\u201d paragraph, each layer aggregates features from the direct neighbors. So consider an $\\ell$-layer GNN operating on a target node $v$:\n* The $\\ell$-th (i.e., last) layer aggregates information from $v$\u2019s 1-hop neighbors, and outputs a single embedding for $v$ itself.\n* The $(\\ell-1)$-th layer aggregates information from $v$\u2019s 2-hop neighbors, and outputs embeddings for each of $v$\u2019s 1-hop neighbors. \n* \u2026\n* The 1st layer aggregates information from $v$\u2019s $\\ell$-hop neighbors, and outputs embeddings for each of $v$\u2019s $(\\ell -1)$-hop neighbors. \n\nAs defined in *Section 2.6*, let $b_{k}$ be the number of $k$-hop neighbors. For the 1st layer, the \u201cneighbor aggregation\u201d step aggregates the $b_{\\ell}$ features into $b_{\\ell-1}$ ones. The \u201cfeature transformation\u201d step further operates on the $b_{\\ell-1}$ vectors. For simplicity, let\u2019s ignore the cost of neighbor aggregation. If \"feature transformation\" is via a linear transformation (common design choice), then it performs matrix multiplication with a $f\\times f$ weight matrix. So its cost is $f^2 b_{\\ell-1}$. \n\nIn general, for layer $k$, the cost of feature transformation is $f^2 b_{\\ell-k}$. So the total cost is $\\sum_{1\\leq k \\leq \\ell} f^2 b_{\\ell-k} = f^2 \\sum_{1\\leq k \\leq \\ell} b_{\\ell-k} > f^2 (b_{\\ell-1} + \\sum_{2\\leq k \\leq \\ell} 1 ) = f^2 (\\ell + b_{\\ell-1} - 1)$. So the cost of an $\\ell$-layer GCN is $\\Omega(f^2(\\ell+b_{\\ell-1}))$, as shown in *Section 2.6*. \n\nFor an MLP, its complexity can be directly obtained by plugging in $b_{\\ell-1} = 1$ in the above equation (see the reasoning in the \u201c*MLP is a simplified version of GNN*\u201d paragraph). So the MLP complexity is $O(f^2 \\ell)$. \n\nTo compare the complexity of an MLP and a GNN, note that $b_{\\ell-1} \\gg \\ell$. In fact, $b_{\\ell}$ can grow exponentially w.r.t. $\\ell$ on realistic graphs. Such a phenomenon is well-known as \u201c*neighborhood explosion*\u201d in the GNN literature. In summary, consistent with our conclusion in *Section 2.6*, **the computation cost of an MLP is much less than that of a GNN**. \n\n> the cost associated with the confidence mechanism, even if estimated using a simple MLP, should be clearly explained in the paper.\n\n\nThe neural network to compute confidence can be a light-weight MLP (i.e., it can have even lower computation cost than the MLP expert). Even if the confidence MLP has the same structure (and thus the same cost) as the MLP expert, the total cost of the MLP expert *plus* the confidence MLP is **still much less than** the cost of the GNN, according to our above analysis. In the *Section 4* experiments, to simplify hyperparameter search, we let the confidence MLP have an identical structure as the MLP expert. We have noticed that Mowst's execution time is still very close to that of the baseline GNN, thus validating our complexity analysis. We will follow up with the exact timing measurements in the rebuttal as well as in the revision."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6162/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700209153335,
                "cdate": 1700209153335,
                "tmdate": 1700209153335,
                "mdate": 1700209153335,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uoIzJhzixh",
                "forum": "wYvuY60SdD",
                "replyto": "wxve1lfL1i",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6162/Reviewer_SJRD"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6162/Reviewer_SJRD"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment"
                    },
                    "comment": {
                        "value": "I appreciate the authors for their comprehensive rebuttal. The addressed concerns, especially regarding the clarity of Alg1 and Alg2 and their connection to the loss function, have been notably improved in the new version. Consequently, I am revising my score from 5 to 6.\n\nHowever, some shortcomings persist. The claim about robustness and higher quality remains unconfirmed in its current state. The limitations of MLP and GNN, particularly when these experts are selected, haven't been precisely addressed. The simplicity of the model with two experts raises questions about its extensibility for a multiple-experts strategy. Additionally, the gating function is still susceptible to the randomness of the parameter q."
                    }
                },
                "number": 43,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6162/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700657034558,
                "cdate": 1700657034558,
                "tmdate": 1700657034558,
                "mdate": 1700657034558,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "x6XPiJl2dp",
            "forum": "wYvuY60SdD",
            "replyto": "wYvuY60SdD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6162/Reviewer_MV5e"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6162/Reviewer_MV5e"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduced a mixture-of-experts design to combine a weak MLP expert with a strong GNN. The proposed method relies on a confidence mechanism for controlling the contribution of each expert. The idea is to leverage the MLP for learning from nodes with self-expressive features that do not gain much from its connections and largely resort to the GNN for other cases. Their experiments show the superior performance of their method which is also well supported by several theoretical proofs."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper tackles an interesting problem and proposes a relatively simple solution of mixing a weak model like MLP with a strong expert like GNN. \n2. The method is fairly intuitive and the paper is easy to follow. \n3. The proposed method is very effective as shown by the experiments. The experimental setup is extensive and shows the effectiveness of the method in several aspects including ablation study, visualization of learned embeddings and training dynamics.\n4. The paper is theoretically well-supported through theorems and propositions."
                },
                "weaknesses": {
                    "value": "1. Although it has been briefly mentioned in supplementary section C2, it would be interesting to study the effect of the number of experts on the performance. This leads to further questions regarding the choice of multiple weak experts vs multiple strong experts. Is this even helpful in the given task? I think these are interesting follow up questions one could ask.\n2. The github page with code base is private and not accessible via the provided link. \n3. Minor grammatical and spelling errors which need to be improved. \n4. See questions."
                },
                "questions": {
                    "value": "1. Is there any correlation/relationships between the edge density of the graph dataset and performance of the MLP part of the method? Intuitively, if the graph is sparsely connected, one may assume that the MLP contributes more in that case compared to when the graph is densely connected. \n2. What is the $\\circ$ symbol in Proposition 2.2? It has not been defined. \n3. The task for the experiments mentions \"node prediction\". Do the authors mean it's a node classification task?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6162/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698787657174,
            "cdate": 1698787657174,
            "tmdate": 1699636669039,
            "mdate": 1699636669039,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zBxib39PAK",
                "forum": "wYvuY60SdD",
                "replyto": "x6XPiJl2dp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6162/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6162/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors' Response (Part 1)"
                    },
                    "comment": {
                        "value": "We greatly appreciate the positive feedback. Thank you for acknowledging the *effectiveness & simplicity of our solution*, the *clarity of our presentation*, the *significant gains from our experiments* and the *soundness of our theoretical analysis*. \n\n## Code link\n\nThanks for checking! We have double-checked the GitHub setting. Please check again under: https://github.com/mowstgnn/mowst\n\n## Generalization to many experts\n\n> it would be interesting to study the effect of the number of experts on the performance. This leads to further questions regarding the choice of multiple weak experts vs multiple strong experts. \n\nWe very much agree that an empirical evaluation on the many-expert scenario is valuable as future work. In addition, thanks for noticing our existing theoretical analysis on the many-expert generalization in *Section 2.7*. \n\nWe are more than happy to share our understandings on the potential of the many-expert generalization:\n\n**Graph learning task**: if we limit our thinking within the graph learning domain, there are two directions to choose \u201cprogressively stronger\u201d experts. \n1. One simple way is to progressively make the GNN deeper. E.g., an MLP can be seen as a 0-hop GNN, and so expert $i$ can just implement a GNN aggregating $i$-hop neighbor information. \n2. Another way is from the architecture perspective. Some GNN architectures are theoretically more expressive than the others. For example, simplified GNN models like SGC [1] can be an intermediate expert between a weak MLP and a strong GCN. Another possibility is to follow some general theoretical frameworks (e.g., [2]) to construct GNNs with progressively strong expressive power. In this case, the stronger expert do not necessarily have more layers. \n\n*The choices of progressively stronger GNN experts should be dependent on the property of the graph*. For example, if neighbors from many hops away still provide useful information [3], then it makes sense to follow the above direction 1 to make a stronger expert deeper. Otherwise, if most of the useful information concentrates within a shallow neighborhood [4], then it may be better to follow direction 2 to define stronger experts as having more expressive layer architectures. \n\n**Other domains**: the concept of weak and strong experts perfectly holds in other domains like natural language processing and computer vision. E.g., experts may become various forms of Transformers when considering NLP tasks. From our theoretical understanding in *Section 2*, we know that the design of the many-expert Mowst does not depend on any specific model architecture, and thus we see **large potential of generalizing Mowst beyond graph learning**. In addition, we believe that the benefits of many-expert Mowst would be larger when the **data is more complicated and contains more modalities** (e.g., graphs with multimedia features, spatial-temporal graphs, etc.). \n\n**Hierarchical mixture**: another straightforward way to integrate many experts is to use the 2-expert Mowst to construct a hierarchical mixture. The strong expert of the 2-expert Mowst can be an existing MoE model. Thus, the strong expert now contains many \u201csub-experts\u201d which can be controlled by traditional gating modules (e.g., symmetric softmax gating). The interaction between the weak expert and the strong expert is still via the confidence-based gating. \n\n--------------------\nReferences \n\n[1] Felix Wu et al. Simplifying Graph Convolutional Networks. In ICML 2019. \n\n[2] Lingxiao Zhao et al. From Stars to Subgraphs: Uplifting Any GNN with Local Structure Awareness. In ICLR 2022. \n\n[3] Uri Alon and Eran Yahav. On the Bottleneck of Graph Neural Networks and its Practical Implications. In ICLR 2021. \n\n[4] Zeng et al. Decoupling the Depth and Scope of Graph Neural Networks. In NeurIPS 2021."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6162/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700261901269,
                "cdate": 1700261901269,
                "tmdate": 1700261901269,
                "mdate": 1700261901269,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "v9HMbrIOQ9",
                "forum": "wYvuY60SdD",
                "replyto": "x6XPiJl2dp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6162/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6162/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors' Response (Part 2)"
                    },
                    "comment": {
                        "value": "## Correlation between edge density and MLP performance\n\n> Is there any correlation/relationships between the edge density of the graph dataset and performance of the MLP part of the method? Intuitively, if the graph is sparsely connected, one may assume that the MLP contributes more in that case compared to when the graph is densely connected.\n\nThanks for this interesting question! We also believe that there may be some graph properties correlated with the MLP performance. We are running additional experiments on this and will update in the revision. \n\nIntuitively, when the edge density is higher, there could be more neighborhood information. However, it may not be sufficient to just use the edge density alone to reflect the amount of neighborhood information. For example,\n* **Noises in neighbor features** (e.g., heterophily): given two graphs with *identical* edge connections (thus, the same edge density). In graph A, the neighbor node features are useful. In graph B, the neighbor node features are noisy. Then the MLP expert of Mowst will perform better on graph B because the GNN expert is relatively weaker in that case. Realistically, the graph B scenario may correspond to graphs with high heterophily. \n* **Subgraph topology**: the structure / topology of the neighborhood may contain useful information. The amount of structural information, however, is not proportional to the edge density. For example, in a complete graph (each node is connected with all other nodes), the edge density is maximized. However, there is very little structural information, since the neighborhood structures of any two different nodes are identical. On the other hand, if most nodes in the graph have low degree, but there is a small cluster where nodes are densely connected with each other, then the overall edge density is low, but the amount of structural information is high. In this case, it is not the absolute value of edge density but the relative differences in different neighborhood structures that provides useful information. \n\n## Clarification on math symbol\n\n> What is the $\\circ$ symbol in Proposition 2.2? It has not been defined.\n\nIt means the \u201cfunction composition\u201d operation. So $C = G\\circ D$ means $C(x) = G(D(x))$ for input $x$. We will clarify this in the revision. \n\n## Clarification on experimental task\n\n> The task for the experiments mentions \"node prediction\". Do the authors mean it's a node classification task?\n\nCorrect. In the node classification task, the model predicts the class that each node belongs to."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6162/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700264999193,
                "cdate": 1700264999193,
                "tmdate": 1700264999193,
                "mdate": 1700264999193,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BKP3vIWYxE",
                "forum": "wYvuY60SdD",
                "replyto": "v9HMbrIOQ9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6162/Reviewer_MV5e"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6162/Reviewer_MV5e"
                ],
                "content": {
                    "title": {
                        "value": "Acknowledgement of response"
                    },
                    "comment": {
                        "value": "I thank the authors and acknowledge their responses."
                    }
                },
                "number": 24,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6162/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700500312054,
                "cdate": 1700500312054,
                "tmdate": 1700500312054,
                "mdate": 1700500312054,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "8ePywt11mB",
            "forum": "wYvuY60SdD",
            "replyto": "wYvuY60SdD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6162/Reviewer_DN88"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6162/Reviewer_DN88"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a new method named Mowst for processing graphs, which decouples node features and neighborhood structures by employing a mixture of MLP and GNN. These two models are coordinated through a \"confidence\" mechanism that activates the GNN when MLP is uncertain, indicated by the spread of prediction logits.  Empirically, Mowst has proven to enhance accuracy on various node classification benchmarks and operates with a computational cost on par with a single GNN model."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This authors introduced a new method named Mowst that structures by employing a mixture of MLP and GNN. \n\nThe proposed method outperform existing methods in the experiments."
                },
                "weaknesses": {
                    "value": "The manuscript would be improved if the authors could more clearly explain the reasons behind the superior performance of their proposed method compared to existing ones. They attribute their success to the integration of weak and strong experts; yet, it appears that the true advantage may lie in the combination of Multilayer Perceptrons (MLPs) and Graph Neural Networks (GNNs). The distinction is that while MLPs leverage features of individual nodes, GNNs also capitalize on information transmitted across edges. This, I believe, might be the actual contributing factor to their method's effectiveness. To substantiate their claims, the authors should consider conducting additional experiments. Specifically, they could compare the performance of a combination of shallow and deep MLPs, as well as  a combination of shallow and deep GNNs. Such experiments would provide a more convincing validation of their results.\n\nAdditionally, the introduction to the datasets used in the study requires enhancement. The authors should provide a detailed description of how each network dataset is constructed and clarify the specific features attributed to the nodes within these datasets."
                },
                "questions": {
                    "value": "1. Are there experimental results show that the superior performance was due to the integration of weak and strong experts?\n\n2. How is the network in each dataset constructed? What are the features attributed to the nodes within these datasets.?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6162/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699244431790,
            "cdate": 1699244431790,
            "tmdate": 1699636668910,
            "mdate": 1699636668910,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "TMzb73wfZj",
                "forum": "wYvuY60SdD",
                "replyto": "8ePywt11mB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6162/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6162/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors' Response (Part 1)"
                    },
                    "comment": {
                        "value": "We greatly appreciate the valuable feedback from the reviewer. We thank the reviewer for acknowledging the *novelty* of our method as well as the *significant improvements in experimental evaluations*. \n\n## Choices on weak & strong experts\n\n> The manuscript would be improved if the authors could more clearly explain the reasons behind the superior performance of their proposed method compared to existing ones.\n\n> yet, it appears that the true advantage may lie in the combination of Multilayer Perceptrons (MLPs) and Graph Neural Networks (GNNs). \n\n\nThanks for thinking deeply into the reasons behind the performance improvement. To **rephrase the question** based on our understanding, we believe that the reviewer is asking why we pick two different architectures for the two experts, rather than the weaker & stronger versions of the same architecture (e.g., a shallow GNN & a deep GNN). \n\nOur conclusion is that **an MLP does not have a fundamentally different architecture than a GNN, and an MLP is indeed a weaker version of a GNN**. Thus, the current results can support our claim that the performance gains come from the combination of weak & strong experts. \n\nMore specifically, an MLP is a special version of a GNN -- **it is a shallow GNN with 0-hop aggregation**. Let\u2019s take a closer look at the two Mowst variants evaluated in the experiments in *Section 4*, Mowst-GCN and Mowst-SAGE. We show in the following how **a multi-layer GCN and GraphSAGE both reduce to the standard MLP when we perform a 0-hop, shallow neighbor aggregation**: \n* **GCN**: based on the equation here https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GCNConv.html, a GCN layer performs $X' = \\hat{D}^{-1/2}\\hat{A} \\hat{D}^{-1/2} X\\Theta$, where $X$ and $X'$ are node feature matrices before and after the GCN operation, $\\Theta$ is the layer weight matrix, $\\hat{D}$ and $\\hat{A}$ are both related to the graph structure (i.e., degree & adjacency matrix). Without neighbors (i.e., 0-hop aggregation), the term related to the graph structure, $\\hat{D}^{-1/2}\\hat{A} \\hat{D}^{-1/2}$ becomes the identity matrix, and so the layer performs $X\u2019=X \\Theta$, which is exactly the MLP layer operation (here we consider layer operation before non-linear activation). \n* **GraphSAGE**: based on the equation here https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.SAGEConv.html, a GraphSAGE layer performs $x_i' = W_1 x_i + W_2\\cdot mean_{j\\in N(i)}(x_j)$, where $W_1$ and $W_2$ are both weight matrices, and $N(i)$ is the set of neighbors of node $i$. Without neighbors, the $W_2$ term becomes 0, and the layer performs $x_i\u2019 = W_1 x_i$, which is again exactly the MLP layer operation. \n\nA similar reduction procedure can be applied to many other state-of-the-art GNN architectures, such as **GIN**. For this reason, we will also include the Mowst-GIN results in the revision. \n\n> Are there experimental results show that the superior performance was due to the integration of weak and strong experts?\n\nWhile we believe the current MLP + GNN results can well support our claim on weak & strong experts, we greatly appreciate the reviewer\u2019s feedback and are actively running the suggested experiments. We will clarify the above points in the revision."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6162/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700251113008,
                "cdate": 1700251113008,
                "tmdate": 1700251113008,
                "mdate": 1700251113008,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tEI9xBKyYP",
                "forum": "wYvuY60SdD",
                "replyto": "8ePywt11mB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6162/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6162/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors' Response (Part 2)"
                    },
                    "comment": {
                        "value": "## Details on datasets\n\n> How is the network in each dataset constructed? What are the features attributed to the nodes within these datasets.?\n\n\nWe thank the reviewer for the suggestion on enhancing dataset introduction. We will incorporate a more detailed discussion on how the datasets were constructed in the revision. Our benchmarks cover a **diverse set of applications with different kinds of node features**. A brief description is as follows:\n\n**Flickr** is from [1]. Each node in the graph represents one image uploaded to the Flickr online community https://flickr.com. If two images share some common properties (e.g., same geographic location, same gallery, commented by the same user), an undirected edge is drawn between the two corresponding nodes. Node features are 500-dimensional bag-of-word representation of the images.\n\n**ogbn-arxiv** represents a paper citation network which is curated by [2]. Each node is an arXiv paper and each directed edge stands for that one paper cites another one. Node features are the 128-dimensional feature vectors obtained by averaging the embeddings of the words in its title and abstract. The embeddings are generated by the word2vec model trained over the MAG corpurs [3].\n\n**ogbn-products** represents an Amazon products co-purchasing Network which is also curated by [2]. Each node is a product sold in Amazon and edges between two nodes indicate that the two corresponding products are purchased together. Node features are 100-dimensional bag-of-words vectors of the product descriptions.\n\n**Penn94** [4, 5] is a friendship network from the Facebook 100 networks of university from 2005. Each node represents a student. The node features are major, second major/minor, dorm/house, year, and high school.\n\n**Pokec** [4, 6] is the friendship graph of a Slovak online social network, where nodes are users and edges are directed friendship relations. Node features are profile information such as geographical region, registration time, and age.\n\n**twitch-gamer** [4, 7] is a connected undirected graph of relationships between accounts on the streaming platform Twitch (https://www.twitch.tv). Each node represents a Twitch user. An edge means that they are mutual followers. Node features include number of views, creation and update dates, language, life time, and whether the account is dead.\n\n-------------\nReferences:\n\n[1] Zeng et al. GraphSAINT: Graph Sampling Based Inductive Learning Method. ICLR 2020.\n\n[2] Hu et al. Open Graph Benchmark: Datasets for Machine Learning on Graphs. NeuIPS 2020.\n\n[3] Wang et al. Microsoft Academic Graph: When Experts Are Not Enough. Quantitative Science Studies 2020.\n\n[4] Lim et al. Large Scale Learning on Non-Homophilous Graphs: New Benchmarks and Strong Simple Methods. NeuIPS 2021.\n\n[5] Traud et al. Social Structure of Facebook Networks. Physica A: Statistical Mechanics and its Applications 2012.\n\n[6] Leskovec and Krevl. Snap Datasets: Stanford Large Network Dataset Collection. 2014.\n\n[7] Rozemberczki and Sarkar. Twitch Gamers: A Dataset for Evaluating Proximity Preserving and Structural Role-based Node Embeddings. arXiv 2021."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6162/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700251558988,
                "cdate": 1700251558988,
                "tmdate": 1700251558988,
                "mdate": 1700251558988,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "92BRuyGv9B",
                "forum": "wYvuY60SdD",
                "replyto": "8ePywt11mB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6162/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6162/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Revision with new experimental results"
                    },
                    "comment": {
                        "value": "Dear reviewer DN88,\n\nWe have uploaded a revision containing new experimental results on a Mowst variant consisting of a 2-layer GCN (weak expert) and a 3-layer GCN (strong expert) in Appendix B.3. We observe that this weak-strong variant can still lead to accuracy improvement, but the scale of such improvement is overall smaller than the original Mowst design with MLP + GNN. \n\nWe believe such an observation is consistent with our analysis in the main text. Our interpretations are as follows: \n\nIn the original Mowst design, when the two experts specialize, the MLP expert would specialize to nodes with rich self-features but high level of structural noises. The GNN expert would specialize to nodes with a large amount of useful structure information. Following the above logic, in the newly implemented weak-strong variant, the 2-layer GCN expert will specialize to nodes with rich information in the 2-hop neighborhood, but noisy connections in the 3-hop neighborhood. However, in realistic graphs, a shallow neighborhood (e.g., 2-hop) already contains most of the useful structural information [1] (consequently, the accuracy boost by upgrading a 2-layer GNN to a 3-layer one is much smaller than the boost by upgrading an MLP to a GNN). As a result, the 3-hop noises in a 3-layer GNN can be much less harmful than the 1-hop or 2-hop noises. Thus, it would be challenging for the 2-layer GCN to find nodes that it can specialize on. The intuitive explanation is that, the functionality of the 2-layer GCN expert overlaps significantly with the 3-layer GCN expert, and thus the benefits reduces when mixing two experts that play a similar role. \n\nOnce again, thank you for your positive review!\n\nBest,\n\nAuthors\n\n---------\n\n[1] Zeng et al., Decoupling the depth and scope of graph neural networks. In NeurIPS 2021."
                    }
                },
                "number": 48,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6162/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700707510067,
                "cdate": 1700707510067,
                "tmdate": 1700707510067,
                "mdate": 1700707510067,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "112G7B66Pp",
            "forum": "wYvuY60SdD",
            "replyto": "wYvuY60SdD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6162/Reviewer_3gAn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6162/Reviewer_3gAn"
            ],
            "content": {
                "summary": {
                    "value": "The Mowst approach is a novel way of handling self-features and neighborhood structures in GNNs, using a mixture of weak and strong experts with a confidence mechanism to activate the strong expert in low-confidence regions. The authors demonstrate Mowst's effectiveness on multiple benchmark datasets and provide an analysis of its expressive power and computational efficiency. The contributions of the paper encompass an innovative mechanism for expert collaboration, a variant of Mowst designed for directed graphs, and insights into the training dynamics that foster expert specialization and the gradual partitioning of the graph."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper is well-written and organized, with clear explanations of the Mowst approach and its variants.\n- The experiments are comprehensive, with ablation studies on several design choices"
                },
                "weaknesses": {
                    "value": "**Failure cases**: MLPs can exhibit both incorrect predictions and high confidence levels. This is particularly common when dealing with graph datasets featuring imbalanced classes or containing a small number of outlier nodes that may not be sufficiently representative. The authors didn't discuss the limitations of such cases and how mowest may or may not solve them.\n\n**Clarity:** There are several different design choices other than the proposed version. For instance, the authors do not provide a justification for positioning the weak expert at a lower order while placing the strong expert at a higher order. An alternative approach could involve computing confidence using the GNN and then determining the weight for further combining the MLP results. Another version might not involve confidence computation but instead rely on a learnable module to decide which expert is more suitable. Supervision can be obtained through self-supervision, i.e., comparing with a previously trained MLP and a GNN in terms of predictive accuracy on each instance.\n\n**Experimental justification:** The paper lacks results for Mowst*-GIN in Table 1. Including these results would be valuable for assessing whether Mowst can further enhance performance, especially since GIN is the top-performing model on Penn94. Moreover, demonstrating how Mowst can complement different GNN architectures would strengthen the assessment of its utility.\n\n**Experimental comparison:** It may not be fair to compare Mowst with either MLP or GNN in isolation. A more informative comparison could be made between Mowst and a GNN with an MLP skip connection since both have similar expressiveness (theoretically, when the strong expert is activated) and the same number of model parameters.\n\n**Minor:**\n\nIn the abstract, the authors mentioned \"GNN\" before \"... the strong expert is an off-the-shelf Graph Neural Network (GNN).\""
                },
                "questions": {
                    "value": "- How does mowest solve the scenarios where MLPs can exhibit both incorrect predictions and high confidence levels?\n- How do the alternative design choices  (in Weakness 2) compare to mowest?\n- What are the performances of Mowst*-GIN and GNN+MLP sc on Table 2?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6162/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6162/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6162/Reviewer_3gAn"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6162/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699254077078,
            "cdate": 1699254077078,
            "tmdate": 1700723299403,
            "mdate": 1700723299403,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "MBKbuvzSZ8",
                "forum": "wYvuY60SdD",
                "replyto": "112G7B66Pp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6162/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6162/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors' Response (Part 1)"
                    },
                    "comment": {
                        "value": "We greatly appreciate the valuable feedback from the reviewer. We thank the review for acknowledging our presentation quality and overall contributions. In addition to the response below, we are actively preparing additional experiments according to the suggestion. \n\n## Mowst handles the \u201cfailure case\u201d well\n\n> MLPs can exhibit both incorrect predictions and high confidence levels \u2026 when dealing with graph datasets featuring imbalanced classes or containing a small number of outlier nodes. \n\nWe are glad that the reviewer asked about this interesting scenario. Recall that it is the relative performance of the two experts rather than the accuracy of the MLP itself that determines the MLP\u2019s confidence (*Section 2.3*, second last paragraph). Based on this fact, we conclude the following:\n* **Mowst discourages such a failure case by moderating the MLP-GNN interaction** (case 1 below). \n* When the reviewer\u2019s proposed scenario is unavoidable, **we achieve an overall win of the entire Mowst system at the cost of small failure of the MLP expert** (case 2 below). \n\nWe use a case study to illustrate the idea. Detailed theoretical analysis can be found in *Section 2.3*. Let\u2019s say the MLP generates the same prediction on a group of 100 nodes, where 90 nodes belong to class 1 and the remaining 10 belong to class 2. If we consider the MLP model alone, it will learn the dominant class very well and its prediction will be close to $[1, 0]$ -- Let\u2019s say the MLP predicts $[0.9, 0.1]$ on all the 100 nodes. \n\nThus, the MLP is highly confident since $[0.9, 0.1]$ has high dispersion. But its predictions on the 10 minority-class nodes are wrong. In other words, **such 10 nodes correspond to the reviewer\u2019s failure case**. For sake of discussion, let $L_{1}$ be the average loss on the 100 nodes corresponding to the $[0.9, 0.1]$ prediction. \n\nNow let\u2019s see how the Mowst loss in *Equation 1* changes the MLP and GNN behaviors. There are two possibilities for the GNN:\n\n### Case 1\n\nThe GNN can outperform MLP on the 100 nodes. For example, 5 of the 10 minority-class nodes contain useful neighborhood information, and so the GNN can correctly predict 90 + 5 = 95 nodes. Let $L_{2}$ be the GNN loss on the 100 nodes. Reasonably, $L_{2} < L_{1}$. \n\nTo minimize the total loss of *Equation 1*, the MLP has the incentive to reduce its confidence: while doing so will hurt the MLP\u2019s own performance on the 90 majority-class nodes, it will reduce the overall loss of the MLP + GNN system. For simplicity, let\u2019s consider the extreme case: the MLP degrades the initial $[0.9, 0.1]$ prediction to a trivial prediction $[0.5, 0.5]$ (i.e., random guess). Then the MLP loss $L_{1}$ will increase to $L_{1}'$, but $C$ will decrease to $C'=0$ (by *Definition 2.1*). The overall loss becomes $C' \\cdot L_{1}' + (1-C')\\cdot L_{2} = L_{2}$. On the other hand, the loss corresponding to MLP\u2019s original $[0.9, 0.1]$ prediction is $C\\cdot L_{1} + (1-C)\\cdot L_{2} > L_{2}$ since $L_{1} > L_{2}$. Therefore, decreasing MLP\u2019s confidence will reduce the total loss of Mowst, and **the failure case is addressed**. \n\n### Case 2\n\nThe GNN cannot outperform MLP on the 100 nodes. For example, some nodes may contain structural noises. In this case, $L_{2} \\geq L_{1}$. \n\nFollowing the analysis on case 1, if the GNN loss $L_{2}$ is *significantly* higher than the MLP loss $L_{1}$, then the MLP may even increase its confidence further to reduce the total Mowst loss. This seemingly exacerbates the MLP failure. However, the interesting part is that **the MLP sacrifices its performance on the 10 minority-class nodes in exchange for the GNN\u2019s greater success on other nodes in the graph**. When a strong expert performs worse than a weak one, this means that the 100 nodes contain a significant amount of noise. Such noise may directly degrade the GNN model quality, resulting in worse prediction on all nodes in the graph (not just the 100 nodes that contain the noise). The strategy of Mowst is to filter out such noise via high MLP confidence, so that the harm from the MLP's failure case is justified by a higher quality GNN model. \n\n### Remark\n\nin the above cases, why is the tradeoff between the MLP and GNN always worthwhile? This is guaranteed by the design of our loss function (*Equation 1*) and the confidence function (*Definition 2.1*). Since $C$ is learnable (*Section 2.2*), there always exists a set of Mowst parameters such that the overall loss of *Equation 1* is no larger than the loss of an MLP alone or a GNN alone. See the construction of such $C$ in the proof of *Proposition 2.6 & 2.8*. \n\nIn summary, Mowst does not always prevent \u201chighly confident wrong predictions\u201d from happening. However, Mowst always handles it in a beneficial way that reduces the overall prediction loss."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6162/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700037148708,
                "cdate": 1700037148708,
                "tmdate": 1700037148708,
                "mdate": 1700037148708,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "W1jqCO2F3x",
                "forum": "wYvuY60SdD",
                "replyto": "112G7B66Pp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6162/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6162/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors' Response (Part 2)"
                    },
                    "comment": {
                        "value": "## Clarity on alternative designs\n\nWe appreciate the reviewer's suggestion on alternative designs. We have also thought about them and here are our reasonings:\n\n### Order of experts (i.e., MLP-GNN vs. GNN-MLP)\n\n> the authors do not provide a justification for positioning the weak expert at a lower order while placing the strong expert at a higher order. An alternative approach could involve computing confidence using the GNN and then determining the weight for further combining the MLP results\n\nWhen one expert is more powerful than the other, the order of experts matters. Consider Mowst with two experts A & B. According to the analysis in *Section 2.3* (especially the last paragraph), **when we compute confidence based on expert A, Mowst will be biased towards the other expert B**. In other words, on some training nodes, if expert B achieves a lower loss, then Mowst will simply accept predictions from B and ignore those from A. Otherwise, when expert A achieves lower loss, Mowst may still have some non-zero probability (controlled by learnable $C$) to accept B\u2019s prediction. \n\nWhen B is the stronger expert with **better generalization capability**, the above **bias is desirable** when applying Mowst on the *test* set. Since GNN generalizes better (Yang et al., 2023 cited in original submission), we prefer the MLP-GNN order (current design) over GNN-MLP (alternative design). \n\n### Non-confidence based learnable module\n\n> Another version might not involve confidence computation but instead rely on a learnable module to decide which expert is more suitable. \n\nFirst, we would like to point out that **our confidence function is also learnable**. Basically, all our analysis (*Sections 2.3* to *2.7*) is based on a general class of $C$ defined by *Definition 2.1*, which is decomposed as a dispersion function $D$ and a simple scalar function $G$. We fix $D$ as standard dispersion functions such as variance or negative entropy, and use a light-weight neural network to learn $G$ (see end of *Section 2.2*). \n\nSecond, the reviewer\u2019s proposal is reasonable, and we think it is similar to the gating modules in existing Mixture-of-Expert systems (e.g., Shazeer et al., 2017; Wang et al., 2023 cited in original submission). For example, the gating module can be a neural network whose input is the target node feature, and outputs are the weights in front of each expert (the weights can further go through a softmax to ensure the sum across all experts equals 1). Theoretically, such a gate can also simulate our confidence function -- the initial layers of the gating neural net can learn to exactly generate the prediction logits of the MLP expert, and the remaining layers can learn to calculate dispersion and the $G$ function. In this sense, our confidence module can be seen as a specific gate, which is significantly simplified based on the inductive bias of the weak-strong combination. Our confidence-based gating makes the model explainable (*Sections 2.3 & 2.4*), expressive and efficient (*Sections 2.6 & 2.7*). More importantly, it enables Mowst to achieve significantly higher accuracy than GNNs based on traditional gating (e.g., GraphMoE, *Table 1*) due to our simplified design.\n\n## Experimental justification\n\n> The paper lacks results for Mowst*-GIN in Table 1.\n\nWe thank the reviewer for mentioning the Mowst*-GIN results. We agree that including Mowst*-GIN adds value to the paper and we are actively running experiments."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6162/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700037750574,
                "cdate": 1700037750574,
                "tmdate": 1700037750574,
                "mdate": 1700037750574,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YDs8HJtveg",
                "forum": "wYvuY60SdD",
                "replyto": "112G7B66Pp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6162/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6162/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors' Response (Part 3)"
                    },
                    "comment": {
                        "value": "## Experimental comparison\n\n> A more informative comparison could be made between Mowst and a GNN with an MLP skip connection\n\nWhen selecting baselines, we actually have similar consideration as the reviewer that the baselines should have some flavor of residual connection. We would like to clarify that **existing baselines** such as GraphSAGE, GIN, GAT and GPR-GNN all implement some architectural components to **facilitate the propagation of the node\u2019s self-features**. Nevertheless, we are running additional experiments according to the reviewer's suggestion. \n\nSpecifically, for each of the above mentioned baselines:\n* **GraphSAGE**: in each layer, there are two parallel branches (see equation here: https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.SAGEConv.html). Branch 1 performs linear transformation on the self-feature from the previous layer. Branch 2 performs another linear transformation on the mean-aggregated neighbor features from the previous layer. Results of the two branches are summed and sent to non-linear activation, to generate the layer output. Branch 1 can be seen as a **per-layer residue connection**. In addition, we can understand **an $L$-layer MLP as embedded into an $L$-layer GraphSAGE**, if we see all branch 1 of the L layers as a whole. \n* **GAT**: similar to GraphSAGE, each GAT layer contains two parallel branches (see equation here: https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GATConv.html). The main difference from GraphSAGE is that GAT contains additional \u201cattention-based\u201d scaling factors in front of each branch. Likewise, GAT can also be seen as having **dedicated residual connections** integrated into the model. \n* **GIN**: in each layer, the features of all neighbors are summed, while the self-feature is scaled by $(1 + \\epsilon)$, where $\\epsilon$ is learnable (see equation here: https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GINConv.html). Although GIN does not have an explicit residual connection, the model can learn a large $\\epsilon$ to **facilitate the propagation of self-features**. \n* **GPR-GNN**: this model generates prediction via $\\sum_{k=0}^{K}\\gamma_{k} H_{k}$ where $H_{k}$ is the embedding generated from the $k$-hop neighborhood and $\\gamma_{k}$ is a scaling factor. GPR-GNN is like Mowst (with a $K$-layer GNN expert) when $k$ only takes values of $0$ and $K$. Thus, GPR-GNN includes an **explicit residual connection** from the $k=0$ branch. The main difference from Mowst is that GPR-GNN's scaling factor $\\gamma_{k}$ is applied on all graph nodes, while our confidence-based weighting is customized for each node."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6162/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700038266139,
                "cdate": 1700038266139,
                "tmdate": 1700038266139,
                "mdate": 1700038266139,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "er0zrwrus6",
                "forum": "wYvuY60SdD",
                "replyto": "112G7B66Pp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6162/Reviewer_3gAn"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6162/Reviewer_3gAn"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the detalied response! My concerns remain as follows:\n\nFor my comment #1, I think you may oversimplify the scenarios. For example, GNN and MLP can be both biased but towards different directions. What if MLP is right about a low-confidence instance while GNN is wrong? That would be another failure case I meant. I think the cases you mentioned are scenarios where one expert absolutely outweighs another.\n\nFor my comment #2, I am still not so sure about the design choice here. Another concern raised is that, in the revision, \"Our main motivation is to improve model capacity without significant trade-offs in computation overhead and optimization difficulty\" - The computation of GNNs in general is not intensive, and if the authors are mostly motivated by the model capacity, there are lots of works on automatic graph learning, e.g., [1], which I think you should compare with. This could be important to justify the usefulness of this algorithm. And they are closely related the way I see it.\n\nFor my comment #3: Would be interesting to see the experimental results. I feel skip connection, (depends on the implementation ofc) could be very different from self-features propagation in terms of preventing oversmoothing and localization.\n\nFor my comment #4: Does the GNN in your implementation also facilitate the propagation of the node\u2019s self-features? If so, I found it a bit confusing since you pointed out that \"However, many widely-used GNNs have a fundamental limitation since they are designed by global properties of the graph. ....\". Moreover, what are the number of parameters of Mowst and the baselines methods. If the baselines already have local attention on the node features, then another fair comparison should be that they have the same or close number of parameters.\n\n[1] Design Space for Graph Neural Networks. Jiaxuan You, Rex Ying, Jure Leskovec. NeurIPS 2020."
                    }
                },
                "number": 33,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6162/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700556878887,
                "cdate": 1700556878887,
                "tmdate": 1700556892266,
                "mdate": 1700556892266,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ldtxTfMZh6",
                "forum": "wYvuY60SdD",
                "replyto": "112G7B66Pp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6162/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6162/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors' follow-up (Part 1)"
                    },
                    "comment": {
                        "value": "Thank you for the follow-up questions. \n\nWe would like to provide an update regarding the additional experiments based on our new revision, as well as clarify your main concerns in your follow-up comments. \n\n## Computation of GNN is intensive\n\nComputation complexity is one of the major blockers in deploying GNNs on realistic large graphs (here we consider large graphs like social networks or citation networks evaluated in our experiments, rather than graphs in the science domain like small molecules). The fundamental reason behind the high computation complexity lies in the recursive neighbor aggregation, which results in the well-know challenge of \"neighborhood explosion\". \n\n* There are many works in the recent literature aiming to reduce the computation cost of GNNs (e.g., [1,2,3,4,5])\n* One representative realistic example is shown by the seminal work PinSAGE [5], where the GNN model is very small (2 layers, with hidden dimension of 1024 to 2048) and the GNN computation is very expensive (training a single instance takes 3 days on 16 GPUs). The main factor contributing to its high computation cost is the *neighborhood explosion* on a large Pinterest graph. \n* In **Appendix D.3** of the new revision, we have explained in detail \n    * why the GNN computation cost is high\n    * why adding a skip connection is much less efficient than adding an MLP expert in our design, in terms of computation cost. \n\n\n## Skip connection results\n\nFirst, we agree that \"skip connection in each GNN layer\" is very different from \"self-feature propagation via an MLP expert\", both from the architecture perspective and from the computation cost perspective. The fundamental reason is that skip connection in **intermediate** GNN layers will take effect on the neighbors of the target node as well (in fact, for an $\\ell$-layer GNN, skip connection in the layer 1 will operate on the features of the $(\\ell-1)$-hop neighbors), while the self-feature propagation via an MLP expert will only operate on the target node itself. For this reason, a GNN augmented with skip connection is in fact more expensive than a Mowst with an MLP expert. \n\nWe have shown the detailed derivation of the above argument in **Appendix D.3**\n\nSecond, we have implemented new baselines, GCN-skip, GIN-skip as well as H2GCN. For GCN and GIN, we add an additional skip connection on top of GCN and GIN. For H2GCN, it already includes skip connection (specifically, 0-hop, 1-hop and 2-hop propagation in each layer) in its native definition. Results show that\n\n* GCN-skip does not improve accuracy of GCN. \n* GIN-skip improves accuracy of GIN significantly on 3 benchmarks. \n* **Mowst-GIN boosts the accuracy of GIN significantly**\n* **Mowst-GIN-skip & Mowst-H2GCN further boost the accuracy significantly** compared with their corresponding skip-connection baselines. \n\nPlease refer to **Tables 4 & 5** in **Appendix B** for details. \n\n## Fair comparison criteria\n\nWe understand your point that total number of parameters can be a reasonable criteria for fair comparison. However, we also believe that the overall computation cost can be another reasonable criteria. The issue here is that there is not a perfect criteria that makes the baseline and Mowst simultaneously have the similar model size and computation cost. \n\nIn **Appendix A.4** (with the support from **Appendix D.3**), we reasonably conclude that \"keeping the model computation cost similar\" is more important in practice (please refer to the PinSAGE example). \n\nWe will attach the model size numbers soon. \n\n-------------------------\n[1] Fey et al., GNNAutoScale: Scalable and Expressive Graph Neural Networks via Historical Embeddings. In ICML 2021. \n\n[2] Zeng et al., Decoupling the depth and scope of graph neural networks. In NeurIPS 2021. \n\n[3] Shi et al., LMC: Fast Training of GNNs via Subgraph Sampling with Provable Convergence. In ICLR 2023. \n\n[4] Chen et al., Stochastic Training of Graph Convolutional Networks with Variance Reduction. In ICML 2018. \n\n[5] Ying et al., Graph Convolutional Neural Networks for Web-Scale Recommender Systems. In KDD 2018."
                    }
                },
                "number": 37,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6162/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700569585866,
                "cdate": 1700569585866,
                "tmdate": 1700574254792,
                "mdate": 1700574254792,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6L6qduQMUa",
                "forum": "wYvuY60SdD",
                "replyto": "112G7B66Pp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6162/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6162/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Second follow-up from authors"
                    },
                    "comment": {
                        "value": "Dear reviewer 3gAn,\n\nWe have uploaded a new revision after incorporating our responses to your second-round comments. Specifically:\n\n* Appendix E: illustration of how Mowst handles the two failure cases proposed (see \"follow-up response\", Part 2)\n* Appendix F: explanation of potential alternative designs (see response to original review, Part 2)\n\nAs the rebuttal period is about to end, we hope that our **two rounds of comprehensive responses** to your initial and follow-up reviews have addressed your major concerns. We also hope that you find our **new results on Mowst-GIN and the skip connection variants** significant. If so, please kindly consider increasing your score. \n\nOnce again, thank you very much for your time during the review and rebuttal period. We appreciate your feedback and look forward to your reply! \n\nBest,\n\nAuthors"
                    }
                },
                "number": 51,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6162/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700718139786,
                "cdate": 1700718139786,
                "tmdate": 1700718159842,
                "mdate": 1700718159842,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Qp3mIU4mZ6",
                "forum": "wYvuY60SdD",
                "replyto": "6L6qduQMUa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6162/Reviewer_3gAn"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6162/Reviewer_3gAn"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks the author for the response. Sorry for didn't make my points clear.\n\nBy \"failure cases\", I didn't only mean the modeling ability during training time - surely you can avoid the two failure cases in your discussion. However, the failure case can always happen if MLP takes unseen features, and this is conditionally independent with the training process your have.\n\nBy \"local attention\", I didn't mean attention mechanism, but in general, either the self-feature prop or the locationzation by MLP, as opposed to the message-passing integration.\n\nThe authors did address my concern about Experimental justification and partly about Experimental comparison. I appreciate the authors' response and I know it takes large effort. However, my major concerns remain not fully addressed. I changed my evaluation on the soundness but inclined to maintain my initial rating. I will discuss with other reviewers as well.  Besides, I would encourage the authors to keep the response more concise in the future."
                    }
                },
                "number": 52,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6162/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700723234420,
                "cdate": 1700723234420,
                "tmdate": 1700723234420,
                "mdate": 1700723234420,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "MCksgXeUb0",
            "forum": "wYvuY60SdD",
            "replyto": "wYvuY60SdD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6162/Reviewer_BGLX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6162/Reviewer_BGLX"
            ],
            "content": {
                "summary": {
                    "value": "The heterogeneity of node classes raise unique challenges to message passing graph neural networks. Iterative aggregating neighbors can magnify the negative impact of dissimilar nodes on the representation quality. This work tries to deal with this problem by decoupling node presentation into MLP and GNN. MLP mainly captures node-self property and receives much more less impact from neighbor than GNNs. With the help of confidence value calculated based on the output logits of MLP, the proposed approach tends to obtain a better balance on the predictions between homophilous and heterophilous nodes."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Strengths:\n1. This work proposes to combine MLP and GNN for balancing the predictions for homophilous and heterophilous nodes.\n2. Authors add theoretical analysis that are trying to fold out more insight to show the superiority of the proposed loss.\n3. Experiments are conducted on various scale of datasets to demonstrate the performance of the proposed method."
                },
                "weaknesses": {
                    "value": "Weaknesses:\n1. The presentation quality still has lots of space to improve. That's also the main drawback of this work. The motivation is not clear enough. I guess that the main motivation of this work is about how to overcome the challenges raised by heterophily since most of GNNs work well on homophily network. But there is no literature overview on overcoming heterohpily in the related work section. Moreover, no baselines for dealing with heterophily are included in the experimental section. It's better to illustrate the diagram of learning objective to show how the MLP and GNN collaborate to each other, since they are not applied to mode layers.\n\nApart from the missing literature, many claims in this work are not clear. Here a incomplete list. \n  - \"GNNs are both expensive to compute and hard to optimize\". By far, many efficient way to run GNNs are already proposed and applied to solve industrial problems. It should not be the main drawback of advanced GNN layer or model. Comparing with MLP, GNN indeed requires more computation cost, but we still have solutions to make it applicable to deal with large scale data. With the claim, what is it relation to the proposed method? It's difficult see that the proposed method has dominated advantage in terms of time complexity since it still relies on the GNN. \n   - \"a cluster of high homophily, the neighbors are similar, and thus aggregating their features via a GNN layer may not be more advantageous\". The \"similar neighbors\" should be more clear. Assume that this work mainly focuses on node classification according to the experimental tasks, similar neighbors tend to have the same class, but not mean that they should have close similar feature distributions. In this case, aggregating neighbors could bring missing information to the target node for better classification performance.\n  - \"The MLP can help \u201cclean up\u201d the dataset for the GNN, enabling the strong expert to focus on the more complicated nodes whose neighborhood structure provides useful information for the learning task. \" Could you please explain why a weak classifier can provide high-quality dataset for GNNs? From the information presented in this work, I guess it's just use the classification confidence distribution. Suppose that the cleaned nodes by MLP are those the low-quality nodes that have heterophilous neighbors, can the GNNs still distinguish them?\n  -  \"since the confidence score solely depends on MLP\u2019s output, the system is biased and inherently favors the GNN\u2019s prediction. However, such bias is desirable since under the same training loss\". It's difficult to follow the statement. Could you please explain the system is biased by what? why it favors the GNN prediction and the bias is desirable?\n  -  \"Second, our model-level (rather than layer-level) mixture makes the optimization easier and more explainable.\" What is the difference between model- and layer-level mixture, and why the optimization is more easier and explainable?\n  - \"During training, the system dynamically creates a soft splitting of the graph based on not only the nodes\u2019 characteristics but also the two experts\u2019 relative model quality.\" The splitting of graph is difficult to follow, is the graph divided by nodes or edges?\n\n2. The theoretical analysis can be further improved. For me, it's difficult to follow the conclusion from the series of theorems. It comes with weak connections to the proposed learning objective. It's better to make the point stand out with a clear statement, such as the convergence, stability of confidence value, insight about how to select probable confidence value function. By the way, the exact definition of the confidence value function seems to be missed.\n\n3. The experimental results are incrementally improved comparing to previous methods. Typical baselines for overcoming heterophilous issue are ignored, such as but limited to $H_2$$GCN [1].\n\nReferences:\n1. Beyond Homophily in Graph Neural Networks: Current Limitations and Effective Designs, NeurIPS 2020."
                },
                "questions": {
                    "value": "please refer to question above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6162/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6162/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6162/Reviewer_BGLX"
                    ]
                }
            },
            "number": 6,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6162/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699500593006,
            "cdate": 1699500593006,
            "tmdate": 1700532140107,
            "mdate": 1700532140107,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "lvE2hiIran",
                "forum": "wYvuY60SdD",
                "replyto": "MCksgXeUb0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6162/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6162/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors' response (Part 1)"
                    },
                    "comment": {
                        "value": "We appreciate the valuable feedback from the reviewer. We are actively preparing a paper revision according to the suggestions. The main concerns are in the presentation clarity -- specifically, most bullets in \u201cWeakness\u201d are regarding statements in *Introduction*. In the following, we respond to each question based on the **main text of our original submission**.\n\n## Motivation\n> I guess that the main motivation of this work is about how to overcome the challenges raised by heterophily since most of GNNs work well on homophily network. \n\nThe main motivation (*Section 1, paragraph 3*) is to **improve GNN\u2019s model capacity without increasing the computation cost**, where the GNN can be a standard GNN or a scalable one. \n\nOur motivation is not limited to heterophilous graphs. Generally speaking, we target graphs with varying local properties (*Section 1, paragraph 1*). They include \n* graphs with hybrid local-homophily / local-heterophily patterns\n* homophilous graphs with different signal mixing rate in different homophilous local regions\n* graphs, whether they are homophilous or heterophilous, containing some noisy nodes (e.g., outliers, mislabels, etc.)\n\nThe reviewer has the right intuition on how Mowst handles Case 1 (heterophily). We further complete the full picture on the remaining 2 cases: \n* Case 2 (homophily): the GNN needs to learn complicated signal filtering functions for various homophilous regions. The MLP expert simplifies the learning task for the GNN expert by eliminating nodes with rich self-features. Such specialization between the MLP and GNN is in principle consistent with the \u201cnon-universal treatment\u201d idea of NDLS (cited in *Sections 1 & 3*). Yet, Mowst eliminates NDLS\u2019s strong assumption on \u201cmixing time\u201d and thus relies less on manually tuned heuristics. \n* Case 3 (noises): our confidence mechanism leads to a unique MLP-GNN collaboration mode, elaborated in detail in \u201cdenoised fine-tuning\u201d in *Section 2.4*. In short, the small amount of noises in a realistic graph can generate harmful gradients that are significant enough to trap the GNN model in sub-optimality. Our confidence mechanism allows the MLP expert to \u201coverfit\u201d on such noisy nodes, thus providing the GNN with a cleaner dataset to escape its local optimum and further fine-tune. \n\n## Related work & experiments on heterophilous GNNs\n\n> there is no literature overview on overcoming heterohpily in the related work section. Moreover, no baselines for dealing with heterophily are included in the experimental section.\n\n> Typical baselines for overcoming heterophilous issue are ignored, such as but limited to H2GCN\n\nWe appreciate the reviewer\u2019s suggestion on H2GCN. **We have discussed a number of representative heterophilous GNNs**, including H2GCN, in \u201cRelated Work\u201d (*Section 3*). **We have also compared Mowst with state-of-the-art heterophilous GNN baselines** in \u201cExperiments\u201d (*Section 4*). We are working on adding the H2GCN baseline in Experiments. \n\nNote that our Related Work (*Section 3*) has covered a broader & more general literature **beyond** just the heterophilous GNNs. This is consistent with our motivation (see above) that our target graphs include but are not limited to those with high heterophily. Specifically, the heterophilous GNNs in *Section 3* adopt different ways to decouple the information from self-features and (multi-hop) structures. This is the rationale behind Section 3\u2019s current subtitle. Nevertheless, in our revision, we will explicitly point out that these related works are suitable for heterophilous graphs. \n\nJustification to current experiment baselines: Platonov et al., 2023 (citation in original submission) shows that even on heterophilous graphs, the \u201cstandard\u201d GNNs like GCN, GraphSAGE and GAT achieve comparable or better performance than popular heterophilous GNNs. In addition, GPR-GNN is widely considered as a state-of-the-art heterophilous GNN in the literature (e.g., see Lim et al., 2021 and Platonov et al., 2023 in the original submission). As our benchmarks include both homophilous and heterophilous graphs, we believe the **current set of baselines** (MLP, GCN, GraphSAGE, GAT, GIN, GPR-GNN, GraphMoE, AdaGCN) is **reasonable and comprehensive**. Nevertheless, we will add the H2GCN results in our revision."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6162/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699926274514,
                "cdate": 1699926274514,
                "tmdate": 1699926274514,
                "mdate": 1699926274514,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "iUG7rxPcPO",
                "forum": "wYvuY60SdD",
                "replyto": "MCksgXeUb0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6162/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6162/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors' response (Part 2)"
                    },
                    "comment": {
                        "value": "## Scalable GNNs\n\n> By far, many efficient way to run GNNs are already proposed and applied to solve industrial problems. It should not be the main drawback of advanced GNN layer or model.\n\n> It's difficult see that the proposed method has dominated advantage in terms of time complexity since it still relies on the GNN.\n\nOur claim is to improve the model capacity **at a comparable cost as** the backbone GNN. Here the GNN can be a standard GNN, as well as a scalable GNN designed for large graphs (it should be clear from the analysis in *Section 2* that the GNN expert of Mowst can also be an off-the-shelf \u201cscalable\u201d GNN). \n\n*Clarifying reviewer\u2019s misunderstanding*: we do not improve scalability of existing GNNs. Instead, we **improve their model capacity without hurting their scalability**. The reviewer also agrees that to date, GNNs are still very expensive. Even though there have been successful industrial applications, their model computation costs are still very high (e.g., training of the seminal PinSAGE [a] model (2 layers) takes more than 3 days on 16 GPUs). \n\nIn summary, our description, \u201c*GNNs are .. expensive to compute*\u201d, is valid even with the numerous scalable GNN designs existing in the literature. Likewise, it is a valid motivation to improve model capacity without tradeoff in increased computation complexity. \n\n[a] Ying et al., Graph Convolutional Neural Networks for Web-Scale Recommender Systems. In KDD 2018. \n\n\n## Clarification on similar neighbors\n\n> similar neighbors tend to have the same class, but not mean that they should have close similar feature distributions. \n\n> aggregating neighbors could bring missing information to the target node for better classification performance.\n\nWe thank the reviewer for considering various scenarios of homophilous graphs. It is certainly possible that nodes with similar labels can have dissimilar features. We are thus happy to clarify in the revision that \u201csimilar neighbors\u201d here refers to neighbors with similar features, which is also a reasonable scenario for homophilous neighborhoods. \n\nTo provide more context: the main objective of this sentence in *Introduction* is to help readers build up intuitions for typical use cases. When the neighbor features are dissimilar and provide valuable information (i.e., the case mentioned by the reviewer), the MLP expert will be automatically demoted by our confidence mechanism (see *Section 2.3 & 2.4* for the theories behind this behavior). \n\n## Clean up the dataset by the weak MLP\n\n> Could you please explain why a weak classifier can provide high-quality dataset for GNNs?\n\n> Suppose that the cleaned nodes by MLP are those the low-quality nodes that have heterophilous neighbors, can the GNNs still distinguish them?\n\nYes, \u201ccleaning up\u201d the dataset means that Mowst learns a confidence distribution where nodes handled badly by the MLP expert receive low confidence scores. Correspondingly, such nodes have a high $(1-C)$ weight to influence the GNN expert. \n\nThe dataset is \u201cclean\u201d because the GNN will not be \u201cdistracted\u201d by the rich self-features of the nodes assigned to the MLP. Notice the following to clarify confusion:\n* \u201cClean nodes\u201d may not equal to \u201cnodes with high GNN accuracy\u201d. \n* The benefit of \u201ccleaning up\u201d is to improve the accuracy of the **entire Mowst system** (MLP + GNN) rather than the accuracy of the **GNN expert alone**. \n\nFor conceptual illustration, let\u2019s consider a hypothetical case. We divide all nodes in a graph into two sets: \n* $V_1$ consists of 20% nodes whose self-features are rich enough for the classification task. Yet some $V_1$ nodes may have noisy structural information.\n* $V_2$ consists of the remaining 80% nodes, all containing insufficient self features. Some $V_2$ nodes contain useful structural information. \n\nIf we train a baseline GNN on the full graph (i.e., $V_1 + V_2$), it may achieve 90% accuracy on $V_1$ and 65% accuracy on $V_2$, leading to an overall accuracy of 20% * 90% + 80% * 65% = 70%. If we train a Mowst, the MLP expert is assigned with $V_1$, and let\u2019s say it achieves a high accuracy of 91%. The GNN is assigned with $V_2$ (i.e., the cleaned-up dataset), and it may achieve 67% accuracy. Note that the accuracy of the GNN alone drops from 70% on the full dataset to 67% on the \u201cclean\u201d $V_2$. However, the overall accuracy on all $V_1 + V_2$ increases from 70% (baseline GNN) to 20% * 91% + 80% * 67% = 71.8% (Mowst with MLP + GNN). \n\nIn the above example, on $V_1$, the MLP expert can achieve higher accuracy than the baseline GNN because MLP is not affected by structural noise (which can exist in both homophilous and heterophilous graphs). \u201cCleaning up\u201d by removing $V_1$ improves the GNN accuracy on $V_2$ because the GNN \n* does not need to learn how to extract self-features from $V_1$, and\n* is not affected by the structural noises in $V_1$."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6162/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699927477620,
                "cdate": 1699927477620,
                "tmdate": 1700292776005,
                "mdate": 1700292776005,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "A200MuEJY8",
                "forum": "wYvuY60SdD",
                "replyto": "MCksgXeUb0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6162/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6162/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors' response (Part 3)"
                    },
                    "comment": {
                        "value": "## Desirable bias\n\n> Could you please explain the system is biased by what? why it favors the GNN prediction and the bias is desirable?\n\nThe \u201cbias\u201d is conceptually illustrated in the last paragraph of *Section 2.1*, and formally elaborated in the last paragraph of *Section 2.3*. \u201cBias\u201d means that it is harder for the MLP to be activated than the GNN. Rephrasing the last paragraph of *Section 2.3*: given a target node, if the GNN loss is lower than the MLP, then Mowst will always take GNN\u2019s prediction and ignore MLP\u2019s prediction. Otherwise, if the MLP loss is lower than the GNN, there is some non-zero probability that the system will still take the GNN\u2019s prediction. \n\nSuch bias emerges from our confidence-based mixture mechanism. **The extent of such bias is learnable** based on overall training loss. *Section 2.3* theoretically explains factors controlling the bias. \n\nRight after the reviewer\u2019s quote, we have a sentence explaining the reason why such bias is desirable (*Section 1*, last paragraph): \u201csince under the same training loss, a GNN can generalize better than an MLP (Yang et al., 2023)\u201d. We have again re-iterated the \u201cgeneralization\u201d explanation in *Section 2.3*. \n\n\n## Model-level vs. layer-level mixture\n\n> What is the difference between model- and layer-level mixture, and why the optimization is more easier and explainable?\n\nModel-level mixture (our design) means that the two experts \u201cexecute independently through their last layer\u201d (*Section 1*, last paragraph), and then the final predictions of the MLP and the GNN are mixed. In other words, the intermediate / hidden layers of the MLP and the GNN do not interact with each other. \n\nLayer-level mixture (described in *Section 3*, paragraph 1) means that in each layer $i$, each expert will generate its own embedding, and the multiple embeddings will be mixed together as the input embedding to layer $i+1$. \n\nLayer-level mixture can achieve high model capacity, but the experts\u2019 interactions are more complicated. Model-level mixture is easier to optimize because we can completely decouple the execution of the MLP and the GNN layers until when the predictions are generated. Thus, we can fix one expert while optimizing the other (*Algorithm 2*) without worrying about the different convergence behaviors of the MLP and GNN (*Section 2.5*, last paragraph). Model-level mixture is more explainable because the mixture happens after the non-linear activations of all intermediate layers, which facilitates rich theoretical analysis in *Section 2.3 & 2.4*. In addition, it is straightforward to attribute the predictions to a particular expert under model-level mixture. \n\n\n## Graph splitting\n\n> is the graph divided by nodes or edges?\n\n\u201cSplitting\u201d refers to node splitting since we study the node classification task. The splitting means some nodes are assigned with high confidence and thus handled by the MLP, and the remaining nodes are assigned with low confidence and handled by the GNN. \u201cSoft\u201d means the expert assignment is via a continuous confidence value between 0 and 1. \n\n## Clarify of theoretical analysis\n\n> It comes with weak connections to the proposed learning objective. \n\n> It's better to make the point stand out with a clear statement\n\n> exact definition of the confidence value function seems to be missed.\n\nThanks for sharing the feedback. We will update the paper to make the conclusions stand out more clearly. \n\nTo clarify, we have two series of theoretical analyses:\n* Convergence behavior (*proposition 2.2, proposition 2.3, theorem 2.4, corollary 2.4.1*): we have summarized the important conclusions in 3 bullets in the last two paragraphs of *Section 2.3*. The theoretical analysis reveals factors affecting the balance between the weak & strong experts. It also provides a direct explanation on the two major collaboration behaviors described in *Section 2.4*. We thus believe the theoretical analysis **justifies our design choices (including learning objective)** in a solid way. \n* Total loss & expressive power (*proposition 2.5, proposition 2.6, theorem 2.7, proposition 2.8*): these theoretical statements are self-explanatory. They are essential to help readers understand the various important properties of Mowst. \n\n*Exact definition of confidence function*: See *Definition 2.1* for $C$\u2019s formal definition. One good property of our design and analysis is that they apply to a general class of $C$ -- all functions decomposable as a quasiconvex dispersion function $D$ and a monotonic scalar function $G$. In practice, we set $D$ as the variance or negative entropy function, and $G$ as learnable by a light-weight neural network (last paragraph of *Section 2.2*). \n\n\n## Experiments\n\n> Typical baselines for overcoming heterophilous issue are ignored, such as but limited to H2GCN\n\nSee our \"Part 1\" response. We are working on the experiments for H2GCN. As mentioned, we have discussed H2GCN in \u201cRelated Work\u201d (*Section 3*) in our original version."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6162/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699928860241,
                "cdate": 1699928860241,
                "tmdate": 1699928860241,
                "mdate": 1699928860241,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "J1yYxnb4dY",
                "forum": "wYvuY60SdD",
                "replyto": "MCksgXeUb0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6162/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6162/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Follow-up from Authors: New Results & Revision"
                    },
                    "comment": {
                        "value": "Dear reviewer BGLX, \n\nWe would like to give you an update regarding revision and new experimental results. \n\nRegarding your 3 main concerns, we have:\n* uploaded a revision to address your main concerns 1 & 2, and \n* included new results on H2GCN to address your main concern 3. \n\nIn the revision, we have significantly improved the clarify of Introduction, specifically for all your bullets mentioned in Weakness 1. We have also rephrased the description on theoretical results to emphasize how they solidly justify our fundamental design choices. \n\nIn Table 4 of Appendix B.1, we have included new results showing **significant and consistent accuracy improvement of Mowst-H2GCN over H2GCN** (as well as Mowst-GIN over GIN) on the three large-scale heterophilous benchmarks. \n\nFor your convenience, we show the results regarding H2GCN as follows. \n\n|   | Penn94  | pokec  | twitch-gamer  | Avg gain |\n|---|---|---|---|---|\n| GCN | 82.17\u00b10.04 | 76.01\u00b10.49 | 62.42\u00b10.53 | - |\n| Mowst-GCN | 83.19\u00b10.43 | 77.28\u00b10.08 | 63.74\u00b10.23 | - |\n| (gain)  | (+1.02)  | (+0.29)  | (+0.83)  | (+0.71) |\n| GraphSage | 76.75\u00b10.52 | 75.76\u00b10.04 | 61.99\u00b10.30 | - |\n| Mowst-Sage | 79.07\u00b10.43 | 77.84\u00b10.04 | 64.38\u00b10.14 | - |\n| (gain)  | (+2.03)  | (+1.33)  | (+1.05)  | (+1.47) |\n| GIN | 82.68\u00b10.32 | 53.37\u00b12.15 | 61.76\u00b10.60 | - |\n| Mowst-GIN | **84.56**\u00b10.31 | 76.11\u00b10.39 | 64.32\u00b10.34 | - |\n| (gain)  | (+1.88)  | (+22.74)  | (+2.56)  | (+9.06) |\n| **H2GCN**  | 82.71\u00b10.67  | 80.89\u00b10.16  | 65.70\u00b10.20  | - |\n| **Mowst-H2GCN**  | 83.39\u00b10.43 | **83.02**\u00b10.30  | **66.03**\u00b10.16  | - |\n| **(gain)**  | (+0.68)  | (+2.13)  | (+0.33)  | (+1.05) |\n\nWe observe that \n* H2GCN is effective in handling heterophilous graphs, as it achieves high accuracy among baselines. \n* Applying Mowst on top of H2GCN can further enable significant accuracy improvements (**+1.05** on average for the 3 heterophilous graphs). \n* We observe **consistent and significant accuracy boost** by Mowst on 4 different GNN architectures (the gains are especially large on GIN). \n\nWe believe the above additional results, together with the original results in Table 1, concretely show that our **improvements are significant** rather than incremental. Please do not hesitate to let us know of any questions. \n\nThanks!\n\nAuthors"
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6162/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700471249537,
                "cdate": 1700471249537,
                "tmdate": 1700503725700,
                "mdate": 1700503725700,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CShrQjA6hp",
                "forum": "wYvuY60SdD",
                "replyto": "J1yYxnb4dY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6162/Reviewer_BGLX"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6162/Reviewer_BGLX"
                ],
                "content": {
                    "title": {
                        "value": "Upgrade my score based on the response"
                    },
                    "comment": {
                        "value": "Dear Authors:\n\nThanks so much for the detailed response. I go back to check the updated manuscript. The revised one looks much better. I'd like to update my score since most of my concerns are addressed very well.\n\nMinor issues:\nIt likely misses the exact kind of confidence function C that is used across the experimental section since it has different choices. It's better to indicate it out in the main content. \n\nBest regards"
                    }
                },
                "number": 31,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6162/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700532673913,
                "cdate": 1700532673913,
                "tmdate": 1700532673913,
                "mdate": 1700532673913,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]