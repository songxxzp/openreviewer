[
    {
        "title": "GeoDiffusion: Text-Prompted Geometric Control for Object Detection Data Generation"
    },
    {
        "review": {
            "id": "pDIITSXmny",
            "forum": "xBfQZWeDRH",
            "replyto": "xBfQZWeDRH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission145/Reviewer_VRFf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission145/Reviewer_VRFf"
            ],
            "content": {
                "summary": {
                    "value": "This paper aims to generate high-quality detection dataset via text-to-image diffusion models. The main novelty lies in GeoDiffusion is that it encodes not only the bounding boxes but also extra geometric conditions such as camera views in selfdriving scenes.\nTo embed the bounding box locations, it discretizes the continuous coordinates by dividing the image into a grid of location bins, creates unique location tokens, and inserts the  to the text encoder vocabulary. To embed other conditions along with bounding boxes, it uses a prompt template \u201cAn image of {view} camera with {boxes}\u201d. To enforce the model focus on foreground and balance the bounding boxes of different sizes during training, it proposes an area re-weighting method to dynamically assign higher weights to smaller boxes.\nIt is claimed that GeoDiffusion outperforms previous L2I methods while maintaining faster training time."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is well written. Despite the missing of some important baselines, the paper contains very thorough experiments, demonstrating the effectiveness of the proposed method."
                },
                "weaknesses": {
                    "value": "1. The baselines, such as ControlNet and GLIGEN are missing in Table 3 and Table 6. Similarly ControlNet is missing in Table 5. They are apparently more comparable than those GAN-based methods. The claimed improvement in the introduction (+21.85 FID and +27.1 mAP) is questionable and misleading as they are obtained by comparing with old GAN-based methods.\n2. Related to 1), Stable Diffusion is not a good option for comparison of layout-based inpainting in Figure 5 as Stable Diffusion does not consume bounding box explicitly. I suggest to replace it with ControlNet and GLIGEN for a fair comparison.\n3. Related to 1), the importance of camera views is not clearly due to the missing baselines. Furthermore, there are no qualitative illustration of the effect of camera views except a single example in Figure 1."
                },
                "questions": {
                    "value": "In Table 4, the proposed GeoDiffusion outperforms other methods except in AP50. ControlNet significantly outperform all the other method at AP50 by a very large margin. I am quite curious what leads to this? I think it worths some discussion in the paper."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission145/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission145/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission145/Reviewer_VRFf"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission145/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697703431203,
            "cdate": 1697703431203,
            "tmdate": 1700694524674,
            "mdate": 1700694524674,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "WDpFl7PgAT",
                "forum": "xBfQZWeDRH",
                "replyto": "pDIITSXmny",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission145/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission145/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer VRFf"
                    },
                    "comment": {
                        "value": "Thank you for your valuable and constructive reviews. Here we address your questions one by one.\n\n---\n\n**W1. Additional baseline methods on NuImages and COCO-Stuff datasets.**\n\nThank you for your suggestions. In the revision, we have 1) reported the fidelity and trainability performance of ControlNet and GLIGEN on the NuImages dataset in Table 2 and 3 respectively, 2) reported the trainability performance of ControlNet on the COCO dataset in Table 5, 3) reported the inpainting performance of ControlNet and GLIGEN on the COCO dataset in Table 6, and 4) refine the claimed improvement to be more specific in the introduction section. For detailed experimental results, please refer to **the common response to Reviewers WbtJ and VRFf**.\n\n---\n\n**W2. Additional baseline methods for the inpainting comparison in Figure 5.**\n\nWe provide a qualitative comparison between GeoDiffusion and Stable Diffusion in Figure 5 primarily to demonstrate how exactly the geometric-awareness enhances the inpainting ability. As shown in Figure 5, GeoDiffusion can better understand the requested inpainting locations especially under multi-object circumstances, and therefore, better deal with the inpainting requests.\n\nFollowing your suggestions, we further provide comparison with ControlNet and GLIGEN in Figure 5, where GeoDiffusion still demonstrates more superior inpainting results. \n\n---\n\n**W3. More discussion on the camera view control generation.**\n\n**(1) Importance of camera views** has been verified by the ablation study conducted in Table 10 and Appendix B. Specifically, we train a GeoDiffusion without camera view tokens with all the other settings unchanged. As reported in Table 10 (rows 1 and 2), the variant without camera view tokens is surpassed by the default GeoDiffusion with respect to both the image quality (*i.e.*, FID) and layout consistency (*i.e.*, mAP), revealing the importance of camera views.\n\n**(2) More qualitative comparison of camera views** is provided in Figure 10 following your suggestions.\nAs demonstrated, GeoDiffusion can effectively convert the camera views simply by changing the camera view tokens in text prompts, while maintaining consistent with the given semantic layouts. \n\n---\n\n**Q1. Discussion on the high AP50 but low mAP and AP75 performance of ControlNet in Table 4.**\n\nThis is primarily because ControlNet can only take image-level condition inputs instead of instance-level ones as GeoDiffusion.\n\n**(1) Implementation of ControlNet.** To facilitate the layout-to-image generation, ControlNet is implemented by converting the semantic layouts to $C$ binary semantic masks, with each mask for a single semantic class, following the ControlNet paper. Therefore, the model can no longer distinguish different instances of the same class. When two instances of the same class are close with each other, the model would tend to generate a single but large instance to cover the areas of both instances, which is a typical reason to obtain a high AP50 but low mAP and AP75.\n\n**(2) Qualitative verification.** A qualitative example is provided in Figure 5. As demonstrated, ControlNet completes the inpaiting requests by considering the left two people as a single instance, and therefore, only complements the right part of the human body."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission145/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700544276371,
                "cdate": 1700544276371,
                "tmdate": 1700544276371,
                "mdate": 1700544276371,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9lsucZZYng",
                "forum": "xBfQZWeDRH",
                "replyto": "WDpFl7PgAT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission145/Reviewer_VRFf"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission145/Reviewer_VRFf"
                ],
                "content": {
                    "title": {
                        "value": "Response to the authors"
                    },
                    "comment": {
                        "value": "Thanks for the additional experiments and the clarifications. The response has solve the majority of my concerns. I have increased my rating."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission145/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700694497901,
                "cdate": 1700694497901,
                "tmdate": 1700694497901,
                "mdate": 1700694497901,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "bxu8uPupVh",
            "forum": "xBfQZWeDRH",
            "replyto": "xBfQZWeDRH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission145/Reviewer_qLFr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission145/Reviewer_qLFr"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes GeoDiffusion that transforms locations/layouts of objects and view directions into a prompt which can be fed as input to text-to-image diffusion models. The generated data by the proposed GeoDiffusion is beneficial to improve object detection performance, especially for categories within the low-data regime."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is written well. The method is simple and intuitive to understand.\nThe results are promising, especially in improving object detection performance."
                },
                "weaknesses": {
                    "value": "Though the paper claims that the model supports flexible geometric conditions, the current model only supports location tokens and 6 view directions. However, geometric conditions may include depth, exact 3D locations and angles, etc. It is unclear whether the approach is \u201cflexible\u201d enough for these conditions.\n\nIt is unclear why the proposed method is better at trainability on object detection, compared to other layout-control synthesis methods such as GLIGEN. The authors mentioned that the method excels at low-data object categories but where does the advantage come from?\n\nThe paper exceeds the 9 page limit by ICLR, which violates the authors\u2019 code of ICLR.\n\nMinor typos. It should be \u201c...embarrassingly simple\u2026\u201d rather than \u201c...embarrassing simple\u2026\u201d"
                },
                "questions": {
                    "value": "See the second point in the weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission145/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698641761660,
            "cdate": 1698641761660,
            "tmdate": 1699635939564,
            "mdate": 1699635939564,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "3tdOPA5rmD",
                "forum": "xBfQZWeDRH",
                "replyto": "bxu8uPupVh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission145/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission145/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer qLFr"
                    },
                    "comment": {
                        "value": "Thank you for your acknowledgement and valuable suggestions. Here we address your questions one by one.\n\n---\n\n**W1: Extension to 3D geometric controls.**\n\nThank you for your insightful comments. It is nature for GeoDiffusion to intergrate various geometric controls, as long as they can be discretized and integrated into the text prompts. As highlighted, our current work primarily demonstrates the integration of camera views, a vital aspect in 2D object detection for autonomous driving, as evidenced in our ablation study (Table 10, rows 1 and 2).\n\nTo address the extension into 3D domains, one needs to incorporate 3D conditions, such as depth, exact 3D locations, and angles, into the generation process. This can be achieved by projecting 3D LiDAR coordinates onto the 2D image plane, followed by text-prompted 2D conditional generation, akin to the process used in GeoDiffusion. Such methodologies have been preliminarily explored in recent studies [1, 2]. Limited by the scope of this paper, our primary focus is on data generation for 2D object detection. The implementation of support for 3D geometric controls is left as a subject for future work.\n\n---\n\n**W2. Advantages of GeoDiffusion over methods such as GLIGEN, particularly in terms of trainability for object detection.**\n\n**(1) Utilization of foundational pre-trained models.** Unlike GAN-based methods, a significant advantage of GeoDiffusion lies in leveraging large-scale pre-trained text-to-image diffusion models, such as Stable Diffusion. These models enable the generation of highly realistic and diverse detection data, which is crucial for effective data augmentation.\n\n**(2) Transferability of the text encoder.** In contrast to existing methods like GLIGEN and ControlNet, which require designing specific bounding box encoding modules and training parameters from scratch, GeoDiffusion capitalizes on the transferability of the text encoder. This approach allows for more efficient adaptation and reduces the need for annotated training data. This is particularly beneficial for long-tailed classes with scarce data annotations. For instance, GeoDiffusion shows remarkable improvement in rare classes such as trailers and construction vehicles, achieving +11.9 and +15.0 mAP, respectively, compared to GLIGEN in the Top-2 rare classes on NuImages, as demonstrated in Table 2. The efficacy of the text encoder's transferability for geometric conditions is further supported by our ablation studies in Table 9 (rows 1 and 2).\n\n**(3) Usage of the foreground prior re-weighting.** This strategy significantly enhances generation performance by focusing on the modeling of foreground objects. The positive impact of this approach is evident in the comparison between the results in Table 9 (rows 2 and 5).\n\n---\n\n**W3-W4. Paper formatting and typos.**\n\nSorry for the careless mistakes. We have fixed the formatting and typo problems in the revision.\n\n---\n\n[1] Gao R, Chen K, Xie E, et al. Magicdrive: Street view generation with diverse 3d geometry control[J]. arXiv preprint arXiv:2310.02601, 2023.\n\n[2] Yang K, Ma E, Peng J, et al. BEVControl: Accurately Controlling Street-view Elements with Multi-perspective Consistency via BEV Sketch Layout[J]. arXiv preprint arXiv:2308.01661, 2023."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission145/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700544145387,
                "cdate": 1700544145387,
                "tmdate": 1700544145387,
                "mdate": 1700544145387,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Tm6HSndoXB",
                "forum": "xBfQZWeDRH",
                "replyto": "3tdOPA5rmD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission145/Reviewer_qLFr"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission145/Reviewer_qLFr"
                ],
                "content": {
                    "title": {
                        "value": "After reading author response"
                    },
                    "comment": {
                        "value": "Thanks for the authors to post responses to my questions. I'm satisfied with most but one remaining issue is designing a specific bounding box encoding module should not be counted as a drawback, as long as it works well. Besides, the proposed method also leverages annotation of bounding boxes during training, which makes no huge difference to those approaches. I also enjoy a more free-form of controllability. It is just that the comparison, especially the rationale, to GLIGEN style layout-to-image approaches should be made more clear and sound."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission145/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700673930391,
                "cdate": 1700673930391,
                "tmdate": 1700673930391,
                "mdate": 1700673930391,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "5fJwe8QFXe",
            "forum": "xBfQZWeDRH",
            "replyto": "xBfQZWeDRH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission145/Reviewer_wbtJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission145/Reviewer_wbtJ"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a method that utilizes a pretrained T2I diffusion model, e.g., Stable Diffusion, for generating images given the box layout. Also, the pair of images and provided box layouts can be used to train an object detector. They propose a technique to encode the box layout into the text prompt and fine-tune the whole network except the VQ-VAE. Furthermore, the authors propose a reweighting mask for balancing between foreground and background regions to generate small objects with higher quality. The experiments show the effectiveness of their methods in three aspects: fidelity, trainability, and generability."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The ablation study is done thoroughly including the parameter choices. \n2. This paper provides a clear metric for validating their results against other generative methods. For fidelity, trainability, and universality, this paper surpasses other methods by a large margin.\n3. The authors show the effectiveness of this method in the aspect of data augmentation for training an object detector."
                },
                "weaknesses": {
                    "value": "1. Given the fact that recent diffusion-based methods such as ControlNet, ReCo, and GLIGEN can also do Layout-to-Image tasks, the authors should include them in Table 2 (Comparison of generation fidelity on NuImages) and Table 3 (Comparison of generation trainability on NuImages). Current comparisons in Tables 2 and 3 are not up-to-date.\n2. The proposed camera-dependent conditioning prompt is very similar to the view-dependent prompting in DreamFusion. The authors need to ablate more on why their method is novel compared to the one proposed in DreamFusion.\n3. The idea of using synthetic images generated from the model to augment training object detectors is reasonable. However, the authors need to ablate how much data is needed to train GeoDiffusion (in the Trainability section), since if the number of real data (and box annotations) required to train GeoDiffusion is significantly larger than the numbers needed to train an object detector, the application of this paper is not realistic.\n4.  The method is sensitive to the image size and aspect ratio of each dataset. In other words, the number of bins is different across datasets, therefore, greatly affecting the transferability of the proposed approach when trained on a dataset and tested on another dataset. In contrast, other baselines such as Copy and Paste Synthesis can use Stable Diffusion to generate the object of interest better in this situation.\n5. Lack of experiments of long-tail (rare) object detection datasets such as LVIS and domain adaptation such as GTAV to Cityscapes or Cityscapes to Foggy Cityscapes. In these settings, the importance of synthetic data is more significant than in the domain where real images and box annotations are largely available. \n6. Among these two contributions, which one is more important?\n7. It would be better to move the ablation study from supp (table 9, 10) to the main paper."
                },
                "questions": {
                    "value": "1. Does the proposed camera-view conditioning work on other autonomous-driving datasets such as Waymo Open Dataset\n2. Can the model generate rare cases in the dataset? For example, can the model generate diverse night-time, foggy, raining scenes in NuImages? Another interesting question is can the model generate more samples of rare classes on a long-tailed dataset (LVIS).\n3. In Table 2 (Comparison of generation fidelity on NuImages), there is a version of GeoDiffusion with an input resolution of 800x456, how is this obtainable while Stable Diffusion input is 512x512?\n4. How do the location bins resolutions affect generation in testing? For instance, if the location bins are trained at 256x256, can the model generate a prompt with a 512x512 grid in test time?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission145/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission145/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission145/Reviewer_wbtJ"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission145/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698680994885,
            "cdate": 1698680994885,
            "tmdate": 1700726570740,
            "mdate": 1700726570740,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "I3OAKbHidD",
                "forum": "xBfQZWeDRH",
                "replyto": "5fJwe8QFXe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission145/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission145/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer wbtJ (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for your valuable and constructive reviews. Here we address your questions one by one.\n\n---\n\n**W1. Additional baseline methods on NuImages in Tables 2 and 3.**\n\nAs discussed in Section 4.3, we compared GeoDiffusion with recent diffusion-based methods, including ControlNet, ReCo, and GLIGEN, on the COCO-Stuff dataset using publicly available checkpoints. In these comparisons, GeoDiffusion has demonstrated significant superiority.\n\nFollowing your suggestions, we have also reported the performance of ControlNet, ReCo, and GLIGEN on the NuImages dataset in Tables 2 and 3, where GeoDiffusion continues to show consistent improvement. \nFor detailed experimental results, please refer to **the common response to Reviewers WbtJ and VRFf**.\n\n---\n\n**W2. Comparison between the camera-dependent generation in GeoDiffusion and the view-dependent prompting in DreamFusion.**\n\n**(1) Controllability.** In GeoDiffusion, camera view tokens are explicitly utilized as input conditions to generate images from different camera views. This approach differs fundamentally from DreamFusion, where view-dependent prompting serves primarily as prior knowledge to enhance the supervision signal in training the NeRF model. DreamFusion still relies on explicit camera parameters to generate images from various views. In contrast, our method leverages direct input conditions, offering more intuitive controllability over the image generation process.\n\n**(2) Purpose.** Our primary goal with camera views in GeoDiffusion is not to introduce a novel view encoding method but to demonstrate that text prompts can serve as a unified representation for various geometric conditions, facilitating independent manipulation without interdependencies. As illustrated in Figure 10, the simple conversion of camera view tokens in GeoDiffusion can directly generate images from different perspectives while maintaining semantic integrity. This showcases our method's ability to handle geometric conditions more flexibly than DreamFusion.\n\n---\n\n**W3. Ablation study on the amount of real data required to effectively train GeoDiffusion.**\n\nIn Section 4.2.2, under the \"necessity of real data\" paragraph, we detail an experiment designed to ascertain the minimum quantity of images needed to train GeoDiffusion efficiently. \nOur approach involves the \"subset\" mode, where we randomly select subsets of the real image dataset (*e.g.*, 10%, 25%, 50%, and 75%) for training individual GeoDiffusion models. Each of these models then generates synthetic images to augment their respective subsets. Subsequently, these augmented datasets, comprising both real and synthetic images, are used to train object detectors.\n\nAs illustrated in Figure 1(b) and reported in Table 8, our findings reveal that GeoDiffusion can achieve consistent performance improvements even with as little as 10% of the real images. Notably, the benefits of GeoDiffusion are more pronounced under data-scarce conditions. This demonstrates that the amount of real data required to train GeoDiffusion is considerably less than what would typically be needed to train an object detector directly, thereby enhancing the practicality and applicability of our approach.\n\n---\n\n**W4. Transferability across datasets and comparison with copy-and-paste synthesis.**\n\n**(1) Different number of bins across datasets affecting transferability.** To address your concern, we would like to clarify that in our experiments, all optimization hyperparameters, including the number of pixels in each location bin, learning rates, and foreground loss weights, have been consistently maintained across both datasets (NuImages and COCO-Stuff). We acknowledge that we have not explicitly tested the model's transferability from one dataset to another. However, the diversity of classes and scenarios covered in NuImages and COCO-Stuff provide a robust assessment of our method's transferability. The distinct scenarios presented in the training and test sets of NuImages, in particular, attest to the effectiveness of our approach across varied contexts.\n\n**(2) Comparison with copy-and-paste synthesis.** In the comparison with copy-and-paste methods, it's important to highlight that our paper's primary objective is to introduce a unified framework capable of not only generating high-quality detection data but also supporting geometric-aware image generation and inpainting requests. While copy-and-paste methods may offer greater flexibility in data augmentation, they typically fall short in generating realistic images and providing additional geometric controls. For a more comprehensive comparison with related works, we invite you to review the detailed analyses presented in Table 1 and Section 2 of our main paper."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission145/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700544031122,
                "cdate": 1700544031122,
                "tmdate": 1700544088078,
                "mdate": 1700544088078,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2PREpMCOPb",
                "forum": "xBfQZWeDRH",
                "replyto": "5fJwe8QFXe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission145/Reviewer_wbtJ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission145/Reviewer_wbtJ"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks the authors for their response. I found the response just partly addressed my concerns. The concern regarding training in one dataset and testing in another dataset (different image sizes or aspect ratios in transfer testing) is not addressed, so basically, cannot be handled right? Also, the long-tail and domain adaptation is not addressed either. Therefore, I can only marginally increase my score to 6."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission145/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700699963575,
                "cdate": 1700699963575,
                "tmdate": 1700699963575,
                "mdate": 1700699963575,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "eTar0n4L0W",
                "forum": "xBfQZWeDRH",
                "replyto": "5fJwe8QFXe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission145/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission145/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer wbtJ"
                    },
                    "comment": {
                        "value": "Thank you so much for your acknowledgement of this work. Here we address your concerns one by one.\n\n---\n\n**Q5. Transfer between datasets with different image sizes and aspect ratios.**\n\nUsually, it is difficult to require large-scale image datasets like COCO-Stuff to have a unified image size and aspect ratio for all the images.Therefore, in order to perform mini-batch-based optimization, we follow common practices [2, 3, 4] to first resize all images to a unified resolution $H\\times W$ (*e.g.*, $H\\times W=512\\times512$ for the default Stable Diffusion) as the model input shape during training, while during testing, the model will first generate images of $H\\times W$, which are then resized to the original image resolution through (bilinear) interpolation. Thus, transfer between different datasets with different image sizes and asepct ratios would not be a problem here.\n\n---\n\n**Q6. Further discussion on the application for long-tailed and domain adaptation.**\n\n**(1) Further discussion on long-tailed generation.** Following your suggestions, we conduct a preliminary experiment on the LVIS dataset with the exact same optimization recipe with the COCO-Stuff dataset as discussed in Section 4.3, and provide a qualitative evaluation in Figure 12, where the annotations of **\"rare classes\"** are highlighted in the images. As shown in Figure 12, GeoDiffusion demonstrates superior generation capabilities even for the long-tailed rare classes. We will keep improving the performance in the final revision.\n\n**(2) Further discussion on domain adaptation.** As shown in Figure 9, GeoDiffusion has already demonstrated flexible domain adaptation capabilities between **daytime, rainy and night** circumstances, while the generation of foggy images are currently not support primarily due to the **absence of foggy images** in the NuImages dataset. We will further explore this specific application in the future work.\n\n*Besides, we notice that the score of the paper in the OpenReview System has not been changed. To complete the score-changing procedure, you might also need to edit the **\"Rating\"** option in your official review. Thank you.*  \n\n---\n\n[2] Rombach R, Blattmann A, Lorenz D, et al. High-resolution image synthesis with latent diffusion models. In CVPR. 2022.\n\n[3] https://github.com/CompVis/latent-diffusion\n\n[4] https://github.com/huggingface/diffusers/"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission145/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700726403330,
                "cdate": 1700726403330,
                "tmdate": 1700726550938,
                "mdate": 1700726550938,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "7sDqbwNnSR",
            "forum": "xBfQZWeDRH",
            "replyto": "xBfQZWeDRH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission145/Reviewer_2g8v"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission145/Reviewer_2g8v"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a data generation pipeline to utilize diffusion models to generate text-prompted data with flexible geometric controls. The results are impressive in many 2D applications. It proves that the generated data can be used to improve the training of object detectors, providing the effectiveness of this line of research."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+ The paper is easy to follow with clear motivation. The writing is good. \n\n+ The experiments are extensive with clear explanations. It is verified on a wide variety of tasks.\n\n+ It is verified that the layout-image models could be used to facilitate the conventional object detection pipeline, which is very useful."
                },
                "weaknesses": {
                    "value": "Some results need to be discussed in more details. How to apply into the 3D domain deserves to be discussed. Please see questions below."
                },
                "questions": {
                    "value": "1. Inconsistent performance in some metrics. For example, FID with 512 setting is better in Table 2. The mAP metric for Ped and Cone is slightly lower in Table 3. Any discussions on this?\n\n2. As claimed in Section 4.2.1, the proposed method achieves 4X acceleration in the training procedure, how is it measured?\n\n3. The pipeline is quite straightforward with diffusion process in an encoder-decoder architecture. The results are impressive. What's the key ingredient? Why previous methods with more complicated design fail to achieve the performance?\n\n---\nMinor: catpion in Figure 2 (b), should be \"utilized\" to train."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission145/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission145/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission145/Reviewer_2g8v"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission145/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698760271433,
            "cdate": 1698760271433,
            "tmdate": 1700624814620,
            "mdate": 1700624814620,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BDt8Jk7F0j",
                "forum": "xBfQZWeDRH",
                "replyto": "7sDqbwNnSR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission145/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission145/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 2g8v (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for your acknowledgement and valuable suggestions. Here we address your questions one by one.\n\n---\n\n**W1: Extension to 3D Geometric Controls.**\n\nThank you for your insightful comments. It is nature for GeoDiffusion to integrate various geometric controls, as long as they can be discretized and integrated into the text prompts. As highlighted, our current work primarily demonstrates the integration of camera views, a vital aspect in 2D object detection for autonomous driving, as evidenced in our ablation study (Table 10, rows 1 and 2).\n\nTo address the extension into 3D domains, one needs to incorporate 3D conditions, such as depth, exact 3D locations, and angles, into the generation process. This can be achieved by projecting 3D LiDAR coordinates onto the 2D image plane, followed by text-prompted 2D conditional generation, akin to the process used in GeoDiffusion. Such methodologies have been preliminarily explored in recent studies [1, 2]. Limited by the scope of this paper, our primary focus is on data generation for 2D object detection. The implementation of support for 3D geometric controls is left as a subject for future work.\n\n---\n\n**Q1. Justification of inconsistent performance metrics.**\n\n**(1) FID score for the 512 $\\times$ 512 model is better than that of the 800 $\\times$ 456 variant in Table 2.** This can be attributed to the alignment between the pre-trained resolution of the Stable Diffusion model and the fine-tuning resolution of the 512 $\\times$ 512 model. The Stable Diffusion model, in our study, is originally pre-trained at a resolution of 512 $\\times$ 512. When we fine-tune this model at the same resolution (512 $\\times$ 512), it results in a more effective transfer of learned features and parameters. This alignment of resolutions minimizes the discrepancies that might arise from adapting to different resolutions, and thereby enhancing the model's performance in terms of the FID score. However, when fine-tuning at a different resolution (as in the case of the 800 $\\times$ 456 variant), the model faces the challenge of adapting to a new resolution, which can lead to a slight decrease in FID score.\n\n**(2) Lower mAP for pedestrian and traffic cone than the real-image-only baseline in Table 3.** In Section 4.2.1, we discuss the challenges that GeoDiffusion faces, particularly with the high variance and small objects, such as pedestrians and traffic cones, where the performance of our model is slightly lower compared to the Oracle real-image-only baseline. This indicates that the quality of the generated data for these two categories may not be sufficient to significantly enhance the performance of object detectors. Similar difficulties are encountered by other generative models as well. As shown in Table 3, except for the real-image baseline, our method outperforms other generative competitors, including LostGAN, LAMA, Taming, ReCo, GLIGEN, and ControNet. We are the first to demonstrate that generated data can serve as effective data augmentation to improve the overall performance of object detectors. We will continue to focus on improving the unsatisfactory performance in generating high variance and small objects in our future work.\n\n---\n\n**Q2. Clarification on the 4X training acceleration in Section 4.2.1.**\n\nSorry for the confusion caused. The measure of training acceleration is calculated based on the number of training epochs required to achieve a comparable level of performance. In Table 2, we demonstrate that our GeoDiffusion model, trained for only 64 epochs, significantly outperforms models like LostGAN, LAMA, and Taming, which were trained for 256 epochs. This indicates that GeoDiffusion achieves the same or better performance four times faster in terms of epochs, hence the 4X acceleration in training. To ensure clarity, we have revised the manuscript to explain this comparison more explicitly."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission145/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700543925921,
                "cdate": 1700543925921,
                "tmdate": 1700543925921,
                "mdate": 1700543925921,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jZe1BxuL46",
                "forum": "xBfQZWeDRH",
                "replyto": "7sDqbwNnSR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission145/Reviewer_2g8v"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission145/Reviewer_2g8v"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the clarification! I have read the rebuttal and most of my concerns have been addressed. I would raise my score to clear accept."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission145/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700624776660,
                "cdate": 1700624776660,
                "tmdate": 1700624776660,
                "mdate": 1700624776660,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]