[
    {
        "title": "Unified Language-Vision Pretraining in LLM with Dynamic Discrete Visual Tokenization"
    },
    {
        "review": {
            "id": "LoN85FAvHI",
            "forum": "FlvtjAB0gl",
            "replyto": "FlvtjAB0gl",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3261/Reviewer_54hM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3261/Reviewer_54hM"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a vision-language framework with unified objectives for both language and image.\nThe crux of the proposed method is the design of a dynamic discrete visual tokenization module.\nIn particular, the proposed LaVIT leverages a dynamic token sparsification approach to embed the given image with fewer vision tokens.\nAfter that, the learned tokenizer is employed to generate discrete vision tokens.\nIn this way, the proposed framework can achieve a unified pre-training objective."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The proposed method achieves very impressive performance on several vision-language tasks such as VQA and image captioning.\n- The strategy to pre-train the vision-language models with a unified objective seems interesting.\n- The method can generate high-quality images based on the given textual prompt."
                },
                "weaknesses": {
                    "value": "- The structure of the writing, especially the method section, is very confusing to me.\nIn fact, the key to the proposed method is the learning of the vision token codebook.\nThis codebook and token selector are built on the first stage of learning.\nHowever, the authors' description of this part makes it look like an end-to-end training framework, which confuses me a lot.\nI would suggest the authors carefully revise the structure of this section.\n\n- The authors claim their method can achieve in-context learning capability. \nHowever, I cannot find any experiments pertaining to this merit.\n\n- The authors did not provide a good explanation of why inequitable treatment for language and vision harms multi-modal learning.\n\n- More details about the language-vision pre-training part are required."
                },
                "questions": {
                    "value": "- Is it possible that the impressive performance of the proposed method is due to more training data, especially for the vision tokenization codebook?\n\n- Why do the authors still use the unselected tokens as keys and values? \nOne intuitive approach for these tokens is simply discarding them.\nMoreover, these tokens should contribute minorly to the final performance and are redundant as authors claimed.\n\n- For the causal attention, which tokens are seen as previous ones? \nAs there is actually no sequence for an image, using causal attention for image encoding is thus not convincing.\n\n- What is the function of the second half of Eqn. 4?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3261/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697531153898,
            "cdate": 1697531153898,
            "tmdate": 1699636274489,
            "mdate": 1699636274489,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "DZ1EcIento",
                "forum": "FlvtjAB0gl",
                "replyto": "LoN85FAvHI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3261/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3261/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 54hM - Part1"
                    },
                    "comment": {
                        "value": "We sincerely appreciate the time and effort you dedicated to reviewing our paper and providing valuable feedback. Below are our responses to the raised questions.\n\n\n1. *The structure of the writing, especially the method section, is very confusing to me.  In fact, the key to the proposed method is the learning of the vision token codebook. ...  I would suggest the authors carefully revise the structure of this section.*\n\n* Sorry for the confusion caused by our description of the methodology section. We will make further revisions to the presentation of this section based on your constructive suggestions. We would appreciate it if you have any other detailed revision suggestions. \n\n2. *The authors claim their method can achieve in-context learning capability. However, I cannot find any experiments pertaining to this merit.*\n\n* Thanks for pointing out this. The \"in-context capability\" mentioned in this work means that the model can take multi-modal prompt input as context and successfully generate context-related contents. We have evaluated LaVIT\u2019s few-shot in-context learning ability on the downstream multi-modal understanding benchmarks. Specifically, we sample several examples from the training data to make task-specific prompts in the form of interleaved [image, text]. For a fair comparison, the few-shot samples are selected by employing the Retrieval In-Context Example Selection (RICES) strategy [3] following the existing methods [1,2]. As observed, LaVIT can learn to complete specific tasks given only a few examples in the form of demonstration. Furthermore, the performance of LaVIT will improve as the number of support samples increases. This phenomenon is consistent across the three benchmarks, demonstrating its remarkable multi-modal in-context learning potential. \n\n| Method |  | VQAv2 |  | | Vizwiz |  |\n| :-----:| :----: | :----: | :----: | :----:|:----: |:----: |\n|  | k=2 | k=4 | k=8 | k=2 | k=4 | k=8 |\n| KosMos-1 | 51.4 | 51.8 |  51.4 | 31.4 | 35.3 | 39.0 |\n| Flamingo-9B | - | 56.3 | 58.0 | - |  34.9 | 39.4 |\n| Emu | 56.4 | 58.4 | 59.0 | 37.8 | 41.3 | **43.9** |\n| Ours | **66.5** | **67.2** | **67.8** | **38.5** | **41.7** | 42.1 |\n\n\n* We also provide more visualization examples of the multi-modal in-context image synthesis results in Figure 9 of the appendix. It can be seen that LaVIT is capable of understanding the semantics of multi-modal prompts and generating the context-related images without any other fine-tuning. \n\n3. *The authors did not provide a good explanation of why inequitable treatment for language and vision harms multi-modal learning.*\n\n* The training objective of most existing multi-modal large language models is centered on predicting the textual descriptions dependent on visual content, where the visual input is merely regarded as the prompts. Since there is no supervision for visual modality input, the model only learns to generate text based on images and can not possess the capacity to generate images like language. Therefore, we claim that \"the inequitable treatment of different modal inputs severely constrains the model\u2019s potential, limiting it to only performing comprehension tasks like generating text based on images\" (instead of \"harms multi-modal learning\"). We hope this clarification will address your concerns.\n\n\n4. *More details about the language-vision pre-training part are required.*\n\n* The detailed training hyperparameter settings are reported in Table 9 of our appendix. Please let us know if there are any training details that we have not expressed clearly.\n\n5. *Is it possible that the impressive performance of the proposed method is due to more training data, especially for the vision tokenization codebook?*\n\n* For a fair comparison, we employ the same multi-modal pre-training data as BLIP-2 to exclude the performance gains from additional training data. All the used training data (including about 93M samples for understanding and 100M samples for image generation) are from the publicly available datasets introduced in Section 3.3. This data scale is smaller than that of most other methods. For example, the concurrent work EMU use 2.6B data, KOSMOS adopts 3.1B image-caption pairs, and CM3Leon-7B leverage 340M closed-source data. In comparison, our LaVIT has achieved better performance while utilizing relatively less multi-modal data. Besides, the visual codebook is trained using pure images from the same multi-modal data mentioned above without the corresponding captions. This stage does not require additional data."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3261/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700203262667,
                "cdate": 1700203262667,
                "tmdate": 1700203277211,
                "mdate": 1700203277211,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fn7cQ7yJgf",
                "forum": "FlvtjAB0gl",
                "replyto": "LoN85FAvHI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3261/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3261/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 54hM - Part2"
                    },
                    "comment": {
                        "value": "6. *Why do the authors still use the unselected tokens as keys and values? One intuitive approach for these tokens is simply discarding them. Moreover, these tokens should contribute minorly to the final performance and are redundant as authors claimed.*\n\n* Admittedly, merely using the selected tokens can approximately encode the global semantics of the image content. However, in our experiments, we have found that directly discarding the unselected patches may result in some information loss, such as the semantics of some background objects, which may impact the performance of multimodal understanding. For instance, in visual question answering tasks, if the question refers to these background objects, the model may be unable to provide the correct answer. Therefore, we design the token merger to compress the semantics of unselected patches onto the retained ones instead of directly discarding them. This process is fulfilled according to the semantic similarity between these patches, so the unselected tokens serve as keys and values. We also conducted the ablation study to explore the role of token merger. As shown by the below Table, we can observe a distinct performance drop in multi-modal understanding tasks without the token merger. In summary, the combination of token selector and merger contributes to reducing the redundancy among tokens while maximally preserving the image details.\n\n| Setting  | Flickr | VQAv2 | OKVQA | GQA |\n| :-----:| :----:|:----: |:----: |:----: |\n| w/o Token Merger | 70.8 |  51.9  |  42.1 |  39.7  | \n| w/ Token Merger | **72.7**   |  **56.4**   | **46.2** | **42.3** | \n\n\n7. *For the causal attention, which tokens are seen as previous ones? As there is actually no sequence for an image, using causal attention for image encoding is thus not convincing.*\n\n* In the causal attention, the remaining merged visual tokens are flattened into the 1D sequence in the raster order, where the previous tokens of each one are originally on its top left side. Though the original input image is 2D shaped, the usage of causal attention will contribute to converting 2D visual features into a 1D sequence with causal dependency in the output space of token merger. To validate the effectiveness, we replace causal attention with bi-directional attention in the token merger. The results are shown in Table 6 of the appendix. Compared to bi-directional attention, adopting causal attention achieves superior performance across all multi-modal understanding tasks. This phenomenon is reasonable as the next-token prediction objective of LLM requires the causal dependency between input tokens, using causal attention will ensure the consistency with the LLM.\n\n| Setting  | Flickr | VQAv2 | OKVQA |\n| :-----:| :----:|:----: |:----: |\n| Bi-directional Attention | 69.9 |  55.6  |  44.1 | \n| Causal Attention | **73.2** | **57.3** | **46.4** | \n\n\n8. *What is the function of the second half of Eqn. 4?*\n\n* The second half of Eq-4 serves as a regularization term to impose constraints to minimize the number of retained tokens. We expect to preserve the whole semantic information of an image with as few tokens as possible. \n\nThank you again for your insightful reviews and valuable suggestions. Please let us know if there are further questions.\n\n---\n\n[1] Quan Sun, et al. Generative pretraining in multimodality, arXiv preprint 2023\n\n[2] Jean-Baptiste, et al. Flamingo: a visual language model for few-shot learning, NeurIPS 2022\n\n[3] Zhengyuan, et al. An empirical study of gpt-3 for few-shot knowledge-based vqa, AAAI 2022"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3261/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700203319132,
                "cdate": 1700203319132,
                "tmdate": 1700203319132,
                "mdate": 1700203319132,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "7L7TnbitmB",
            "forum": "FlvtjAB0gl",
            "replyto": "FlvtjAB0gl",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3261/Reviewer_L5Te"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3261/Reviewer_L5Te"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes the idea of treating images and text equally, transforming images into a series of discrete tokens to input into LLM, and training LLM from scratch to complete understanding and generation. The experimental part of the paper yielded so-called advanced results."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper proposes to convert the image into a series of discrete tokens and adjust the token length according to the image content.\n2. The visualization of discrete tokens is displayed in Experiments to help readers understand the semantic content carried by tokens.\n3. Better results than the baseline method are achieved in the experiment."
                },
                "weaknesses": {
                    "value": "1. The novelty of this paper is limited and the ideas of the paper have already been explored in previous work. For example, end-to-end training LLM has been explored in Emu[MR1], and the transformation of visual information into discrete tokens have been explored in [MR2, MR3, MR4, MR5].\n\n2. The generation process relies on a stable diffusion model, which makes the verification of the visual generation capabilities of the proposed model limited. Do the image generation results rely on strong stable diffusion, or are they more attributable to the discrete labels generated by the model? It is unclear to what extent the good results achieved are due to stable diffusion.\n\n3. Visual token merger part is not well validated. How much additional complexity does this stage bring to the model? Are different codebooks needed for data in different domains (animation, painting, natural landscape, etc.)? Why is a complex merger mechanism needed? Can a simple attention map from a pre-trained model indicate the importance of individual image patches?\n\n3+. BTW, the processing of the decoder (using the stable diffusion model) is the same as Emu's [MR1] strategy. A reference to the corresponding location and some justifications are required.\n\n4. In the experimental results in Table 3, using a larger number of tokens (fixed) achieves worse results, and more explanations need to be added.\n\n\n\n\n[MR1] Generative Pretraining in Multimodality. 2023.\n[MR2] Image as a Foreign Language: BEIT Pretraining for Vision and Vision-Language Tasks. 2023.\n[MR3] Visual Transformers: Token-based Image Representation and Processing for Computer Vision. 2020.\n[MR4] Learning to Mask and Permute Visual Tokens for Vision Transformer Pre-Training. 2023.\n[MR5] Beit v2: Masked image modeling with vector-quantized visual tokenizers. 20022."
                },
                "questions": {
                    "value": "Refer to weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3261/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697943584071,
            "cdate": 1697943584071,
            "tmdate": 1699636274399,
            "mdate": 1699636274399,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "YXCqAQfeHO",
                "forum": "FlvtjAB0gl",
                "replyto": "7L7TnbitmB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3261/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3261/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer L5Te - Part1"
                    },
                    "comment": {
                        "value": "We sincerely appreciate the time and effort you dedicated to reviewing our paper and providing valuable feedback. Below we clarify the raised concerns one by one.\n\n1. *The novelty of this paper is limited and the ideas of the paper have already been explored in previous work. ... has been explored in Emu[MR1], and the transformation of visual information into discrete tokens have been explored in [MR2, MR3, MR4, MR5]*\n\nWe would clarify the unique technical contributions of LaVIT as follows: \n* (1) This work mainly focuses on exploring what makes a good visual token design when extending text-oriented LLM to deal with multimodal inputs. Although the vector quantization for visual features was explored in some previous works [2,3,4,5], directly applying these codebook techniques to tokenize images into discrete tokens trained with LLM are sub-optimal. As a key difference from textual tokens, visual tokens (image patches) exhibit a notable interdependence. In other words, the mutual information between different visual tokens is bigger compared to text tokens, making the semantics of a visual token can be deduced from its adjacent patches. This is also why the majority of mask image modeling methods [4,5] employ an exceptionally high mask rate for image patches. Analogously, such visual interdependence will render the next-token paradigm less effective through self-supervision. Hence, the proposed dynamic visual tokenizer introduces the token selector and merger to pick the most informative tokens and merge the trivial ones. This mechanism contributes to reducing the redundancy and interdependence among visual patches, rendering the unified next-token prediction objective more effective. We have conducted experiments about different visual tokenization techniques on the same 30M pre-training data to validate this. As observed, these methods are not specially tailored for LLM and thus achieve inferior performance compared to LaVIT. \n\n| Method | Codebook Size |  Flickr | GQA | VQAv2 | OKVQA |\n| :-----:| :----: |  :----: | :----:|:----: |:----: |\n| VQ-VAE | 8192 | 60.5 |  35.0  | 48.8 | 40.2 |\n| BEIT-2 | 8192 | 65.8  | 38.3 | 51.1 | 41.4 |\n| Ours | 8192 | **70.0** |  **40.8**  | **53.0** | **44.5** |\n\n* (2) The developed visual tokenizer can produce tokens with a dynamic sequence length varying from the complexity of the image content itself. However, the existing methods tokenize the image into a long and fixed sequence length (e.g., 512 or 1024). Given that the attention computation in LLM exhibits a quadratic relationship with respect to the token length, a long sequence will invariably reduce the computation efficiency. The tokens sparsification of LaVIT's tokenizer thus enables efficient multi-modal training and inference (as shown in Table 3 (b)).\n\n* (3) The same discrete format for two modalities' tokens contributes to the unification of both training and inference. For training, both visual and textual tokens can be optimized under a unified next-token classification objective indiscriminately. In inference, the LaVIT can take any modality combinations as input and produce both visual and textual content by generative sampling. Although the concurrent work Emu [1] also proposes to unlock the LLM during pre-training, it optimizes the visual modality by regressing the next visual features. As validated by Table 3 (a) in Section 4.3, the inconsistent optimization objectives for image and text are not conducive to unified multi-modal modeling within LLM. Therefore, our LaVIT can achieve superior performance than Emu using less training data and smaller model size."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3261/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700202969764,
                "cdate": 1700202969764,
                "tmdate": 1700202969764,
                "mdate": 1700202969764,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KF1ydd70vn",
                "forum": "FlvtjAB0gl",
                "replyto": "7L7TnbitmB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3261/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3261/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer L5Te - Part2"
                    },
                    "comment": {
                        "value": "2. *The generation process relies on a stable diffusion model, which makes the verification of the visual generation capabilities of the proposed model limited. Do the image generation results rely on strong stable diffusion ... It is unclear to what extent the good results achieved are due to stable diffusion.*\n\n* The denoising U-Net in LaVIT serves as a pixel decoder to recover image pixels (rendering) from the corresponding discrete visual token. In other words, this U-Net is responsible for mapping from discrete space to pixel space rather than generating new image contents based on input prompts. As shown in Figure 7 of the appendix, the reconstructed images from token IDs by the U-Net exhibit a high degree of semantics and general structure to the original images. Therefore, the quality of the generated image (i.e., prompt-following, aesthetic, diversity) is predominantly determined by the discrete visual token generated by the capability of LaVIT rather than this decoder. This claim is further supported by evidence from the following two aspects:\n\n\n* (1) Despite the weight of conditional U-Net in LaVIT is initialized from the Stable Diffusion v1.5, the LaVIT achieves the superior text-to-image generation performance than the SD v1.5 (7.4 (Ours) v.s. 9.9 (SD 1.5) in FID metric as in Table 2). Hence, this improvement stems from the LaVIT's capability to better correlate the visual and textual tokens. \n\n* (2) Both GILL[6] and Emu[1] have leveraged the same pre-trained weight of Stable Diffusion v1.5 like ours as the weight initialziation for the decoder. After training on the same data (i.e., the open-source LAION-Aesthetics), they still achieved inferior performances than LaVIT (e.g., 7.4 (Ours) v.s. 12.2 (GILL) and 11.7 (Emu)) as shown in Table 2. \n\n* Therefore, all the above evidence can fully demonstrate that the good zero-shot image generation results are attributed to the strong capability of LaVIT. \n\n\n3. *Visual token merger part is not well validated. ... Why is a complex merger mechanism needed? Can a simple attention map from a pre-trained model indicate the importance of individual image patches?*\n\n* **Why is a complex merger mechanism needed?** The token merger aims to compress the semantics of unselected visual patches onto the retained ones according to their feature similarity. Compared to directly discarding these unselected patches, this merge mechanism not only reduces the redundancy among patches but also maximally preserves the visual details. The ablation study to explore the role of token merger is shown as follows. As observed, the introduction of this module can significantly improve the performance of multimodal understanding.\n\n| Setting  | Flickr | VQAv2 | OKVQA | GQA |\n| :-----:| :----:|:----: |:----: |:----: |\n| w/o Token Merger | 70.8 |  51.9  |  42.1 |  39.7  | \n| w/ Token Merger | **72.7**   |  **56.4**   | **46.2** | **42.3** | \n\n* **attention map from a pre-trained model.** Although the attention map from the pre-trained ViT can reflect the importance of individual image patches to some extent, it is difficult to manually determine a threshold based on attention weights to select which tokens should remain as input to LLM. In contrast, the proposed token select and merge mechanism can dynamically pick the most informative visual tokens based on the complexity of the image content itself.\n\n* **Additional complexity.** The token merger consists of 12 transformer blocks. It has approximately 280 million parameters, comprising just 3% of the total model. This allocation of parameters does not significantly increase the complexity of the model.\n\n* **Different codebooks needed.** Our LaVIT does not require different codebooks for data from different domains. The learned codebooks can be adapted to diverse domains. As shown in Figures 10 and 11 of the appendix, LaVIT can generate images with different styles, such as oil painting, DSLR photos, animation, and landscape."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3261/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700203096959,
                "cdate": 1700203096959,
                "tmdate": 1700203096959,
                "mdate": 1700203096959,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BdNnXkbmeB",
                "forum": "FlvtjAB0gl",
                "replyto": "7L7TnbitmB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3261/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3261/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer L5Te - Part3"
                    },
                    "comment": {
                        "value": "4. *BTW, the processing of the decoder (using the stable diffusion model) is the same as Emu's [MR1] strategy. A reference to the corresponding location and some justifications are required*\n\n* Thank you for pointing out the lack of discussion with the Emu decoder. There are the following main differences between these two decoders. (1) The Emu's decoder takes the continuous visual embeddings yielded by LLM as the conditions to generate images. In contrast, our pixel decoder is conditioned on the discrete visual token IDs. It contains an additional module to recover the dynamic discrete token sequence back to the corresponding visual semantic features $x_{\\text{rec}}$. The pixel decoding procedure of LaVIT is based on the reconstructed $x_{\\text{rec}}$. (2) The training strategies for two decoders are different. the training of Emu's decoder requires the image-text pairs and LLM, during which text is first fed into LLM to generate the visual embeddings. However, our decoder only takes pure image data as input and does not need the combination with LLM during training (only at first stage of training), which reduces the computational overhead and the requirement of the rare multi-modal training data. We have added these discussions in the appendix following your valuable suggestions.\n\n\n5. *In the experimental results in Table 3, using a larger number of tokens (fixed) achieves worse results, and more explanations need to be added.*\n\n* The fixed tokenizer variant transforms all the image patch features into the visual tokens as input to LLM during pre-training.  As mentioned above, the redundancy and high interdependence between the image patches may affect the effectiveness of next-token prediction through self-supervision. In comparison, the proposed dynamic tokenizer can reduce the trivial visual redundancy and thus achieve superior zero-shot performance. \n\nThank you again for the detailed reviews of our paper. We hope these responses have settled your concerns. Please let us know if there are further questions or concerns.\n\n---\n\n[1] Quan Sun, et al. Generative pretraining in multimodality, arXiv preprint 2023\n\n[2] Esser, Patrick, et al. Taming transformers for high-resolution image synthesis, CVPR 2021\n\n[3] Aditya Ramesh, et al. Zero-shot text-to-image generation, ICML 2021\n\n[4] Wenhui Wang, et at. Image as a Foreign Language: BEIT Pretraining for Vision and Vision-Language Tasks. CVPR 2023\n\n[5] Peng, Zhiliang, et al. Beit v2: Masked image modeling with vector-quantized visual tokenizers, arXiv preprint 2022\n\n[6] Jing, et al. Generating images with multimodal language models, arXiv preprint 2023"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3261/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700203184016,
                "cdate": 1700203184016,
                "tmdate": 1700203184016,
                "mdate": 1700203184016,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wz8i2smdHl",
                "forum": "FlvtjAB0gl",
                "replyto": "YXCqAQfeHO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3261/Reviewer_L5Te"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3261/Reviewer_L5Te"
                ],
                "content": {
                    "title": {
                        "value": "Feedback"
                    },
                    "comment": {
                        "value": "Got it. Thanks the authors for answering my question.\n\nI read some baseline papers again, and, I find another question. Could the authors give some information about the dataset of different methods?"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3261/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700458315475,
                "cdate": 1700458315475,
                "tmdate": 1700458315475,
                "mdate": 1700458315475,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "r8DlgVJ0C5",
            "forum": "FlvtjAB0gl",
            "replyto": "FlvtjAB0gl",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3261/Reviewer_K5nK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3261/Reviewer_K5nK"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes an image tokenization method for visual-language model (VLM) pretraining, called dynamic discrete visual tokenization. The proposed method has two key characteristics:\n* Discrete: Continuous feature vectors from each image patch are mapped to a learnable codebook, resulting in discrete visual tokens compatible with text tokens. The discrete visual tokens make it possible to pre-train VLMs using the same autoregressive language modeling paradigm as text-only LMs.\n* Dynamic: The number of visual tokens is dynamically adjusted depending on the image content using a token selector and token merger. The token selector chooses the most crucial patches in the image, and the token merger reacquires the visual semantics of the unimportant patches, respectively.\n\nThe proposed method is pre-trained in two steps:\n1. Tokenizer learning: The tokenizer is pre-trained to reconstruct the original image from the quantized compressed visual tokens.\n2. Vision-language model learning: The VLM is pre-trained to predict the next visual/text token in a sequence, using the same autoregressive language modeling paradigm as text-only LMs.\nThe proposed method achieves high performance on multimodal understanding and generation tasks, with the relatively small size of model parameters and training data requirements. The paper also shows that quantization and token compression are effective techniques for improving the efficiency of VLM pre-training.\n\nOverall, this paper presents a novel and effective method for VLM pretraining. The proposed method looks simple yet effective and achieves state-of-the-art results on various multimodal tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is well-organized and easy to read, and the advantages of the proposed tokenizer are clearly described and demonstrated through various experiments and evaluations. In particular, the method for improving the inefficiency of the conventional vision-text fusion method is effective, and this study can be considered primary research for further improvement."
                },
                "weaknesses": {
                    "value": "There are a few suggestions for the ablation study.\n1. Compare the performance of text generation using merged continuous embeddings and quantized visual tokens. The loss of detailed information due to quantization is likely to affect the performance of text generation. However, it would be interesting to see how much of a difference there is.\n2. Show the performance difference with and without token merger. It would help to understand the importance of the token merger module."
                },
                "questions": {
                    "value": "1. What is the computational (speed) improvement during inference using a dynamic token length? It may be important information because training and inference efficiency may vary.\n\n2. There are a few things that could be improved in the manuscript. Please see the following suggestions.\n\n- Figure 1 (a): `Adater-Style` -> `Adapter-Style`\n- The first line in Section 3: `the Large language model` -> `the large language model`\n- The last line in `Token Selector` of Section 3.1 : `the binary decision M` -> `the binary decision mask M`\n- Equation 3: $||l_{2}(\\hat{x_i})-l_{2}(c_{j})||$ -> $||\\hat{x_i}-c_{j}||_2$\n- The 2nd line in Section 4.4: `the token predictor` -> `the token selector`"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3261/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3261/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3261/Reviewer_K5nK"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3261/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698765673845,
            "cdate": 1698765673845,
            "tmdate": 1699636274308,
            "mdate": 1699636274308,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4yw71PYC4p",
                "forum": "FlvtjAB0gl",
                "replyto": "r8DlgVJ0C5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3261/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3261/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer K5nK"
                    },
                    "comment": {
                        "value": "Dear reviewer, we sincerely appreciate for recognizing our work and providing valuable feedback. Below are our responses to the raised questions in your review.\n\n1. *Compare the performance of text generation using merged continuous embeddings and quantized visual tokens. The loss of detailed information due to quantization is likely to affect the performance of text generation. However, it would be interesting to see how much of a difference there is.*\n\n* Yes, the loss of detailed information due to quantization will affect the multi-modal understanding performance. The comparisons between using merged continuous embeddings and quantized visual tokens have been presented in Table 7 (a) of the appendix. For your convenience, we have listed the detailed results in the following Table:\n\n| Setting |   Flickr | VQAv2 | OKVQA |\n| :-----:| :----:|:----: |:----: |\n| Quantized Visual Tokens | 70.3 |  53.0  | 44.8 |    \n| Merged Continuous Embeddings | **72.7**   |  **56.4**   | **46.2** |  \n\n* As observed, the continuous visual features achieve better multi-modal understanding performance than quantized visual codes. This phenomenon is reasonable as quantized embeddings represent the centroid of continuous visual features, potentially leading to the loss of visual details. Therefore, when visual input serves as the condition (i.e., sequence like [image, text]), we use the merged continuous visual features as the input to LLM.\n\n2. *Show the performance difference with and without token merger. It would help to understand the importance of the token merger module.*\n\n* Thanks for your constructive suggestions. We have added the ablation study about the effect of the proposed token merger. From the presented results, it can be observed a distinct performance drop in multi-modal understanding tasks when eliminating the token merger module. Especially for visual question answering tasks that need fine-grained visual information as the reference, directly dropping the unselected visual tokens will lose some visual details. The proposed token merger can preserve the image details to some extent by compressing the unselected image patches onto the retained ones.\n\n| Setting  | Flickr | VQAv2 | OKVQA | GQA |\n| :-----:| :----:|:----: |:----: |:----: |\n| w/o Token Merger | 70.8 |  51.9  |  42.1 |  39.7  | \n| w/ Token Merger | **72.7**   |  **56.4**   | **46.2** | **42.3** | \n\n\n3. *What is the computational (speed) improvement during inference using a dynamic token length? It may be important information because training and inference efficiency may vary.*\n\n* We compared the text-to-image generation speed of the proposed dynamic tokenizer and static tokenizer (fixed 256 sequence length) for 100 text prompts on a single NVIDIA A100 GPU with batchsize=1. The average time costs and average numbers of generated visual tokens per image are reported in the following Table. The proposed dynamic tokenizer generates 89 visual tokens on average for each text prompt, about 35% of the static one. Since the attention computation in LLM exhibits a quadratic relationship with respect to the token length, the sparsification of the dynamic tokenizer can thus accelerate the per-batch image synthesis speed by 120%. \n\n| Setting  | Average Visual Token  | Time per Image (s) |\n| :-----:| :----:|:----: |\n| Static Tokenizer | 256 |  9.93  | \n| Dynamic Tokenizer | 89  |  4.52  | \n\n\n4. *There are a few things that could be improved in the manuscript. Please see the following suggestions*\n\n* Thanks for pointing out these typos in the draft. We have corrected them in the revision following your valuable suggestions.\n\nThank you again for the detailed reviews of our paper and providing insightful suggestions. Please let us know if there are further questions or concerns."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3261/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700202853417,
                "cdate": 1700202853417,
                "tmdate": 1700202853417,
                "mdate": 1700202853417,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "3XB3MN7KAB",
            "forum": "FlvtjAB0gl",
            "replyto": "FlvtjAB0gl",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3261/Reviewer_6o4w"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3261/Reviewer_6o4w"
            ],
            "content": {
                "summary": {
                    "value": "This work proposed to unify the pre-training objectives for visual and textual tokens while adapting the LLM for vision-language tasks. To this end, they first train a visual codebook to quantize the visual embeddings and assign each visual token a discrete label. To reduce the length of visual tokens, a token selector is proposed to select important visual tokens dynamically, and the token merger is used to merge dropped tokens with selected ones. The visual tokens and text tokens are then concatenated and fed to the LLM, which is trained for classification-based language modeling tasks. The authors conduct extensive experiments to evaluate the proposed methods, and the experimental results show that the proposed methods achieve state-of-the-art performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The experimental results demonstrate that the proposed method achieves a significant improvement compared to previous works.\n- The improvements from each proposed component is well supported by the ablation study results."
                },
                "weaknesses": {
                    "value": "- The authors train a visual codebook to assign discrete labels to visual tokens, which can be computationally costly. In the context of efficiently adapting LLM for vision-language tasks, it would be interesting to explore whether existing codebooks [1, 2, 3] can be utilized to provide useful supervision for training LLM.\n\n- Hard to be completely fair when compared against other baselines. For instance, compared to BLIP-v2, this work adds additional English text corpus and further fine-tunes the LLM. The significant performance improvement over previous work may be attributed to the additional training data and trainable parameters.\n\n- The implementation details of the ablation studies could be clearer. It is confusing why the proposed method achieves different results in Tables 3, 6, and 7 when doing ablation studies. \n\n- The authors claim that the learned codebook encodes high-level semantic concepts. However, the visualization results of codebooks in Figure 5 may suggest that the same code groups image patches based on low-level visual patterns, such as color and textures. It would be interesting to see additional image-level visualization results, similar to those in [4], to demonstrate that the codebook encodes high-level semantic concepts.\n\n- Minor suggestions/typos: I noticed several typos while reading the paper: \n\n  - Figure 1 (a) caption: Adater -> adapter\n  - The symbols X and when calculating K and V of Equation (2) should be X_d.\n\n---\n- [1] Chen, Yuxiao, et al. \"Revisiting multimodal representation in contrastive learning: from patch and token embeddings to finite discrete tokens.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\n- [2] Esser, Patrick, Robin Rombach, and Bjorn Ommer. \"Taming transformers for high-resolution image synthesis.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2021.\n- [3] Bao, Hangbo, et al. \"Beit: Bert pre-training of image transformers.\" arXiv preprint arXiv:2106.08254 (2021)\n- [4] Peng, Zhiliang, et al. \"Beit v2: Masked image modeling with vector-quantized visual tokenizers.\" arXiv preprint arXiv:2208.06366 (2022)."
                },
                "questions": {
                    "value": "Refer to the weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3261/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3261/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3261/Reviewer_6o4w"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3261/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699231307497,
            "cdate": 1699231307497,
            "tmdate": 1699636274235,
            "mdate": 1699636274235,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "F32Jktan0s",
                "forum": "FlvtjAB0gl",
                "replyto": "3XB3MN7KAB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3261/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3261/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 6o4w - Part1"
                    },
                    "comment": {
                        "value": "We sincerely appreciate the time and effort the reviewer dedicated to reviewing our paper and providing constructive comments. Below are our responses to the raised concerns.\n\n1. *The authors train a visual codebook to assign discrete labels to visual tokens ... it would be interesting to explore whether existing codebooks can be utilized to provide useful supervision for training LLM.*\n\n* We fully agree with the reviewer that exploring the influence of existing codebooks is meaningful. Actually, in our earlier experiments, we have tested the impact of different codebooks (e.g., VQ-VAE [1,2] and BEIT-2 [3]), but found that they were not effective in serving as a good visual tokenizer for multimodal LLM. The detailed comparisons between these codebooks and our proposed dynamic one are shown in the following Table. All these ablations are conducted on the same 30M pre-training data with clip ViTL/14 as the visual encoder.\n\n| Method | Codebook Size |  Flickr | GQA | VQAv2 | OKVQA |\n| :-----:| :----: |  :----: | :----:|:----: |:----: |\n| VQ-VAE | 8192 | 60.5 |  35.0  | 48.8 | 40.2 |\n| BEIT-2 | 8192 | 65.8  | 38.3 | 51.1 | 41.4 |\n| Ours | 8192 | **70.0** |  **40.8**  | **53.0** | **44.5** |\n\n* As observed, directly applying these existing codebook techniques to train LaVIT leads to sub-optimal downstream performance. We speculate that this may be due to the fact that the current codebooks were not explicitly tailored to accommodate text-oriented language models. For example, VQ-VAE's codebook is learned by reconstructing image pixels, which may cause the learned code to focus on low-level visual patterns and be incompatible with high-level word tokens. Although BEIT-2's codebook is derived by distilling high-level features from the CLIP teacher model, their visual codes do not have the 1D causal dependency and thus are not well compatible with LLM. Moreover, most of these existing codebooks tokenize an image into a long, fixed sequence (e.g., 1024 for DALLE [2]). Such a long sequence will invariably result in low efficiency, especially for generating images. Therefore, we introduce a well-designed dynamic visual tokenizer for better compatibility with LLM in the unified training.\n\n2. *Hard to be completely fair when compared against other baselines ... The significant performance improvement over previous work may be attributed to the additional training data and trainable parameters.*\n\n* In terms of data, for a fair comparison, we employ the same multi-modal pre-training data (about 93M samples) as BLIP-2 [4] to exclude performance gains from additional data usage This data scale is smaller than that of most other methods. For example, the concurrent work Emu [5] uses 2.6B data, and KOSMOS [6] adopts about 3.1B image-caption pairs. Besides, the purpose of including English text corpus during pre-training is to preserve the already learned language understanding ability of LLM (e.g., the performance on linguistic benchmarks like MMLU) when acquiring good multimodal capabilities. The leveraged text corpus is Open-Source data (Redpajama [7]) for training the LLaMA model from scratch, which therefore, does not introduce new textual data and is just for alleviating the forgetting of previously learned knowledge.\n\n* While LaVIT fine-tunes the LLM compared to BLIP-2, our objective is to enable the visual modality to integrate better with LLM like a foreign language. In this way, text-oriented LLM can not only understand visual modality input but also generate visual content like language. In contrast, BLIP-2 can only comprehend the visual inputs but lacks the capability to generate them. Moreover, tuning the LLM instead of the adapter with limited trainable parameters can leverage the remarkable reasoning capabilities of LLM to learn the interaction across different modalities. \n\n3. *The implementation details of the ablation studies could be clearer. It is confusing why the proposed method achieves different results in Tables 3, 6, and 7 when doing ablation studies.*\n\n* All the ablation studies were conducted on part of pre-training data (about 30M image-text pairs) by using the ViTL/14 of CLIP as the visual encoder. The language model is scheduled to train for 10K steps in these experiments. All other hyper-parameters are set to the same values as those listed in Table 9 of the appendix. Due to the costly training resources, these ablation studies did not progress to the final 10K steps and were stopped when the model performance did not change much. But it's important to note that we make sure that the same step was taken in each set of experiments for a fair comparison. For example, the experiments in Table 3 (a) are stopped at 9K steps, Table 6 for 9.5K steps and Table 7 (a) for 9.2K steps. As a result, the reviewer will find slight performance differences (within 1% change) in Tables 3, 6, and 7."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3261/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700202588831,
                "cdate": 1700202588831,
                "tmdate": 1700202588831,
                "mdate": 1700202588831,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "x3m1GDgX6j",
                "forum": "FlvtjAB0gl",
                "replyto": "3XB3MN7KAB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3261/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3261/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 6o4w - Part2"
                    },
                    "comment": {
                        "value": "4. *The authors claim that the learned codebook encodes high-level semantic concepts. However, the visualization results ... to demonstrate that the codebook encodes high-level semantic concepts.*\n\n * Thanks for your constructive suggestions. We have updated the visualizations for the learned codebook in the draft and added more examples of image patches belonging to the same visual code. As observed in Figure 8 of the appendix, the learned discrete visual codes can convey explicit high-level visual semantics. For example, code 13014 represents the human arm, code 7260 shows the part of the car, code 15235 represents the bus, and code 15090 indicates the donuts. These visualizations demonstrate the interpretability of the learned codebook.\n\n5. *Minor suggestions/typos: I noticed several typos while reading the paper*\n\n * Thanks for pointing out these typos and we have corrected them in the draft.\n\nWe hope these responses have settled your concerns. Thank you again for your detailed and valuable feedback. Please let us know if there are further questions or concerns.\n\n---\n\n[1] Esser, Patrick, et al. Taming transformers for high-resolution image synthesis, CVPR 2021\n\n[2] Aditya Ramesh, et al. Zero-shot text-to-image generation, ICML 2021\n\n[3] Peng, Zhiliang, et al. Beit v2: Masked image modeling with vector-quantized visual tokenizers, arXiv preprint 2022\n\n[4] Junnan, et al. Blip-2: Bootstrapping language image pre-training with frozen image encoders and large language models, ICML 2023\n\n[5] Quan Sun, et al. Generative pretraining in multimodality, arXiv preprint 2023\n\n[6] Shaohan, et al. Language is not all you need: Aligning perception with language models, arXiv preprint 2023\n\n[7] Redpajama: An open source recipe to reproduce llama training dataset, https://github.com/togethercomputer/RedPajama-Data"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3261/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700202708119,
                "cdate": 1700202708119,
                "tmdate": 1700202708119,
                "mdate": 1700202708119,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]