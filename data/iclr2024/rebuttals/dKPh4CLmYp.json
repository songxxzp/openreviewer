[
    {
        "title": "Fishnets: Information-Optimal, Scalable Aggregation for Sets and Graphs"
    },
    {
        "review": {
            "id": "C7WeN5j9Zq",
            "forum": "dKPh4CLmYp",
            "replyto": "dKPh4CLmYp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5575/Reviewer_dYDc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5575/Reviewer_dYDc"
            ],
            "content": {
                "summary": {
                    "value": "The paper delves into set-based learning, emphasizing the importance of Graph Neural Networks (GNNs) and DeepSets for handling complex datasets. The authors introduce \"Fishnets\", an aggregation strategy tailored for Bayesian inference and graph aggregation. This strategy showcases adaptability, resilience, and superiority in capturing Bayesian information. Moreover, when integrated with GNNs, Fishnets enhances performance and efficiency. This research positions Fishnets as a potential game-changer in deep learning and network science."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "A large number of experiments are carried out to prove the effectiveness of the method."
                },
                "weaknesses": {
                    "value": "1. The logic of the introduction part is illogical and too short. The reader does not understand what problem this article is going to solve and what the shortcomings of existing methods are. The authors do not emphasize the main contribution of this article and what are the differences from existing methods.\n2. \"Fisher\" is mentioned frequently in the paper. The author should add a \"preliminary\" section to introduce what Fisher are to make it easier for readers with different backgrounds.\n3. There exist a large number of grammatical errors and spelling errors in the article, such as:\n\n- In \"GNNs can acheive state-of-the-art\", \"acheive\" should be \"achieve\".\n- In \"On one hand, frequentist analyses\", it's better to use \"On the one hand\" for clarity.\n- In \"In a deep learning context graph neural networks (GNNs) rely\", a comma is needed after \"context\".\n- In \"for predictive or regression tasks\", consider using \"for either predictive or regression tasks\" for clarity.\n- In \"Up until now, graph aggregation schemes\", the comma after \"now\" is unnecessary.\n- In \"This paper is organised as follows:\", \"organised\" might be considered a British spelling. You might want to use \"organized\", the American spelling.\n- In \"In this paper we built up\", a comma is needed after \"paper\"."
                },
                "questions": {
                    "value": "please refer to the Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5575/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698681455545,
            "cdate": 1698681455545,
            "tmdate": 1699636574215,
            "mdate": 1699636574215,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "vPcMYv13Ax",
                "forum": "dKPh4CLmYp",
                "replyto": "C7WeN5j9Zq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5575/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5575/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your thorough response and questions ! \n\n\n**Updates to the paper:** We have revised the introduction in Section 2 to clarify the rigorous theoretical foundation, based on the likelihood principle, upon which we base our claim of information optimality. We include a detailed treatment of the information inequality and the Cramer-Rao bound (with new proof in the appendix), and then proceed to set-based likelihoods and Fishnets embedding. We additionally provide a new, more in-depth empirical analysis of Fishnets aggregation on three OGB graph datasets (for graph- and node-level prediction). Replacing their aggregators with Fishnets we are able to reproduce or exceed the SOTA benchmarks of Li et al\u2019s DeeperGCN paper but with far fewer GNN layers and learnable parameters. Finally, we improved the organisation and flow of the paper from theory to Bayesian information saturation to GNN prediction.\n\n**Response to comments:** We have revised our introduction section and explicitly defined information optimality and the Fisher information matrix. We also include an Appendix in which we prove information saturation with knowledge of the score function.\n\nGrammatical and spelling errors have been addressed."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5575/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700672526928,
                "cdate": 1700672526928,
                "tmdate": 1700672792222,
                "mdate": 1700672792222,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "eMC4WSSI6p",
            "forum": "dKPh4CLmYp",
            "replyto": "dKPh4CLmYp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5575/Reviewer_XpEu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5575/Reviewer_XpEu"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a method of aggregation based on Fisher information. The authors show significant problems in existing aggregation methods such as mean or softmax, and conduct several experiments to show that the proposed method does not suffer like existing methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper shows significant problems with using existing aggregation methods.\n\n- The experiments seem to show that the proposed method does not suffer from the same issues as using other aggregation methods"
                },
                "weaknesses": {
                    "value": "- The experiments are not very convincing. While I do appreciate the simple examples, I think that additional comparisons on real world benchmarks should be conducted to truly show the effectiveness of the proposed aggregation scheme, also given that the authors say the proposed mechanism can be implemented as a 'drop in' aggregation function. Therefore it should be shown on multiple datasets and multiple GNN backbones. \n\n- Also, in the experimental results on the ogb protein dataset, the authors should compare with more methods. The results of other methods are significantly better than just using GCN.\n\n- It is not clear what is the computational cost of the method. It is not discussed and not reported in runtimes.\n\n- The authors say both in title and in abstract that the method is optimal, but it is not proven anywhere, and again the experimental results are lacking."
                },
                "questions": {
                    "value": "- The method itself seems to be more general than just to be applied to GNNs. Did you try to use it for other types of neural networks? what are the limitations of the method?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5575/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698784892159,
            "cdate": 1698784892159,
            "tmdate": 1699636574104,
            "mdate": 1699636574104,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "M5nbOwNfbQ",
                "forum": "dKPh4CLmYp",
                "replyto": "eMC4WSSI6p",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5575/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5575/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your thorough response and questions ! \n\n**Updates to the paper:** We have revised the introduction in Section 2 to clarify the rigorous theoretical foundation, based on the likelihood principle, upon which we base our claim of information optimality. We include a detailed treatment of the information inequality and the Cramer-Rao bound (with new proof in the appendix), and then proceed to set-based likelihoods and Fishnets embedding. We additionally provide a new, more in-depth empirical analysis of Fishnets aggregation on three OGB graph datasets (for graph- and node-level prediction). Replacing their aggregators with Fishnets we are able to reproduce or exceed the SOTA benchmarks of Li et al\u2019s DeeperGCN paper but with far fewer GNN layers and learnable parameters. Finally, we improved the organisation and flow of the paper from theory to Bayesian information saturation to GNN prediction.\n\n**Response to comments:** We have incorporated more real-world benchmark datasets, which show improvement in performance with fewer learnable parameters (see new Section 5.1).\n\nWe wanted to keep the study limited to highlight the advantage of Fishnets aggregation compared to Li et al\u2019s learned softmax aggregation. We do not compare to other architectures since we wish to highlight the improvement of the aggregation scheme with other hyperparameters held fixed.\n\nWe report improvement in training epoch reduction to optimality, as well as smaller networks (fewer learned parameters to perform gradient descent on).\n\nOptimality is now discussed in depth in Section 2 and Appendix A."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5575/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700672479040,
                "cdate": 1700672479040,
                "tmdate": 1700672804712,
                "mdate": 1700672804712,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "uJKsDwAkyk",
            "forum": "dKPh4CLmYp",
            "replyto": "dKPh4CLmYp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5575/Reviewer_ZQ1B"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5575/Reviewer_ZQ1B"
            ],
            "content": {
                "summary": {
                    "value": "Paper proposes a way to aggregate a set of variable-sized feature vectors into a fixed vector. This is essential for set-based inputs (where the number of items in the set is variable) and graph neural networks (where number of neighbors changes per node). Paper models  data distributions, $p(d | \\theta)$, using the Fisher information matrix $F = \\sum_{d \\in dataset} \\nabla_\\theta p(d|\\theta) \\cdot \\nabla_\\theta p(d|\\theta)^T$ where the derivatives (**I think**) are calcuated at a given value of $\\theta$. Then, the value of $\\theta$ is iteratively updated (**I thnk**) by repeatedly updating with  $F^{-1} t$, where $t = \\nabla_\\theta \\sum_{d \\in dataset}   p(d|\\theta) $. Paper claims that this yields an information-optimal representation of data.\n\n\n# Update\n\nI read the author's response and updated manuscript. I will keep my score as-is\n\n## Response\n\nThank you for your hard work and quick turn-around.\n\nUnfortunately, the paper still seems incomplete in my opinion. I will unlikely change my rating at this point. The following are points you may consider for future versions (e.g., resubmissions)\n\n* Add some prelim section. \n* Equation 6: Please be clear on **iterate**. I expect to see something like parameter at $t$ versus $t - 1$,  etc.\n* Above Eq.12: it is not clear what is meant by \"draw data-parameter pair\" -- does this mean, randomly draw subset of data, and on it, train a neural network (to convergence?)\n* Please avoid using \"$\\cdot$\" on LHS of Equations 14 & 15."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* **Direction**: The paper addresses an important general direction: Combining variable-length (orderless) information is applicable to many tasks, including graphs (social networks, biological networks, user-product interactions, etc), sets, or ordered sequences (text or video) if positional sinusoidal embeddings are added. Their *subdirection*, i.e., learning a neural net that output summary statistics for distributions, is also important and impactful (e.g., Neural Statistician, https://arxiv.org/abs/1606.02185) with many possible combinations: Given a set, whats the probability of an item given others in the set.\n\n* **Practical**  (1)The method computes per-example a gradient term $J_i$ (and its outer-product $J_i J_i^T$), which can be computed in parallel. Then, the sum across all examples. (2) It can be used as drop-in replacement for GNN training. (3) Method trains fast and doesnt use many parameters\n\n* **Pointing out primary weakness of related ( / previous) work.**: Computing $F^-1$ (where F is fisher matrix) gives \"asymptotic optimality\"."
                },
                "weaknesses": {
                    "value": "* The primary weakness is that the paper does not stand on its own. While nicely written and makes important contributions (e.g., practical &  theoretical), the missing (preliminary) information makes this paper difficult to understand and reproduce, unless the reader goes in-depth into related work. I will detail this and ask the authors some clarification questions.\n\n* \"**optimality**\" is mentioned a few times. However, it is not clear from the text: \"**optimal, from what sense?**\". In my opinion, there needs to be a *clearly-stated (i) objective function or (ii) inequality, and showing that the method either recovers the (i) (unique) optimal or (ii) satisfies the inequality with equality. Currently, this discussion is skipped.\n\n* Experiments. The only real dataset is ogbn-protein. If you already integrated with PyG and have ogbn training setup, wouldn't it be straight forward to rerun your code while changing only the dataset name?"
                },
                "questions": {
                    "value": "# Please be more exact\n\n* Define $\\theta_{fid}$.\n\n* What is the iteration in \"(iteratively)\" mentioned before Equation 3?\n\nI think that all of the above should be incorporated in the paper\n\nIn Equation (9), please list-down the parameters which the objective function is optimized w.r.t., e.g.\n\n$$\\mathcal{L}(w_t, w_F)$$\n\nThe reason I am asking for this, because it is not immediately obvious if $\\theta$ is a constant, though it is clear that $\\hat{\\theta}_{NN}$ is parameterized by $(w_t, w_F)$. Further, it is not clear if $\\theta_{fid}$ is always fixed -- if not, please list in LHS of Eq.9.\n\n# Questions \n\n\n1. What is $\\theta_{fid}$? what is the what is the **iterate** in ``(iteratively)''  mentioned before Equation 3? Do the authors imply that $\\theta_{fid}$ is the previous iteration value and $\\hat{\\theta}^{MLE}$ is the next iteration?\n\n2. What is $\\theta$ in Eq.9? \n\n3. In equation (9), $\\theta_{NN}$ is defined in terms of $F_{NN}$ and $t_{NN}$ (see Eq.8).\n\n4. What is the space / dimensionality of $d$ (the input examples). You can be general. Specifically, do Equations 11 and 12 have a range equal to space of $d$, or do they output scalars? -- this question is tied to next.\n\n5. Eq.11 & Eq.12: Can you clarify the input argument? Is $\\beta$ a vector or scalar?\n\nIf the rebuttal answers gives me a homework to \"read the referenced papers well\", then perhaps it is very much worth making a \"Preliminaries\" section to define missing expressions."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5575/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5575/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5575/Reviewer_ZQ1B"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5575/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698789373233,
            "cdate": 1698789373233,
            "tmdate": 1700934859627,
            "mdate": 1700934859627,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GPdYryrfge",
                "forum": "dKPh4CLmYp",
                "replyto": "uJKsDwAkyk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5575/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5575/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your thorough response and questions ! \n\n**Updates to the paper**: We have revised the introduction in Section 2 to clarify the rigorous theoretical foundation, based on the likelihood principle, upon which we base our claim of information optimality. We include a detailed treatment of the information inequality and the Cramer-Rao bound (with new proof in the appendix), and then proceed to set-based likelihoods and Fishnets embedding. We additionally provide a new, more in-depth empirical analysis of Fishnets aggregation on three OGB graph datasets (for graph- and node-level prediction).  Replacing their aggregators with Fishnets we are able to reproduce or exceed the SOTA benchmarks of Li et al\u2019s DeeperGCN paper but with far fewer GNN layers and learnable parameters. Finally, we improved the organisation and flow of the paper from theory to Bayesian information saturation to GNN prediction.\n\n\n**Response to Comments:** In our new introduction section (Section 2) we clarify our notion of information optimality and show how our neural embeddings (the score $t(d_i)$) and weights ($F(d_i)$) are learned as a function of the data and underlying implicit likelihood. Please refer to this section for clarification on the method (no gradients at a value of $\\theta$ are needed). \n\nWeaknesses:\nW1: the new introduction section and accompanying appendix should clarify the practical and theoretical components that were missing from the original submission.\n\nW2: Optimality is now described in Section 2 (proof in Appendix A)\n\nW3: We have now produced more OGB experiments, with similar levels of improvements once Fishnets is adopted. \n\nQ1: $\\theta_{\\rm fid}$ has been renamed $\\theta_*$ and defined in the update\n\nQ2: Please see Section 2 and Appendix A\n\nQ3: this has been clarified in Section 2\n\nQ4: $\\textbf{d}_i \\in \\mathbb{R}^N$; $\\textbf{t}_i \\in \\mathbb{R}^{n_p}$\n\nQ5: Eqs 11&12: the input to the softmax aggregation is either a data vector $\\textbf{d}_i \\in \\mathbb{R}^N$,\n\nor some neural network embedding $ q_i \\in \\mathbb{R}^{n_{\\rm h}} $. This has been clarified in both cases."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5575/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700672429191,
                "cdate": 1700672429191,
                "tmdate": 1700672771183,
                "mdate": 1700672771183,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "EszZmWUi6R",
            "forum": "dKPh4CLmYp",
            "replyto": "dKPh4CLmYp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5575/Reviewer_KY3Y"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5575/Reviewer_KY3Y"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes Fishnets, an aggregation strategy for learning information-optimal embeddings for sets of\ndata for both Bayesian inference and graph aggregation.\nThe methods contribute to the general set aggregation problems."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Good methodological motivations regarding optimal aggregation of independent heterogeneous data"
                },
                "weaknesses": {
                    "value": "- The paper is poorly structured. The Introduction is too short to introduce and motivate the problem. The method and related work section is too limited. The experimental section appears on page 3. Overall, the paper does not appear to be properly written.\n- The Bayesian Information saturation experiments are detached from graph neural network aggregation experiments. Why they should be separately discussed? And why the GNN aggregation experiments cannot share the same analysis from the Bayesian Information saturation experiments?\n- Only experimenting on OGBN-Proteins seems to be too limited. Experiments on more graph datasets are needed.\n- Only using deep GNNs is a weird setting. Why not experiment with shallow GNNs? The paper should at least include results regarding shallow GNNs for comparison."
                },
                "questions": {
                    "value": "Why the GNN aggregation experiments cannot share the analysis from the Bayesian Information saturation experiments?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5575/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698821393638,
            "cdate": 1698821393638,
            "tmdate": 1699636573834,
            "mdate": 1699636573834,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kjcMcfW2Ss",
                "forum": "dKPh4CLmYp",
                "replyto": "EszZmWUi6R",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5575/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5575/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response and questions !\n\n**Updates to Paper:** We have revised the introduction in Section 2 to clarify the rigorous theoretical foundation, based on the likelihood principle, upon which we base our claim of information optimality. We include a detailed treatment of the information inequality and the Cramer-Rao bound (with new proof in the appendix), and then proceed to set-based likelihoods and Fishnets embedding. We additionally provide a new, more in-depth empirical analysis of Fishnets aggregation on three OGB graph datasets (for graph- and node-level prediction).  Replacing their aggregators with Fishnets we are able to reproduce or exceed the SOTA benchmarks of Li et al\u2019s DeeperGCN paper but with far fewer GNN layers and learnable parameters.\nFinally, we improved the organisation and flow of the paper from theory to Bayesian information saturation to GNN prediction.\n\n**Response to Comments:** The Bayesian Information saturation section is detached from the graph neural network aggregation experiments because in the latter we wished to compare to a ground truth inference, e.g. the HMC sampler, which is not possible for graph aggregation tasks because we do not have access to sampling distributions for the ogbn-proteins dataset features. The noisy edge protein analysis demonstrates how GNN point estimators can be made robust to noise.\n\nWe have incorporated more OGB dataset comparisons (Section 5.1). We show that we obtain same- or better results with shallow Fishnets GCNs in these benchmark cases."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5575/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700672083168,
                "cdate": 1700672083168,
                "tmdate": 1700672842120,
                "mdate": 1700672842120,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "VTqfyclfJv",
            "forum": "dKPh4CLmYp",
            "replyto": "dKPh4CLmYp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5575/Reviewer_BUmE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5575/Reviewer_BUmE"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors propose Fishnets, a strategy that can better learn embeddings for sets of data, used in DeepSets environments or in graphs as well. The method is statistically-backed with solid motivation and organization behind the writing, and achieves solid performance boosts when it comes to the empirical tests."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "The strengths of this paper lie in its rigor, novelty, and strong performance. The Fishnets strategy is well-motivated in the second section, with both intuition and theory combined. In addition, because of this it seems to be relatively novel compared to similar works, which also helps Fishnets achieve great performance empirically, no matter the dataset/task."
                },
                "weaknesses": {
                    "value": "The main weaknesses of the paper lie in not that many empirical comparisons, as well as a slight decline in organization as the paper goes on. For example, there could be better organization between sections 4 and 5; perhaps differentiating a little bit more or talking about the experiments in the introduction. The sections do not seem to have a reasonable ordering to them, but rather are jumbled with one another, which also makes experiments much harder to conduct, given that there are so many applications and so many claims being made. For example, one key paradigm, being the graph learning, is only given one dataset to work with. For such an important aspect of the work, it would be expected to include performance on multiple graph-like datasets."
                },
                "questions": {
                    "value": "1. The authors mention faster training time in multiple occurrences, but to the best of my knowledge data on computation time is not included in the paper. Would it be possible to shed a little bit of light on how much the improvement is on that front?\n2. Why was the proteins dataset chosen in the first place over any other potential graph benchmark?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5575/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5575/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5575/Reviewer_BUmE"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5575/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698840278413,
            "cdate": 1698840278413,
            "tmdate": 1699636573753,
            "mdate": 1699636573753,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0WAep3sFY2",
                "forum": "dKPh4CLmYp",
                "replyto": "VTqfyclfJv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5575/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5575/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response and questions ! \n\n**Updates to paper:** We have revised the introduction in Section 2 to clarify the rigorous theoretical foundation, based on the likelihood principle, upon which we base our claim of information optimality. We include a detailed treatment of the information inequality and the Cramer-Rao bound (with new proof in the appendix), and then proceed to set-based likelihoods and Fishnets embedding. We additionally provide a new, more in-depth empirical analysis of Fishnets aggregation on three OGB graph datasets (for graph- and node-level prediction).  Replacing their aggregators with Fishnets we are able to reproduce or exceed the SOTA benchmarks of Li et al\u2019s DeeperGCN paper but with far fewer GNN layers and learnable parameters. Finally, we improved the organisation and flow of the paper from theory to Bayesian information saturation to GNN prediction.\n\n**Response to Comments:**\n\nQ1: Faster training time here refers to fewer learning (gradient descent) epochs needed to achieve convergence of the GNN model. We indicate this improvement by showing test ROC-AUC curves in the ogbn-proteins focus section. We additionally indicate improved benchmark comparisons (see Section 5.1) with drastically fewer learnable parameters than Li et al\u2019s best GCN models.\n\nQ2: We initially chose to work with the ogbn-proteins dataset because the prediction task consisted of aggregating weighted edges (association scores) between proteins (nodes) in the graph. These association scores could be modelled with noise akin to the linear regression and BHM models in previous sections to demonstrate the robustness of the Fishnets aggregation to changing noise because the Fisher weighting scheme explicitly re-weights embeddings in a heterogeneous manner as a function of data (and in the linear regression and noisy proteins case, the $\\sigma$ and $N$ noise amplitudes)."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5575/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700672019128,
                "cdate": 1700672019128,
                "tmdate": 1700672627303,
                "mdate": 1700672627303,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "46G4kQSiiW",
                "forum": "dKPh4CLmYp",
                "replyto": "0WAep3sFY2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5575/Reviewer_BUmE"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5575/Reviewer_BUmE"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your answers and edits.\n\nI don't believe there to be any significant/major changes to the paper to warrant an increase/decrease in score, so I will retain my score for now."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5575/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700701413933,
                "cdate": 1700701413933,
                "tmdate": 1700701413933,
                "mdate": 1700701413933,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "v0FTWuKNK3",
            "forum": "dKPh4CLmYp",
            "replyto": "dKPh4CLmYp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5575/Reviewer_wjmC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5575/Reviewer_wjmC"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes an aggregation strategy for sets of data that aims to learn \u201cinformation-optimal\u201d embeddings. The approach resides around training neural networks to learn the score and Fisher information matrix for individual data points. This permits the construction of optimal embeddings by aggregating the scores and Fisher matrices and applying the Fisher scoring method iteratively. The paper reports the results of multiple experiments that demonstrate the potential applications of the methodology and and highlight its strengths - robustness to distribution shift, ability to saturate Bayesian information content, capability to take into account uncertainty."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "S1: The paper proposes a principled approach for aggregation of set data to construct improved embeddings.\n\nS2. Several experiments demonstrate the potential of the proposed methodology including (i) its ability to saturate Bayesian information content; (ii) its robustness to distribution shift; and (iii) its usage as an aggregation technique in GNNs."
                },
                "weaknesses": {
                    "value": "W1: The presentation of the method could be improved. In particular, the key concepts of \u201cinformation optimal\u201d and \u201cinformation saturation\u201d are not clearly defined. Section 2 first explains that the Fisher scoring method can be used to \u201c(iteratively) form a maximum likelihood estimator\u201d and then transitions to a \u201cpseudo-MLE\u201d. Alsing & Wandelt (2018) provide a considerably more complete and clearer discussion. This paper should aim to be more precise. For example, it\u2019s not clear what \u201c(iteratively)\u201d means here \u2013 Alsing & Wandelt spell it out that it converges in the limit and explain that after k iterations it provides a (clearly defined) quasi maximum-likelihood estimator. The paper doesn\u2019t explain the transition to a \u201cpseudo-MLE\u201d later in the section. It doesn\u2019t explain the concept of \u201csaturating\u201d the Fisher information; it doesn\u2019t provide the Cramer-Rao bound. One could argue that some of this might be considered necessary background knowledge, but I would suggest that the paper should be accessible to practitioners who are interested in learning about a new aggregation method and perhaps do not have an information theoretic background.\n\nW2: Although the experiments provide illustrations of how the approach can prove beneficial, none of them constitutes a thorough experimental study that demonstrates that the methodology is superior for a given application. In each case, the proposed approach is compared to one baseline, for one dataset or model. This is sufficient to illustrate the potential of the method, but it does not provide compelling evidence that the method can be a state-of-the-art choice for a particular problem. \n\nAn example is the GNN aggregation experiment. Rather than providing a systematic and complete study over multiple datasets and comparing against multiple baselines to demonstrate how the proposed method acts as the best aggregator (in some sense) the provided experiments are for a single dataset, with essentially two variants of single baseline as comparison points. The selected baseline does not achieve close to the state-of-the-art performance \u2013 as asked below, it\u2019s not even clear that the selected baseline is configured to achieve its best performance. So these experiments allow us to conclude that for one dataset the proposed method outperforms one questionably-configured baseline that is itself well behind the state-of-the-art (although one could make an argument that the leaders above it are not \u201cpure\u201d GNNs and are achieving outperformance through processing steps that are not related to graph learning or aggregation). The experiments definitely do not establish that the proposed aggregator can be reliably combined with multiple different GNN architectures and employed confidently for a new dataset. \n\nIt's not essential that the paper provide such evidence for every application of the proposed method. Indeed, it is good to see the versatility via illustrations of how it could be employed in a range of settings. However, the paper would be much stronger if it provided compelling evidence for at least one of the suggested applications. The paper does not make a strong theoretical contribution, although the proposed technique is well-founded theoretically. As such, the bar for experimentation is higher; one expects convincing empirical evidence to support the claims.\nW3. Related work: The Deep Sets paper was published in 2018 and has been cited over 2000 times. While many of these papers involve applications of the method, many others propose extensions and examine the aggregation process. As just one example, [R1] examines learnable recurrent aggregation functions. A related work section that discusses two papers and then cites three other examples with a single sentence (all dating from 2020 or earlier) is inadequate. This section does not cite any papers published in 2021-2023, making it highly questionable whether the paper is correctly positioning the proposed method in the context of existing research on the topic.  \n\n[R1] Soelch, Maximilian, et al. \"On deep set learning and the choice of aggregations.\" Artificial Neural Networks and Machine Learning\u2013ICANN 2019."
                },
                "questions": {
                    "value": "Q1. Why does the AUC-ROC performance for the baseline differ so much from the value reported in Li et al. 2020 (where the best performing model is 0.855 for a 28 layer network)? Do the experiments avoid residual connections for some reason? (The description \u201cout-of-the-box 28-layer GCN\u201d is unclear to me). Li et al. explicitly state \u201cresidual connections significantly improve the performance of deep GCN models. PlainGCN without skip connections does not gain any improvement from increasing depth.\u201d \n\nQ2. \u201cExplicitly learning the $F^{\u22121}$ weights in addition to the score allows us to achieve 1) asymptotic optimality\u201d \u2013 can the paper concretely define the nature of the optimality? is there a simple proof? Or is it considered obvious? Presumably there a reliance on a universal function approximation result for the neural networks learning the score and the Fisher? Does this combine obviously with the asymptotics of the Fisher method?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 6,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5575/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698855789624,
            "cdate": 1698855789624,
            "tmdate": 1699636573643,
            "mdate": 1699636573643,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2cIlPO6zKJ",
                "forum": "dKPh4CLmYp",
                "replyto": "v0FTWuKNK3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5575/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5575/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your detailed review ! \n\n\n**Updates to Paper:**\nWe have revised the introduction in Section 2 to clarify the rigorous theoretical foundation, based on the likelihood principle, upon which we base our claim of information optimality. We include a detailed treatment of the information inequality and the Cramer-Rao bound (with new proof in the appendix), and then proceed to set-based likelihoods and Fishnets embedding.\nWe additionally provide a new, more in-depth empirical analysis of Fishnets aggregation on three OGB graph datasets (for graph- and node-level prediction).  Replacing their aggregators with Fishnets we are able to reproduce or exceed the SOTA benchmarks of Li et al\u2019s DeeperGCN paper but with far fewer GNN layers and learnable parameters. Finally, we improved the organization and flow of the paper from theory to Bayesian information saturation to GNN prediction.\n\n**Response to Comments:**\n\nAnswers to Questions:\n\nQ1: We have updated the codebase to compare Li et al\u2019s best-performing models to our aggregation technique. We first do a drop-in replacement study on three OGB datasets with Li et al\u2019s DeeperGCN code and achieve similar results with their baseline models before proceeding with our Fishnets modifications.  We revised our initial ogbn-proteins study into a focus study. We used a stripped-down version of Li et al\u2019s codebase initially in order to modify the raw data during training in the noisy edge case. We provide our benchmark models within this training and preprocessing scheme to provide a benchmark against which to compare the noisy setting. \n\nQ2: This optimality guarantee (under the likelihood principle) has been explicitly provided in Section 2 with proof in Appendix A."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5575/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700671926460,
                "cdate": 1700671926460,
                "tmdate": 1700672747062,
                "mdate": 1700672747062,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "G89nsbjY6B",
                "forum": "dKPh4CLmYp",
                "replyto": "2cIlPO6zKJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5575/Reviewer_wjmC"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5575/Reviewer_wjmC"
                ],
                "content": {
                    "title": {
                        "value": "Acknowledgement of the response"
                    },
                    "comment": {
                        "value": "Thank you for the response to my review and the changes that have been made to the paper. While the paper has improved substantially, I think there is still the need for improved clarity. Overall, this seems like a very clever and promising idea, but the paper needs a careful and thorough revision before it is ready for publication.\n\nThe experimentation has been extended to include more datasets, which is good, but there is still the limitation to a single baseline for the GNN aggregation, and the experimental details are hard to follow. \n\nMy concern regarding the positioning with regard to more recent related work was not addressed - the paper still includes very limited discussion of other aggregation strategies, and does not discuss any related work published since 2020. \n\nThe meaning and claim of asymptotic optimality would be clearer with a formal theorem/lemma/corollary and an associated proof. Currently, it seems that the discussion in Appendix A is connected to Section 2.1. Both of these sections develop the material using the true Fisher and the true score. But the claim of optimality is being made for the technique in Section 2.3. In that section, the Fisher and the score are learned (estimates). There doesn't seem to be a sufficiently concrete linkage between the optimality (based on true Fisher and score) in Section 2.1 & Appendix A, and the claimed optimality for learned/estimated Fisher and score in Section 2.3. Phrases like \"Provided the embeddings ... are learned sufficiently well\" are a concern, because there is no specification of what \"sufficiently well\" means. This is where a formal statement of the result would be helpful, because it would be made clear exactly what is being assumed."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5575/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700708562982,
                "cdate": 1700708562982,
                "tmdate": 1700708562982,
                "mdate": 1700708562982,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ZwqppVBiWi",
            "forum": "dKPh4CLmYp",
            "replyto": "dKPh4CLmYp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5575/Reviewer_D4AK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5575/Reviewer_D4AK"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a scalable aggregator Fishnets for sets and graphs from the perspective of MLE. Fishnets can approximate score and Fisher matrix from data with arbitrary size with neural networks. It has comparable performance with DeepSets and Softmax Aggregation while using fewer parameters."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This model has good scalability as it simply keeps the summation form regardless of the size of data points.\n2. The model architecture is simple and it has fewer parameters than the other counterparts."
                },
                "weaknesses": {
                    "value": "1. The model requires to calculate the inverse of Fisher matrix in each iteration of the estimator. I am concerned with the computation complexity. I believe the complexity analysis and runtime comparison with other counterparts are needed. \n2. The proposed model has limited flexibility, as it is only permutation invariant. While its counterpart DeepSets have both invariant and equivariant formulations.\n3. The proposed models are only compared with the mentioned two counterparts, which are not enough to demonstrate the effectiveness. More extensive experiments are needed.\n4. The clarity of this paper needs to be improved. Many symbols have unexplained meanings and shapes."
                },
                "questions": {
                    "value": "1. What makes the Fishnets robust to noise?\n2. In section 5, why does the experiments only perform on deep GNNs? Edge aggregation can be also performed on shallow GNNs, such as GAT [1].\n\n[1] Veli\u010dkovi\u0107, Petar, et al. \"Graph attention networks.\"\u00a0arXiv preprint arXiv:1710.10903\u00a0(2017)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 7,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5575/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698887487805,
            "cdate": 1698887487805,
            "tmdate": 1699636573541,
            "mdate": 1699636573541,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rQHzwYLgOf",
                "forum": "dKPh4CLmYp",
                "replyto": "ZwqppVBiWi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5575/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5575/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response and questions ! \n\n**Updates to Paper:** We have revised the introduction in Section 2 to clarify the rigorous theoretical foundation, based on the likelihood principle, upon which we base our claim of information optimality. We include a detailed treatment of the information inequality and the Cramer-Rao bound (with new proof in the appendix), and then proceed to set-based likelihoods and Fishnets embedding. We additionally provide a new, more in-depth empirical analysis of Fishnets aggregation on three OGB graph datasets (for graph- and node-level prediction).  Replacing their aggregators with Fishnets we are able to reproduce or exceed the SOTA benchmarks of Li et al\u2019s DeeperGCN paper but with far fewer GNN layers and learnable parameters. Finally, we improved the organisation and flow of the paper from theory to Bayesian information saturation to GNN prediction.\n\n**Response to Comments:**\nTo respond to your questions, we refer you to our revised Section 2. Fishnets is robust to heterogeneous noise because we explicitly parameterise the inverse-Fisher weighting scheme as a learned function of the data (which in the linear regression and noisy proteins cases includes estimates of the noise variance).\n\nWe additionally revised our GNN section to include far more examples of the Fishnets aggregation. We elected to use the GCN framework due to many readily-available examples, as well as to show that the Fishnets aggregation could improve performance and reduce learnable parameters as a drop-in replacement."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5575/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700671849712,
                "cdate": 1700671849712,
                "tmdate": 1700672730062,
                "mdate": 1700672730062,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]