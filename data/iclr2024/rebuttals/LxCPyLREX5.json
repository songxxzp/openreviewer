[
    {
        "title": "Federated Learning under Label Shifts with Guarantees"
    },
    {
        "review": {
            "id": "xUVZHYhqDo",
            "forum": "LxCPyLREX5",
            "replyto": "LxCPyLREX5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7235/Reviewer_VBZD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7235/Reviewer_VBZD"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the label shift adaptation problem under the federated learning setting. There are two parts. For the first part, this paper proposes VRLS, a regularized version of the MLLS method. The authors further show that the optimization problem can be approximately solved by an EM procedure. The second part introduced how to apply the VRLS algorithm to the federated learning setting. Theoretical guarantees on the sample complexity of the density ratio label density ratio estimator and the convergence rate of the IW-ERM algorithm are provided. Extensive experiments are also conducted to evaluate the proposed methods."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The strengths of this paper as as follows:\n- Interesting problem formulation: this paper studies the label shift problem under the Federated learning setting. The problem formulation is both interesting and practically relevant.\n- Superior empirical performance: this paper proposes a regularized version of the MLLS method. Experiments show that the proposed method achieves superior empirical performance than MLLS method."
                },
                "weaknesses": {
                    "value": "The weaknesses of the paper are as follows:\n- Unclear main focus: While the paper is titled \"Federated Learning under Label Shifts with Guarantees,\" I noticed that a significant portion appears to study the classical label shift problem in supervised learning. In particular, Section 3 only briefly touches upon applying the proposed density ratio estimation to federated learning, but it lacks a detailed algorithm description and a thorough discussion of its contributions. In Section 4, Theorem 1 and Theorem 2 seem to offer limited insight into how the proposed algorithm might effectively tackle the challenges of federated learning. Specifically, Theorem 1 seems to pertain only to a single client, and the presentation of Theorem 2 is somewhat confusing, as discussed in the next point.\n\n- Clarity of Theorem 2: The statement of Theorem 2 strikes me as somewhat informal, particularly due to my uncertainty regarding the definition of the notation $\\ell$. In Section 2, it is defined as the loss function on a single sample. In light of this, it's not unclear to me how the theorem relates to the objective Eq.(IW-ERM) that the authors are trying to minimize. Moreover, the proof of Theorem 2 seems to be a straightforward application of (Liu et al., 2023, Theorem 4.1). It's not clear how this theorem perspective helps to enhance our understanding of the federated learning problem.\n\n- Unclear theoretical advantages: As indicated by Eq (3.1) and Eq. (3.2), the proposed method appears similar to MLLS with the difference that the model $\\theta^*$, is trained with an additional regularization term. While the experiments demonstrate the benefit of this additional regularization, Theorem 3 shows a similar convergence rate for the proposed VRLS when compared to MLLS. I think the theorem will be more appealing if the authors can provide a more precise explanation of why the regularization helps with the label shift problem.\n\n- Empirical comparison: a closely related work is [1] as cited by the authors, which also considers adding a regularization term to perform for the label density ratio estimation. I think the experiments will be more convincing by also taking [1] as a compared method.\n\n[1]  Azizzadenesheli et al.  Regularized learning for domain adaptation under label shifts. In ICML 2019."
                },
                "questions": {
                    "value": "1. The connection between the optimization problem in Eq.(Reg-Est) and that shown by Eq.(3.1) and Eq.(3.2) is not immediately clear to me. In Eq.(Reg-Est), the regularization term is incorporated into the training of the density ratio, whereas in Eq.(3.1), it appears to be a part of the loss function to train the classifier $\\theta^*$. It would be beneficial if the author could clarify this by providing a more formal statement that establishes the equivalence between these two optimization problems.\n\n2. Could you provide a more comprehensive theoretical explanation of how the proposed help to minimize goal Eq.(IW_ERM) mentioned in Section 2?  (Please refer to the second point of weaknesses for more details.)\n\n3. I think this paper would be more appealing if the authors could show the advantage of the proposed method over MLLS from a theoretical view. (Please refer to the third point of weaknesses for more details.)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7235/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698830288222,
            "cdate": 1698830288222,
            "tmdate": 1699636861392,
            "mdate": 1699636861392,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "I4uDnsBYzA",
                "forum": "LxCPyLREX5",
                "replyto": "xUVZHYhqDo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7235/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7235/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer VBZD"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their thoughtful comments which we address one by one below.\n\n**Empirical comparison**\n\n For your reference, we upload two plots anonymously [here](https://imgur.com/a/bCFhOTi). Regularized Learning under Label Shift (RLLS) [1] is a variation of BBSE, based on the confusion matrix approach. This method has been already compared with MLLS in [2]. I also implemented this method and obtained consistent results as Figure 1 (b)(e) in [2]. To sum up, RLLS, which is based on the confusion matrix, improved based on BBSE, underperforms divergence-based and EM-based methods. The potential reason has been studied by [2]: \u201cThis concludes MLLS\u2019s superior performance is not because of differences in loss function used for distribution matching but due to differences in the granularity of the predictions, caused by crude confusion matrix aggregation\u201d, and \u201cMLLS\u2019s superior performance (when applied with more granular calibration techniques) is not due to its objective but rather to the information lost by BBSE via confusion matrix calibration\u201d. \n\n **The advantage of the proposed method over MLLS from a theoretical view** \nThe generalization bounds in Theorem 1 are there to  highlight the dependence on the complexity of the function class in which the optimization is conducted. The presence of an additional regularizer in our method induces a reduction in the size of the model. We have not been able to characterize exactly this reduction yet, but we believe this reduced complexity is behind our improved results. It is an interesting direction for future work.\n\n**Formal statement that establishes the equivalence between these two optimization problems**\n As you correctly pointed out, the IW-ERM framework under federated learning can indeed be seen as an extension of the single-client IW-ERM to a multi-client scenario. This generalization is succinctly encapsulated in the optimization problem outlined in section C.3 of the Appendix.\n\nFrom an optimization perspective, the IW-ERM's adaptation to multiple clients translates to the aggregation of the numerator of the weight ratio for each individual client. This modification not only preserves the essence of the IW-ERM approach but also effectively scales it for federated learning environments.\n\nRegarding the convergence rate, the established bounds, and communication costs, our study presents several theorems that address these aspects in depth. These theorems provide a solid theoretical foundation, demonstrating how the IW-ERM under federated learning efficiently navigates these challenges. Specifically, they illustrate that despite the inherent complexities of federated learning, our approach remains both robust and scalable, effectively managing the balance between communication efficiency and convergence rate.  Additionally, the updated PDF includes various theorems concerning the lower and upper bounds of convergence rate and communication guarantees. Here is a summary: To sum up, we have:\n\n1. Negligible impact on Convergence Rate and Communication Guarantees (Theorem 2):  $O(r_{max} h(T))$.\n2. Upper Bound on Convergence Rate for Convex and Smooth Optimization (Theorem 3):\n\\begin{align}\nE[l(h_{w_T})-l(h_{w^\\star})]\n\\lesssim \\frac{r_{max}b D^2}{\\tau R}+\\frac{(r_{max}b D^4)^{1/3}}{(\\sqrt{\\tau}R)^{2/3}}+\\frac{D}{\\sqrt{K\\tau R}}.\n\\end{align}\n3. Lower Bound on Convergence Rate for for Convex and Second-order Differentiable Optimization (Theorem 4):\n\\begin{align}\nE[l(h_{w_T})-l(h_{w^\\star})]\n\\gtrsim \\frac{r_{\\max}b D^2}{\\tau R}+\\frac{(r_{max}b D^4)^{1/3}}{(\\sqrt{\\tau}R)^{2/3}}+\\frac{D}{\\sqrt{K\\tau R}}.\n\\end{align}\n4. Convergence and Communication Bounds for Nonconvex Optimization for  PL with Compression (Theorem 5):\n\\begin{align}\nR\\lesssim \\Big(\\frac{q}{K}+1\\Big)\\kappa\\log\\Big(\\frac{1}{\\epsilon}\\Big)\\quad and \\quad \\tau\\lesssim\\Big(\\frac{q+1}{K(q/K+1)\\epsilon}\\Big).\n\\end{align}\n5. Convergence and Communication Guarantees for Nonconvex Optimization with Adaptive Step-sizes for Non-convex Optimization with Adaptive Step-sizes Assumptions (Theorem 6):\n\\begin{align}\nT\\lesssim  \\frac{r_{max}}{K\\epsilon^3}\\quad and \\quad R\\lesssim\\frac{r_{max}}{\\epsilon^2}.\n\\end{align}\n6. Oracle Complexity of Proximal Operator for  Composite Optimization with Proximal Operator (Theorem 7):  $O\\big(r_{max}\\sqrt{\\kappa}\\log(1/\\epsilon)\\big)$\n\n7. High-probability Bound for Nonconvex Optimization for Sub-Gaussian Assumption (Theorem 8):\n\n\\begin{align}\n\\frac{1}{T}\\sum_{t=1}^T\\|\\nabla_{w}l(h_{w_t})\\|_2^2= O(\\sigma \\sqrt{\\frac{r_m \\beta}{T}} + \\frac{\\sigma^2\\log(1/\\delta)}{T}).\n\\end{align}\n\n\n[1]. Azizzadenesheli et al. Regularized learning for domain adaptation under label shifts. In ICML 2019.\n\n[2]. Saurabh Garg, Yifan Wu, Sivaraman Balakrishnan, and Zachary C. Lipton. A unified view of label\t\nshift estimation. In Advances in neural information processing systems (NeurIPS), 2020."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7235/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700216795158,
                "cdate": 1700216795158,
                "tmdate": 1700230895495,
                "mdate": 1700230895495,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CsrIBWaVfu",
                "forum": "LxCPyLREX5",
                "replyto": "xUVZHYhqDo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7235/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7235/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer VBZD"
                    },
                    "comment": {
                        "value": "Dear Reviewer VBZD,\n\nWe would like to express our gratitude for your review of our ICLR submission. We have taken your insightful comments into account and have made the necessary adjustments. Should you have any questions or require further information, please feel free to contact us. We look forward to any additional feedback you may have."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7235/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700478590009,
                "cdate": 1700478590009,
                "tmdate": 1700478590009,
                "mdate": 1700478590009,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Hq6epIjSP1",
                "forum": "LxCPyLREX5",
                "replyto": "xUVZHYhqDo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7235/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7235/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer VBZD"
                    },
                    "comment": {
                        "value": "This is a friendly reminder that we submitted our responses to your comments and are looking forward to your feedback for further refinement of our paper."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7235/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700726477151,
                "cdate": 1700726477151,
                "tmdate": 1700726477151,
                "mdate": 1700726477151,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "n4kk5tAGp8",
            "forum": "LxCPyLREX5",
            "replyto": "LxCPyLREX5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7235/Reviewer_PGzm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7235/Reviewer_PGzm"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on addressing the label shift problems in both single-client and federated settings. To address the statistical heterogeneity in FL, the authors proposed an importance-weighting ERM method to address joint intra-client and inter-client label shifts. Moreover, the paper offers theoretical generalization guarantees for the proposed density ratio estimation, encompassing adjustments for label shifts across and within clients. Empirical evaluations using the CIFAR-10 and MNIST datasets, along with a series of ablation studies, corroborate the efficacy of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper pioneers the exploration of label shift challenges within federated learning, introducing a novel framework that distinguishes between inter-client and intra-client variations. This foundational work opens avenues for future scholarly inquiry in this underexplored but critically relevant domain.\n\n2. By establishing a connection with existing label shift literature, such as BBSE and MLLS, the authors have advanced these theories by integrating a regularized objective function. This enhanced formulation not only addresses the label shift in latent space but also embeds regularization within the predictor training phase, allowing for an adaptive response to distribution shifts.\n\n3. The paper excels in its delivery of a straightforward and comprehensible methodology, underpinned by a thorough theoretical analysis across various scenarios. Its clarity and depth offer great insights for practical applications."
                },
                "weaknesses": {
                    "value": "1. While the authors have conducted experiments across a variety of settings, the scope of their datasets remains limited. To more convincingly demonstrate the robustness and practicality of the proposed method, it would be beneficial to extend these experiments to larger-scale datasets and real-world application scenarios.\n\n2. For greater clarity and understanding, a detailed derivation of Equation (Reg-Est) within the main body of the paper would be advantageous."
                },
                "questions": {
                    "value": "Please see the comments in the Strengths and Weaknesses sections."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7235/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698934269995,
            "cdate": 1698934269995,
            "tmdate": 1699636861300,
            "mdate": 1699636861300,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "LYhY7qiZYi",
                "forum": "LxCPyLREX5",
                "replyto": "n4kk5tAGp8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7235/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7235/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer PGzm"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their thoughtful comments which we address one by one below.\n\n **Larger-scale datasets and real-world application scenarios**\n\nWe appreciate your recommendation to incorporate larger-scale datasets and explore real-world application scenarios. We extended our experiments from 100 clients to 200 clients on CIFAR10, obtained uniformly increased accuracy among methods, and our VRLS still dominated. \n\n------------\n\n| CIFAR-10 | Our IW-ERM | FedAvg | FedBN |\n|----------|------------|--------|-------|\n| Avg. accuracy (100 clients) | **0.5354** | 0.3915 | 0.1537 |\n| Avg. accuracy (200 clients) | **0.6216** | 0.5942 | 0.1753 |\n----------\n\nWhile we acknowledge the value of conducting experiments in real-world application scenarios, there are specific constraints and challenges that influenced our decision to not pursue it in this study.\nFor instance, as indicated in [1], while larger, more real-world datasets provide a valuable benchmark, $D(p_{te}(x|y) \\Vert p_{tr}(x|y)) < \\epsilon$ is hard to quantitatively assess in these contexts. An attempt to use test samples from CIFAR10.1-v6 [2, 3] was made. However, due to significant feature changes in this dataset, all methods including ours experienced a substantial increase in estimation errors. This led to comparisons that were not meaningful, as the estimations were poor and did not provide reliable insights.\nGiven these challenges and the nature of our study, we chose to focus on datasets where the constraints and feature stability allowed for more meaningful and accurate analysis. We believe that this approach, while more constrained, provides valuable insights within its scope and serves the objectives of our research effectively. In contrast, we can turn to directly focus on the conditional probability \\( p(x|y) \\) to address the relaxed label shift problem, which would be more efficient and straightforward to handle real-world problems.\n\n**For greater clarity and understanding, a detailed derivation of Equation (Reg-Est) within the main body of the paper would be advantageous**\n\nWe acknowledge that there were certain oversights in the formulation of the equation initially, leading to some variations in the expression. However, despite this, the experimental setup was thoughtfully designed and executed. In the final version, we will add more details to the body since we have an additional page to elaborate further.\nMost importantly, our study demonstrated impressive performance. The strength of our work lies in the application of ratio estimation methods in practical scenarios, specifically in the context of federated learning. We successfully implemented the model in training sets without any additional information updates, and the performance on test sets nearly matched the theoretical upper bounds, using a fixed ratio for training. Besides, our approach shines, effectively addressing and overcoming challenges related to privacy, convergence rate, and communication costs.\n\n[1]. Zachary C. Lipton. Rlsbench: Domain adaptation under relaxed label shift. In International Conference on Machine Learning (ICML), 2023.\n\n[2]. Antonio Torralba, Rob Fergus, and William T. Freeman. 80 million tiny images: A large data set for nonparametric object and scene recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 30(11):1958\u20131970, 2008.\t\t\t\n\n[3]. Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do CIFAR-10 classifiers generalize to CIFAR-10? arXiv preprint arXiv:1806.00451, 2018."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7235/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700216934418,
                "cdate": 1700216934418,
                "tmdate": 1700216934418,
                "mdate": 1700216934418,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "L61eyV3agu",
                "forum": "LxCPyLREX5",
                "replyto": "n4kk5tAGp8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7235/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7235/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer PGzm"
                    },
                    "comment": {
                        "value": "Dear Reviewer PGzm,\n\nWe sincerely appreciate your review of our ICLR submission. Your valuable feedback has been carefully considered and addressed. If you have any further inquiries or require additional details, please do not hesitate to reach out. Your continued input is highly valued."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7235/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700478504375,
                "cdate": 1700478504375,
                "tmdate": 1700478523865,
                "mdate": 1700478523865,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "NORDdTUtsY",
                "forum": "LxCPyLREX5",
                "replyto": "n4kk5tAGp8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7235/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7235/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer PGzm"
                    },
                    "comment": {
                        "value": "Just a quick reminder that we have responded to your comments and look forward to your feedback to enhance our work."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7235/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700726319151,
                "cdate": 1700726319151,
                "tmdate": 1700726319151,
                "mdate": 1700726319151,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "9SbsI4gVAi",
            "forum": "LxCPyLREX5",
            "replyto": "LxCPyLREX5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7235/Reviewer_HSCY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7235/Reviewer_HSCY"
            ],
            "content": {
                "summary": {
                    "value": "This manuscript considers the problem of training a global model in a federated learning setting under challenging inter-client and intra-client label shifts. The authors propose a new method for density ratio estimation and establish a high probability estimation and convergence bounds. Experimental results on MNIST and CIFAR datasets show the effectiveness of the proposed methods."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper studied a relevant problem in federated learning: a special type of data heterogeneity with client label shift. \n2. The paper is generally well-written and easy to follow."
                },
                "weaknesses": {
                    "value": "1. I am not sure of the relevance of the statistical results (Theorem 1) and optimization results (Theorem 2) in the context of federated learning (FL). In FL, there is only limited communication and this critical aspect is not categorized by these theorems. I am wondering how many communication rounds the algorithm requires to obtain a statistical and computational guarantee.\n\n2. Experimental results are weak. Tacking data heterogeneity is a well-known problem in federated learning (e.g., FedProx [r1], SCAFFOLD [r2], minibatch SGD [r3], etc.). However, it seems that the authors only consider FedAvg and FedBN as baselines. I suggest the authors also compare against these papers, which were also proposed to learn a global model with client data heterogeneity.\n\n[r1] Li, Tian, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith. \"Federated optimization in heterogeneous networks.\" Proceedings of Machine learning and systems 2 (2020): 429-450.\n\n\n[r2] Karimireddy, Sai Praneeth, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and Ananda Theertha Suresh. \"Scaffold: Stochastic controlled averaging for federated learning.\" In International conference on machine learning, pp. 5132-5143. PMLR, 2020.\n\n\n[r3] Woodworth, Blake E., Kumar Kshitij Patel, and Nati Srebro. \"Minibatch vs local sgd for heterogeneous distributed learning.\" Advances in Neural Information Processing Systems 33 (2020): 6281-6292.\n\n\n3. The authors did not report the communication round result in the experiments section. It is unclear whether improves over FedAvg or  FedBN when there is only limited communication. There is still a huge gap between the IW-ERM and the upper-bound performance (Table 3)."
                },
                "questions": {
                    "value": "1. Can you elaborate on the communication round results theoretically and empirically?\n\n2. Can you compare against more baselines for tackling data heterogeneity in federated learning?\n\nI am happy to consider increasing the score if these concerns are addressed."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7235/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699246463374,
            "cdate": 1699246463374,
            "tmdate": 1699636861203,
            "mdate": 1699636861203,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "LtKozFHYhC",
                "forum": "LxCPyLREX5",
                "replyto": "9SbsI4gVAi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7235/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7235/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer HSCY"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their thoughtful comments which we address one by one below.\n\n **Theorems 1 and 2. Elaborate on the communication rounds theoretically and empirically?**\n\nTheorem 1 primarily provides assurances for the accuracy of ratio estimation on an individual client, establishing that the upper bound of the estimation error is contingent upon the size of the test sample and the calibration quality of the pre-trained predictor. In the context of Federated Learning (FL), this theorem applies to each client by leveraging all available unlabeled test samples. By doing so, we locally estimate the ratio on each client, which allows us to closely approximate the true ratios, thereby ensuring that our estimation is as accurate as feasible. As a result, the implementation of Importance Weighted Empirical Risk Minimization (IWERM) will be as unbiased as possible with respect to the test distribution.\n\nTheorem 2 gives the convergence rates in the context of our ratios with data-dependent constant terms which increase linearly with negligible communication overhead over the ERM-solver baseline without importance weighting. In Appendix F, we establish tight convergence rates and communication guarantee for Eq.(IW-ERM) with a broad range of importance optimization settings.\n\nWhat\u2019s more, the estimation of ratios and the implementation of IW-ERM introduce negligible additional communication costs during the global training phase. The estimation process is conducted prior to global training, which takes place locally on each client with an off-the-shelf predictor, which can potentially be MLP or other simple networks already existing. Subsequently, the estimated ratios, rather than the raw data, are shared across clients, entailing N^2 exchanges of K-dimensional vectors within one round of communication. Once global training commences, these ratios remain static with iterations T\u2014they are neither updated nor exchanged again.\nTo sum up, we have:\n\n1. Negligible impact on Convergence Rate and Communication Guarantees (Theorem 2):  $O(r_{max} h(T))$.\n2. Upper Bound on Convergence Rate for Convex and Smooth Optimization (Theorem 3):\n\\begin{align}\nE[l(h_{w_T})-l(h_{w^\\star})]\n\\lesssim \\frac{r_{max}b D^2}{\\tau R}+\\frac{(r_{max}b D^4)^{1/3}}{(\\sqrt{\\tau}R)^{2/3}}+\\frac{D}{\\sqrt{K\\tau R}}.\n\\end{align}\n3. Lower Bound on Convergence Rate for for Convex and Second-order Differentiable Optimization (Theorem 4):\n\\begin{align}\nE[l(h_{w_T})-l(h_{w^\\star})]\n\\gtrsim \\frac{r_{\\max}b D^2}{\\tau R}+\\frac{(r_{max}b D^4)^{1/3}}{(\\sqrt{\\tau}R)^{2/3}}+\\frac{D}{\\sqrt{K\\tau R}}.\n\\end{align}\n4. Convergence and Communication Bounds for Nonconvex Optimization for  PL with Compression (Theorem 5):\n\\begin{align}\nR\\lesssim \\Big(\\frac{q}{K}+1\\Big)\\kappa\\log\\Big(\\frac{1}{\\epsilon}\\Big)\\quad and \\quad \\tau\\lesssim\\Big(\\frac{q+1}{K(q/K+1)\\epsilon}\\Big).\n\\end{align}\n5. Convergence and Communication Guarantees for Nonconvex Optimization with Adaptive Step-sizes for Non-convex Optimization with Adaptive Step-sizes Assumptions (Theorem 6):\n\\begin{align}\nT\\lesssim  \\frac{r_{max}}{K\\epsilon^3}\\quad and \\quad R\\lesssim\\frac{r_{max}}{\\epsilon^2}.\n\\end{align}\n6. Oracle Complexity of Proximal Operator for  Composite Optimization with Proximal Operator (Theorem 7):  $O\\big(r_{max}\\sqrt{\\kappa}\\log(1/\\epsilon)\\big)$\n\n7. High-probability Bound for Nonconvex Optimization for Sub-Gaussian Assumption (Theorem 8):\n\n\\begin{align}\n\\frac{1}{T}\\sum_{t=1}^T\\|\\nabla_{w}l(h_{w_t})\\|_2^2= O(\\sigma \\sqrt{\\frac{r_m \\beta}{T}} + \\frac{\\sigma^2\\log(1/\\delta)}{T}).\n\\end{align}"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7235/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700222628575,
                "cdate": 1700222628575,
                "tmdate": 1700224998992,
                "mdate": 1700224998992,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "a2xWFXeYhe",
                "forum": "LxCPyLREX5",
                "replyto": "9SbsI4gVAi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7235/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7235/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer HSCY"
                    },
                    "comment": {
                        "value": "Dear Reviewer HSCY,\n\nThank you for reviewing our ICLR submission. We have diligently addressed your valuable concerns. If you have any questions or require additional information, please feel free to reach out. We sincerely appreciate your continued feedback."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7235/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700478427777,
                "cdate": 1700478427777,
                "tmdate": 1700478427777,
                "mdate": 1700478427777,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "AfkFgyVNjY",
                "forum": "LxCPyLREX5",
                "replyto": "9SbsI4gVAi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7235/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7235/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer HSCY"
                    },
                    "comment": {
                        "value": "I am writing to kindly remind you that we submitted our responses, but we have yet to receive your feedback. We highly value your opinions and are eager to further improve our paper based on your insights."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7235/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700726197845,
                "cdate": 1700726197845,
                "tmdate": 1700726197845,
                "mdate": 1700726197845,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]