[
    {
        "title": "LMExplainer: A Knowledge-Enhanced Explainer for Language Models"
    },
    {
        "review": {
            "id": "bK2CNuXN4y",
            "forum": "VRJzlm2ecv",
            "replyto": "VRJzlm2ecv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9174/Reviewer_hNAs"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9174/Reviewer_hNAs"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the interpretability issue of large language models.    \nDifferent from some existing studies that use attention-based methods, the authors introduce knowledge graphs to generate explanations.\nSpecifically, the key decision signals (elements in the constructed knowledge graph) are extracted and used as instructions for large language models to generate explanations.     \nThe experimental results reflect the explanations could help model accuracy."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The interpretability of LLMs deserves to be investigated and the proposed approach is model-agnostic. \n- Knowledge graphs are leveraged to extract key elements to support the LLMs to generate explanations.\n- The experimental results show both good model accuracy and interpretability."
                },
                "weaknesses": {
                    "value": "- Although the proposed approach is model-agnostic, it is only tested on RoBERTa-large and its variants. Larger language models are suggested to be tested. This is because LLMs have strong capacity and knowledge graphs might not help them. \n- The approach is not clearly explained. How to optimize f_{enc} and MLP is not stated. Moreover, how the explanations affect the model accuracy is not clear. As shown in Figure 1, the generated explanations do not affect answer selection. Besides, how to optimize the whole method?\n- The baselines seem to be not very new. More recent and strong baselines are preferred.\n- From my personal perspective, the novelty of the techniques are not well validated.   \n     \nTypos:\n(1) \"their decision process lack transparency\" -> lacks"
                },
                "questions": {
                    "value": "Please see the weakness part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9174/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698242137695,
            "cdate": 1698242137695,
            "tmdate": 1699637154721,
            "mdate": 1699637154721,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zkRgw4DJNE",
                "forum": "VRJzlm2ecv",
                "replyto": "bK2CNuXN4y",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9174/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9174/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We sincerely appreciate your thorough review and insightful feedback on our manuscript. Your comments have not only helped us in refining our work but also provided an opportunity to clarify and elaborate on key aspects of our research. Below, we address each of your points in detail.\n\n\n**Response to Weaknesses:**\n\n**1. (W1) Testing on Larger LM:**\n\nIn our current experiments, we selected RoBERTa-large and its variants as our primary test subjects due to their effectiveness and widespread recognition in the NLP community. We acknowledge the impressive capabilities of larger LMs like GPT-3.5 in language understanding and generation. However, we posit that integrating KGs with these models can yield substantial benefits. KGs provide structured, domain-specific knowledge that may not be entirely encompassed by the pre-trained parameters of LLMs. This integration is particularly advantageous in tasks requiring specific knowledge domains.\n\nTo further support this claim, we conducted a zero-shot experiment with GPT-3.5-turbo, maintaining the same experimental settings as our tests on the CommonsenseQA dataset. The results were telling: GPT-3.5-turbo achieved an accuracy of 73.54%, while our approach yielded a higher accuracy of 77.31% (Table below). \n| Model            | Accuracy (%) |\n|------------------|--------------|\n| GPT-3.5-turbo    | 73.54        |\n| **LMExplainer (ours)**| 77.31        |\n\nThis outcome indicates that the performance of advanced LLMs like GPT-3.5 may not be as robust as anticipated. This underscores the necessity and relevance of our approach that integrates KGs, demonstrating its potential to fill gaps where even sophisticated LLMs might fall short.\n\nWe want to emphasize the primary benefit of our graph modular design is the transformation of the 'black-box' nature of LM into a more transparent graph structure. By narrowing down the knowledge search space of the LM, our approach provides a clearer understanding of the model's decision-making process and contributes to improve model performance. Our approach demonstrates that it is possible to achieve both high performance and high explainability in LMs, addressing a key challenge in the field of AI.\n\n\n**2. (W2) Clarification of Approach:**\n\nThe MLP in Section 3.2 is a learned part of the pre-trained LM. This design employs the probability computation function of the pre-trained LM to calculate the score. To reduce the misunderstanding, we have revised equation (2), and added a clear description in the updated version. Please review the latest version. The recently modified sections are highlighted in blue for easy identification.\n\nThe MLPs in the later section are trained with the LM. When LM is fine-tuned, the MLPs are trained simultaneously. The loss function used is cross-entropy loss. The dimension of hidden layers is 200, and the hidden layers are 2. The optimization involves fine-tuning both the knowledge representation (via the GAT) and the predictive components (encoder and MLPs). \n\nIn our design, explanations are generated based on the predicted answer and key reason-elements. The explanations are not used to affect the answer selection. The explanations are generated to reflect the decision-making process of the model in a human-understandable way. \n\n**3. (W3) Clarification of Baselines:**\n\nWe have included what we believe are the latest and most typical works as baselines. This selection was made to provide a comprehensive benchmark. We recognize that the field is continuously evolving, and we are dedicated to adapting our work to reflect these changes. Currently, we are committed to conducting a thorough review and update of our baseline. We will ensure that our study remains relevant and provides a meaningful contribution to the field. If there are specific recent works or baselines that you believe would enhance the robustness of our comparison, we kindly invite you to list them. \n\n\n**It's not over yet, please check our next comment.**"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9174/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700494788069,
                "cdate": 1700494788069,
                "tmdate": 1700494788069,
                "mdate": 1700494788069,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1UMh91kG7i",
                "forum": "VRJzlm2ecv",
                "replyto": "bK2CNuXN4y",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9174/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9174/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Look Forward to Your Response"
                    },
                    "comment": {
                        "value": "Dear Reviewer,\n\nSince we are on the last day of author-reviewer discussions, we keenly await your feedback on our rebuttal and the paper modifications to address your comments. We believe we have addressed all the concerns raised by you. If there are any outstanding concerns, please let us know.\nWe look forward to your response and appreciate any feedback.\n\n\\\nThank you,\n\nAuthors"
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9174/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700660972214,
                "cdate": 1700660972214,
                "tmdate": 1700660972214,
                "mdate": 1700660972214,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "29m34Zqfcf",
            "forum": "VRJzlm2ecv",
            "replyto": "VRJzlm2ecv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9174/Reviewer_d4rH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9174/Reviewer_d4rH"
            ],
            "content": {
                "summary": {
                    "value": "Language models like GPT-4 are powerful for natural language processing tasks but lack transparency due to their complex structures, which can hinder user trust. To address this issue, LMExplainer is proposed, which is a knowledge-enhanced explainer that provides human-understandable explanations by locating relevant knowledge in a large-scale knowledge graph using graph attention neural networks. Experiments show that LMExplainer outperforms existing methods on CommonsenseQA and OpenBookQA datasets, generating more comprehensive and clearer explanations compared to other algorithms and human-annotated explanations."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The proposed model is clear and easy to follow.\n2. The studied topic of explaining LLM with knowledge graph is a reasonable direction. The idea of improve accuracy and explanation simutaneously is interesting.\n3. The experimental results are sound."
                },
                "weaknesses": {
                    "value": "1. The code is not available to the public.\n2. The modular design of proposed model made it hard to judge where the benefits of the model is from."
                },
                "questions": {
                    "value": "1. It seems the proposed model is not learned in an end-to-end way. How was the parameters of MLP learned in line 5 of Algorithm 1? and how does the generated graph influence the final results?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9174/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698591236139,
            "cdate": 1698591236139,
            "tmdate": 1699637154609,
            "mdate": 1699637154609,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qr0AoblBpC",
                "forum": "VRJzlm2ecv",
                "replyto": "29m34Zqfcf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9174/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9174/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We sincerely appreciate your thorough review and insightful feedback on our manuscript. Your comments have not only helped us in refining our work but also provided an opportunity to clarify and elaborate on key aspects of our research. Below, we address each of your points in detail.\n\n\n**Response to Weaknesses:**\n\n**1. (W1) Availability of Code:**\n\nWe care about the reproducibility of our work very much. It is our responsibility to provide the code and data to the community. We are currently preparing our code for public release. We will ensure that it is well-documented and accessible to the community. \n\n**2. (W2) Modular Design:**\n\nThank you for your comment regarding the modular design of our proposed model. We appreciate the opportunity to clarify how this design contributes to the model's effectiveness and transparency.\n\nThe primary benefit of our model's modular design is the transformation of the 'black-box' nature of LM into a more transparent graph structure. By narrowing down the knowledge search space of the LM, our approach provides a clearer understanding of the model's decision-making process and contributes to improve model performance. This transparency is crucial for applications where understanding the rationale behind model predictions is as important as the predictions themselves. \n\nWe include an ablation study analyzing the main components of our model to further validate the benefits of our design in the paper. The results from this study clearly indicate that our interpretation design, a core aspect of the modular approach, is instrumental in enhancing the final performance of the model. This study provides empirical evidence supporting the advantage of our design. \n\n\n**Response to Questions:**\n\n**1. (Q1) Design and Influence Clarification:**\n\nOur model operates in an end-to-end manner in terms of obtaining key elements from the reasoning process for explanation purposes. Our model extracts these elements and integrates them into a constrained structure, which is then processed by the GPT-3.5-turbo. The GPT-3.5-turbo is utilized primarily for organizing these elements into a human-understandable way. \n\nThe MLP in Algorithm 1 is a learned part of the pre-trained LM. This design employs the probability computation function of the pre-trained LM to calculate the score. To reduce the misunderstanding, we have revised equation (2), and added a clear description in the updated version. Please review the latest version. The recently modified sections are highlighted in blue for easy identification.\n\nThe generated graph provides the structured representations for knowledge. Such structure is more interpretable. The graph is also used to guide the reasoning process of the LM. The LM is constrained to search for knowledge from the graph, which narrows down the knowledge search space and improves the performance. The accuracy and relevance of the retrieved graph are important, as they directly impact the LM's performance in generating predictions or explanations. By ensuring that the graph accurately captures the relative knowledge, our model effectively utilizes these insights for decision-making and explanation generation.\n\n---\nWe are grateful for the opportunity to enhance our manuscript based on your feedback. We hope that our response and revised version will address your concerns and improve the quality of our work. We look forward to hearing from you."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9174/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700494467878,
                "cdate": 1700494467878,
                "tmdate": 1700494467878,
                "mdate": 1700494467878,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4EbfWqDNXF",
                "forum": "VRJzlm2ecv",
                "replyto": "29m34Zqfcf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9174/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9174/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Look Forward to Your Response"
                    },
                    "comment": {
                        "value": "Dear Reviewer,\n\nSince we are on the last day of author-reviewer discussions, we keenly await your feedback on our rebuttal and the paper modifications to address your comments. We believe we have addressed all the concerns raised by you. If there are any outstanding concerns, please let us know.\nWe look forward to your response and appreciate any feedback.\n\n\\\nThank you,\n\nAuthors"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9174/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700660954648,
                "cdate": 1700660954648,
                "tmdate": 1700660954648,
                "mdate": 1700660954648,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Jjzaclfimk",
            "forum": "VRJzlm2ecv",
            "replyto": "VRJzlm2ecv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9174/Reviewer_gb89"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9174/Reviewer_gb89"
            ],
            "content": {
                "summary": {
                    "value": "Language models like GPT-4 are powerful but opaque. To make their decisions understandable, a tool called LMExplainer has been developed. It harnesses Knowledge Graphs and the Graph Attention Neural Network (GAT) to pinpoint and explain the model's reasoning. In tests on two QA datasets, LMExplainer outperformed benchmarks and adeptly translated the model's logic into plain language."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The authors adeptly use natural language to elucidate the inference results of LMs.\n- LMExplainer was tested on QA datasets and benchmarked against leading models. The results indicated a superior performance of LMExplainer in most scenarios."
                },
                "weaknesses": {
                    "value": "- The notation could be improved for clarity, especially inconsistencies like h_e^k and h_{v_e}^k in Equation (5) and Equation (6).\n- Equations in section 3 lack immediate clarity. Incorporating variable and function dimensions would enhance readability.\n- The study exclusively utilizes RoBERTa for LMExplainer. It's uncertain if similar results would be achieved with different LMs.\n- The structure of the experiment section needs refinement. In tables like Table 1 and Table 2, clearer categorization of the baseline models, such as which belong to fine-tuned LM RoBERTa-large or KG-augmented RoBERTa-large, would be beneficial."
                },
                "questions": {
                    "value": "- In Equation 5, the authors use f to represent MLP as m_{es}=f_n(,,). What does the \u201ccomma\u201d signify? Is it indicating concat, inner product, element-wise addition, or another operation?\n- What does \\hat{n} in Equation 5 represent? The authors haven't previously defined it.\n- In Algorithm 1, line 4, the authors apply f_{enc} to encode node v and use the encoded embeddings for KG extraction. Is f_{enc} equivalent to the LM - Encoder in Figure 1? If yes, it would be clearer if there were an arrow connecting LM - Encoder and KG Retrieval in the figure.\n- The term u^e is derived from a linear transformation of one-hot node-type vectors. What are these node-type vectors? An example would help in understanding."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9174/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698638921801,
            "cdate": 1698638921801,
            "tmdate": 1699637154493,
            "mdate": 1699637154493,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "IW10vlNgdD",
                "forum": "VRJzlm2ecv",
                "replyto": "Jjzaclfimk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9174/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9174/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We sincerely appreciate your thorough review and insightful feedback on our manuscript. Your comments have not only helped us in refining our work but also provided an opportunity to clarify and elaborate on key aspects of our research. Below, we address each of your points in detail.\n\n\n**Response to Weaknesses:**\n\n**1. (W1) Notation Clarity:**\n\nThank you for pointing out the inconsistency in our notation between Equations (5) and (6). In the mentioned equations, the notations h^k_e and h^k_{v_e} were intended to represent the same concept. We apologize for the confusion caused by this typo.\n\nWe noticed that Section 3.3 was potentially confusing to readers. In response, we have actively adopted your suggestions and made corresponding revisions. We have optimized the paragraph structure and improved the presentation of the notation to ensure the content is concise and clear. Regarding the content previously found in equations (5) and (6), we have now moved it to Appendix B.2 and provided a more detailed explanation. Please review the latest version. The recently modified sections are highlighted in blue for easy identification.\n\nWe have ensured that the notation accurately reflects the intended meaning and enhances the overall clarity of the mathematical expressions.\n\n**2. (W2) Clarity of Equations in Section 3:**\n\nWe understand that the readability and comprehensibility of mathematical expressions are important for the effective communication of our methodology. To address your suggestion, we have specified the dimensions of each variable and the input-output dimensions of each function in the revised version. Please review the latest version. The modified sections are highlighted in blue.\n\n\n**3. (W3) Utilization of LM:**\n\nWhile our current study focuses on RoBERTa, we designed LMExplainer with the intention of it being a universal method, applicable to a wide range of LMs. The core mechanisms of LMExplainer, such as the integration of knowledge graphs and the interpreting component, are not specific to RoBERTa. These components are designed to be adaptable to the architectures and functionalities of various LMs. Moreover, the method of extracting and utilizing knowledge from KGs, and the way explanations are generated and validated, are largely model-agnostic and can be applied to other LMs with minimal modifications. We acknowledge that empirical validation with other models is necessary to fully demonstrate the universality of LMExplainer. As part of our future work, we plan to extend our experiments to include a variety of LMs, to validate the adaptability of our method. \n\n**4. (W4) Table Structure:**\n\nAll models presented in Tables 1 and 2 are integrations of LM and KG. In our revised version, we will add detailed legends and footnotes to these tables to provide additional context and clarification.\n\n**Response to Questions:**\n\n**1. (Q1) Comma Notation:**\n\nIn previous Equation 5, the commas within the function f_n(,,) denote the concatenation of inputs. We have clarified this in our latest version, and ensure that the notation throughout the manuscript is consistent and correct. \n\n**2. (Q2) Clarification of \\hat{n}:**\n\nIn the previous version, \\hat{n} means the embedding of node scores. We have actively adopted your suggestions and made corresponding revisions. We have reduced the overload of symbols to lessen reader strain, simplified our descriptions, and moved the detailed implementation content to the appendix. Please refer to Section 3.3 and Appendix B.3 to review our modification. \n\n**3. (Q3) Function f_{enc}:**\n\nYes, f_{enc} is equivalent to the LM-Encoder in Figure 1. We will add an arrow connecting LM-Encoder and KG Retrieval in the figure.\n\n**4. (Q4) Node-type Vectors:**\n\nThe node-type vectors are the one-hot vectors of the node types. The type is according to the node's origin form, the input content $z$, question $\\{q\\}$, answer $\\mathcal{A}$, or the node in the KG. We have added the details of node-type information in the latest version (Appendix B.2).\n\n---\n\n**Our main responses to your feedback are incorporated in the latest version. We kindly hope you review these updates. All modifications have been highlighted in blue for easy identification.**\n\nWe are grateful for the opportunity to enhance our manuscript based on your feedback. We hope that our response and revised version will address your concerns and improve the quality of our work. We look forward to hearing from you."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9174/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700493955129,
                "cdate": 1700493955129,
                "tmdate": 1700650646662,
                "mdate": 1700650646662,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Lt8cRsQ0SE",
                "forum": "VRJzlm2ecv",
                "replyto": "Jjzaclfimk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9174/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9174/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Look Forward to Your Response"
                    },
                    "comment": {
                        "value": "Dear Reviewer,\n\nSince we are on the last day of author-reviewer discussions, we keenly await your feedback on our rebuttal and the paper modifications to address your comments. We believe we have addressed all the concerns raised by you. If there are any outstanding concerns, please let us know.\nWe look forward to your response and appreciate any feedback.\n\n\\\nThank you,\n\nAuthors"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9174/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700660932374,
                "cdate": 1700660932374,
                "tmdate": 1700660932374,
                "mdate": 1700660932374,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "FRx0sNqThz",
            "forum": "VRJzlm2ecv",
            "replyto": "VRJzlm2ecv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9174/Reviewer_gatd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9174/Reviewer_gatd"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a knowledge-enhanced explainer, LMExplainer. First, LMExplainer constructs a subgraph by integrating external knowledge into the tokens of the question. Subsequently, it utilizes graph attention neural networks to represent the subgraph and identify the most critical reasoning components within it. Finally, LMExplainer leverages a predefined structure containing the extracted explanation components. These components are used to prompt GPT-3.5 to generate explanations regarding why LMs selected a specific answer and why they rejected others. Experimental results from CommonsenseQA and OpenBookQA datasets demonstrate the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The idea of utilizing a knowledge graph as a more transparent surrogate to generate comprehensible explanations seems reasonable. This can, to some extent, reveal the reasoning process of language models. \n2. The proposed approach that extracts reason-elements as one part of the key components which are essential to the LM decision-making process is somewhat novel.\n3. The experiments and subsequent analyses demonstrate that LMExplainer not only enhances the performance of language models in open domain question answering tasks, but also exhibits the capacity to generate comprehensible explanations for these models."
                },
                "weaknesses": {
                    "value": "1. In Section 3.2, the paper employs all the tokens in the question sentence as queries to retrieve relevant knowledge. However, if there are non-entity tokens such as \"what,\" \"is,\" and so on, they can introduce noise into the process of constructing subgraphs. This could potentially increase the time and space complexity of the graph construction algorithm. As far as I am concerned, it is imperative to filter out non-entity words from the question prior to constructing subgraphs.\n2. In constructing subgraphs, the authors extract L-hop neighbors of z, indicating that 'L' is a crucial factor that influences the introduction of an appropriate number of knowledge. However, the authors did not specify how 'L' was set and did not conduct ablation experiments to investigate how 'L' affects model performance. \n3. In the ablation studies, the experimental results reveal a significant decrease in accuracy when the interpreting component in LMExplainer is removed. However, in Section 3.4, the authors note that the explanations are generated using the predicted answer. Consequently, it appears that the accuracy of model predictions is not influenced by whether or not explanations are generated. The authors should therefore clarify the relationship between generating explanations and answer predictions. \n4. There are no human evaluations of the accuracy of the explanations generated by GPT-3.5, which weakens the case presented in Table 3. This could potentially be a rare occurrence where the generated explanations happen to be correct. Further studies should be conducted to ascertain the proportion of accurately generated explanations."
                },
                "questions": {
                    "value": "1.  In Section 3.2, the authors explain that the score of each node, which represents the correlation between node v and input content, is derived by passing token embeddings through an MLP. Could you please provide additional information on how the MLP is trained? \n2. There are numerous traditional methods that also integrate knowledge graphs into language models. Undoubtedly, incorporating an external knowledge module can greatly enhance the performance of the model. Could you please elaborate further on the advantages of your knowledge integration method? \n3. This paper employs predefined structures to prompt GPT-3.5 to produce explanations. However, given the propensity of large models to suffer from hallucination, it is highly likely that they will generate explanations that are unrelated to the question, despite the presence of relevant words in the prompt. This appears to contradict the statement made in the Conclusion section, which asserts that the model can generate trustworthy explanations. How can you ensure that the generated explanations are indeed trustworthy?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9174/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698693063770,
            "cdate": 1698693063770,
            "tmdate": 1699637154354,
            "mdate": 1699637154354,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "j4Utf6s80J",
                "forum": "VRJzlm2ecv",
                "replyto": "FRx0sNqThz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9174/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9174/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We sincerely appreciate your thorough review and insightful feedback on our manuscript. Your comments have not only helped us in refining our work but also provided an opportunity to clarify and elaborate on key aspects of our research. Below, we address each of your points in detail.\n\n\n**Response to Weaknesses:**\n\n**1. (W1) Token Filtering in Subgraph Construction:**\n\nWe would like to clarify that our methodology, in fact, aligns with the approach recommended by the reviewer.\n\nIn our implementation, we have adopted a token filtering process following Yasunaga et al (2021). This approach specifically involves the exclusion of non-entity words, such as common stopwords (\"what,\" \"is,\" etc.), from the queries used to retrieve relevant knowledge. By doing so, we effectively mitigate the risk of introducing noise into the subgraph construction and maintain the efficiency of our algorithm in terms of both time and space complexity.\n\n**2. (W2) Clarification on 'L':**\n\nIn our current implementation, the choice of 'L' was based on preliminary experiments and domain expertise. Our setting aims to balance the depth of the captured knowledge against the risk of overloading the model with irrelevant information. Specifically, a larger 'L' introduces many noise nodes, decreasing the quality of the extracted knowledge. On the other hand, a smaller 'L' may not capture sufficient knowledge to support the model's reasoning. In our experiments, we found when 'L=3', an average of over 400 nodes were included in the subgraph. This number is sufficient to capture the relevant knowledge for building element-graph, and effectively narrow down the knowledge search space.\n\nWe plan to further elaborate on this in the future revised version.\n\n\n\n**3. (W3) Significance of the Interpreting Component:**\n\nThe interpreting component is designed to reflect the reasoning process of the LM. It is the model's internal representation (from GAT) that contributes to the final prediction. \nThis component is important in transforming the reasoning of the model into an interpretable and human-understandable format. Our approach to generating explanations is deeply rooted in the interpreting component. It uses the model's predicted answer as a basis and leverages the interpreting component to trace back the reasoning process. \n\n**1. (W4) Human Evaluations of Explanation Accuracy:**\n \nWe understand the significance of such evaluations in validating the accuracy of our explanations. To address this, we have conducted a human study using a crowdsourcing platform. We will discuss this part in our revised version. \nBelow are the key details and findings of our study:\n- **Participants Statistics:** Our human evaluations were conducted on the Prolific (https://www.prolific.co) platform. We had 50 participants who were native English speakers, with a minimum education level of high school. The participant group was balanced in terms of gender, and 54% of them held an undergraduate degree or higher. This diverse participant pool ensures that our findings are broadly representative.\n- **Evaluation Process and Metrics:** Participants were provided with detailed instructions and examples to maintain consistent rating standards. They evaluated 20 randomly selected QA from our test dataset. Each QA has our generated explanations. Our evaluation metrics included overall quality, understandability, trustworthiness, detail sufficiency, completeness, and accuracy. Each explanation was accompanied by evaluation questions, with responses gathered using a three-point (1-3) Likert scale (higher is better).\n- **Results:** The results of our human evaluation are as follows:\n\n    | Evaluation Metric      | Score    |\n    |------------------------|----------|\n    | Overall Quality        | 0.845    |\n    | Understandability      | 0.89     |\n    | Trustworthiness        | 0.856    |\n    | Sufficiency of Detail  | 0.831    |\n    | Completeness           | 0.811    |\n    | Accuracy               | 0.845    |\n    \n    These scores was normalized to the range [0, 1], indicating a high level of effectiveness and accuracy in the explanations generated by our approach.\n\n- **Insights:** The majority of our participants (90%) have used AI, and 92% are under the age of 35. This demographics aligns well with real-world using scenarios, suggesting that our human-centric explanations have significant potential for future human-centered applications.\n- **Efforts to Minimize Bias:** While completely eliminating bias is challenging, we made concerted efforts to minimize it. We followed the methodology outlined in Hoffman et al. (2018) [1] for cross-human alignment and evaluation questions design. \n\n---\n[1] Hoffman, Robert R., et al. \"Metrics for explainable AI: Challenges and prospects.\" arXiv preprint arXiv:1812.04608 (2018).\n\n\n**It's not over yet, please check our next comment.**"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9174/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700489860128,
                "cdate": 1700489860128,
                "tmdate": 1700489860128,
                "mdate": 1700489860128,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "IHiH7hWI8P",
                "forum": "VRJzlm2ecv",
                "replyto": "FRx0sNqThz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9174/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9174/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Response to Questions:**\n\n**1. (Q1) Equation in Section 3.2:**\n\nThe MLP in Section 3.2 is a learned part of the pre-trained LM. This design employs the probability computation function of the pre-trained LM to calculate the score. To reduce the misunderstanding, we have revised equation (2), and added a clear description in the updated version. \nPlease review the latest version. The recently modified sections are highlighted in blue for easy identification.\n\n**2. (Q2) Our Advantage of Knowledge Integration:**\n\n1. LMExplainer addresses the challenge of integrating large-scale KGs with LMs. We utilize GATs to efficiently obtain the key reasoning elements. The GATs are trained with the LM, which learns the reasoning patterns of the LM. \n\n2. Central to our approach is a transparent surrogate model, deeply rooted in the KG. This model reflects the reasoning process of LMs, offering a clear window into their decision-making mechanisms and increasing interpretability.\n\n3. Furthermore, the integrated knowledge from KGs constrains and guides the reasoning process. This enables us to map the complex reasoning of LMs onto a graph structure, which we then translate into explanations that are easily understandable by humans. Our method transforms intricate decision-making processes into clear, comprehensible explanations, making the inner reasoning of LMs more accessible than ever before.\n\n**3. (Q3) Addressing Hallucination Issues:**\n\nHallucination is a common problem and big topic in LLMs. We are not aiming to remove all the hallucinations but using strong constraints to narrow down the knowledge search space of LLMs, to maintain the trustworthy. The use of predefined structures in prompting is designed to guide the model towards generating more relevant and focused explanations, thereby reducing the hallucinated content. \n\nWe have also conducted a human evaluation to verify the quality of the explanations (see W4). The results confirm the high quality and reliability of the explanations generated by LMExplainer. Example feedback included comments such as: \"In comparison to prior explanations, these explanations provide a more intuitive understanding of the model's decision-making process. The explanations are cogent, and even in instances of erroneous predictions, the underlying reasoning remains transparent and comprehensible.\" \n\n---\nWe are grateful for the opportunity to enhance our manuscript based on your feedback. We hope that our response and revised version will address your concerns and improve the quality of our work. We look forward to hearing from you."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9174/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700490468162,
                "cdate": 1700490468162,
                "tmdate": 1700494939707,
                "mdate": 1700494939707,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uYKxMR7Mok",
                "forum": "VRJzlm2ecv",
                "replyto": "FRx0sNqThz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9174/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9174/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Look Forward to Your Response"
                    },
                    "comment": {
                        "value": "Dear Reviewer,\n\nSince we are on the last day of author-reviewer discussions, we keenly await your feedback on our rebuttal and the paper modifications to address your comments. We believe we have addressed all the concerns raised by you. If there are any outstanding concerns, please let us know.\nWe look forward to your response and appreciate any feedback.\n\n\\\nThank you,\n\nAuthors"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9174/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700660901409,
                "cdate": 1700660901409,
                "tmdate": 1700660901409,
                "mdate": 1700660901409,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XsAjG8G1bs",
                "forum": "VRJzlm2ecv",
                "replyto": "uYKxMR7Mok",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9174/Reviewer_gatd"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9174/Reviewer_gatd"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the authors responses"
                    },
                    "comment": {
                        "value": "Dear Authors,\n\nThank you for your detailed response to our comments and your efforts in addressing the concerns identified during the initial review. It is acknowledged that your responses have resolved some of my initial queries. The additional experimental work conducted to validate the accuracy of the explanations has strengthened the paper to some extent.\n\nHowever, the question of how the interpreting component has an impact on the accuracy of the prediction has not been addressed carefully yet: After the model predicts answers, the interpreting component uses them to generate explanations, but how do the generated explanations influence the accuracy? Besides, though the authors provide some discussions on the advantages of the knowledge integration method, I still don't feel that the proposed GAT-based training approach is novel enough.\n\nIn summary, I will keep my evaluation."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9174/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700671299422,
                "cdate": 1700671299422,
                "tmdate": 1700671299422,
                "mdate": 1700671299422,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1S8CjHw2px",
                "forum": "VRJzlm2ecv",
                "replyto": "FRx0sNqThz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9174/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9174/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer,\n\nThank you for acknowledging our efforts in addressing the initial concerns and for your continued engagement with our work. We appreciate your feedback and would like to provide additional clarification on the points you raised.\n\nThe interpreting component in our model plays an important role in enhancing the transparency and understandability of the predictions made by the LMs. We demonstrated and analyzed how the interpreting component influences the performance in our ablation study. Please note that our generated explanations will NOT influence the performance of the LMs. The explanation is used to help human understand the reasoning process of the LMs. The LMs will not be trained on the explanations.\n\nWe understand your concerns regarding the novelty of our GAT-based training approach. To further emphasize our noval aspects, we would like to highlight the following points:\n1. We utilize GATs to provide a more transparent and interpretable decision-making process. Our main contribution is not proposing a new GAT architecture to help LM training. Instead, we leverage the KG and build a GAT-based surrogate to demystify the reasoning process of LMs. However, when we investigate the performance with our designed GAT fine-tuning approach, we found that it can also improve the performance of the LM. This is an interesting finding and we believe that it is worth mentioning in our paper.\n   \n2. We want to emphasize that our novelty lies in **(1) explaining how LMs work by generating explanations on a more transparent surrogate, and (2) providing human-understandable and faithful explanations for LMs' decision-making process.** \nHere is an example in XAI:\n   \n    ``\n    Q: What is someone doing if he or she is sitting quietly and his or her eyes are moving? \n    ``\n\n    ``\n    A. reading B. meditate C. fall asleep D. bunk E. think\n    ``\n\n    ```\n    Explanation of [Path-Reasoner [1]]:\n    \n    quietly [related to] quiet [at location] a library [used for] reading eyes [used for] reading,\n    eyes [form of] eye [related to] glasses [used for] reading,\n    sitting [related to] sit [related to] relaxing [has subevent] reading\n    ```\n    Our results:\n    ```\n    Explanation of [LMExplainer] (ours):\n\n    Ranked Reason-elements:\n    1. quiet chattering mind, 2. not making sound, 3. mind focuses, 4. glasses for people with poor eyesight, 5. war\n\n    Explanation (why-choose):\n    Since the person is described as sitting quietly and their eyes are moving, it is likely that they are engaged in a visual activity.\n    Based on the keyword \u201cglasses for people with poor eyesight\u201d, option \u201cA. reading\u201d is the most likely answer, as reading is a common visual activity that requires focusing one\u2019s eyes on a page and is often aided by glasses for people with poor eyesight.\n\n    Explanation (why-not-choose): \n    The other options , such as \u201cB. meditate\u201d or \u201cC. fall asleep\u201d, involve closing one\u2019s eyes or having a still mind,\n   so it is unlikely that the person is doing either of those activities if their eyes are moving.\n    Similarly, \u201cD. bunk\u201d and \u201cE. think\u201d do not seem to be related to the visual activity of\n    having one\u2019s eyes move while sitting quietly.\n    ```\n\n    Although Path-Reasoner is a very advanced approach in this area, it is still difficult for human to understand the reasoning process of the LM. And most works in this area did the same things. \n\n\n    To the best of our knowledge, LMExplainer first work capable of leveraging graph-based knowledge in generating natural language explanations on the rationale behind LM behaviors.\n\nWe hope these additional explanations address your concerns. We are open to any further suggestions or queries you may have. \n\n\\\nThank you,\n\nAuthors\n\n---\n[1] Xunlin Zhan, Yinya Huang, Xiao Dong, Qingxing Cao, and Xiaodan Liang. Pathreasoner: Explainable reasoning paths for commonsense question answering. Knowledge-Based Systems, 235: 107612, 2022a."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9174/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700687391564,
                "cdate": 1700687391564,
                "tmdate": 1700689086006,
                "mdate": 1700689086006,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]