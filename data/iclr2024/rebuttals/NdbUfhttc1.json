[
    {
        "title": "Learning to Optimize for Reinforcement Learning"
    },
    {
        "review": {
            "id": "KUMoyBiF0r",
            "forum": "NdbUfhttc1",
            "replyto": "NdbUfhttc1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4847/Reviewer_wz35"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4847/Reviewer_wz35"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a method for meta-learned optimization in RL, named Optim4RL. This consists of three components - pipeline training, gradient transformations, and an update formulation - which are designed to tackle specific issues in this setting. The evaluation investigates the performance of this method when meta-trained on simple grid-world environments and evaluated on much more challenging environments, primarily evaluating against Adam and RMSProp, in addition to a collection of alternative meta-learned optimizers on in-distribution tasks.\n\nI am recommending rejection for this paper, primarily due to the lack of ablations. However, I would be very willing to increase my score if the suggested ablations were performed, in addition to a discussion of the wider literature for this problem setting."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Related work extensively covers RL optimization and meta-optimization literature.\n2. The paper is impressively well-written and structured. Section 4 is particularly well-structured, presenting a clear set of hypotheses about the problems with meta-optimization in RL.\n3. Each proposed component is simple, but clearly motivated and presented in Section 5 and Figure 3.\n4. Many related methods are compared against Optim4RL in Figure 4, however, these could be evaluated further (see weaknesses)."
                },
                "weaknesses": {
                    "value": "1. The predominant flaw with this paper is the evaluation of the proposed components. Section 5 is highly systematic in motivating each problem, before proposing a component as a solution. However, the evaluation does not ablate the components, making it impossible to discern their individual impact. The exception to this is LinearOptim, which is an ablation of the proposed inductive bias, however, I believe this should be highlighted. Whilst the comparison to existing baselines is interesting, this is a __fundamental__ requirement when evaluating a model composed of multiple novel components.\n2. The presentation of the results could be clearer for drawing conclusions. Whilst training curves are useful, they make it difficult to quantitatively determine significance.\n3. The wide range of baselines in Figure 4 is good, but it is unclear why these are not carried forward for the remainder of the evaluation. Given that meta-training is the largest computational cost, these shouldn't be out-of-budget to run. In particular, STAR is performative enough that it is plausible it would achieve competitive performance on the remaining tasks, which should be investigated.\n4. The inability of LinearOptim to learn anything is very surprising and should be investigated further to ensure it is not erroneous.\n5. There is a broad base of related work on meta-learned RL objective functions, which is not discussed or compared against. While these are a different class of inductive bias, they are solving the same problem as the class of meta-optimizers discussed here. Notably, the evaluation procedure and environments are from Oh et al. (2020), a learned objective algorithm, but it is not compared against. While it is understandable to not compare Optim4RL against all of these methods, they should at least be discussed as alternative approaches to the same problem in the related work. Namely, EPG (Houthooft et al., 2018), LPG (Oh et al., 2020), MetaGenRL (Kirsch et al., 2020), ML^3 (Bechtle et al., 2021), SymLA (Kirsch et al., 2022), DPO (Lu et al., 2022), GROOVE (Jackson et al., 2023).\n6. Many of the claims are misleading or ambiguous. In conclusion, the claim that Optim4RL is \"the first learned optimizer that can be meta-learned to optimize RL tasks entirely from scratch\" is confusing, since all existing learned optimizers can be and are applied to RL in this paper. If this is intended to claim this is the first meta-learned optimizer designed for RL, then the omission meta-learned objective function literature becomes even more apparent, since there is extensive work solving the same problem for RL."
                },
                "questions": {
                    "value": "1. Major typo in Algorithm 1: the sign and magnitude of your update are both computed from o_1. I assume this is a typo since, if correct, your method would only be capable of outputting large positive updates and small negative updates.\n2. Transformation of the gradient output is a major component of gradient processing, but only the input transformations are discussed in the main body. An expanded form of the output could be presented in the main body.\n3. In Figure 5, you suggest the overlap in the support of the gradients is a predictor of performance. A simple experiment to evaluate this would be retraining the optimizer with rescaled rewards on the grid-world tasks, which would shift the support of the meta-training gradients. If this improved performance, this would significantly strengthen the hypothesis."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4847/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4847/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4847/Reviewer_wz35"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4847/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698598474807,
            "cdate": 1698598474807,
            "tmdate": 1699636468413,
            "mdate": 1699636468413,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ZfXW3YYzvx",
                "forum": "NdbUfhttc1",
                "replyto": "KUMoyBiF0r",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4847/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4847/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer wz35"
                    },
                    "comment": {
                        "value": "Thank you for your detailed feedback, suggestions, and comments. We address your points in the following.\n- **Add more ablation studies.** Please check the general response.\n- **In Figure 5, you suggest the overlap in the support of the gradients is a predictor of performance. A simple experiment to evaluate this would be retraining the optimizer with rescaled rewards on the grid-world tasks, which would shift the support of the meta-training gradients.** Yes, this is exactly what we did. We designed six gridworlds with different reward scales such that the union of all agent-gradients covers a large range. With carefully rescaled rewards, we can significantly improve the performance of the learned optimizer trained in gridworlds for Brax tasks. In fact, simply rescaling the agent-gradients before feeding them into the optimizer also works.\n- **The inability of LinearOptim to learn anything is very surprising and should be investigated further to ensure it is not erroneous.** We checked the implementation again and ensured it was not erroneous.\n- **Transformation of the gradient output is a major component of gradient processing. An expanded form of the output should be presented.** We will add more explanations for this part.\n- **Add more citations, modify misleading claims, and correct typos.** We will add those citations and fix the typos and misleading claims."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4847/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700257753143,
                "cdate": 1700257753143,
                "tmdate": 1700257753143,
                "mdate": 1700257753143,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rE7iZKFmAf",
                "forum": "NdbUfhttc1",
                "replyto": "KUMoyBiF0r",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4847/Reviewer_wz35"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4847/Reviewer_wz35"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response and new experiments.\n\n**New ablation studies** - Thank you for including these ablations. Regarding *pipeline training*, the results are somewhat promising but I would like to see them strengthed with the full environment set and labeled significance. Regarding *optimizer structure*, it is not clear from the experimental setup that Figure 4 shows an ablation of this specifically, since Optim4RL is proposed as a combination of methods. This should be clearly described in the experimental setup or Figure 4. Regarding *gradient preprocessing*, it is quite significant for the story of this paper that this method does not aid (and possibly harms) performance. I commend the authors for being candid about these results and the paper still contains useful contributions without them, however incorporating this and the other new results would require a *significant revision*, which is likely out of scope for this rebuttal period.\n\n**Gradient distribution experiments** - Thank you for pointing this out. Even though I agree with the motivation for this experiment, it is not clearly labeled in the paper (see fig 5, 6 captions and section 6.2 title) and incomplete without replotting the agent-gradient distribution in figure 6. Furthermore, adding training environments (rather than just rescaling the reward magnitude on the original environment) adds a confounder beyond the gradient distribution. I would strongly recommend merging figures 5 and 6, such that figure 5 a-d contains one more curve for the Optim4RL on extended training set and 5 e-h contains the distribution over the extended training set.\n\n**LinearOptim results** - I was suggesting that the results be extended with further environments, since they are highly surprising and necessitate further investigation. The release of anonymized source code would also be reassuring.\n\n**Remaining edits** - Thank you for being receptive to these suggestions - I would like to see them included in the revision."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4847/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700423217119,
                "cdate": 1700423217119,
                "tmdate": 1700423325275,
                "mdate": 1700423325275,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "MgYmA3Wd7E",
                "forum": "NdbUfhttc1",
                "replyto": "KUMoyBiF0r",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4847/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4847/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Question about related work"
                    },
                    "comment": {
                        "value": "Dear Reviewer,\n\nWe did not find the mentioned related work GROOVE (Jackson et al., 2023). Could you please provide more details about this work?\n\nThanks."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4847/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700596098575,
                "cdate": 1700596098575,
                "tmdate": 1700596098575,
                "mdate": 1700596098575,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xVQanEsI3S",
                "forum": "NdbUfhttc1",
                "replyto": "L94elcegeP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4847/Reviewer_wz35"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4847/Reviewer_wz35"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response.\n\nI am currently maintaining my score, but I am inclined to raise it after reviewing the new revision. I believe the paper will be a strong submission once the promised edits and new results are implemented, but I do not believe it is in an acceptable state prior to this.\n\nThis is the paper I was referring to: Jackson, Matthew Thomas, et al. \"Discovering General Reinforcement Learning Algorithms with Adversarial Environment Design.\" Thirty-seventh Conference on Neural Information Processing Systems. 2023."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4847/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700598717066,
                "cdate": 1700598717066,
                "tmdate": 1700598717066,
                "mdate": 1700598717066,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "91MqdU1Gi4",
            "forum": "NdbUfhttc1",
            "replyto": "NdbUfhttc1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4847/Reviewer_8FzQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4847/Reviewer_8FzQ"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a meta-learning procedure for learning optimizers for reinforcement learning called Optim4RL. Their method has the following key components:\n- Pipeline training: Use multiple agents each at different training stages (early/mid/late) to make data distribution (more) stationary. \n- Gradient pre-processing: Transform the gradient so that the input is sensitive to changes of the gradient in (approximately) log space. \n- Inductive bias: Structure the update using a form similar to Adam, providing good inductive bias\n\nIn their experiments Optim4RL is shown to (1) outperform existing learnt optimizers, and (2) generalise to problems outside of the training set."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Each of the parts of the proposed learnt optimizer solve important problems in meta-learning optimizers, and are well motivated. \n- The toy problem where the optimizer has to learn the identity function is simple and informative. \n- The paper has informative analysis on the gradient distribution (e.g. I like the plots in Figure 5 visualising the train-test marginal distribution of gradients). \n- Achieves generalisation to different tasks (Brax) from simple grid based problems. \n- Generally the paper is clear and easy to follow."
                },
                "weaknesses": {
                    "value": "- The optimizer achieves marginally worse performance than Adam on the tasks that it is meta-trained on. It seems like the learnt optimizer should at least be able to \"overfit\" to the training task to outperform Adam here. \n- On unseen tasks the optimizer is significantly worse than Adam. \n- The training and test tasks are relatively toy problems (the authors do acknowledge this weaknesses)."
                },
                "questions": {
                    "value": "- The below text confused me - why do we need to check that the model has enough capacity to represent the identity function (e.g. a single linear layer can represent the identity function easily)?\nAlso, why do we need an RNN on this problem (is the input not just the current gradient?)? \n\n> To verify that the model has enough expressiveness and capacity to represent an identity function, we further train it with randomly generated data where data size is similar to the data size of the agent-gradients.\n\n- Transforming the gradient passed to the RNN into a richer representation makes sense. Additionally this seems to help a lot in terms of performance so it seems worth digging deeper into. Were other transformations tried - e.g. fourier feature embedding? \n\n- Would it be possible to add the STAR benchmark to Figure 4 (ant), and for Table 5? This would allow us to see how well Optim4RL generalizes relative to another learnt optimizer. \n\n- In figure 4 (b) it seems like STAR is starting to learn a bit. Is it possible that with a bit more hyper-parameter tuning it would match the other optimizers in performance? \n\nI acknowledge that a lot of my questions require more compute, and that this is a very compute heavy task. I do think that these would significantly strengthen the paper - as they would help make the results more decisive."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4847/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698744554403,
            "cdate": 1698744554403,
            "tmdate": 1699636468335,
            "mdate": 1699636468335,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rLdoXb9Sku",
                "forum": "NdbUfhttc1",
                "replyto": "91MqdU1Gi4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4847/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4847/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer 8FzQ"
                    },
                    "comment": {
                        "value": "Thank you for your detailed feedback, suggestions, and comments. We address your points in the following.\n- **Add more ablation studies.** Please check the general response.\n- **Why can\u2019t the learned optimizer \"overfit\" to the training task to outperform Adam?** Note that we choose a very small network to represent the learned optimizer so that we can train it within the computation and memory budget. This also helps reduce the deployment computation burden, i.e. reducing the inference computation of a fixed learned optimizer. However, the disadvantage of using a small network is that it is hard for the learned optimizer to \u201coverfit\u201c a relatively complex task, such as the gridworlds which have image inputs. Instead, we did experiments on a relatively simple task \u2014 Catch form [bsuite](\u200b\u200bhttps://github.com/google-deepmind/bsuite). The results indeed showed that the learned optimizer can \u201coverfit\u201d Catch, converging faster than Adam and RMSProp.\n- **Why do we need to check that the model has enough capacity to represent the identity function (e.g. a single linear layer can represent the identity function easily)? Why do we need an RNN for this problem? Isn\u2019t the input just the current gradient?** We want to verify whether a learned optimizer represented by an RNN model can approximate the simplest gradient optimization method \u2014 SGD with learning rate 1. In this setting, essentially the RNN model is required to approximate an identity function with agent-gradients as inputs. We use RNN models so that historical gradients can be taken into account and are not completely ignored. For example, historical gradients are important to compute the momentum in RMSProp. Theoretically, a neural network can represent an identity function. However, in practice, it might not be true. Please check [this paper](http://arxiv.org/abs/1902.04698) for an example.\n- **In Figure 4 (b) it seems like STAR is starting to learn a bit. Is it possible that with a bit more hyper-parameter tuning it would match the other optimizers in performance?** We tuned STAR with more hyper-parameter options and improved its performance (average return) to 10; however, this is still much worse than the performance of our method (average return>20)."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4847/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700257621181,
                "cdate": 1700257621181,
                "tmdate": 1700257621181,
                "mdate": 1700257621181,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1psfil2HuL",
                "forum": "NdbUfhttc1",
                "replyto": "rLdoXb9Sku",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4847/Reviewer_8FzQ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4847/Reviewer_8FzQ"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the clarification and updated results. I will stick to my original recommendation."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4847/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700502462013,
                "cdate": 1700502462013,
                "tmdate": 1700502462013,
                "mdate": 1700502462013,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "iktcj4xqAi",
            "forum": "NdbUfhttc1",
            "replyto": "NdbUfhttc1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4847/Reviewer_tcjw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4847/Reviewer_tcjw"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a new learned optimizer, Optim4RL, to address the challenges of using learned optimizers in RL.\n\nWhile learned optimization has shown benefits in the supervised learning community, SOTA optimizers for Supervised Learning (SL) fail in the RL setting. The authors investigate this phenomenon by analyzing the distribution of the gradients of agent parameters at the start, middle, and end of training. Through this analysis, the authors demonstrate that the gradients are non-I.I.D. Moreover, the absolute values of the gradients lie in a small range. The authors then demonstrate the difficulty of the RNN module -- commonly used in the learned optimizer -- to approximate an identity function using the gradient data (75% accuracy). Using this analysis, they underscore the bias and variance in the gradients as a key issue that makes learned optimization hard in RL and argue that this is further exacerbated by the bi-level optimization in learned optimizers. (poor optimizer -- poor policy -- lower quality data)\n\nThe authors then propose three key ways to mitigate these issues: \n- Gradient Processing: a 1-1- mapping that uses a log transformation to magnify absolute value differences between small gradients to mitigate the logarithmic gradient variation. This boosts the accuracy of the RNN on the identity task from 75% to 90%\n- Pipeline Training: add diversity to the gradient inputs to the learned optimizer through a distributed training regime by parallelly training multiple agents being reset at different periods and using all of their gradients for the learned optimizer. This mitigates the non-iid nature of the data since data now comes from different points of training\n- Biasing the optimizer: Building on the analysis of [Harrison et al., 2022], they utilize both the gradient and its squared value as inputs to two RNNs. This mitigates the need to approximate square roots and division by the learned optimizer and stabilizes the meta-update\n\nThe combination of these three components -- Optim4RL -- demonstrates improved stability and effectiveness in optimizing RL tasks compared to baselines of hand-designed optimizers (Adam and RMSProp), learned optimizers for Supervised learning (L2LGD$^2$, STAR, and VeLO), and linear parameter update instead of a squared\n\n[Harrison et al., 2022] Harrison, J., Metz, L., & Sohl-Dickstein, J. (2022). A closer look at learned optimization: Stability, robustness, and inductive biases. Advances in Neural Information Processing Systems, 35, 3758-3773."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "### Originality\nThe paper tackles a novel direction of RL-specific learned optimization by looking deeper into what kind of RL-specific inputs need to be adapted.\n\n### Quality\nThe work is insightful and generally conducted comprehensively.\n\n### Clarity\nThe paper is written clearly and understandably. Overall, the presentation is clear and well done.\n\n\n### Significance\nThe research direction is significant since learned optimization is yet to take hold in RL properly and is very important if achieved."
                },
                "weaknesses": {
                    "value": "There seem to be a lot of central design decisions/hyperparameters in the training procedure that are not justified:\n- 4 inner updated per outer update\n- The decision to average returns over ten runs\n- The threshold for gradient processing\n- Epsilon in the parameter update\n\nThe agglomerative procedure to incorporate diversity in the gradient distribution seems not fully ablated. See my questions on this for further details.\n\nI am unsure if 10 GPU years is a realistically feasible budget for most practitioners. One of the issues with learned optimization in SL has been this exact problem. I think commentary on how to bring this cost down would be highly beneficial for hte community, especially given the recent surge in JAX-based parallelization with developments such as PureJAXRL (https://github.com/luchris429/purejaxrl)."
                },
                "questions": {
                    "value": "- What happens when we don't do individual pre-processing steps?  -- Are there any ablations that demonstrate the effectiveness of individual modifications?\n- What constitutes the middle of training? is it the same for each environment or different across environments?\n- How many seeds were the experiments reported on? How did the authors determine them?\n- Resetting provides the optimizer data at different training stages. Have the authors analyzed how different values of m and n impact this? Do we require them to be equal all the time? \n- Given that pipeline training can be computationally expensive, have the authors examined methods to extract maximum benefit from this procedure? For example, could reset times be adapted by leveraging optimizer reset properties? [Asadi et al., 2023]\n- Does the learned optimizer mitigate the requirement for dynamic hyperparameter optimization [Mohan et al., 2023]? To what extent is the problem addressed in this work related to the AutoRL problem, given that there are still optimizer-related hyperparameters? \n\n[Asadi et al., 2023]  Asadi, K., Fakoor, R., & Sabach, S. (2023). Resetting the Optimizer in Deep RL: An Empirical Study. arXiv preprint arXiv:2306.17833.\n\n[Mohan et al, 2023] Mohan, A., Benjamins, C., Wienecke, K., Dockhorn, A., & Lindauer, M. (2023). AutoRL Hyperparameter Landscapes. AutoML Conference 2023 (https://openreview.net/forum?id=Ec09TcV_HKq)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4847/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698764497166,
            "cdate": 1698764497166,
            "tmdate": 1699636468237,
            "mdate": 1699636468237,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1yYobP3sIg",
                "forum": "NdbUfhttc1",
                "replyto": "iktcj4xqAi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4847/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4847/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer tcjw"
                    },
                    "comment": {
                        "value": "Thank you for your detailed feedback, suggestions, and comments. We address your points in the following.\n- **Add more ablation studies.** Please check the general response.\n- **Add more citations and training details.** We will add those citations and more training details to clarify.\n- **Have the authors analyzed how different values of m and n impact this? Could reset times be adapted by leveraging optimizer reset properties? Do we require them to be equal all the time?** m and n are not required to be equal all the time. In our experiments, n is the number of training environments that depends on the task itself; m is affected by the training steps of each task and it has a similar magnitude as n. We leave the study of the performance impact of m and n as future work."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4847/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700257411005,
                "cdate": 1700257411005,
                "tmdate": 1700257411005,
                "mdate": 1700257411005,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Qtz1VQzIqx",
                "forum": "NdbUfhttc1",
                "replyto": "1yYobP3sIg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4847/Reviewer_tcjw"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4847/Reviewer_tcjw"
                ],
                "content": {
                    "title": {
                        "value": "Reply to authors"
                    },
                    "comment": {
                        "value": "Thank you for the response and the clarifications! \n\n- **Ablation studies:** The ablations are extremely helpful in demonstrating the impact of gradient preprocessing and the inductive bias.\n- **Impact of m and n:** Thank you for clarifying that point. Learning optimizers can help ease or alleviate the need for highly task-specific hyperparameter optimization. However, in the experimental setup, the authors introduce two new task-specific hyperparameters that can impact the diversity of data in the shared buffer and the training speed of the optimizer. Therefore, whether these require precise tuning for each task or whether general values such as m=3 and n=3 can work across tasks is relevant for anyone hoping to use this optimizer. A discussion about this point would be highly beneficial in strengthening the usability of this optimizer.\n- **Related Work:** Thank you for including these works. I additionally agree with the literature references pointed out by Reviewer wz35 and would recommend the authors discuss these in the revision."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4847/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700503404982,
                "cdate": 1700503404982,
                "tmdate": 1700503404982,
                "mdate": 1700503404982,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "RXdjbfoEWp",
            "forum": "NdbUfhttc1",
            "replyto": "NdbUfhttc1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4847/Reviewer_ssUw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4847/Reviewer_ssUw"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose meta-learning an optimizer for RL. They show that RL is uniquely challenging to optimize for. They then propose multiple techniques to learn an optimizer for RL from scratch that can generalize from toy tasks to Brax."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Originality:\n\n- This is a new problem setting that I have not seen before. It is clear that existing learned optimizers do not perform well in RL and it is good to see initial works in this direction.\n\n- The gradient processing is well-motivated and significant.\n\n- The architecture is also well-motivated and elegant.\n\nQuality:\n\n- The authors perform neat investigations into hypotheses about gradient-related challenges in RL.\n\n- The authors show impressive transfer performance.\n\nClarity:\n\n- The paper is very clearly written.\n\nSignificance:\n\n- This could ultimately lead to a superior optimizer for RL, which would be very significant."
                },
                "weaknesses": {
                    "value": "Originality:\n\n- There is a section missing from the related works. In particular, it's the \"learning update rules / algorithms\" for RL literature. The setup seems to be *very closely related to* the setup from \"Discovering Reinforcement Learning Algorithms\" (LPG) [1], which is part of a broader field of meta-learning general RL update rules [2], [3]. \n\n- The pipeline training and the training setup of LPG seem closely related.\n\nClarity:\n\n- (Minor) Section 4.1: SeeAppendix B <= missing a space.\n\n- See clarification-related questions below.\n\nQuality:\n\n- On Section 4.1: The authors show that the agent-gradient distribution is non-IID. However, they do not show that the gradient distribution for normal supervised learning (SL) **is** IID. This is rather important to show, if the authors are claiming that RL is a uniquely challenging setting.\n\n- On Section 4.2: Again, the authors did not compare RL and SL, which is the purpose of this section. The authors could train a SL model and then see if the RNN can learn the identity function on the gradients from that training process. \n\n- The synthetic data is not representative of the SL training process. Furthermore, the injection of non-iid dynamics of the synthetic data seems to have been done ad-hoc and is not particularly meaningful. For example, what if the iid shift was far more extreme?\n\n- On Section 4.3: The problem of non-stationary targets is a well-studied phenomenon in RL, with plenty of possible prior works the authors could cite. This includes the deadly triad [4] and capacity loss [5]. \n\nSignificance:\n\n- The primary technical contributions seem to be the gradient processing, and the optimizer structure. The gradient processing is an impactful trick. The optimizer structure is hardly ablated or compared when there is plenty of literature on learned optimizer architectures.\n\n- The authors show limited transfer and the optimizer does not seem to generally perform on-par with Adam, despite being heavily inductively biased towards an Adam-like update.\n\n\n[1] Oh, Junhyuk, et al. \"Discovering reinforcement learning algorithms.\" Advances in Neural Information Processing Systems 33 (2020): 1060-1070.\n\n[2] Kirsch, Louis, Sjoerd van Steenkiste, and J\u00fcrgen Schmidhuber. \"Improving generalization in meta reinforcement learning using learned objectives.\" arXiv preprint arXiv:1910.04098 (2019).\n\n[3] Lu, Chris, et al. \"Discovered policy optimisation.\" Advances in Neural Information Processing Systems 35 (2022): 16455-16468.\n\n[4] Van Hasselt, Hado, et al. \"Deep reinforcement learning and the deadly triad.\" arXiv preprint arXiv:1812.02648 (2018).\n\n[5] Lyle, Clare, Mark Rowland, and Will Dabney. \"Understanding and preventing capacity loss in reinforcement learning.\" arXiv preprint arXiv:2204.09560 (2022)."
                },
                "questions": {
                    "value": "1. What is the difference between your pipeline training and the pipeline training of LPG?\n\n2. Is pipeline training desirable? Ideally we want a non-myopic optimizer, and the training dynamics early on in training heavily affect the distribution of parameters at the end of training. Can you ablate this?\n\n3. Why is the neural network so small? Is this common in the literature?\n\n4. In Section 6.1: Are you re-training STAR and L2LSGD, or are you taking pre-trained weights?\n\n5. Why are the optimizers and environments different in each plot in Figure 4? (e.g. why is VeLO exclusively for Ant and STAR for big_dense_long). Can you generate a more complete plot here?\n\n6. Many of the learned optimizers use ES to train their learned optimizers. Is there any particular reason you decided not to do this?\n\n7. On Section 4.2: How did the authors choose the hyperparameters (the rate and total amount of change) for generating non-iid data? \n\n8. On \"Quality\" Weaknesses (4.1 and 4.2) from above: These seem easy for the authors to address and I would be very curious about the results!\n\n9. Is there a reason you did not try other architectures from the learned optimizer literature?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4847/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4847/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4847/Reviewer_ssUw"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4847/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698809942054,
            "cdate": 1698809942054,
            "tmdate": 1700586152083,
            "mdate": 1700586152083,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "JU4kRbjeQE",
                "forum": "NdbUfhttc1",
                "replyto": "RXdjbfoEWp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4847/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4847/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer ssUw"
                    },
                    "comment": {
                        "value": "Thank you for your detailed feedback, suggestions, and comments. We address your points in the following.\n- **Add more ablation studies.** Please check the general response.\n- **The authors show that the agent-gradient distribution is non-IID in RL. However, they do not show that the gradient distribution for normal supervised learning (SL) is IID.** We did not claim that non-iid gradient distribution is a unique challenge for learning to optimize for RL. In fact, the gradient distribution in supervised learning is also non-iid. For example, there is a larger portion of zero gradients at the end of the training for both RL and SL. However, we expect the gradient distribution to be more iid in RL tasks since RL tasks are inherently more non-stationary. We will rewrite relevant parts to clarify this.\n- **The difference between pipeline training and the training style of LPG.** The major difference between pipeline training and the training style of LPG is that pipeline training is designed to make the agent-gradient distribution more iid. Pipeline training resets training units in a specific way such that the input agent-gradients are more diverse, spreading across a whole training interval.\n- **Why is the neural network so small?** We choose a small network so that we can train an optimizer within the computation and memory budget. This also helps reduce the deployment computation burden, i.e. reducing the inference computation of a fixed learned optimizer.\n- **Many of the learned optimizers use ES to train their learned optimizers. Is there any particular reason you decided not to do this?** Learned optimizers are mainly trained with the meta-gradient method and evolutionary method. Following previous works, we choose one of the methods to train our learned optimizer, i.e. the meta-gradient method. Using the evolutionary method is an interesting future direction.\n- **Add more citations and correct typos.** We will add those citations as well as fixing the typos.\n- **Are you retraining STAR and L2LGD2?** Yes, we retrain them from scratch."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4847/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700257254383,
                "cdate": 1700257254383,
                "tmdate": 1700257254383,
                "mdate": 1700257254383,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UdXS58OcHi",
                "forum": "NdbUfhttc1",
                "replyto": "JU4kRbjeQE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4847/Reviewer_ssUw"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4847/Reviewer_ssUw"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the reply"
                    },
                    "comment": {
                        "value": "Thank you for the response and clarifying points.\n\n> more ablation studies\n\nThese are very helpful. Thank you for including them. It would be great to see the updated manuscript before the end of the discussion period, if the authors could upload it.\n\n> We did not claim that non-iid gradient distribution is a unique challenge for learning to optimize for RL...the gradient distribution in supervised learning is also non-iid\n\nI completely agree with the fact that the gradient distribution in SL is non-iid (which is why I asked the question). The author's first sentence, that \"they did not claim that non-iid gradient distribution is a unique challenge for learning to optimize for RL\" is odd, given that Section 4 begins with:\n\n\"Learned optimizers for SL are infamously hard to train, suffering from high training instability. Learning an optimizer for RL is even harder. In the following, we identify three issues in learning to optimize for RL.\" \n\nAnd then jump into section 4.1, which is titled \"THE AGENT-GRADIENT DISTRIBUTION IS NON-IID\". \n\nIf the gradient distribution is generally non-iid in supervised learning as well, the point in Section 4.1 seems completely trivial.\nThe authors mentioned in the reply that they would \"expect the gradient distribution to be more iid in RL tasks since RL tasks are inherently more non-stationary\". I *completely agree with this*, which is *why I asked them to run the experiments showing this*. The point of the section should be to show that RL is *more nonstationary* than SL, but instead just shows that it is non-stationary (which is completely trivial and true in simple SL). The same applies to Section 4.2. This is a significant portion and point in the paper.\n\nKeep in mind that these are not difficult experiments to run and would greatly strengthen the claims in the paper."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4847/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700348901250,
                "cdate": 1700348901250,
                "tmdate": 1700348901250,
                "mdate": 1700348901250,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "64ZiMUwcA0",
                "forum": "NdbUfhttc1",
                "replyto": "RXdjbfoEWp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4847/Reviewer_ssUw"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4847/Reviewer_ssUw"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the additional results"
                    },
                    "comment": {
                        "value": "Thank you for the additional results! Using the WD to the total distribution is a neat way to calculate the iid-ness. It would be good to see these plots and histograms. Are you able to update the paper with them?\n\nMinor nits: It would be good to make sure the problems are similarly-scaled. E.g. would it be possible to do something like doing behavioral cloning or contextual bandits on transitions sampled from an expert. This way, you assure that the input spaces match and the loss functions / objectives are effectively the same, but the problem is now iid.\n\n> the non-iid issue in learning to optimize is not explicitly identified in previous works\n\nThis doesn't make it non-trivial. My understanding is that it is so commonly understood that it's not something most people would write in a paper. Adding the comparisons between RL and SL make it non-trivial. It's good to see the authors do this.\n\nI hope that the authors will update the paper so that we can see the latest results (that should hopefully replace Sections 4.1 and 4.2) in more detail as soon as possible. My understanding is that the authors can re-write and update the manuscript during the discussion period."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4847/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700586049448,
                "cdate": 1700586049448,
                "tmdate": 1700586187646,
                "mdate": 1700586187646,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]