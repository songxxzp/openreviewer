[
    {
        "title": "Learning Hierarchical World Models with Adaptive Temporal Abstractions from Discrete Latent Dynamics"
    },
    {
        "review": {
            "id": "J3hURTNOk2",
            "forum": "TjCDNssXKU",
            "replyto": "TjCDNssXKU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5350/Reviewer_yLBk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5350/Reviewer_yLBk"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposed Temporal Hierarchies from Invariant Context Kernels, a method for extracting a hierarchical world model using the proposed Context-specific RSSM (which extends the existing RSSM model (Hafner et al., 2018)) using a coarsely-updating context variable. CRSSM predicts in two levels, selectively updating parts of its latent state (context) sparsely in time. They combine the hierarchical world model (CRSSM) with Dreamer (Hafner et al., 2020) to propose THICK Dreamer, a model-based RL method that combines value estimates from low- and high-level predictions of state and context variables to compute a long-horizon value function. The authors evaluate THICK on MBRL tasks and qualitatively show context switches in their world model."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1) The paper is easy to follow and well-structured with the main contributions listed clearly in the introduction. The proposed world model, Context-specific RSSM, makes a simple yet effective addition to RSSM, and the authors contextualize it well within the existing RSSM model. Contrasting with the existing literature in this area, I find the model visualizations to be very clear and self-explanatory.\n2) The authors show interesting qualitative results using CRSSM, extensive evaluation of their world model when fitted in Dreamer and PlaNet for MBRL, and provide thorough details about their task setups in the appendix adding to the reproducibility of this work."
                },
                "weaknesses": {
                    "value": "1) Looking at figure 7, there does not seem to be a significant difference between the performance of Dreamer and the proposed method. Can the authors justify the marginal performance improvement? \n2) The reported results using Director show close to 0 success on all the tasks. Can the authors explain why was that the case?"
                },
                "questions": {
                    "value": "None"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No concerns."
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5350/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5350/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5350/Reviewer_yLBk"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5350/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698856812866,
            "cdate": 1698856812866,
            "tmdate": 1699636538877,
            "mdate": 1699636538877,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "PYctASww4l",
                "forum": "TjCDNssXKU",
                "replyto": "J3hURTNOk2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5350/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5350/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer yLBk"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their great feedback. \n\n### Performance improvements\n\nSparse reward tasks such as MiniHack pose the challenges of exploration and learning long-horizon behavior. With THICK Dreamer we only tackle the latter through long-horizon reward propagation. This amplifies reward signals once rewards are discovered but does not necessarily help discover rewards earlier. This is reflected in Fig. 7. A stronger performance gain is achieved when the high-level guides the low-level by setting subgoals (THICK PLaNet, Fig. 9). \n\n\n### Director in MiniHack\n\nThank you for this question. To analyze when Director fails in MiniHack, we carefully checked our implementation and analyzed the performance of Director in more detail. This lead us to find a small inconsistency in our Director implementation affecting the MiniHack tasks. We reran all MiniHack experiments. In the simplest problem (KeyRoomFixed-S5), in which Director sometimes managed to solve the task, Director now consistently solves the task. Thank you for making us revisit Director to catch this. \n\nHowever, the problem of Director never learning to solve most MiniHack tasks remains and we identified **two problems** that impede Director\u2019s performance in MiniHack, which we now outline and discuss in Suppl. F.3. \n\nOne problem of Director is that its learning can be slowed down during early stages of training depending on the **diversity of initial data**. Director trains a goal encoder, which defines the input space for the goal-conditioned policy, on its replay buffer. Thus, if in the beginning not enough diverse data is collected this is reflected in the goal space. As a result, the manager (high-level) does not set meaningful goals for the worker (low-level). We exemplify this for variants of the KeyDoor problem in MiniHack in Suppl. F.3 and Fig. 19. By increasing the diversity of collected data, e.g., randomizing spawn positions and increasing level layout, the performance of Director during initial training strongly improves.\n\nA more severe problem of Director is **unobservable information for setting goals**. The RSSM encodes both observable as well as unobservable information within its deterministic latent state $h_t$. If the unobservable information, e.g., item pick-up in MiniHack, does not directly affect rewards or substantially influence image reconstruction, it might be encoded only locally in $h_t$ and wash out over time. In Dreamer, this is not a problem because the policy can amplify task-relevant information in $h_t$. Director, however, compresses $h_t$ into a low-dimensional goal-encoding. Thereby, task-relevant latent information in $h_t$ could get lost. Note that all novel tasks proposed in the Director paper [Ref a] avoid long-horizon memory, for example by coloring walls in a maze (Egocentric Ant Maze) or by providing a visible history of past button presses (Visual Pin Pad). \n\nIn smaller MiniHack tasks, e.g., KeyRoomFixed-S5, this can sometimes be circumvented by specifying goals via observable information. For example, if both the key and door are visible a goal would be the same observation without the key visible (picked up) and an open door. This creates a curriculum, in which the worker can first learn from such simpler situations and later learn to pick up a key and open the door automatically from the high-level goal of an open door. In larger task spaces, e.g. KeyCorridor-8, Director never encounters such simpler situations to begin with.\n\nWe thank the reviewer for their review, and we are happy to answer further questions.\n\n### References\n\n[Ref a] Hafner et al. Deep hierarchical planning from pixels. NeurIPS 2022"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5350/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700434389628,
                "cdate": 1700434389628,
                "tmdate": 1700434389628,
                "mdate": 1700434389628,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "piUpp6xT77",
            "forum": "TjCDNssXKU",
            "replyto": "TjCDNssXKU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5350/Reviewer_h1NE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5350/Reviewer_h1NE"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel way of learning a hierarchical world model. On top of Dreamer's world model implementation, RSSM, this paper introduces Context-specific RSSM (C-RSSM) with a slowly (sparsely) changing discrete context state $\\mathbf{c}_t$, which represents some static scene info that is preserved for a long time horizon. The continuously changing states $\\mathbf{h}_t$ and $\\mathbf{z}_t$ are then conditioned on this context state $\\mathbf{c}_t$. With the trained C-RSSM, a high-level transition is defined as a contiguous transition segment with the same $\\mathbf{c}_t$. The high-level world model (THICK) can be trained to predict the context and stochastic state of the next segment. The paper utilizes C-RSSM and THICK for model-based RL by combining with Dreamer and model-based planning by using MPC. The experiments on MiniHack, PinPad, and robotic manipulation environments show that THICK+Dreamer and THICK+MPC outperform the flat world model baselines when the task horizon becomes longer."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* This paper tackles an important problem of learning a hierarchical world model. \n\n* The idea of learning a sparsely changing context state is intuitive and using context switches to define high-level transitions is a sensible choice. Moreover, this enables THICK to naturally incorporate variable-length skills.\n\n* The learned hierarchical world models (THICK and C-RSSM) can be integrated with both model-based RL and model-based planning.\n\n* The paper is clearly written and the figures help us understand the complex concepts of the proposed method.\n\n* The experimental results support that the hierarchy in the world model improves long-horizon prediction."
                },
                "weaknesses": {
                    "value": "* The design choices of the proposed approach are not explained and justified. Especially, the high-level world model learns to predict the future state and action just before the context switch, rather than the context and state right after the context switch. Predicting a low-level action sometime in the future sounds very difficult to me. Although this design choice also makes sense and works in practice, it would be great to explain the rationale behind this specific design choice. \n\n* Section 2.1 introduces the coarse prior of the stochastic state but it is unclear what is the rationale behind this design. It would be great to further explain why C-RSSM needs both coarse and precise priors. Similarly, more explanation about the need for coarse predictions in Equation 8 would help understand the proposed method.\n\n* Generally, the experimental results of THICK+Dreamer and THICK+PlaNet are similar to those of DreamerV2 and PlaNet. Despite its novelty, these weak experimental results may make its impact less significant. Stronger results in more diverse environments would be greatly appreciated. If possible, it would be great to see its performance on the common RL benchmarks, such as Atari and DMC tasks.\n\n* The paper claims that Thick Dreamer is more sample efficient than DreamerV2 in PinPadFour and PinPadFive but the improvement is relatively marginal to strongly claim this.\n\n* Moreover, the experiments on the PinPad environments use Plan2Explore to fill in initial exploratory data. As RL is inherently a combination of exploration and exploitation, it would be important to see how it works for exploration. Thus, it is recommended to include experiments on RL from scratch.\n\n* Although the investigation of world model hierarchies is important, many deep learning approaches seem in favor of scaling model sizes instead of injecting hierarchies. In this sense, it might be interesting to see comparisons between scaling models [a] vs. C-RSSM.\n\n\n[a] Deng et al. Facing Off World Model Backbones: RNNs, Transformers, and S4. NeurIPS 2023"
                },
                "questions": {
                    "value": "Please address the weaknesses mentioned above.\n\n\n### Minor questions and suggestions\n\n* It might be better if Figure 2 could illustrate that $\\mathbf{c}$ is changing slowly and the high-level transition happens when $\\mathbf{c}$ has changed.\n\n* In Equation 11, $\\delta$ starting from 0 makes more sense?\n\n* In Equation 21, $t < K$ should be $\\tau(t) < K$ or $A_{\\tau(t):K}$ should be $A_{t+1:K}$?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5350/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699197254046,
            "cdate": 1699197254046,
            "tmdate": 1699636538691,
            "mdate": 1699636538691,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "z2oPW7z1Wc",
                "forum": "TjCDNssXKU",
                "replyto": "piUpp6xT77",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5350/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5350/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer h1NE (1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their super helpful comments and suggestions and for thoroughly checking our equations and catching some mistakes.\n\n\n### Design choice 1: Predicting states before context transitions\n\nOur high-level model is a generative model that learns to predict situations in which latent generative factors are assumed to change. Such situations can be related to context transitions [Ref b] or event boundaries [Ref c], as well as to context switches when considering the RL options framework [Ref d]. To predict such situations, two challenges arise: (1.) learning in which situation such transitions are currently possible and (2.) how these transitions affect the latent generative factors. The low-level world model already learns to encode (2). Accordingly, we train the high-level model to only learn (1) and then prompt the low-level model for (2).\n\nWe further emphasize the reasons for this design choice now in Sec 2.2 and explain this in detail in a new \u201cdesign choices\u201d subsection in Suppl. D.1.\n\n### Design choice 2: Coarse predictions\n\nThank you for pointing out that we may have not highlighted this sufficiently. The coarse prediction are important for THICK for two main reasons:\n\n**(1.) Prediction-relevant contexts $c_t$:** Context $c_t$ is designed to encode latent information that is necessary for prediction and reconstruction. If we would omit the coarse prior predictions (Eq. 5) and coarse output reconstructions (Eq. 8) the C-RSSM would have no incentive to encode prediction-relevant latent information in $c_t$, reducing the system to the RSSM. As a result, no hierarchy would develop and hierarchical planning would stay impossible.\n\n\n**(2.) Predictions without $h$:**  The high-level model attempts to predict a future state of the system. The full latent state would contain the deterministic component $h$. However, for the high-level model it would be very challenging to predict the unregularized high-dimensional hidden state $h$ many time steps in the future. Thus, it is advantageous that the C-RSSM can make predictions without $h_t$. After a high-level prediction we can simply feed the high-level outputs ($\\hat z_{\\tau(t) -1}, \\hat a_{\\tau(t) -1}$) into the low-level world model. This allows us to predict rewards or episode terminations/discounts at particular situations (used in THICK Dreamer and THICK PlaNet) or reconstruct images to visualize predictions (see Sec 3.1). \n\nWe emphasize the reasons for this design choice now more clearly in Sec 2.1 and 2.2 and add a paragraph to the Suppl. D.1 to explain these critical design choices in further detail. \n\n### Common RL benchmarks\n\nTHICK world models attempt to tackle partially-observable problems with long task horizons. The problems in the DMC suite have a short task horizon. The problems in Atari are almost (via frame stacking) fully observable. Only a few problems have longer task horizons. Thus, we believe that these suites would not showcase the advantage of using THICK world models particularly well.\n\n### Exploration data\n\nPlease note that in all MiniHack experiments we do not use exploration data. However, with THICK Dreamer we only tackle exploitation through reward propagation and do not use the hierarchy for exploration. This is why we sidestepped the hard challenge of exploration in Visual Pin Pad. \n\nHowever, we now plot the return curves for PinPadThree without exploration data in Suppl. F.5 and Fig. 20a. For PinPadFour and PinPadFive no rewards are discovered within 600k steps for THICK Dreamer and Dreamer without exploration data. \n\nTo paint a full picture,we analyzed the effect of the number of exploration samples on performance by comparing different amounts of exploration samples (250k, 500k, 1000k) in Suppl F.5, Fig. 20 and Tab. 3. The effects of slightly increased sample efficiency for THICK Dreamer in PinPadFour becomes more pronounced when fewer exploration samples are available. Regardless of the amount of exploration data THICK Dreamer always matches or outperforms its flat counterpart.\n\n### Comparison model size vs. hierarchy\n\nThank you for the excellent suggestion. Unfortunately, we could not compare against [Ref a] since their code is not publicly available. But in a new additional experiment (Suppl. F.7 and Fig. 22), we compare Dreamer and PlaNet with different RSSM model sizes against THICK Dreamer and THICK PlaNet in MiniHack-WandOfDeathAdv and Multiworld-Door. We change model size by modifying the number of hidden units per layer and the size of the deterministic hidden state $h_t$ by different factors ($\\in \\{0.5, 1, 2, 4\\}$ for S-, M-, L-, XL-sized models). Increasing model size does not improve Dreamer or PlaNet in these tasks. Our unmodified THICK world models always achieve the best performance."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5350/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700433793413,
                "cdate": 1700433793413,
                "tmdate": 1700433793413,
                "mdate": 1700433793413,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "M5fKjrUyjy",
                "forum": "TjCDNssXKU",
                "replyto": "tknj2q5TJi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5350/Reviewer_h1NE"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5350/Reviewer_h1NE"
                ],
                "content": {
                    "title": {
                        "value": "Thank for author responses"
                    },
                    "comment": {
                        "value": "Thank the authors for the detailed responses and updates in the paper!\n\n**Design choice 1: Predicting states before context transitions**\n\nI understand that reusing the low-level world model could be efficient. However, this is better only when predicting $\\mathbf{z}_{\\tau(t)-1}, \\mathbf{a}_{\\tau(t)-1}$ is easier than directly predicting $\\mathbf{c}_{\\tau(t)}$. I don't quite get why the former is better and the latter is worse. It would be great if the authors could elaborate on this in the paper.\n\n**Common RL benchmarks**\n\nMy suggestion for the common RL benchmarks is not to show the benefit of THICK dreamer on these tasks but to show its generality (i.e. THICK dreamer can work on diverse environments), hopefully not loosing too much performance on them. This can provide a good indicator for readers when to expect performance gains and when not to use this approach, if any."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5350/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700638239661,
                "cdate": 1700638239661,
                "tmdate": 1700638239661,
                "mdate": 1700638239661,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "U8VW26MBub",
            "forum": "TjCDNssXKU",
            "replyto": "TjCDNssXKU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5350/Reviewer_sX3c"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5350/Reviewer_sX3c"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a method called THICK for learning hierarchical world models with adaptive temporal abstractions from discrete latent dynamics. The key idea is to use a context-specific recurrent state space model (C-RSSM) to create a sparsely changing context latent variable. This context encodes higher-level transitions that are used to train a high-level predictor. The high-level model predicts states that lead to context changes, enabling temporal abstract predictions. The hierarchical predictions can be integrated into model-based RL and planning methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The C-RSSM provides a simple yet effective way to learn sparsely changing context variables from pixel observations without supervision.\n\n- The high-level model is trained in a self-supervised manner to anticipate context changes based on the C-RSSM's discrete dynamics.\n\n- The method is generally applicable across various environments with visual observations.\n\n- Experiments show that the learned hierarchies capture meaningful abstractions related to subgoals.\n\n- Integrating THICK's hierarchical predictions improves sample efficiency of model-based RL in long-horizon tasks.\n\n- The high-level plans can be visualized, enhancing model interpretability."
                },
                "weaknesses": {
                    "value": "1) The sparsity hyperparameter for the C-RSSM needs to be tuned for each environment.\n\n2) The high-level model operates at a fixed abstract timescale, less flexible than methods with hierarchical policies. \n\n3) The high-level plans are not actively updated during execution, being replanned only at context changes.\n\n4) The expressiveness of temporal abstractions may be limited compared to methods with backing task priors or curriculum learning.\n\nHowever, I think this paper still take a step further to hierarchical world model."
                },
                "questions": {
                    "value": "1) How does the performance compare to hierarchical RL methods like h-DQN or FeUdal Networks?\n\n2) Could the hierarchy be extended to have multiple abstraction levels instead of just two?\n\n3)  How well does THICK scale to even longer time horizons or higher-dimensional state spaces?\n\n4)  Could active exploration be used to shape useful context abstractions instead of relying on a sparsity loss?\n\n5) Is there some mechanisms could be add to make the high-level plans more reactive to execution errors or environment changes?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5350/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699554954307,
            "cdate": 1699554954307,
            "tmdate": 1699636538607,
            "mdate": 1699636538607,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qXpQ9S29wh",
                "forum": "TjCDNssXKU",
                "replyto": "U8VW26MBub",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5350/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5350/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer sX3c"
                    },
                    "comment": {
                        "value": "Thank you for the valuable feedback and questions. First, we would like to clarify potential misunderstandings before answering questions.\n\n\n### Sparsity loss scale\nWhile it is true that ideally $\\beta^\\mathrm{sparse}$ can be set individually for each task, in practice the same value works well across similar tasks (e.g., same setting per suite, see Suppl B).\n\n\n### Time scale of the high-level\n\nIt is not true that the high level operates on a fixed time scale. This is one important feature of THICK - we apologize for not clarifying this sufficiently well.  As we explain in Sec 2.2, \u201cTHICK uses [...] discrete context dynamics as an **adaptive** timescale for training a high-level world model\u201d. The function $\\tau(t)$ implements a **variable** temporal abstraction that maps a point in time to the next point in time with context changes. Thus, the high-level learns **predictions of variable length**, anticipating context transitions. As we analyze in Suppl. F.1 (e.g., Figure 16) the duration between context changes can widely vary within a task or across tasks. We highlight this now more in the text. \n\n### Reactivity of high-level plans\nIn the paper, we had THICK PlaNet replan on the higher level only when the lower level context changes. This decreases computational effort. In a new ablation (Suppl F.8 & Fig. 27) we show that replanning on the higher level in every step does not drastically change performance in Multiworld-Door. Thus, while THICK PlaNet may run model-based planning on the higher level in every time-step, we recommend running it sparsely or only when context changes to save computational energy. \n\n### Task priors or curriculum learning\nWe unfortunately do not understand this point. Could you please elaborate? We do not see curriculum learning and our learning of temporal abstractions to be exclusive. They could be easily combined.\n\n### Questions\n\n> How does the performance compare to hierarchical RL methods like h-DQN or FeUdal Networks?\n\nh-DQN requires pre-determined abstractions [Ref a], white our method discovers them from scratch. \n\nFUN performs worse or less sample efficiently than DreamerV2 when both methods are applied to the same problems (compare Fig. 4 of [Ref b] with Figure F.1 in [Ref c], or compare Fig. 2 of [Ref b] with Fig. B.2 in [Ref c]). Seeing that THICK Dreamer performs as good or better than DreamerV2, we believe it would outperform FUN. \n\n> Could the hierarchy be extended to have multiple abstraction levels instead of just two?\n\nAs we write in our conclusion, THICK is not restricted to learning a two-level hierarchy. By replacing level 2 with a C-RSSM, in principle, the same mechanism could be applied to learn a level 3 world model etc. In this way an N-level hierarchy of world models with nested time scales could develop. \n\nAnother option would be to foster the development of  multiple coarse pathways on the low level by means of varying sparsity regularization strengths. For each pathway a separate high-level model could be trained. In this approach, the developing hierarchy would not necessarily be strictly nested. \n\n> How well does THICK scale to even longer time horizons or higher-dimensional state spaces?\n\nWe believe THICK should scale to very long time horizons, since it does not depend on the exact time scale of the environment but on the developing time scale of context updates. Similarly, we expect THICK to be also applicable to problems with high-dimensional state spaces. Potentially the dimensionality of $c_t$ would need to be increased to capture all possible context changes in the environment.\n \n> Could active exploration be used to shape useful context abstractions instead of relying on a sparsity loss?\n\nThis is an excellent point. Active exploration might support the shaping of context abstractions. We keep this as future work, also as it would be a complementary direction. \n\n> Is there some mechanisms could be add to make the high-level plans more reactive to execution errors or environment changes?\n\nAs outlined above and shown in our new ablation in Suppl. F.8 and Fig. 27, high-level planning could be active at every time step\n\n\n### References:\n\n[Ref a] Kulkarni et al. Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation. NeurIPS 2016\n\n[Ref b] Vezhnevets et al. FeUdal Networks for Hierarchical Reinforcement Learning. ICML 2017\n\n[Ref c] Hafner et al. Mastering Atari with Discrete World Models. ICLR 2020"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5350/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700433064539,
                "cdate": 1700433064539,
                "tmdate": 1700433064539,
                "mdate": 1700433064539,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]