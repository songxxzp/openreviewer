[
    {
        "title": "De novo Protein Design Using Geometric Vector Field Networks"
    },
    {
        "review": {
            "id": "A7LT3OGgPV",
            "forum": "9UIGyJJpay",
            "replyto": "9UIGyJJpay",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission626/Reviewer_AKqf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission626/Reviewer_AKqf"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a frame-based architecture for operating on protein structure. The key idea is to parameterize a set of virtual atoms within each residue frame. For each pair of interacting frames, the coordinates of both sets of virtual atoms in the destination frame are used to compute attention weights and message values. The aggregated message is then used to update the node features and virtual atom coordinates. In experiments, the authors swap out the architectures of PiFold and FrameDiff and achieve significantly improved performance under identical experimental settings."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The paper establishes a new entry in the design space of residue frame-based architectures, an exciting direction for protein representation learning.\n* The experimental results are quite strong and establish that VFN could be used as a drop-in replacement for alternative SOTA architectures.\n* The non-exchangeable treatment of virtual atoms (unlike IPA) leaves open the possibility of using the framework for real sidechain atoms."
                },
                "weaknesses": {
                    "value": "* The thesis of the paper would be improved by better contextualization relative to IPA. The authors should not shy away from acknowledging significant similarities, but highlight the key changes and the insights behind them. I would suggest a side-by-side algorithmic comparison.\n* The paper could be further strengthened by additional comparisons with IPA. Particularly, if we replace IPA in AlphaFold/ESMFold with VFN, does the performance persist? It should not be too hard to run this experiment since the structural module is not very large.\n* The claims about a \"universal encoder\" are not well-supported. It would be nice to see actual experiments where sidechains are also involved.\n* From the novelty standpoint, an argument can be made that the architecture is similar enough to IPA and / or PiFold to count against its technical significance.\n\nJustification for score: I think this is a good paper and am happy to recommend acceptance if the Questions are fully addressed."
                },
                "questions": {
                    "value": "* Please describe more details on how the VFN-IF+ training split is constructed.\n* Table 5 shows results for FrameDiff+ProteinMPNN vs VFN-Diff+VFN-IFE. What was the exact setting for reporting these numbers reported here, relative to Table 4? Are there equivalent results for FrameDiff+VFN-IF or VFN-Diff+VFN-IF? Please show these for all experimental settings in Table 4.\n* In Figure 1, why does FrameDiff suffer in scTM more on medium-length proteins than the longest proteins?\n* Why was it necessary to retrain FrameDiff, as claimed in the appendix?\n* Please clarify if the PiFold and FrameDiff numbers are taken directly from the respective papers. Please affirm that the results and claims made here are indeed under *identical experimental conditions* relative to PiFold and FrameDiff, or if they have been modified, please be very direct about these modifications."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission626/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission626/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission626/Reviewer_AKqf"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission626/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698170259080,
            "cdate": 1698170259080,
            "tmdate": 1699635990337,
            "mdate": 1699635990337,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6LLu5iQJgs",
                "forum": "9UIGyJJpay",
                "replyto": "A7LT3OGgPV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission626/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission626/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Part.1"
                    },
                    "comment": {
                        "value": "Dear Reviewer:\n\nWe thank the reviewer for the time and effort. The concerns raised by the reviewer are highly professional and meaningful. In the following, we will address each issue in detail and provide the necessary experimental results.\n\n\n\n### 1) The issues regarding the presentation\n\nWe appreciate the reviewer's constructive suggestions. Indeed, our current paper may not be reader-friendly for those unfamiliar with IPA. We will carefully address this issue and make substantial improvements to our writing in the next version. In the **global response**, we presented a comparison between VFN and IPA, and highlighted insights that were overlooked by the reviewer. Please refer to the global response for more details.\n\n### 2) Replacing the IPA with VFN in AlphaFold2\n\nThis is a very meaningful experiment that we have attempted before. However, training AlphaFold2 is resource-intensive, requiring approximately **2 weeks** of training with **128 TPUs**, as demonstrated in the official paper. Moreover, intricate data processing methods are employed during this process. Currently, we do not have sufficient resources to conduct this experiment, even for fine-tuning AlphaFold2. Therefore, we leave this experiment for future work. But thanks for the suggestion.\n\n### 3) The comparative experiments on tasks related to sidechains.\n\nWe think that experiments related to sidechains are also meaningful, so we conducted comparative experiments on the benchmark established within GearNet[1], as shown in the table below:\n\n| |GearNet[1]|VFN|\n|:-|:-:|:-:|\n|Fold3D (Acc) | 52.17 | **55.98** |\n|EC ($f_{max}$) | 76.22 | **77.74** |\n|GO-BP ($f_{max}$) | 44.05 | **44.96** |\n\nThe above experimental settings were meticulously aligned, constituting rigorous ablation experiments. Due to time constraints, the training duration for both methods was set at 2/3 of the official setting. For detailed information on the specific benchmark, please refer to the GearNet paper. One advantage of VFN is that it **eliminates** the need for a **featurizer** to provide **hand-crafted features** such as atomic distances and directions. Instead, it simply requires **treating real atoms as virtual atoms** and feeding them into the vector field operator. This experimental outcome highlights **the potential** of our approach in future tasks related to **sidechain**.\n\n[1] Protein Representation Learning by Geometric Structure Pretraining\n\n### 4) The issues regarding the novelty\n\nWe believe that some insights regarding SE(3) invariance have been overlooked, leading to concerns related to novelty. Therefore, we elaborate on our insights in detail in the global response. We would greatly appreciate it if the reviewer could carefully read our global response to understand our insights. Our novelty and contributions mainly include the following points: 1) insights into **the atom representation bottleneck** in IPA, 2) **a novel method ensuring SE(3)-invariance**, and 3) **the vector field operator** (as depicted in equation 1-4). The VFN employs many **commonly used** operations, such as MLP-based attention (used in PiFold). However, these operations are **not the focus** of this paper, and the use of common operations should not undermine the novelty of our work.\n\n### 5) The question regarding the the VFN-IF+ training split\n\nWe initially **filtered out** proteins that exhibit a high degree of similarity (>**80%**) to sequences in the **validation set** and **test set**. Therefore, our training split does not pose any concerns regarding data leakage. Subsequently, proteins with a resolution greater than 9A were also excluded. Ultimately, our training set consists of 120,934 proteins.\n\n### 6) The question regarding the scTM result on medium-length proteins\n\nWe currently believe that this phenomenon is attributed to **data bias** in the PDB. Since this phenomenon is observed not only in FrameDiff but also in VFN and other published methods, the most likely cause is the bias in the training data. It is noteworthy that this phenomenon is particularly pronounced in scTM (Figure 1.B), while it is not evident in scRMSD (Figure 5). VFN also experiences this issue; however, VFN is more **robust** to data bias, and the problem is not as pronounced.\n\n**`Due to word limit, more information is shown on the next page`**"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission626/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700151280292,
                "cdate": 1700151280292,
                "tmdate": 1700151280292,
                "mdate": 1700151280292,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2IaYGiKCCj",
                "forum": "9UIGyJJpay",
                "replyto": "A7LT3OGgPV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission626/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission626/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Part.2"
                    },
                    "comment": {
                        "value": "### 7) The question regarding the fairness of the comparative experiment.\n\nWe understand the reviewer's concerns regarding the fairness of our experimental setting. However, we would like to clarify that **all comparative experiments in the main text** were conducted with not only **strict alignment** but also **fairness** in the experimental setup, except for VFN-IF+ which utilized a larger training dataset. \n\n### 8) The question regarding the table 5\n\nThe settings for Table 5 are as follows: Noise Scale = 1, Num. step = 500, Num Seq. = 1. We do have results for VFN-Diff+VFN-IF, as shown below:\n\n|Methods|scTM>0.5|RMSD<2|Diversity|\n|:-| :-:|-:|-:|\n|FrameDiff+ProteinMPNN |37.25%|6.88%|38.02%|\n|VFN-Diff+VFN-IF |44.81%|11.48%|42.72%|\n|VFN-Diff+VFN-IFE |**45.43**%|**11.73**%|**43.46**%|\n\n\n**We are conducting experiments** on more settings as per the reviewer's suggestions. However, the runtime for these experiments is exceedingly long. We plan to release these results soon. However, the lack of uniformity in the experimental settings within this field poses challenges for such experiments. The reasons are as follows:\n\n1) The ProteinMPNN in Table 5 utilized a **larger training set**, **additional data augmentation**, and an autoregressive decoder, which is **unfair to VFN-IF**. We are incorporating these tricks employed by ProteinMPNN into VFN-IF to obtain experimental results.\n\n2) The experimental results in Table 5 indicate that, even in an unfair setting with **fewer training data** and **no data augmentation**, VFN can still **outperform** the FrameDiff pipeline.\n\n3) The ProteinMPNN in **Table 1** is reproduced by PiFold, validated by peer review, and **strictly aligned with our setting**. This experiment demonstrates that VFN-IF can significantly outperform Protein MPNN in a fair setting (**54.74%** vs. 45.96%).\n\n4) VFN-IF employs PiFold's one-shot decoder, leading to faster inference speed. However, it is not easy to sample multiple results, so the Num Seq. can only be set to 1.\n\n5) One fact is that, in this experiment, inverse folding is currently **not** the primary bottleneck and does not result in significantly noticeable differences. The performance of this table is determined by the diffusion model. The final results will be very similar with Table 4.\n\n### 9) The question regarding the FrameDiff retraining\n\nFrameDiff is trained on the PDB. On one hand, the data in the PDB may increase daily, and on the other hand, the author of FrameDiff does not release the sample IDs used in their training. **If we do not retrain FrameDiff, it will result in misalignment of the training data.** To avoid this issue, we have retrained FrameDiff to ensure that the training sets for all comparative experiments are consistent.\n\n### 10) The question regarding the identical experimental conditions\n\nOur comparative experiments with FrameDiff are **rigorously aligned without any tricks**. We directly adopted and reported the official results of PiFold, and our comparative experiments are also precisely aligned with PiFold. However, it is important to note that PiFold employs a featurizer, whereas VFN-IF **does not use a featurizer**. This constitutes an advantage for VFN, as PiFold relies on manually extracted distances and directions between atoms, which VFN does not require.\n\n### 11) Emphasizing the Contributions of VFN-IFE\n\nThe challenge of enhancing the performance of inverse folding using ESM remains unresolved. We propose a novel fine-tuning method to address this challenge. Our approach, VFN-IFE, significantly outperforms the ICML **Oral** paper, LM-Design[2] (**62.67%** vs. 55.65%).\n\n[2] Structure-informed Language Models Are Protein Designers. ICML 2023."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission626/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700151304949,
                "cdate": 1700151304949,
                "tmdate": 1700366567726,
                "mdate": 1700366567726,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "k8cYs5Yez0",
                "forum": "9UIGyJJpay",
                "replyto": "A7LT3OGgPV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission626/Reviewer_AKqf"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission626/Reviewer_AKqf"
                ],
                "content": {
                    "comment": {
                        "value": "The authors have addressed some of my concerns; however:\n\n(1) The contextualization relative to IPA should be more substantive and I would prefer to see this as a section in the Appendix at the very least. The authors should not hesitate to say that VFN can be viewed as building on IPA. The current manuscript does not mention IPA anywhere in the Methods section, which I believe is a misleading omission. It would strengthen, rather than weaken the paper to do a side-by-side comparison with IPA, highlighting the differences.\n\n(2) The new sidechain experimental results are poorly contextualized, and from a quick skim of the names of these tasks, it is not obvious to me that sidechain information is essential to these tasks.\n\n(3) The authors have revealed that the VFN-IFE+ training split is constructed with an 80% sequence similarity cutoff, which strikes me as far too high. It is not even clear whether sequence-based splits are appropriate; note that CATH is a structure-based split.\n\n(4) As for Table 5, even if ProteinMPNN is trained on more data, I'd like to see a head-to-head comparison VFN vs ProteinMPNN on a common set of generated backbones. In any case, it is shouldn't be hard to retrain ProteinMPNN if the authors have managed to retrain FrameDiff.\n\nAltogether, the response has not resolved most of my concerns and I will keep the current score."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission626/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700518240082,
                "cdate": 1700518240082,
                "tmdate": 1700518240082,
                "mdate": 1700518240082,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Nqv9ons9GA",
                "forum": "9UIGyJJpay",
                "replyto": "A7LT3OGgPV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission626/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission626/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Part 1"
                    },
                    "comment": {
                        "value": "Dear Reviewer:\n\nWe sincerely appreciate the responsible feedback from the reviewer. At the same time, we apologize for our delayed response, because, over the past week, we have been urgently conducting experiments aligning VFN-IF with ProteinMPNN, as mentioned by the reviewer. We have achieved astonishing results, with **VFN-IF outperforming the official ProteinMPNN by up to 14% in terms of scRMSD!!!** Additionally, we have addressed concerns regarding the comparison between IPA and VFN side by side. Therefore, we kindly request the reviewer to read the materials provided here. *We sincerely appreciate the reviewer's time and effort once again*. For better readability, we have uploaded a PDF as supplementary material. **Please download the supplementary material** (on openreview), which includes clearer experimental tables and a more **vivid side-by-side comparison**. Next, we will provide a detailed explanation for each of the questions you raised.\n\n\n\n### **1) The Comparison between VFN and IPA**\n\nWe agree with the reviewer and have incorporated the **updates to the paper** in **Appendix A.1** as per the reviewer's instructions. To facilitate the reviewer's examination, we have included this section in **Section 2 of the supplementary materials** we provided. **Please download the supplementary materials**, or review section A.1 in the updated paper for a clearer explanation. Here, for the sake of conciseness, we are only presenting crucial information beyond supplementary materials.\n\n1) We vividly illustrate the comparison between IPA and VFN using pseudocode, as shown in Algorithm 1 and 2 in the supplementary material. This comparison is presented **side by side**. Due to the limitations of Markdown, we are unable to provide such vivid comparison here; please refer to our supplementary material.\n2) From the intuitive comparison of pseudocode, VFN and IPA appear to be vastly different.\n3) The reason we use IPA as an analogy for VFN in the paper is to facilitate better understanding for our readers. However, this does not imply a high degree of similarity between VFN and IPA. While VFN and IPA do share the same paradigm (virtual atoms), their specific methods are significantly different, as illustrated in the pseudocode.\n4) We have revised and updated the paper as per the reviewer's instructions. In the next version, we will further enhance our presentation to better compare VFN and IPA.\n\n### **2) Sidechain-related Experiments**\n\nHere, we begin by referencing the introduction to the relevant tasks in GearNet.\n\n> GearNet: Enzyme Commission (**EC**) number prediction seeks to predict the EC numbers of different proteins, which describe their catalysis of biochemical reactions. The EC numbers are selected from the third and fourth levels of the EC tree, forming 538 binary classification tasks. Gene Ontology (**GO**) term prediction aims to predict whether a protein belongs to some GO terms. Fold classification (**Fold3D**) is first proposed in Hou et al. (2018), with the goal to predict the fold class label given a protein.\n\nThose tasks are **full-atom**, and the majority are associated with protein functionality. Therefore, they constitute experiments related to side-chain interactions. In addition, many papers, e.g. [2], have been published based on the same tasks to investigate better representation methods related to side chains. The representation method of VFN is different from theirs, representing a potential future approach that can be integrated with existing methods. We leave this aspect of the research for future work.\n\nThe experimental results are shown below:\n| |GearNet[1]|VFN|\n|:-|:-:|:-:|\n|Fold3D (Acc) | 52.17 | **55.98** |\n|EC ($f_{max}$) | 76.22 | **77.74** |\n|GO-BP ($f_{max}$) | 44.05 | **44.96** |\n\n[1] Protein Representation Learning by Geometric Structure Pretraining\n[2] Learning Hierarchical Protein Representations via Complete 3D Graph Networks\n\n### **3) The issues regarding VFN-IFE+**\n\nThis is indeed a significant issue. Thank you for the valuable suggestions, reviewer. We have removed VFN-IF+ from our Table 1. We apologize for this issue and have removed the relevant results in the updated paper (now available for download). However, it's important to note that the primary results of our work, namely VFN-IF, VFN-Diff, and VFN-IFE, remain unaffected, and this does not impact the core contributions of our paper.\n\n**`Due to word limit, more information is shown on the next page`**"
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission626/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700732608585,
                "cdate": 1700732608585,
                "tmdate": 1700733699899,
                "mdate": 1700733699899,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DrmLvqR6No",
                "forum": "9UIGyJJpay",
                "replyto": "A7LT3OGgPV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission626/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission626/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Part 2: Important Experimental Results"
                    },
                    "comment": {
                        "value": "### **4) The issues regarding VFN-Diff + VFN-IF (`Important`)**\n\nOver the past week, we urgently aligned the settings of VFN-IF and ProteinMPNN and conducted experiments across the entire pipeline. The experimental results demonstrate that **VFN-IF outperforms the official ProteinMPNN by up to 14% in terms of scRMSD!!!** To the best of our knowledge, VFN-IF is **the new SoTA in designability (scRMSD and scTM) surpassing ProteinMPNN.**  Detailed experimental tables are presented more clearly in **Section 1 of the supplementary materials**; **Please download the supplementary material**. The information is repeated here for your convenience. \n\n`Note: In the end of this response, we also present the experimental results comparing RFDiffusion + ProteinMPNN with RFDiffusion + VFN-IF.`\n\nNoise Scale=0.1, Num. step=100, Num Seq.=8\n| |FrameDiff + ProteinMPNN |VFN-Diff + ProteinMPNN|VFN-Diff + VFN-IF|\n|:-|:-:|:-:|:-:|\n|$\\text{scTM}_{0.5}$ | 76.67% | 83.83% | **90.49**% |\n|$\\text{scRMSD}_{2}$ | 26.42% | 40.25% | **51.36**% |\n\nNoise Scale=0.1, Num. step=500, Num Seq.=8\n| |FrameDiff + ProteinMPNN |VFN-Diff + ProteinMPNN|VFN-Diff + VFN-IF|\n|:-|:-:|:-:|:-:|\n|$\\text{scTM}_{0.5}$ | 77.41% | 83.95% | **93.46**% |\n|$\\text{scRMSD}_{2}$ | 28.02% | 44.20% | **`58.27%`** |\n\nNoise Scale=0.5, Num. step=500, Num Seq.=8\n| |FrameDiff + ProteinMPNN |VFN-Diff + ProteinMPNN|VFN-Diff + VFN-IF|\n|:-|:-:|:-:|:-:|\n|$\\text{scTM}_{0.5}$ | 76.42% | 81.23% | **91.60**% |\n|$\\text{scRMSD}_{2}$ | 23.46% | 40.00% | **53.33**% |\n\nNoise Scale=1.0, Num. step=500, Num Seq.=8\n| |FrameDiff + ProteinMPNN |VFN-Diff + ProteinMPNN|VFN-Diff + VFN-IF|\n|:-|:-:|:-:|:-:|\n|$\\text{scTM}_{0.5}$ | 53.58% | 67.04% | **72.84**% |\n|$\\text{scRMSD}_{2}$ | 10.62% | 25.93% | **26.79**% |\n\nDue to the time limit during the discussion period, the program for the setting with num sequence = 100 is still running. This is because under this setting, each sample requires time-consuming ESMFold to predict 100 structures. We will include the results for this setting in the next version. However, **the metrics for num sequence = 8 are the official setting used by ProteinMPNN**, and the above experiments are sufficient to demonstrate the superiority of VFN-IF.\n\n*Diversity and novelty metrics are used to evaluate the diffusion model, not the performance of inverse folding.* For the sake of brevity, we have omitted diversity and novelty metrics from the table. If reviewer is interested, these metrics are detailed in Table 4 and Table 5 in the paper.\n\n### **5) Enhancing RFDiffusion Performance with VFN-IF. (`Important`)**\n\nWe replaced ProteinMPNN in the RFDiffusion (A concurrent study) pipeline with VFN-IF and achieved an also approximately **6% improvement** in designability. The results are as follows:\n\n|Methods|scTM>0.5|RMSD<2|\n|:-| :-:|-:|\n|RFDiffusion+ProteinMPNN |87%|35%|\n|RFDiffusion+VFN|**92**%|**41**%|\n\nDue to time constraints of discussion, the results above are based on the analysis of a protein with a length of 300 residues. However, this is sufficient to demonstrate the efficacy of VFN-IF.\n\n[1] De novo design of protein structure and function with RFdiffusion. Nature 2023.\n\n### **5) The Issues Regarding the Additional Experiments**\n\nWe will incorporate these additional experiments into our paper and release the source code for public use."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission626/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700732841197,
                "cdate": 1700732841197,
                "tmdate": 1700739088615,
                "mdate": 1700739088615,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "NLbx2AB3RB",
            "forum": "9UIGyJJpay",
            "replyto": "9UIGyJJpay",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission626/Reviewer_MXeu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission626/Reviewer_MXeu"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes vector field network to model to better model local frames. Building on this VFN, this paper constructs two sequential models, respectively targeting at designing protein structures and generating protein sequence based on given protein backbone structure.  The paper achieves a new SOTA score on CATH 4.2 on inverse folding task and performs better than FrameDiff on protein structure design."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The proposed model achieves a new SOTA score on the CATH 4.2 benchmark."
                },
                "weaknesses": {
                    "value": "1. **The architecture design lacks some novelty:** It seems for the protein structure design part (VFN-Diff) borrows some ideas from FrameDiff, while for the inverse folding part (VFN-IF), the virtual atom is similar to that of PiFold and the node interaction (Equation 4, 5, 6) is similar to the node gating mechanism in PiFold.\n\n2. **The problem setting is unfair:** In the first paragraph of section 4, the author mentioned \"In the protein diffusion part, the protein structure is designed and represented using backbone frames T. Subsequently, these backbone frames are fed into the inverse folding network to obtain the corresponding protein sequence for the designed structure.\" If I didn't understand wrongly, this means the author first design the protein structure  and then generate protein sequence based on the designed structures.  Therefore, it's kind of like a pipeline. However, in Table 1, for the sequence design task, the author only compared with the inverse folding models, say sequence design based on real structure instead of designed structures. I think a more fair comparison should compare to baseline like structure design model plus inverse folding model, such as FrameDiff + ProteinMPNN, RFDiffusion [1]+ProteinMPNN and also some structure-sequence co-design model like ProtSeed [2]. \n\n[1] De novo design of protein structure and function with RFdiffusion. Nature 2023.\n\n[2] Protein Sequence and Structure Co-Design with Equivariant Translation. ICLR 2023.\n\n3.**Lack of baselines:** The paper lacks some important baselines. For example, [2] for sequence part. For structure design part, the author only compares with FrameDiff, while the current SOTA protein structure design model is RFdiffusion [1]. Also, there is some other protein structure design model like SCMDiff [3], \n\n[3] DIFFUSION PROBABILISTIC MODELING OF PROTEIN BACKBONES IN 3D FOR THE MOTIF-SCAFFOLDING PROBLEM. ICLR 2023.\n\n[4] Generating Novel, Designable, and Diverse Protein Structures by Equivariantly Diffusing Oriented Residue Clouds. ICML 2023.\n\n4. **The writing is unclear:** The author may need to use a unified annotation system, like sometimes {i, j} sometimes k is confusing while they mean the same thing. Additionally, the author may need to provide an overall graph of the architecture to help reader understand this paper."
                },
                "questions": {
                    "value": "1. In figure 2, $T_{i\\rightarrow j}$ means T_j to T_i, while in line below Equation 1, it means T_i to T_j. What does this term really mean?\n\n2. Why he range of H can be negative, say -200 A?\n\n3. $d_q$ is the number of channels in $g_{i,j}$. Does that mean the number of virtual atoms between node i and node j? What is the specific value used in this paper?\n\n4. Are the MLP in equation 7 and 4 are the same one?\n\n5. Using the ESM as initialization and then testing the model on CATH benchmark may have data leakage issues. How did the author deal with this problem?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission626/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission626/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission626/Reviewer_MXeu"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission626/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698711630642,
            "cdate": 1698711630642,
            "tmdate": 1700623010017,
            "mdate": 1700623010017,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1qnqlDgYXy",
                "forum": "9UIGyJJpay",
                "replyto": "NLbx2AB3RB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission626/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission626/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Part.1"
                    },
                    "comment": {
                        "value": "Dear Reviewer:\n\nWe thank the reviewer for the time and effort. The concerns raised by the reviewer are highly professional and meaningful. However, there may be a **misunderstanding on the objective of our paper**, leading to those concerns raised. Please allow us to first provide an explanation of the objective of our paper before addressing each of the reviewer's concerns.\n\n### 1) The objective of our paper is the protein encoder, not the design method.\n\n**VFN does not aim to propose a new protein design method, but rather introduces a novel protein structure encoder to enhance and replace the widely used encoder in protein design, IPA.** Improving IPA is highly meaningful, as acknowledged by the other two reviewers, **unsL** and **AKqf**. At the same time, our fair comparison experiments also demonstrate that VFN can indeed **significantly outperform IPA**. We understand that the concerns raised by the reviewer are *crucial* and *professional* *if our paper were about protein design methods*. However, this is a paper concerning the protein encoder specifically. We would sincerely appreciate it if the reviewer could consider our paper from **the perspective of protein encoder**. \n\n### 2) The issues regarding the lack of novelty\n\nThe VFN employs many **commonly used** operations, such as MLP-based attention (used in PiFold). However, these operations are **not the focus of this paper**, and the use of common operations **should not undermine** the novelty of our work. We have extensively elucidated our novelty and insights in the **global responses**. Please refer to global responses for more details. In short, **the core innovations** in this article revolve around: 1) insights into **the atom representation bottleneck** in IPA, 2) **a novel method ensuring SE(3) invariance**, and 3) **the vector field operator** (as depicted in equation 1-4). Regarding the virtual atoms in PiFold, the crucial point to emphasize is that **there are no virtual atoms within PiFold's GNN**. Its virtual atoms **only** exist in the featurizer, outside the GNN. This is distinctly different from the implicit representation of virtual atoms in VFN and IPA. Please avoid confusing these concepts.\n\n\n### 3)  The issues regarding the inverse folding benchmarks and diverse settings\n\nWe did provide a comparison of the settings (**VFN-Diff+VFN-IFE vs. FrameDiff + ProteinMPNN**), mentioned by the reviewer, in the appendix. Our method exhibits a **11.18% advantage** in designability. The experimental results are presented in the table below, and for details, please refer to **appendix** A.2 as well as Table 5 and Figure 10.\n\n|Methods|scTM>0.5|RMSD<2|Diversity|\n|:-| :-:|-:|-:|\n|FrameDiff+ProteinMPNN |37.25%|6.88%|38.02%|\n|VFN-Diff+VFN-IF |44.81%|11.48%|42.72%|\n|VFN-Diff+VFN-IFE |**45.43%**|**11.73%**|**43.46%**|\n\nIn addition, we would like to emphasize the following:\n1) Once again, **the purpose of VFN is not to propose a new protein design pipeline but rather to introduce a new protein design encoder.**\n2) **The significance of the inverse folding benchmark we adopted.** Inverse folding is a crucial task, and the benchmark we adopted is widely recognized. At least **20** papers have conducted tests using the same or similar benchmarks on **real protein structures**, including **science** paper, ProteinMPNN, and ProtSeed mentioned by the reviewer (VFN-IF significantly outperforms ProtSeed, **54.7% vs. 43.8%**). Moreover, inverse folding can be widely used in various protein design pipelines, **not just protein diffusion**. The outstanding performance of VFN-IF is a **significant objective contribution**.\n3) We partially agree with the reviewer's thoughts on benchmarks based on generated proteins. However, **such benchmarks face some obvious issues**. One fact is that current protein generation models still encounter many issues. Testing based on generated data introduces many **incorrect structures** and **biases** from the **generation model**. Additionally, the random seed of the generation model introduces random factors. Therefore, for inverse folding, such benchmarks may not be rigorous enough. The CATH benchmark adopted by the community may be better, so we follow the CATH benchmark.\n\n**`Due to word limit, more information is shown on the next page`**"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission626/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700151145588,
                "cdate": 1700151145588,
                "tmdate": 1700151145588,
                "mdate": 1700151145588,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pTIZBqODRL",
                "forum": "9UIGyJJpay",
                "replyto": "NLbx2AB3RB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission626/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission626/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Part.2"
                    },
                    "comment": {
                        "value": "### 4) The issues regarding the lack of baselines\n\nThank you for the reviewer's valuable suggestions. In the following, **we present a comparison between VFN-Diff and the methods mentioned by the reviewer**. The experimental results demonstrate the significant **superiority of VFN-Diff**. However, before showcasing the results, we would like to reiterate that the focus of our paper lies in proposing **a new encoder** rather than a designing method. The comparison between VFN and FrameDiff is sufficient to validate the effectiveness of our approach.\n##### 4.1) **Comparison with SCMDiff.**\nVFN-Diff significantly surpasses SCMDiff[1]. The comparative results are presented in the following table, with strict alignment of experimental settings.\n\n|Methods|scTM>0.5|RMSD<2|\n|:-|:-:|-:|\n|VFN-Diff|**67.04%**|**25.93%**|\n|SCMDiff|1.92%|1.92%|\n\n[1] DIFFUSION PROBABILISTIC MODELING OF PROTEIN BACKBONES IN 3D FOR THE MOTIF-SCAFFOLDING PROBLEM. ICLR 2023.  \n\n##### 4.2) **Comparison with Genie.**\nVFN-Diff significantly surpasses Genie[2]. The comparative results are presented in the following table, with strict alignment of experimental settings (the best setting for Genie). Since Genie is **unable** to generate proteins exceeding a length of **256**, the comparison provided below is limited to proteins with a length less than 256.\n\n|Methods|scTM>0.5|RMSD<2|Diversity|\n|:-|:-:|-:|-:|\n|VFN-Diff|**89.38%**|**50.00%**|**77.50%**|\n|Genie|73.75%|25.00%|72.50%|\n\n[2] Generating Novel, Designable, and Diverse Protein Structures by Equivariantly Diffusing Oriented Residue Clouds. ICML 2023.  \n\n##### 4.3) **Comparison with RFDiffusion.** \nWhen both VFN and RFDiffusion do not utilize pretraining weights, VFN can surpass RFDiffusion and achieve SoTA. The comparisons of designability (**median scRMSD, lower values are better**) are presented in the table below:\n\n|Protein Length |70|100|200|300|\n|:-| :-:|:-:|:-:|:-:|\n|RFDiffusion |12.5|14.5|20.2|26.0|\n|VFN-Diff |**0.8**|**0.9**|**2.1**|**2.2**|\n\nIn an unfair setting (RFDiffusion w/ pretraining, VFN w/o pretraining), **VFN stands out as the method closest to RFDiffusion**:\n\n| |RFDiffusion|VFN-Diff|FrameDiff|\n|:-|:-:|:-:|:-:|\n|scRMSD<2 |**100%**|89%|35%|\n\nAs highlighted in the FrameDiff paper, RFDiffusion employs pretraining trick, **more training data and neural network parameters**. Directly comparing with RFDiffusion is unfair. While VFN-Diff could certainly enhance performance using the same approach (e.g. more training data), this is beyond the scope of our focus, and the comparison of RFDiffusion should not be a weakness of our paper. We will add those results into our paper.\n\n**`Due to word limit, more information is shown on the next page`**"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission626/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700151213099,
                "cdate": 1700151213099,
                "tmdate": 1700151213099,
                "mdate": 1700151213099,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fM9yThO3FG",
                "forum": "9UIGyJJpay",
                "replyto": "NLbx2AB3RB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission626/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission626/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Part.3"
                    },
                    "comment": {
                        "value": "### 5)  The issues regarding the writing\n\nWe appreciate the reviewer's constructive feedback. We acknowledge that there are some issues with the writing in our paper. We will heed your suggestions and make improvements to the presentation of our paper in the next version.\n\n### 6)  The question regarding $\\mathbf{T}_{i\\leftarrow j}$\n\nThe reviewer is correct. There is a typo here. We confirm that $\\mathbf{T}_{i\\leftarrow j}$ signifies the transition from $\\mathbf{T}_j$ to $\\mathbf{T}_i$.\n\n### 7)  The question regarding $\\vec{\\mathbf{h}}_k$\n\nAs $\\vec{\\mathbf{h}}_k$ is a vector, it possesses a direction. The positive or negative sign of $\\vec{\\mathbf{h}}_k$ signifies the direction of the vector.\n\n### 8)  The question regarding $d_q$\nSorry, there is a typo here. The number of channels for $\\mathbf{g}_{i,j}$ is $d_g$, not $d_q$; please refer to Eq.3. The reviewer's understanding of $d_q$ is correct, referring to the quantity of atoms. We used 16 atoms in the paper, which is much less than the quantity in IPA.\n\n### 9)  The question regarding Eq.7 and Eq.4\n\nThose MLPs are **not** the same one. The two MLPs do not share parameters. We will emphasize this point in the next version.\n\n### 10)  The question regarding ESM model\n\nThe use of ESM to assist inverse folding[3] has been **accepted by peer review** (**LM-design[3], ICML 2023 Oral**). We adopted exactly the same setting. Therefore, we believe this is **not a flaw** in our paper. While we acknowledge the risk of data leakage, we argue that it should be considered from a different perspective.\n\n1) The objective of inverse folding is to map structures into sequences. Since ESM has not been trained on structures but only on sequences, this does not constitute a data leak in the strict sense.\n\n2) The approach[3][4] of inverse folding based on ESM has demonstrated its efficacy in sequence design. Therefore, from this standpoint, it holds significance.\n\n3) Even from the strictest perspective, LM-Design and VFN-IFE can be considered as retrieval-based sequence design methods. However, in the ESM training set, potentially thousands of sequences exhibit a high degree of similarity to the predictions of VFN-IF. **Retrieving** the correct sequences remains an ongoing challenge, and significant progress has been made by VFN-IFE in addressing this issue (**62.67%** vs. 55.65%, compared to LM-design[3]).\n\n[3] Structure-informed Language Models Are Protein Designers. ICML 2023.  \n[4] Knowledge-Design: Pushing the Limit of Protein Deign via Knowledge Refinement"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission626/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700151255064,
                "cdate": 1700151255064,
                "tmdate": 1700151255064,
                "mdate": 1700151255064,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9laau0r21w",
                "forum": "9UIGyJJpay",
                "replyto": "fM9yThO3FG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission626/Reviewer_MXeu"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission626/Reviewer_MXeu"
                ],
                "content": {
                    "title": {
                        "value": "Reply to the rebuttal"
                    },
                    "comment": {
                        "value": "I appreciate the authors' efforts in providing empirical evidence to demonstrate the effectiveness of their method. I have two follow-up questions:\n\n1. **Goal of this paper:** Can the authors further elaborate the meaning of protein encoder? The authors keep emphasizing their model is a protein encoder in the responses. Did that mean the goal of this paper is to learn better protein representation? Then the evaluation benchmark should not be limited to design tasks only, but also should include some understanding tasks like protein binding affinity prediction. \n\n2. **Data leakage in ESM:** For the data leakage problem of ESM, the author mentioned LM-design accepted by ICML 2023. However, I also discussed this issue with the authors of LM-design before and they said they actually used some data filtering techniques to prevent data leakage issue. The authors said you followed exactly the same setting as LM-design. Can the author explain the data filtering techniques you used in this paper?"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission626/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700241950466,
                "cdate": 1700241950466,
                "tmdate": 1700241950466,
                "mdate": 1700241950466,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vDRUa7reUA",
                "forum": "9UIGyJJpay",
                "replyto": "NLbx2AB3RB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission626/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission626/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Part 1: Response for the goal of this paper"
                    },
                    "comment": {
                        "value": "Dear Reviewer:\n\nThank you for your response and thorough review of our paper. Below, we address your concerns in detail and provide **empirical evidence (experiments)** supporting the effectiveness of VFN.\n\n### **1) VFN's Objective: Residue Frame Representation, Commonly Utilized in Protein Design**\n\n> Reviewer: Can the authors further elaborate the meaning of protein encoder? The authors keep emphasizing their model is a protein encoder in the responses. Did that mean the goal of this paper is to learn better protein representation?\n\nYes, the goal of VFN is to improve **residue frame representation**, a common type of protein representation.  Protein representation encompasses two common forms of representation: **atom-based** representation and **residue frame-based** representation. The distinction of frame representation is that the most of atoms within proteins are typically **unknown**, and each amino acid is treated as a rigid body (a residue frame). It is a fact that the frame representation paradigm[1,2,3,4,5] is **frequently utilized in protein design tasks** because, in protein design tasks, atoms are often unknown, and frame representation is a better choice. Therefore, VFN, as a protein encoder **tailored for frame representation**, conducts its main experiments based on **protein design**, which is a reasonable choice. The significance of frame representation has also been acknowledged by the other two reviewers, **unsL** and **AKqf**.\n\n[1] Protein Sequence and Structure Co-Design with Equivariant Translation. ICLR 2023.  \n[2] De novo design of protein structure and function with RFdiffusion. Nature 2023.  \n[3] SE(3) diffusion model with application to protein backbone generation. ICML 2023.  \n[4] Protein structure generation via folding diffusion.  \n[5] Generating Novel, Designable, and Diverse Protein Structures by Equivariantly Diffusing Oriented Residue Clouds. ICML 2023.\n\n### **2) VFN Targets in Residue Frame Representation \u2013 Distinct From Conventional Protein Understanding Tasks**\n> Reviewer: Then the evaluation benchmark should not be limited to design tasks only, but also should include some understanding tasks like protein binding affinity prediction.\n\nThe protein understanding task [6,7,8] typically employs atom-based representation, **not residue frame representation**. This is because, in protein understanding tasks, atoms are usually known, and there is no need to use a residue frame representation designed for atom-unknown scenarios. VFN, on the other hand, is a GNN designed **for residue frame representation**. Therefore, the protein understanding task is not suitable for evaluating VFN.\n\n[6] Protein representation learning by geometric structure pretraining. ICLR 2023.  \n[7] Unsupervised Protein-Ligand Binding Energy Prediction via Neural Euler's Rotation Equation.  \n[8] EQUIBIND: Geometric Deep Learning for Drug Binding Structure Prediction. ICML 2022.\n\n### **3) Experimental Support for VFN as the Pioneering Frame-Based GNN with Atomic Representation (Protein Understanding) Compatibility**\n\n> Reviewer: Then the evaluation benchmark should not be limited to design tasks only, but also should include some understanding tasks like protein binding affinity prediction.\n\nIf we are not mistaken, VFN is the first frame-based GNN with compatibility for atom-based representation (protein understanding). Experimental evidence (protein understanding) supporting this claim will be presented in the following.\n\n| |GearNet[6]|VFN|\n|:-|:-:|:-:|\n|Fold3D (Acc) | 52.17 | **55.98** |\n|EC ($f_{max}$) | 76.22 | **77.74** |\n|GO-BP ($f_{max}$) | 44.05 | **44.96** |\n\nThe above experiments utilized the benchmark proposed by GearNet[6]. Subsequently, we present an overview within GearNet of these tasks:\n\n> GearNet: Enzyme Commission (**EC**) number prediction seeks to predict the EC numbers of different proteins, which describe their catalysis of biochemical reactions. The EC numbers are selected from the third and fourth levels of the EC tree, forming 538 binary classification tasks. Gene Ontology (**GO**) term prediction aims to predict whether a protein belongs to some GO terms. Fold classification (**Fold3D**) is first proposed in Hou et al. (2018), with the goal to predict the fold class label given a protein.\n\nThe above experimental settings were meticulously aligned, constituting rigorous ablation experiments. Due to time constraints, the training duration for both methods was set at 2/3 of the official setting. \n\nThe experimental results demonstrate that VFN not only exhibits superior performance in frame representation **but also proves compatible with atom-based representation (protein understanding), showcasing promising potential in the domain of protein understanding.**\n\n[6] Protein representation learning by geometric structure pretraining. ICLR 2023.\n\n**`Due to word limit, more information is shown on the next page`**"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission626/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700558051851,
                "cdate": 1700558051851,
                "tmdate": 1700560812893,
                "mdate": 1700560812893,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nhpOSBrFZT",
                "forum": "9UIGyJJpay",
                "replyto": "oYRzIKJ15s",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission626/Reviewer_MXeu"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission626/Reviewer_MXeu"
                ],
                "content": {
                    "title": {
                        "value": "Reply to the rebuttal"
                    },
                    "comment": {
                        "value": "Thanks for the further clarification and experiments! Most of my concerns are addressed by the authors. Therefore, I raised my score from 5 to 6. I think the author needs to explain the motivation and goal of this paper more clearly in the revised version. Besides, the author also should add the necessary experimental results which have been done during rebuttal process into the paper to help reader to better understand this paper."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission626/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700623263939,
                "cdate": 1700623263939,
                "tmdate": 1700623263939,
                "mdate": 1700623263939,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "xTJL6w5eCo",
            "forum": "9UIGyJJpay",
            "replyto": "9UIGyJJpay",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission626/Reviewer_unsL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission626/Reviewer_unsL"
            ],
            "content": {
                "summary": {
                    "value": "The authors introduce a new architecture (VFN) for processing protein structures, which allows to represent residue positions with many virtual atoms. This allows for more fine-grained modeling of residue interactions. The proposed VFN architecture is shown to outperform standard architectures in both protein generation using diffusion models and inverse folding tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "I agree with the authors that there has been an over-reliance on IPA in the literature for protein tasks. It makes a lot of sense to investigate improvements to it, so the paper does target a very important problem in my eyes.\nIntroducing effectively more data channels into the model to increase its capacity is also very sensible. Importantly the model is shown to improve the results on the most common and important protein modeling tasks."
                },
                "weaknesses": {
                    "value": "The general reasoning of why the proposed architecture works better and should be constructed the way it is lies on the concept of atom representation bottleneck. But this bottleneck is not really introduced or investigated in a rigorous manner. Maybe the authors can at least give concrete theoretical counter examples of what problem can be modeled with VFN but not IPA. At least an experimental ablation on varying the virtual node count would be interesting to see how the performance changes.\n\nIt would also be nice if authors could show the theoretical expressivity of their proposed construction. E.g. is there something it cannot model or is it provably universal.\n\nGeneral GNNs have seen some improvements on modeling equivariant structures, e.g. [1, 2, 3] which in some ways have some similarities to the current work (e.g. higher dimensional embedding in frame averaging is a bit like virtual nodes here). It would be nice to see how the proposed VFN architecture stacks up to those general GNN constructions. Although admittedly, those papers usually test their models on molecular and other physics-inspired 3D tasks, but not proteins tackled in this paper. However the extension, especially in case of frame averaging or the multi-channel EGNN, would be trivial. Note that there are also many more improved 3D GNNs, especially in molecule domain that could be applied to this problem. I would like to see the authors test against at least a few of these options, especially frame-averaging as it has been used for proteins a couple of times now [4, 5] and would tackle a similar problems as the proposed model in a very general way.\n\nSpeaking of proteins, in the abstract authors say that only basic encoders such as IPA have been proposed for proteins so far. E.g. [4] applies frame averaging [2] to antibodies, with a quite intersting non-relational architecture for antibody design. While its restricted to a certain protein family it's still worth mentioning that 'less simple' encoders do exist, at least in specific cases.\n \n[1] Du, Weitao, et al. \"A new perspective on building efficient and expressive 3D equivariant graph neural networks.\"\n\n[2] Puny, Omri, et al. \"Frame averaging for invariant and equivariant network design.\" \n\n[3] Levy, Daniel, et al. \"Using Multiple Vector Channels Improves E (n)-Equivariant Graph Neural Networks.\"\n\n[4] Martinkus, Karolis, et al. \"Abdiffuser: Full-atom generation of in-vitro functioning antibodies.\"\n\n[5] Jin, Wengong, et al. \"Unsupervised Protein-Ligand Binding Energy Prediction via Neural Euler's Rotation Equation.\""
                },
                "questions": {
                    "value": "I would mainly like the authors to provide a more detailed theoretical and/or experimental analysis of the introduced atom representation bottleneck, as I mentioned in the weaknesses.\n\n\n-------\n### After Rebuttal\nThank you for the extensive rebuttal. I read through all the reviews and all the answers and I think the work has noticeably improved.\nIt's a bit strange the authors have not updated the paper itself with all the new results, but I trust that they will for the final version.\nI now recommend acceptance."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission626/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission626/Reviewer_unsL",
                        "ICLR.cc/2024/Conference/Submission626/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission626/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698775767699,
            "cdate": 1698775767699,
            "tmdate": 1700781749550,
            "mdate": 1700781749550,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "EW4EeiXs1i",
                "forum": "9UIGyJJpay",
                "replyto": "xTJL6w5eCo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission626/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission626/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Part.1"
                    },
                    "comment": {
                        "value": "Dear Reviewer:\n\nWe thank the reviewer for the time and effort. In the following, we will address each issue in detail and provide the necessary experimental results.\n\n### 1) The issues regarding the concept of atom representation bottleneck.\n\n> Reviewer: 'The general reasoning of why the proposed ...'\n\nIn our **global response**, we rigorously defined the atom representation bottleneck and the underlying insight using formulas. Please refer to the global response for more details. In brief, the way IPA satisfies SE(3) invariance **restricts the use of activation functions**. Subsequently, we will address the specific questions raised by the reviewer. It is important to note that these responses will **utilize the notation established in the global response**.\n\n1) **Theoretical counter example.** According to global responses, in IPA, 1) the **activation function** cannot be applied to the virtual atomic coordinates $\\vec{\\mathbf{q}}_l,\\vec{\\mathbf{k}}_l$, whereas VFN can. 2) The $h \\in \\mathbb{R}$ generated by IPA is a scalar and **cannot represent direction**, whereas the vector $\\vec{\\mathbf{h}}_k \\in \\mathbb{R}^3$ generated by VFN can. 3) In IPA, the operator for calculating $h$ **lacks learnable parameters** and cannot yield specific interatomic features (as illustrated in Figure 2.B of the paper), but VFN can.\n\n2) **Ablation study on varying the virtual node count.** We are conducting ablation experiments on the diffusion model, but this typically takes 2-3 weeks for training. Therefore, we firstly present experiments based on the inverse folding here. Following the reviewer's guidance, we provide the following experiments concerning the virtual node count (the number of $\\vec{\\mathbf{h}}_k$). Upon completion of the experiments on the diffusion model, we will include them in the revised version. \n\n|Num. of $\\vec{\\mathbf{h}}_k$|4|8|16|32(Default)|\n|:-:|:-:|:-:|:-:|:-:|\n|Acc|53.08%|53.75%|54.12%|**54.28**%|\n\n### 2) VFN possesses a unique advantage in theoretical expressivity.\n> Reviewer: 'It would also be nice if authors could show the theoretical expressivity ...'\n\nIn the previous question, we actually addressed the differences between IPA and VFN in this regard, such as the **inability** of $h$ to represent **direction** and the **limitations in using activation functions**. This is already a solid fact. However, beyond this, we would like to emphasize **another advantage** of our approach. Protein modeling currently involves two tasks: **frame modeling** and **atomic modeling**. For frame modeling (diffusion), **PiFold cannot directly be compatible**, and for atomic modeling (inverse folding), **IPA also faces challenges**. In contrast, VFN can handle both tasks, and the reasons are as follows:\n1) PiFold is not designed for frame modeling, very different.\n2) VFN, due to the flexibility of the vector field operator, **can treat real atoms as virtual atoms**, thus allowing the extraction of features between real atoms. However, IPA **cannot** employ such an approach. Because IPA **lacks learnable parameters** when computing features between the atomic coordinates $\\vec{\\mathbf{q}}_l,\\vec{\\mathbf{k}}_l$. Clearly, using distance pooling cannot extract features between specific real atoms, as shown by the equation in the global response:\n$h=\\sum_l \\Vert\\vec{\\mathbf{q}}_l-\\vec{\\mathbf{k}}_l\\Vert^2 \\quad$.\n\n**`Due to word limit, more information is shown on the next page`**"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission626/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700151094716,
                "cdate": 1700151094716,
                "tmdate": 1700151094716,
                "mdate": 1700151094716,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dZZCcdb6u4",
                "forum": "9UIGyJJpay",
                "replyto": "xTJL6w5eCo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission626/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission626/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Part.2"
                    },
                    "comment": {
                        "value": "### 3) The issues regarding other GNN, such as frame averaging.\n\n> Reviewer: 'General GNNs have seen some improvements on modeling equivariant structures ...'\n\nIn this part, we incorporate these methods into our study, conducting experiments to compare and elucidate the distinctions between VFN and these approaches.\n\n\n**Experimental Results**: Once again, due to the very long training time required for the diffusion model, we present the results of inverse folding here, and the results on diffusion will be included in the next version. The following experimental settings are **rigorously aligned**. We implemented frame averaging and multi-channel EGNN very carefully, ensuring that they achieve their intended effects. However, despite these efforts, these methods still lag noticeably behind VFN.\n\n|Methods|VFN-IF|Frame Averaging|multi-channel EGNN|\n|:-:|:-:|:-:|:-:|\n|Acc|**54.70%**|47.33%|45.57%|\n\n**Discussion on frame averaging:** In comparison to frame averaging, the approach employed by VFN to maintain SE(3) invariance is **more concise**, while also introducing an **local inductive bias**. Specifically, each amino acid is treated as a rigid body, inherently possessing the properties of a local frame. VFN directly leverages the properties of local frames, circumventing the complicated virtual frame construction in frame averaging. Additionally, since VFN utilizes the local frame, it introduces a local inductive bias similar to CNN (Convolutional Neural Network). In contrast, frame averaging constructs frames based on global atoms, lacking this particular attribute.\n\n**Discussion on IPA:** In comparison to IPA, the advancement of VFN lies **not only** in higher-dimensional embedding. As mentioned in the global response, the more important factors of VFN are: 1) VFN can utilize **activation functions** 2) VFN introduces **learnable parameters** 3) The $\\vec{\\mathbf{h}}_k$ in VFN can represent **direction**.\n\n**Discussion on other works:** We have carefully examined the related works provided by the reviewer, and we confirm that VFN cannot be simply stacked on these methods. For instance, frame averaging proposes a method to maintain SE(3) invariance, and we have also introduced a method to preserve SE(3) invariance. However, these two methods are not compatible. Additionally, some of these works ([1][2][3]) are contemporaneous with our paper, so we do not delve into detailed discussions on those. However, we will cite these papers in the next version.\n\n[1] A new perspective on building efficient and expressive 3D equivariant graph neural networks.  \n[2] Abdiffuser: Full-atom generation of in-vitro functioning antibodies.  \n[3] Using Multiple Vector Channels Improves E (n)-Equivariant Graph Neural Networks.\n\n### 4) The issues regarding numerous protein structure encoders\n\n> Reviewer: 'Speaking of proteins, in the abstract authors say that only...'\n\nThank you for the thorough review. We will enhance the precision of our expression, as pointed out by the reviewer. It's essential to note that in IPA, each node is a frame represented by the origin coordinates and rotation matrix. In Abdiffuser, however, each node is represented by the atomic coordinates of amino acids. Therefore, the task in Abdiffuser is not entirely identical to the residue frames modeling in IPA. In our abstract, we are referring to IPA's frame modeling. We will enhance our clarity in the next version.\n\n\n### 5) Emphasizing the Contributions of VFN-IFE\n\nThe challenge of enhancing the performance of inverse folding using ESM remains unresolved. We propose a novel fine-tuning method to address this challenge. Our approach, VFN-IFE, significantly outperforms the ICML **Oral** paper, LM-Design[4] (**62.67%** vs. 55.65%).\n\n[4] Structure-informed Language Models Are Protein Designers. ICML 2023."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission626/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700151123991,
                "cdate": 1700151123991,
                "tmdate": 1700151123991,
                "mdate": 1700151123991,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VbbaIzaghL",
                "forum": "9UIGyJJpay",
                "replyto": "xTJL6w5eCo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission626/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission626/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Anticipating your valuable feedback and the opportunity for further constructive discussion."
                    },
                    "comment": {
                        "value": "We sincerely appreciate the time and effort you dedicated to reviewing our paper. In our response (also the global response), we have conscientiously addressed the meaningful concerns and suggestions raised by the reviewer. Please don't hesitate to let us know if you have any further questions."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission626/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700559362319,
                "cdate": 1700559362319,
                "tmdate": 1700559588505,
                "mdate": 1700559588505,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "63oLzy2UU0",
                "forum": "9UIGyJJpay",
                "replyto": "xTJL6w5eCo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission626/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission626/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer:\n\nWe have addressed the concerns raised by the reviewer in our initial response, and we look forward to further discussion with you. However, as the discussion is coming to a close, we can only provide a final update and emphasize some additional information. We hope this helps resolve your concerns. Once again, we appreciate the time and effort you have dedicated to reviewing our work.\n\n### **1) The Comparison between VFN and IPA**\n\n> Reviewer: 'The general reasoning of why the proposed ...'\n\n\nIn our initial response, we have already presented relevant information in the global response. Here, to further clarify the distinctions between VFN and IPA, we have provided a vivid comparison based on pseudocode in the supplementary materials. **Please download the supplementary material** (on openreview) for this comparison.\n\nVFN is a highly innovative approach. In Section 2 of the supplementary materials, we clearly demonstrate the differences between VFN and IPA. The reason we use IPA as an analogy for VFN in the paper is to facilitate better understanding for our readers. However, in reality, IPA and VFN are entirely different in terms of methodology and implementation, as shown in the pseudocode. We hope this clarification can address the concerns raised by the reviewer.\n\n### **2) The Updated Experimental Results of VFN-IF.**\n\nHere, we present a new experimental update. The latest experimental results demonstrate that VFN-IF is **the new SoTA in designability (scRMSD and scTM) surpassing ProteinMPNN**. VFN-IF outperforms the official ProteinMPNN by up to **14% in terms of scRMSD!!!** In Section 1 of the supplementary materials, we have presented relevant experimental results. Please refer to Section 1 of the supplementary materials for clearer experimental tables. We repeat part of findings here.\n\nNoise Scale=0.1, Num. step=500, Num Seq.=8\n| |FrameDiff + ProteinMPNN |VFN-Diff + ProteinMPNN|VFN-Diff + VFN-IF|\n|:-|:-:|:-:|:-:|\n|$\\text{scTM}_{0.5}$ | 77.41% | 83.95% | **93.46**% |\n|$\\text{scRMSD}_{2}$ | 28.02% | 44.20% | **`58.27%`** |\n\nNoise Scale=0.5, Num. step=500, Num Seq.=8\n| |FrameDiff + ProteinMPNN |VFN-Diff + ProteinMPNN|VFN-Diff + VFN-IF|\n|:-|:-:|:-:|:-:|\n|$\\text{scTM}_{0.5}$ | 76.42% | 81.23% | **91.60**% |\n|$\\text{scRMSD}_{2}$ | 23.46% | 40.00% | **53.33**% |\n\nWe hope that the comprehensive information we have provided can address your concerns."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission626/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700734020968,
                "cdate": 1700734020968,
                "tmdate": 1700734020968,
                "mdate": 1700734020968,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]