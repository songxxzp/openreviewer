[
    {
        "title": "Put Your Money Where Your Mouth Is: Evaluating Strategic Planning and Execution of LLM Agents in an Auction Arena"
    },
    {
        "review": {
            "id": "l5TSEWNGG9",
            "forum": "crMMk4I8Wy",
            "replyto": "crMMk4I8Wy",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3788/Reviewer_Yv5K"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3788/Reviewer_Yv5K"
            ],
            "content": {
                "summary": {
                    "value": "This paper present AucArena, an open ascending auction protocol for LLM-based bidding agent. The auction protocol specifies the value of items and communication process. The LLM agent is assumed to follow a certain architecture, which consist of (1) building belief, e.g., the sufficient statistics like remaining budget, profits (2) Desire, the objective that a bidder tries to optimize and (3) planning, the bidder's strategy to fulfill its goal. These components structuralized an LLM agent's decision-making process, and is purely generated by LLM itself. The evaluation studies the correlations between the agents' actual bidding behaviors, and the output of each of these components."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The presentation of the paper is good. The usage of LLMs as bidding agents are novel and promising."
                },
                "weaknesses": {
                    "value": "Some of the settings and experimental results are not clear. Please see the question section."
                },
                "questions": {
                    "value": "I like the setting of using auctions as benchmark to evaluate LLMs. However, there are a few questions I have in mind:\n\n1. About the auction protocol. Section 3.1 wrote the true value of a particular type of item is the same for all players, but the players cannot observe. This is very confusing to me. In a standard auction setting a player will have a private valuation, or some prior distribution of the ground truth value. The distributions could be correlated among different players, but I cannot understand the setup of not having any information of the ground truth values at all. Also, can the bidder observe its own utility every round? If it does then it can infer the ground-truth utility right? Can the author please clarify what is the observation space for a bidder each round.\n\n2. In section 3.3. Also I believe winner's curse happens when the players' beliefs of valuations are positively correlated.\n\n3. In the adaptivity experiment, what is the learning rule mainly about? And why not incorporate this learning rule in previous experiments?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3788/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3788/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3788/Reviewer_Yv5K"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3788/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698184120407,
            "cdate": 1698184120407,
            "tmdate": 1699636335742,
            "mdate": 1699636335742,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "LDMtsYFR5J",
                "forum": "crMMk4I8Wy",
                "replyto": "l5TSEWNGG9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3788/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3788/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your encouraging feedback! Here are the answers to your questions:\n\n> Q1.1: I cannot understand the setup of not having any information of the ground truth values at all. Also, can the bidder observe its own utility every round? If it does then it can infer the ground-truth utility right? Can the author please clarify what is the observation space for a bidder each round.\n\nA1.1: We would like to clarify that the bidders have an estimation of the true value. This is because in the traditional auction scenario, the bidders usually have an estimated value for the item based on their prior knowledge, market research or historical prices. However, this estimated value may not exactly be the item's true value [1], due to overestimation or underestimation of the item value (in our case for example, by a percentage of 10%). We follow this setting to approach real-world cases, so that we can study the risk management skills of LLM agents (e.g., winners' curse), where overbids can lead to apparent victories that are actually strategic losses. We will clarify this detail in the revision.\n\n> Q1.2: Can the bidder observe its own utility every round? If it does then it can infer the ground-truth utility right? Can the author please clarify what is the observation space for a bidder each round.\n\nA1.2: Yes, each bidder can observe its own utility (i.e., profit or items won) every round to infer the ground-truth utility. For each round, the observation space for each bidder includes: the remaining budget of oneself (budget information is not shared), profits so far for everyone (can be calculated based on the announced true value and the winning bid of an item), the items won so far and their winners, and the auction bidding history.\n\n\n> Q2: In section 3.3. Also I believe winner's curse happens when the players' beliefs of valuations are positively correlated.\n\nA2: Yes, it seems players' beliefs of high valuations are more likely to lead to the winner's curse. However, the winner's curse doesn't necessarily happen if bidders are risk-aware and consider certain auction strategies. For example, they might take this risk into consideration to anticipate the winner's curse, and intentionally undervalue an item or limit their maximum bid (e.g., hedging strategy) to avoid paying too much for it.\n\n\n> Q3: In the adaptivity experiment, what is the learning rule mainly about? And why not incorporate this learning rule in previous experiments?\n\nA3: Just to clarify, did you mean the learning module from Sec 4.3 (Figure 6)? This learning module enables agents to learn from the logs of previous auctions, and summarize some textual takeaways to guide future auctions. We didn't incorporate this since it does not bring much improvement to the agent in general, and it requires previous context for the agent to learn from, whereas most of the experiments are conducted in an \"zero-shot\" manner. \n\nThere are some works on non-parametric learning (i.e., no training for the model) for language agent. The results somewhat contradict those found in [2], where a similar non-parametric learning technique was applied to tasks like HotpotQA. We think this could be because the insights gained from such NLP tasks, such as \"don't neglect a negation\" or \"decompose a sentence\" are rather static. However, they might not apply to our auction arena. Unlike the static tasks, our auction environment is dynamic and unpredictable. The past experiences may not be effective in a new context influenced by fresh rounds of conversations and bids.\n\n\n[1] Klemperer P. Auction theory: A guide to the literature[J]. Journal of economic surveys, 1999, 13(3): 227-286.\n\n[2] Zhao, Andrew, et al. \"Expel: Llm agents are experiential learners.\" arXiv preprint arXiv:2308.10144 (2023)."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3788/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700321210728,
                "cdate": 1700321210728,
                "tmdate": 1700321210728,
                "mdate": 1700321210728,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "X3RKfPaaZu",
            "forum": "crMMk4I8Wy",
            "replyto": "crMMk4I8Wy",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3788/Reviewer_TdM7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3788/Reviewer_TdM7"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces AucArena, a simulated auction environment for language model agents, and conducts comprehensive benchmarks among PaLM-2, Claude-1.2, Claude-2, GPT-3.5, and GPT-4.\nThe authors test language model agents based on Belief-Desire-Intention (BDI) Model framework, where the agents are expected to keep the state of auctions (remaining budget, total profit, winning bids of all items) in mind, to follow the given instructions, and to replan the future bids dynamically. While language model agents can follow the given instructions during the auction (budget, priorities, objective, etc.), human players or even rule-based agents still work as competitive baselines."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "### quality and clarity\n- This paper is clearly written and easy to follow.\n\n ### originality and significance\n- The auction might be a unique setting for language model agents.\n- In contrast to Reflexion [1],  tested on ALFWorld (a housing simulator) and HotpotQA (Wikipedia-based QA), the self-improvement loop from the previous experience is not observed (Section 3.3), which might be an interesting finding.\n\n[1] https://arxiv.org/abs/2303.11366"
                },
                "weaknesses": {
                    "value": "- While auction might be a unique setting, it is unclear what aspects of language model agents would be evaluated through this benchmark.\n- (Re)Planning [1,2], Instruction-following [3], and belief (internal state of the agents) [4,5] is a typical design for language model agents. It does not seem as a novel proposal or evaluation of language model agents.\n- The authors mentioned AgentBench in Section 2 as `... are limited in terms of evaluating LLMs in dynamic environments.`, but in fact, they have static offline task planing evaluation (Mind2Web).\n- I'm not sure if the variety of items is enough (\\\\$2000 and \\\\$10000). Some justification would be helpful.\n- The descriptions of AucArena in Section 3.1 `For simplicity, we\ndo not introduce additional values such as various personal preferences and emotional value of items.` and the descriptions of BDI model in Section 3.2 `... yet some might\nalso have non-monetary motivations, such as a personal desire to own a specific item.` seem contradictory.\n\n\n[1] https://arxiv.org/abs/2303.17491\n\n[2] https://arxiv.org/abs/2305.16653\n\n[3] https://arxiv.org/abs/2207.01206\n\n[4] https://arxiv.org/abs/2210.03629\n\n[5] https://arxiv.org/abs/2207.05608"
                },
                "questions": {
                    "value": "- How many participants do you have in ActArena? One LLM-based agent you are focusing on, and others are rule-based agents?\n- While GPT-4 achieves the lowest Corrected Failure Rate in Figure 2, the average profit of Claude-1.2/2 with adaptive planning is higher than GPT-4 in Figure 3 (GPT-3.5 does not seem a bad bidder here). Why does this happen?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3788/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698599526005,
            "cdate": 1698599526005,
            "tmdate": 1699636335651,
            "mdate": 1699636335651,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "46wRRGaIOP",
                "forum": "crMMk4I8Wy",
                "replyto": "X3RKfPaaZu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3788/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3788/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response #1"
                    },
                    "comment": {
                        "value": "Thank you for the valuable comments! Please find our responses to your questions and concerns below.\n\n> Q1: While auction might be a unique setting, it is unclear what aspects of language model agents would be evaluated through this benchmark.\n\nTo emphsize again, our main focus was measuring the strategic planning and execution capabilities of LLM agents in dynamic scenarios, for which the auction setting is well suited. We conducted a detailed breakdown and assessment. A reminder of these different aspects: \n\n- In Sec 4.1, Rationality represents the ability of strategic planning, specifically divided into whether the agent can correctly perceive the environment (measured by CFR) and whether the agent can implement the plan into the correct actions (measured by the correlation between the priority score for a specific item and the number of bids). \n- Sec 4.2's Adaptability is the measure of the agent's execution ability in a \"dynamic\" environment, specifically including the impact of Adaptive Planning and whether resources can be allocated correctly. Additionally, we conducted a competitive analysis in a multi-agent environment, comparing the strengths and weaknesses of different LLMs.\n\n\n> Q2: (Re)Planning [1,2], Instruction-following [3], and belief (internal state of the agents) [4,5] is a typical design for language model agents. It does not seem as a novel proposal or evaluation of language model agents.\n\nWe acknowledge that the fundamental abilities of language agents, such as (re)planning, instruction-following, and belief tracking, are indeed well-established in the literature. However, the primary contribution of our work is not in the exhibit of these basic abilities for language agents per se, but rather about how they are applied strategically and adaptively in a real-time, dynamic setting.\n\nOur research introduces a dynamic auction arena that uniquely challenges these agents in strategic decision-making, adaptability, and execution under competitive and resource-constrained conditions. This environment significantly differs from traditional static benchmarks and provides a more realistic and unpredictable setting, where the strategic application of these foundational abilities is critical. \n\nMoreover, the evaluation of agents in this auction arena offers valuable insights into their strategic capabilities and limitations. It demonstrates how these agents perform in real-time, adapt to rapidly changing scenarios, and make strategic decisions to achieve their objectives. This aspect of our research is particularly relevant, as it sheds light on the current state of agent abilities and highlights areas for future development in agent design to enhance their strategic planning and execution.\n\nIn summary, the novelty of our work lies in the application of established agent abilities in a unique, competitive auction setting, providing new insights into the strategic capabilities and limitations of current language model agents. Our research contributes significantly to the field by moving beyond static evaluations and demonstrating the potential and challenges of applying these agents in dynamic, real-world scenarios.\n\n\n> Q3: The authors mentioned AgentBench in Sec 2 as\u00a0... are limited in terms of evaluating LLMs in dynamic environments., but in fact, they have static offline task planing evaluation (Mind2Web).\n\nIn this context, \"dynamic\" specifically refers to an environment that is constantly changing, shaped by the actions of other agents. This description emphasizes the \"unpredictability\" of the outcomes that agents face after performing an action. That is, the results are determined not only by the agent's own actions but also by the concurrent actions of other agents. In contrast, Mind2Web, while being a complex system, has a relatively predictable environment, as the outcome is constituted by information that websites provide. This differs significantly from the environment in AucArena. In AucArena, due to the uncertainty of other agents' actions, the environment and challenges faced by agents are more dynamic and unpredictable.\n\n\n> Q4: On the variety of bidding items.\n\nThe variety of bidding items is not the primary concern of our simulation. Instead, the choice of items valued at $2000 and $10000 is mainly to make the evaluation more controllable. This setup is intended to mimic two common scenarios encountered in reality: products that are inexpensive with limited subsequent markups/profit, and expensive items that may attract more imaginative premiums.\nThe rationale for setting these two types of items is to prevent the complication of too many confounding factors at play."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3788/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700322152250,
                "cdate": 1700322152250,
                "tmdate": 1700322152250,
                "mdate": 1700322152250,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2FRY3CkmaT",
                "forum": "crMMk4I8Wy",
                "replyto": "X3RKfPaaZu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3788/Reviewer_TdM7"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3788/Reviewer_TdM7"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the detailed response.\n\nMy question on (Q7) the difference between Corrected Failure Rate and average profit becomes clear now. Here are the remaining comments:\n\n**(Q1, Q2, Q3, Q4)** \n\nAfter reading the response, I still concern that it is unclear what aspects of language model agents would be evaluated through this benchmark. For instance, the authors mention that `\"dynamic\" specifically refers to an environment that is constantly changing, shaped by the actions of other agents. This description emphasizes the \"unpredictability\" of the outcomes that agents face after performing an action`. However, other environments solved with LLMs, such as household simulators, web/computer control, MineCraft, robotic navigation, are also equally \"unpredictable\" for LLMs because of their open-endedness (LLMs cannot know all the possible consequence in advance). Also, I wonder if enough diverse situations happen in AucAreana to satisfy the authors arguments. In fact, (1) this simulator only has two items ($2000, $10000), (2) except for multi-agent (i.e. multi LLM) settings, the other participants are rule-based agents, (3) as far as checking Figure 5, different LLMs often achieves the similar TrueSkill Scores. If the possible situations are not diverse, we cannot test the LLM's ability of adaptive strategic thoughts.  If you write tldr on the insights, that might be \"GPT-4 works well and Claude sometimes well\"?  Even compared to other agentized LLM works, I'm not fully sure the novel insights from the paper.\n\n\n\n\n**(Q5, Q4)**\n\nBecause the revision is allowed in this period, the update of paper would be expected. For Question 5, it is quite confusing to mix the proposed methods (based on BDI) and common belief (participants have their own desire)."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3788/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700555998998,
                "cdate": 1700555998998,
                "tmdate": 1700556322901,
                "mdate": 1700556322901,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ZghGi093FV",
            "forum": "crMMk4I8Wy",
            "replyto": "crMMk4I8Wy",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3788/Reviewer_eqke"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3788/Reviewer_eqke"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes AucArena, a simulated auction environment for evaluating the strategic planning and execution abilities of LLMs. The idea of using auctions to assess skills like strategic reasoning, execution, and adaptivity in a multi-agent environment is interesting. With AucArena, the paper also proposes a new LM auction agent: LLMs act as bidders using prompting for actions like planning/ replanning, bidding, and belief updating. Experiments analyze LLM in different tasks in different variations of the environment that test rationality, adaptivity, and competition (with some asymmetry)."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The auction simulation is a creative testbed for evaluating LLMs in a dynamic, competitive setting requiring strategic talents.\n- The experiments thoroughly probe diverse LLM models in the proposed arena. Testing multiple LLMs provides useful insights into their relative proficiencies.\n- Assessing strategic abilities of LLMs is an important open problem and this work makes a valuable first step. The arena concept has promise if further developed."
                },
                "weaknesses": {
                    "value": "- The introduction and problem framing focus heavily on general capabilities, detached from the specific auction setting and contributions. Centering the intro around auctions would improve coherence. \n- I would have loved to see a motivation of specifically why agents competing in auctions should be studied where lying and deception could be emergent. Why are auctions special? And not other cooperative settings? A discussion on this is improtant.\n- The auction design lacks complexity with only basic factors like item values and bidder budgets. Expanding to more realistic auctions could better evaluate strategic skills. Authors provide suggestions in Appendix C, which if included would greatly improve the paper.\n- Prompting details and ablations could provide more insights: Why is the specific prompt design superior to others? Ablations of the prompt with comparisons to other prompts and agents would make the work stronger.\n- No discussion on the advantages of an LLM agent over a specialized strategic agent. The choice of LLMs as the agent model (over a symbolic RL agent for example) needs justification.\n- Missing Citations: Strategic Reasoning with Language Models [1] seems closely related to the method in the paper with planning, replanning, belief tracking and value estimation of other agents. Other works relating to factoring [2], llm cascades [3], need to be cited and discussed.\n- The comparison to human play provides useful context, but the protocol for human evaluation seems ad hoc. Standardizing the process and using skilled human players as a benchmark could make this comparison more rigorous. Increasing the number of human participants could be a good place to start.\n- The method was not immediately clear, parts of the instructions could be included in the main paper to make things clearer.\n- The evaluation metrics seemed narrow, focusing on the outcomes. It would be great if the authors could translate some of the qualitative measures about strategy into objective numbers.\n- Adding an open source model to the benchmark would be interesting!\n\n[1] Gandhi, K., Sadigh, D., & Goodman, N. D. (2023). Strategic Reasoning with Language Models. arXiv preprint arXiv:2305.19165.\n\n[2] A. Stuhlm\u00fcller and J. Reppert and L. Stebbing (2022). Factored Cognition Primer. Retrieved December 6, 2022 from https://primer.ought.org.\n\n[3] Dohan, David, et al. \"Language model cascades.\" arXiv preprint arXiv:2207.10342 (2022)."
                },
                "questions": {
                    "value": "### Suggestions\nSpecified in detail in weaknesses, summarized here:\n- Improve the introduction and motivation, being specific to auctions and using LLMs with auctions. \n- Enrich the auction design with more realism and complexity as outlined in the paper (App C).\n- Ablate prompting methodology, compare to stronger baselines.\n- Increase and formalize human evaluations. Compare to skilled players.\n- Discuss how the approach builds on related work in strategic, factored LLMs."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3788/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3788/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3788/Reviewer_eqke"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3788/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698736089088,
            "cdate": 1698736089088,
            "tmdate": 1699672564245,
            "mdate": 1699672564245,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "uiA3IPDyG0",
                "forum": "crMMk4I8Wy",
                "replyto": "ZghGi093FV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3788/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3788/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response #1"
                    },
                    "comment": {
                        "value": "Thank you for your careful reading of our paper and your valuable suggestions! We will revise the paper as suggested. Please find our responses to your questions below.\n\n> Motivation for Choosing Auctions\n\nThe major idea behind our paper is to activate and evaluate the potential and abilities of agents powered by LLMs for tasks that require strategic planning and resource management, which are general but critical in many scenarios like auctions. In the original manuscript, we discuss these general capabilities of LLMs in the context of dynamic environments. We will revise the introduction to more directly link these capabilities to our specific auction setting, and outline the unique challenges and opportunities presented by auction environments for LLMs.\n\nThe rationale for focusing on auctions, as opposed to other cooperative settings, is grounded in the unique strategic complexities auctions offer. Moreover, the convenience in quantifying its outcomes, such as total profit or number of items won, can give us clear ranks to the competitors in this arena, showing their differences in terms of these abilities. Since our auction arena is quite extensible, in future work, we will add discussion and negotiation sessions to the auction, with a specific focus to analyze deceptive and cooperative abilities of LLM agents.\n\n> Auction Design Complexity\n\nThank you for pointing this out. It is easy to change the setting of our auction arena by setting different parameters. However, we deliberately reduce the complexity of the auction design to facilitate analysis (which is difficult in researches on simulations), otherwise too many moving parts will add unnecessary confounders to the results. One benefit from the auction arena is that we can control the setting of the arena according to the ability we want to test LLM agents. For example, as shown in the paper, we can change the objective that drives an agent, or change the number and price of items to create more demanding and long-term auctions. \n\n> Prompting Methodology Ablations\n\nThank you for the suggestions. Our prompt basically shares the idea of ReAct [1] and Reflexion [2], which asks the model to reason before taking any actions, and then asks the model to reflect on the results and update for upcoming events. We have done ablation studies on the modular level, such as taking out planning/replanning/learning modules from the system to see their effects. We didn't do specific wording-level ablation study because 1) every model in our experiments shares the same, working prompt, so a fair comparison can be maintained, and 2) it's a little expensive to spend our limited API credits on such experiments while we can focus on so many intriguing possibilities from auction arena.\n\n> Justification for Using LLM Agents\n\nCompared with language agents, our choice of LLM-powered agents is underpinned by their demonstrated flexibility and (zero-shot) generalizability in complex, language-based tasks. LLMs, with their ability to process, reason, and generate human-like language, are particularly suited for the nuanced decision-making required in many real-life scenarios. Consequently, agents powered by LLM demonstrate superior capabilities in addressing real-world tasks compared to previous specialized RL agents. These RL agents typically necessitate extensive training within specific environments and often struggle to adapt to communicative contexts during multi-agent interactions. Importantly, the challenge lies in feasibly training these RL agents in such environments, particularly given the absence of coherent training data. On the other hand, our LLMs are zero-shot agents, thus eliminating this issue. Therefore, it is imperative to understand and analyze their abilities before we actually put these LLM agents under complicated, high-stake situations. \n\n> Evaluation Metrics\n\nThank you for the suggestion! Quantifiable metrics is an advantage of our auction arena, compared with some other agent simulations. We have the following quantifiable metrics, which are not only about outcomes: \n1) final performance, e.g., total profit for profit-driven bidders, number of items for item-driven bidders, \n2) measuring bidding rationality, i.e., Corrected Failure Rate, recording belief errors, \n3) behavioral faithfulness, i.e., correlation between executions and plans, \n4) performance ranking between multiple agents, i.e., TrueSkill scores.\n\nAlthough we agree that further digging into strategies such as intermediate reasoning process could be interesting, it is difficult to objectively do so other than manual check or case study, and using LLMs as evaluators creates unintentional biases, such as GPT-4 favors texts generated by GPT-4.\n\n[1] Yao, Shunyu, et al. \"React: Synergizing reasoning and acting in language models.\" In ICLR 2023.\n\n[2] Shinn, Noah, et al. \"Reflexion: Language agents with verbal reinforcement learning.\" In NeurIPS 2023."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3788/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700321140903,
                "cdate": 1700321140903,
                "tmdate": 1700321140903,
                "mdate": 1700321140903,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Bs5cgrbgjk",
                "forum": "crMMk4I8Wy",
                "replyto": "ZghGi093FV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3788/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3788/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response #2"
                    },
                    "comment": {
                        "value": "> Adding an Open Sourced Model\n\nThat's a great suggestion! We add an open source model, [OpenChat-3.5 7B](https://huggingface.co/openchat/openchat_3.5), which proves to be better even than ChatGPT (GPT-3.5) on some benchmarks. We add it into the intra-specific multi-agent competition, with PaLM-2, GPT-3.5, GPT-4, Claude-2.0, Claude-Instant-1.2, and Rule bidder as fellow bidders. The setting is the same as that of Sec 4.3, except we have 7 bidders instead of 6. We repeat the experiments by 20 times, and the average results are as follows. The results shows that OpenChat-3.5 is at least good at numerical skills and processing facts, as shown by its less belief errors (according to CFR scores) than GPT-3.5 and Palm-2.\n\n| Model | # Engagement | CFR (% Failed Bids) | CFR (% Self Belief Error) | CFR (% Other Belief Error) | # Items Won | Money Left ($) | Profit ($) |\n|-------|------------------|-----------------|-----------------|------------------|-----------|------------|--------|\n| Rule | 24.86 | - | - | - | **4.86** | 425.0 | 3425.0 |\n| GPT-3.5 | 6.86 | **0.18** | 12.70 | 13.80 | 1.04 | 6840.9 | 1840.9 |\n| GPT-4 | 25.5 | 1.09 | **3.68** | **0.90** | 3.27 | 953.4 | 4044.3 |\n| Claude-2 | 20.95 | 6.36 | 1.77 | 4.04 | 2.54 | 2131.8 | **4131.8** |\n| Claude-1.2 | 14.41 | 10.54 | 11.30 | 8.09 | 1.95 | 2786.4 | 3240.9 |\n| Palm-2 | 24.77 | 17.45 | 10.09 | 12.80 | 4.68 | 654.5 | 3654.5 |\n| OpenChat-3.5 | 6.5 | 2.32 | 9.50 | 11.40 | 0.68 | 7750.0 | 931.8 |\n\nAs for the TrueSkill score, there is still plenty room for improvement for OpenChat-3.5 in such a dynamic competition.\n\n| Model | $\\mu$ | $\\sigma$ |\n|-------|-----|--------|\n| Claude-2 | 28.4392 | 1.1258 |\n| GPT-4 | 26.8398 | 1.1187 |\n| Palm | 26.6751 | 1.0832 |\n| Rule | 26.3237 | 1.0710 |\n| Claude-1.2 | 25.4109 | 1.0818 |\n| GPT-3.5 | 22.7858 | 1.1307 |\n| OpenChat-3.5 | 17.8331 | 1.2252 |"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3788/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700321167446,
                "cdate": 1700321167446,
                "tmdate": 1700404551450,
                "mdate": 1700404551450,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bI4zaItrwN",
                "forum": "crMMk4I8Wy",
                "replyto": "Bs5cgrbgjk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3788/Reviewer_eqke"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3788/Reviewer_eqke"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors"
                    },
                    "comment": {
                        "value": "I thank the authors for their reply!\nI am glad that they added an open-source model to the benchmark and found it to be competitive. This certainly makes the work more accessible and replicable.\nI understand the justification for LLM agents, but I would have liked to see a revised manuscript with the discussion.\n\nI will maintain my initial evaluation score. This paper's main contribution is an interesting environment for testing language models. However, I believe that incorporating the features outlined in Appendix C is necessary to realize the full potential of this environment.\n\nBest"
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3788/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700709948431,
                "cdate": 1700709948431,
                "tmdate": 1700709948431,
                "mdate": 1700709948431,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Xg4ryT4nvt",
            "forum": "crMMk4I8Wy",
            "replyto": "crMMk4I8Wy",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3788/Reviewer_PeRU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3788/Reviewer_PeRU"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies an interesting problem about the LLM's ability in auction, which can test whether the LLM can obtain high reward. The authors designed some prompts, and the results show LLMs are still limited."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The studied problem is quite interesting.\n2. The paper is written well.\n3. The appendix provides enough details for the prompts."
                },
                "weaknesses": {
                    "value": "1. The technical contribution is limited. The fundamental mechanism for the LLM agent is quite simple.\n2. The results are not so promising. There is no sufficient comparison among different LLMs.\n3. The setting of the auction is quite simple. I suggest the authors test different settings (those standard ones), and try to provide more insightul conclusions, compared with human."
                },
                "questions": {
                    "value": "Please address my concerns in Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "There is no ethics concern."
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3788/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3788/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3788/Reviewer_PeRU"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3788/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699640529591,
            "cdate": 1699640529591,
            "tmdate": 1699640529591,
            "mdate": 1699640529591,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "G1X7YelDXf",
                "forum": "crMMk4I8Wy",
                "replyto": "Xg4ryT4nvt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3788/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3788/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your comments. Please find our responses to your questions below.\n\n> Q1: The technical contribution is limited. The fundamental mechanism for the LLM agent is quite simple.\n\nOur prompting strategy seems simple, but our zero-shot CoT-style prompting is consistent with the current SOTA for working with these models, and novel in that we are applying it to dynamic scenarios.\n\nFurthermore, the design of Agents in the auction context is NOT simple. This is due to the many changes in the situation as the auction progresses. Key processes like Belief Update, Replanning, and Learning from Past Experiences are highlighted as areas where LLM Agents are expected to improve toward greater intelligence. \n\n> Q2:  The results are not so promising. There is no sufficient comparison among different LLMs.\n\nFor clarification, we note that our experiments involve an exhaustive set of the current best LLMs, namely PaLM-2, Claude-1.2, Claude-2, ChatGPT and GPT-4, all studied individually (Sec 4.1 & 4.2), and in combination and competition with one another (Sec. 4.3) and against human agents (Fig 6). In our view, this makes our studies fairly comprehensive, with 7 experiments.\n\nRegarding the results, we do not take the view that ``The results are not so promising``, in the absence of more details here we are unsure what you mean. On the contrary, while our results are nuanced, our best models such as GPT4 do indeed exhibit considerable skills in the auction arena, such as the ability to maintain coherent belief states (Fig 2), generate rational long-term plans (Fig 4) that are highly correlated with their behavior (Tab 1) and ultimately behave in a way that achieves their intended goals (Fig 3 & 5). Of course, there is still variability in the skills of different models, and models are indeed not perfect and have much room for improvement; this, in our view, is the main value of our simulation environment and our work.\n\n\n> Q3: The setting of the auction is quite simple. I suggest the authors test different settings (those standard ones), and try to provide more insightul conclusions, compared with human.\n\nOur auction setting closely standard the open ascending price auction model. In the absence of more details here, we are not sure what the reviewer means by ``standard scenario``.\n\nAdditionally, we have tried different settings in our experiments: \n- In Sec. 4.1, two bidders auction for ten items with different prices. \n- In Sec. 4.2, we adjusted the order of the items as a control variable. \n- In Sec. 4.3, we had six different auction agents competing with each other for different items. \n- In the Interspecific Competition experiment, different agents were set with two different types of objectives.\n- In Figure 6, we provided a comparison with human bidding, and we indeed found that humans are not always far ahead in this kind of competitive environment. As a benchmark, our conclusions have already been written in the abstract."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3788/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700321037688,
                "cdate": 1700321037688,
                "tmdate": 1700321037688,
                "mdate": 1700321037688,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]