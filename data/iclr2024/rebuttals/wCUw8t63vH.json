[
    {
        "title": "Spectral learning of shared dynamics between generalized-linear processes"
    },
    {
        "review": {
            "id": "OfNUFrQv4E",
            "forum": "wCUw8t63vH",
            "replyto": "wCUw8t63vH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7049/Reviewer_XxLi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7049/Reviewer_XxLi"
            ],
            "content": {
                "summary": {
                    "value": "I am unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "I am unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers."
                },
                "weaknesses": {
                    "value": "I am unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers."
                },
                "questions": {
                    "value": "I am unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7049/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7049/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7049/Reviewer_XxLi"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7049/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698770243371,
            "cdate": 1698770243371,
            "tmdate": 1699636828877,
            "mdate": 1699636828877,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "AdIRs1PmJg",
            "forum": "wCUw8t63vH",
            "replyto": "wCUw8t63vH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7049/Reviewer_NhF4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7049/Reviewer_NhF4"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a new method called PG-LDS with its corresponding inference and learning algorithm that can find both shared and residual dynamics from coupled observations. Experiments on both simulated and real-world dataset validate the effectiveness of the proposed model and corresponding algorithms. The provided algorithm can be generalized to other similar models with paired observationa and shared latent dynamics."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The whole paper is clear in presentation. Method derivation is detailed with maths.\n* The new model is interesting to me, since the usual way of treating the behavior data of a neural dataset is to treat it as external input, or some ground truth to be compared. This paper provides a new perspective of dealing this problem. Specifically, the Poisson spike train and behaivor data are treated as a coupled dataset with shared latent dynamics. By this way, we are able to use both the spike train data and the behavior data to find some common factors that accounts for the observations in an experiment."
                },
                "weaknesses": {
                    "value": "* First line of sec 2.2, typo: \"Given an $H$\". Page 3 -2 line, typo: \"we need to\".\n* See questions."
                },
                "questions": {
                    "value": "* What is \"either colored or white\" in Sec 3.1. This is confusing.\n* In Eq. 5, why Gaussian observations $z_k$ don't include a bias term? What's the distribution of $\\epsilon_k$?\n* What about the comparisons of the log-likelihood on test datasets?\n* Are there any existing models or methods that can learn shared latent dynamics from the joint dataset: e.g. neural spike trains plus movement?\n* Have authors tried other datasets? Is the proposed model widely applicable to similar tasks?\n* Since authors claim that the algorithm is able to be genearalized to non-Poisson/non-Gaussian model, have authors tried that on at least some synthetic datasets from simple models, which are not Poisson+Gaussian?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7049/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7049/Reviewer_NhF4",
                        "ICLR.cc/2024/Conference/Submission7049/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7049/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698809256347,
            "cdate": 1698809256347,
            "tmdate": 1700682587122,
            "mdate": 1700682587122,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gB1KA8Rzy8",
                "forum": "wCUw8t63vH",
                "replyto": "AdIRs1PmJg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7049/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7049/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for taking the time to review our manuscript and provide feedback. We address each of the raised points inline below.\n\n> What is \"either colored or white\" in Sec 3.1. This is confusing.\n\nWhite noise refers to temporally uncorrelated zero-mean Gaussian noise whereas colored noise refers to temporally correlated zero-mean noise. We have made the following clarification in the manuscript:\n\n*\u201c(either white, i.e., zero-mean temporally uncorrelated Gaussian noise, or colored, i.e., zero-mean temporally correlated Gaussian noise)\u201d*\n\nWe had also provided more context on how colored noise is simulated in appendix section A.3.1:\n\n*\u201cFor every simulated model, the colored noise for the Gaussian process, $\\epsilon_k$, was taken as the output of a 4-dimensional latent linear dynamical system, with random parameters that were generated similarly. By using a general colored noise, we can simulate dynamics present in the continuous modality that are unshared with the discrete modality.\u201d*\n\n> In Eq. 5, why Gaussian observations $z_k$ don't include a bias term? What's the distribution of $\\epsilon_k$?\n\nBecause $z_k$ is a linear Gaussian process with stationarity assumptions, a bias term would effectively correspond to an affine shift. As a result, and without loss of generality, we can assume the random process is zero-mean (i.e., no bias term). All predictions can be done with demeaned time-series and, if desired, the constant mean can be added back once inference is completed. For real data, a bias term can be learned by computing a mean parameter across time for the training data stream and demeaning the data as a preprocessing step before fitting the model. The learned bias can then be added back to the predicted time series for evaluation.\n\nWe refer the reviewer to the previous response in regards to the distribution of $\\epsilon_k$:\n\n*\u201c(either white, i.e., zero-mean temporally uncorrelated Gaussian noise, or colored, i.e., zero-mean temporally correlated Gaussian noise)\u201d*\n\n> What about the comparisons of the log-likelihood on test datasets?\n\nWe thank the reviewer for the suggestion. We are working on adding log-likelihood as another metric in the test datasets. \n\n> Are there any existing models or methods that can learn shared latent dynamics from the joint dataset: e.g. neural spike trains plus movement?\n\nWe thank the reviewer for their question. No analytical methods exist for learning shared latent dynamics from joint Poisson and Gaussian datasets, including for generalized linear dynamical system models which is what we focus on here.\n\nIt is worth noting that there do exist nonlinear deep learning methods for fitting multiple time-series, as cited in the manuscript. However, our work focuses on generalized-linear dynamical systems, which have remained popular given their interpretability for neuroscience investigations, analytical properties, rich theory, and broad applicability for developing real-time brain-computer interfaces. Thus, our goal here is to enable such generalized-linear dynamical systems models to prioritize the learning of shared dynamics between Poisson and Gaussian time-series data and dissociate them from disjoint dynamics.\n\n> Have authors tried other datasets? Is the proposed model widely applicable to similar tasks?\n\nWe have now added a completely new dataset and show that the same results/conclusions hold. This is in a new appendix section. The new dataset is an independent public NHP dataset (CRCNS pmd-1 dataset from the Miller lab), with a different behavioral task involving moving a cursor to random targets. Briefly, we compared stage 1 of PG-LDS-ID against PLDSID when both methods have $n_x=8$ latent states. As in dataset 1 (Fig. 2), our method outperformed PLDSID in behavior decoding. Further, the neural self-prediction at this dimension is lower than PLDSID, as expected, due to the use of stage 1 only. The cross-validated results were\n\n|Method|Poisson self-prediction AUC (mean&plusmn;STE)|Gaussian prediction CC (mean&plusmn;STE)|\n|-|-|-|\n|PG-LDS-ID|0.6176&plusmn;0.0054|**0.4025 &plusmn;0.0133**|\n|PLDSID|**0.6569&plusmn;0.0068**|0.3415 &plusmn;0.0114|"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7049/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700606915473,
                "cdate": 1700606915473,
                "tmdate": 1700606915473,
                "mdate": 1700606915473,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gJ7s4XypjH",
                "forum": "wCUw8t63vH",
                "replyto": "gB1KA8Rzy8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7049/Reviewer_NhF4"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7049/Reviewer_NhF4"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the authors' detailed response. I'm more confident on this paper, but I still think test (predictive) likelihood is an important metric to be added. Even though it might not be the most effective one in this case since there are two or multiple types of observations, test likelihood is still the most widely applied (or even the only one that can always be applied on real-world datasets). I understand running extra experiments requires some time. Given such a situation, I would like to raise my confidence to 4 and raise my score to 8. However, I think a score of 7 is more suitable, but this option is not available here. Overall, I think this paper is good. Thanks again!"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7049/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700682544797,
                "cdate": 1700682544797,
                "tmdate": 1700682544797,
                "mdate": 1700682544797,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "nyNAChMR9q",
            "forum": "wCUw8t63vH",
            "replyto": "wCUw8t63vH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7049/Reviewer_vavJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7049/Reviewer_vavJ"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a novel analytical approach, the PG-LDS-ID algorithm, designed for modeling Poisson data streams while disentangling shared dynamics with Gaussian data streams. This capability addresses the challenge of predicting the dynamics of one data stream from another with different statistical properties. Through simulations and real-world data, the authors demonstrate the effectiveness of their method in accurately identifying shared Poisson dynamics with Gaussian observations. The proposed algorithm's flexibility extends to various generalized-linear models, making it a valuable tool for modeling shared and distinct dynamics in data streams across diverse application domains."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper's most notable strength lies in its innovative decomposition technique introduced through Equation (6). This decomposition significantly simplifies the modeling of shared dynamics between data streams with different statistical properties. By breaking down the problem into manageable components, the paper enhances the overall approach's ease of handling and implementation.\n\nFor practical applicability, the introduced decomposition technique, as demonstrated in the paper, holds practical applicability in real-world scenarios. By simplifying the modeling of shared dynamics, the method provides a valuable tool for researchers in different domains. This practical aspect strengthens the paper's significance as it offers a solution that can be directly applied to address challenging problems."
                },
                "weaknesses": {
                    "value": "1. While the decomposition introduced in Equation (6) is a notable strength, it lacks clarity regarding the conditions under which it can be effectively implemented. The paper does not sufficiently discuss the scenarios where this decomposition may not be feasible, and whether alternative methods should be considered.\n\n2.  After the system decomposition, it is evident that r depends on both x^1 and x^2. However, the paper does not sufficiently explain why, in Section 3.2.1, C_r^1 can be estimated independently without considering C_r^2. \n\n3. The experimental evaluation in the paper primarily compares the proposed algorithm with PLDSID, which was introduced in 2012. It is essential to explore whether newer and more competitive algorithms have been developed. A more comprehensive comparative analysis involving the most up-to-date methods would provide a clearer picture of the proposed algorithm's strengths and weaknesses in the current research landscape."
                },
                "questions": {
                    "value": "See Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7049/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7049/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7049/Reviewer_vavJ"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7049/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698824460459,
            "cdate": 1698824460459,
            "tmdate": 1700704485037,
            "mdate": 1700704485037,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rvbhhsHVCf",
                "forum": "wCUw8t63vH",
                "replyto": "nyNAChMR9q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7049/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7049/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for taking the time to review our manuscript and provide feedback.\n\n> While the decomposition introduced in Equation (6) is a notable strength, it lacks clarity regarding the conditions under which it can be effectively implemented. The paper does not sufficiently discuss the scenarios where this decomposition may not be feasible, and whether alternative methods should be considered.\n\nWe thank the reviewer for their feedback. One can prove that any linear dynamical system can be written in this block-structured form by applying a change of basis (a similarity transform), and so this block-structured form does not lose generality (Katayama, 2006). We have now added a new appendix section in the manuscript that clarifies the mathematical derivation of the block structure in equation (6). Briefly, based on linear systems theory an equivalent basis for the latent space always exists that has the block structure in equation (6), in which the latent subspace that is observable via $z_k$ is placed as the first few dimensions of the latent state (see for example Theorem 3.8 from Katayama, 2006). Importantly, the blocked formulation from equation (6) also covers the special cases of non-blocked formulation when $n_1=n_x$, i.e., when all latent states contribute to both data streams (equivalent to full-rank observability). For more details about why this formulation is general and how the appropriate value for $n_1$ can be determined in real data, we invite the reviewer to see our longer response to reviewer QEhH, who had a similar question.\n\nReferences:\nTohru Katayama. Subspace Methods for System Identification. Springer London, 2005. doi: 10.1007/1-84628-158-x.\n\n> After the system decomposition, it is evident that $r$ depends on both $x^1$ and $x^2$. However, the paper does not sufficiently explain why, in Section 3.2.1, $C_r^1$ can be estimated independently without considering $C_r^2$.\n\nThis is a great question. We have made edits to section 3.2.1 and appendix A.1.2 to make this point more clear. The core reason for why $C_r^{(1)}$ can be estimated in the first stage of our algorithm without the need to simultaneously estimate $C_r^{(2)}$ is the block structure in equation (6), which is obtained without loss of generality, as noted in the previous question. Since the top right block of A in equation (6) is zero, we can see that the evolution of the shared dynamics (captured by $x^{(1)}$) does not depend on the evolution of the distinct dynamics in the predictor data stream (captured by $x^{(2)}$). This allows the parameters associated with the shared dynamics to be learned directly, as derived in the first stage of our method, and for the remaining parameters ($C_r^{(2)}$) to be learned in an optional second stage (section 3.2.2). We present the detailed derivations in appendix A.1.2, specifically equations (20), (21), and (22). Briefly, the block structure introduced in equation (6) yields a block structure to the observability and controllability matrices defined in equation (16) as $\\mathbf{H} = \\Gamma\\Delta$. Further, this block structure can be used in conjunction with SVD to separate $H$ into two components defined as $H = \\Gamma_{\\mathbf{r}}^{(1)}\\Delta^{(1)} + \\Gamma_{\\mathbf{r}}^{(2)}\\Delta^{(2)}$, with $\\Delta^{(1)}$ orthogonal to $\\Delta^{(2)}$. As a result, right multiplication with the pseudoinverse of $\\Delta^{(1)}$ allows independent estimation of $C_r^{(1)}$."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7049/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700201756710,
                "cdate": 1700201756710,
                "tmdate": 1700201756710,
                "mdate": 1700201756710,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SupxRMIUwL",
                "forum": "wCUw8t63vH",
                "replyto": "rvbhhsHVCf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7049/Reviewer_vavJ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7049/Reviewer_vavJ"
                ],
                "content": {
                    "comment": {
                        "value": "For the second question, I would like further understand why C_r^1 can be independently estimated. For now, my understanding is that in stage 1, the variable r is only involved in the computation of its covariance with z, and the contribution of C_r^2 to the covariance is zero. Therefore, C_r^1 can be independently estimated. Is this understanding correct?"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7049/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700454499658,
                "cdate": 1700454499658,
                "tmdate": 1700454499658,
                "mdate": 1700454499658,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4MUw7d4F0k",
                "forum": "wCUw8t63vH",
                "replyto": "SWRqYyUozd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7049/Reviewer_vavJ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7049/Reviewer_vavJ"
                ],
                "content": {
                    "comment": {
                        "value": "Thank author for the detailed reply. Considering the discussion, I raise my score to 8."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7049/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700704553683,
                "cdate": 1700704553683,
                "tmdate": 1700704553683,
                "mdate": 1700704553683,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "zSxsnyyNC9",
            "forum": "wCUw8t63vH",
            "replyto": "wCUw8t63vH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7049/Reviewer_1pgP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7049/Reviewer_1pgP"
            ],
            "content": {
                "summary": {
                    "value": "The authors present a method for estimating latent dynamical processes observed through two distinct observations processes - one that delivers continuous-time observations (here Gaussian observation process) and one that produces discrete time observations (here Poisson process). This in a practical setting might be continuous-time behavioral trajectories (i.e. arm movements) and neuronal activity data recorded from relevant brain regions.\nThe authors follow a covariance-based subspace system identification method to estimate what they call both the shared dynamics observed through the Gaussian and Poisson observation processes, and the disjoint (residual) dynamics that are not observable by both processes, but only through the Poisson observation process. \n\nThe proposed method follows a two stage approach for learning first the shared dynamics that are jointly observed by the two observation processes, and a second stage that identifies the residual dynamics only observed by the Poisson process. \nThey demonstrate their method on a simulated linear model system, and on non-human primate dataset of discrete population spiking activity recorded during continuous arm movements."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Under the assumption of linearity the authors can estimate the dimensionality of the latent process, and of the shared and residual subspace between the two observation processes.\n- An issue with covariance-based sub-space identification methods is that there is no guarantee that a valid set of parameters that satisfy the positive semi-definite covariance sequence will be recovered. Here the authors optimise by ensuring the validity of noise statistics."
                },
                "weaknesses": {
                    "value": "- The main weakness is the strong assumption of latent linear dynamics, that nevertheless is essential for the development of the method. However for the applications the authors have in mind, most systems are nonlinear. Thus I would expect some comparison of the performance of the method when applied to observations generated by a latent nonlinear system (as also mentioned in the questions below).\n- The authors do not outline the difference of the proposed approach with recent similar approaches that use subspace identification method relying both on behavioural and neural data, i.e. [1], [2]."
                },
                "questions": {
                    "value": "- The dimensionality of the shared and residual dynamics can be estimated from the singular value decomposition/low rank approximation of the Henkel matrices, as mentioned in Appendix A.1.5. However this estimation method will be accurate under the assumption that the observed latent dynamical system is indeed linear (instead of approximated by a linear system). Do the authors have any estimation on how the method will perform in cases where the latent system is nonlinear? In this case both the evolution equation of the latent process will be an approximation, but also importantly the dimensionalities of the shared and residual dynamics will probably be estimated inaccurately. I think it would be helpful to see systematic evaluations on how the method performs:\n   - i) when the dimensionalities of the subspaces are misestimated, and \n  - ii) when the linearity assumption does not hold (assuming that the dimensionalities of the subspaces have been correctly assessed). For example In Figure 2 (where the method is applied on primate data) I wouldn\u2019t say that the estimation of the dimensionality actually works (Figure 2 a and b).\n- In Section 3.1 in mentioning the model parameters that the method identifies, the authors do not include the noise variance of the Gaussian observation process $z_k$ (I.e. the characteristics of the noise term $epsilon_k$). Similarly in Section 3.2.3. \n   - Does this mean that the noise variance of the Gaussian observation process does not influence the performance of the method. Can the authors comment on this? \n   - Moreover I would also include the dimensionality of the latent process and the dimensionality of the shared subspace dynamics in the model parameters that are estimated.\n- How would the approach compare to estimation based on the Gaussian observation process? In Figure 1 the authors compare their framework with the PLDSID one, but I wonder how the method would compare to a method relying only on the continuous observations for identifying the shared subspace dynamics.\n- In Figure 1 caption the authors mention: \u201cPG-LDS-ID stage 1 used a dimensionality given by $\\min(4, n_x )$ \u201c. Can you explain what is meant with this phrase, and how the value 4 was chosen?\n\nMinor comments on writing:\n\n- In the first paragraph of the introduction the authors mention \u201cSecond, disjoint dynamics present in either observation can obscure and confound modeling of their shared dynamics\u201d. Up to here in the text it is still unclear what \u201cdisjoint dynamics\u201d is, and what \u201ceach observation\u201d refers to. For the latter I would propose to replace with \u201ceach observation stream\u201d or something along these lines. For the first, I would use something referring to \u201cuncorrelated\u201d part of the dynamics or residual as you call it later, but with a brief explanation what exactly is meant with this term.\n\n- Similarly in Page 4 point 2., the authors mention the transition matrix presented later in the text without giving more detail at this point transition matrix of which of the processes they consider they refer to.\n- In the introduction the authors refer to the part of the dynamics that is only observable through one observation process as \u201cdisjoint\u201d part, while in the main text they refer to this part as residual dynamics. I would propose to stick to one of those terms (preferably the latter one) to avoid confusing the readers.\n\n----\n\n**References:**\n\n[1] Ahmadipour, P., Sani, O. G., Pesaran, B., & Shanechi, M. M. (2023). Multimodal subspace identification for modeling discrete-continuous spiking and field potential population activity. bioRxiv, 2023-05.\n[2] Vahidi, P., Sani, O. G., & Shanechi, M. M. (2023). Modeling and dissociation of intrinsic and input-driven neural population dynamics underlying behavior. bioRxiv, 2023-03."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7049/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7049/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7049/Reviewer_1pgP"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7049/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699149223162,
            "cdate": 1699149223162,
            "tmdate": 1699636828539,
            "mdate": 1699636828539,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BQJMXT76NM",
                "forum": "wCUw8t63vH",
                "replyto": "zSxsnyyNC9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7049/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7049/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> The authors do not outline the difference of the proposed approach with recent similar approaches that use subspace identification method relying both on behavioural and neural data, i.e. [1], [2].\n\nWe have expanded our discussion of the differences with these works. Briefly, neither of these methods can dissociate or prioritize the shared dynamics between Poisson and Gaussian data streams as we do here, nor do they aim to predict one data stream from the other. The first cited reference models the collective dynamics (i.e., union) of concatenated spiking and field potential signals, not their shared dynamics. This work also does not dissociate shared versus distinct dynamics into separate latent states, and does not prioritize learning of the shared dynamics \u2013 both of which are our goals. Depending on data statistics, such a multimodal approach may dedicate the latent states largely to explaining the modality with the higher variance and/or to the mixture/union of dynamics (i.e., both shared and residual). Thus, it may fail to learn some shared dynamics that can be masked by the residual dynamics. In contrast, our approach explicitly learns the shared dynamics first (with priority) by forming a Hankel matrix with the secondary data stream as the future and the predictor data stream as the past. We make a note of this point in Introduction section (end of the first paragraph on the second page):\n\n*\u201cFinally, prior multimodal learning algorithms do not explicitly tease apart the shared vs. disjoint dynamics in a predictor (primary) time-series, but instead model the collective dynamics of two modalities in the same latent states (Abbaspourazad et al., 2021; Kramer et al., 2022; _Ahmadipour et al., 2023_).\u201d*\n\nRegarding the second cited reference, there are three main distinctions. First, the cited work is just for linear Gaussian processes and therefore is not applicable to Poisson or other generalized-linear processes considered here. Second, the subspace identification approach developed in the cited work uses projection-based instead of covariance-based subspace identification methods, which is not applicable to Poisson. Finally, instead of modeling Poisson and Gaussian shared dynamics, which is our goal here, the focus of that work is to incorporate a third data stream (i.e., input) into the model to dissociate input-driven and intrinsic dynamics (i.e., model Gaussian time-series in the presence of input). We have added a discussion of these differences to the manuscript.\n\n> In Section 3.1 in mentioning the model parameters that the method identifies, the authors do not include the noise variance of the Gaussian observation process z_k (i.e. the characteristics of the noise term epsilon_k). Similarly in Section 3.2.3. Does this mean that the noise variance of the Gaussian observation process does not influence the performance of the method. Can the authors comment on this?\n\nWe thank the reviewer for their great question and clarify two points. First, the model parameters stated in section 3.1 correspond to the parameters required by the point-process filter for state estimation (i.e., inference) (Eden et al., 2004). However, if desired, the noise covariance term under Gaussian assumptions can be learned by computing the covariance of the prediction residuals as such $E[(\\hat{z}_k - z_k)(\\hat{z}_k - z_k)^T]$, where $\\hat{z}_k$ denotes the predicted value of $z$ and the expectation denotes the empirical average across all time samples. When $\\epsilon_k$ is not white, the behavior prediction residuals under Gaussian assumptions can be computed in the same way (i.e., $\\hat{z}_k - z_k$) and modeled using Gaussian SID.\n\nThe second point is in regards to the impact the Gaussian process\u2019 noise has on learning. The effect is similar to that of the disjoint dynamics present in the primary (i.e., Poisson) data stream: it reduces the overall signal-to-noise ratio of the shared dynamics present in both observation data streams. However, with increasingly more training samples and improved estimates of the second-order moments (i.e., covariances), the algorithm would be more robust to the impact of the Gaussian observation noise.\n\nWe have included a new appendix section discussing the effect of Gaussian observation noise on learning and how its statistics (i.e., $\\epsilon_k$) can be learned if necessary.\n\n> In Figure 1 caption the authors mention: \u201cPG-LDS-ID stage 1 used a dimensionality given by min(4, $n_x$)\u201d. Can you explain what is meant with this phrase, and how the value 4 was chosen?\n\nFor the simulation results presented in Figure 1c and d, the true model parameter for the shared latent states is $n_1 = 4$. For each $n_x$ value along the x-axis, the corresponding $n_1$ (i.e., the dimensionality of the first stage which aims to identify the shared latents) is set to be either the minimum of the true $n_1=4$ value or $n_x$, as $n_x$ designates the total number of latents ($n_1 + n_2$)."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7049/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700671593164,
                "cdate": 1700671593164,
                "tmdate": 1700671593164,
                "mdate": 1700671593164,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "is83pBXq8U",
                "forum": "wCUw8t63vH",
                "replyto": "zSxsnyyNC9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7049/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7049/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> How would the approach compare to estimation based on the Gaussian observation process? In Figure 1 the authors compare their framework with the PLDSID one, but I wonder how the method would compare to a method relying only on the continuous observations for identifying the shared subspace dynamics.\n\nWe thank the reviewer for their question. The same mathematical issues will arise in this case and thus the performance of using only the Gaussian observation process for identifying the shared dynamics subspace would face the same shortcomings as using the discrete observations only. This is because identification of the shared subspace is dependent on the ratio of the shared vs. residual/unshared dynamics present in the observation process being modeled. If the residual/unshared dynamics explain the majority of the variance in the continuous observations, then these residuals will mask/confound the shared dynamics during identification from continuous observations. Indeed, because the identification method will aim to explain as much variance in the continuous observations as possible, the identification of the residual dynamics will precede identification of the shared dynamics and occupy most of the latent state dimensions. As such, the shared dynamics will be missed or inaccurately learned. In contrast, our approach prioritizes the identification of shared dynamics -- even if they are a minority of the total variance -- by looking at both continuous and discrete observations together during learning. Thus our method learns these shared dynamics more accurately. Only if the majority of the Gaussian observation variance is due to the shared dynamics, then using the continuous observations alone will yield comparable results to our method. However, this is typically not the case in most neuroscience datasets because signals are complex and a multitude of sources contribute to their variance, not a single source.\n\n> In the first paragraph of the introduction the authors mention \u201cSecond, disjoint dynamics present in either observation can obscure and confound modeling of their shared dynamics\u201d. Up to here in the text it is still unclear what \u201cdisjoint dynamics\u201d is, and what \u201ceach observation\u201d refers to. For the latter I would propose to replace with \u201ceach observation stream\u201d or something along these lines. For the first, I would use something referring to \u201cuncorrelated\u201d part of the dynamics or residual as you call it later, but with a brief explanation what exactly is meant with this term.\n\nWe thank the reviewer for their suggestion. We have made the following adjustment to the sentence in question:\n\n*Second, residual (i.e., unshared or unique) dynamics present in each observation stream can obscure and confound modeling of their shared dynamics*\n\n> Similarly in Page 4 point 2., the authors mention the transition matrix presented later in the text without giving more detail at this point transition matrix of which of the processes they consider they refer to.\n\nWe thank the reviewer for their comment. We have now revised the sentence in question to read as follows:\n\n*As a result, we derived a new least-squares problem for learning the components of the state transition matrix $\\boldsymbol{A}$ corresponding to the unique predictor process dynamics (section 3.2.2), without changing the shared components learned in the first stage (section 3.2.1).*\n\n> In the introduction the authors refer to the part of the dynamics that is only observable through one observation process as \u201cdisjoint\u201d part, while in the main text they refer to this part as residual dynamics. I would propose to stick to one of those terms (preferably the latter one) to avoid confusing the readers.\n\nWe have revised the text to consistently use \u201cresidual\u201d instead of \u201cdisjoint\u201d throughout the text and define it upon first use, as noted above."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7049/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700675372492,
                "cdate": 1700675372492,
                "tmdate": 1700734448991,
                "mdate": 1700734448991,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "fkOJWDxKRa",
            "forum": "wCUw8t63vH",
            "replyto": "wCUw8t63vH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7049/Reviewer_QEhH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7049/Reviewer_QEhH"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a multi-stage algorithm based on method of moments and positive semidefinite programming to estimate a dynamical system, where the observations come from two processes with shared dynamics. The authors consider specifically the setting where one process is Gaussian and another is Poisson, and the latent state of the Gaussian process leads to better prediction of the Poisson process but not vice versa. Simulation results show that the proposed method can accurately recover the shared dynamic, and real data experiment shows that the proposed method has better prediction accuracies compared with prior work."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper provides an algorithm that is able to estimate the shared dynamics of two generalized linear process. Compared to previous work (Buesing et al 2012), the inclusion of another correlated process leads to better prediction.\n\n2. By using second order moments, the proposed method can now deal with generalized linear processes instead of Gaussian."
                },
                "weaknesses": {
                    "value": "1. The paper seems to be motivated by solid applications in neuroscience; but I am not sure if this is of interest to the more general machine learning community.\n\n2. I am a little concerned about the structure of the dynamical system formulation. See questions."
                },
                "questions": {
                    "value": "Why can you assume that the coefficients follow the block structure as in equation (6)? Is this something motivated by the application? If the true coefficient matrix $\\mathbf A$ is non-zero on the upper right block, is the proposed method still going to work?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7049/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699228265874,
            "cdate": 1699228265874,
            "tmdate": 1699636828423,
            "mdate": 1699636828423,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KVYDHOcvXo",
                "forum": "wCUw8t63vH",
                "replyto": "fkOJWDxKRa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7049/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7049/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for taking the time to review our manuscript and provide feedback. The major concern from the reviewer was in regards to the block structure of the dynamical systems formulation, specifically:\n\n> Why can you assume that the coefficients follow the block structure as in equation (6)? Is this something motivated by the application? If the true coefficient matrix is non-zero on the upper right block, is the proposed method still going to work?\n\nThis is a great question. One can prove that any linear dynamical system can be written in this block-structured form by applying a change of basis (a similarity transform), and so this block-structured form does not lose generality (Katayama, 2006). We have now added a new appendix section in the manuscript that clarifies the mathematical derivation for the block structure in equation (6). Briefly, based on linear systems theory an equivalent basis for the latent space always exists that has the block structure in equation (6), in which the latent subspace that is observable via $z_t$ is placed as the first few dimensions of the latent state (see for example Theorem 3.8 from Katayama, 2006). This equivalent formulation, which is also known as the canonical Kalman form (for observability), is always available via an invertible linear transformation of the latent space, without any loss of generality. Formally, we define the dimension of the shared states (denoted by $n_1$) based on the rank of the observability matrix for the pair $(A, C_z)$. Importantly, this blocked formulation from equation (6) even covers the special case the reviewer refers to \u2013 when \u201cthe true coefficient matrix is non-zero on the upper right block\u201d \u2013 which happens if $n_1=n_x$, i.e., when all latent states contribute to both data streams and the observability matrix is full-rank. In this case, only the first stage of learning would be applied as $n_1=n_x$ and so $n_2=0$. Importantly, though, within the application of modeling neural and behavioral data, we typically expect a minority of neural dynamics to be related to a particular behavior of interest and so we expect the most appropriate $n_1$ to be smaller than $n_x$ in the vast majority of cases, thus requiring both stages of learning.\n\nOne can estimate the most appropriate $n_1$ and $n_x$ from the data as follows:\n1. Sweep over values of $n_1$, increasing $n_1$ while keeping $n_x=n_1$ (i.e., using stage 1 only to learn). Quantify the prediction of the secondary data stream (e.g., behavior) in each case to find the $n_1$ at which the prediction plateaus or reaches a peak. This value gives the appropriate $n_1$. Alternatively, $n_1$ can be estimated as the number of non-zero (or non-negligible) singular values of the Hankel matrix $\\boldsymbol{H}_{zr}$ (equations (8) and (9)).\n2. Using the selected $n_1$ from above, sweep over values of $n_x$, starting from $n_1$ and increasing the latent state dimension. Quantify the self-prediction of the predictor data stream (e.g., spiking activity) in each case, and find the $n_x$ at which the self-prediction reaches a peak. \n\nWe have also included discussion regarding the above procedure in the newly added appendix section in the manuscript. \n\nReferences:\nTohru Katayama. Subspace Methods for System Identification. Springer London, 2005. doi: 10.1007/1-84628-158-x."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7049/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700181129205,
                "cdate": 1700181129205,
                "tmdate": 1700181129205,
                "mdate": 1700181129205,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]