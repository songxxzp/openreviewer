[
    {
        "title": "Learning to Compute Gr\u00f6bner Bases"
    },
    {
        "review": {
            "id": "JLDIt3Hj5Z",
            "forum": "wdteczB4mQ",
            "replyto": "wdteczB4mQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2532/Reviewer_Sama"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2532/Reviewer_Sama"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the problem of generative model-learned algorithm for Grobner bases. Grobner bases are ubiquitous object for solving polynomial systems, but currently most known libraries that compute Grobner bases do require doubly exponential runtime in the worst case. It is therefore crucial to use tools such as transformers to facilitate this task. This paradigm also poses new interesting problems for Grobner bases: given a Grobner basis, how can one generate a polynomial set that is not a Grobner basis but spanned by the given basis (they call this problem backward Grobner problem)? Moreover, how can one randomly sample a Grobner basis? This paper shows how to solve these problems whenever the polynomials are in shape position and thus for zero-dimensional radical ideals. For the backward Grobner problem, they show a suitable linear mapping can indeed generate a non-Grobner basis while preserving the target set. Their characterization of such linear mapping is rather general, and they show how to sample a subset of these maps via Bruhat decomposition. Finally, experiments are performed, and the accuracy are relatively good. Interestingly, their experiments show that transformers perform much better for lexicographical order instead of graded reverse lexicographical, which is the most popular ordering to my knowledge."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper studies a very important problem, namely generating Grobner basis for polynomial system solving. Many algorithms for solving polynomial systems and related do require Grobner basis. However, Grobner basis algorithm is notably inefficient, so studying machine learning-based approaches is crucial. \n\nThe study of transformer-based approach also elucidates new problems for Grobner bases, such as how to sample them, and how to generate a non-Grobner basis given a Grobner basis. These are novel and intriguing problems and might have further applications. This paper attempts to address the first problem by sampling over zero-dimensional radical ideals, so that most polynomials are in shape position that are easy to handle. It is worth noting that zero-dimensional ideals is a vast and popular family in which many applications lie in. For the backward Grobner problem, they provide a characterization of linear maps that do enable the transformation from Grobner basis to non-basis.\n\nOverall, I think the problems imposed in this paper are interesting, and the theoretical results while not super surprising, are solid."
                },
                "weaknesses": {
                    "value": "While the main selling point is to use transformers for solving the Grobner bases problem, the actual experimental results are not that good. In particular, for the popular grevlex order, the transformer-based approach is very bad. It is surprising that the support accuracy is much higher than the actual accuracy for basis generation. One can argue that the blackbox nature of transformers makes it very hard to interpret the bottleneck of this method, but I do hope other architectures are tried to obtain a better empirical result. This is essential, as the main selling point of this paper is to use machine learning blackbox method to compute Grobner bases more efficiently.\n\nEven though the main motivation of this paper is to use transformers, the theoretical parts and the two problems regarding Grobner bases are intriguing to me than the experiments."
                },
                "questions": {
                    "value": "Typo: on page 2, summary of contributions the second item, it should be \"we uncover...\" instead of \"we uncovered...\".\n\nQ: How fast is your transformer-based approach compared to standard algorithms?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2532/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2532/Reviewer_Sama",
                        "ICLR.cc/2024/Conference/Submission2532/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2532/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697606313251,
            "cdate": 1697606313251,
            "tmdate": 1700291555921,
            "mdate": 1700291555921,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ns82eeiATC",
                "forum": "wdteczB4mQ",
                "replyto": "JLDIt3Hj5Z",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2532/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2532/Authors"
                ],
                "content": {
                    "title": {
                        "value": "To Reviewer Sama"
                    },
                    "comment": {
                        "value": "*(Please kindly check our global response \"To all reviewers\" beforehand as our reply is based on it.)*\n\nWe appreciate your critical reading of our paper and many positive comments on our work. \n\n> ... the actual experimental results are not that good. In particular, for the popular grevlex order, the transformer-based approach is very bad. \n\nYes, we consider that this demonstrates the challengingness of our newly introduced learning task: addressing NP-hard math problems. The math problems addressed in the prior math transformers are of moderate level. The only exception is a line of work of attacking LWE, the accuracy of which is also far from 100%. The popularity of grevlex in computational algebra derives from its empirical efficiency. Usually, one computes a Gr\u00f6bner basis with grevlex and then transforms it to lex order using FGLM. Our results suggest that it is better for Transformers to compute lex Gr\u00f6bner bases directly, which we consider is an interesting observation. \n\n\n> It is surprising that the support accuracy is much higher than the actual accuracy for basis generation. \n\nSupport accuracy is a relaxed accuracy metric, so it is not surprising to see that the support accuracy is higher. However, yes, high support accuracy suggests a possibility of hybridization of Transformer and math algorithms, which is one of our next targets.\n\n> One can argue that the blackbox nature of transformers makes it very hard to interpret the bottleneck of this method, but I do hope other architectures are tried to obtain a better empirical result. This is essential, as the main selling point of this paper is to use machine learning blackbox method to compute Grobner bases more efficiently.\n\nThank you for the important suggestion. We used a vanilla transformer architecture, as in many math transformer studies, so that one can know what and to what extent Transformers can learn by comparing these studies (including ours). As the accuracy is found to be limited, yes, the next step is to design a better architecture. To this end, we have to be careful about the difference in the nature of natural languages and symbolic computations; a correct output for the former can be calculated from the part of the input, while for the latter, the whole input should be taken into account. \n\n\n> Even though the main motivation of this paper is to use transformers, the theoretical parts and the two problems regarding Grobner bases are intriguing to me than the experiments.\n\nWe really appreciate this comment, and we do agree with it. As presented in the global response, we consider that one of the core contributions is posing an interdisciplinary challenge from which various studies can appear.\n\n\n**Q1**\n> How fast is your transformer-based approach compared to standard algorithms?\n\nThe runtime of Transformer at Table~2 for $n=2, 3, 4, 5$ is 5.879, 11.419, 14.371, 20.622 seconds, respectively. The test batch size was 500, 250, 250, 250, respectively, to fit the input to our 48GB GPU memory. The runtime of Transformer is 5-10 times slower than the math algorithms we tested. Theoretically, the runtime of a Transformer does not depend on the difficulty of the problem as math algorithms do. Shorter encoding of polynomials and better attention mechanisms are important future works. For the former, we may be able to exploit some nature of polynomials; for the latter, sparse attention may not be helpful because, unlike NLP tasks, symbolic computation cares about all the input symbols."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2532/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700280221806,
                "cdate": 1700280221806,
                "tmdate": 1700280221806,
                "mdate": 1700280221806,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZKlOtiAKVJ",
                "forum": "wdteczB4mQ",
                "replyto": "ns82eeiATC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2532/Reviewer_Sama"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2532/Reviewer_Sama"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your reply! Overall, I think this paper studies a very important problem, and try to solve it using ML models do require much more care than some of the other math problems. This is an interesting perspective, and I appreciate the development of it. Though this paper can be improved in terms of its experiments, I think this paper makes nontrivial contribution to a significantly important math problem. Hence, I raise my score to accept."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2532/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700291542809,
                "cdate": 1700291542809,
                "tmdate": 1700291542809,
                "mdate": 1700291542809,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "e5ovJi54gr",
            "forum": "wdteczB4mQ",
            "replyto": "wdteczB4mQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2532/Reviewer_t6to"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2532/Reviewer_t6to"
            ],
            "content": {
                "summary": {
                    "value": "This paper discuss using Transformer technology to solve a really hard problem from computational linear algebra: computing the Grobner basis. This problem is known to be NP-hard (with best known double exponential algorithms). Thus, using heuristic methods like ML can be attractive. \n\nMost of the focus of the paper on how to generate the training set. To that end, the authors solve two problems: generating random Grobner bases, and computing non-trivial varities with a prescribed basis. The authors claim that these problems have not be explored before.\n\nI need to qualify my review: I am not an expert, nor even knowledgable, regarding computational algebra. Thus, my review is based on the author's background and claims. I cannot verify the correctness of their claims from an algebraist point of view. This is a low confidence review."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Solving an important problem, which required solving problems not considered before, and allow solving problems that are very expensive to solve without a lot of computational power.\n- Part of growing literature on using transformers for symbolic mathematics.\n- Good empirical results.\n- Well executed study."
                },
                "weaknesses": {
                    "value": "- Limited applicability: \na) The authors acknowledge that transformers may only perform well for in-distribution samples, citing Dziri et al 2023. They do dismiss this as a \"a fundamental challenge of transformers, and outside the score, but nevertheless this limits the applicability of their algorithm.\nb) In particular, due to the limitations of transformers, they can learn only with instances of bounded size. I am unclear whether this limits also testing, but regardless in means that in-distribution is only of limited size. As a side note, is the problem of finding the Grober basis also NP-hard for bounded size instances?\nc) The authors impose additional restrictions on instances in the tranining set, and they claim these are reaslistic. \n\n- Limited theory. although there are some theorems, they are simple and their proofs seem straightforward. The  contribution from the mathematical side seem very limited. \n\n- Algorithm for randomly generating Grobner bases seems very simple, and seem to \"engineer\" the problem a lot.  There is no analysis of the actual distribution of the instances the algorithm will generate. Is it uniform in any way?\n\n- Contrary to what they authors claim, no real light is shed on the algebraic problems themselves. \n\n- Seems that only experiments on synthetic data was considered. \n\n- Will interest only specialist on work on computational algebra."
                },
                "questions": {
                    "value": "- Did you try you method on \"real world\" problems?\n- I understand that learning can only be done on limited size instances. But does this limit testing as well? If no, why not report experiments. \n- On page 6, where you say that you sample O(s^2) polynomails, how are these sampled? What distribution?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2532/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2532/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2532/Reviewer_t6to"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2532/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698750864167,
            "cdate": 1698750864167,
            "tmdate": 1699636189733,
            "mdate": 1699636189733,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "p3xpwaVr3p",
                "forum": "wdteczB4mQ",
                "replyto": "e5ovJi54gr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2532/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2532/Authors"
                ],
                "content": {
                    "title": {
                        "value": "To Reviewer t6to (1/2)"
                    },
                    "comment": {
                        "value": "*(Please kindly check our global response \"To all reviewers\" beforehand as our reply is based on it.)*\n\nWe appreciate your critical reading of our paper. We first answer the questions you raised. Replies to other comments will come later or have already been addressed in the global response.\n\n**Q1**\n> Did you try your method on \"real world\" problems?\n \n No, because our focus lies on a \"general case\" rather than special cases that largely vary across applications (see our global response). We believe this is reasonable for the first work. Our work discovered a new challenging machine learning task and new math problems for it, proposed baseline dataset generation method, and provided the experiments using it. We can test the Transformers trained on our dataset to \"real world\" problems, but such an out-distribution test should result in almost zero accuracy.\n\n**Q2**\n> I understand that learning can only be done on limited size instances. But does this limit testing as well? If not, why not report experiments?\n\nThank you for your suggestion. Yes, it's limited for now because the maximum sequence length of positional encoding was set based on the maximum sequence length of training samples. We retrained a few models with extended limits for $n=2, 3$. For $n=2$, we generated a new test test for which we set the maximum number of terms of polynomials in $U_1, U_2$ to 4 (previously 2). For $n=3$, we set the density $\\sigma=1.0$. We observed a large accuracy drop for such out-distribution samples~(approximately 20-30% decrease). Note that the results relate to out-distribution accuracy, which is beyond the scope of current work (see global response). However, it may be true that the results will serve as a baseline for future work. \n\n**Q3**\n> On page 6, where you say that you sample O(s^2) polynomails, how are these sampled? What distribution?\n\nAll the polynomials in this paper are sampled uniformly from the polynomials with a bounded total degree and a bounded number of terms. This is implemented by `random_element` function in SageMath.\n\n**Weakness 1: Limited applicability**\n\nGlobal response mostly replies to this comment. We admit that our current results are not very practical; however, we'd like to emphasize that this is because the learning of Gr\u00f6bner basis computation is extremely challenging compared to the math problems addressed in the prior math transformers, requiring many machine learning and mathematical problems to be resolved. Besides, we consider that our task is based more on practical motivation. There is no scalable algorithm for computing Gr\u00f6bner bases, while there are for the tasks in the related studies. The only exception is the line of works for attacking LWE using Transformers, which is also currently restricted to small problems and not considering generalization.\n\n> a) The authors acknowledge that transformers may only perform well for in-distribution samples, ... nevertheless this limits the applicability of their algorithm. \n\nYes, and this is actually why prior studies of math transformers explored better training distributions and tested generalization liability. However, this is strongly based on the fact that for their tasks, the random generation of samples is trivial and in-distribution accuracy is already high enough. We found that this is not the case for the Gr\u00f6bner basis computation and thus focus on this step.\n\n> b) [... The omitted part was answered at Q2. ...] As a side note, is the problem of finding the Grober basis also NP-hard for bounded size instances? \n\nWe are not very sure what is meant by \"bounded size of instances.\" An instance size of an NP-hard problem is always bounded. For reference, \"computing a Gr\u00f6bner basis from a given $F$ of finite size\" is NP-hard. For example, the polynomials in MQ challenge, a post-cryptography challenge, are degree-2 polynomials. The difficulty of solving degree-2 polynomial systems is the foundation of the multi-variate cryptosystem.\n\n> c) The authors impose additional restrictions on instances in the training set, and they claim these are realistic.\n\nWe consider that our setup is reasonable and oriented for practical scenario. Refer to the global response. \n\n**Weakness 2: Limited theory**\n> Limited theory. although there are some theorems, they are simple and their proofs seem straightforward. The contribution from the mathematical side seems very limited.\n\nWe have to admit the current theorems and algorithms are simple. This is partially because that the efficiency matters in the large-scale dataset generation. We may design a sophisticated method based on the necessary and sufficient condition (i.e., Theorem 4.7 (3)). Particularly, we may utilize the sygyzy module of $G$, the representatives of which can be efficiently computed as we have an access to a Gr\u00f6bner basis $G$ (Schreyer's Theorem). We believe that this approach can give a more \"uniform\" sampling, but it needs a few more steps and is left to our future work."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2532/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700279891037,
                "cdate": 1700279891037,
                "tmdate": 1700280127255,
                "mdate": 1700280127255,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FkX9VVIEfD",
                "forum": "wdteczB4mQ",
                "replyto": "e5ovJi54gr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2532/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2532/Authors"
                ],
                "content": {
                    "title": {
                        "value": "To Reviewer t6to (2/2)"
                    },
                    "comment": {
                        "value": "**Other Weaknesses**\n> Algorithm for randomly generating Grobner bases seems very simple, and seem to \"engineer\" the problem a lot. There is no analysis of the actual distribution of the instances the algorithm will generate. Is it uniform in any way?\n\n> Seems that only experiments on synthetic data was considered.\n\nSee the global response. \n\n> Contrary to what the authors claim, no real light is shed on the algebraic problems themselves.\n\nOur work introduces a motivation for unexplored algebraic problems and also presents an algorithm and theories for a base case. The intention of this comment is unclear to us, and we would appreciate it if we could have more elaboration.\n\n> Will interest only specialist on work on computational algebra.\n\nWe believe that this work may attract interdisciplinary interests. Our new problems should encourage computational algebraists to work on a machine-learning topic. We showcase an example of math problems with difficulty in dataset generation, which should be of interest to the math transformer community. As you've pointed out, our algorithm and theory are designed simply, which may allow readers with limited knowledge of algebra to follow it. The limited in-distribution accuracy at Gr\u00f6bner basis computation also provides an empirical limit of the Transformer's learnablity, which may interest the researchers in developping a better model. Futher, our work has a potential impact on the cyptography as many cryptosystems are in the scope of *algebraic attack*, which use Gr\u00f6bner basis computation."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2532/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700279929666,
                "cdate": 1700279929666,
                "tmdate": 1700280170910,
                "mdate": 1700280170910,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "NmvyJWuPpi",
            "forum": "wdteczB4mQ",
            "replyto": "wdteczB4mQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2532/Reviewer_DA1f"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2532/Reviewer_DA1f"
            ],
            "content": {
                "summary": {
                    "value": "Grobner basis computation is an important problem in computational algebra, and has applications in cryptography. In this problem we are given as input a non-grobner set, and the output is a grobner basis of the set. The problem is NP-hard, and is also considered to be in hard in practice. This paper investigates the use of transformers in speeding up the solving.\n\nTraining a transformer requires a large set of input output pairs from a distribution resembling the distribution of interest,  and this set is not available for the grobner basis problem, due to the computational complexity of generation. To solve this issue, the authors propose a novel method to uniformly sample from the output domain, i.e. set of grobner bases and then find a corresponding input in polytime. The authors then train a transformer on this set and demonstrate the efficacy of their approach. Interestingly, even in the cases where the grobner computation is wrong, the support is correct, which is enough material for other tools to efficiently compute the bases."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper overall is written and arranged well. The data generation approach proposed by the paper might be of independent interest."
                },
                "weaknesses": {
                    "value": "I found the experimental evaluation unconvincing. \n\n1) The test set seems to be randomly generated instances, rather than problems arising out of some real applications.  Moreover they are generated from the same distribution as the training set. The authors mention the problem of out of distribution generalisation, however it is not addressed at all as far as I could tell. \n\n2) It is usual for solvers to test their performance on some standard datasets (for ex. the SAT competition for SAT solvers), and random instances are usually considered irrelevant. It is not clear whether the state of the art Grobner basis tools have been optimised for random instances or practical instances, hence the comparision may not be fair. \n\nI felt that the use of transformers for this problem is not motivated well enough. The paper does mention that transformers have been used for other math problems, including for a step in the related Buchberger algorithm; however it is not at all clear why they should be used here. Why transformers and not something more basic?"
                },
                "questions": {
                    "value": "1) Does the backward generation process induce a uniform distribution on the input space as well?\n2) Do typical Grobner basis problems come from use cases which match the distribution of the backward generation?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2532/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698837113987,
            "cdate": 1698837113987,
            "tmdate": 1699636189658,
            "mdate": 1699636189658,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "vtS2uBz8QY",
                "forum": "wdteczB4mQ",
                "replyto": "NmvyJWuPpi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2532/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2532/Authors"
                ],
                "content": {
                    "title": {
                        "value": "To Reviewer DA1f (1/2)"
                    },
                    "comment": {
                        "value": "*(Please kindly check our global response \"To all reviewers\" beforehand as our reply is based on it.)*\n\nWe appreciate your critical reading of our paper. Your comments are quite reasonable and have been the focus of the prior studies. However, we would like to emphasize that (we found) learning of Gr\u00f6bner basis computation has fundamental challenges that other math transformer papers do not have. We have already emphasized it in the global response, so please refer to it. We sincerely ask for your re-evaluation of our work by taking into account our rebuttal.\n\n**Weakness 1 : Out-distribution case**\n> The test set seems to be randomly generated instances rather than problems arising out of some real applications. ... however it [the problem of out-of-distribution generalization] is not addressed ... .\n\nThis question is addressed in the global response. In short, the out-distribution case is beyond the scope of this study because, unlike the prior studies, the dataset generation itself is non-trivial, and in-distribution accuracy is not high enough.\n\n\n**Weakness 2: Standard datasets**\n> It is usual for solvers to test their performance on some standard datasets (for ex. the SAT competition for SAT solvers), and random instances are usually considered irrelevant. \n\nThis strongly relates to our main claim --- preparing many (non-Gr\u00f6bner set, Gr\u00f6bner set) pairs is non-trivial. To our knowledge, there are some classical families of polynomials for benchmarking Gr\u00f6bner basis computation algorithms (or polynomial system solving algorithms), but these are very far from standard datasets. One may consult [this websight](https://www-sop.inria.fr/coprin/logiciels/ALIAS/Benches/node1.html). The listed examples are artificial, not necessarily zero-dimensional. They are empirically found difficult and/or designed for easily generating variations in the number of variables (e.g., testing for polynomial system Katsura-4 in 4 variables, then Katsura-5 in 5 variables, and so on). Classical benchmarks are useful for math algorithms, i.e., the algorithms proved to work for all the cases (i.e., 100 % \"in/out-distribution\" samples) except for the timeout, but not for our current work because they are out-distribution samples. \n\nAnother benchmark can be found in cryptography challenges (e.g., [MQ Challenge](https://www.mqchallenge.org)). As we have discussed above, the polynomials appearing there are also strongly biased.\n\n> It is not clear whether the state of the art Grobner basis tools have been optimised for random instances or practical instances, hence the comparision may not be fair.\n\nThe general purpose algorithms are Buchberger's algorithm and F4 algorithm (SoTA). We tested both of them in our experiments. Thus, the comparison must be fair as the focus of our study is oriented to the general case. Those specialized for particular \"practical instances\" are not discussed in this paper.\n\n**Weakness 3: Motivation of using Transformer**\n\nIn the prior studies, math transformers were studied for testing the learnability of Transformers, rather than using them as practical solvers because we already have well-developed scalable algorithms for the adopted moderate-level math problems.\n\nIn contrast, the use of Transformers here is strongly motivated by the fact that there are no scalable mathematical algorithms for computing Gr\u00f6bner bases. Unlike math algorithms, the runtime of the Transformer does not depend on the difficulty of the problem but on the sequence length. For a large-scale instance on which math algorithms cannot run, Transformers can return an answer with some probability or at least give a hint of the solutions (see discussion on the support accuracy of Table 2). We admit that the current results do not achieve this ultimate goal; resolving this challenge has a long way to go both in machine learning and computational algebra, and this study has initiated an essential step.\n\n> The paper does mention that transformers have been used for other math problems, including for a step in the related Buchberger algorithm;\n\nQuick correction. Transformers have not been used for Gr\u00f6bner basis computation before. The technique applied to the Buchberger algorithm is simple reinforcement learning. Further, this algorithm assumes binomials and does not work for general polynomials."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2532/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700279979436,
                "cdate": 1700279979436,
                "tmdate": 1700279979436,
                "mdate": 1700279979436,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]