[
    {
        "title": "Voila-A: Aligning Vision-Language Models with User's Gaze Attention"
    },
    {
        "review": {
            "id": "jfkfkqzcTM",
            "forum": "KrOmLMFYHi",
            "replyto": "KrOmLMFYHi",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1204/Reviewer_gfYi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1204/Reviewer_gfYi"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces the concept of incorporating gaze information (referred to as human attention) into VLM models to enhance their performance and potentially improve their interpretability. To support this, the authors collected hundreds of minutes of gaze data and developed an automated data annotation pipeline using GPT-4 to generate the VOIA-COCO dataset.\n\nThe paper makes a valuable contribution to the relevant academic community by demonstrating the significance of gaze information for VLM tasks, such as visual question answering. The presented results show that the proposed method is qualitatively and quantitatively superior to baseline models like Otter."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "\u2022\tThe authors introduce the novel concept of utilizing gaze information in the development of VLMs.\n\u2022\tUnlike baseline and several other studies, the experimental analysis is not limited to qualitative results but also demonstrates quantitative results."
                },
                "weaknesses": {
                    "value": "1.\tThe paper's presentation is lacking. Many important sections have been relegated to the appendix, especially the technical details. For example, Section 4.1 is challenging to understand due to the limited text. 2)The model heavily depends on the baseline model Otter. The method of injecting gaze information is quite straightforward. The way in which the authors handle catastrophic forgetting can be observed in the literature, thus not introducing technical novelty.\n2.\tThe experimental analysis appears somewhat unfair because the proposed method uses additional modalities to achieve the same results. Therefore, its better performance is not surprising, particularly in cases where the query does not clearly define the object's name, and several other objects are present in the scene, with the gaze heatmap aligning with the queried object. It is also worth to mention that both baseline methods are still only in ArXiv.\n3.\tThere is uncertainty about the cases in which gaze information was found to be less relevant.\n4.\tThe caption for Figure 3 lacks informativeness.\n5.\tThere exist a few typos to be fixed, e.g., Fiture\n6.\tHallucination issue can be better presented qualitatively and better discussed.\n7.\tGaze data collection procedure is also very scarse and it is not possible to understand if the annotators have a reliable consensus to use the collected data in model evaluation and comparisons.\n8.\tIt is doubtful whether 100 gaze samples are sufficient for conducting a comprehensive comparative study. I have reservations about the potential bias in the collected dataset."
                },
                "questions": {
                    "value": "\u2022\tSection 4.1 and gaze data annotation should be described in detail. It is not possible to validate the procedures perform in these context.\n\u2022\tWeakness Q4\n\u2022\tPls. comment on Weakness (2) for the technical novelty of the method.\n\u2022\tPls. comment on Weakness (3).\n\u2022\tHow the authors evaluate the interpretability? Several places in the paper interpretability was mentioned, however its evaluation is unclear given that such a keyword is being used in several different content of AI.\n\u2022\tWeakness Q9"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1204/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698053620638,
            "cdate": 1698053620638,
            "tmdate": 1699636047070,
            "mdate": 1699636047070,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "neVCpJNSQy",
                "forum": "KrOmLMFYHi",
                "replyto": "jfkfkqzcTM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1204/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1204/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer gfYi (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for your insightful feedback on our work. Your input is invaluable in helping us identify improvements and refine our research. We're committed to addressing any shortcomings and appreciate your constructive suggestions.\n\n**Presentation and Technical Details:** \n\nWe recognize the need to enhance our presentation and include crucial information in the main paper by providing detailed explanations where needed. To address your concerns, we have made significant improvements to our manuscript, including extensive reorganization, clarification, and rewriting as we replied in **General Response**.\n\n**Baseline Selection and Technical Novelty**\n\nWe would like to begin by clarifying the rationale behind our choice of Kosmos-2 and Otter as the two baselines for comparison. Our method aims to demonstrate improvements in two key areas: state-of-the-art multimodal conversation capabilities and grounding ability with additional modality input.\n\nFirst, Otter is based on the Flamingo architecture, a well-recognized milestone in vision-language research. Otter has been fine-tuned on egocentric data (e.g., Ego4D) and instructional data (e.g., COCO), making it more suitable for the scenarios we are targeting compared to other similar VLMs. Second, Kosmos-2, at the time of submission, is a widely known and comprehensively evaluated model with superior grounding capabilities. Since our model has not been pre-trained on massive grounding data like Kosmos-2, comparing our grounding capability to Kosmos-2 presents a significant challenge, requiring us to explore efficient ways to guide VLMs in learning grounding.\n\nRegarding the reliability of our chosen baselines, both Kosmos-2 and Otter are cited extensively (72 and 110 citations, respectively), open-sourced, and actively maintained. We believe that the research community widely recognizes and reviews these models.\n\nIn terms of fairness in comparison to the baselines, for grounding, we contend that our method performs on par with, if not surpassing, Kosmos-2, which also has an additional modality indicating user focus and pre-trained on a large grounding dataset. For Otter, our method not only demonstrates significant grounding improvements but also exhibits better helpfulness. The quality of our coreference question responses is comparable to the direct question responses of Otter's base model, which we believe is a fair comparison.\n\nAs for technical novelty and contribution, we argue that our model design is far from trivial. Numerous approaches exist for incorporating gaze data, and our final choice was made after conducting a series of experiments detailed in the ablation study. To further elaborate on these aspects, we have included a detailed illustration of different gaze integration methods in Figure 13. Additionally, our work extends beyond model design, as we strive to establish a method for a new problem. The data annotation pipeline and gaze benchmark construction have not been explored by other works, making it difficult to claim that our work lacks novelty.\n\n**Interpretability**\n\nWe mentioned interpretability twice in our conclusion, as we believe integrating more human intention signals into the model can enhance mutual understanding between the model and the user. This means the model can better interpret the user's intent based on the additional modality, and the user can understand the model's specific response according to the pre-input gaze grounding signal. However, we acknowledge that using the term \"interpretability\" might lead to confusion with the traditional interpretation concept in machine learning. Therefore, we have decided to temporarily remove this term and reserve it for further evaluation and demonstration. We genuinely appreciate your valuable feedback on this matter."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1204/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700426434517,
                "cdate": 1700426434517,
                "tmdate": 1700426434517,
                "mdate": 1700426434517,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "yYqg9KpiRN",
            "forum": "KrOmLMFYHi",
            "replyto": "KrOmLMFYHi",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1204/Reviewer_2Muy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1204/Reviewer_2Muy"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents \"Voila-A,\" a novel approach aimed at aligning Vision-Language Models (VLMs) with user gaze attention. The authors highlight the challenges faced by existing VLMs in handling complex scenes and diverse human attention patterns. They propose utilizing gaze information collected from AR or VR devices as a proxy for human attention to guide VLMs.\n\nThe paper provides a thorough explanation of the methodology, including data collection, automatic data annotation using GPT-4, and the design of the Voila Perceiver modules. The authors conduct experiments, comparing Voila-A with baseline models (Otter and Kosmos-2) on both synthesized and real-world gaze datasets.\n\nThe results demonstrate the effectiveness of Voila-A, showcasing its balanced capability between helpfulness and fact grounding. The evaluation metrics, including GPT-4 Ranking and Reward Score, support the authors' claims. Additionally, ablation studies and qualitative analyses provide further insights into the model's performance and capabilities.\n\nOne notable contribution is the introduction of trace data as a proxy for gaze data, offering a cost-effective and scalable alternative for aligning VLMs with user gaze attention. The method of transforming trace data to mimic gaze data is well-described and substantiated with empirical evidence."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The proposed method for aligning gaze data with trace data proves to be both effective and straightforward. It introduces a fresh approach to integrating gaze information with Vision-Language Models (VLLM). The approach has been rigorously examined through studies, yielding results that substantiate its efficacy."
                },
                "weaknesses": {
                    "value": "The data collection section (4.1) lacks detailed information on the methodology and content of the dataset. Providing specific examples and clearer explanations would enhance comprehension. Additionally, Figure 3 needs a caption and more in-depth explanations to convey its intended message. The figures also need higher resolution for better readability when printed.\n\nSection 4.2 requires clearer explanations, particularly regarding the parameters X, L, and G. The concept of 'latent data' (L) needs better elucidation. A structured approach, starting with an explanation of the inputs and employed encoders, followed by a deep dive into the new approach, would enhance clarity. A comprehensive figure illustrating how gaze is integrated into visual language models would be beneficial. It's unclear how the 'Voila Perceiver Resampler' module is integrated with the VLLM.\n\nIn Section 4.3, the meaning and specifics of 'gaze linear layer' and 'gaze key linear of attention' need clarification. It's not clear which layers these terms refer to or if there's a specific formula involved.\n\nMerging Section 5.1.2 with Section 4.1 would improve the overall clarity of the paper.\n\nThe summary of the main results in Figure 5 is not easily understandable. Using a table with percentages might provide clearer insights.\n\nThe claim that Voila exhibits a 'superior ability to handle multi-turn real conversations' in the last sentence of Section 5.3 needs stronger support or clarification in the results section."
                },
                "questions": {
                    "value": "Could you provide a clearer depiction of how the ViolapercieverBlock and Resampler are integrated within the larger VLLM framework? A simplified architectural overview would be immensely helpful in understanding the bigger picture.\n\nIt would be beneficial to have more details on what exactly is included in the automatic data annotation process and how it is carried out. Providing specific examples in the main paper would greatly enhance comprehension.\n\nFor Figure 4 and 5, additional guidance on how to interpret the results would be appreciated. Specifically, clarification on what constitutes the 'Overall score' and a detailed explanation of how the 'Helpfulness' and 'Grounding score' are calculated would be invaluable.\n\nI would also onsider providing a brief discussion of potential applications and future directions in the conclusion section.\nClarify any specific limitations or potential challenges associated with the proposed approach."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1204/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1204/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1204/Reviewer_2Muy"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1204/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698768265738,
            "cdate": 1698768265738,
            "tmdate": 1699636046993,
            "mdate": 1699636046993,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "oTt6OodHhu",
                "forum": "KrOmLMFYHi",
                "replyto": "yYqg9KpiRN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1204/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1204/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 2Muy"
                    },
                    "comment": {
                        "value": "We are sincerely grateful for your positive assessment of our work and appreciate the constructive and valuable suggestions you've provided to help us improve it further.\n\n**Automatic data annotation process in Section 4.1**\n\nWe acknowledge that our original presentation of this process was not sufficiently clear, making it challenging for readers to follow. In the revised version, and in accordance with your recommendations, we have provided detailed formulations and explanations of the annotation process alongside Figure 3. We have also included final annotated data examples in Figure 11 to enhance comprehension. Furthermore, to facilitate better viewing, we have replaced the original PNG figure with a PDF format, allowing readers to zoom in as needed.\n\n**Voila Perceiver and Resampler Explanation**: \n\nIn the revised version, we have provided a clearer architectural overview to better illustrate how the Voila Perceiver and Resampler components are integrated within the VLM framework. Additionally, we have marked the different fine-tuning stages on the structure to clarify which components are tuned in each phase. We acknowledge that the original terms `gaze linear` and `gaze attention` were not clear, and we have revised the text to specify that they refer to the components tuned during the first stage, without introducing any new formulations. Furthermore, we have rewritten the introductory part of the method section to provide more context on our base structure and input, ensuring a more comprehensive understanding for our readers.\n\n\n\n**Interpretation of Figures 4 and 5**:\n\n In the revised manuscript, we have provided additional insights on interpreting the results, along with explanations for the 'Overall score,' 'Helpfulness,' and 'Grounding score,' which can be found in Figure 12 in the appendix. The primary distinction between these scores is as follows:\n\n1. For Helpfulness, the ground truth answer is provided as a reference, allowing GPT-4 to determine which model's output effectively addresses the user's question.\n2. For Grounding, the original captions from COCO and Localized Narratives are given, enabling GPT-4 to decide which one aligns better with the actual content of the image.\n3. The Overall score involves both ground truth captions and answers as input, allowing GPT-4 to evaluate which of the two models' answers is superior in general.\n\n**Conclusion section**: \n\nIn the revised conclusion section, we have incorporated a discussion on potential applications, future directions, and limitations of our method. Specifically, our approach can be employed in HMD AR/VR devices to serve as an egoview copilot for everyone, including visually impaired individuals who may not have clear vision but can still use their gaze to convey their intent. Our method offers greater applicability than similar mobile apps that require users to raise their phones to capture a scene for assistance.\n\nThe limitations and future directions include:\n\n1. Improving inference efficiency to enable instantaneous responses.\n2. Incorporating voice modalities to facilitate seamless interaction.\n3. Supporting higher resolutions to accommodate scenarios requiring OCR and screen or UI understanding.\n\nAdditionally, we have addressed the hallucination problem and discrepancies with GPT-4V in the appendix. By discussing these aspects, we aim to provide a comprehensive understanding of our method's potential and areas for future improvement.\n\n**Other Issues**\n\n1. We have merged Section 5.1.2 with Section 4.1 to streamline the presentation of the related content and improve the overall flow of the manuscript.\n\n2. Regarding the phrase \"superior ability to handle multi-turn real conversations,\" we have updated the statement to clarify its meaning and avoid any potential misunderstandings. The updated statement emphasizes that when appending more in-context conversations, our method's performance continues to improve compared to single-turn baselines. This change ensures that readers have a more accurate understanding of the method's capabilities.\n\n**We hope that our responses have adequately addressed your questions and concerns. If so, we would greatly appreciate it if you could consider increasing your score to further support our work. Thank you!**"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1204/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700425482606,
                "cdate": 1700425482606,
                "tmdate": 1700425482606,
                "mdate": 1700425482606,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "cyuvHIGOZD",
            "forum": "KrOmLMFYHi",
            "replyto": "KrOmLMFYHi",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1204/Reviewer_428q"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1204/Reviewer_428q"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a way to integrate gaze information into large VLMs. It uses mouse trace data as a proxy for gaze track with proper sampling. Such gaze information is then used in an attention mechanism to enhance visual feature perception. The authors report that the proposed approach outperforms baselines."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* Introduces a scalable way to leverage human attention cue in VLM models."
                },
                "weaknesses": {
                    "value": "* Technical or scientific contribution is very incremental and limited.\n* Writing can be improved; not always easy to follow and clear. \n* Baseline methods considered are not comprehensive or fair. Mouse trace data as a proxy for gaze sounds reasonable but there are many off-the-shelf saliency model that are designed to mimic human gaze. Some of the existing saliency model can be used or at least need to be discussed and reviewed in the paper."
                },
                "questions": {
                    "value": "Please address the comments above regarding baseline."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1204/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698819817000,
            "cdate": 1698819817000,
            "tmdate": 1699636046890,
            "mdate": 1699636046890,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "iSvYy4XKCY",
                "forum": "KrOmLMFYHi",
                "replyto": "cyuvHIGOZD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1204/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1204/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 428q"
                    },
                    "comment": {
                        "value": "We would like to express our gratitude to Reviewer 428q for your astute and concise critique of our work. While we appreciate your valuable feedback, we believe that there may be some misunderstandings about certain aspects of our study. To address your concerns, we have provided detailed explanations below and welcome the opportunity for further discussion:\n\n**Baseline Methods:**\n \nWe are grateful for your suggestion to discuss saliency models in our paper. In the revised version, we have included a dedicated discussion of saliency models in Section G.4 and clarified their relationship with our proposed method. However, we would like to emphasize that saliency models, which predict human gaze fixation from images, cannot be directly compared to or used in our approach as baselines. In our task, the ground truth gaze, image, and user query are used as inputs to generate a response, which is a different problem setting.\n\nThe most relevant work in this context involves using gaze as input for generating captions, such as the work by Sugano and Bulling [1]. However, these methods predate the era of large-scale language models and are designed for single-purpose tasks like captioning. As they have not been pretrained on massive datasets, comparing their performance to our modern approach would be unfair, as their performance is undoubtedly lower than ours.\n\nInstead, we have conducted an ablation study to investigate different aspects of the model design space, such as input types, gaze integration methods in VLMs, tuning techniques, and data variations. We believe that these analyses provide a more meaningful baseline for comparison and offer valuable insights into the impact of various design choices on performance. And we have update a clear illustration of those baselines in Figure 13, it may be helpful for you to check.\n\n[1] Yusuke Sugano and Andreas Bulling. Seeing with humans: Gaze-assisted neural image captioning. ArXiv, abs/1608.05203, 2016.\n\n**Writing and Presentation:** \n\nIn response to all reviewers' suggestions, we have extensively reorganized, clarified, and rewritten the manuscript, as well as expanded the material in the revised version as we replied in general response. We hope that these changes address your concerns effectively. We remain open to further suggestions and would appreciate any additional feedback on areas that may require improvement or clarification.\n\n**Technical Contribution** \n\nWe respectfully disagree with your assessment of our work's technical valuation and contribution, and we would like to provide the following reasons for your reconsideration:\n\n1. *Motivation:* As discussed in the revised version, although there is existing research on gaze-related user interfaces and gaze behavior modeling such as saliency models, there has been no investigation into integrating gaze data into modern models and applications. Our work pioneers this area by providing a scalable solution to address this challenge, paving the way for further advancements with high potential.\n\n2. *Effort and Innovation:* We are the first to define a new gaze-enhanced assistance task that is crucial for the upcoming era of spatial computing. We have invested considerable effort in building a reliable automated data annotation pipeline and providing guidance on potential issues that may arise during this process. Additionally, we have developed a comprehensive solution for scalable training and evaluation of models, established new benchmarks using real-life user data, and offered various insights into model design and training through extensive ablation studies. The work presented is far from trivial and should be recognized as a significant contribution.\n\n3. *Contributions to the Research Community:* We plan to release both datasets, the automated data annotation pipeline, pretrained model weights, and training code. Given the challenges in obtaining gaze collection equipment, invoking massive APIs for automated annotation, and fine-tuning and evaluating comprehensive ablations on models with billions of parameters, we believe the resources we are providing will be valuable and directly enhance future research in this area.\n\n**In light of the points mentioned above, we kindly request that Reviewer 482q reconsider the rating for this work. We also welcome any further questions or suggestions you may have.**"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1204/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700425112234,
                "cdate": 1700425112234,
                "tmdate": 1700425112234,
                "mdate": 1700425112234,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ZvrbaW8aoC",
            "forum": "KrOmLMFYHi",
            "replyto": "KrOmLMFYHi",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1204/Reviewer_tDrZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1204/Reviewer_tDrZ"
            ],
            "content": {
                "summary": {
                    "value": "In AR/VR scenarios, gaze is one of the most natural way to represent the regions interesting to users. This paper studied an interesting problem: suppose we are using vision-language model under AR/VR, how to incorporate human gaze attention into vision-language model and how much improvement can it bring? The paper proposed to use mouse trace to approximate gaze and use the collected gaze heatmap into attention mechanism in vision language models (Otter) while freezing the language encoder MPT-7B and vision encoder CLIP ViT-L/14. The models is evaluated on the collected Voila-COCO data set and a VOILA Gaze data which is more close to real life scenarios. The proposed method with extra gaze information outperforms baselines Otter and Kosmos-2. Ablation study also shows that gaze heatmap is better than alternatives ways to use gaze data like discrete gaze position, gaze bounding box as patch, etc."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "--The idea of including gaze information to vision-language model is quite interesting, which might be one important aspect when people use the vision-language model in VR/AR scenarios. The idea of human using gaze/attention in compute vision models is not new, but the idea of using gaze/attention to improve vision-language model is relatively novel to the best of my knowledge.\n\n--Some interesting experiment results are shown to demonstrate that the gaze/attention data are helpful for VQA tasks of vision-language models."
                },
                "weaknesses": {
                    "value": "--Will the data set VOILA-COCO be released? I did not see this information in the paper. \n\n--Using mouse trace to approximate human gaze/attention is a popular approach in attention related area, however, the authors does not mention existing works like BubbleView https://bubbleview.namwkim.org/ or Salicon http://salicon.net/\n\n--The organization and presentation of the paper can be improved. It is not clear how the gaze data will be used in vision-language model until section 4.3. Instead, the authors can provide an illustrator figure about it at the beginning."
                },
                "questions": {
                    "value": "See the weakness part. Especially, the dataset might be an important contribution of this paper. However, it is not clear whether the data set will be released or not."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Responsible research practice (e.g., human subjects, data release)"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "Human gaze data are collected and used."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1204/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1204/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1204/Reviewer_tDrZ"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1204/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699292050922,
            "cdate": 1699292050922,
            "tmdate": 1700683349917,
            "mdate": 1700683349917,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "RZ1iHQBp2C",
                "forum": "KrOmLMFYHi",
                "replyto": "ZvrbaW8aoC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1204/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1204/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer tDrZ"
                    },
                    "comment": {
                        "value": "We appreciate your recognition of our work's ideas and experiments. We would like to address your concerns as follows:\n\n**VOILA-COCO dataset release**\nWe thank the reviewers' interest in the VOILA-COCO and VOILA-GAZE datasets. In response to their inquiries, we would like to confirm our intention to **release both two datasets** upon the publication of our work. Our aim is to foster further research and collaboration within the community by providing these valuable resources. In addition to the datasets, we will also make our annotation pipeline, training code, and model weights publicly available, ensuring that our work is transparent and easily reproducible. To address this concern explicitly, we have included a reproducibility statement in the revised manuscript. We hope that these efforts will contribute positively to the field and facilitate the development of new ideas and methods.\n\n**Ethical Considerations**\n\nOur experiments have been reviewed and approved by the Institutional Review Board (IRB) at our institute. All participants have provided informed consent, agreeing that their data will be used as part of a publicly available dataset. To ensure privacy, all personal information has been properly anonymized. Additionally, we will conduct a thorough review of any potential privacy issues before releasing the dataset to the public. We are commit to upholding ethical standards in research and protecting the privacy of our study participants.\n\n**Existing works on mouse trace approximation**\n\nWe sincerely appreciate your suggestion to consider the relevant background literature on mouse trace approximation. It is encouraging to discover that, despite differing motivations and experimental settings, these works provide robust support for some of our hypotheses. In response to your feedback, we have conducted a comprehensive survey and included an overarching discussion in Section 2, as well as detailed analyses from various perspectives in Appendices G.3 and G.4.\n\nIn our revised manuscript, we have incorporated references to prominent works such as SALICON and BubbleView, and provided a comparative analysis to elucidate the connections and distinctions between our approach and these existing methods. We believe this addition strengthens our paper and offers valuable context for readers. Thank you once again for your valuable input.\n\n**Improved Organization and Presentation** We recognize that the paper's organization and presentation could benefit from enhancements. In the revised version, we have introduced a comprehensive Figure 4 to elucidate the utilization of gaze data in vision-language models. Furthermore, we have restructured the sections pertaining to data collection and result analysis to improve readability and facilitate a better understanding for our readers.\n\n**We hope that our responses effectively address your questions and concerns. If so, we kindly request that you consider raising your score to further support our work. Thank you!**"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1204/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700424191715,
                "cdate": 1700424191715,
                "tmdate": 1700424191715,
                "mdate": 1700424191715,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "s9eKJLDBPT",
                "forum": "KrOmLMFYHi",
                "replyto": "RZ1iHQBp2C",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1204/Reviewer_tDrZ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1204/Reviewer_tDrZ"
                ],
                "content": {
                    "title": {
                        "value": "Resolved most of my concerns"
                    },
                    "comment": {
                        "value": "Increase my rating"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1204/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700683408144,
                "cdate": 1700683408144,
                "tmdate": 1700683408144,
                "mdate": 1700683408144,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]