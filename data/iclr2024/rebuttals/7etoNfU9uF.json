[
    {
        "title": "SpikePoint: An Efficient Point-based Spiking Neural Network for Event Cameras Action Recognition"
    },
    {
        "review": {
            "id": "dx3M2M0AJg",
            "forum": "7etoNfU9uF",
            "replyto": "7etoNfU9uF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4456/Reviewer_tiMk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4456/Reviewer_tiMk"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a spiking neural network that applies to event-driven camera output and is applied to action detection (agents moving in the visual scene). The authors show that their method achieves performance comparable to state-of-the-art methods, but with significantly lower latency and energy consumption."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The method is clearly presented in the paper and is built around the use of point clouds, which are used to represent events. The method uses a relatively classical global architecture that consists in extracting local features, in order to group them intermediately to form a representation that will be efficiently processed by a final classification layer. Overall, the paper is well written and the results are clearly presented."
                },
                "weaknesses": {
                    "value": "A major argument of the paper is to propose a method that deals directly with events that are constituted by the output of an event camera. The authors' argument is to be able to transform events into point clouds and thus improve network performance: \"SpikePoint, is an end-to-end point-based SNN architecture\". However, the figure shows that after the grouping and sampling stage, the information is transformed by coding the firing rate: \"The coordinate is converted into spikes by rate coding, and the results of action recognition are obtained by the local feature extractor, global feature extractor, and classifier in turn\". This point needs to be clearly justified, and in particular why isn't the temporal information kept precisely at this point in the processing process. Is that information rather represented in the previous stages?"
                },
                "questions": {
                    "value": "In addition, I think the paper could be improved by the following points:\n\n- Numerous methods have been developed in the past to study dynamic scenes, such as particle importance sampling, and in particular the \"condensation\" method by Isard and Blake. What parallels do you see between your method and these methods?\n- In Table 7, you show that performance is optimal for a given number of time steps... What can you deduce from this result in relation to the complexity of the data representation?\n\nMinor:\n- \"C represents the set of moments\" - you mean instants?\n- The point \"A detailed derivation can be found in Appendix A.4, which describes how this connection solves the problem of backpropagation.\" is vaguely introduced, please describe minimally the method in the main text.\n- The syntax of the paper did not allow me to fully follow all arguments. I have not taken this into account in my evaluation, but the authors should use a service, even an automatic one, that allows clarification of certain points. Fix for instance \"bionic neurons\" > \"biological neurons\" or vague statements like \"to harmoniously extract local...\", . Also check the sentence \"We do identity mapping by changing the residual module to the following equation in SNN refer (Hu et al., 2021; Fang et al., 2021a; Feng et al., 2022). And the coefficient \u03c3\u2032 (Il+m\u22121 + Sl ) in Eq. 29 of error propagation of the corresponding residual term is canceled.\n- The LaTeX formatting of the paper could be improved. In particular, quotations in the text should be enclosed in parentheses, e.g. using `citep`. Text appearing in equations (\"erf\", \"clip\", \"centroid\", \"lif\", ...) should be formatted as text, e.g. using `\\text``."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4456/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698661946151,
            "cdate": 1698661946151,
            "tmdate": 1699636421105,
            "mdate": 1699636421105,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "MSXu7S0X7K",
                "forum": "7etoNfU9uF",
                "replyto": "dx3M2M0AJg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4456/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4456/Authors"
                ],
                "content": {
                    "title": {
                        "value": "We sincerely  appreciate your recognition of our presentation and your precise summary of SpikePoint.\u00a0  We will provide detailed answers to your questions and hopefully solve your confusion."
                    },
                    "comment": {
                        "value": "**Weaknesses  1. The  inconsistency in our arguments that  \u201cdeal directly with events that are constituted by the output of an event camera\u201d and the arguments that  \u201ctransform events into point clouds and thus improve network performance\u201d.**\n\nThese two arguments are not fundamentally contradictory. We apologize for the misuse of the word \u201cconvert\u201d and have changed the expression \"the event cloud is converted into pseudo-point clouds\" to \u201cthe event cloud is regarded as pseudo-point clouds\u201d in line 119. In the former argument of \u201cdeal directly with events that are constituted by the output of an event camera\u201d, we emphasize that we don't need to stack events through a frame-based method, as this goes against the sparse and asynchronous nature of events. The latter's argument \u201cthe event cloud is regarded as pseudo-point clouds\u201d  also maintains that we do not do extra operations with the points. As shown in Section 3.1 in the manuscript, we treat the timestamps of these points as if they were the z-axis of 3D, with no additional transform operations.\n\n**Weakness  2.\u00a0 SpikePoint is an end-to-end point-based SNN architecture.**\n\nAn \"end-to-end\" architecture, by definition, means that the entire model can be learned and optimized directly from the inputs to the outputs without having to manually design the intermediate steps or features [1].\u00a0  Processes such as Grouping, Sampling, and Coding require no manual intervention and can seamlessly integrate with subsequent components, including the Local Feature Extractor, Global Feature Extractor, and Classifier. Consequently, SpikePoint necessitates solely the input event cloud and corresponding labels to achieve the complete training of the model.\n\n[1] 3d-siamrpn: An end-to-end learning method for real-time 3d single object tracking using raw point cloud[J]. IEEE Sensors Journal, 2020, 21(4): 4995-5011.\n\n**Weakness 3. Why isn't the temporal information kept precisely at this point in the processing process?  Is that information rather represented in the previous stages?**\n\nOur proposed point cloud processing network fundamentally treats temporal information as a dimension equivalent to 'x' and 'y', as depicted in Eq. 2 in the manuscript. This treatment regards explicit temporal information as an implicit form during the feature extraction process. While lacking the direct temporal context of an RNN, the extracted features inherently encapsulate temporal information through this unique dimensional representation.\u00a0  Our evaluation results on the five datasets also demonstrate the feasibility and superiority of this approach.  Temporal information ('t') is represented as an implicit feature in the entire model along with spatial information ('x', 'y').\n\n**Q1. What parallels do you see between your method and these methods?**\n\nWe express our sincere appreciation for this question and bring the excellent work to our attention! Sampling is a critical process, and it serves as the foundational mechanism for data acquisition and subsequent analysis.\n\nThe random sampling employed in our paper for event cloud sampling, as well as the farthest point sampling method utilized in SpikePoint, are characterized by their simplicity and straightforwardness.  We adopt this sampling method the same as PointNet ++ and cite on line 94. \n\nIn the Condensation method, factored sampling entails randomly selecting particles from a prior distribution, updating their weights based on observed data, and using the weighted particles as a representation of the posterior density for state estimation. The condensation-based approach based on the probabilistic model,  possesses the capability to effectively track the recognize [2-4].\u00a0\n\nWe believe the condensation method could serve as a potential replacement for our aforementioned sampling methods, thereby augmenting the performance of SpikePoint. We anticipate that this exemplifies how traditional vision and learning-based vision methods can synergistically complement each other. We thank the reviewer again for pointing out this method and we will explore this approach in our future work.\n\n[2] Isard M, Blake A. Condensation conditional density propagation for visual tracking[J]. International journal of computer vision, 1998, 29(1): 5-28.\n\n[3] A probabilistic framework for matching temporal trajectories: Condensation-based recognition of gestures and expressions[C]ECCV'98.\n\n[4] Dynamic gesture recognition by using CNNs and star RGB: A temporal information condensation[J]. Neurocomputing."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4456/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700036841929,
                "cdate": 1700036841929,
                "tmdate": 1700036841929,
                "mdate": 1700036841929,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "2D3mR7miV6",
            "forum": "7etoNfU9uF",
            "replyto": "7etoNfU9uF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4456/Reviewer_qCNh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4456/Reviewer_qCNh"
            ],
            "content": {
                "summary": {
                    "value": "In this study, the authors present a spiking neural network tailored for event-based action recognition, utilizing event cloud data. The designed network adeptly captures both global and local features. Notably, the introduced method sets new benchmarks by achieving state-of-the-art results on four distinct event-based action recognition datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The proposed method is novel and interesting. \n\nThe proposed method achieves sota performances on four event-based action recognition datasets."
                },
                "weaknesses": {
                    "value": "1. It is suggested to explain why employing the ResFB in the local extractor and the ResF in the global extractor.\n\n2. Regarding the experiments conducted on DVS Gesture, please specify whether the setting encompasses 10 classes or 11.\n\n3. For clarity in Table 1, it would be more efficient to consolidate all pertinent information within a single row.\n\n4. Could you clarify the term \"Single-stream\"? Based on Figure 1, the entire network appears to consist of two distinct streams.\n\n5. In the related work section, consider incorporating more contemporary research related to both 'event-based action recognition' and 'point cloud network in ann'.\n\nMinor issues:\n\nThere's an inconsistency in the experimental outcomes for SEW-Resnet as presented in Table 2 and Table 6."
                },
                "questions": {
                    "value": "Please refer to 'Weaknesses'."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4456/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4456/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4456/Reviewer_qCNh"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4456/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698742835334,
            "cdate": 1698742835334,
            "tmdate": 1699636421030,
            "mdate": 1699636421030,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ANRFeh67uI",
                "forum": "7etoNfU9uF",
                "replyto": "2D3mR7miV6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4456/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4456/Authors"
                ],
                "content": {
                    "title": {
                        "value": "We sincerely appreciate your comments and your recognition of this paper\u2019s novelty and significance. We would like to address your questions with the utmost diligence and clarity."
                    },
                    "comment": {
                        "value": "**Weakness 1. It is suggested to explain why employing the ResFB in the local extractor and the ResF in the global extractor.**\n\nExploring different blocks for local and global extractors is one essential design within our whole architecture. As shown in Fig. 1, the input dimension of local and global extractor differs drastically: the local feature extractor\u2019s input and output dimensions are [M, N\u2019, D]=[1024\uff0c24\uff0c6], and [M, N\u2019, D1] =[1024, 24, 32], respectively. The global feature extractor\u2019s input and output dimensions are  [M, D2]=[1024\uff0c32] and [M\uff0c D4]=[1024, 256], respectively. We designed ResFB and ResF for the local and global extractors tailored to accommodate this difference in input and output dimensions of the two feature extractors. \n\nThe global feature extractor\u2019s output is directly fed into the classifier, and we need to expand the output dimension and ensure the advanced nature of features to guarantee the effectiveness of the classifier. To expand the output dimension from 32 to 256, we applied 3 Conv1D layers and 2  ResF blocks.\n\nAs for the ResFB,  there are a large number of the local feature extractor\u2019s inputs, and these inputs need to go through the shared weight local feature extractor N' x M times for extracting the local features. In order to deal with the problem, we introduce the bottleneck structure with a compression Conv1D layer and an expansion Conv1D layer to efficiently extract features while minimizing computational resources.\n\n**Weakness 2. Regarding the experiments conducted on DVS Gesture, please specify whether the setting encompasses 10 classes or 11.**\n\nWe should have been more clear with our experiments. We use 11 classes for model evaluation, which is consistent with other comparative work such as PLIF, SEW-Resnet, and Spikingformer.\n\n**Weakness 3. For clarity in Table 1, it would be more efficient to consolidate all pertinent information within a single row.**\n\nWe apologize for adopting this formatting due to page limitations, we have changed it in the revised version.\n\n**Weakness 4. Could you clarify the term \"Single-stream\"? Based on Figure 1, the entire network appears to consist of two distinct streams.**\n\nThe mainstream ANN-based Point Cloud methods often utilize a hierarchy structure with multiple stages. Whereas SpikePoint has only one stage. A stage contains both global and local feature extraction, first seen in  PointNet++, the pioneer of point cloud networks, in the paper named Set Abstraction (SA). Additionally, the hierarchy structure with many stages is not readily applicable to SNNs and often performs inferior compared to the singular stage. This is because spike-based features tend to become sparse and indistinguishable as the depth of the stage increases and the training method based on backpropagation has a serious gradient problem.\n\n**Weakness 5. In the related work section, consider incorporating more contemporary research related to both 'event-based action recognition' and 'point cloud network in ann'.**\n\nWe sincerely appreciate your valuable suggestion. In response, we have added the last two years of works in Sections 2.1 and 2.2 of the revised version.\n\n[1] Starting From Non-Parametric Networks for 3D Point Cloud Analysis[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023: 5344-5353.\n\n[2] Pointnext: Revisiting pointnet++ with improved training and scaling strategies[J]. Advances in Neural Information Processing Systems, 2022, 35: 23192-23204.\n\n[3] MPCT: Multiscale Point Cloud Transformer with a Residual Network[J]. IEEE Transactions on Multimedia, 2023.\n\n[4] TTPOINT: A Tensorized Point Cloud Network for Lightweight Action Recognition with Event Cameras[C]//Proceedings of the 31st ACM International Conference on Multimedia. 2023: 8026-8034.\n\n[5] Sparser spiking activity can be better: Feature Refine-and-Mask spiking neural network for event-based visual recognition[J]. Neural Networks, 2023, 166: 410-423.\n\n[6] EventMix: An efficient data augmentation strategy for event-based learning[J]. Information Sciences, 2023, 644: 119170.\n\n**Minor issues: There's an inconsistency in the experimental outcomes for SEW-Resnet as presented in Table 2 and Table 6.**\n\nWe apologize for the misunderstanding.  Indeed the SEW-Resnet in Table 2 and Table 6 are from different works. The former has higher accuracy,  but the power consumption is missing. We cite the former in Table 2 to compare the accuracy.\u00a0 The latter is from [7], they provide the power consumption details. We change the names and make them easily distinguishable.\n\n[7]Going deeper with directly-trained larger spiking neural networks[C]//Proceedings of the AAAI conference on artificial intelligence. 2021, 35(12): 11062-11070.\n\nThank you once again for your review and for asking highly professional questions. We sincerely hope that our answers have successfully resolved any confusion you may have had."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4456/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700036971700,
                "cdate": 1700036971700,
                "tmdate": 1700036971700,
                "mdate": 1700036971700,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fN0BAfz8Yz",
                "forum": "7etoNfU9uF",
                "replyto": "ANRFeh67uI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4456/Reviewer_qCNh"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4456/Reviewer_qCNh"
                ],
                "content": {
                    "title": {
                        "value": "Discussion"
                    },
                    "comment": {
                        "value": "The authors' response effectively tackled my concerns. I will maintain my rating as 'weak accept.'"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4456/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700631153110,
                "cdate": 1700631153110,
                "tmdate": 1700631153110,
                "mdate": 1700631153110,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "AzddBWX9gf",
            "forum": "7etoNfU9uF",
            "replyto": "7etoNfU9uF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4456/Reviewer_NnWo"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4456/Reviewer_NnWo"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes an SNN framework for event stream processing, termed SpikePoint. It first processes the event stream into point groups and encodes using the rate coding method. Then, the local and global feature extractors are proposed to learn the deep features based on spiking activation neurons.\n\nthe writing of this work needs further polishment; a lot of typos can be found all through the paper;\nthe idea of pure snn for event point stream processing is not new; as the key components are all off-the-shelf modules;\nthe experiments on large-scale event-based recognition datasets are missing; which is hard to judge whether the proposed method works."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper proposes an SNN framework for event stream processing, termed SpikePoint. It first processes the event stream into point groups and encodes using the rate coding method. Then, the local and global feature extractors are proposed to learn the deep features based on spiking activation neurons."
                },
                "weaknesses": {
                    "value": "the writing of this work needs further polishment; a lot of typos can be found all through the paper;\nthe idea of pure snn for event point stream processing is not new; as the key components are all off-the-shelf modules;\nthe experiments on large-scale event-based recognition datasets are missing; which is hard to judge whether the proposed method works."
                },
                "questions": {
                    "value": "1. further polish this paper; \n2. re-organize the contributions of this work, as the current version does not shown significant difference with existing works;\n3. more experiments on large-scale event datasets are needed."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4456/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698763746063,
            "cdate": 1698763746063,
            "tmdate": 1699636420952,
            "mdate": 1699636420952,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7lnrCBh9wh",
                "forum": "7etoNfU9uF",
                "replyto": "AzddBWX9gf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4456/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4456/Authors"
                ],
                "content": {
                    "title": {
                        "value": "We appreciate your reviews and respect the reasons provided for the rejection. We have provided a comprehensive response addressing the weaknesses raised and have revised the manuscript accordingly. We remain hopeful that it could potentially change your decision."
                    },
                    "comment": {
                        "value": "**Q1. Further polish this paper.**\n\nWe sincerely appreciate your feedback on the paper writing. All the authors at SpikePoint have diligently checked and polished it, and we have uploaded the revised version to the rebuttal version.\n\n**Q2. The current version does not show a significant difference from existing works.**\n\nYou mentioned that \u201cThe idea of pure SNN for event point stream processing is not new.\u201d We agree that there have been many event-processing SNN networks in literature. However, to our best knowledge, all these SNNs are frame-based, resulting in the loss of fine-grained temporal information. We pioneered the application of SNNs to process event streams specifically for Point-based.  The effectiveness of our approach is corroborated by the fact that our results surpass all existing SNN networks. As summarized in the manuscript and also mentioned by other reviewers the novelty includes:\n\n1. We propose the first SNN-based framework for handling event-based data using the point cloud representation.\n\n2. We innovate a novel encoding method that effectively captures and encodes negative values pertaining to the relative positioning within the point cloud.\n\n3. We introduce a streamlined, singular-stage processing framework designed for the efficient adaptation of SNNs.\n4. We propose powerful local and global feature extractors that can extract features within and between event groups.\n5. We performed comprehensive evaluations on different scales of event-based action recognition datasets, and SpikePoint achieved SOTA in SNN on all five datasets.\n\nWe should have given a clearer explanation of the SNN architecture and its novelty. We have reorganized the manuscript in the last paragraph of the introduction.\n\n**Q3. More experiments on large-scale event datasets are needed.**\n\nWe appreciate your invaluable suggestions. Subsequently, we have conducted additional experiments using the large-scale event-based UCF101-DVS dataset, comprising 101 classifications, with a dataset size of nearly 150k.  The preliminary results are summarized in Table 1 below. We have expanded the manuscript to include the results in Section 4.4.\n\n\nTable 1: SpikePoint's performance on UCF101-DVS\n| Name | Method | Param | Acc |\n| -------- | -------- | -------- | -------- |\n| RG-CNN+Incep.3D  [1]   | ANN     | 6.95M     | 63.2%     |\n| I3D     [2]| ANN     | 12.4M     | 63.5%     |\n| ResNext-50  [3]   | ANN    | 26.05M     | 60.2%     |\n| ECSNet-SES  [4]  | ANN     |  -     | 70.2%     |\n| Res-SNN-18    [5] | SNN    |  -     | 57.8%     |\n| RM-RES-SNN-18 [6]    | SNN     | -     | 58.5%     |\n| **SpikePoint**     | **SNN**     | **1.05M**     | **68.46%**     |\n\n\nAs illustrated in Table 1, SpikePoint outperforms the state-of-the-art Spiking Neural Networks with a precision of 68.46%. Notably, it represents the latest advancement in the field, building upon RM-RES-SNN [6].  SpikePoint also outperforms the vast majority of ANN's work and is very close to ANN's SOTA. This large-scale experiment further demonstrates the generalization and effectiveness of SpikePoint. Moreover, great performance is achieved with a parameter of only 1.05M,  9.4% of SOTA SNN highlighting the compactness.\n\n[1]Graph-based spatio-temporal feature learning for neuromorphic vision sensing. TIP 2020.\n\n[2]Action recognition? a new model and the kinetics dataset. CVPR 2017.\n\n[3]Can spatiotemporal 3d cnns retrace the history of 2d cnns and imagenet? CVPR 2018.\n\n[4]Ecsnet: Spatio-temporal feature learning for event camera.TCSVT 2022.\n\n[5]Deep residual learning in spiking neural networks. Neurips 2021.\n\n[6]Sparser spiking activity can be better: Feature refine-and-mask spiking neural network for event-based visual recognition. Neural Networks 2023.\n\nFinally, we would like to express our gratitude for your time and effort once again. We sincerely hope that our answers have successfully resolved any confusion you may have had."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4456/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700036995247,
                "cdate": 1700036995247,
                "tmdate": 1700036995247,
                "mdate": 1700036995247,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DuGcosXVtq",
                "forum": "7etoNfU9uF",
                "replyto": "7lnrCBh9wh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4456/Reviewer_NnWo"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4456/Reviewer_NnWo"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your response to my questions. \nAfter reviewing the comments of other reviewers and the responses, I may change my rating to a higher score. However, a question still needs to be considered: if the application on event point other than the event frame novel enough for ICLR?"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4456/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700703444072,
                "cdate": 1700703444072,
                "tmdate": 1700703444072,
                "mdate": 1700703444072,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "uFgM6SdYH1",
            "forum": "7etoNfU9uF",
            "replyto": "7etoNfU9uF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4456/Reviewer_XXP9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4456/Reviewer_XXP9"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a novel and efficient network approach for event based action recognition. The network leveraged spike neural network as backbone. The preprocessing of the events includes grouping, sampling and rate coding to feed in spike format. The grouping takes special consideration to avoid asymmetric information pass-through. The proposed approach also has shown improving the mean relative error and coefficient of variation. \n\nThe SNN learns from both the point cloud centroids and the processed representations. The feature learning part contains both local and global feature extractors as well as residual connection to avoid weight explosion/vanishing.\n\nThe approach has been tested on various datasets including small and large ones. The paper has also compared with SOTA methods for similar tasks. \n\nThe proposed approach has significantly low power consumption, especially compared to other non SNN based networks. The results are strong and the advantages are salient."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "The paper has proposed several novel processing steps accompanied by theoretical derivations. The paper first looked at how to convert the events into SNN acceptable format. One of the issues is that directly normalizing delta positions will result in asymmetric information passthrough. The paper calibrated this offset by using the delta of the absolute values. In the SNN part, the paper incorporated residual learning modules to prevent weight explosion/vanishing. \n\nThe performance of the proposal has been demonstrated on several datasets and has strong improvement over existing methods."
                },
                "weaknesses": {
                    "value": "I don't find notable weaknesses. I only find the proposed methods could also be extended to other relevant tasks, which this paper has deferred to future work. Otherwise, I think the paper results are pretty solid."
                },
                "questions": {
                    "value": "None."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4456/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699249555346,
            "cdate": 1699249555346,
            "tmdate": 1699636420869,
            "mdate": 1699636420869,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "eP3SCE9YxC",
                "forum": "7etoNfU9uF",
                "replyto": "uFgM6SdYH1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4456/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4456/Authors"
                ],
                "content": {
                    "title": {
                        "value": "We sincerely appreciate your recognition of spikepoint's methods, experiments, and performance!"
                    },
                    "comment": {
                        "value": "As a powerful tool to process event data, we believe SpikePoint could extend to more application scenarios. We are endeavoring to broaden the application scope of SpikePoint. We are still working on several applications, such as camera pose relocalization, eye-tracking, and object detection.  Also, SpikePoint also has the potential in low-level event-based tasks such as deblur. Our long-term goal is to run demonstration with our point cloud-based methods on real applications with ultra-low-power systems.\n\nFinally, we extend our heartfelt gratitude once again for acknowledging our research. If you have any new questions or suggestions, we will respond and discuss them with you in a prompt manner."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4456/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700037004669,
                "cdate": 1700037004669,
                "tmdate": 1700037004669,
                "mdate": 1700037004669,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]