[
    {
        "title": "Learning invariant representations of time-homogeneous stochastic dynamical systems"
    },
    {
        "review": {
            "id": "lbHV6dGFeK",
            "forum": "twSnZwiOIm",
            "replyto": "twSnZwiOIm",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2492/Reviewer_UfpG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2492/Reviewer_UfpG"
            ],
            "content": {
                "summary": {
                    "value": "The authors provide a method to learn an invariant representation for dynamical systems helps give an approximation of the transfer operator. They do this while also alleviating weaknesses of previous approaches, such as metric distortion. To do this, the authors show that the search for a good representation can be cast as an optimization problem over the space of neural networks. They provide and an objective function, and tractable approximations to it. They compare their method to other state-of-the-art approaches."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The approach seems novel and interesting enough.\n- The authors list the weaknesses of previous approaches (e.g. the weaknesses of previous methods on page 3, under \"Learning transfer operators\") and show how their method attempts to alleviate them.\n- While the authors don't optimize over the main objective, equation (4), they provide tractable approximations, and also qualify and quantify when these approximations are good (e.g. Theorem 1, Lemma 3 in Section C of the supplementary materials)."
                },
                "weaknesses": {
                    "value": "- The organization and the explanation of the method could be more straightforward. For example, Algorithm 1 is rather terse, and a more expanded version would be instructive to the reader. It's hard to get birds-eye view of the method because the explanation is spread throughout the text. Perhaps a separate section in the supplementary materials that gives a more direct explanation of the method would help the reader.\n- The method requires computing covariance matrices of neural network outputs, which could affect the computation if the output dimension is high.\n- I have some reservations about the experiments:\n    - In the beginning paragraph of section \"5 Experiments\", the authors state they only optimize over the learning rate to keep things fair, but I can't seem to find the set of learning rates that were optimized over.\n    - On the previous point, the authors state at the top of page 7 that the mini-batch size should be sufficiently large to mitigate the impact of biased stochastic estimators. But, some neural network models benefit from the mini-batch stochasticity of SGD, as opposed to full-batch normal gradient descent. In this case, I would have liked to have seen optimizing over the batch sizes as well, and not just the learning rate.\n    - I'm also not sure if optimizing over just the learning rate this is completely fair. I would have liked to see optimizing over more hyperparameters, or at least in addition to the current experiments, to also provide the performance evaluation of the original model implemented in the paper, if possible. For example, in the fluid flow example, Azencot et al. (2022) has a similar fluid flow example, but uses a different architecture and I would have liked to see this one in comparison to the author's method.\n    - In the molecular dynamics example, I see Ghorbani et al. (2022) have done similar experiments, but at a smaller scale. Even though the authors performed their experiment at a larger scale, I would have liked to have seen a comparison with Ghorbani et al. (2022) at the smaller scale, or even at the larger scale. This would allow comparing accuracy and training times."
                },
                "questions": {
                    "value": "**Questions:**\n- It seems in most experiments, the output dimension of the neural networks, $r$, is low. Would this be the case for most practical applications?\n- On page 7 in \"Downstream tasks\", in order to make predictions of the next step, the authors need to compute: $(\\hat{\\Psi}_{w}^\\top)^\\dagger$, i.e. the pseudo-inverse, is that right? If so, it seems this would slow down the computation of the predicted next steps, so how do the computation times compare with other methods for predicting next steps? Any numerical instabilities?\n- In Algorithm 1, the authors use $\\hat{\\mathcal{S}}^\\gamma_m$, implying this is the relaxed version, is that right? If so, this isn't stated anywhere in Algorithm 1. Perhaps the authors should have two Algorithms that gives the regular DPNets and also DPNets-relaxed.\n- When training the unrelaxed DPNets, how was $\\mathcal{P}^\\gamma$ evaluated? It is stated in page 5 that in general the covariances are non-invertible and thus $\\mathcal{P}^\\gamma$ is non-differentiable during the optimization. Also, as the authors state, the use of the pseudo-inverse can introduce numerical instabilities. How much of a problem is this, in your experience? I see that some experiments were unable to use unrelaxed DPNets because of this.\n- For Figure 3, in the upper panel - \"Ordered MNIST accuracy\", I'd imagine that the \"Reference\" (dashed-line) is just random guessing, i.e. $1/5 = 0.2$? I don't see this stated anywhere, and would like it explicitly stated.\n- For Figure 3 in the lower panel, \"Fluid Flow RMSE\", how many prediction steps were taken? Namely, what are the values of the $x$-axis?\n\n**Remarks:**\n- On page 5, on the line above equation (8), the authors say \"we show in Lem. 3 in App. B\", but I think you mean Section C in the appendix/supplementary materials.\n- On page 6, in \"Learning the representation\", might also need to state that we are also given $\\psi'$, i.e. \" In practice,given a NN architecture $\\psi$ and $\\psi$'. \""
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2492/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2492/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2492/Reviewer_UfpG"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2492/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698737320836,
            "cdate": 1698737320836,
            "tmdate": 1699636185992,
            "mdate": 1699636185992,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zLLkhy4G4X",
                "forum": "twSnZwiOIm",
                "replyto": "lbHV6dGFeK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2492/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2492/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We gratefully thank the reviewer for their review and comments, which we now address.\n\nWeaknesses:\n- As you suggested, we have included an expanded version of Algorithm 1 in the supplementary section. Furthermore, upon completion of the review process, we intend to publicly share the GitHub repository containing not only the method's implementation but also the implementation of each experiment. \n- Thank you for raising this important point. Please see our global reply where we address this point in detail, as well as our answer to reviewer **hdvQ** for further comments on the computational complexity\n- Experiments:\n\t- The grid used for the search of the best learning rate was reported in appendix F: 100 equally spaced values in the interval $(10^{-6}, 10^{-2})$. To aid reproducibility, in the updated version we also added a table with the learning rate found by the search for each (experiment, method) pair.\u00a0\n\t- We believe this is a valuable suggestion, and while we keep the batch-size for DPNets and VAMPNets as high as possible to not incur any bias upon estimation of the score function (see our discussion in Sec 4), we are in the process of re-running the Auto-Encoder methods with smaller batch-sizes. We will give an update on this point as soon as the new results are available.\n\t- Given the number and diversity of the baselines (each with a different set of hyperparameters) the evaluation methodology we devised is to fix every parameter defining the model expressivity (architecture, feature-dimension) and train the best possible model within a budget of epochs. Hence we optimized learning rates and (thanks to your suggestion) batch sizes. Taking a closer look at the fluid flow example in Azencot et al. (2022), one can see that our setting is remarkably close to theirs: almost identical underlying PDE, same Reynolds number, as well as similar size of the dataset. With the exception of a different activation function, we also use a similar MLP architecture (though the one in our experiment is slightly bigger, with 3.8 million parameters compared to theirs which is 2.49 millions). Finally, our implementation agrees with their findings, for which in this experiment they expect results in line with dynamical AEs (see our Fig. 1).\n\t- The experiment in Ghorbani et al. (2022) considers different proteins. Their method (which is equivalent to VAMPNets) fails on the larger scale Chignolin experiment as we report in Sect. 5. On the other hand applying our method to their smaller scale dataset should be doable, although the hyperparameter tuning may be time consuming. While we cannot guarantee to have it done in the time span before the end of the rebuttal period, we will definitely include this experiment in the revised version.\nQuestions:\n\n- The answer is yes. Please see our global reply for an in-depth discussion of the computational complexity of using covariances.\n    \n- That formula is just for definition purposes. In practice we simply compute it as $(\\widehat{\\Psi}^\\top_w)^\\dagger \\widehat{F}\u2019 =( \\widehat{\\Psi}^\\top_w \\widehat{\\Psi}_w)^\\dagger \\widehat{\\Psi}_w^\\top\u00a0 \\widehat{F}\u2019$, which is equivalent to solving $\\ell$ least squares with $r\\times r$ matrix $\\widehat{\\Psi}^\\top_w \\widehat{\\Psi}_w$. Finally, notice that this is performed only once when fitting the operator regression model. Once computed it can be stored and used to predict future states through simple matrix multiplications. Additionally, low rank estimators, as well as Nystr\u00f6m based methods are available to considerably speed up this initial fitting cost even when $r$ or $n$ are very large (see Meanti et al. 2023).\u00a0\n- In Section 4, just before the \u201cOperation regression\u201d paragraph we actually state: In Alg. 1 we report the training procedure for DPNets, which can be also applied to the other scores (8) and (14) in step 4, respectively. When using mini-batch SGD methods to compute \u2207 \u0302 P\u03b3 m(w) or \u2207 \u0302 S\u03b3 m(w) via backpropagation at each step, m \u2264 n should be taken sufficiently large to mitigate the impact of biased stochastic estimators\u201d. As you point out, the algorithm is for DPNets-relaxed, while in the main text we stated that it is for DPNets. We have now corrected this.\n- $\\mathcal{P}^\\gamma$ was evaluated using `torch.pinv` with the \u201chermitian\u201d flag set to True. The problem of using pseudo-inverses lies in the backpropagation step, as the pseudo-inverse is known not to be differentiable whenever the rank is not stable over the parameter space (see Golub & Pereyra, 1973). On the other hand, on the forward pass, the torch implementation seemed to be stable enough, and when we weren\u2019t able to train DPNets (unrelaxed) and VAMPNets it was a failure of the back-propagation step.\u00a0\n- This is right, we are now stating it explicitly, thank you.\n- 40 prediction steps, that is the full test set.\n\nRemarks:\n- That\u2019s right. We have now fixed it, thank you.\n- Absolutely. Fixed it as well in the newly uploaded draft."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2492/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700059476752,
                "cdate": 1700059476752,
                "tmdate": 1700059476752,
                "mdate": 1700059476752,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Y0GpQIHjtZ",
                "forum": "twSnZwiOIm",
                "replyto": "lbHV6dGFeK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2492/Reviewer_UfpG"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2492/Reviewer_UfpG"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for their detailed response, revisions, and additional experiments. I have read the other reviews and the discussions that followed (as of this writing).\n\nMy questions and concerns have been mostly addressed. Thank you for the reference, although I'll note that in reviewing the reference and in the context of the submission, they mention when training in large batch regimes, it may help to tune other hyperparameters as well, as they can differ from their small batch regimes (e.g. learning rate, momentum, etc.). In fact, in Section 4.7 of the reference [(Shallue, 2019)](https://jmlr.org/papers/volume20/18-789/18-789.pdf) they explicitly state: \n\n> \"We also found that the best effective learning rate should be chosen by jointly tuning the learning rate and momentum, **rather than tuning only the learning rate.**\" [emphasis mine]\n\nSo it may help to also tune the momentum if the authors also believe that will be productive. But I do recognize and sympathize that hyperparameter tuning can be computationally expensive.\n\nBut my current thoughts are: after reading the response and the other reviews, and due to the limited gradations of the scoring system (score would go up if there were sufficient gradations), I am inclined to have my score remain unchanged. I acknowledge the hard work put into the rebuttal and revisions, and am very grateful and extremely appreciative to the authors."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2492/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700537173745,
                "cdate": 1700537173745,
                "tmdate": 1700537897270,
                "mdate": 1700537897270,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "rBO3BikRlF",
            "forum": "twSnZwiOIm",
            "replyto": "twSnZwiOIm",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2492/Reviewer_1QLL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2492/Reviewer_1QLL"
            ],
            "content": {
                "summary": {
                    "value": "In this paper a particular representation of a state space of a dynamical system is learned. \nThe representation is intended to correspond to the subspace of highest singular values of the transfer operator of the system, of a fixed predifined dimension. It is learned from the observed trajectories of the system and parametrised by neural networks. \n\nThe corresponding optimisation objective can be expressed in terms of the covariance matrices of the data (in the representation space).  While the direct involves matrix inversion and is computationally difficult, a proxy objective, which is proven to be a lower bound, is proposed. The proxy objective has better computational properties. \n\nOnce the representation is learned, the transfer operator can be learned using standard approaches based on operator regression.  Experiments are performed to demonstarte a competitive performance of the approach."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper is clearly written and the topic of modeling dynamical systems is important. \nThe proposed representation space is very natural."
                },
                "weaknesses": {
                    "value": "This is mainly an empirical paper, as it proposes to model a certain space using neural networks with gradient descent optimsed objective.  However, the empirical evaluation of the method is not completely convincing, and some points are not clear.  \n\n\nIn addition, even the relaxed objective introduced in the paper is computationally heavy. It involves \ncomputing the covariannce matrices as _differentiable functions_. This means the computation graph involves summing $n$ matrices of size $r \\times r$, each coordinate of which is a differnetiable function. \nAs noted in the paper, minibatches could be used but would introduce bias. Even with minibatches, the \nrepresentation dimension $r$ would have to stay low, and appears to be low in all experiments (in which it was specified). \n\n\nA more detailed notes on experiments:\n\n**Ordered MNIST:** My understanding is that this example is new in this paper. Is there an explanation/intuition why other methods fails here? Are convolutional networks used to model the \nrespresentation? I assume the non NN methods fail simply since representing images is easier with convolutions. Why do other methods fail? I believe explaining the difference here could make the paper considerably stronger. \n\n\n**Logistic Map:** The takeaway here is not clear.  DPNets has the worst spectral error among all methods. \nIn addition, there appears to be no theoretical reason why DPNets-realxed should perform better on this metric, as it was introduced as a computational relaxation. Do the suggest it has properties allowing it to be a better estimator in general? \n\n**Continuous dynamics:**\nThe scale of Fig. 2 lower pane is that of 200. This makes is very hard to understand whether the smaller \neigenvalues are estimated accurately and what is the order of magnitude.  Displaying ratios estimated/true would be more informative. Also, are there other ways to estimate the eigenvalues? \n\n\n**FlowRMSE:**\nThe general magnitude of the quantities in this experiment is not clear.  If the quantities are of order 10, then the difference between errors of order 0.1 and 0.001 is less impactful, and all methods perform well. \nWhat are the typical values of the quantities for which RMSE is computed? Again, percentage ratios would be much clearer than absolute values. \n\n\n\n**Chignolin Experiment:**\n In Table 2, describing the Chignolin experiment, while estimated quantities are somewhat closer to the reference values than the true quantities, they still look quite far. It is thus not clear what is the point of this part of the experiment.  \nThe second part of that experiment, involving Fig.4, unfortunately can not be understood from the main text. (what is the free energy surface, what is known about its proper minima, why matching the results of Bonati et al   is of interest? )."
                },
                "questions": {
                    "value": "In addition to the points above:\n\nIs the relaxation objective (9) a new contribution of this paper, or were similar relaxations employed in previous work (for instance VAMPSNets, DeepCCA)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2492/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2492/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2492/Reviewer_1QLL"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2492/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698781097815,
            "cdate": 1698781097815,
            "tmdate": 1699636185888,
            "mdate": 1699636185888,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "JgdqtsfZR0",
                "forum": "twSnZwiOIm",
                "replyto": "rBO3BikRlF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2492/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2492/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We gratefully thank the reviewer for their review and comments, which we now address.\u00a0\n\nWhile we address the specific comments on the empirical evaluation of the methods below, we respectfully disagree on the point that the paper is \u201cmainly empirical\u201d. Our aim is to provide a method firmly grounded in state-of-the-art statistical learning theory, as evidenced by our detailed discussions in Sec. 2 and 3, as well as App. A\u2013E, outlining the theoretical foundations of DPNets. In reply to your question, the metric distortion loss $\\mathcal{R}$, as well as the relaxed score in eq. (9) haven\u2019t been proposed before and are novel contributions. We will be happy to discuss any theoretical question/comment, should it arise during the rebuttal period.\n\nThe point on the high cost of evaluating covariances is important, and we are grateful to the reviewer for raising it. In this respect, we refer to the general answer for a detailed account on why we expect that in many scenarios a low dimension to be sufficient and what one can do if, for any reason, the representation dimension needs to be large.\u00a0\n\nWe also refer to point 1 of our answer to reviewer **hdvQ** for further comments on the computational complexity.\n\nOn the specific experiments:\n\n**Ordered MNIST**: this example was actually introduced in Kostic et al. 2022. In the revised version of the manuscript we now acknowledge it, thanks for pointing it out. In our DPNets methods as well as in VAMPNets, oracle-features, and in the AE baselines we use the same CNN (the specific architecture is reported in App. F.4), and as you said non-NN methods fail because representing images is easier with convolutions. VAMPNet, which is the closest to DPNet in terms of methodology, fails to converge (see our discussion on its instability in Sec 3). The DynamicalAE actually learns a reasonable representation, making it the fourth best model behind DPNets and the oracle-features (which, however, are trained using also the true labels). Upon visual inspection, indeed, DynamicAE returns readable images (see the Figure on the very last page of the Appendix). Finally, we speculate that ConsistentAE does bad in this setting as they also enforce a \u201cconsistency\u201d constraint for the time-reversed system, which is not satisfied by the true dynamics of this example. Finally, even though oracle-features have been trained using the true labels, they are oblivious of the dynamics of this system. This is why, even attaining very good performances, they eventually fail at long term prediction.\n\n**Logistic Map**: We do not have any indication that DPNets-relaxed provides a better estimation in general. Indeed, for larger feature dimensions, the unrelaxed DPNets is on par with DPNets-relaxed. To clarify the takeaway of this experiment, we significantly expanded the App. F.1 with an in-depth discussion of the inherent difficulty when estimating the spectra (high sensitivity due to nonnormality of the operator) and the role of the dimension of the feature map, which is particularly important in this context.\n\n**Continuous dynamics:** For improved readability we have changed the scale to logarithmic, and plotted the implied transition rate, that is the absolute value of the non-null eigenvalues. We are only aware of the alternative methods presented in (Klus et al. 2019) and (Hou et al 2023), which correspond to DMD and kernel-DMD, respectively, adapted for learning the generator. Our example corresponds to using the DMD method coupled with a DPNet representation.\u00a0\n\n**Fluid flow**: We completely agree with your comment, and in the updated draft we plot the results in terms of ratios as you suggest. Working with RMSE, we find it sensible to use the standard deviation of the data (which is approximately ~ 0.1675) as a proxy for the general magnitude of the quantities involved.\u00a0\n\n**Chignolin**: Indeed, quantities such as the free energy are very specific to the context of physical sciences. In the revised version of the draft we added an explanation of what free energy is, and how it relates to the metastable states. Concerning your question about the estimated quantities being far from the true ones, it basically boils down to the amount of data at our disposal. Molecular dynamics simulations are great in that they allow to simultaneously probe the dynamics of a physical system, as well as their equilibrium properties if the simulation is long enough. For big systems, however, to accurately estimate equilibrium quantities such as the transition time or the enthalpy one needs extremely long simulations, often out of reach. The trajectory that we used, while large scale, is simply not long enough to entirely capture the equilibrium properties with high accuracy. We point out that speeding up molecular dynamics simulations is an area of active and intense research, and algorithms to estimate the equilibrium properties from short, imperfect simulations are highly valuable."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2492/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700059434615,
                "cdate": 1700059434615,
                "tmdate": 1700059434615,
                "mdate": 1700059434615,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "PEyRpt53FV",
            "forum": "twSnZwiOIm",
            "replyto": "twSnZwiOIm",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2492/Reviewer_sgxX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2492/Reviewer_sgxX"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a relatively new approach to learning the operator for dynamical systems. Instead of one-step modeling of the neural operator, the authors suggest first learning the invariant representations, and then leveraging these learned representations to perform operator regression. The authors assessed their results on various datasets, including an ordered MNIST, a 2D fluid flow, and a molecular dynamics simulation, primarily using metrics such as spectral error and RMSE."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper provides a new way of learning invariant representations of dynamical system.\nThe authors provide theoretical justifications for the derivation of their objectives functions."
                },
                "weaknesses": {
                    "value": "W1. The pipeline design suggests that there might be a possibility of error accumulation from the first phase of learning representations to the second phase of learning dynamics. It would be beneficial if the authors could provide empirical justification to show that their design is comparable to or better than learning the neural operator in a single step. For instance, establishing baselines using Frameworks like FNO or Deeponet would be ideal.\n\nW2. Tying in with W1, the efficacy of the proposed method, particularly the operator regression in the second part, remains uncertain when applied to chaotic systems where acquiring accurate representations in the initial phase may prove challenging\n\nW3. It would be great if the authors could evaluate the performance on insightful metrics like Lyapunov spectrum. And including discussion on recent related works (e.g., https://arxiv.org/pdf/2304.12865.pdf) should be necessary.\n\nW4. If the primary goal is representation learning, I feel that discussions and comparisons related to popular representation learning methods (e.g., contrastive learning) should be discussed and included."
                },
                "questions": {
                    "value": "Q1. I am curious about the empirical performance of the results concerning different hyperparameters, particularly the value of $r$.\n\nQ2. Additionally, what would be the loss for the OLS estimator of the transfer operator across different datasets? It would be concerning if the performance is highly sensitive to the value of $r$.\n\nQ3. For the dynamical data, i.e., the 2D fluid flow and the molecular dynamics, it would be beneficial if the authors could provide the visualizations of the dynamics."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2492/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2492/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2492/Reviewer_sgxX"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2492/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698820550763,
            "cdate": 1698820550763,
            "tmdate": 1700724360027,
            "mdate": 1700724360027,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "U9FnAsEdVp",
                "forum": "twSnZwiOIm",
                "replyto": "PEyRpt53FV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2492/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2492/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We gratefully thank the reviewer for their review and comments, which we now address.\n\nW1. We recall that, among the baselines we analyze, both KernelDMD, DMD, DynamicalAE and ConsistentAE learn the transfer operator in a single step. Indeed, the first two do not require to pre-learn a representation, while the latter two optimize a sum of losses encompassing both representation (i.e. the reconstruction error of the AE) and dynamics. As explained in Sect. 2, and specifically in the paragraph \u201cLearning transfer operators\u201d, off-the-shelf representations often used alongside DMD-alike methods are detrimental in that they induce metric distortion as well as representation biases. In contrast, DPNets are designed to mitigate these effects. Further, the discussion around Eq. (3) entails that for all compact transfer operators, the cumulative error of DPNets plus consistent operator regression is controlled; This is also what we empirically show in every experiment. Does the reviewer agree on this point? We\u2019d be happy to clarify any aspect of our response if needed.\n\nRegarding establishing baselines using FNO/Deep-O-net, we would like to highlight that these techniques are solving a different problem. Although the concept of \u201cneural-operator\u201d is quite broad, the methods you mentioned have been specifically designed to learn surrogate maps for the solution operators of PDEs; see (Kovachki et al., 2023). Moreover, these methods often require knowledge of the underlying physical equations to generate the training data, which is something not at hand for general stochastic processes. Apart from the fluid flow example, for which we already included a physics-informed baseline, the applicability of FNO/Deep-O-net to solve the problem of transfer operator learning remains unclear. We are open to further discussion on this aspect if the reviewer deems it necessary.\n\nW2 and W3. Ergodic theory is usually studied using the geometric approach or the operator theoretic approach. While the first is related to the study of Lyapunov exponents and fractal dimension, the latter is related to the Transfer/Koopman operators of Markovian processes. Connecting these two complementary approaches to ergodicity is an ongoing research, see e.g. (Tantet et al. 2018, Brunton et al\u00a0 2017; 2022, Mezi\u0107 and Avila 2023). Due to the nature of our work, and the central role played by stochastic dynamical systems, we have focused on the spectrum of the transfer operator as relevant information about the dynamics. On the other hand, the Lyapunov spectrum is much more interesting in the context of deterministic, and especially chaotic systems.\u00a0 We can elaborate on both of these approaches in Sec. 1 also mentioning (Platt et al. 2023) as an example of deep learning methods for deterministic systems based on the Lyapunov spectrum approach. Would the reviewer consider this discussion appropriate and to be included in the revision? Finally, concerning the synthetic experiment of Noisy Logistic map we can report that, in the noiseless case the Lyapunov exponent is log(2), while introducing the noise in the model yields the Lyapunov exponent (in the stochastic sense) equal to 0, see (Baxendale, 1989).\u00a0\n\nW4. Since representation learning is a broad field, in the \u201cPrevious work\u201d paragraph we only covered representation learning in the context of transfer operator (Azencot et al., 2020; Lusch et al., 2018; Morton et al., 2018; Otto and Rowley, 2019). We are not aware of contrastive learning approaches in the context of dynamical systems. However, we welcome any suggestion on additional references to be included in the discussion.\u00a0\n\nQ1. In the revised version of the manuscript (Appendix F.1) we added an in-depth discussion on the role of the feature dimension in the Logistic map experiment. It is also interesting to note that DPNet showed to be a quite \u201cparsimonious\u201d representation, needing only a small dimension to well approximate the dynamics. This is particularly relevant also on the ordered MNIST example, where a feature map of just 5 floats achieves > 98% accuracy on validation.\u00a0\n\nQ2.\u00a0 Our DPNets method learns a representation of dimension $r$ and subsequently, an OLS operator estimator can be trained using this representation. To evaluate the quality of the learned $r$-dimensional representation it is not meaningful to look at the OLS error (or risk) but rather to the forecasting error or the spectral error, which we have reported in Q1 for the logistic map. Indeed, although the OLS error seems the natural object to look at, it does not reflect the quality of the representation. For instance, a trivial representation mapping every state to zero would have a zero error. We\u2019d be happy to further clarify any aspect of our response if needed.\n\nQ3. We have added visualizations for both experiments (see Figures 8 and 9)."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2492/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700059345172,
                "cdate": 1700059345172,
                "tmdate": 1700059345172,
                "mdate": 1700059345172,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tnTOJK5Gsf",
                "forum": "twSnZwiOIm",
                "replyto": "PEyRpt53FV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2492/Reviewer_sgxX"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2492/Reviewer_sgxX"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your detailed reply"
                    },
                    "comment": {
                        "value": "I want to thank the authors first for their detailed reply. I acknowledge the authors' replies regarding my W1 & W2. And I appreciate the authors provided additional experiments and visualizations for my Q1 & Q3. I still hold hesitations towards related works and notice Reviewer 1QLL's question regarding the comparison of the objective function against VAMPNets and DeepCCA (for which I have not found the authors' response).\n\nAn update: After careful rethinking and as we are approaching to the closing of the discussion period, I decided to change my score to 6. I feel in general impressed by the authors\u2019 hard work. And I hope further clarifications regarding the novelty in terms of the objective function comparing against previous works, as well as the concerns regarding the computational cost would be added into the revision."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2492/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700717907877,
                "cdate": 1700717907877,
                "tmdate": 1700724342917,
                "mdate": 1700724342917,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "tCKFi69Hum",
            "forum": "twSnZwiOIm",
            "replyto": "twSnZwiOIm",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2492/Reviewer_hdvQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2492/Reviewer_hdvQ"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a method for learning a representation of continuous and discrete stochastic dynamical systems that achieves state of the art results on many datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper identifies the learning problem as an optimization problem and provides an efficient way to solve it. To overcome singularity, they have a relaxed score function and they also prove that the relaxed score has theoretical guarantees. The proof is mathematically solid.\n\n2. Many experiments are done to verify their claims with impressive results."
                },
                "weaknesses": {
                    "value": "1. The computation for covariances for $\\psi_{w_j}$ and $\\psi'_{w_j}$ might be costly. How does this computation cost compare to the previous methods? Is there a way to speed up?\n\n2. For the continuous-time dynamic, the score function might be unstable. Have you observed this in experiments?"
                },
                "questions": {
                    "value": "See weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2492/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2492/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2492/Reviewer_hdvQ"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2492/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699490989986,
            "cdate": 1699490989986,
            "tmdate": 1699636185692,
            "mdate": 1699636185692,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "RJ3iWRg9vW",
                "forum": "twSnZwiOIm",
                "replyto": "tCKFi69Hum",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2492/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2492/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We gratefully thank the reviewer for their review and comments, which we now address.\u00a0\n\n1. This point was raised by different reviewers and we refer to the general response for a detailed discussion. Concerning the more specific questions about the computational complexity:\n\t1. **DPNets**: the computational complexity for both the forward and backward pass is exactly the same of **VAMPNets**, as the addition of the metric distortion loss $\\mathcal{R}$ (see also Eq. 8) is actually of $\\mathcal{O}(r^2)$, $r$ being the dimension of the representation. Thus, when $ r > n$, the computational complexity is dominated by the (pseudo-)inversion which scales as $\\mathcal{O}(r^3)$. Kernel-based operator regression algorithms such as DMD also share the same (forward) computational complexity, requiring the evaluation and inversion of a covariance ($\\mathcal{O}(nr^2 + r^3)$) or kernel matrix ($\\mathcal{O}(n^3)$). These algorithms do not-require backpropagation, but often fall short in representation power \u2014 as shown in the experiments.\n\t2. **DPNets-relaxed**: The computational complexity of the relaxed score is actually _lower_ than that of DPNets and VAMPNets, since it does not require the full inversion of a matrix, but only its leading singular value (i.e. its operator norm), which can be computed via standard Arnoldi/Lanczos methods, yielding a total complexity of just $\\mathcal{O}(nr^2)$.\u00a0\n2. The optimization of the score in the Langevin dynamics experiment did not show any signs of instability. We speculate that this is due to both the low-dimensionality of the state space and the infinitely-smooth ($C^{\\infty}$) potential function. Indeed, since the score for continuous dynamical systems is the sum of the (finite) eigenvalues of the symmetric eigenvalue problem $C^w_{X\\partial} - \\lambda C^w_X$, in non-pathological cases, its value and it derivatives can be computed efficiently in a numerically stable way, see e.g. (Andrew, A. L. and Tan, R.C.E., Computation of Derivatives of Repeated Eigenvalues and the Corresponding Eigenvectors of Symmetric Matrix Pencils, SIMAX 20/1, 1998), ). We have included this remark in the revised manuscript. Notwithstanding these favorable conditions, we already acknowledged the potential instability of the score for continuous dynamics in the conclusions. While beyond the scope of the paper, an intriguing avenue for future research involves an in-depth analysis of continuous dynamics."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2492/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700059197153,
                "cdate": 1700059197153,
                "tmdate": 1700059197153,
                "mdate": 1700059197153,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "IryBPMtZEM",
                "forum": "twSnZwiOIm",
                "replyto": "RJ3iWRg9vW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2492/Reviewer_hdvQ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2492/Reviewer_hdvQ"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you so much for addressing my questions. After careful consideration, I decided to keep my rating unchanged."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2492/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700667683733,
                "cdate": 1700667683733,
                "tmdate": 1700667683733,
                "mdate": 1700667683733,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]