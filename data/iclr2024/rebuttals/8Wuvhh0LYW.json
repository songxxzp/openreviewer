[
    {
        "title": "OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models"
    },
    {
        "review": {
            "id": "c50MBXpnem",
            "forum": "8Wuvhh0LYW",
            "replyto": "8Wuvhh0LYW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1585/Reviewer_PTbL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1585/Reviewer_PTbL"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces \"OmniQuant,\" a quantization method of Large Language Models (LLMs) for efficient deployment. Unlike traditional post-training quantization (PTQ) methods that manually select quantization parameters, OmniQuant learns these parameters, enabling effective low-bit quantization. It features two main components: Learnable Weight Clipping (LWC) which adjusts the clipping thresholds, and Learnable Equivalent Transformation (LET) that shifts quantization challenges from activations to weights. OmniQuant operates within a differentiable framework, making it efficient for both weight-only and weight-activation quantization. Experiments on OPT, LLaMA-1, LLaMA-2, and Falcon model family demonstrate the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is well-written and easy to follow. The proposed method, though simple in design, proves to be remarkably effective, notably diminishing the performance degradation for low-bitwidth quantization. The proposed OmniQuant does not introduce extra computation or parameters for the quantized model since the introduced learnable parameters can be fused into quantized weights."
                },
                "weaknesses": {
                    "value": "The major weakness of the paper is the noticeable absence of experimental comparisons with Outlier Suppression+ (OS+) (Wei et al., 2023). Despite OmniQuant's learnable equivalent transformation being conceptually similar to OS+, the paper does not provide a direct comparison or a detailed discussion highlighting the distinctions between the two methods. Such a comparison would be invaluable for readers and would significantly augment the paper's credibility and depth."
                },
                "questions": {
                    "value": "1.\tThe novelty of the proposed learnable equivalent transformation is limited as the main idea learning channel-wise shifting and scaling is similar to Outlier Suppression+ (OS+) (Wei et al., 2023). A comparative discussion elucidating the distinctions between OmniQuant and OS+ would be beneficial for readers. Additionally, the absence of experimental comparisons with OS+ is a notable omission that should be addressed.\n\n2.\tThe proposed LWC learns a clipping strength instead of clipping threshold in PACT (Choi et al., 2018) and LSQ (Esser et al., 2019). However, the paper lacks a clear articulation of the advantages of LWC over PACT and LSQ, particularly in scenarios where it is combined with LET and weights are frequently changed. A more thorough explanation of the benefits and underlying mechanics of LWC in such contexts would be beneficial. Additionally, an investigation into whether an iterative application of LWC and LET would yield performance improvements could provide valuable insights.\n\n3.\tIn Section 3.1, the authors delineate the incorporation of learnable parameters, denoted as $\\gamma$ and $\\beta$, to learn the clipping threshold. While the methodology is clear, the experimental section does not furnish a thorough illustration of these parameters' distribution across layers. An inclusion of this visualization would strengthen the paper.\n\n4.\tThe authors apply the LET to all linear layers, with the notable exception of the second linear layer of the FFN within the proposed method. This selective application raises an intriguing question: Do all instances of LET actively contribute to the model's final performance? An investigation into the individual and cumulative impact of LET on each linear layer could provide deeper insights into the efficacy and necessity of LET across different layers of the model.\n\n5.\tIn the experiments, the authors mention retaining the Softmax output at full-precision owing to its long-tail distribution. It would be insightful to know the implications of quantizing the Softmax output to 8-bit. How does this quantization impact the overall model performance and accuracy?\n\n6.\tIn the experimental section, the authors mention initializing the channel-wise scaling factor using SmoothQuant (Xiao et al., 2023) and the channel-wise shifting factor with Outlier Suppression+. A pertinent question arises: Is the proposed method sensitive to these initializations? It would be elucidative to explore the effects when the channel-wise scaling factor is initialized to 1 and the channel-wise shifting factor to 0. How does this affect the quantization performance?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1585/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1585/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1585/Reviewer_PTbL"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1585/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698674197597,
            "cdate": 1698674197597,
            "tmdate": 1700468761702,
            "mdate": 1700468761702,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9zw0adNPKD",
                "forum": "8Wuvhh0LYW",
                "replyto": "c50MBXpnem",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1585/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1585/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer PTbL (Part: 1/3)"
                    },
                    "comment": {
                        "value": "We sincerely appreciate the time and efforts you have dedicated to reviewing our paper. Should you have any further inquiries, please let us know and we are more than delighted to discuss with you and run more experiments for any pieces of your interests in our work.Reviewer PTbL\n\nWe appreciate your valuable insights and suggestions. We have summarized the updating of our revision in [Summary of Paper Updating](https://openreview.net/forum?id=8Wuvhh0LYW&noteId=YV1oX701CV) and provided an in-depth discussion about the novelty of our method in [Genera Response](https://openreview.net/forum?id=8Wuvhh0LYW&noteId=D6esYYnEi2). Additionally, we have made the following improvements to address your concerns.\n\n**Q1:** The major weakness of the paper is the noticeable absence of experimental comparisons with Outlier Suppression+ (OS+) (Wei et al., 2023).\n\n**A1:** We have added the comparisons with OS+ in Table 2 of our manuscript. A comparison overview is as follows.\n\n| W4A4        | Quantization | Average Acc. |\n| ----------- | ------------ | ------------ |\n| LLaMa-1-7B  | OS+          | 48.43        |\n| LLaMa-1-7B  | OmniQuant    | 52.65        |\n| LLaMa-1-13B | OS+          | 49.86        |\n| LLaMa-1-13B | OmniQuant    | 54.37        |\n| LLaMa-1-30B | OS+          | 52.62        |\n| LLaMa-1-30B | OmniQuant    | 56.63        |\n| LLaMa-1-65B | OS+          | 52.52        |\n| LLaMa-1-65B | OmniQuant    | 59.23        |\n\n\n\n\n\n**Q2:** Distinctions with Outlier Supression+. \n\n**A2:** OmniQuant differs from Outlier Suppression+ in several aspects, including parameter optimization through gradient updates, block-wise optimization, expansion of equivalent transformation to attention operations, and the introduction of learnable weight clipping (LWC). Details are as follows.\n\n- Both OmniQuant and OS+ try to find the optimal equivalent transformation parameters. OS+ solves this problem by pre-defined shifting strength and grid-searched scaling parameters. However, OmniQuant obtains both shifting and scaling parameters through gradient optimization.\n\n- OmniQuant leverages block-wise optimization while OS+ executes grid searching through a mix of single linear layer and multiple linear layers objective(Eq.6 and Eq.7 in their paper). Block-wise optimization considers interaction within a block and can produce better performance as demonstrated in BRECQ. However, it is a challenge for OS+ to expand to block-wise quantization, which would significantly enlarge the solution space and increase the time of grid searching.\n\n- LET expends the equivalent transformation to attention operation (Eq. (5) in our paper), but OS+ only does the equivalent transformation on linear layers. \n\n- OmniQuant also introduces learnable weight clipping (LWC), which further facilitates the low-bits (2,3) weight-only quantization, while Outlier Suppression only supports weight-activation quantization.\n\nWe have added this discussion in Section A2 in our revision. You can also refer [Genera Response]() for more discussion about the novelty of our paper.\n\n\n\n**Q3:** The proposed LWC learns a clipping strength instead of clipping threshold in PACT (Choi et al., 2018) and LSQ (Esser et al., 2019). However, the paper lacks a clear articulation of the advantages of LWC over PACT and LSQ, particularly in scenarios where it is combined with LET and weights are frequently changed. A more thorough explanation of the benefits and underlying mechanics of LWC in such contexts would be beneficial.\n\n**A3:** To clarify the advantages of LWC, we have included Figure A5 in the appendix, illustrating the dynamic change in weight ranges during training with different clipping-based methods. The figure emphasizes that LET, by dramatically altering weight ranges, poses a challenge for traditional absolute clipping thresholds or step size learning. In contrast, LWC calculates relative clipping strength, utilizing extremums after LET as proxies. This approach enables LWC to adapt to changing weight ranges more naturally, addressing the optimization of difficulty introduced by LET."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1585/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700225762035,
                "cdate": 1700225762035,
                "tmdate": 1700273022567,
                "mdate": 1700273022567,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zWFR7BwXy8",
                "forum": "8Wuvhh0LYW",
                "replyto": "c50MBXpnem",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1585/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1585/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer PTbL (Part: 2/3)"
                    },
                    "comment": {
                        "value": "**Q4:** Additionally, an investigation into whether an iterative application of LWC and LET would yield performance improvements could provide valuable insights.\n\n**A4:** We want to emphasize that in our approach, LWC and LET are trained simultaneously, and we have also explored an iterative training approach by iterations or epochs. The results, as presented in the following table, clearly indicate that training LWC and LET simultaneously yields the best performance. As aforementioned in Q3, LWC leverages the extremums after LET as proxies, allowing it to capture the dynamic changes in the weight range introduced by LET. This synergy between LET and LWC creates a progressive process, where both techniques reinforce each other rather than interfere. To further support this statement, we conducted an additional experiment (last row in the table), training LWC and LET iteratively with double the epochs. The results show that simultaneous training with 20 epochs achieves comparable performance to iterative training with 40 epochs. This demonstrates the effectiveness and efficiency of training LWC and LET simultaneously. We have added this experiment in Table A8 of our revision.\n\n| LLaMa-1-7B/ W4A4/ Iterative LWC and LET | Average PPL | Average Acc. |\n| --------------------------------------- | ----------- | ------------ |\n| simultaneous training (OmniQuant)       | 12.87       | 52.65        |\n| each iteration                          | 13.56       | 50.91        |\n| each epoch                              | 13.51       | 52.06        |\n| each epoch + double training epochs     | 12.80       | 52.50        |\n\n\n\n**Q5**: Visualization of $\\gamma$ and $\\beta$ distribution.\n\n**A5:** Indeed, we have plotted the distribution of learned \\gamma and \\beta in Figure A1 and presented the analysis in Section A5 in the Appendix. Through the visualization in Figure A1, we can find that LWC can learn more pronounced clipping when meeting with lower quantization precision (lower bits or greater group size). Such a phenomenon makes sense due to that lower quantization precision has smaller representation space, and consequently is expected to clip more weights to obtain better trade-offs.\n\n\n\n**Q6:** An investigation into the individual and cumulative impact of LET on each linear layer could provide deeper insights into the efficacy and necessity of LET across different layers of the model.\n\n**A6:** Good question. In our paper, we exclude the LET of the second linear layer due to the high sparsity of features after the non-linear layer leads to unstable gradients. Therefore, we have four LET pairs, represented as [ln1, (q_proj, k_proj, v_proj)], [v_proj, out_proj], [Q, K], and [ln2, fc1]. To validate the effect of each part, we respectively remove the LET from each layer and report the average perplexity and the average accuracy on 6 zero-shot tasks like Table 2. As shown in the following table, we can find that all four LETs can improve the performance, specially for the [ln1, (q_proj, k_proj, v_proj)] pair. Such results also demonstrate that the activation outliers are more serious after layer normalization layers. We have added this experiment in Table A5 of our revision.\n\n| LLaMa-1-7B                              | Average PPL | Average Acc. |\n| --------------------------------------- | ----------- | ------------ |\n| FP                                      | 6.38        | 64.09        |\n| W4A4 (ours)                             | 12.87       | 52.65        |\n| W4A4  - [ln1, (q_proj, k_proj, v_proj)] | 19.87       | 46.79        |\n| W4A4  -[v_proj, out_proj]               | 13.03       | 51.68        |\n| W4A4  -[Q, K]                           | 13.34       | 51.47        |\n| W4A4  -[ln2, fc1]                       | 14.47       | 51.04        |"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1585/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700225789560,
                "cdate": 1700225789560,
                "tmdate": 1700225789560,
                "mdate": 1700225789560,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5ErGuSizaw",
                "forum": "8Wuvhh0LYW",
                "replyto": "c50MBXpnem",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1585/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1585/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer PTbL (Part: 3/3)"
                    },
                    "comment": {
                        "value": "**Q7:** In the experiments, the authors mention retaining the Softmax output at full-precision owing to its long-tail distribution. It would be insightful to know the implications of quantizing the Softmax output to 8-bit. How does this quantization impact the overall model performance and accuracy?\n\n**A7**: We have conducted experiments on quantizing the Softmax output, reporting the impact on model performance. As shown in the following table, we can find that quantizing the softmax into 8-bit and 6-bit brings acceptable performance degeneration, which demonstrates that block-wise calibration can compensate for the loss of 8-bit and 6-bit Softmax quantization. However, 4-bit Softmax quantization brings significant performance loss, which requires further exploration and additional trick such as log2 quantization in RepQViT[1]. We have added this experiment in Table A7 of our revision.\n\n| LLaMa-1-7B           | Average PPL | Average Acc. |\n| -------------------- | ----------- | ------------ |\n| FP                   | 6.38        | 64.09        |\n| W4A4 + Softmax 16bit | 12.87       | 52.65        |\n| W4A4 + Softmax 8bit  | 12.91       | 51.93        |\n| W4A4 + Softmax 6bit  | 13.20       | 51.70        |\n| W4A4 + Softmax 4bit  | 18.80       | 48.52        |\n\n\n\n**Q8:**  Sensitivity to channel-wise scaling and shifting initialization.\n\n**A8**: Good advice, sorry for the missing of this ablation. As shown in the following table, we can find that both careful initialization of scaling and shifting can improve the final performance. Specifically, scaling initialization is more important than shifting due to scaling plays the main role in alleviating outliers. We have added this experiment in Table A6 of our revision.\n\n| LLaMa-1-7B W4A4          | Average PPL | Average Acc. |\n| ------------------------ | ----------- | ------------ |\n| careful initialization   | 12.87       | 52.65        |\n| initialize scaling as 1  | 13.64       | 51.37        |\n| initialize shifting as 0 | 12.95       | 52.22        |\n\n\n\nWe sincerely appreciate the time and efforts you have dedicated to reviewing our paper. Should you have any further inquiries, please let us know and we are more than delighted to discuss with you and run more experiments for any pieces of your interests in our work.\n\n\n\n[1] Repq-vit: Scale reparameterization for post-training quantization of vision transformers"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1585/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700225823900,
                "cdate": 1700225823900,
                "tmdate": 1700225823900,
                "mdate": 1700225823900,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Ut7PVG9F0E",
                "forum": "8Wuvhh0LYW",
                "replyto": "c50MBXpnem",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1585/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1585/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Look forward to further discussion."
                    },
                    "comment": {
                        "value": "Dear Reviewer PTbL,\n\nWe hope that our response can address your concerns. As the deadline for discussion period is approaching, we really appreciate if you can let us know whether there still exists any further question about the paper or the response. We are looking forward to further discussion.\n\nBest regards,\n\nAuthors of Paper 1585"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1585/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700468183230,
                "cdate": 1700468183230,
                "tmdate": 1700468183230,
                "mdate": 1700468183230,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SI9nMkLPey",
                "forum": "8Wuvhh0LYW",
                "replyto": "Ut7PVG9F0E",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1585/Reviewer_PTbL"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1585/Reviewer_PTbL"
                ],
                "content": {
                    "title": {
                        "value": "Response to rebuttal"
                    },
                    "comment": {
                        "value": "Thanks to the authors for their informative rebuttal, which has successfully addressed my concerns. In light of this, I am inclined to increase my rating to 6."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1585/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700468747517,
                "cdate": 1700468747517,
                "tmdate": 1700468747517,
                "mdate": 1700468747517,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "CNUxdOX1PU",
            "forum": "8Wuvhh0LYW",
            "replyto": "8Wuvhh0LYW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1585/Reviewer_ngyH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1585/Reviewer_ngyH"
            ],
            "content": {
                "summary": {
                    "value": "OmniQuant represents Learnable Weight Clipping (LWC) and Learnable Equivalent Transformation (LET). In contrast to the traditional min-max scaling threshold, LWC applies a sigmoid function on the factors over min and max. The loss is computed by comparing the distortion of the output, which can be easily back-propagated to the scaling factors. Similar work on weight clipping could be found in PACT [1] and LSQ [2]. However, OmniQuant's formulation is cleaner (no need to incorporate with learnable step-size) and more general (applies to both weight and activation).\n\nThe LET in OmniQuant decides the computation operands ordering for max hardware efficiency. The implementation is based on MLC [3]. \n\n* [1] Choi, J., Wang, Z., Venkataramani, S., Chuang, P.I.J., Srinivasan, V. and Gopalakrishnan, K., 2018. Pact: Parameterized clipping activation for quantized neural networks. arXiv preprint arXiv:1805.06085.\n* [2] Esser, S.K., McKinstry, J.L., Bablani, D., Appuswamy, R. and Modha, D.S., 2019. Learned step size quantization. arXiv preprint arXiv:1902.08153.\n* [3] Feng, S., Hou, B., Jin, H., Lin, W., Shao, J., Lai, R., Ye, Z., Zheng, L., Yu, C.H., Yu, Y. and Chen, T., 2023, January. Tensorir: An abstraction for automatic tensorized program optimization. In Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2 (pp. 804-817)."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The biggest contribution of this work is the learnable weight clipping (LWC). \n\n1. When compared to AWQ [1] which only optimize the scale, LWC uses the same objective function (minimize output distortion) but also handles clipping.\n\n2. When compared to PACT, which only applies to activation with positive values, LWC is more general and applies to weight, activation, and negative values.\n\n3. When compared to LSQ, which is a combination of multiple optimization goals (step-size, gradient scaling, and clipping), LWC is straightforward to implement and speedy to train.\n\n[1] Lin, J., Tang, J., Tang, H., Yang, S., Dang, X. and Han, S., 2023. AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration. arXiv preprint arXiv:2306.00978."
                },
                "weaknesses": {
                    "value": "1. While I highlight the advantage of LWC over AWQ, PACK, and LSQ, this evaluation for PACK and LSQ is lacking.\n\n2. Authors should present Figure 1 (a) in a quantitative way. It lacks actual numbers for cost of time and performance.\n\n3. Learnable Equivalent Transformation (LET) isn't novel and can be seen in common quantization kernels such as LLM.int8. I suggest authors to elaborate more on comparison with PACK and LSQ instead of LET.\n\n4. The latency benchmark only contain fp16 and OmniQuant. Strong baseline such as GPTQ is lacking."
                },
                "questions": {
                    "value": "1. How does OmniQuant runtime efficiency compare to GPTQ and AWQ?\n\n2. What is the performance when compared to PACK and LSQ?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1585/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1585/Reviewer_ngyH",
                        "ICLR.cc/2024/Conference/Submission1585/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1585/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698699405626,
            "cdate": 1698699405626,
            "tmdate": 1700495059509,
            "mdate": 1700495059509,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "glIirxr6Yy",
                "forum": "8Wuvhh0LYW",
                "replyto": "CNUxdOX1PU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1585/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1585/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ngyH (Part: 1/2)"
                    },
                    "comment": {
                        "value": "Thanks for your valuable and supportive reviews. We have summarized the updating of our revision in [Summary of Paper Updating](https://openreview.net/forum?id=8Wuvhh0LYW&noteId=YV1oX701CV) and provided an in-depth discussion about the novelty of our method in [Genera Response](https://openreview.net/forum?id=8Wuvhh0LYW&noteId=D6esYYnEi2). Below, we explain our method and contribution more clearly in Q1 to Q2 and address the weakness from Q3 to Q6 as follows. \n\n**Q1:** The LET in OmniQuant decides the computation operands ordering for max hardware efficiency. The implementation is based on MLC.\n\n**A1:** With full respect, Learnable Equivalent Transformation (LET) in OmniQuant focuses on channel-wise scaling and shifting to facilitate low-bits quantization by transferring quantization difficulty between weights and activation. Importantly, LET is independent of hardware efficiency considerations. After training, all LET and LWC parameters are absorbed into the original module without introducing additional memory or computational burden. Therefore, OmniQuant's quantized models can seamlessly deploy through existing frameworks such as MLC-LLM. \n\n\n\n**Q2:** The biggest contribution of this work is the learnable weight clipping (LWC).\n\n**A2:** With all due respect, the novelty and primary contribution of OmniQuant lies in being the first to introduce differentiable gradient optimization into learnable equivalent transformation (LET) and learnable weight clipping (LWC) for large language models. Previous PTQ methods for LLMs such as Outlier Suppression+ or AWQ solve such problems by grid searching, which limits the performance in low-bit scenarios. Furthermore, while the concepts of equivalent transformation and clipping are not new, we have made some innovative improvements to make the optimization process more stable. For equivalent transformation, we expend the equivalent transformation from linear layer to attention operation (Eq. (5) in our paper). For weight clipping, we propose taking the extremums after LET as proxies, which helps LWC to catch the dramatic changs led by LET as described in Section A5. These advancements enable OmniQuant to excel in both weight-only and weight-activation quantization scenarios.\n\n\n\n**Q3:** While I highlight the advantage of LWC over AWQ, PACT, and LSQ, this evaluation for PACT and LSQ is lacking. What is the performance when compared to PACK and LSQ?\n\n**A3:**   As described in Algorithm 1 in Appendix, OmniQuant firstly leverages LET to transfer magnitude from activation to weight, and then quantizes the weight after LET with LWC. To demonstrate the advantage of LWC, we replace LWC in our pipeline with PACT (train the clipping threshold directly) and LSQ (train the quantization step size directly) in Table A14, showcasing that LWC consistently outperforms PACT and LSQ. Additionally, Figure A5 in the Appendix further clarifies the advantages of LWC by illustrating its adaptability to changing weight ranges introduced by LET, addressing optimization difficulties not addressed by traditional methods. Please also see Q2/A2 in responses to WDuc."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1585/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700225620594,
                "cdate": 1700225620594,
                "tmdate": 1700225620594,
                "mdate": 1700225620594,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cOf4bfm6nW",
                "forum": "8Wuvhh0LYW",
                "replyto": "CNUxdOX1PU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1585/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1585/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ngyH (Part: 2/2)"
                    },
                    "comment": {
                        "value": "**Q4:** Authors should present Figure 1 (a) in a quantitative way. It lacks actual numbers for cost of time and performance.\n\n**A4:** Thank you for the suggestion. We have incorporated quantitative results into Figure 1(a) to provide a more comprehensive understanding of the cost of time and performance.\n\n\n\n**Q5:**  Learnable Equivalent Transformation (LET) isn't novel and can be seen in common quantization kernels such as LLM.int8. I suggest authors to elaborate more on comparison with PACK and LSQ instead of LET.\n\n**A5:** As mentioned in Q2, LET is the first one to introduce gradient optimization into the equivalent transformation of large language models, which significantly pushes the limitation of low-bits quantization. To demonstrate the superiority of the proposed method, we have compared it with state-of-the-art weight-only (GPTQ, AWQ) and weight-activation (LLM-QAT, Outlier Suppression +) quantization methods in our paper. Additionally, we have transferred PACT and LSQ to the weight quantization in Table A14, showcasing that LWC outperforms previous clipping-based methods. As for the activation quantization, we leverage MinMax quantization due to that directly leverages LSQ in activation quantization leading to poor performance than MinMAx quantization as shown in Table 4 of the LLM-QAT paper. You can also refer [Genera Response](https://openreview.net/forum?id=8Wuvhh0LYW&noteId=D6esYYnEi2) for more discussion about the novelty of our paper.\n\n\n\n\n\n**Q6:** The latency benchmark only contain fp16 and OmniQuant. Strong baseline such as GPTQ is lacking. How does OmniQuant runtime efficiency compare to GPTQ and AWQ?\n\n**A6:** OmniQuant, GPTQ, and AWQ are all uniform quantization methods, allowing their quantized models to be deployed with the same latency. In our paper, the weight-only quantization kernel is not our primary focus, and we leverage existing MLC-LLM to validate the practical deployment of our proposed methods. As shown in the following table, we present the latency compared between MLC-LLM, GPTQ kernel, and AWQ kernel. Note that all of these three manners can be used to deploy OmniQuant\u2019s quantized models.\n\n|                  | LLaMa-7B      | LLaMa-13B     |\n| ---------------- | ------------- | ------------- |\n| FP               | 69.2 token/s  | 52.5 token/s  |\n| 4bit-MLC-LLM     | 134.2 token/s | 91.3 token/s  |\n| 4bit-GPTQ kernel | 91.30 token/s | 63.2 token /s |\n| 4bit-AWQ kernel  | 155.4 token/s | 94.86 token/s |\n\n\n\nWe sincerely appreciate the time and efforts you have dedicated to reviewing our paper. Should you have any further inquiries, please let us know and we are more than delighted to discuss with you and run more experiments for any pieces of your interests in our work."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1585/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700225667288,
                "cdate": 1700225667288,
                "tmdate": 1700225667288,
                "mdate": 1700225667288,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "X28oAIWP67",
                "forum": "8Wuvhh0LYW",
                "replyto": "cOf4bfm6nW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1585/Reviewer_ngyH"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1585/Reviewer_ngyH"
                ],
                "content": {
                    "comment": {
                        "value": "I thank authors for the rebuttal. The updated benchmarks and appendix addressed my concern and I increased my rating from 6 to 8."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1585/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700495184808,
                "cdate": 1700495184808,
                "tmdate": 1700495184808,
                "mdate": 1700495184808,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "iDiUKY61tl",
            "forum": "8Wuvhh0LYW",
            "replyto": "8Wuvhh0LYW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1585/Reviewer_WDuc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1585/Reviewer_WDuc"
            ],
            "content": {
                "summary": {
                    "value": "The paper works on quantization for large language models. It first proposes to learn the weight clipping threshold with optimization on the ratio of weight ranges. Then, it proposes to learn the equivalent parameters for learnable equivalent transformation with a block-wise loss. Experiments are done on 4-bit weight activation quantization and 4-bit weight-only quantization with LLaMA and OPT models. Especially, the paper evaluates the inference speed with 4-bit weight-only quantized models."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* Experiments are done across several datasets including common sense reasoning and perplexity evaluation. Also, the paper tries the hard setting with 4-bit weight and activation quantization. Especially, it evaluates the latency with a 4-bit weight quantized model.\n* The structure of the paper is clear and figures are drawn well.\n* The method is simple and considers the quantization difficulties both for weights and activations."
                },
                "weaknesses": {
                    "value": "* The paper lacks a necessary detailed explanation for the motivation and effectiveness of the weight clipping method. \n  * In 3.2, the paper claims that directly employing prior LSQ and PACT would produce unsatisfactory performance, as demonstrated in LLM-QAT. However, LLM-QAT says that the outliers for activation have a notable impact, bringing difficulty for clipping while this method works on weights here. Also, how can the proposed Eq. (2) solve the problem of learning clipping thresholds for outliers? In other words, what is the optimization difficulty (concept given in 3.2) of previous techniques, and how can Eq.(2) solve it? More explanation about the motivation is preferred.\n  * In the appendix, the paper says that LET would decline the convergence of LSQ and PACT because LET alters the weight distribution. However, weight distribution altering is a common case for LSQ and PACT in QAT. Also, combined with LET, the \\gamma and \\beta in the proposed Eq. (2) can also go up and down during learning as it optimizes the ratio of the changeable weig\n\n* What is the core novelty of the LET? I find it looks similar to Outlier Suppression+. While the paper says that Outlier Suppression+ takes a pre-defined migration strength, but this method does not and proposes to optimize the output. I'd like to point out that Outlier Suppression+ did not take a pre-defined strength and proposed to optimize the output for channel-wise scaling parameters earlier than this paper. Meanwhile, the paper also states that AWQ adopts a grid-searched channel-wise scaling, which also seems relevant to the technique in this paper. Therefore, can the paper compare these different designs and explain why the proposed way is the best? I did not find these and this could help us better understand the effectiveness.\n\n* Experiments shall be compared with the paper Outlier Suppression+ because you and they work on the same quantization problem, take the same equivalent transformation, and have similar optimization designs.\n\t\n* I noticed that the paper requires careful equivalent parameter initialization via the compared baseline SmoothQuant. I might wonder how it behaves without good initialization. For example, under asymmetric cases W4A8, W4A6, and W8A4, where the LLM-QAT shows SmoothQuant can behave terribly.\n\t\n* To conclude, I find the proposed two techniques are not novel and the paper lacks the necessary explanation and comparison. I see the challenge of the two techniques is how to combine the two kinds of learning together as they influence each other. However, current techniques seem can not solve this problem well. Thus, I think it would be better if the paper gives more description, and design consideration to the combination part, which might increase the novelty. For example, maybe alternately train these two techniques."
                },
                "questions": {
                    "value": "Please check the weakness part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1585/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1585/Reviewer_WDuc",
                        "ICLR.cc/2024/Conference/Submission1585/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1585/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698790618656,
            "cdate": 1698790618656,
            "tmdate": 1700699035942,
            "mdate": 1700699035942,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "NV2eUKV4eA",
                "forum": "8Wuvhh0LYW",
                "replyto": "iDiUKY61tl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1585/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1585/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer WDuc (Part: 1/3)"
                    },
                    "comment": {
                        "value": "We would like to express our sincere gratitude to Reviewer WDuc for their valuable time and effort in reviewing our work. We have summarized the updating of our revision in [Summary of Paper Updating](https://openreview.net/forum?id=8Wuvhh0LYW&noteId=YV1oX701CV) and provided an in-depth discussion about the novelty of our method in [Genera Response](https://openreview.net/forum?id=8Wuvhh0LYW&noteId=D6esYYnEi2). Below, we address each identified weakness. \n\n**Q1:** In 3.2, the paper claims that directly employing prior LSQ and PACT would produce unsatisfactory performance, as demonstrated in LLM-QAT. However, LLM-QAT says that the outliers for activation have a notable impact, bringing difficulty for clipping while this method works on weights here.\n\n**A1:** We appreciate your clarification regarding LLM-QAT. Indeed, the confusion arises from our initial claim about the unsatisfactory performance of LSQ and PACT in weight quantization in Sec. 3.2 and Table A14. We have modified the expression to address any potential confusion. Thanks for the correction.\n\n\n\n**Q2:**  How can the proposed Eq. (2) solve the problem of learning clipping thresholds for outliers? In other words, what is the optimization difficulty (concept given in 3.2) of previous techniques, and how can Eq.(2) solve it? More explanation about the motivation is preferred. In the appendix, the paper says that LET would decline the convergence of LSQ and PACT because LET alters the weight distribution. However, weight distribution altering is a common case for LSQ and PACT in QAT. Also, combined with LET, the \\gamma and \\beta in the proposed Eq. (2) can also go up and down during learning as it optimizes the ratio of the changeable weight.\n\n**A2:** Thanks for your question. We claim that LWC reduces the optimization difficulty in learning clipping thresholds because our LWC reparameterizes the learnable  clipping threshold into the product of a learnable 0~1 clipping strength and the  maximum or minimum of weights. The benefit is that no matter how the maximum and minimum of weights change caused by LET with the training going, our LWC would always learn a clipping strength under the reference of the current maximum and minimum of weights . Such a reparameterization technique has been utilized in various areas for better training properties. For example, batch normalization decomposes the neural activation into a variable approximately following normal distribution and learnable scale and shift parameters, which improves the training stability.\n\nHere we provide more empirical evidence to clarify the motivation and advantages of LWC. As shown in Figure A5 in the appendix, it presents the dynamic change in weight ranges (maximun minus mininum) during training with different clipping-based methods. We can see that the weight range varies a lot as the training goes on (see the curve of Min-Max) when training our OmniQuant, which is caused by our proposed another component LET. It poses a challenge for traditional absolute clipping thresholds (PACT) or step size learning (LSQ). As we can see from Figure A5 (a & b), no matter how the weight range of Min-Max increases, PACT and LSR would always retain a relatively small weight range, which would clip many regular weights values to the threshold. In contrast, LWC calculates relative clipping strength and enables it to adapt to varying weight ranges, as shown in Figure A5 (c).\n\n We hope that our explanations and visualizations will address your concerns."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1585/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700225425944,
                "cdate": 1700225425944,
                "tmdate": 1700225425944,
                "mdate": 1700225425944,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VeO8sm9i3K",
                "forum": "8Wuvhh0LYW",
                "replyto": "iDiUKY61tl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1585/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1585/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer WDuc (Part: 2/3)"
                    },
                    "comment": {
                        "value": "**Q3:** What is the core novelty of the LET? I find it looks similar to Outlier Suppression+. While the paper says that Outlier Suppression+ takes a pre-defined migration strength, but this method does not and proposes to optimize the output. I'd like to point out that Outlier Suppression+ did not take a pre-defined strength and proposed to optimize the output for channel-wise scaling parameters earlier than this paper. Meanwhile, the paper also states that AWQ adopts a grid-searched channel-wise scaling, which also seems relevant to the technique in this paper. Therefore, can the paper compare these different designs and explain why the proposed way is the best? I did not find these and this could help us better understand the effectiveness.\n\n**A3:** We have provided a detailed comparison of LET with Outlier Suppression+ (OS+) and AWQ in the table presented below. \n\n- For the equivalent transformation, AWQ only considers scaling operation, while OS+ and our LET consider both scaling and shifting operation.\n\n- For the execution position, AWQ and OS+ only carry equivalent transformation on linear layers, while LET also considers the matrix multiplication within attention (Eq. (5) in our paper). This point enlarges the solution space of equivalent transformation and brings further improvements as demonstrated in Table A5 of Appendix.\n\n-  For the optimization, OS+ leverages pre-defined shifting, both AWQ and OS+ find the scaling factors through grid searching based on some heuristic strategy. However, LET optimized all equivalent transformation parameters through end-to-end gradient descent, which significantly improve the performance. As acknowledged by reviewer ZyAK, making the learning of equivalent transformation stable and effective is a good contribution. Additionally, OmniQuant leverages block-wise optimization objectives, while both AWQ and OS+ take layer-wise optimization objectives. Block-wise optimization considers interaction within a block and can produce better performance as demonstrated in BRECQ. However, it is a challenge for OS+ and AWQ to expand to block-wise quantization, which would significantly enlarge the solution space and increase the time of grid searching.\n\n|      | operation     | position                 | optimization                                          |\n| ---- | ------------- | ------------------------ | ----------------------------------------------------- |\n| AWQ  | scale         | linear layer             | grid searching                                        |\n| OS+  | scale & shift | linear layer             | grid searching for scaling & pre-defined for shifting |\n| LET  | scale & shift | linear layer & attention | gradient decent                                       |\n\nWe have added this discussion in Section A2 of our revision.  In Figure A3 of Appendix, we also compare the quantization error of each block. We can find that OmniQuant significantly reduce the quantization loss compared with the grid-searching based method such as AWQ and Outlier Suppression +. You can also refer [Genera Response](https://openreview.net/forum?id=8Wuvhh0LYW&noteId=D6esYYnEi2) for more discussion about the novelty of our paper.\n\n\n\n**Q4:** Experiments shall be compared with the paper Outlier Suppression+ because you and they work on the same quantization problem, take the same equivalent transformation, and have similar optimization designs.\n\n**A4:**  We have incorporated comparisons with OS+ in Table 2 of our manuscript, and offered an overview of the W4A4 quantization with six zero-shot tasks as follows. We can find that OmniQuant outperforms OS+ in a range from 4.01% to 6.68% accuracy.\n\n| W4A4        | Quantization | Average Acc. |\n| ----------- | ------------ | ------------ |\n| LLaMa-1-7B  | OS+          | 48.43        |\n| LLaMa-1-7B  | OmniQuant    | 52.65        |\n| LLaMa-1-13B | OS+          | 49.86        |\n| LLaMa-1-13B | OmniQuant    | 54.37        |\n| LLaMa-1-30B | OS+          | 52.62        |\n| LLaMa-1-30B | OmniQuant    | 56.63        |\n| LLaMa-1-65B | OS+          | 52.52        |\n| LLaMa-1-65B | OmniQuant    | 59.23        |"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1585/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700225461934,
                "cdate": 1700225461934,
                "tmdate": 1700272997593,
                "mdate": 1700272997593,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "f1MozaKj6A",
                "forum": "8Wuvhh0LYW",
                "replyto": "iDiUKY61tl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1585/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1585/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer WDuc (Part: 3/3)"
                    },
                    "comment": {
                        "value": "**Q5:** I noticed that the paper requires careful equivalent parameter initialization via the compared baseline SmoothQuant. I might wonder how it behaves without good initialization. For example, under asymmetric cases W4A8, W4A6, and W8A4, where the LLM-QAT shows SmoothQuant can behave terribly.\n\n**A5:** Good advice, sorry for missing this ablation. To validate the impact of initialization, we first try to initial scaling as 1 and initial shifting as 0. We do experiments on LLaMa-1-7B with W4A4 quantization, and report the average perplexity and average accuracy. As shown in the following table, we can find that both careful initialization of scaling and shifting can improve the final performance. Specifically, scaling initialization is more important than shifting due to that scaling plays the main role in alleviating outliers. \n\n| LLaMa-1-7B W4A4          | Average PPL | Average Acc. |\n| ------------------------ | ----------- | ------------ |\n| careful initialization   | 12.87       | 52.65        |\n| initialize scaling as 1  | 13.64       | 51.37        |\n| initialize shifting as 0 | 12.95       | 52.22        |\n\nMoreover, as shown in the following table, OmniQuant also demonstrates robust performance in asymmetric cases, ensuring effectiveness across various scenarios.\n\n| LLaMa-1-7B | Average PPL | Average Acc. |\n| ---------- | ----------- | ------------ |\n| FP         | 6.38        | 64.09        |\n| W4A4       | 12.87       | 52.65        |\n| W4A8       | 6.60        | 62.40        |\n| W4A6       | 6.85        | 61.64        |\n| W8A4       | 11.52       | 53.46        |\n\nWe have added these experiments in Table A6 and Table A17 of our revision.\n\n\n\n**Q6:** I see the challenge of the two techniques is how to combine the two kinds of learning together as they influence each other. However, current techniques seem can not solve this problem well. Thus, I think it would be better if the paper gives more description, and design consideration to the combination part, which might increase the novelty. For example, maybe alternately train these two techniques. For example, maybe alternately train these two techniques.\n\n**A6:** Thanks for the suggestion. We want to emphasize that in our approach, LWC and LET are trained simultaneously, and we have also explored an iterative training approach by iterations or epochs. The results, as presented in the following table, clearly indicate that training LWC and LET simultaneously yields the best performance. As aforementioned in Q2, LWC leverages the extremums after LET as the reference, allowing it to capture the dynamic changes in the weight range introduced by LET. This synergy between LET and LWC creates a progressive process, where both techniques reinforce each other rather than interfere. To further support this statement, we conducted an additional experiment (last row in the table), training LWC and LET iteratively with double the epochs. The results show that simultaneous training with 20 epochs achieves comparable performance to iterative training with 40 epochs. This demonstrates the effectiveness and efficiency of training LWC and LET simultaneously. We have added this experiment in Table A8 of our revision.\n\n| LLaMa-1-7B/ W4A4/ Iterative LWC and LET | Average PPL | Average Acc. |\n| --------------------------------------- | ----------- | ------------ |\n| simultaneous training (OmniQuant)       | 12.87       | 52.65        |\n| each iteration                          | 13.56       | 50.91        |\n| each epoch                              | 13.51       | 52.06        |\n| each epoch + double training epochs     | 12.80       | 52.50        |\n\n\n\nWe sincerely appreciate the time and efforts you have dedicated to reviewing our paper. Should you have any further inquiries, please let us know and we are more than delighted to discuss with you and run more experiments for any pieces of your interests in our work."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1585/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700225490971,
                "cdate": 1700225490971,
                "tmdate": 1700225490971,
                "mdate": 1700225490971,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "oMhCv7iCSl",
                "forum": "8Wuvhh0LYW",
                "replyto": "iDiUKY61tl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1585/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1585/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Look forward to further discussion."
                    },
                    "comment": {
                        "value": "Dear Reviewer WDuc,\n\nWe hope that our response can address your concerns. As the deadline for discussion period is approaching, we really appreciate if you can let us know whether there still exists any further question about the paper or the response. We are looking forward to further discussion.\n\nBest regards,\n\nAuthors of Paper 1585"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1585/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700468247021,
                "cdate": 1700468247021,
                "tmdate": 1700468247021,
                "mdate": 1700468247021,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nw1QzsAplF",
                "forum": "8Wuvhh0LYW",
                "replyto": "iDiUKY61tl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1585/Reviewer_WDuc"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1585/Reviewer_WDuc"
                ],
                "content": {
                    "title": {
                        "value": "Post-rebuttal"
                    },
                    "comment": {
                        "value": "Thanks for your detailed reply. Some of my concerns have been addressed. I'd like to increase my score to 5. However, for the distinction compared to AWQ and Outlier Suppression+, I knew the difference but as stated in my first review, what I wonder is why the optimization method proposed in this paper is more beneficial. For example, the shifting technique is first proposed by Outlier Suppression+, though they use a pre-defined way, the paper seems not to show the results that learning the shifting vector can help a lot. Also, the table above shows the results of OmniQuant which takes both the LET and weight clipping. However, it did not show how much benefit can the gradient descent of LET bring compared to that of Outlier Suppression+ as that paper demonstrated lower accuracy obtained by learning. I think detailed analyses are very important as the difference is not big and novelty is limited in this part."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1585/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700518394041,
                "cdate": 1700518394041,
                "tmdate": 1700559315890,
                "mdate": 1700559315890,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FGUW9HJX6v",
                "forum": "8Wuvhh0LYW",
                "replyto": "HrjzzDJZoC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1585/Reviewer_WDuc"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1585/Reviewer_WDuc"
                ],
                "content": {
                    "title": {
                        "value": "Post-rebuttal"
                    },
                    "comment": {
                        "value": "Thank you for the reply. I'd like to increase my score to 6. Besides, I wonder that as the activation outlier challenge is more pronounced in OPT, why this method behaves better on OPT compared to LLaMA. Also, I notice that the 4-bit OPT results in the appendix can reach the FP baseline while the 4-bit LLaMA still suffers a large accuracy drop. Could the author provide further explanation for this phenomenon?"
                    }
                },
                "number": 25,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1585/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700699015673,
                "cdate": 1700699015673,
                "tmdate": 1700699015673,
                "mdate": 1700699015673,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "wKCbLSlSOB",
            "forum": "8Wuvhh0LYW",
            "replyto": "8Wuvhh0LYW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1585/Reviewer_ZyAK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1585/Reviewer_ZyAK"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses the challenges faced by large language models (LLMs) by optimizing quantization parameters. It is based on the SmoothQuant and Outlier Suppression+ and mainly contributes to a learnable pipeline. The idea is simple and trivial. However, the effect is good on various models and datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Extend the existing quantization methods based on the thought of transformation to a learnable one, and give a pipeline with a stable optimization process\n- the idea of learnable scaling is simple, but making the learning stable and effective is a good contribution\n- conduct experiments on various models"
                },
                "weaknesses": {
                    "value": "- The novelty is limited. The overall framework is based on two existing methods.\n- The learnable idea is not new. In Outlier Suppression+, the scaling has been designed to be learned via a scheme that does not depend on gradient."
                },
                "questions": {
                    "value": "- Both outlier suppression+ and this paper highlight the scaling to be learned. An in-depth comparison needs to be provided, including the experimental perspective and the theoretical perspective.\n- The optimization based on little data and backward propagation makes the learning easy to be overfitted. More validation should be conducted to prove the generalization ability of this learning.\n- There are some new kinds of ways to decompose the outliers, e.g., https://arxiv.org/abs/2310.08041. Comprehensive experiments are suggested to further enrich the validation."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1585/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698833956848,
            "cdate": 1698833956848,
            "tmdate": 1699636087202,
            "mdate": 1699636087202,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wFnxdAnW0S",
                "forum": "8Wuvhh0LYW",
                "replyto": "wKCbLSlSOB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1585/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1585/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ZyAK (Part: 1/2)"
                    },
                    "comment": {
                        "value": "We thank the Review ZyAK for the time and effort to review our paper. We are glad that the reviewer found our contribution to constructing the gradient optimization framework. We have summarized the updating of our revision in [Summary of Paper Updating](https://openreview.net/forum?id=8Wuvhh0LYW&noteId=YV1oX701CV) and provided an in-depth discussion about the novelty of our method in [Genera Response](https://openreview.net/forum?id=8Wuvhh0LYW&noteId=D6esYYnEi2). Below, we address each identified weakness. \n\n\n\n**Q1**: The novelty is limited. The overall framework is based on two existing methods.\n\n**A1:** Thanks for the comment. We argue that although optimally determining clipping threshold and equivalent parameters have been explored in existing literature, our proposed OmniQuant is still novel enough by the optimal design. We clarify our novelty from $4$ aspects. 1) the first differentiable quantization framework for LLMs; 2) the novelty of LET; 3) the novelty of LWC; 4) the seamless combination of LWC and LET. Please refer [General Response](https://openreview.net/forum?id=8Wuvhh0LYW&noteId=D6esYYnEi2) for more details.\n\n\n\n**Q2:** The learnable idea is not new. In Outlier Suppression+, the scaling has been designed to be learned via a scheme that does not depend on gradient.\n\n**A2:** Respecting your observation, we highlight that the novelty of our learnable approach lies in differential gradient updates for various quantization parameters. As illustrated in Figure A3, OmniQuant's differential gradient-based learning significantly reduces the quantization error compared with the grid-searching method employed in Outlier Suppression+. In experiments, Table 2 in the paper demonstrates the superiority of OmniQuant to OS+.\n\n\n\n**Q3:** An in-depth comparison needs to be provided, including the experimental perspective and the theoretical perspective.\n\n**A3:**  OmniQuant differs from Outlier Suppression+ in several aspects, including parameter optimization through gradient updates, block-wise optimization, expansion of equivalent transformation to attention operations, and the introduction of learnable weight clipping (LWC). Details are as follows.\n\n- Both OmniQuant and OS+ try to find the optimal equivalent transformation parameters. OS+ solves this problem by pre-defined shifting strength and grid-searched scaling parameters. However, OmniQuant obtains both shifting and scaling parameters through gradient optimization.\n\n- OmniQuant leverages block-wise optimization while OS+ executes grid searching through a mix of single linear layer and multiple linear layers objective(Eq.6 and Eq.7 in their paper). Block-wise optimization considers interaction within a block and can produce better performance as demonstrated in BRECQ. However, it is a challenge for OS+ to expand to block-wise quantization, which would significantly enlarge the solution space and increase the time of grid searching.\n\n- LET expends the equivalent transformation to attention operation (Eq. (5) in our paper), but OS+ only does the equivalent transformation on linear layers. \n\n- OmniQuant also introduces learnable weight clipping (LWC), which further facilitates the low-bits (2,3) weight-only quantization, while Outlier Suppression only supports for weight-activation quantization.\n\nWe also incorporated a comparison with Outlier Suppression+ in Table 2 . An overview is as follows.\n\n| W4A4        | Quantization | Average Acc. |\n| ----------- | ------------ | ------------ |\n| LLaMa-1-7B  | OS+          | 48.43        |\n| LLaMa-1-7B  | OmniQuant    | 52.65        |\n| LLaMa-1-13B | OS+          | 49.86        |\n| LLaMa-1-13B | OmniQuant    | 54.37        |\n| LLaMa-1-30B | OS+          | 52.62        |\n| LLaMa-1-30B | OmniQuant    | 56.63        |\n| LLaMa-1-65B | OS+          | 52.52        |\n| LLaMa-1-65B | OmniQuant    | 59.23        |"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1585/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700225196401,
                "cdate": 1700225196401,
                "tmdate": 1700323935336,
                "mdate": 1700323935336,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "za2mI1rjZ9",
                "forum": "8Wuvhh0LYW",
                "replyto": "wKCbLSlSOB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1585/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1585/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ZyAK (Part: 2/2)"
                    },
                    "comment": {
                        "value": "**Q4:** The optimization based on little data and backward propagation makes the learning easy to be overfitted. More validation should be conducted to prove the generalization ability of this learning.\n\n**A4:** Acknowledging the potential for overfitting, we highlight that OmniQuant mitigates this issue through a constrained solution space. Specifically, optimization focuses on LET and LWC rather than all weights. Our results in Table A11 of the Appendix demonstrate OmniQuant's superior performance even with only 16 calibration samples. Additionally, we present performance on MMLU to further showcase the generalization ability of our approach.\n\n| LLaMa-1-7B (FP: 38.41 ) | W4A16g128 | W3A16g128 | W2A16g128 | W4A4  |\n| ----------------------- | --------- | --------- | --------- | ----- |\n| RTN                     | 37.37     | 33.43     | 22.55     | 23.31 |\n| GPTQ                    | 35.39     | 30.53     | 23.83     | -     |\n| AWQ                     | 37.71     | 35.43     | 22.58     | -     |\n| Outlier Suppression+    | -         | -         | -         | 25.72 |\n| OmniQuant               | 37.50     | 35.60     | 26.03     | 26.93 |\n\n**Q5:** There are some new kinds of ways to decompose the outliers, e.g., https://arxiv.org/abs/2310.08041. Comprehensive experiments are suggested to further enrich the validation.\n\n**A5:** Thanks for your suggestion. Your referred paper QLLM (https://arxiv.org/abs/2310.08041) alleviates outlier channels by disassembling instead of scaling. In my view, Outier disassembling only considers the outlier channels. However, LET in our paper, as shown in Figure A2, can also adjust the range of normal channels to achieve a more quantization-friendly distribution. Therefore, the combined disassembling operation of QLLM and LET of our paper may further boost the performance of quantization models.  We will undertake this exploration after QLLM releases its code and engage in discussions with the QLLM team for further insights.\n\nWe sincerely appreciate the time and efforts you have dedicated to reviewing our paper. Should you have any further inquiries, please let us know and we are more than delighted to discuss with you.\n\n[1] Brecq: Pushing the limit of post-training quantization by block reconstruction"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1585/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700225220187,
                "cdate": 1700225220187,
                "tmdate": 1700225860941,
                "mdate": 1700225860941,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TIawWtCq4D",
                "forum": "8Wuvhh0LYW",
                "replyto": "wKCbLSlSOB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1585/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1585/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Last day reminder and looking forward to discussion"
                    },
                    "comment": {
                        "value": "Dear reviewer ZyAK,\n\nThanks again for your valuable time and insightful comments. As the deadline for the Author/Reviewer discussion is approaching, it would be nice of you to let us know whether our answers have solved your concerns so that we can better improve our work. We are happy to provide any additional clarifications that you may need.\n\nBest regards!"
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1585/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700627895460,
                "cdate": 1700627895460,
                "tmdate": 1700627895460,
                "mdate": 1700627895460,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "I4ox934Ez5",
            "forum": "8Wuvhh0LYW",
            "replyto": "8Wuvhh0LYW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1585/Reviewer_FuSa"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1585/Reviewer_FuSa"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes OmniQuant,  a novel quantization technique for large language models (LLMs). OmniQuant introduces two learnable approaches to calibrate the quantized model, which are Learnable Weight Clipping (LWC) and Learnable Equivalent Transformation (LET). The calibration is conducted in a block-wise manner and uses gradient updates to minimize the quantization error. The paper evaluates OmniQuant on various LLMs, quantization configurations, and natural language tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The LWC method proposed in the paper is simple yet effective, outperforming previous clipping-based approaches. The LET method addresses the shortcomings of SmoothQuant and contributes to better activation quantization performance. \n- The OmniQuant framework can be applied to both weight-only quantization and weight & activation quantization. The calibration process is relatively simple and fast.\n- Comprehensive ablation studies are conducted to analyze the effectiveness of each proposed technique."
                },
                "weaknesses": {
                    "value": "- It would be beneficial to have additional experiments on more complex tasks. I am wondering how OmniQuant impacts the reasoning ability of LLMs, which can be evaluated by MMLU. GPT-4 evaluation is a bit ad-hoc nowadays, and there are also several better benchmarks to measure the instruction-tuned models performance, such as MT-Bench or AlpacaEval (correction: should be AlpacaEval instead of AlpacaFarm). Evaluating some stronger chatbots like Vicuna-v1.5 on them should be conducted.\n- Some strong related work is not discussed or compared, such as SpQR [1] and SqueezeLLM [2]. For instance, SqueezeLLM outperforms the proposed approach for wiki and c4 perplexity on LLaMA v1 7b and 13b under 3-bit and 4-bit weight-only quantization settings (see table 1 in their paper). Additional discussion and results should be added to compare OmniQuant with them. \n\n[1] Dettmers, T., Svirschevski, R., Egiazarian, V., Kuznedelev, D., Frantar, E., Ashkboos, S., Borzunov, A., Hoefler, T. and Alistarh, D., 2023. SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression. arXiv preprint arXiv:2306.03078.  \n[2] Kim, S., Hooper, C., Gholami, A., Dong, Z., Li, X., Shen, S., Mahoney, M.W. and Keutzer, K., 2023. SqueezeLLM: Dense-and-Sparse Quantization. arXiv preprint arXiv:2306.07629."
                },
                "questions": {
                    "value": "Please address the weaknesses mentioned above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "na"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1585/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1585/Reviewer_FuSa",
                        "ICLR.cc/2024/Conference/Submission1585/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1585/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698834373684,
            "cdate": 1698834373684,
            "tmdate": 1700172189214,
            "mdate": 1700172189214,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "R4tBpF9NNU",
                "forum": "8Wuvhh0LYW",
                "replyto": "I4ox934Ez5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1585/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1585/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer FuSa"
                    },
                    "comment": {
                        "value": "Thanks for your thoughtful review that will help us strengthen the manuscript. We have summarized the updating of our revision in [Summary of Paper Updating](https://openreview.net/forum?id=8Wuvhh0LYW&noteId=YV1oX701CV). Below, we address each identified weakness. \n\n**Q1**: Results on MMLU.\n\n**A1:** As shown in the following table, we report the average accuracy on MMLU, showcasing that OmniQuant consistently achieves comparable or superior performance in reasoning ability. We have added this experiment in Table A16 of our revision.\n\n| LLaMa-1-7B (FP: 38.41 ) | W4A16g128 | W3A16g128 | W2A16g128 | W4A4  |\n| ----------------------- | --------- | --------- | --------- | ----- |\n| RTN                     | 37.37     | 33.43     | 22.55     | 23.31 |\n| GPTQ                    | 35.39     | 30.53     | 23.83     | -     |\n| AWQ                     | 37.71     | 35.43     | 22.58     | -     |\n| Outlier Suppression+    | -         | -         | -         | 25.72 |\n| OmniQuant               | 37.50     | 35.60     | 26.03     | 26.93 |\n\n\n\n**Q2:**  Evaluating instruction-tuned models on MT-Bench or AlpacaFarm and evaluating stronger Chatbot.\n\n**A2:** We have quantized Vicuna-v1.5 using W3A16g128 quantization and compared the performance of different methods through MT-Bench. The results reveal that OmniQuant consistently outperforms RTN and AWQ. \n\n| Vicuna-v1.5-7B-W3A16g128 | OmniQuant Win | Tie  | OmniQuant Lost | Win rate |\n| ------------------------ | ------------- | ---- | -------------- | -------- |\n| Omniquant v.s. RTN       | 101           | 239  | 20             | 83.4 %   |\n| Omniquant v.s. AWQ       | 70            | 225  | 65             | 51.8%    |\n\n\n\n**Q3:** Discussion and comparison with related work- SpQR and SqueezeLLM\n\n**A3:** Thanks for your suggestion, we have included the discussion and comparison with SqQR and SqueezeLLM in Section A7. However, we would like to emphasize two crucial points. Firstly, SpQR and SqueezeLLM are concurrent works, both available on arXiv in June 2023, which falls within four months of the ICLR submission deadline (Please refer to the policy in the last Q&A at https://iclr.cc/Conferences/2024/ReviewerGuide ). Secondly, Although OmniQuant may perform slightly worse than SqueezeLLM, our OmniQuant focuses on uniform (INT) quantization which offers simplicity and flexibility and has strong hard-ware support. Moreover, OmniQuant achieves competitive performance in both weight-only quantization and weight-activation quantization. In contrast, SpQR and SqueezeLLM only support weight-only quantization. We believe this distinction adds valuable context to the comparison. \n\n\n\nWe sincerely appreciate the time and efforts you have dedicated to reviewing our paper. Should you have any further inquiries, please let us know and we are more than delighted to discuss with you and run more experiments for any pieces of your interests in our work.\n\n\n\n[1] Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1585/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700225056924,
                "cdate": 1700225056924,
                "tmdate": 1700225056924,
                "mdate": 1700225056924,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TcOpZxlCwO",
                "forum": "8Wuvhh0LYW",
                "replyto": "I4ox934Ez5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1585/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1585/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Last day reminder and looking forward to discussion"
                    },
                    "comment": {
                        "value": "Dear reviewer FuSa,\n\nThanks again for your valuable time and insightful comments. As the deadline for the Author/Reviewer discussion is approaching, it would be nice of you to let us know whether our answers have solved your concerns so that we can better improve our work. We are happy to provide any additional clarifications that you may need.\n\nBest regards!"
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1585/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700627848514,
                "cdate": 1700627848514,
                "tmdate": 1700627848514,
                "mdate": 1700627848514,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rPlEUOaUpy",
                "forum": "8Wuvhh0LYW",
                "replyto": "TcOpZxlCwO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1585/Reviewer_FuSa"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1585/Reviewer_FuSa"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Dear Authors,\n\nThank you for the comprehensive rebuttal and the clarifications provided. I have also read other reviews and rebuttals. At present, I have no further questions and would like to keep my current positive rating."
                    }
                },
                "number": 24,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1585/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700684428781,
                "cdate": 1700684428781,
                "tmdate": 1700684428781,
                "mdate": 1700684428781,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]