[
    {
        "title": "Robustness Guarantees for Adversarial Training on Non-Separable Data"
    },
    {
        "review": {
            "id": "a7BeMZWK80",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3898/Reviewer_4oeH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3898/Reviewer_4oeH"
            ],
            "forum": "SQLDXQ3IG8",
            "replyto": "SQLDXQ3IG8",
            "content": {
                "summary": {
                    "value": "This paper provides convergence and generalization guarantees for adversarial training of two-layer neural networks on non-separable data, for both smooth and non-smooth activate functions. The experimental results are consistent with the theory."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- This paper takes a great step toward understanding adversarial training (non-smooth activate functions, non-separable data, and goes beyond the NTK regime)"
                },
                "weaknesses": {
                    "value": "- The model is a two-layer neural network with the weights for the second layer fixed, which is too simple compared with DNNs. And the assumption about the data is too simple.\n- It seems that the paper [1] is very relevant to this work, detailed discussion about the similarities, differences, and the superiority of this paper should be added.\n\n[1] Feature Purification: How Adversarial Training Performs Robust Deep Learning."
                },
                "questions": {
                    "value": "See weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3898/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3898/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3898/Reviewer_4oeH"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3898/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697445924462,
            "cdate": 1697445924462,
            "tmdate": 1699636349525,
            "mdate": 1699636349525,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "3p7Ca8As4V",
                "forum": "SQLDXQ3IG8",
                "replyto": "a7BeMZWK80",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3898/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3898/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 4oeH"
                    },
                    "comment": {
                        "value": "We thank the reviewer for pointing our the related references and will incorporate them into the final manuscript."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3898/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700173487122,
                "cdate": 1700173487122,
                "tmdate": 1700173487122,
                "mdate": 1700173487122,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "McOzS9oGwd",
            "forum": "SQLDXQ3IG8",
            "replyto": "SQLDXQ3IG8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3898/Reviewer_jW96"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3898/Reviewer_jW96"
            ],
            "content": {
                "summary": {
                    "value": "The authors consider adversarial training for neural networks of one hidden layer and prove that, under certain assumptions, it converges to an arbitrarily small robust loss. An important aspect of the theoretical results of the paper is that they hold  for NN of finite width. The results are empirically investigated on an example with a synthetic dataset."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Providing convergence guarantees for adversarial training of neural networks is certainly a problem of interest for the ICLR community\n\n- In contrast with the existing literature, which mostly focus on providing theoretical results in the infinite-width regime, the results of the authors also hold for neural networks of finite width. I see this as an important contribution\n\n- The paper is well written"
                },
                "weaknesses": {
                    "value": "- My main issue with the submission is that the assumptions made by the authors are quite restrictive for most applications. For instance, the assumption that the number of training data is much lower than the dimension of the input space, but still larger than C log(1/\\delta) is restrictive for many applications where neural networks are employed and where the amount of data is generally larger than the input dimension. \n\n- Experimental results are only limited to a synthetic example. The authors also report some experiments on MNIST on the Appendix. Why not reporting them in the main text?"
                },
                "questions": {
                    "value": "- Where the constant C in Assumption 1 comes from? Can you bound it for some specific applications?\n\n- Why in the experiments in Figure 2 robust accuracy seems to decrease with increasing the dimension d for a fixed perturbation ratio? Is this in line with your theoretical results?\n\nMinor:\n\n- why do you need to overload the notation for the l_2 norm in the first line of Page 3. Can't you simply use always || \\cdot || ?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3898/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698504044839,
            "cdate": 1698504044839,
            "tmdate": 1699636349449,
            "mdate": 1699636349449,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8rfTFKeyto",
                "forum": "SQLDXQ3IG8",
                "replyto": "McOzS9oGwd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3898/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3898/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer jW96"
                    },
                    "comment": {
                        "value": "**[W1]** \n\nHigh dimensional settings, wherein $d \\gg n$, are common in applications of machine learning; e.g., bioinformatics, computer vision applications involving video/multimedia data, natural language processing, etc. The requirement that $n$ is larger than a constant, i.e., $n > C \\log(1/\\delta)$ is also very mild. Furthermore, we note that while we limit ourselves to these settings in order to build a theory, in application we may see that our results extend beyond this setting. Also, this paves a path for future work to explore which assumptions are necessary and which are not. \n\n\n**[W2]** \n\nWe report MNIST in the Appendix due to space limitations. We can switch them if the reviewer prefers that. \n\n**[Q1]** \n\nIn Section 3.1, $C$ depends on various problem parameters including $\\gamma, H, c_1,c_2,\\zeta$ and $\\kappa$. \n\n**[Q2]** \n\nYes, this empirical result is in line with our theorem. Higher dimension leads to a lower robust accuracy as per our result, since larger $d$ leads to a larger upper bound on the robust test error.\n\n**[Q3]** \nYes we can simply use $\\| \\cdot \\|$."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3898/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700173350296,
                "cdate": 1700173350296,
                "tmdate": 1700173350296,
                "mdate": 1700173350296,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "u7AbUP54yI",
            "forum": "SQLDXQ3IG8",
            "replyto": "SQLDXQ3IG8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3898/Reviewer_GnZf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3898/Reviewer_GnZf"
            ],
            "content": {
                "summary": {
                    "value": "This paper provides a theoretical analysis of adversarial training. Specifically, it establishes the convergence guarantees for adversarial training of a specific two-layer neural network and provides generalization guarantees for both the clean test error and the robust test error. Additionally, the paper conducts experiments to validate the theoretical results."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Compared to previous theoretical work on adversarial training, this paper does not require some strong assumptions such as linear separability or lazy training."
                },
                "weaknesses": {
                    "value": "1: The setting, proof approach, and techniques in this paper completely follow the previous works [1, 2] for benign overfitting, but this is a high-dimensional data setting that is different from the typical environment of robustness problems.\n\n2: The technical difficulty of this paper is mainly the processing of the adversarial part. However, since the attack intensity $\\alpha$ needs to take a very small value, such an expansion does not have great technical difficulty and contribution.\n\n3: While the authors claim that the results in this paper are applicable to any width, for overparameterized networks which $m\\gg n, d$, the results presented may lack significance.\n\n4: The discussion of overfitting with adversarial training is unconvincing. Such a discussion can only show that the results of this paper are not inconsistent with the phenomenon in [3], but it still cannot provide a reasonable explanation for the phenomenon in [3]."
                },
                "questions": {
                    "value": "I believe that in the setting of this paper, which is similar to the [1, 2] with a relatively small $\\alpha$, one can potentially obtain a similar bound for robust error even without using adversarial training, by using standard SGD. I wonder what the author's perspective on this is.\n\n[1] Spencer Frei, Niladri S Chatterji, and Peter Bartlett. Benign overfitting without linearity: Neural network classifiers trained by gradient descent for noisy linear data. In Conference on Learning Theory, pages 2668\u20132703. PMLR, 2022.\n\n[2] Xingyu Xu and Yuantao Gu. Benign overfitting of non-smooth neural networks beyond lazy training. International Conference on Artificial Intelligence and Statistics, pages 11094\u201311117, 2023.\n\n[3] Leslie Rice, Eric Wong, and Zico Kolter. Overfitting in adversarially robust deep learning. In International Conference on Machine Learning, pages 8093\u20138104. PMLR, 2020."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3898/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3898/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3898/Reviewer_GnZf"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3898/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698618649923,
            "cdate": 1698618649923,
            "tmdate": 1699636349190,
            "mdate": 1699636349190,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "EBUCTa2cyz",
                "forum": "SQLDXQ3IG8",
                "replyto": "u7AbUP54yI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3898/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3898/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer GnZf"
                    },
                    "comment": {
                        "value": "**[W1, W2]** \n\n**Regarding technical difficulty and contribution.**\n\nAdding the adversarial perturbations requires certain ingenuity for the proofs to go through; they are not straightforward as you can check details from the Appendix. We here point out the contributions\n1. We fix mistakes in the proofs \nIn prior work of Frei et al. (2022) and Xu and Gu (2023). Please see **discussion in Appendix Section B.1 first paragraph** for details on technical improvements over Frei et al. (2022), and Prop. B.12-Lemma B.15 for a more rigorous concentration argument over Xu and Gu (2023). \n2. The proof of our convergence guarantee also differs from the original -- our Lemma B.11 has a different proof strategy than Lemma 14 in Frei et al. (2022). Following the previous work to give a result in an adversarial setting leads to a vacuous bound. Please see the beginning of Section 4.2. \n3. We establish a robust test error lower bound and explore its relationship with the upper bound, suggesting an almost tight bound under certain conditions. These are interesting findings not present in previous literature.\n\n\n\n\n\n**Regarding the perturbation size.**\n\nWe disagree with the reviewer's comment that ``intensity $\\alpha$ needs to take a very small value\". For smooth activation function, the perturbation size can be as large as $\\alpha=O(\\|\\mu\\|)$ when $d=\\Theta(n\\|\\mu\\|^2)$.\n\n\n\n**Regarding high dimensionality data in robust setting.**\n\nMost ML problems are high dimensional. The ML community has increasingly been interested in theoretically understanding high-dimensional data under robust learning framework; see Shafahi et al. (2019), Mahloujifar et al. (2019), Richardson and Yair (2021), for example.\n\n$$$$\n\n**[W3]** \n\n**Regarding any width networks.**\n\n We respectfully disagree with the reviewer's comment. Our guarantees hold for networks of any width. This does not mean that our guarantees become insignficant in overparamterized settings. Furthermore, the convergence and generalization guarantees for overparametrized networks in the lazy regime or the Neural Tangent Kernel (NTK) regime (which is the dominant framework in theoretical deep learning), **do not extend to the adversarial setting.** The work of Wang et al. (2022) shows that networks trained in this regime are not robust. Our work shows that adversarial training actually returns networks that are outside the lazy regime (see Proposition 3.5) and are guaranteed to generalize adversarially robustly.  \n\n$$$$\n\n**[W4]**\n\n**Regarding overfitting discussion.**\n\nNowhere in our discussion do we claim to explain the robust overfitting phenomenon observed in Rice et al. (2020). We simply remark that our results are not in conflict with their findings -- that remark is in no way trying to explain any phenomenon. \n\n$$$$\n\n**[Q]** \n\nThat is a good question. We are not sure if the result will hold for SGD, but for GD it is possible that we can give Lemma 4.3, but with a different bound. Note though, that is only an intermediate step and we have not worked out all the details. \n\n$$$$\n\nReference:\n\n[1] Frei et al. Benign overfitting without linearity: Neural network classifiers trained by gradient descent for noisy linear data. In Conference on Learning Theory, 2022.\n\n[2] Xu and Gu. Benign overfitting of non-smooth neural networks beyond lazy training. International Conference on Artificial Intelligence and Statistics, 2023.\n\n[3] Shafahi et al. \"Are adversarial examples inevitable?.\"\u00a0In Seventh International Conference on Learning Representations, 2019.\n\n[4] Mahloujifar et al. \"The curse of concentration in robust learning: Evasion and poisoning attacks from concentration of measure.\"Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 33. No. 01. 2019.\n\n[5] Richardson and Yair. \"A bayes-optimal view on adversarial examples.\"The Journal of Machine Learning Research 22.1 (2021): 10076-10103.\n\n[6] Rice et al. Overfitting in adversarially robust deep learning. In International Conference on Machine Learning, PMLR, 2020.\n\n[7] Wang et al. \"Adversarial robustness is at odds with lazy training.\"Advances in Neural Information Processing Systems, 2022."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3898/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700173098227,
                "cdate": 1700173098227,
                "tmdate": 1700173414470,
                "mdate": 1700173414470,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rKUbR3z65M",
                "forum": "SQLDXQ3IG8",
                "replyto": "u7AbUP54yI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3898/Reviewer_GnZf"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3898/Reviewer_GnZf"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Authors,\n\nThank you for the response. I still have some concerns about this paper:\n\n1: Regarding the perturbation size:\n\nIn my understanding,  $2\\times \\mu$ actually describes the separation distance between the 'centers' of two different classes. Therefore, for Section 3.1, the assumption $\\alpha \\leq 0.99 \\mu$ does not represent a very large perturbation size. And, in Section 3.2, $\\mu$ is large, and $\\alpha \\leq C$ represents a very small perturbation size.\n\n2: Regarding any width networks:\n\nBased on assumptions B.3 and B.7, when the network width tends to infinity, both the number of data n and data dimensions d should also tend to infinity for Theorem 3.3 to hold. That is to say, for fixed n and d, Theorem 3.3 does not hold for any width.\n\n3: Regarding \"lazy training\":\n\nI cannot agree with the author's response: \"Our work shows that adversarial training actually returns networks that are outside the lazy regime (see Proposition 3.5) and are guaranteed to generalize adversarially robustly.\" \n\nThe relationship between the training regime of neural networks and their initialization has been studied [1]. A non-lazy training regime is not induced by adversarial training; without adversarial training, neural network training is non-lazy when the network initialization is sufficiently small, some papers with similar settings also demonstrated similar non-lazy training results without using AT [2]. In fact, there have been experiments indicating that adversarial training becomes 'lazy' earlier than standard training in a lazy training regime [3].\n\n4: Regarding my raised question:\n\n\nI think the authors did not provide a satisfactory answer. Directly setting $\\alpha = 0$ (in training but remaining in robust loss) in the Thm 3.1 and Thm 3.3 would result in the degradation of the results to simple gradient descent. And, the bounds for Theorems 3.1 and 3.3 in this paper are optimal at $\\alpha = 0$ (Because the $\\alpha$ in robust loss only affects the last $\\alpha$ that appears in the Thm 3.1). This implies that under the settings of this paper, a good robust loss can be attained using gradient descent without adversarial training (and potentially even better than adversarial training). This seems contradictory to the motivation of this paper.\n\n\nThank you,\n\nReviewer GnZf\n\nReference:\n\n[1] Tao Luo, et al. Phase diagram for two-layer relu neural networks at infinite-width limit. JMLR, 2021.\n\n[2] Spencer Frei, et al. Benign Overfitting without Linearity: Neural Network Classifiers Trained by Gradient Descent for Noisy Linear Data. COLT 2022.\n\n[3] Nikolaos Tsilivis, et al. What Can the Neural Tangent Kernel Tell Us About Adversarial Robustness? NeurIPS 2022."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3898/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700516847315,
                "cdate": 1700516847315,
                "tmdate": 1700700865388,
                "mdate": 1700700865388,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qzjSKquLVF",
                "forum": "SQLDXQ3IG8",
                "replyto": "u7AbUP54yI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3898/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3898/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the discussion.\n\n*Regarding the perturbation size and networks of any width:*\n\nBelow we discuss smooth activation function and non-smooth activation function separately. Please remember that the analysis for non-smooth activation function is more challenging.\n\nFor smooth activation functions:\n\n1. The best achievable condition is $\\alpha \\leq 0.99||\\mu||$; $\\alpha > ||\\mu||$ results in a robust test error of at least $0.5$ (refer to Theorem 3.4). This is not surprising because even for linearly separable data, if the margin is $\\mu$ and class conditionals (i.e., $p(x|y)$) are supported on (marginal) hyper-planes, i.e., at a distance of $\\mu$ from each other, then a perturbation of more than $\\mu$ will result in error of $0.5.$\n\n2. We do not require any assumptions on width for this part.\n\nFor non-smooth activation functions:\n\n1. First of, we do discuss in paper how we can relax this assumption also to $\\alpha\\leq 0.99\\|\\mu\\|$ if we select a data-dependent initialization. See Remark 4.4 and Remark B.16. But, note that in literature, we are most concerned about settings where the adversarial perturbations are so small, that they are nearly imperceptible. This is why we consider $\\alpha\\leq \\sqrt{n/d}||\\mu||$ (or $\\alpha\\leq$ constant) which is reasonable in high-dimensional setting. Again, existing literature, e.g., [1,2,3], focuses on understanding the robustness guarantees with perturbation of size $O(||x||/\\sqrt{d})$.\n\n2. We note that what we mean when we say that ``our results hold for networks of any width'' is that we do not require any strong assumptions (e.g., extreme overparametrization) on the network. This is standard usage to characterize results of this type and very typical in nearly all papers in theory of deep learning. \nWe only require very mild assumptions on width, specifically $\\Omega(\\log(n/\\delta)) \\leq m \\leq O(\\exp(n)\\delta)$. These requirements are also found in [4]. \n\n[1] Bartlett et al. \"Adversarial examples in multi-layer random relu networks.\"Advances in Neural Information Processing Systems 34 (2021): 9241-9252.\n\n[2] Montanari and Wu. \"Adversarial examples in random neural networks with general activations.\" Mathematical Statistics and Learning 6.1 (2023): 143-200.\n\n[3] Wang et al. \"Adversarial robustness is at odds with lazy training.\"Advances in Neural Information Processing Systems, 2022.\n\n[4] Xu and Gu. Benign overfitting of non-smooth neural networks beyond lazy training. International Conference on Artificial Intelligence and Statistics, 2023.\n\n\n\n$     $\n\n\n\n\n*Regarding \"lazy training\":*\n\n**Nowhere** in our paper or in our response, do we claim that a non-lazy regime is ONLY induced by adversarial training. All we are saying is that our analysis shows that for the setting we consider, adversarial training returns networks that are outside the lazy regime and, therefore, our results are not in contradiction with previous results that suggest that lazy regime is at odds with robustness. We feel that our comment was twisted badly here for the sake of a baseless argument. In any case, our result does not contradict with [1], and we are happy to include [1] in our related work discussion.\n\n[1] Tao Luo, et al. Phase diagram for two-layer relu neural networks at infinite-width limit. JMLR, 2021.\n\n\n$     $\n\n\n\n\n*Regarding the question.*\n\nIf $\\alpha=0$, the L.H.S. of the bound is no longer robust loss. \n\nRegardless, we agree that there exist a similar form (albeit with different constants in the bound) of robust generalization guarantees for neural network trained non-adversarially with gradient descent. We want to point out that this observation doesn't contradict the motivation of our work. We would like to emphasize again that our results serve as the first step towards understanding the adversarial trained neural network without linearly separable assumption and beyond lazy training regime."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3898/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700601330328,
                "cdate": 1700601330328,
                "tmdate": 1700601649228,
                "mdate": 1700601649228,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "O4fwc9NQai",
                "forum": "SQLDXQ3IG8",
                "replyto": "qzjSKquLVF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3898/Reviewer_GnZf"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3898/Reviewer_GnZf"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Authors,\n\nFirstly, I'm glad that the author agrees with my viewpoint: \"There exists a similar form of robust generalization guarantees for a neural network trained non-adversarially with gradient descent.\" and \"the results in this paper regarding non-smooth activation functions are not entirely applicable to networks of any width\". For the author's other responses, I have the following supplements:\n\nRegarding the comparison between adversarial training and non-adversarial training with gradient descent:\n\nIn the author's response, there's a statement: \"If $\\alpha = 0$, the L.H.S. of the bound is no longer robust loss.\" So, I want to clarify a potential misunderstanding the author might have from my previous response.\n\nConsider Theorem 3.1, there are three $\\alpha$. In the order of appearance, the first two $\\alpha$ are associated with $\\alpha$ in adversarial training (Algorithm 1), denoted as $\\alpha_1$, while the third $\\alpha$ is associated with $\\alpha$ in the robust loss, denoted as $\\alpha_2$ (as $\\alpha_1$ and $\\alpha_2$ are mutually independent due to the separation of the robust loss impact in the first step of the proof in the paper). As I mentioned earlier, directly setting $\\alpha_1 = 0$ yields a bound with better constants compared to adversarial training. This setting corresponds to non-adversarial training with the same robust loss analyzed in this paper.\n\nI don't deny that the author provides new bounds for adversarial training. However, both from the results and technically, this bound is derived from adding small perturbations under non-adversarial training (even neglecting almost all adversarial directions and directly using random perturbations, resulting in worse constants than non-adversarial training). In my view, this doesn't effectively inform our understanding of adversarial training.\n\nRegarding lazy training:\n\nMy previous response explicitly stated my disagreement with the statement in the response section, where the comparison between \"the NTK network of non-adversarial training is lazy\" and \"the non-laziness of networks trained by adversarial training in the specific setting of this paper\" without further elaboration. Such a statement in a discussion section of a paper focused on adversarial training will create ambiguity for readers. As I mentioned earlier, the non-lazy training regime of the network in the setting of this paper is a natural consequence and should be seen as a supplementary clarification to the main theorem of the paper.\n\nMoreover, I disagree with this statement in the author's newest response: \"Our results are not in contradiction with previous results that suggest that lazy regime is at odds with robustness.\" In fact, under the specific data setting of this paper, there exists a similar form of robust generalization guarantees for a neural network under the lazy training regime by applying similar perturbations in this paper to [1]. This is also why I mentioned in my previous response that this specific high-dimensional data setting is not a reasonable setting for studying robustness.\n\nIn conclusion, I believe the author needs to modify the assumptions, results, and proofs of the paper to give more meaningful theoretical results, add necessary discussions, and revise inaccurate statements.\n\nThank you,\n\nReviewer GnZf\n\nReference:\n\n[1] Zhenyu Zhu, et al. Benign Overfitting in Deep Neural Networks under Lazy Training. ICML, 2023."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3898/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700671557973,
                "cdate": 1700671557973,
                "tmdate": 1700671557973,
                "mdate": 1700671557973,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YaQ0l1PAV7",
                "forum": "SQLDXQ3IG8",
                "replyto": "u7AbUP54yI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3898/Reviewer_GnZf"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3898/Reviewer_GnZf"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Authors,\n\nFirstly, I'd like to clarify my previous responses.\n\n1. $\\alpha$:\n\nI have corrected the possible ambiguities in my previous responses. I think I was clear in my previous response regarding the origin of $\\alpha$ in the results.  The author seems to be evading this issue.\n\n2. Lemma 4.3:\n\nThe proof of the lemma relies on Lemma 4.9 in [1], differing only by constants. Moreover, Lemma 4.9 in [1] itself is a non-adversarial version of this setting. So the author\u2019s previous statement about Lemma 4.3 is actually not true.\n\n3. Lazy training:\n\nMy concern lies in this excerpt:\n\"Furthermore, the convergence and generalization guarantees for overparametrized networks in the lazy regime or the Neural Tangent Kernel (NTK) regime (which is the dominant framework in theoretical deep learning), do not extend to the adversarial setting. The work of Wang et al. (2022) shows that networks trained in this regime are not robust. Our work shows that adversarial training actually returns networks that are outside the lazy regime (see Proposition 3.5) and are guaranteed to generalize adversarially robustly.\"\n\nAnd this recent response: \"Our work shows that adversarial training actually returns networks that are outside the lazy regime (see Proposition 3.5) and are guaranteed to generalize adversarially robustly.\u2019\u2019\n\nBoth statements attempt to establish a connection between adversarial training and the non-lazy regime but overlook the crucial influence of initialization in the lazy/non-lazy regime. This might mislead readers. In fact, under the settings of this paper, whether adversarial training is used or not, it will fall into the non-lazy regime. However, this is more of a formulation issue than a core concern of the paper.\n\n4. Relation between [2] and this paper:\n\nBoth this paper and Section 3 in [2] primarily draw from [1] for their data distributions. The difference lies in initialization, resulting in different training regimes. Their proof strategies are similar (follow [1]). The authors attempt to refute Assumption 6 in [2]. But Assumption 6 applies only to Section 4 in [2]. It does not affect any of the results in Section 3 in [2].\n\n5: \"The perturbation size\":\n\nThe author reiterated the results from the paper: for this issue, smooth activations need perturbations smaller than the distance from the data center to the boundary, while non-smooth activations need perturbations significantly smaller than this distance. I believe further discussion on whether these values are large or small is meaningless. In fact, the data distribution in this paper is considered 'linearly separable' in many references [3][4].\n\n6: \"The width of the network\":\n\nThe author's response stated, \"We do not require any strong assumptions (e.g., extreme over parametrization) on the network,\" which significantly differs from the original statement: \"Our results hold for networks of any width.\"\n\n7: \"The technical difficulty and contribution\":\n\nI previously raised the issue: \"even neglecting almost all adversarial directions and directly using random perturbations,\" and the author seems not to have responded to this.\n\nAs a reviewer, I aimed to discuss and enhance the paper with the author. However, the author's responses seem to diverge from this, so I feel further discussion would be futile. Given the author's evasiveness regarding some crucial issues, I will maintain my score.\n\nReviewer GnZf\n\nReference:\n\n[1] Spencer Frei, et al. Benign Overfitting without Linearity: Neural Network Classifiers Trained by Gradient Descent for Noisy Linear Data. COLT 2022.\n\n[2] Zhenyu Zhu, et al. Benign Overfitting in Deep Neural Networks under Lazy Training. ICML, 2023.\n\n[3] Zhiwei Xu, et al. Benign Overfitting and Grokking in ReLU Networks for XOR Cluster Data. arXiv:2310.02541.\n\n[4] Xuran Meng, et al. Benign Overfitting in Two-Layer ReLU Convolutional Neural Networks for XOR Data. arXiv:2310.01975."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3898/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700700563052,
                "cdate": 1700700563052,
                "tmdate": 1700700602545,
                "mdate": 1700700602545,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "X5HJrqDBuz",
                "forum": "SQLDXQ3IG8",
                "replyto": "u7AbUP54yI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3898/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3898/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer GnZf:\n\n1. Regarding $\\alpha=0$: your claim was blatantly wrong. It was not ambiguous. You have since changed the question to something that has nothing to do with the paper. We have already said that GD would also have similar robustness guarantees. So, entertaining your question further makes no sense but to engage in this meaningless back-and-forth, which has no constructive outcome. We are not being evasive here. You are not willing to see the reason. \n\n2. Regarding Lemma 4.3: What we have said before about Lemma 4.3 is that it is possible to give an analogue of it for GD but with different constants. This is exactly what you say here. What is it about what we said that is not true? You seem to be in an alternate reality where you repeatedly think we are saying something we are not. \n\n3. Regarding lazy training. For the nth time, the prior work shows that networks trained in a lazy regime are not robust. We show that adversarial training in our setting yields networks that are outside the lazy regime. Nowhere in the paper or our response, even the ones you copied here, say anything you claim we say. Again, you are twisting the reality to suit your needs. Nobody is being misled by these statements. You are simply not willing to listen to reason.\n\n4. Regarding reference [2]. Benign overfitting in a lazy regime has nothing to do with adversarial training or robustness. You are trying to make connections that are tangential at best and definitely outside the scope of the paper.\n\n5. Regarding our data distribution being linearly separable, you are flat-out wrong again here. Not only can our class conditionals, i.e., p(x|y), overlap, but we also have non-zero probability $\\beta > 0$ of flipping the labels. \n\n6. Regarding the width. Again, as we said before, **for smooth activation functions, we make NO assumption on width**. For non-smooth activation functions, we do make a mild assumption on width $m$ of the network; we assume that $\\Omega(\\log(n/\\delta)) < m < O(\\exp(n) \\delta)$. We argue that this essentially means that our results hold for networks of any width. This is standard usage in literature as many theoretical results require the networks to be sufficiently over-parametrized. You have an issue with our phrasing that our \"results hold for a network of any width\u2019\u2019\u2014  this is such a minutia; is it really such a big deal? \n\n7. We have already addressed the issue of novel technical contributions, including making the reviewer aware that Lemma 4.1 and Lemma 4.11 in the prior work of Spencer Frei et al. are broken. We provide alternate arguments to give guarantees, and they are sound and rigorous. \n\nThanks for clarifying your stand. None of the reasons you list above are central to the results or technical contributions we make in the paper. You are focusing on minutiae like \"Does this result really hold for a network of any width?\u2019\u2019 or ``is this a comment on training in a lazy regime more generally?\u2019\u2019 Even if these were genuine concerns, none of this can be a basis for rejection. None of your comments justify the low rating. \n\nBest,\nAuthors"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3898/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700712424603,
                "cdate": 1700712424603,
                "tmdate": 1700712923298,
                "mdate": 1700712923298,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "KHcHnIq6xF",
            "forum": "SQLDXQ3IG8",
            "replyto": "SQLDXQ3IG8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3898/Reviewer_ycei"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3898/Reviewer_ycei"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies convergence and generalization guarantees of adversarial training of two-layer neural networks with arbitrary width on non-separable data. It provides theoretical guarantees on both smooth and non-smooth activation functions. For moderately large networks, the paper shows the robust test error behaves differently on different perturbation budgets. The theoretical findings are supported by experiments on both synthetic and real-world data."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is well-written. References are well cited with detailed comparisons."
                },
                "weaknesses": {
                    "value": "The additional assumption in Section 3, which states that $\\phi'(z)z$ and $\\phi(z)$ are close, is hard to grasp. Is it another notion on the Liptshitzness on the activation function? How strong is it to assume that $c_1,c_2=0$ in ReLU network, for example?"
                },
                "questions": {
                    "value": "See discussion in Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No concerns."
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3898/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698856390464,
            "cdate": 1698856390464,
            "tmdate": 1699636348711,
            "mdate": 1699636348711,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "iM1nDbosEx",
                "forum": "SQLDXQ3IG8",
                "replyto": "KHcHnIq6xF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3898/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3898/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ycei"
                    },
                    "comment": {
                        "value": "In Section 3.1, the additional assumption approximates the homogeneous property, which is used for proving the convergence guarantee. \nThis almost homogeneous property is not very important, because it is only used to prove the bound on training error (not test error). From this assumption we showed that if the smooth loss function is close to ReLU or leaky ReLU, we can have a small training error. \n\nAnother example of almost homogeneous function: \n\\begin{equation}\n\\phi(x)= \\begin{cases} \nx &x \\leq 1,\\newline\n\\sqrt{2x-1} &x>1. \\end{cases}\n\\end{equation}\n\nWe are not assuming $c_1=c_2=0$ for ReLU. \nThe approximate homogeneous property holds trivially for ReLU."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3898/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700171721954,
                "cdate": 1700171721954,
                "tmdate": 1700171721954,
                "mdate": 1700171721954,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]