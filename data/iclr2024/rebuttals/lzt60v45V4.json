[
    {
        "title": "Variational Federated Continual Learning"
    },
    {
        "review": {
            "id": "FqdqAqtYUr",
            "forum": "lzt60v45V4",
            "replyto": "lzt60v45V4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2499/Reviewer_ouu6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2499/Reviewer_ouu6"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a Bayesian framework for federated continual learning. For local updates, they combine previous posterior and global posterior into a mix prior to mitigate both the catastrophic forgetting and overfitting encountered in federated continual learning. For global aggregation, they use conflation that minimizes Shannon information loss."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Writing is good. The problem formulation is clear.\n2. They propose a comprehensive Bayesian framework to FCL, including both solutions in local updates and global aggregation.\n3. They provide detailed theoretical analysis.\n4. Experiments are conducted on large-scale complex datasets like CIFAR-100 and Tiny-ImageNet."
                },
                "weaknesses": {
                    "value": "I'm not familiar with previous works in federated continual learning. So I cannot provide a specific assessment regarding the novelty. The performance seems pretty good so I don't have any additional feedback on the limitations of the work."
                },
                "questions": {
                    "value": "How do you compare with this work?\n\n[1] Guo, H., Greengard, P., Wang, H., Gelman, A., Kim, Y., & Xing, E. P. (2023). Federated Learning as Variational Inference: A Scalable Expectation Propagation Approach. arXiv preprint arXiv:2302.04228."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2499/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698601858597,
            "cdate": 1698601858597,
            "tmdate": 1699636186496,
            "mdate": 1699636186496,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "RDLC0gMXWh",
                "forum": "lzt60v45V4",
                "replyto": "FqdqAqtYUr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2499/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2499/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the advice you provided !"
                    },
                    "comment": {
                        "value": "**Comparison with \"Federated Learning as Variational Inference: A Scalable Expectation Propagation Approach\" (FedEP).**\n\nWe compare FedEP with ours in terms of problem setting and method.\n\nProblem setting: FedEP studies the classical federated learning problem, where all clients learn a task together, where all clients jointly learn a task, and the task definition is static and predefined. In our problem setting, each client learns sequentially on a private sequence of tasks, and clients learn different tasks. Our problem is more challenging, and the challenges include catastrophic forgetting due to continual learning, as well as heterogeneity across client tasks.\n\nMethod: FedEP is a centralized federated learning approach, which output is a single global model. Our approach is personalized, and it learns a personalized model for each client. The same point is that both FedEP and ours use variational inference to solve the approximate posterior.\n\nThanks for providing this work, and we have added it to the related work."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2499/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700448641913,
                "cdate": 1700448641913,
                "tmdate": 1700448641913,
                "mdate": 1700448641913,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7lCrOSPT9T",
                "forum": "lzt60v45V4",
                "replyto": "RDLC0gMXWh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2499/Reviewer_ouu6"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2499/Reviewer_ouu6"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the comparison. I would like to keep my original score."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2499/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700650083798,
                "cdate": 1700650083798,
                "tmdate": 1700650083798,
                "mdate": 1700650083798,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "VGlkYumMFs",
            "forum": "lzt60v45V4",
            "replyto": "lzt60v45V4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2499/Reviewer_J3Yt"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2499/Reviewer_J3Yt"
            ],
            "content": {
                "summary": {
                    "value": "The paper addresses federated continual learning (FCL). There are two novelties proposed:\n1. (Primary) There have been variational generalisations, from continual learning (CL) to variational continual learning (VCL) and from federated learning (FL) to variational federated learning (VFL). This paper is the first to introduce variational federated continual learning (VFCL), by introducing variational inference with mixture prior aimed at mitigating both local overfitting and catastrophic forgetting.\n2. (Secondary) In (variational) federated learning, the task of aggregating client posterior distributions into a global distribution is done via parameter average. This paper proposes an alternative: conflation.\nApart from these novelties, the paper includes theoretical analysis on generalisation bounds on VFCL's performance, and experiments and adequate ablation studies showing the advantages of the proposed approach over existing related works."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Good introduction. Good flow. Easy to read. Good review of existing works.\n\nThe contributions are clear and not difficult to understand. Many aspects of the contributions are adequately addressed with ablation studies.\n\nThe theoretical bounds in section 4 are welcome. It would be good if some connection to existing approaches can be established.\n\nExperimental results show reasonable accuracy gains against existing approaches."
                },
                "weaknesses": {
                    "value": "There are mainly 2 weak points in my view.\n\n### Speed\n\nThe paper focuses mainly on accuracy, showing VFCL outperforming other state-of-the-art methods in a number of cases. However, the introduction of variational inference does come with complications in speed. In training, VFCL involves an additional iterative, MCMC sampling step (at optimizing eq (9)). In inference, you also need MCMC or variational inference to form the output distributions. There seems to be very little in the paper that addresses speed concerns. Whether or not speed can be a big concern for using the approach in practice seems to be left unanswered.\n\n### Experimental Datasets\n\nOne weakness is the choice of the datasets conducted for experiments. The paper uses CIFAR100 and TinyImageNet, which are originally designed neither for federated learning nor continual learning. The non i.i.d. property was simulated. While it makes sense from a theoretical point of view, it begs the question of how the model performs in real applications. Would it still be better than existing approaches in real applications? Have you conducted your approach on any real dataset for FCL?"
                },
                "questions": {
                    "value": "I hope to get some feedback regarding the weak points above. Apart from those points, there are some further questions:\n\nEq (8) in training has to be solved using a sampling approach like MCMC. How does that impact training in terms of time?\n\nIn the experiments, Monte Carlo sampling was set to $n=1$ for the purpose of comparing with other approaches. But if the approach were to be used in a real application, is there any study to suggest in which range the value of $n$ should be?\n\nThere is an experiment on varying $\\lambda_p$. Is there a similar experiment on varying $\\lambda_k$? How do we know whether $\\lambda_k$ is a sensitive parameter or not?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2499/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698677880695,
            "cdate": 1698677880695,
            "tmdate": 1699636186429,
            "mdate": 1699636186429,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "brXImmX4Jw",
                "forum": "lzt60v45V4",
                "replyto": "VGlkYumMFs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2499/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2499/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you very much for your comments !"
                    },
                    "comment": {
                        "value": "**Comparison of computational speed.**\n\nWe added the comparison of training and testing time overheads of different methods in Appendix F.4. For the same number of parameters, the training and inference speeds of VFCL are almost the same as those of the other compared methods.\n\nIt is worth noting that, in inference, VFCL does not perform Monte Carlo sampling, which uses the mean of the learned parameter distributions as the model parameters, and thus its inference efficiency is the same as that of a plain model.\n\n\n\n**Would our method still be better than existing approaches on real FCL datasets ?**\n\nTo the best of our knowledge, existing FCL methods all utilize existing datasets to simulate FCL scenarios,  the datasets used in the comparison methods are listed in the following table:\n\n| FCL methods | Experimental Datasets                  |\n| ----------- | -------------------------------------- |\n| FedWeIT     | Overlapped CIFAR-100, NonIID-50        |\n| GLFC        | CIFAR100, ImageNetSubset, TinyImageNet |\n| CFeD        | CIFAR10, CIFAR100, Caltech-256         |\n| FedRecon    | mini-ImageNet                          |\n\nIn addition, due to the privacy issues, it is very difficult to collect an FCL dataset of real scenarios, and to the best of our knowledge, there are no available FCL datasets from real scenarios, which is a research area need to be filled.\n\nTo further validate the effectiveness of our method, we added the comparison experiments on Overlapped CIFAR-100 and NonIID-50, which are benchmark datasets introduced by \"Federated Continual Learning with Weighted Inter-client Transfer\". Detailed results are reported in Appendix F.1, and our method still outperforms the comparison method on both datasets.\n\nWe consider constructing FCL datasets of real scenarios in our future work to better validate the effectiveness of the FCL methods.\n\n\n\n**How does Monte Carlo sampling affect training speed ?**\n\nWe compare the training time for the number of Monte Carlo sampling $n$ from 1 to 10 in Fig. 8 of Appendix F.3. The results show that the training time is linearly correlated with $n$, the training time increases as $n$ increases. It is worth noting that $n$ does not affect the inference time, and if the training budget is sufficient, a larger $n$ can be used for better performance.\n\n\n\n**What range should be recommended for $n$ in real applications ?**\n\nWe compared model accuracies for $n$ from 1 to 10 in Fig. 8 of Appendix F.3. We find that when $n$ is greater than 4, an increase in $n$ will not result in more performance gains, so we recommend setting $n$ no greater than 4.\n\n\n\n**Analytical experiments on varying $\\lambda_k$.**\n\n$\\lambda_k$ controls the relative weight between global and historical knowledge. We compare the accuracy and forgetting under different $\\lambda_k$ in Table 7 in Appendix F.2. We found that both global and historical knowledge play important roles in model performance, and that missing either one leads to a decrease in accuracy. In addition, we found that the absence of historical knowledge causes severe forgetting."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2499/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700448606996,
                "cdate": 1700448606996,
                "tmdate": 1700448606996,
                "mdate": 1700448606996,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "EKSKIHmpvx",
                "forum": "lzt60v45V4",
                "replyto": "brXImmX4Jw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2499/Reviewer_J3Yt"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2499/Reviewer_J3Yt"
                ],
                "content": {
                    "title": {
                        "value": "Thank you!"
                    },
                    "comment": {
                        "value": "Thank you for your rebuttal."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2499/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700550005998,
                "cdate": 1700550005998,
                "tmdate": 1700550005998,
                "mdate": 1700550005998,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Wd5uxc6yDB",
            "forum": "lzt60v45V4",
            "replyto": "lzt60v45V4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2499/Reviewer_Htsy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2499/Reviewer_Htsy"
            ],
            "content": {
                "summary": {
                    "value": "This paper looks at the problem of Federated Continual Learning, and proposes a variational inference solution where there is a variational (Gaussian) distribution over the parameters of a neural network. Previous papers have not looked at FCL using such variational methods. They use a mixture prior to mix together the priors coming from the continual learning and federated learning components. As they are interested in having different (personalised) models on each client, they have a specific global posterior aggregation method too. They also have some theoretical analysis on (upper bound of) the generalisation error of their algorithm."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The FCL (Federated Continual Learning) problem is interesting, and no previous methods have tried a BNN approach to it. It makes a lot of sense to try this. \n\n2. I liked the mixture prior as a simple yet effective way to combine the terms coming from the continual learning and federated learning parts of the problem. \n\n3. I was able to follow the method and explanations quite well (although, it should be said that I am well-versed in BNNs/VI in general). I particularly found Figure 1 helpful. (That said, a small suggestion: the plots in Figure 1 with Gaussians did not make any sense until reading the paper in detail, and mostly just distracted me.) \n\n4. There are two experiments, and I liked the various ablations and comparisons of computation overheads."
                },
                "weaknesses": {
                    "value": "1. My main issue is with the global posterior aggregation method (Section 3.3). Equation 10 makes sense to me (using a mixture of local approximate posterior distributions). However, Equation 11 does not seem related to Equation 10: Equation 11 looks like you have just simply multiplied together all the various local approximate (mean-field) Gaussians. IE it is a *product* of local posterior distributions. Am I misunderstanding something? If this is indeed the case, why write it through Equation 10 instead of just saying you are multiplying together the distributions? \n\n2. Although it is nice to see an attempt at theoretical analysis, I did not see what benefit this brings to the paper or method. Could the authors perhaps discuss how such analysis might be helpful for current or future analysis? The three bullet points at the end of Section 4 all seemed fairly obvious to me (but perhaps that is the point? But then what use is this section?). \n\n3. The experiments only reported one number per method. It is important to run many times (at least 3 or 5) with a mean and std deviation across runs to get an idea of if these results are significant in some way. Further questions about the experiments: \n- Why are the first 2 tasks called 'warm-up tasks'? \n- It is not clear exactly what is being reported in Tables 1 and 2. Is it the average accuracy after training on task t (where t is the number in the first row), or is it the final accuracy on task t's data after training on the last task? What is the Avg over? These details should ideally go in the caption (and the main text)."
                },
                "questions": {
                    "value": "Please see Weaknesses section. \n\nOne more minor point: \n- In the second paragraph of the Introduction, the authors talk about previous methods (regularisation based and knowledge distillation based), which apparently use global knowledge / models. But then the authors say these methods do not combat local overfitting. I thought the point of use global knowledge was to combat local overfitting, so I did not understand why these other methods do not do this? I think the writing could be made clearer here."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2499/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698875069171,
            "cdate": 1698875069171,
            "tmdate": 1699636186359,
            "mdate": 1699636186359,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7fg8o9yUIh",
                "forum": "lzt60v45V4",
                "replyto": "Wd5uxc6yDB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2499/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2499/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you very much for your constructive comments !"
                    },
                    "comment": {
                        "value": "**In global posterior aggregation, Eq. (11) does not seem related to Eq. (10).**\n\nThe method described in Eq. (10) is a straightforward way to obtain a global posterior, however, it is intractable due to the non-i.i.d. client data. Although it is possible to approximate the solution using Monte Carlo sampling, the huge computational overhead and the need for a training dataset make it infeasible in practice.\n\nTherefore, we introduce the conflation, which fuses multiple distributions into a single distribution, the aggregated distribution is with density proportional to the product of the densities of input distribution, and Eq. (11) describes how the parameters in the product of Gaussian distributions are calculated, i.e., the parameters of the final aggregated distribution.\n\nWe have revised this section to provide a more smooth transition and added a detailed explanation of global posterior aggregation in Appendix E.\n\n\n\n**What benefits does theoretical analysis bring to the paper ?**\n\nAs a reminder, the key contribution of our approach is to integrate historical and global knowledge into the mixture prior and migrate the knowledge to the current model, mitigating local overfitting and catastrophic forgetting.\n\nThis theoretical analysis proves that the mixture prior can reduce the upper bound of generalization error (as $\\lambda_{p}$ increases, the overall error decreases), and clarifies the effectiveness of the mixture prior, which is the core contribution of our work.\n\nFurther, we also experimentally (see Table 1) verify the positive effect of the mixed prior on the model performance, which is consistent with the findings of the theoretical analysis. In addition, we also find that too strong prior constraints, while reducing model forgetting, also reduce the model's adaptability to new tasks, ultimately undermining the model's learning ability.\n\nWe have revised this section of the article to provide a clearer explanation of the utility of theoretical analysis for method.\n\n\n\n**The experiments need to be run several times to validate the effectiveness of the method.**\n\nOur comparison experiments were run 3 times. We modified tables 1,2, and added the deviation bar (due to space constraints, we only added it on \"Avg.\", which represents the average of the results of the rows in the table.).\n\n\n\n**Why are the first 2 tasks called 'warm-up tasks' ?**\n\nIn incremental learning, when the model learns the first few tasks, the model generalization performance will be poor due to less exposure to data, and the adaptation to new tasks is slow, which is reflected in the low accuracy of the new task learning, so we call the first few tasks of incremental learning 'warm-up tasks' .\n\n\n\n**What are the evaluation metrics in Tables 1 and 2 ?**\n\nThe metrics under 1-10 in the table are the average accuracies of all the tasks that have been learned, e.g. after learning the 3rd task, the accuracy under 3 is the average of the model's accuracies on tasks 1,2,3.\n\n\"Avg.\" represents the average of the results of the rows in the table, which is the same as in \"Federated Class-Incremental Learning\".\n\nWe revised the caption of the tables and added a detailed description of the evaluation metrics in Appendix A.\n\n\n\n**Why we say previous methods do not combat local overfitting ?** \n\nAlthough existing FCL works have utilized the global model as the initialization for local training, the local model will gradually diverge from the initialized global model after starting local training, and the limited local data of the client will lead to local overfitting, which reduces the generalization ability and convergence rate. Existing methods focus on solving the catastrophic forgetting without explicitly preventing the divergence of the client local learning process, so we argue that these works do not address the overfitting problem.\n\nOur method uses a personalized model on the client, and instead of replacing the local model with the weights of the global model, global knowledge is fused into the mixture prior, preventing local overfitting through regularization during the entire local training process, which can also be interpreted as passing global knowledge to the local model.\n\nWe have revised the introduction to provide a clearer explanation of why previous work ignored the overfitting problem.\n\n\n\n**The confusion caused by the Gaussian notation in Fig. 1.**\n\nWe revised Figure 1 to clarify the meaning of the symbols."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2499/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700448555486,
                "cdate": 1700448555486,
                "tmdate": 1700448555486,
                "mdate": 1700448555486,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VU44zaKr5X",
                "forum": "lzt60v45V4",
                "replyto": "Wd5uxc6yDB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2499/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2499/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Kind mention on response"
                    },
                    "comment": {
                        "value": "Dear reviewer Htsy:\n\nWe greatly thank for your comments and suggestions on our submission.  We have tried our best to address all the concerns in the response, including:\n\n- Questions on global posterior aggregation.\n- Questions about the contribution of theoretical analysis to our method.\n- Suggestions and questions about the experiment.\n\nDue to the coming deadline, we sincerely hope to know your thoughts about our responses. If you have further questions, please feel free to let us know.  Thanks again.\n\nBest regards,"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2499/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700617884608,
                "cdate": 1700617884608,
                "tmdate": 1700617884608,
                "mdate": 1700617884608,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "DuXTuZQ1l8",
            "forum": "lzt60v45V4",
            "replyto": "lzt60v45V4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2499/Reviewer_uvyG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2499/Reviewer_uvyG"
            ],
            "content": {
                "summary": {
                    "value": "This paper integrates the framework of variational inference with mixture model into the federated continual learning scenario. \n\nThe key idea is for each client to adopt variational inference on a mixture prior comprising its locally learned posterior & the aggregated global posterior of the previous task to optimize for a local posterior of the current task. In turn, such local posteriors will be shared with the server for aggregation (via conflation) & the aggregated posterior will be sent back to the clients so that they can update their mixture prior for the next iteration and so on.\n\nThe proposed method is compared with the latest work in FCL on two simulated continual learning scenarios on CIFAR100 & TinyImageNet."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper aims to address a relatively new, less addressed problem in federated learning.\n\nThe writing is mostly clear & communicates well the high-level idea.\n\nThe technical exposition is rigorously detailed, which is great. The experiments also show consistent positive results across two datasets."
                },
                "weaknesses": {
                    "value": "I have the following concerns of this work (in decreasing order of importance)\n\nFirst, I believe the empirical setting studied in this paper is a simplified version of the FCL setting in FedWeIT where clients have different task sequence. In this case, however, the clients are assumed to have the same task sequence, which is reducible to a pure CL setting: (1) existing CL parameterization can be adopted \"as is\" for the common model architecture; and (2) existing FL techniques can be applied to handle the federation of data. This ignores the essence of federated CL where there can be another cause for catastrophic forgetting which is client interference. This happens when clients have different task sequence and a naive attempt to aggregate their models (solving different tasks) can also lead to forgetting.\n\nThus, unfortunately, what is proposed here has not addressed that challenge because the global posterior aggregation here assumes local posteriors are derived from the same task (albeit with different data distributions).\n\nSecond, I also find the experiment too limited in comparison to the setting that was investigated under FedWeIT. For a thorough comparison, its exact same setting should have been adopted here. Also, the current reported performance is without deviation bar with relatively thin margin between best and second best methods, which is not very conclusive.\n\nThird, I do not see how the developed theory is specific to continual and federated learning. Its theoretical setup is entirely oblivious to the FCL setting so I am not sure what it really implies here.\n\nLast, while the writing is mostly clear, several parts still remain unclear. For example, background on conflation is missing, making it hard to see what is the loss function that replaces (10) in Section 3.3. The FL characterization in Eq. (1) is also strange: it suggests the optimization of all clients is decoupled (even thought it is not supposed to be the case). Also, are the mixing weights in Eq. (5) learnable?"
                },
                "questions": {
                    "value": "Unless I misunderstand this work, I believe it is focused on a setting that assumes away a key challenge of federated CL. Please let me know if I misunderstand something important here.\n\nIn addition, I'd suggest re-running the experiments on the benchmark data introduced in https://proceedings.mlr.press/v139/yoon21b/yoon21b.pdf\nAll experiment results should have deviation bar reported.\n\nI also think the authors need to elaborate more on their theoretical results, and explain (if possible) how it specifically accounts for the continual and federated learning setup."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2499/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699011276596,
            "cdate": 1699011276596,
            "tmdate": 1699636186298,
            "mdate": 1699636186298,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6nb1w0RnMB",
                "forum": "lzt60v45V4",
                "replyto": "DuXTuZQ1l8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2499/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2499/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks very much for your feedback and pointing out the key challenge of FCL"
                    },
                    "comment": {
                        "value": "**Do the empirical setting ignore the catastrophic forgetting brought by client interference (a key challenge of FCL) ?**\n\nWe agree that the catastrophic forgetting caused by client interference is a key challenge of FCL. However, we are afraid that the reviewer misunderstood our experimental setup. We would like to clarify that the client task sequences in our experiments are different across clients. In addition, we conducted experiments on the benchmark datasets according to the reviewer's suggestions, and the results validated the effectiveness of our method (see later answer for more details).\n\nWe provide a detailed explanation of how the client private task sequences in our experiments were constructed in Appendix A. First, the original dataset is split into several tasks that contain no overlapping classes. Second, the split tasks are randomly selected as the client's task and and the classes of tasks included are also randomly selected. Private task sequences from different clients may have similar (i.e., overlapping classes), unrelated, or interfering tasks. Clients utilize the knowledge of other clients in the learning process to improve the performance of the local model while preventing interference from the knowledge of other clients.\n\nIn FedWeIT, client interference is addressed through parameter decoupling and selective knowledge migration, while our approach addresses this challenge through personalization and client selection. We describe the client personalization in detail in Appendix A.2, the weights of the backbone as well as the structure of the classifiers are personalized for different clients, which allows VFCL to learn on personalized client task sequences. Client selection is described in Algorithm 1, where the server coordinates the learning of clients with similar tasks in a synchronized manner, which speeds up the convergence rate and reduces client interference.\n\n**Limited experimental setting compared with FedWeIT.**\n\nWe have restated our experimental setup in the first reply, our setup is equally challenging with FedWeIT, the client task sequences are heterogeneous, capable of responding the client interference.\n\nTo further validate the effectiveness of our method, we adopted the reviewer's suggestion to rerun the experiments on benchmark datasets Overlapped-CIFAR100 and NonIID-50 (repeated 3 times to obtain the mean and deviation bar), and added the detailed results to Appendix F.1, where the experimental results show that our method outperforms the others.\n\n\n**Experimental results that lack deviation bar are not conclusive.**\n\nAll of our comparison experiments were repeated 3 times and averaged, and we revised Tables 1,2 to add deviation bar to the results. Due to space constraints, we only added deviation bar on \"Avg.\", and \"Avg.\" denotes the average of the results in a single row of the table.\n\n\n**Relationship between theoretical analysis and method ?**\n\nIn FCL, multiple tasks are learned sequentially, and each task is learned by a separate federated learning (FL) process. Since all FL processes are isomorphic, we analyze the upper bound of the generalization error of a single FL process to analyze the overall FCL process.\n\nThe theoretical analysis guided our approach as follows: First, the upper bound of the generalization error of our method is affected by sample size, model capacity, and mixture prior (which is the core innovation of our work). Second, mixture prior is theoretically shown to reduce the upper bound on generalization error. Third, we obtain results in our experiments that are consistent with the theoretical analysis, verifying the positive effect of the mixture prior on the model performance.\n\n\n\n**Lack of background on conflation.**\n\nThe goal of global aggregation is to find a distribution which can integrate the posterior distributions of all clients. Conflation can fuse information from multiple distributions into a single distribution, and the aggregated distribution is with density proportional to the product of the densities of input distribution. Eq. (11), which is the calculation of the parameters of the global posterior distribution, replaces the optimization process in Eq. (10).\n\nWe revised this section and added a detailed description of conflation in Appendix E.\n\n**The FL characterization in Eq. (1) is strange.**\n\nThe client models in our approach are personalized, so the final result obtained in Eq. (1) is all personalized models $\\{\\boldsymbol\\theta_c\\}_{c=1}^{C}$. In contrast, traditional centralized federated learning outputs only one global model, we thought this distinction might bother reviewers.\n\n**Are the mixing weights in Eq. (5) learnable ?**\n\nThe mixing weights in Eq. (5) are not learnable, they can be set manually (in our paper) or calculated using existing client evaluation methods."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2499/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700448482400,
                "cdate": 1700448482400,
                "tmdate": 1700448482400,
                "mdate": 1700448482400,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "R6tS7LYgJo",
                "forum": "lzt60v45V4",
                "replyto": "DuXTuZQ1l8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2499/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2499/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Kind mention on response"
                    },
                    "comment": {
                        "value": "Dear reviewer uvyG:\n\nWe sincerely appreciate your kind efforts in reviewing our paper. Now, we tried our best to clarify all the concerns in our response, including major concerns:\n\n- Concerns about the experimental setup.\n- Suggestions for re-running the experiment on the benchmark datasets introduced in FedWeIT.\n- Questions on the relationship between theoretical analysis and method.\n\nDue to the coming deadline, please feel free to let us know your thoughts, if you have further questions. Great thanks.\n\nRegards,\n\n---"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2499/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700617811593,
                "cdate": 1700617811593,
                "tmdate": 1700617811593,
                "mdate": 1700617811593,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ouTYjX6HCc",
                "forum": "lzt60v45V4",
                "replyto": "DuXTuZQ1l8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2499/Reviewer_uvyG"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2499/Reviewer_uvyG"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the detailed update"
                    },
                    "comment": {
                        "value": "Dear Authors,\n\nThank you for the detailed update. I need more time to go over all the new results you have provided.\n\nFor now, I have one question:\n\nIn Algorithm 1 (see page 15), the server workflow will loop over each task & for each task, it selects clients who will be assigned to the task. Is it supposed to know each client's task sequence in advance?\n\nIf that is the case, how will this algorithm be used in practice where the client's task sequence is not known in advance? \n\n--\n\nFurthermore, assuming that we know the task sequence in advance is the same as assuming away the client's interference. \n\nThe pseudocode clearly suggests that the clients will not be trained in the order of the tasks they received. Instead, the server dictates which client solves each task at which step. This is no longer continual learning where you cannot manipulate the order in which tasks are presented at will.\n\n--\n\nOtherwise, assuming the server only selects clients that are assigned the task in the current round, doesn't it mean that it already forgot about the other clients' solution model of this task who were constructed in previous rounds?"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2499/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700639834682,
                "cdate": 1700639834682,
                "tmdate": 1700639854627,
                "mdate": 1700639854627,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]