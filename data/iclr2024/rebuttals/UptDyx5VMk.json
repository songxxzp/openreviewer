[
    {
        "title": "Continuous Multi-step Predictions of Highly Imbalanced Multivariate Time Series via Deep Learning Network"
    },
    {
        "review": {
            "id": "6P6UJiGjDt",
            "forum": "UptDyx5VMk",
            "replyto": "UptDyx5VMk",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission149/Reviewer_sXU7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission149/Reviewer_sXU7"
            ],
            "content": {
                "summary": {
                    "value": "The paper addresses the problem of zero inflation in time series\nforecasting, i.e., to forecast a continuous-valued channel that\ncontains many zeros. The authors propose a method that\ntrains a recurrent neural network with an attention decoder\n(for multiple timesteps) in two stages with two losses, a quantile\nloss and a weighted quantile loss. In an experiment on a\npublic dataset they show that their method outperforms\na two stage approach from the literature, that first tries to\npredict if the value is non-zero and then conditionally\non being non-zero the value."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "s1. interesting problem: zero inflation, esp. in time series forecasting."
                },
                "weaknesses": {
                    "value": "w1. aspects of the proposed method are not clear.\nw2. experiments are run on a single dataset.\nw3. the method is compared against a single, simple baseline."
                },
                "questions": {
                    "value": "w1. aspects of the proposed method are not clear.\n- how exactly is the weight w_i (p. 5) computed?\n  - the \"x\" here have nothing to do with the covariates \"x\" in eq. 1, right? \n  - why is \"x_{(1)}\" missing in line 148 ?\n  - denotes \"x_j\" the same as \"x^{(j)}\" ?\n  - will not always be exactly one \"I(x_j \\leq y_i < x_{j+1})\" be\n    one (for one j), and for all others be zero? So the weight is\n    basically 1 / \\int_{x_i}^{x_{i+1} f(x) dx ?\n  - what is \"f\" ?\n\nw2. experiments are run on a single dataset.\n- a wider experimentation on more datasets will provide better\n  evidence that the proposed method performs well.\n\nw3. the method is compared against a single, simple baseline.\n- As a decision tree ensemble, how is the baseline \"2Stage-LGBM\"\n  treating the time series aspect of the problem?\n  It will be more convincing to compare against a proper time\n  series forecasting model.\n- A simple, principled baseline is to decouple the target channel into\n  two target channels: 1) the indicator for being zero and 2) the value,\n  and then train any state-of-the-art forecasting model to forecast\n  both channels at the same time (where the value channel is masked\n  for zero values).\n- A comparison against state of the art time series forecasting models\n  such as Nie et al. 2023 and Zeng et al. 2023 is missing.\n\nWriting needs some attention, e.g.,\n- p. 1 \"the prediction the user's payment values\"\n\nReferences:\n- Nie, Yuqi, Nam H. Nguyen, Phanwadee Sinthong, and Jayant\n  Kalagnanam. \u201cA Time Series Is Worth 64 Words: Long-Term Forecasting\n  with Transformers.\u201d  ICLR 2023.\n- Zeng, Ailing, Muxi Chen, Lei Zhang, and Qiang Xu. \u201cAre Transformers\n  Effective for Time Series Forecasting?\u201d In AAAI, 2023."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission149/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698424315060,
            "cdate": 1698424315060,
            "tmdate": 1699635940179,
            "mdate": 1699635940179,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "jEwuYDZ0ru",
            "forum": "UptDyx5VMk",
            "replyto": "UptDyx5VMk",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission149/Reviewer_BcAV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission149/Reviewer_BcAV"
            ],
            "content": {
                "summary": {
                    "value": "**Summary**\n\nThe authors aim to address the data imbalance problem in multi-step time series forecasting by a joint model consisting of two parts: representation learning part and label learning part. They present experiments on a real-world dataset to show the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. The research topic is interesting."
                },
                "weaknesses": {
                    "value": "Majors:\n\n1. The contribution of the paper is not well clarified.\n\n2. For Step 1 Representation Learning, model structure details are missing. What is Emb() in Algorithms 2 and 3?\n\n3. For Step 2 Label Learning, why choose LSTM, GLU, and multi-head self-attention? Why not use a Transformer-based model directly? The philosophy is unclear.\n\n4. The authors introduce a Weighted Quantile Loss function, but no experiments are conducted to verify the effectiveness of this loss. Does it work better compared to the common Quantile Loss function or MAE?\n\n5. The authors claim that FLIMTS has much less time cost compared to 2Stage-LGBM, but no experiments have been conducted to demonstrate that.\n\n6. The dataset is split into a train set and a test set. As there is no validation set, how the hyperparameters are decided is not clear.\n\n7. The authors claim that rAUC is a good metric for downstream tasks in reality, but the experiments show that FLIMTS does not have a significant advantage against 2Stage-LGBM.\n\n8. Ablation studies and More methods for comparison are needed.\n\n9. The content in the Appendix is more important than Fig. 2.\n\nMinors:\n\n10. I suggest the authors provide a citation and a full name for the dataset when it first appears in the paper. (AVSC first appears at Line 179, but is not explained until Line 185.)\n\n11. Some citations are missing. For example, LSTM, GLU, and Multi-head Self Attention.\n    \n12. The authors use a date for the train/test split, but do not provide the starting/ending dates of the dataset.\n\n13. At Line 87, the authors claim they do experiments on two datasets. However, in Section 3, there is only one dataset, AVSC.\n\n14. There are some typos in the paper. For example,\n\n  Line 31: the predicting the user\u2019s payment values is a critical business requirement\n\n  Line 71: It increases the model maintenance cost with a unsatisfactory prediction accuracy\n\n  Line 94: where $\\epsilon_t^{(q)}$ is the random error, $\\mathcal{F}$ is a unknown nonlinear mapping function"
                },
                "questions": {
                    "value": "Please see the questions in the \"Weaknesses\" part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission149/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698647514953,
            "cdate": 1698647514953,
            "tmdate": 1699635940058,
            "mdate": 1699635940058,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "CW7t5WlOZE",
            "forum": "UptDyx5VMk",
            "replyto": "UptDyx5VMk",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission149/Reviewer_wbNx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission149/Reviewer_wbNx"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes feature learning architecture FLIMTS, for imbalanced multivariate time-series which is a two-stage learning, (i) representation learning, which learns embedding for static and continuous features for multi-step prediction through quantile loss and (ii) label learning which learns weight for each prediction step of the quantile. FLIMTS has been adopted on AVSC shopped dataset with 11 transaction features and compared with an ML algorithm LightGBM."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. FLIMT improvement is very high compared to LightGBM in terms of MSE, MAE, rAUC.\n2. The proposed approach for multivariated imbalanced time-series data is novel and challenging.\n3. FLIMTS seem to have better performance over LightGBM for different category imbalanced user dataset."
                },
                "weaknesses": {
                    "value": "1. Comparison with some deep learning multivariate time-series architeture would have been better to analyze. E.g., Temporal Fusion Trasnformer (TFT), Deep AR are also based on multi-step quantile loss for multivariate time-series. \n2. Fix some typos:\n\t- line 118: 'the' written twice\n\t- Author Contibution and acknowledgement section empty. You can remove these sections for submision pre-print."
                },
                "questions": {
                    "value": "1. Would be convincing as well to see comparison with TFT on imbalance dataset. TFT is not a 2-stage learning but they also adopt self-attention mechanism and adapted for static and continuous time-series features.\n\n2. Can you add some statistics, on overall for different category users how much FLIMTS have improved compared to baseline?\n\n3. Overall paper and architecture is novel and impactful. However authors have compared with only one baseline. Is there no other baseline (specially deep learning-based) that can be slightly adapted and compared with such work? If TFT is not a comparable model, some explanation would also be easier to be convinced FLIMTS performance."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission149/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission149/Reviewer_wbNx",
                        "ICLR.cc/2024/Conference/Submission149/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission149/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698701444460,
            "cdate": 1698701444460,
            "tmdate": 1700466179482,
            "mdate": 1700466179482,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "AWI0bVD4vK",
                "forum": "UptDyx5VMk",
                "replyto": "QNBSDgZnAe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission149/Reviewer_wbNx"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission149/Reviewer_wbNx"
                ],
                "content": {
                    "comment": {
                        "value": "I second the weaknesses pointed out by reviewer Ngnu and bcav. I think the paper lacks several experiments: \n1. It did not consider enough competitive baselines\n2. Not enough metric\n3. Only one dataset which is not enough to verify the soundness of proposed approach\n\nHence, I changed my decision to weak reject from weak AC."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission149/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700466491375,
                "cdate": 1700466491375,
                "tmdate": 1700466491375,
                "mdate": 1700466491375,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "QNBSDgZnAe",
            "forum": "UptDyx5VMk",
            "replyto": "UptDyx5VMk",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission149/Reviewer_NGnU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission149/Reviewer_NGnU"
            ],
            "content": {
                "summary": {
                    "value": "In this work, the authors introduce a continuous multi-step prediction model for imbalanced multivariate time series data, which integrates a representation learning module, a label learning module, and a weighted quantile loss for imbalance issue. The model is evaluated against a basic machine learning model using real-world transaction datasets, demonstrating better performance."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is well written, with clear experimental details."
                },
                "weaknesses": {
                    "value": "My major comments are:\n\n1. The selected baseline appears weak as it represents a basic machine learning model rather than comparative temporal deep learning models like GRU or LSTM. Moreover, the paper lacks ablation studies which would demonstrate the individual contributions of the different components of the proposed model. Without such experiments, it is difficult to evaluate the technical contributions of this work.\n\n2. While data imbalance is positioned as a significant challenge, I didn't see the detailed statistics or plots showing the imbalance in the data. The authors claims that a 3:1 ratio is extremely imbalanced; however, in various prediction contexts, ratios of 10:1 or even 100:1 (positive to negative) are not uncommon. The authors should substantiate why this ratio brings significant challenges in their chosen domain.\n\n3. What's the difference between the data imbalance issue here and the long-tail regression? There are many existing works addressing the long-tail regression issue. If relevant, please include these works in the discussion or baselines.\n\n4. The paper mentions only a training set and a test set. How were the model's hyperparameters determined?"
                },
                "questions": {
                    "value": "Please address the weaknesses above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission149/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission149/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission149/Reviewer_NGnU"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission149/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698853487682,
            "cdate": 1698853487682,
            "tmdate": 1699635939828,
            "mdate": 1699635939828,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "nwRVgFUUle",
            "forum": "UptDyx5VMk",
            "replyto": "UptDyx5VMk",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission149/Reviewer_jHHC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission149/Reviewer_jHHC"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on data imbalanced problem in time-series problem. Authors introduce a novel model, named FLIMTS, incorporates a two-stage learning process: representation learning and label learning. The representation learning stage focuses on embedding static and continuous features for multi-step prediction using quantile loss, while the label learning stage assigns weights to each prediction step. The model also includes a weighted quantile loss to address the imbalance issue. FLIMTS has been tested against a basic machine learning model, LightGBM, using the AVSC shopped dataset, which includes 11 transaction features. The results demonstrate that FLIMTS outperforms the basic machine learning model, showcasing its effectiveness in handling multi-step time series forecasting in the presence of data imbalance."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Problem is important.\n2. Proposed method is novel.\n3. FLIMT seems to outperform LightGBM in various datasets."
                },
                "weaknesses": {
                    "value": "1. The definition of 'Emb()' is missing in Algorithms 2 and 3.\n\n2. There is a lack of both theoretical and empirical evidence supporting the superiority of the Weighted Quantile Loss.\n\n3. The results for one of the datasets are missing, despite the authors' assertion of testing on two datasets (referenced in line 87).\n\n4. Figure 5 should be relocated to the main body of the paper for better context and clarity.\n\n5. Certain models and claims made by the authors require additional references or evidence for validation.\n\n6. Typos present in the document need further correction."
                },
                "questions": {
                    "value": "See above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission149/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission149/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission149/Reviewer_jHHC"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission149/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699971211593,
            "cdate": 1699971211593,
            "tmdate": 1699971211593,
            "mdate": 1699971211593,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]