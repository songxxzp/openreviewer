[
    {
        "title": "Bridge-TTS: Text-to-Speech Synthesis with Schrodinger Bridge"
    },
    {
        "review": {
            "id": "vHUaIoVrg2",
            "forum": "F9ApWtHVac",
            "replyto": "F9ApWtHVac",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1017/Reviewer_zyj4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1017/Reviewer_zyj4"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents Bridge-TTS, which incorporates the Schr\u00f6dinger Bridge concept into text-to-melspectrogram generation. By introducing the Schr\u00f6dinger bridge that directly connects the deterministic latent representation from the text encoder and the data, it allows for more direct use of the text encoder output compared to Grad-TTS. It demonstrated better sample quality with fewer sampling steps than Grad-TTS on the LJSpeech dataset."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper applies a strong theoretical background on the Schr\u00f6dinger Bridge and the recent sampling methods from diffusion models to TTS.\n\n2. Bridge-TTS shows better sample quality with fewer sampling steps compared to Grad-TTS, and even with very few sampling steps (2-step generation), it produces reasonably good quality samples."
                },
                "weaknesses": {
                    "value": "1. The performance improvement compared to Grad-TTS is marginal. Grad-TTS is a paper published in ICML 2021, and improving upon this baseline for a single speaker dataset (LJSpeech) doesn't seem to be a challenging issue in speech synthesis at present and appears to be a straightforward application. Thus, I believe it would be difficult for it to be published in ICLR 2024. Exploring this new generative model seems to have a fresh aspect, so targeting a speech-related venue might be more appropriate. Personally, I feel that the TTS problem for single speaker datasets in the years 2023-2024 is relatively a toy problem. Generating high-quality samples from LJSpeech doesn't appear to be a challenging issue, and I'm not particularly motivated by applying the Schr\u00f6dinger bridge to TTS given the experimental results in the paper."
                },
                "questions": {
                    "value": "* The paper explores the scalar values of f and g, and shows the CMOS ablation results in Table 4. If the mel-spectrogram data is normalized to have values between [-1, 1] before training, couldn't we simply use 0 for f and 1 for g? By using f=0 and some scaling value for g, this approach appears to be the application of the Brownian bridge by Tong et al to TTS, as also mentioned in the paper.\n\n* The Schr\u00f6dinger bridge is used in the image domain for applications like Image-to-image translation. If the Schr\u00f6dinger bridge offers advantages in TTS not only for sampling speed on a single speaker but also for applications like translation in the speech domain (e.g., speech-to-speech translation, voice conversion, etc.), highlighting such results would have provided stronger motivation for its application. This could have made the research more compelling."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1017/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1017/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1017/Reviewer_zyj4"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1017/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698818224066,
            "cdate": 1698818224066,
            "tmdate": 1700728565483,
            "mdate": 1700728565483,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7ZGlpouZlM",
                "forum": "F9ApWtHVac",
                "replyto": "vHUaIoVrg2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1017/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1017/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer zyj4"
                    },
                    "comment": {
                        "value": "We appreciate your valuable comments and feedback on our work. We would like to provide our answer and update as follows:\n\n> W1: The performance improvement compared to Grad-TTS is marginal.\n\n**A**: We admit the performance improvement was marginal in our initial submission. **In our latest submission, we outperform our diffusion counterpart Grad-TTS by a large margin and achieve higher quality than the state-of-the-art fast sampling method CoMoSpeech (ACM Multimedia 2023).** Please refer to *Common Response* and our updated submission for more details. We hope this improvement will be considered in your evaluation.  \n\n> W2: Targeting a speech-related venue rather than ICLR might be more appropriate.\n\n**A**: We respectfully disagree. This work is not a simple application of known techniques to a speech-related task, instead, it makes **solid theoretical and technical contributions**. Specifically, our proposed tractable SB framework elucidates flexible options for design space, such as the form of noise schedules and SDE/ODE samplers in SB, and they are actively under research (for example, a paper called \"Denoising Diffusion Bridge Models\" submitted to ICLR 2024 makes many similar contributions to us). Notably, the first-order Bridge ODE sampler has not been revealed by previous works in SB. Therefore, except for the significant results on the TTS task in our revised paper, we believe our methodology itself is general and has future impacts.\n\n> W3: The TTS problem for single-speaker datasets in the years 2023-2024 is relatively a toy problem.\n\n**A**: Grad-TTS is published in ICML 2021. However, to the best of our knowledge, most following diffusion-based TTS systems (acoustic model instead of end-to-end TTS system) mainly work on **fast sampling methods**, and **none of the following works has reported a new record of sample quality in comparison with Grad-TTS-1000**. By working on the training and sampling of the Schrodinger bridge, we achieved a remarkable advancement in both the sample quality and the inference speed compared to diffusion-based TTS systems. \n\n> Q1: By using f=0 and some scaling value for g, this approach appears to be the application of the Brownian bridge by Tong et al. to TTS, as also mentioned in the paper.\n\n**A**: In our latest submission, we have updated Bridge-TTS systems with **asymmetric noise schedules (Bridge-VP and Bridge-gmax)**, which are of importance in improving TTS sample quality and have not been proposed by previous works. In comparison with previous works using Brownian bridges, we ablate the importance of noise schedules (please refer to \"constant $g(t)$\" in Table 4). By using $f=0$ and some scaling value for $g$, the performance is not as good, and this is in agreement with diffusion models, for which researchers tune the noise schedule and find VP a good choice. We directly use the same hyperparameters as VP, and our performance significantly outperforms Grad-TTS (which also uses VP but under a diffusion backbone). Therefore, we prove that our SB framework is superior to the diffusion counterpart on the TTS task. Moreover, the new noise schedules in the SB framework are non-trivial and distinguished from previous works of Brownian bridges.\n\n> Q2: Other applications like translation in the speech domain \n\n**A**: Thank you for your suggestion. We believe our proposed method could find applications in your mentioned fields, and we would like to explore them in future works. In this work, we expect to focus on our contributions to **improving the sample quality and the inference speed** of diffusion-based TTS systems."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1017/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700048419354,
                "cdate": 1700048419354,
                "tmdate": 1700048419354,
                "mdate": 1700048419354,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "U5EXZr1qaA",
                "forum": "F9ApWtHVac",
                "replyto": "vHUaIoVrg2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1017/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1017/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking forward to further feedback"
                    },
                    "comment": {
                        "value": "Dear Reviewer zyj4,\n\nWe thank you very much again for the great efforts on reviewing our manuscript and providing the valuable comments to further improve. We hope you may find our response and updated submission (paper, website, and synthesized samples) satisfactory, and evaluate our contribution on generative models, TTS sample quality, and inference speed compared to diffusion-based TTS systems. \n\nIf you want any further discussion, we would be very happy to reply. If our feedback is good enough, we wonder if you would like to increase your rating accordingly.\n\nBest, \nAuthors of Bridge-TTS"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1017/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700296637411,
                "cdate": 1700296637411,
                "tmdate": 1700296637411,
                "mdate": 1700296637411,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6GTYRiInqE",
                "forum": "F9ApWtHVac",
                "replyto": "U5EXZr1qaA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1017/Reviewer_zyj4"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1017/Reviewer_zyj4"
                ],
                "content": {
                    "comment": {
                        "value": "I have read the updated paper and the reviewers' responses, and I appreciate the considerable effort put into them. I would like to leave a few comments regarding the response.\n\n* I believe the experimental results presented in the Bridge-TTS paper don't sufficiently motivate the proposed method. As highlighted in the initial review, several previous works, such as Grad-TTS and VITS, have already demonstrated strong performance on the LJSpeech dataset a few years ago. Considering the significant achievements of these works, most of the recent improvements on LJSpeech tend to fall within confidence intervals and do not offer a significant improvement, which somewhat undermines the motivation for the research method. Enhancing sample quality with fewer sampling steps on the LJSpeech dataset is not enough to justify the proposed approach. It is also noteworthy that no papers focusing exclusively on reducing sampling steps in a single speaker dataset have been published at major conferences like NeurIPS 2022, ICML 2023, ICLR 2023, or NeurIPS 2023.\n\n* In the broader context of Text-to-Speech (TTS) advancements, contemporary papers have already shown notable improvements not just in sample quality and sampling speed, but also in multi-speaker and prosody modeling. For the Bridge-TTS method, leveraging the Schr\u00f6dinger Bridge, to establish its distinct advantage, it would be necessary to demonstrate high sample quality and benefits while maintaining the diversity of prosodies with fewer sampling steps, and notably, in a dataset that presents more complex challenges (e.g. LibriTTS, \u2026).\n\n* For a more convincing demonstration of the theoretical contributions, a direct comparison with existing works in the image domain, where Schr\u00f6dinger Bridge-related research is more mature, would be appropriate. Research on DDBM also provides comparisons with more recent studies like I2SB, Rectified Flow, etc.\n\n* Upon reviewing the samples on the demo page, it's noticeable that **Bridge-TTS-gmax-2**, despite achieving a MOS of 4.04, exhibits a somewhat robotic quality."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1017/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700314134542,
                "cdate": 1700314134542,
                "tmdate": 1700314134542,
                "mdate": 1700314134542,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UibEmkfR0x",
                "forum": "F9ApWtHVac",
                "replyto": "vHUaIoVrg2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1017/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1017/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer zyj4 (Round 2)"
                    },
                    "comment": {
                        "value": "We have read your comments, and we appreciate your active participation in the author-reviewer discussion period. We would like to leave a few responses regarding the comments for further clarification.\n\n> Comment 1: Enhancing sample quality with fewer sampling steps on the LJSpeech dataset is not enough to justify the proposed approach.\n\nAccording to Table 1, we have compared the best generation quality of different methods, and in comparison with Grad-TTS-1000 and VITS which achieve strong performance in your comment, we have shown **distinctively better** results instead of falling within confidence intervals (Ground-truth > Bridge-TTS >> other methods). Therefore, we argue that our main contribution is **not only improving sampling speed, but also generation quality**. \n\n> Comment 2: No papers focusing exclusively on reducing sampling steps in a single speaker dataset have been published at major conferences like NeurIPS 2022, ICML 2023, ICLR 2023, or NeurIPS 2023.\n\nAs we have claimed, we do not focus exclusively on reducing sampling steps. Besides, other works are not published just because they do not have enough theoretical contributions or do not make significant experimental advancements.\n**It is not reasonable to evaluate a work based on previous works of similar tasks, but not on the quality of the work itself.**\n\n> Comment 3: For the Bridge-TTS method, leveraging the Schr\u00f6dinger Bridge, to establish its distinct advantage, it would be necessary to demonstrate high sample quality and benefits while maintaining the diversity of prosodies with fewer sampling steps, and notably, in a dataset that presents more complex challenges (e.g. LibriTTS, \u2026).\n\nWe highlight the advantages of our data-to-data process over the data-to-noise process in diffusion models. **To the best of our knowledge, LJ-Speech is the only TTS dataset reported in most published diffusion-based acoustic models**, including Grad-TTS (ICML 2021), ProDiff (ACM MM 2022), DiffSinger (AAAI 2022), Fast Grad-TTS (INTERSPEECH 2022) and CoMoSpeech (ACM MM 2023). Hence, we believe this dataset provides a fair comparison between generative models. When a multi-speaker dataset is preferred to demonstrate our contribution to sample quality and inference speed, we wonder if there are appropriate diffusion methods for a fair comparison.\n\n> Comment 4: For a more convincing demonstration of the theoretical contributions, a direct comparison with existing works in the image domain, where Schr\u00f6dinger Bridge-related research is more mature, would be appropriate. Research on DDBM also provides comparisons with more recent studies like I2SB, Rectified Flow, etc.\n\nWe didn't claim that our method can be directly applied to other domains, though we hope so. **Our theoretical assumptions such as paired data are _motivated by the TTS task_ directly**, and the theoretical contributions on the TTS task itself are still valid. With our developed techniques, we have demonstrated our framework's effectiveness, and we have already conducted the ablation studies to compare with I2SB, (e.g., asymmetric schedules vs symmetric schedule in I2SB, data prediction vs noise prediction in I2SB). Also, Rectified Flow is a distillation method based on flow matching. We have talked about flow matching in Appendix D, where we have proved it performs badly on the TTS task. Also, we have compared with distillation-based methods such as CoMoSpeech in few-step scenarios.\n\n\n> Comment 5: Upon reviewing the samples on the demo page, it's noticeable that Bridge-TTS-gmax-2, despite achieving a MOS of 4.04, exhibits a somewhat robotic quality.\n\nIt should be noted that, for Bridge-TTS-gmax-2, a MOS of 4.04 is achieved in comparison with other fast sampling methods, supporting our contribution to **efficient sampling instead of generation quality**. Moreover, this result is conducted on Amazon Turk with 20 synthesized samples and 25 Master workers, which is already a large number in comparison with other TTS works."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1017/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700321950097,
                "cdate": 1700321950097,
                "tmdate": 1700363153640,
                "mdate": 1700363153640,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OXftNBX4ls",
                "forum": "F9ApWtHVac",
                "replyto": "UibEmkfR0x",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1017/Reviewer_zyj4"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1017/Reviewer_zyj4"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate the authors' efforts. The authors have reiterated that the quality of samples and generation in the LJSpeech dataset has improved. However, in my view, the results from LJSpeech alone appear to be not a significant improvement. The introduction of Schrodinger Bridge into the 4-step or 2-step generation process would have been more convincing if it had clearly outperformed the existing Grad-TTS 50-step or 1000-step generation. As the authors have stated, it's appropriate to judge this paper on its own results. Yet, I feel that the experimental results presented don't adequately motivate or justify the scalability to larger scales or more diverse datasets with the current experimental results. Although the extensive results support the paper and raise its score, I still consider it below the bar for publication in ICLR, leading me to give this paper a score of 5."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1017/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700728536335,
                "cdate": 1700728536335,
                "tmdate": 1700728536335,
                "mdate": 1700728536335,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "A9b0WQhjqp",
            "forum": "F9ApWtHVac",
            "replyto": "F9ApWtHVac",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1017/Reviewer_8tg3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1017/Reviewer_8tg3"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents Bridge-TTS that generates mel-spectrograms from deterministic text latent representations. To achieve this, the authors first introduce a fully tractable Schrodinger bridge for paired data in TTS modeling. Subsequently, they propose a novel first-order discretization of the Bridge SDE/ODE for accelerated sampling. Experimental results emphasize that the proposed approach offers synthesis quality comparable to or surpassing baseline methods, especially in few-step sampling scenarios."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* The authors introduce a theoretically novel approach to employ Schrodinger bridge for TTS that produces outputs from deterministic latent representations.\n* They present a new sampling scheme optimized for fast sampling."
                },
                "weaknesses": {
                    "value": "* A lack of empirical validation for the superiority of the proposed method.\n\nIt is uncertain whether the proposed deterministic latent representations are superior to the noisy Gaussian conditional prior distribution of diffusion-based TTS. Experimental results suggest that for fewer than 50 sampling steps, the proposed method seems to yield better sample quality compared to other models. However, at 1000 steps, the diffusion-based TTS model, namely Grad-TTS, outperforms the proposed methodology.\n\nAccordingly, for a fair comparison concerning its efficient sampling scheme, it would be appropriate to contrast it with diffusion-based TTS models using a sampling scheme like the DDIM."
                },
                "questions": {
                    "value": "It would be essential for the authors to provide an explanation for the observed worse sample quality of their proposed method compared to the baseline diffusion-based approach at 1000 sampling steps.\nAdditionally, the generation quality between the proposed method and diffusion-based methods using a comparable sampling scheme in few-step sampling scenarios would also be an interesting aspect of this research.\n\ntypo: (p.7, Sec.4.1.,) English graphme -> English grapheme"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1017/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1017/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1017/Reviewer_8tg3"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1017/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698839696438,
            "cdate": 1698839696438,
            "tmdate": 1699636027757,
            "mdate": 1699636027757,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "oaBqGf788v",
                "forum": "F9ApWtHVac",
                "replyto": "A9b0WQhjqp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1017/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1017/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 8tg3"
                    },
                    "comment": {
                        "value": "We are grateful for your valuable comments and feedback on our work. We would like to provide our answer and update as follows:\n\n> W1: A lack of empirical validation for the superiority of the proposed method.\n\n**A**: We have significant advancement in our experiment results and achieve distinctively better quality than Grad-TTS at 1000 steps. Please refer to *Common Response* and our updated submission for more details. Our updated synthesized samples can be visited in the supplementary materials and the anonymous website.\n\n> W2: Compare to diffusion-based TTS models with a comparable sampling scheme\n\n**A**: To demonstrate our performance on efficient sampling, we include two new baseline methods: Fast Grad-TTS (INTERSPEECH 2022), which uses a first-order SDE sampler, and CoMoSpeech (ACM Multimedia 2023), which uses consistency distillation. The first-order SDE sampler is proposed by the authors of Grad-TTS, while we observe the improvement is limited because of the difficulty of generating clean samples from noisy prior. In our updated submission, given the same NFE or similar RTF, we have achieved higher sample quality than different fast sampling methods."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1017/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700048531512,
                "cdate": 1700048531512,
                "tmdate": 1700048531512,
                "mdate": 1700048531512,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "EoEuhCy89U",
                "forum": "F9ApWtHVac",
                "replyto": "A9b0WQhjqp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1017/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1017/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking forward to further feedback"
                    },
                    "comment": {
                        "value": "Dear Reviewer 8tg3,\n\nWe thank you very much again for the great efforts on reviewing our manuscript and providing the valuable comments to further improve. We hope you may find our response and updated submission (paper, website, and synthesized samples) satisfactory, and evaluate our contribution on generative models, TTS sample quality, and inference speed compared to diffusion-based TTS systems.\n\nIf you want any further discussion, we would be very happy to reply. If our feedback is good enough, we wonder if you would like to increase your rating accordingly.\n\nBest, \nAuthors of Bridge-TTS"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1017/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700296512019,
                "cdate": 1700296512019,
                "tmdate": 1700296512019,
                "mdate": 1700296512019,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "puyGQrCohT",
                "forum": "F9ApWtHVac",
                "replyto": "A9b0WQhjqp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1017/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1017/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking forward to further discussion"
                    },
                    "comment": {
                        "value": "Dear Reviewer 8tg3,\n\nAs the remaining time for discussion is limited, we sincerely want to hear your feedback on our updated submission. If you have any further question, we would be very happy to reply.\n\nThank you very much again for your time, great efforts, and valuable comments.\n\nBest regards, Authors of Bridge-TTS"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1017/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700443172894,
                "cdate": 1700443172894,
                "tmdate": 1700443172894,
                "mdate": 1700443172894,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wlWzUX5mbf",
                "forum": "F9ApWtHVac",
                "replyto": "puyGQrCohT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1017/Reviewer_8tg3"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1017/Reviewer_8tg3"
                ],
                "content": {
                    "title": {
                        "value": "Response to the authors"
                    },
                    "comment": {
                        "value": "It's positive that the authors have provided updated experimental results supporting the proposed model's performance at both high and low sampling steps. This can now be recognized as a part of the empirical contribution of this work, particularly in terms of the efficacy of an efficient sampling scheme. However, the experimental results with numerous sampling steps (Table 2) show that almost all models, including ground truth mel-spectrogram (gt-mel) with vocoder, have overlapping confidence limits or are even reported higher. This scenario makes it difficult to assert that the model significantly outperforms others, as the majority of the scores are very high and almost all models perform on par with or better than gt-mel + vocoder. Given this lack of clear superiority, I am not currently inclined to switch to a favorable review."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1017/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700660590927,
                "cdate": 1700660590927,
                "tmdate": 1700660590927,
                "mdate": 1700660590927,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SOIOaHovv5",
                "forum": "F9ApWtHVac",
                "replyto": "A9b0WQhjqp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1017/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1017/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking forward to post-rebuttal feedback!"
                    },
                    "comment": {
                        "value": "Dear Reviewer 8tg3,\n\n**Your concern on sample quality when NFE=1000 is exactly our major objective during 29-September-2023 to 15-November-2023**. At this final stage of the author-reviewer discussion, we would like to add further evidence showing our sample quality improvement and emphasize our proposed techniques for achieving it. \n\nWe conduct a CMOS test on Amazon Mechanical Turk between Grad-TTS-1000 (i.e., our diffusion counterpart) and our Bridge-TTS-1000 (gmax), where 15 Master judges compare 20 samples synthesized by these two methods. As a result, we achieve **a CMOS of +0.21** in comparison with Grad-TTS (**CMOS > +0.1 implies distinctively better**). We wonder if this result and our preference test shown in Appendix G.1 could be evidence for our contribution to sample quality.\n\nMoreover, we expect to emphasize the three techniques for improving the sample quality, which is the main content in our updated submission: 1) we developed asymmetric noise schedules (i.e., gmax and VP) that are strongly by the TTS task itself and are presented for the first time in bridge-related research works; 2) we change the model parameterization from noise prediction to data prediction; 3) we verified the function of sampling temperature in Bridge-based TTS system. Each of these three techniques has been verified in ablation studies with the CMOS test.\n\nWe wonder if the above explanations could be helpful to your concerns on sample quality and to clarify our contributions. If you have any questions, we would be very happy to reply. If our feedback is good enough, we wonder if you would like to increase your rating accordingly.\n\nBest regards,\nAuthors of Bridge-TTS"
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1017/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700733429013,
                "cdate": 1700733429013,
                "tmdate": 1700733559218,
                "mdate": 1700733559218,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "MHMqUlmmJb",
                "forum": "F9ApWtHVac",
                "replyto": "SOIOaHovv5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1017/Reviewer_8tg3"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1017/Reviewer_8tg3"
                ],
                "content": {
                    "title": {
                        "value": "Response to the authors"
                    },
                    "comment": {
                        "value": "I find the theoretical contribution of this study to be sufficiently interesting, and I am genuinely grateful to the authors for providing new experimental results up to the rebuttal period to support their findings. In particular, when considering CMOS comparison with a strong baseline and the performance of the demo samples, it appears that this method outperforms the strong baseline. However, as I have previously noted, the proposed method, like most comparison models, uses a pretrained vocoder that is trained with ground-truth mel-spectrograms. Despite this, most models show overlapping confidence limits with gt-mel + vocoder, or are even rated higher. Therefore, while I retain my current evaluation, I would deem it a sufficient contribution for a conference paper if the study reasonably explains why methods using vocoders are evaluated to be better than those generated from gt-mel and vocoder, or if it is restructured to showcase the superiority of this methodology over existing diffusion models in an alternative manner."
                    }
                },
                "number": 26,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1017/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700738464745,
                "cdate": 1700738464745,
                "tmdate": 1700738464745,
                "mdate": 1700738464745,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nLtrTm2vrI",
                "forum": "F9ApWtHVac",
                "replyto": "A9b0WQhjqp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1017/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1017/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer 8tg3:\n\nOur response to your concern about gtmet + vocoder is that, as we adopt the human subjective tests MOS and CMOS to evaluate TTS sample quality, the results are not directly decided by objective metrics like a reconstruction task. We honestly report the results conducted by 25 Master judges on Amazon Mechanical Turk (6000 scores for each Table 2 and Table 3), and do not think gtmel + vocoder should necessarily achieve better results than other synthesized samples.\n\nBest regards,\nAuthors of Bridge-TTS"
                    },
                    "title": {
                        "value": "Thanks for your response"
                    }
                },
                "number": 28,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1017/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700740159333,
                "cdate": 1700740159333,
                "tmdate": 1700740198078,
                "mdate": 1700740198078,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "mdFREvS9hk",
            "forum": "F9ApWtHVac",
            "replyto": "F9ApWtHVac",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1017/Reviewer_SrNQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1017/Reviewer_SrNQ"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a novel neural TTS system based on SB. Informative prior in generative models is an important technical point that is well handled by this paper. In certain scenarios, they show improvement over SOTA methods like Grad-TTS and DiffSinger. The proposed tractable SB is created by defining a reference SDE in alignment with diffusion models. Bridge sampling is discussed in this context to generate the target when trained with paired data (clean text, mel spectrogram). Real-time Factor (RTF) is also discussed alongside MOS, CMOS in evaluation on LJSpeech. Proposal shows improvement in reducing high-frequency artifacts."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Novelty: Introducing SB to TTS domain.\n2. Technical rigor\n3. Ablation studies"
                },
                "weaknesses": {
                    "value": "1. Generalizability of the proposed method is an issue. We don't know if this works on other test sets, other speakers, etc. I am not confident if this model is vast improvement in the TTS research space.\n2. Focus of paper seems to be on technique. TTS-related discussion is lesser than expected.\n3. \"NFE\" is not defined. For new audience, it could be an issue.\n4. References are needed to say that some improvement in MOS is actually significant (which is what authors are conveying).\n5. Some more intuition on SB would be nice for audience with less background knowledge. Maybe a graph of training with loss or some term going down."
                },
                "questions": {
                    "value": "1. I would like to know if pre-trained models (text encoders) can be leveraged to fasten or improve the proposed SB solution.\n2. Would authors like to comment on phoneme-level improvement or provide some information on trends?\n3. Any word on if there is a trade-off between the quality of the synthesized speech and the computational efficiency of the method?\n4. Can you add some additional consistency loss term which can further brings artifacts down?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1017/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1017/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1017/Reviewer_SrNQ"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1017/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699150897150,
            "cdate": 1699150897150,
            "tmdate": 1700740738812,
            "mdate": 1700740738812,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "m2jc3FC0s2",
                "forum": "F9ApWtHVac",
                "replyto": "mdFREvS9hk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1017/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1017/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer SrNQ"
                    },
                    "comment": {
                        "value": "Thank you for your valuable comments and feedback on our work. We would like to provide our answer and update as follows:\n \n> W1: I am not confident that this model is a vast improvement in the TTS research space.\n\n**A**: In our previous submission, our improvement compared to diffusion models was limited. In our updated version, we have proposed new techniques, added more baseline methods, and conducted a comprehensive evaluation with 25 Master judges on Amazon Turk, achieving significantly better quality than diffusion-based TTS systems. Please refer to *Common Response* and our updated submission for more details.\n\n> W2: The focus of the paper seems to be on technique. TTS-related discussion is less than expected.\n\n**A**: We have revised our related work section and tried to include more TTS-related discussion. If it is expected, we are willing to add more TTS discussion in the appendix. We were hoping to elaborate on our framework and the formulation of the data-to-data process in the main content.\n\n> W3: \"NFE\" is not defined. For new audiences, it could be an issue.\n\n**A**: Thank you for your considerate advice! We have added the definition of NFE in Sec 4.2. \n\n> W4: References are needed to say that some improvement in MOS is significant (which is what the authors are conveying).\n\n**A**: MOS and CMOS tests are the main evaluation metrics in TTS synthesis, which could be seen in FastSpeech 2 (ICLR 2021), Grad-TTS (ICML 2021), VITS (ICML 2021), DiffSinger (AAAI 2022), NaturalSpeech, and CoMoSpeech (ACM MM 2023). In our updated version, to guarantee the reliability of evaluation results, we conduct the MOS and CMOS tests on Amazon Mechanical Turk with 25 and 15 Master workers, respectively, and demonstrate significantly better results than diffusion-based systems. \n\n> W5: Some more intuition on SB would be nice for the audience with less background knowledge. Maybe a graph of training with loss or some term going down.\n\n**A**: Thanks for your suggestion. We modify Figure 1 to give a more intuitive overview of the SB framework compared to diffusion models. We also add an overview of our methods on the anonymous website. There are also some intuitive comparisons between SGMs and SB in the last paragraph of Sec 2.3. Our training loss is currently a simple data prediction MSE (Eqn. (13)), and we are sorry that we have no room to plot the training curves in the main text, given the limitation of 9 pages.\n\n> Q1: I would like to know if pre-trained models (text encoders) can be leveraged to fasten or improve the proposed SB solution.\n\n**A**: Using pre-trained text encoders can improve the sample quality in our TTS system. We provide the ablation study in our updated version, i.e., mutable prior and fixed prior. \n\n> Q2: Would authors like to comment on phoneme-level improvement or provide some information on trends?\n\n**A**: Phoneme-level improvement is not specially designed in our framework. We use the same phoneme encoder and duration predictor in Grad-TTS. However, the sample quality of Grad-TTS may be restricted by its prior noise distribution, as it only contains a denoising generation process. As our SDE sampling process includes an encoding (i.e., adding noise) stage and a decoding (i.e., denoising) stage, our sample may suffer less restriction of the prior (i.e., the text encoder output). \n\n> Q3: Any word on if there is a trade-off between the quality of the synthesized speech and the computational efficiency of the method?\n\n**A**: As shown in our updated Table 2, there is a trade-off between the sample quality and computational efficiency of our method. The sample quality degrades as NFE becomes smaller. However, our method is more effective than diffusion models in maintaining the quality when reducing the NFE.\n\n> Q4: Can you add some additional consistency loss terms that can further bring artifacts down?\n\n**A**: Thank you very much for your kind suggestion. For a fair comparison with the data-to-noise process in diffusion models, we keep the single regression loss in our updated version, demonstrating the advantage of the data-to-data process in the Schrodinger bridge. In the future, we would like to study adding additional consistency loss or other loss terms to further enhance the performance of our system."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1017/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700048618630,
                "cdate": 1700048618630,
                "tmdate": 1700048618630,
                "mdate": 1700048618630,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "sqAp65Xh99",
                "forum": "F9ApWtHVac",
                "replyto": "mdFREvS9hk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1017/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1017/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking forward to further feedback"
                    },
                    "comment": {
                        "value": "Dear Reviewer SrNQ,\n\nWe thank you very much again for your positive feedback, the great efforts on reviewing our manuscript and providing the valuable comments to further improve. We hope you may find our response and updated submission (paper, website, and synthesized samples) satisfactory, and evaluate our contribution on generative models, TTS sample quality, and inference speed compared to diffusion-based TTS systems.\n\nIf you want any further discussion, we would be very happy to reply. If our feedback is good enough, we wonder if you would like to increase your rating accordingly.\n\nBest, \nAuthors of Bridge-TTS"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1017/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700296429804,
                "cdate": 1700296429804,
                "tmdate": 1700296429804,
                "mdate": 1700296429804,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ue6IcmaTio",
                "forum": "F9ApWtHVac",
                "replyto": "mdFREvS9hk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1017/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1017/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking forward to further discussion"
                    },
                    "comment": {
                        "value": "Dear Reviewer SrNQ,\n\nAs the remaining time for discussion is limited, we sincerely want to hear your feedback on our updated submission. If you have any further question, we would be very happy to reply.\n\nThank you very much again for your time, great efforts, and valuable comments.\n\nBest regards, Authors of Bridge-TTS"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1017/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700443090024,
                "cdate": 1700443090024,
                "tmdate": 1700443090024,
                "mdate": 1700443090024,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9BLTPayqxR",
                "forum": "F9ApWtHVac",
                "replyto": "mdFREvS9hk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1017/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1017/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking forward to post-rebuttal feedback!"
                    },
                    "comment": {
                        "value": "Dear Reviewer SrNQ,\n\nAt the final stage of the author-reviewer discussion, we would like to kindly ask if our replies have addressed your concerns in the first-round review. \n\nOn 15-Nov-2023, we updated our submission with overall improvement on the experiment results, and our proposed Bridge-TTS achieved significant progress compared to the diffusion counterpart Grad-TTS in both numerous-step and few-step scenarios, which is demonstrated by the improved MOS score and further validated by the preference test in Appendix G.1. \n\nAt last, we would like to emphasize our theoretical contributions. Apart from the bridge sampling, the formulation of our framework is novel itself, which supports the design of asymmetric noise schedules (i.e., gmax and VP) and are presented for the first time in bridge-related research works. We believe that our elaborated theoretical framework and the empirical significance distinguish us from those works that directly apply known techniques to a new task. For now, we have already presented a novel framework with strong performance in both numerous-step and few-step scenarios without using any fancy tuning tricks. We hope our framework can become a new baseline that is neat enough for future works to improve.\n\nWe would like to know if you have any additional feedback on our revised submission, and we would be happy to address any further questions. If our feedback is good enough, we wonder if you would like to increase your rating accordingly.\n\nBest regards,\nAuthors of Bridge-TTS"
                    }
                },
                "number": 24,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1017/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700734306770,
                "cdate": 1700734306770,
                "tmdate": 1700734306770,
                "mdate": 1700734306770,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7hZ50LpWQt",
                "forum": "F9ApWtHVac",
                "replyto": "mdFREvS9hk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1017/Reviewer_SrNQ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1017/Reviewer_SrNQ"
                ],
                "content": {
                    "title": {
                        "value": "Response to the authors"
                    },
                    "comment": {
                        "value": "I appreciate the efforts put by authors in addressing my concerns. Unfortunately, my confidence has decreased on the empirical generality of this method on new domains. Also, I am not able to verify the correctness of all theoretical claims. (Although this doesn't affect my score). In my humble (and perhaps incorrect) opinion, authors may target a more theory-oriented venue for better reception of their method OR demonstrate this method works on multiple domains. Currently, authors are mixing strong theoretical claims (which seems to be domain independent) with highly practical problem of TTS. This is a tricky mix to explore in a paper. Alternatively, I think authors could have chosen image modality for their work as audio domain may need more empirical evaluations to sell the idea (again, this could be a less informed opinion of mine).\n\nFinally, I also don't know if in Table 2 if MOS improvement from 3.98 to 4.05 with no change in (high) standard deviation of 0.07 is enough for ICLR."
                    }
                },
                "number": 25,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1017/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700737730956,
                "cdate": 1700737730956,
                "tmdate": 1700737730956,
                "mdate": 1700737730956,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "o6mU4WjFTZ",
                "forum": "F9ApWtHVac",
                "replyto": "mdFREvS9hk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1017/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1017/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your response"
                    },
                    "comment": {
                        "value": "Dear Reviewer SrNQ,\n\nWe appreciate your feedback on our updated submission. We were hoping to provide our answers as follows:\n\nAt first, when submitting our paper to ICLR, we have chosen the generative model track, welcoming your mentioned theory-oriented evaluation. \n\nSecond, as we have clarified, we didn't claim that our method can be directly applied to other domains, though we hope so.\u00a0**Our theoretical assumptions such as paired data are\u00a0motivated by the TTS task\u00a0directly**, and the theoretical contributions on the TTS task itself are still valid. We are not \"mixing strong theoretical claims with highly practical problem of TTS\". About the reason why comparing results on the LJ-Speech dataset, we follow all the published methods that improve diffusion-based acoustic model and adopt their report training and sampling settings, in order to conduct a fair comparison between different methods.\n\nThird, we were hoping to claim that **a CI of 0.07 is not a high number**, which could be verified in other TTS papers. Though other reviewers are concerned about the performance improvement, we have clarified the misunderstandings thoroughly to verify that we do distinctively outperform previous methods.\n\nWe are sincerely grateful for your effort in reviewing our paper again, while your rating has actually decreased instead of the confidence.\n\nBest regards,\nAuthors of Bridge-TTS"
                    }
                },
                "number": 27,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1017/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700739387997,
                "cdate": 1700739387997,
                "tmdate": 1700740345812,
                "mdate": 1700740345812,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "87LD6A7MAg",
            "forum": "F9ApWtHVac",
            "replyto": "F9ApWtHVac",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1017/Reviewer_vLu2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1017/Reviewer_vLu2"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a non-autoregressive TTS model called Bridge-TTS, which is build on the Schrodinger bridge (SB). Bridge-TTS follows the two-stage TTS pipeline, i.e., the TTS system comprises with a text-to-spectrogram acoustic model and spectrogram-to-wave vocoder model, where the Schrodinger bridge is used in modeling the former. Unlike most diffusion-based TTS models, Bridge-TTS uses deterministic prior, which is learned from the text input in a deterministic way via a text encoder module. Bridge-TTS is able to use diffusion-TTS-like sampling procedure to synthesis samples from the prior, where different SDE/ODE-based samplers can be adopted to trade-off the inference speed and the sample quality. In general, this submission makes a clear presentation, making detailed and well-structured explanations from the theories of diffusion models to that of the Schrodinger bridge, and decent derivation of the methodology in adopting SB in paired data modeling, e.g., TTS task, as long as the training objective and different sampling schemes. A singer-speaker TTS experiment using the well-known TTS benchmark corpus LJ-Speech is conducted to verify the arguments by the paper."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- This paper introduces yet another generative model, i.e., the Schrodinger bridge, for tackling the TTS task.\n- This paper presents derivations for bridge sampling in the context that the number of sampling steps is small for the first time, and gives exact solution and 1st-order discretization of SB SDE and ODE, allowing for efficient sampling with SB-based generative models. Relationship of the solution to some famous sampling schemes, such as DDIM, is also presented.\n- This submission has source codes, which could be helpful for reproducing the results presented."
                },
                "weaknesses": {
                    "value": "Novelty:\n\nThe methodology of Bridge-TTS introduced in this submission does not attempt to address the most urgent issues of in the TTS research field, e.g., the prosody modeling, and the paper doesn\u2019t even specify which duration modeling and text-spectrogram alignment scheme are employed. Moreover, the contribution is incremental since this paper introduces yet another kind of generative model into TTS and does not receive significant performance improvement according the experimental results presented. This submission argues that  replacing the noisy prior in previous systems with the clean and deterministic prior can boost the TTS sample quality and inference speed. However, similar arguments have been made and verified in previous works, such as DiffSinger and DiffGAN-TTS. If we look at the training scheme and loss objective carefully, the text encoder output $z$ is in fact the coarse predicted Mel spectrogram as in DiffSinger, which is learned by using the simple MSE-based reconstruction loss. The SB-based module is indeed a spectrogram post-processing module or a \u201cspectrogram super-resolution module\u201d, and can only refine the details of the produced spectrogram and can not fully leverage the generative modeling power of diffusion-based or SB-based models. In this regard,  the contribution of this paper is not sufficient and only incremental.\n\nExperiments:\n\n- Only conducted single-speaker TTS on the LJ-Speech corpus: this is not sufficient since TTS models have reached human-level quality on this data, e.g., NaturalSpeech and StyleTTS-2. It will be more sound if multi-speaker or even multi-emotion TTS experiments are conducted.\n- The reason for explaining why Grad-TTS with 1000 NFEs has higher MOS score than that of Bridge-TTS with 1000 NFEs is not convincing."
                },
                "questions": {
                    "value": "- Why the MOS scores of \u201cRecording\u201d and \u201cGT-Mel-Voc.\u201d in Table 2 are so different from those in Table 3?\n- Why Grad-TTS (NFE=1000) is faster than Bridge-TTS (NFE=1000) in terms of RTF, and in other cases such as NFE=50 and 4, Grad-TTS is slower than or has equal RTF to Bridge-TTS?\n- Why do you think SB-based spectrogram post-processor is better than diffusion-based ones, e.g., coarse predicted Mel spectrogram as condition to Grad-TTS decoder? Is there an intuitive explanation?\n- How do you align text and spectrogram during training and how do you model phoneme durations?\n\nTypos and minor edits in presentation:\n\n- There is no definition of $\\Psi$ and $\\hat\\Psi$.\n- I think the symbols of \u201cforward score\u201d and \u201cbackward score\u201d in the title of Table 1 are reversed.\n- \u201cIn practice, we prefer the noise prediction\u201d \u2192 \u201cIn practice, we prefer to the noise prediction\u201d"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1017/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1017/Reviewer_vLu2",
                        "ICLR.cc/2024/Conference/Submission1017/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1017/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699333447009,
            "cdate": 1699333447009,
            "tmdate": 1700660858584,
            "mdate": 1700660858584,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "bmBxx4ltd9",
                "forum": "F9ApWtHVac",
                "replyto": "87LD6A7MAg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1017/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1017/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer vLu2"
                    },
                    "comment": {
                        "value": "Thank you for your valuable comments and feedback on our work. We would like to provide our answer and update as follows:\n \n> W1: The contribution is incremental since this paper introduces yet another kind of generative model into TTS and does not receive significant performance improvement according to the experimental results presented.\n\n**A**: We admit our contribution was incremental in the previous version. Recently, we have made great progress on both sample quality and inference speed, demonstrating stronger quality than Grad-TTS and the mentioned DiffSinger and DiffGAN-TTS. Please refer to *Common Response* and our updated submission for more details. Moreover, in comparison with them, we are proposing a TTS system built on a data-to-data process, which is different from simply making the noisy prior distribution more informative. Also, we do not require training an additional acoustic model and finding the intersection point like shallow diffusion systems.  \n\n> W2: The SB-based module is indeed a spectrogram post-processing module or a \u201cspectrogram super-resolution module\u201d and can only refine the details of the produced spectrogram and can not fully leverage the generative modeling power of diffusion-based or SB-based models.\n\n**A**: At first, as our text encoder is adopted from Grad-TTS which includes a phoneme encoder and duration predictor, its output cannot provide reasonable quality like a pre-trained acoustic model. Moreover, as our SDE generation process includes both an encoding (adding noise) stage and a decoding (denoising) process, our method is not always a post-processing or super-resolution module. We have shown the advantages of our data-to-data process on both generation quality and sampling speed in our updated version and we kindly request you to re-evaluate our method based on the new experimental results.\n\n> W3: Only conducted single-speaker TTS on the LJ-Speech corpus.\n\n**A**: NaturalSpeech and StyleTTS-2 are end-to-end TTS systems requiring complex loss functions, longer training time, and more computational resources in training. Meanwhile, our focus is on demonstrating the advantages of the proposed data-to-data generation process over the data-to-noise process in diffusion models. In most diffusion-based TTS systems like Grad-TTS (ICML 2021), DiffSinger (AAAI 2022), ProDiff (ACM MM 2022), CoMoSpeech (ACM MM 2023), and your mentioned NaturalSpeech, LJ-Speech is the only reported TTS dataset, providing a fair comparison. We have shown significantly better quality than other diffusion-based acoustic models in our updated version. In the future, we will study the end-to-end TTS system with our method and find multi-speaker TTS baseline methods to demonstrate our performance.\n\n> W4: The reason for explaining why Grad-TTS with 1000 NFEs has a higher MOS score than that of Bridge-TTS with 1000 NFEs is not convincing.\n\n**A**: We agree with your comment. We have proposed new technologies and updated our results, which are conducted on Amazon Mechanical Turk. We kindly recommend you check our updated results. \n\n> Q1: Why the MOS scores of \u201cRecording\u201d and \u201cGT-Mel-Voc.\u201d in Table 2 are so different from those in Table 3?\n\n**A**: In the previous submission, the invited judges may not be familiar with TTS evaluation, and the results in two Tables are separately evaluated. In our updated version, we have employed 25 Master workers on Amazon Turk to evaluate 20 synthesized samples in order to guarantee the reliability of results.\n\n> Q2: Why Grad-TTS (NFE=1000) is faster than Bridge-TTS (NFE=1000) in terms of RTF, and in other cases such as NFE=50 and 4, Grad-TTS is slower than or has equal RTF to Bridge-TTS?\n\n**A**: We use the first-order sampler, while Grad-TTS uses the Euler sampler. When NFE is 1000, our speed is slightly slower than Grad-TTS. When NFE is 50 or 4, the difference of RTF is not distinctive, and we do not report that Bridge-TTS is faster than Grad-TTS, given the same NFE. \n\n> Q3: Why do you think SB-based spectrogram post-processor is better than diffusion-based ones, e.g., coarse predicted Mel spectrogram as a condition to Grad-TTS decoder? Is there an intuitive explanation?\n\n**A**: This is a great question. Grad-TTS decoder is built on a data-to-noise process, while we are proposing a data-to-data process. Our SDE sampling process adds noise on the prior at first and removes the noise to generate samples. In this way, our generation may be less restricted by the prior (i.e., the coarse predicted mel-spectrogram) than Grad-TTS, as their sampling only contains a denoising process.\n\n> Q4: How do you align text and spectrogram during training, and how do you model phoneme durations?\n\n**A**: For a fair comparison with diffusion models, we use the same phoneme encoder and duration predictor in Grad-TTS, which is leveraged from Glow-TTS."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1017/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700048677322,
                "cdate": 1700048677322,
                "tmdate": 1700048677322,
                "mdate": 1700048677322,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zC3rT0btkz",
                "forum": "F9ApWtHVac",
                "replyto": "87LD6A7MAg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1017/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1017/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking forward to further feedback"
                    },
                    "comment": {
                        "value": "Dear Reviewer vLu2,\n\nWe thank you very much again for the great efforts on reviewing our manuscript and providing the valuable comments to further improve. We hope you may find our response and updated submission (paper, website, and synthesized samples) satisfactory, and evaluate our contribution on generative models, TTS sample quality, and inference speed compared to diffusion-based TTS systems.\n\nIf you want any further discussion, we would be very happy to reply. If our feedback is good enough, we wonder if you would like to increase your rating accordingly.\n\nBest, \nAuthors of Bridge-TTS"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1017/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700296097486,
                "cdate": 1700296097486,
                "tmdate": 1700296151236,
                "mdate": 1700296151236,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "niXQkllF9K",
                "forum": "F9ApWtHVac",
                "replyto": "tCSDDuqLTi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1017/Reviewer_vLu2"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1017/Reviewer_vLu2"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors"
                    },
                    "comment": {
                        "value": "Thank you for your diligent efforts in addressing the concerns raised during the review process. I appreciate the substantial work you have put into improving your paper in response to the feedback. After careful re-evaluation of your revised manuscript and considering the changes made, I acknowledge the progress and improvements you have achieved. The improvements are indeed noteworthy, but they have not sufficiently addressed the critical aspects that influenced my initial evaluation, as mentioned in the initial review comments. In light of these improvements, I have decided to revise my score to a 5."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1017/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700660822191,
                "cdate": 1700660822191,
                "tmdate": 1700660822191,
                "mdate": 1700660822191,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]