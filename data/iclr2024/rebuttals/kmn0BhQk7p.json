[
    {
        "title": "Beyond Memorization: Violating Privacy via Inference with Large Language Models"
    },
    {
        "review": {
            "id": "nHD6rOZnlp",
            "forum": "kmn0BhQk7p",
            "replyto": "kmn0BhQk7p",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9451/Reviewer_uZeq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9451/Reviewer_uZeq"
            ],
            "content": {
                "summary": {
                    "value": "The authors demonstrate how LLMs can be used as to infer sensitive information from comments and text, such as location, occupation, place of birth, education, etc. The attack they propose involves prompting the LLM by asking them to be \u201can expert investigator\u201d tasked with recovering these sensitive attributes from unstructured textual bodies. They formulate two kinds of attacks: a passive attack where the LLM is fed this information and an active method where the agent is presumed to be assisting the user while simulatenously trying to recover personal information. They try and mitigate their attack using a client-side anonymization method and via provider-side alignment."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "The paper is well organized, and presents a new and rising privacy risk. Such an attack was not feasible when a human was tasked with having to recover facts manually, and so the contribution is timely and well motivated. They surface important privacy-related risks and demonstrates that much more work needs to be done to mitigate these kinds of attacks."
                },
                "weaknesses": {
                    "value": "Further analysis into how recovery of different kinds of PII was correlated would have been appreciated."
                },
                "questions": {
                    "value": "- Have you considered using LLMs to \"rewrite\" inputs in a privacy-preserving manner?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9451/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9451/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9451/Reviewer_uZeq"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9451/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698253323603,
            "cdate": 1698253323603,
            "tmdate": 1699637189885,
            "mdate": 1699637189885,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "n3Wq0bW38Y",
                "forum": "kmn0BhQk7p",
                "replyto": "nHD6rOZnlp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9451/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9451/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their feedback and will respond to the raised questions below.\n\n**Q5.1: Further analysis into how recovery of different kinds of PII was correlated would have been appreciated.**\n\nWe agree with the reviewer that such analysis can be very insightful. However even with our labeling efforts the resulting dataset is quite small and combined with varying attributes per users (as well as varying hardness levels and overall profiles) we did not feel confident to make such an analysis across attributes. We tried to explore a more ideal state version of this in our ACS experiments where significantly more data points are available and correlations can be observed in a more direct fashion. In particular we could study the correlation of other personal attributes with a target attribute in isolation for example how well \u201crace\u201d could be predicted from \u201clocation, income, work-class, and citizenship\u201d alone. With the limited amount of data in PersonalReddit we cannot make such conclusions reliably.\n\n**Q5.2: Have you considered using LLMs to \"rewrite\" inputs in a privacy-preserving manner?**\n\nAgain, we agree with the reviewer that this is a potentially exciting direction for future work that may address some of the shortcomings of traditional methods (as highlighted in the anonymization experiment). For the current study, we wanted to focus on the adversarial scenario, including the evaluation of currently existing defenses. As also pointed out by reviewer R-buQH, there is a potentially challenging trade-off between privacy and utility (re-writing parts of the input may change parts of the intended message) whose accurate reflection requires further study and that we did not feel confident to put into this submission. Independently, we agree with the reviewer that upcoming techniques, including LLMs, can improve defenses and better privacy protection."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9451/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700145243914,
                "cdate": 1700145243914,
                "tmdate": 1700145243914,
                "mdate": 1700145243914,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HmmM3ZADR3",
                "forum": "kmn0BhQk7p",
                "replyto": "nHD6rOZnlp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9451/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9451/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "As the interactive rebuttal window will close soon, we thank all reviewers again for all their helpful feedback. We believe we have addressed the reviewer's questions in our answer and are eager to continue the conversation in case of further questions."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9451/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700578761166,
                "cdate": 1700578761166,
                "tmdate": 1700578761166,
                "mdate": 1700578761166,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "DDC8a2FEIK",
            "forum": "kmn0BhQk7p",
            "replyto": "kmn0BhQk7p",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9451/Reviewer_h2Jj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9451/Reviewer_h2Jj"
            ],
            "content": {
                "summary": {
                    "value": "The paper discusses novel privacy threats resulting from inference capabilities of LLMs in two different settings :- \n- They show that LLMs can infer personal user attributes from their online activity. \n- They also discuss how malicious chatbots can steer conversations to uncover private information. Experiments on 9 state-of-the-art LLMs demonstrates their effectiveness in inferring personal attributes from real-world Reddit data.\n\nThey show that common mitigation methods like text anonymization and model alignment are currently ineffective at protecting user\nprivacy against these attacks"
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Novel privacy threats emerging from the strong inference capabilities of current state-of-the-art LLMs in a zero-shot setting are discussed.\n- A full release of a dataset of 525 human-labeled synthetic examples to further research in this area.\n- Ineffectiveness of current mitigation methods against these attacks is discussed"
                },
                "weaknesses": {
                    "value": "- Labelling procedure for obtaining ground truths for the dataset should get multiple labels for each profile to make the results statistically significant. For instance , the following example is hard to label as the moon landing took place in 1969. \n> \u201doh... I remember watching the moon landing in 1959 with my father. he picked me up from school and we went home and watched it on television. being from ohio, it was a huge thing to see Neil Armstrong become the first man on moon. funnily, this is the only specific memory I have from first grade in primary school, was a looong time back, eh\u201d Age: 70 years"
                },
                "questions": {
                    "value": "- Labelling procedure - It seems only one human label was obtained per example as there is no mention of how final labels are aggregated. Is that the case ? \n- More discussion on how you obtained these numbers?\n> achieving up to 85% top-1 and 95.8% top-3 accuracy at a fraction of the cost(100\u00d7) and time (240\u00d7) required by humans."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9451/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9451/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9451/Reviewer_h2Jj"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9451/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698824702671,
            "cdate": 1698824702671,
            "tmdate": 1699637189776,
            "mdate": 1699637189776,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "FAFxm3TLCm",
                "forum": "kmn0BhQk7p",
                "replyto": "DDC8a2FEIK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9451/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9451/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their feedback and will respond to the raised questions below.\n\n**Q4.1: The labeling procedure should include multiple labels for each profile.**\nWe agree with the reviewer that multiple labels for each profile are an optimal solution. At the same time, labeling the profiles is very expensive (as highlighted in the paper). Since then we have cross-labeled $\\sim$25% of the dataset. From this, we found that labelers agree on >90% (222 of 246 attributes) of the labels that both reviewers reported a certainty of at least 3 (i.e., the setting used in the paper). Out of the non-aligned 24 examples, we found only 7 ($\\sim$3%) with strong disagreement (e.g., no relation vs. in relation), while the rest were all either less precise (\"Boston\" vs. \"Massachusetts\") or very close within a neighboring category (e.g., \"divorced\" vs. \"no relation\"). Empirically, we found that such adjacent cases are commonly accounted for in LLMs' top-2 and top-3 accuracies. We added these results to the paper to improve the clarity on the issue. On the 222 labels where both reviewers agreed, GPT-4 has a Top-{1,2,3} accuracy of 92.7%, 98.1%, and 99.09%, respectively.\n\nWe also thank the reviewer for spotting an issue in the synthetically created sample. We introduced this error while writing some synthetic examples by hand, and it has been carried forward. We have corrected this in the updated version of the submission.\n\n**Q4.2: How did we get the cost and time estimates of 100x and 240x respectively.**\n\nWe refer to our answer of CQ1 in the common section. In particular, we compare the estimated time it would take a single human labeler to go through PersonalReddit (as measured by the time it took us) and compare it with the realized runtime of our inference script using GPT-4. From this, we also derive the cost savings by assuming an hourly labeling wage of 20 USD. Since our study, OpenAI both increased rate limitations by 2x and decreased cost on newer versions of GPT-4 by roughly 3x, which in practice would favor LLMs even more."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9451/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700145148366,
                "cdate": 1700145148366,
                "tmdate": 1700145148366,
                "mdate": 1700145148366,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tw3aI5VBag",
                "forum": "kmn0BhQk7p",
                "replyto": "DDC8a2FEIK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9451/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9451/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "As the interactive rebuttal window will close soon, we thank all reviewers again for all their helpful feedback. We believe we have addressed the reviewer's questions in our answer and are eager to continue the conversation in case of further questions."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9451/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700578738362,
                "cdate": 1700578738362,
                "tmdate": 1700578738362,
                "mdate": 1700578738362,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rIyxSKQs3Y",
                "forum": "kmn0BhQk7p",
                "replyto": "FAFxm3TLCm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9451/Reviewer_h2Jj"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9451/Reviewer_h2Jj"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your response!"
                    },
                    "comment": {
                        "value": "Thanks you for your detailed response and thank you for your valuable contribution to this field!"
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9451/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700599288357,
                "cdate": 1700599288357,
                "tmdate": 1700599288357,
                "mdate": 1700599288357,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "5ZYljDrhoa",
            "forum": "kmn0BhQk7p",
            "replyto": "kmn0BhQk7p",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9451/Reviewer_FqCb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9451/Reviewer_FqCb"
            ],
            "content": {
                "summary": {
                    "value": "This paper focusses on extracting personal attributes of Reddit users based on the comments they left on a subset of reddit communities. The methodology involves prompting SOTA LLMs"
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper opens up a really interesting problem, and conducts thorough experiments to demonstrate that LLMs can be used to infer personal attributes from online comments. This is an important and novel research topic. \n\nThe experimental set up is really convincing. I really appreciate the rating of the difficulty of attribute assignment and the anonymisation experiment."
                },
                "weaknesses": {
                    "value": "1. The presentation could be clearer. One of the main contributions of the paper is the release of a synthetic data set. However it is not clear how this data set was created. This should be discussed in the main part of the paper. But also the presentation in the appendix does not make it obv how to reproduce the data set creation.\n2. Re the findings on the ACS data set, I wonder whether it is obv that the LLMs have not seen and memorised the ACS data.\n3. I wonder how much of the results are due to memorisation. While the authors have controlled for memorisation on long comments, I am not sure how convincing the methodology is. It would be interesting to see the subreddit prediction performance of LLMs of a comment for instance. \n4. Do the authors release results on the synthetic examples? Since the experiment is not reproducible, it would be important to have result s on the synthetic data so future work can build upon the results of this paper."
                },
                "questions": {
                    "value": "1. How was the synthetic data set created?\n2. What is the baseline in Fig. 25?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Potentially harmful insights, methodologies and applications"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "This paper is exploring how LLMs can be used to infer personal information from seemingly benign comments. As such this research could be misused by malicious actors. The paper takes steps towards preventing this (no release of real data set), so my ethical concerns are low but an ethics review by experts might still be necessary."
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9451/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9451/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9451/Reviewer_FqCb"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9451/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698835437333,
            "cdate": 1698835437333,
            "tmdate": 1700148120551,
            "mdate": 1700148120551,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "pphcFsASPP",
                "forum": "kmn0BhQk7p",
                "replyto": "5ZYljDrhoa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9451/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9451/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their feedback and will respond to the raised questions below.\n\n**Q3.1: Can you clarify the creation of the synthetic examples?**\n\nCertainly, while we refer to our answer to CQ2 in the common section for a more detailed description, we will briefly summarize it here. In particular, we created over 1000 single-round conversations (across all hardness levels), each consisting of one question to set a random topic and one answer. Both the question and the answer were generated by separate LLM instances grounded in PersonalReddit examples whenever possible. We note that we had a separate grounding for each combination of attribute and hardness (40 in total) for each LLM. We then manually reviewed the answers given as to whether they revealed the respective attribute at the targeted hardness level, adjusting hardness levels or completely removing examples accordingly.\n\n**Q3.2: What are the chances that the ACS dataset has been memorized in the ablation experiment?**\n\nMemorization is hard to evaluate, and it is certainly possible that traces of the ACS dataset have been part of the training data. However, following recent literature [1] on memorization, we do not believe that our examples were memorized as (1) We did not use the representation in which ACS is commonly available on the internet (instead transforming it into new textual prompts) and (2) it being unlikely that the ACS was in the training data a large number of times. Further, we inspected several of the inferences and their corresponding reasonings made by the model, ensuring that they are sensible and self-contained. That said, the model has to have seen census data to make such inferences. However, this should not be categorized as (verbatim) memorization but rather as knowledge available to the model.\n\n**Q3.3: How does memorization impact the inference results presented in this paper? What is the subreddit prediction performance**\n\nSimilar to the previous question, we believe (and tested following the Sota benchmark format) that no memorization occurred as part of the inferences. We further inspected many of the LLM inferences manually and can attest them to be reasonable and self-contained. We note that subreddit prediction is a problematic measurement of memorization as comments can contain phrases, e.g., \u201cUgh, I hate people here in r/relationships sometimes, you all \u2026\u201d, or specific topics (e.g., finances in /r/personalfinance) that enable direct inferences without memorization. Some works even directly treat the subreddit as a (supervised) prediction target for accuracy measurements [2, 3].\n\n**Q3.4: Do you release the prediction performance on the synthetic examples?**\n\nYes, we release the predictions of GPT-4 (the strongest baseline) for all our synthetic examples alongside our code, and we additionally report the overall accuracy and the per-hardness accuracy in App. F.\n\n**Q3.5: What is the baseline in Fig.25?**\n\nAs given in the text, the baseline is a naive majority class baseline classifier that directly predicts the majority for each respective attribute (over the entire training data). We have improved the writing in this section, making this point clearer.\n\n\n[1] Carlini, Nicholas, et al. \"Quantifying memorization across neural language models.\" arXiv preprint arXiv:2202.07646 (2022).\n\n[2] Shejwalkar, Virat, et al. \"Membership inference attacks against nlp classification models.\" NeurIPS 2021 Workshop Privacy in Machine Learning. 2021.\n\n[3] Giel, Andrew, Jonathan NeCamp, and Hussain Kader. \"CS 229: r/Classifier-Subreddit Text Classification.\" (2015)."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9451/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700145001117,
                "cdate": 1700145001117,
                "tmdate": 1700145001117,
                "mdate": 1700145001117,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8oe2SIOmN9",
                "forum": "kmn0BhQk7p",
                "replyto": "pphcFsASPP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9451/Reviewer_FqCb"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9451/Reviewer_FqCb"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for their feedback and am more certain now that this work should be accepted."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9451/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700148090853,
                "cdate": 1700148090853,
                "tmdate": 1700148090853,
                "mdate": 1700148090853,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "aPvo3w4FSN",
            "forum": "kmn0BhQk7p",
            "replyto": "kmn0BhQk7p",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9451/Reviewer_WVSy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9451/Reviewer_WVSy"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents the privacy risks of LLM to infer personal attributes from user-written text. Empirical results show that the current LLM can infer a wide range of personal attributes from text with proper prompts."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This is the first work showing LLM could effectively infer sensitive attributes from user-written text. This work could have a high impact on the community. \n2. The authors have conducted comprehensive experiments to substantiate the key statement presented in the paper.\n3. The paper offers a novel perspective on the study of Language Model Models (LLMs)."
                },
                "weaknesses": {
                    "value": "1. Some experiment setups should be justified. For some attributes (e.g., MSE for age), accuracy is not the correct metric.\n2. Using sensitive topics and need additional ethics review. For example, whether the study is  IRB approved? \n3. Quality checks of the synthetic data generation is missing. In the paper, the author fails to mention what types of quality checks they performed on the collected synthetic data, weakening the soundness of the paper. \n\nMinor:\n 1. Typos: Mititgations -> Mitigations, exampels -> examples"
                },
                "questions": {
                    "value": "Please provide a response to the weaknesses mentioned above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Privacy, security and safety",
                        "Yes, Potentially harmful insights, methodologies and applications"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "The paper leverages LLMs to infer the sensitive attribute of user-written text, which might need a more in-depth ethics review."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9451/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699210584866,
            "cdate": 1699210584866,
            "tmdate": 1699637189524,
            "mdate": 1699637189524,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "12paqP3G5E",
                "forum": "kmn0BhQk7p",
                "replyto": "aPvo3w4FSN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9451/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9451/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their feedback and will respond to the raised questions below.\n\n**Q2.1: Can you explain your choice of metrics / experiment design better? Why did you use accuracy for age?**\n\nCertainly, and we also improved the respective section in the paper. In particular, we did not use MSE for our age prediction experiments as we required a metric that (1) allowed both concrete ages and age ranges (and combinations) as inputs and (2) could be aggregated with our other metrics. A common choice in prior works, including privacy [1] and age estimation [2,3], is to evaluate accuracy based on a thresholding criterion. This was also done in, e.g., past PAN competitions [4], where age prediction accuracy was computed on discrete target ranges. We agree with the reviewer that these choices need to be made more explicit in the paper, which we have accounted for in our updated version uploaded alongside this rebuttal answer.\n\n\n**Q2.2: Did you have ethical considerations before publishing the work?**\n\nYes, hence our decision not to publish any annotated profiles (and only release synthetic examples). We also proactively contacted all major LLM providers in the study to inform them of potential risks (and offering active conversations) before making any draft of our manuscript public. Concerning the dataset itself, we want to note that we only used public comments that are already available in a pre-existing dataset on HuggingFace (in particular, we did not scrape them). The used dataset [5] and (contained) Reddit comments have been extensively used in prior research [6,7,8,9,10]. Given these precautions and our decision not to disclose our annotations, we determined that an IRB review was not necessary. Overall, we share the reviewer's intuition on the ethical and privacy concerns of our findings, which we believe to have addressed to the maximum extent within our means. We also believe that one of the most important contributions we as privacy researchers can make is revealing such vulnerabilities, which ultimately allows for patching them, prohibiting malicious actors from silently exploiting vulnerabilities.\n\n**Q2.3: Which steps were taken to ensure the quality of the synthetic examples?**\n\nWe refer to our answer of CQ2 in the common section. In particular, all synthetic examples were manually inspected by the same human labelers as the PersonalReddit dataset to ensure that hardness levels across attributes are aligned with what has been observed there. This also included re-adjusting hardness scores or completely removing individual samples.\n\n[1] Mark Vero, Mislav Balunovi\u0107, Dimitar I. Dimitrov, Martin Vechev: \u201cTabLeak: Tabular Data Leakage in Federated Learning\u201d, 2022; arXiv:2210.01785.\n\n[2] Morgan-Lopez AA, Kim AE, Chew RF, Ruddle P (2017) Predicting age groups of Twitter users based on language and metadata features. PLOS ONE 12(8): e0183537. https://doi.org/10.1371/journal.pone.0183537\n\n[3] Levi, Gil, and Tal Hassner. \"Age and gender classification using convolutional neural networks.\" Proceedings of the IEEE conference on computer vision and pattern recognition workshops. 2015.\n\n[4] Rosso, Paolo, et al. \"Overview of PAN\u201916: new challenges for authorship analysis: cross-genre profiling, clustering, diarization, and obfuscation.\" Experimental IR Meets Multilinguality, Multimodality, and Interaction: 7th International Conference of the CLEF Association, CLEF 2016, \u00c9vora, Portugal, September 5-8, 2016, Proceedings 7. Springer International Publishing, 2016.\n\n[5] Baumgartner, Jason, et al. \"The pushshift reddit dataset.\" Proceedings of the international AAAI conference on web and social media. Vol. 14. 2020.\n\n[6] Medvedev, Alexey N., Renaud Lambiotte, and Jean-Charles Delvenne. \"The anatomy of Reddit: An overview of academic research.\" Dynamics On and Of Complex Networks III: Machine Learning and Statistical Physics Approaches 10 (2019): 183-204.\n\n[7] Finlay, S. Craig. \"Age and gender in Reddit commenting and success.\" (2014).\n\n[8] Hada, Rishav, et al. \"Ruddit: Norms of offensiveness for English Reddit comments.\" arXiv preprint arXiv:2106.05664 (2021).\n\n[9] Turcan, Elsbeth, and Kathleen McKeown. \"Dreaddit: A reddit dataset for stress analysis in social media.\" arXiv preprint arXiv:1911.00133 (2019).\n\n[10] Proferes, Nicholas, et al. \"Studying reddit: A systematic overview of disciplines, approaches, methods, and ethics.\" Social Media+ Society 7.2 (2021): 20563051211019004."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9451/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700144790696,
                "cdate": 1700144790696,
                "tmdate": 1700144790696,
                "mdate": 1700144790696,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "AxvkhOVjfQ",
                "forum": "kmn0BhQk7p",
                "replyto": "aPvo3w4FSN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9451/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9451/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "As the interactive rebuttal window will close soon, we thank all reviewers again for all their helpful feedback. We believe we have addressed the reviewer's questions in our answer and are eager to continue the conversation in case of further questions. \nWe kindly request the reviewers to consider adjusting their score to reflect the improvements and clarifications made in response to their input."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9451/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700578700642,
                "cdate": 1700578700642,
                "tmdate": 1700578700642,
                "mdate": 1700578700642,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Tlwr70rNCq",
                "forum": "kmn0BhQk7p",
                "replyto": "AxvkhOVjfQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9451/Reviewer_WVSy"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9451/Reviewer_WVSy"
                ],
                "content": {
                    "title": {
                        "value": "Response after Rebuttal"
                    },
                    "comment": {
                        "value": "Thank you for your rebuttal. I think the paper is solid and I will keep my score."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9451/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700711864977,
                "cdate": 1700711864977,
                "tmdate": 1700711864977,
                "mdate": 1700711864977,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "8NbuASNUu3",
            "forum": "kmn0BhQk7p",
            "replyto": "kmn0BhQk7p",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9451/Reviewer_buQH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9451/Reviewer_buQH"
            ],
            "content": {
                "summary": {
                    "value": "The authors show how LLMs (trained with information across the information) can utilize syntactic cues in written text to identify semantic (and personally identifiable) attributes of users. They demonstrate the feasibility of their approach on a custom-curated dataset of posts from Reddit."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. First paper demonstrating feasibility of such an attack.\n2. Reasonably well written (though the paper contains formalisms that are quite honestly unnecessary, and punts  a lot of relevant details to the appendix)."
                },
                "weaknesses": {
                    "value": "1. Irreproducible\n2. Implications are a function of how good the humans are i.e., if the golden labels are inaccurate (e.g., how can I be sure that the age attribute is within error tolerance), all conclusions need to be made with a grain of salt."
                },
                "questions": {
                    "value": "I enjoyed reading the paper. It demonstrates a variant of \u201clinkability attacks\u201d in LLMs and empirically validates it. \n\n1. Apart from the fact that one can launch such an attack, this reviewer has gained no new technical insight from this work. While this demonstrates \u201cfeasibility\u201d and that is of merit, what do follow-up works look like in this area?\n2. The authors motivate their work by stating that identifying certain attributes is potentially hazardous for people since these attributes can be cross-referenced with public databases to de-identify users. This reviewer believes this claim is a stretch; could the authors highlight how one can deanonymize the users that were present in the dataset they considered? While these claims are \u201ctrue\u201d from an academic sense, making these threats practical requires a lot of additional work which the authors do not factor in.\n3. The reviewer agrees with the authors that the LLM can be used to coerce users into sharing more private information. However, In the adversarial interaction scenario, the attack is easy to thwart. Users could perform prompt injection (as noted in this thread: https://x.com/StudentInfosec/status/1640360234882310145?s=20) and can read the instructions. Given how brittle LLMs are to such forms of attacks, how reliable can such \u201ccoercion attempts\u201d be made?\n4. The notion of \u201cdefenses\u201d against such attacks also seems slim. But should this be something that we need to actively defend against? Sharing posts (as done in the status quo) intrinsically contains some notion of utility that will be removed if the deducible information is scrubbed. Can the authors comment on the same?\n5. While the LLMs are certainly faster than humans, I don\u2019t believe the numbers in this study are the very best humans can do. Could the authors describe how their human baseline can be improved? My understanding is that few humans were tasked with identifying attributes using web search (without much training on this front)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9451/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9451/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9451/Reviewer_buQH"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9451/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699292387790,
            "cdate": 1699292387790,
            "tmdate": 1699713159792,
            "mdate": 1699713159792,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "hTRHIynUh2",
                "forum": "kmn0BhQk7p",
                "replyto": "8NbuASNUu3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9451/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9451/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their feedback and will respond to the raised questions below.\n\n**Q1.1: What are potential follow-up works in this area?**\n\nAfter this initial study, we see several promising directions for future research. From the adversarial perspective, how such attacks could be made stronger (e.g., via RAG, fine-tuning, etc.) or translated to new modalities (especially with the rise of multi-modal models) such as images. At the same time, our discussions with model providers have shown us that this issue is taken seriously in the industry, fueling the need for better defenses (both from the provider and user side). This is a challenging topic as such inferences commonly cannot be as easily quantified as, e.g., the generation of toxic comments. Nevertheless, we got feedback that specific alignments against such inferences are planned for newer generations of these models. Additionally, improving user-side defenses, such as anonymization, or developing other mitigation methods is also a promising and worthwhile research direction to pursue. Overall, the ground for all these directions is the awareness that such inferences are possible, and we believe our paper makes a valuable contribution in this regard.\n\n**Q1.2: How could a potential cross-referencing attack work in practice?**\n\nWe agree with the reviewer that such attacks require additional effort. At the same time, it needs to be said that especially with open access to US voter records, there are many instances where having access to city, age, and ethnicity is sufficient to uniquely identify an individual in such freely available databases (e.g., voterrecords.com) and only a limited amount of additional effort is required. Further, LLM inferences could be applied in cases where partial knowledge of these attributes is available (e.g., IP, mail addresses shared with a bot provider or even records that are part of large breaches [1,2,3]), and these could be linked with attributes (e.g., mental health status) extracted from conversations/posts.\n\n**Q1.3: Aren\u2019t coercion attacks reasonably easy to thwart with prompt injections that reveal the system-prompt?**\n\nFirst, we agree with the reviewer that a vigilant security-aware user could (at the current state of LLMs) likely extract the system prompt. At the same time, we have to be aware that a large majority of chatbot users do not possess the knowledge nor the skills necessary to regularly check whether the chatbot they are currently interacting with contains an adversarial system prompt (especially when the bot is instructed not to reveal it). This danger increases with the proliferation of chatbots (character.ai alone has over 16 [4] million chatbots) and the ever-increasing number of people (100 Mio monthly users of OpenAI) using them in various aspects of life."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9451/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700144597503,
                "cdate": 1700144597503,
                "tmdate": 1700144597503,
                "mdate": 1700144597503,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tI0cDk3hrn",
                "forum": "kmn0BhQk7p",
                "replyto": "b9ru3bGsUQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9451/Reviewer_buQH"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9451/Reviewer_buQH"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your response!"
                    },
                    "comment": {
                        "value": "Will internally deliberate. Thanks again for submitting your work to ICLR!"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9451/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700432369891,
                "cdate": 1700432369891,
                "tmdate": 1700432369891,
                "mdate": 1700432369891,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jR5ZiHuthp",
                "forum": "kmn0BhQk7p",
                "replyto": "8NbuASNUu3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9451/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9451/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "As the interactive rebuttal window will close soon, we thank all reviewers again for all their helpful feedback. We believe we have addressed the reviewer's questions in our answer and are eager to continue the conversation in case of further questions. \nWe kindly request the reviewers to consider adjusting their score to reflect the improvements and clarifications made in response to their input."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9451/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700578853390,
                "cdate": 1700578853390,
                "tmdate": 1700578853390,
                "mdate": 1700578853390,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "E6DIsb3iV2",
                "forum": "kmn0BhQk7p",
                "replyto": "tI0cDk3hrn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9451/Reviewer_buQH"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9451/Reviewer_buQH"
                ],
                "content": {
                    "title": {
                        "value": "Based on deliberation"
                    },
                    "comment": {
                        "value": "Hello authors -- I've decided not to increase my score. This is to reflect my stance on the paper, which I think is good and should be accepted, but not without flaws inhibiting practicality. I request the authors to reword the contributions and claims made in light of the discussion with the reviewers, and wish them the best!"
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9451/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700664721236,
                "cdate": 1700664721236,
                "tmdate": 1700664721236,
                "mdate": 1700664721236,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]