[
    {
        "title": "Graph Learning with Distributional Edge Layouts"
    },
    {
        "review": {
            "id": "CZLDoM207f",
            "forum": "N0isTh3rml",
            "replyto": "N0isTh3rml",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission595/Reviewer_VZpe"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission595/Reviewer_VZpe"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a method to sample layouts based on adjacency matrices. The layout can better capture the graph structures by injecting the sampled layout information into the GNN message-passing process. Several layout sampling methods are proposed. The authors evaluate the proposed methods on several benchmarks and increase the performance of the backbone graph learning models.  The layout computation process can be seen as a preprocessing step to boost the GNN's ability to learn graph structures."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is well-written and easy to follow. The proposed approach to sample layout based on free energy seems well-attuned to real-world molecular applications. The proposed method naturally generates layout for each input graph which makes it easier to examine important substructures within the graph, such as functional groups in molecules."
                },
                "weaknesses": {
                    "value": "- The evaluation is limited to smaller datasets (max 4k nodes). However, larger datasets such as QM9 and ZINC contain over 100k graphs. Showing performance increases on these datasets can further validate the method.\n\n- I noticed that the computational efficiency is sub-optimal, requiring ~0.1 to 2 seconds per graph. Note that while preprocessing can save training time, it does not save inference time, which is more critical practically.\n\n- The method is a preprocessing method that computes information unobtainable by MPNNs. While the experiments compare the method with some existing preprocessing methods, a clear comparison/analysis would help understand the mechanism behind the proposed method and why it is superior to RWPE. (In fact, it appears odd to me that the authors omit LapPE, since LapPE also considers global graph information (spectrum). Ideally, a comparison to LapPE should also be provided)."
                },
                "questions": {
                    "value": "Please refer to the weaknesses section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission595/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission595/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission595/Reviewer_VZpe"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission595/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698282748846,
            "cdate": 1698282748846,
            "tmdate": 1699635986858,
            "mdate": 1699635986858,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "R3VNXfREdw",
                "forum": "N0isTh3rml",
                "replyto": "CZLDoM207f",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission595/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission595/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your efforts in proposing these comments and questions. \n# Response to the weaknesses \n* In response to your concern regarding the evaluation's limitations, we appreciate your feedback and have made enhancements. The evaluation now incorporates results from the large-scale ogbg-molhiv dataset [1], presented in Table 10 on Page 20. We observe consistent performance enhancement on this dataset, aligning with existing results. This additional evaluation would provide more comprehensive and accurate support.\n| ogbg-molhiv               | Valid AUROC           | Test AUROC            |\n|---------------------------|-----------------------|-----------------------|\n| GAT                       | 77.67 $\\pm$ 2.78      | 74.45 $\\pm$ 1.53      |\n| GAT (DEL-F)                | **78.99 $\\pm$ 1.69** | **75.27 $\\pm$ 1.32** |\n| Graph transformer         | 78.20 $\\pm$ 1.18      | 76.51 $\\pm$ 0.93      |\n| Graph transformer (DEL-F)  | **80.12 $\\pm$ 1.39** | **76.86 $\\pm$ 0.95** |\n| GPS                       | 80.08 $\\pm$ 1.39      | 76.71 $\\pm$ 1.12      |\n| GPS (DEL-F)                | **82.57 $\\pm$ 0.95** | **76.93 $\\pm$ 1.00** |\n\n* In real-world applications, computing the graph layout for real-time test data does incur additional overhead. However, this overhead can often be mitigated through distributed computing. It's noteworthy that we report a minimal amount of preprocessing time using CPU threads (one or eight). For datasets other than D\\&D, our processing speed for a single graph is typically around 0.1 seconds, while for D\\&D, it is approximately 1 second. Given that the average number of nodes in existing graph classification datasets generally does not exceed that of D\\&D, our method remains feasible in the majority of scenarios.\n\n* We did not deliberately disregard LapPE; our decision to focus on RWSE in the comparative experiment was guided by the conclusion of the paper [2]: \"The benefits of the different encodings are very dataset dependant, with the random-walk structural encoding (RWSE) being more beneficial for molecular data and the Laplacian eigenvectors encodings (LapPE) being more beneficial in image superpixels.\" Therefore, we opted for the more advantageous RWSE for comparison. We will provide updated results for LapPE in Table 1.\n\n[1] Hu, Weihua, et al. \"Open graph benchmark: Datasets for machine learning on graphs.\" Advances in neural information processing systems 33 (2020): 22118-22133.\n\n[2] Ramp\u00e1\u0161ek, Ladislav, et al. \"Recipe for a general, powerful, scalable graph transformer.\" Advances in Neural Information Processing Systems 35 (2022): 14501-14515."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission595/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700469135109,
                "cdate": 1700469135109,
                "tmdate": 1700469135109,
                "mdate": 1700469135109,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "eTVCbTarkw",
                "forum": "N0isTh3rml",
                "replyto": "CZLDoM207f",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission595/Reviewer_VZpe"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission595/Reviewer_VZpe"
                ],
                "content": {
                    "comment": {
                        "value": "After reading the author's response, some of my concerns (computation efficiency) are addressed. However, the proposed method minimally improves the performance of existing models on a larger dataset, ogbg-molhiv. The proposed method still appears to me as a variant of the preprocessing method, and the paper falls short of explaining why the technique outperforms the existing approach (RWSE), which limits its contribution. Hence, I am keeping my original rating."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission595/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700673093412,
                "cdate": 1700673093412,
                "tmdate": 1700673093412,
                "mdate": 1700673093412,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "aGpQ3nwwCB",
            "forum": "N0isTh3rml",
            "replyto": "N0isTh3rml",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission595/Reviewer_8jRK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission595/Reviewer_8jRK"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces Distributional Edge Layouts (DEL) as a novel approach to graph representation learning where topological layouts are generated by sampling from a Boltzmann distribution. DEL is a  pre-processing technique which is independent of subsequent GNN architectures, demonstrating high flexibility and applicability. The integration of DELs into various GNNs yields improvements on diverse datasets, highlighting the practical potential of this approach. While the idea is innovative and shows promise, there are areas that need to be addressed to strengthen the paper"
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1) The concept of DEL, which involves sampling topological layouts from a Boltzmann distribution to improve graph representation learning, is a novel and interesting approach. \n\n2) The paper presents a set of experiments using DEL in combination with various GNN backbones. The results demonstrate that DEL can enhance the performance of GNNs on diverse datasets, which is a notable strength.\n\n3)  The paper provides clear explanations of the methodology, implementation details, and experimental setup."
                },
                "weaknesses": {
                    "value": "- Lack of Theoretical Analysis: The paper could benefit from a more in-depth theoretical analysis of DEL. While the empirical results are promising, a theoretical framework that explains why DEL works and under what conditions would provide stronger support for the proposed approach.\n\n- Dimensionality Exploration: The exploration of DEL in high-dimensional spaces is a valuable experiment. However, the paper would benefit from a more in-depth discussion of the implications of these findings. \n\n- Sampling Layout Numbers: While the study addresses the impact of the number of sampled layouts on the performance, the explanations and discussions in this section are somewhat brief. More detailed insights into how the number of layouts affects the performance and potential trade-offs would enhance the paper's depth.\n\n- Computational Complexity: The paper discusses the computational complexity of DEL, but there is room for a more comprehensive discussion, particularly concerning scalability. Discussing the implications of computational complexity in real-world, large-scale applications is crucial."
                },
                "questions": {
                    "value": "why DEL works and under what conditions would provide stronger support for the proposed approach ?\n\nWhy does DEL perform better in lower dimensions, and what does this mean for practical applications?\n\nWhat are the core theoretical principles behind DEL's effectiveness in graph representation learning?\n\nCould you explain the trade-offs in choosing the number of sampled layouts for DEL, and are there practical constraints to consider?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission595/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699072883682,
            "cdate": 1699072883682,
            "tmdate": 1699635986770,
            "mdate": 1699635986770,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Mlzg86NRwL",
                "forum": "N0isTh3rml",
                "replyto": "aGpQ3nwwCB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission595/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission595/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your support and constructive comments.\n# Response to the weaknesses\n* Theoretical Analysis: Layout algorithms are seldom explored in graph representation learning, and proposing a comprehensive theoretical framework falls somewhat beyond the scope of this article. Nevertheless, DEL holds the potential to connect with certain theoretical frameworks. For instance, in Figure 7 on page 18, it is observed that DEL readily recognizes edges connecting different connected components. This observation could potentially be linked to spectral theory, as such edges often exert a significant influence on spectral distribution [1]. We will investigate more on the theoretical aspect in our future work.\n\n* Sampling Layout Numbers: The determination of the number of sampling layouts is contingent upon the characteristics of specific downstream tasks and datasets. In practical terms, ensuring an adequate number of sampling layouts is essential for the model to capture the potential energy distribution of the graph accurately. However, an excessively high number of layout samples can result in information redundancy and low efficiency, thereby impacting the performance of its practical use.  \n\n* Computational Complexity: In real-world applications, graph sizes for graph classification tasks typically do not exceed those of the D\\&D dataset. If the graph size is stable, the preprocessing overhead of DEL is linearly correlated with the number of graphs. The reported preprocessing times in the paper are based on the utilization of either 1 or 8 CPU threads; employing additional threads would further reduce preprocessing time.  Furthermore, for more efficient handling of node classification tasks of large-scale graphs, we can also combine DEL with some more efficient large-scale graph layout algorithms which capable of accommodating millions of nodes [3].\n\n* Regarding dimensionality exploration, Please refer to the 2nd response to the questions part.\n\n# Response to the questions\nReviewer 8jRK\u2019s question can be summarized as how DEL theoretically enhances graph representation learning and how to select important hyperparameters of DEL such as the number of samples and the dimension of the graph layout space. \n\n* DEL acquires global information, not attainable by MPNN through the sampling of the layouts based on potential energy distribution. This unique capability positions DEL as a potent enhancer of graph representation learning. In Appendix D, we present some initial analysis, reserving the construction of a more solid and comprehensive theoretical framework for future research.\n\n* Though there is not a ready theory interpreting why 2D and 3D layouts are sufficient to deliver good performance, we can regard graph spectral theory as an analogous counterpart. In graph spectral theory, the majority of Laplacian eigenvalues for a graph are very close to 0, while minor eigenvalues far from 0 are sufficient to depict the overall structure of the graph. This surprisingly aligns with the observation that 2D/3D layouts are adequate for capturing the main structure of the entire graph. Therefore, lacking specific prior knowledge, we prioritize performing DEL in a low-dimensional space. Simultaneously, DEL sampling multiple layouts in a low-dimensional space can further assist DEL in learning a more accurate energy distribution of the graph, leading to performance improvement. However, these are initial analyses, and more solid and comprehensive theoretical analyses will be left to future work.\n\n[1] Spielman, Daniel. \"Spectral graph theory.\" Combinatorial scientific computing 18 (2012): 18.\n\n[2] Lin, Lu, Jinghui Chen, and Hongning Wang. \"Spectral augmentation for self-supervised learning on graphs.\" ICLR 2023.\n\n[3] Zhu, Minfeng, et al. \"DRGraph: An efficient graph layout algorithm for large-scale graphs by dimensionality reduction.\" IEEE Transactions on Visualization and Computer Graphics 27.2 (2020): 1666-1676."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission595/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700468483465,
                "cdate": 1700468483465,
                "tmdate": 1700468483465,
                "mdate": 1700468483465,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "f8xGbWMhXf",
            "forum": "N0isTh3rml",
            "replyto": "N0isTh3rml",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission595/Reviewer_YS2Q"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission595/Reviewer_YS2Q"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a new GNN architecture where the main idea is first to sample a graph layout and then use this layout for a standard message-passing GNN. The first step is done by sampling according to a Boltzmann distribution (this is not learned and done before the training of the GNN). It can be combined with any GNN architecture afterwards."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The idea of doing first a preprocessing on the graph in order to get a new layout is interesting as it can be coupled with any GNN architectures for downstream tasks."
                },
                "weaknesses": {
                    "value": "The preprocessing step is not clearly defined in the paper. The authors use notions like Boltzmann distribution and Langevin dynamics, but if you look at the code, they only use a preprocessing using the Python library `networkx`. Algorithm 1 is unclear as it never uses the graph structure. \n\nThe evaluation is very weak because the datasets used (MUTAG, IMDB...) have been used extensively and are known to be very poor in order to compare the power of various architectures. Indeed, the differences between the various GNNs are very small."
                },
                "questions": {
                    "value": "Please do not use complicated notions like Boltzmann distribution or Langevin dynamics unless these are really required. It is OK to use 'networkx` algorithms but then say it explicitly in the paper and remove the unnecessary parts. If I am making a mistake, please give me the lines of your code where your Algorithm 1 and Algorithm 2 are implemented.\n\nFor proper benchmarking of GNNs, you can use: Dwivedi, Vijay Prakash, et al. \"Benchmarking graph neural networks.\" arXiv preprint arXiv:2003.00982 (2020). or more recent datasets."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission595/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699288790362,
            "cdate": 1699288790362,
            "tmdate": 1699635986697,
            "mdate": 1699635986697,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "seDWZWJ5jL",
                "forum": "N0isTh3rml",
                "replyto": "f8xGbWMhXf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission595/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission595/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your efforts in proposing these comments and questions. \n# Response to the weaknesses\n* We have provided related analysis and discussion about noise injection in our initial submission. \nWe clearly define the preprocessing in Section 4.1 of the article, i.e., we use the standard graph layout algorithm as preprocessing, but we still provide the option to add noise in the code, e.g., you can find the Fruchterman Reingold algorithm with a noisy version in lines 44-61 and 248-308 in the utils.py implementation. In Section 4.1 we discussed that adding noise has little effect on the distribution of the sampled layouts as well as the downstream performance (shown in Table 9 of Appendix F at Page 19). Thus, to reduce the extra computational burden caused by noise, we turn off the noise injection in our implementation. As suggested by the reviewer YS2Q, we have reorganized our submission and discussed it in Appendix G to make this point clear. \n| ogbg-molhiv               | Valid AUROC           | Test AUROC            |\n|---------------------------|-----------------------|-----------------------|\n| GAT                       | 77.67 $\\pm$ 2.78      | 74.45 $\\pm$ 1.53      |\n| GAT (DEL-F)                | **78.99 $\\pm$ 1.69** | **75.27 $\\pm$ 1.32** |\n| Graph transformer         | 78.20 $\\pm$ 1.18      | 76.51 $\\pm$ 0.93      |\n| Graph transformer (DEL-F)  | **80.12 $\\pm$ 1.39** | **76.86 $\\pm$ 0.95** |\n| GPS                       | 80.08 $\\pm$ 1.39      | 76.71 $\\pm$ 1.12      |\n| GPS (DEL-F)                | **82.57 $\\pm$ 0.95** | **76.93 $\\pm$ 1.00** |\n\n* Algorithm 1 must consider the graph's topology, as the elastic force computed using Hooke's law applies only to nodes connected by edges. In response to your concerns, we have provided additional clarification in Algorithm 1, with the modifications highlighted in blue.\n\n* In response to your concern regarding the evaluation's limitations, we appreciate your feedback and have made enhancements. The evaluation now incorporates results from the large-scale ogbg-molhiv dataset [1], presented in Table 10 on Page 20. We observe consistent performance enhancement on this dataset, aligning with existing results. This additional evaluation would provide more comprehensive and accurate support.\n\n# Response to the questions\n* Please also refer to the 1st response to the weakness part.\nIn practice, when dealing with simple topologies such as molecules and social networks from real-world applications, the layout algorithms tend to produce wide optima  [2], which are very close to the layouts sampled using noise-injected versions like Langevin dynamics (see Appendix G and Figure 8 in the manuscript).\nThis allows for the use of standard layout algorithms as an efficient surrogate without the need for intricate noise schemes.\nIn Figure 8 of Appendix G, we present new visualization results that confirm the close similarity of layout distributions obtained with or without noise sampling.\n\n[1] Hu, Weihua, et al. \"Open graph benchmark: Datasets for machine learning on graphs.\" Advances in neural information processing systems 33 (2020): 22118-22133.\n\n[2] Fruchterman, Thomas MJ, and Edward M. Reingold. \"Graph drawing by force\u2010directed placement.\" Software: Practice and experience 21.11 (1991): 1129-1164."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission595/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700468089537,
                "cdate": 1700468089537,
                "tmdate": 1700469174683,
                "mdate": 1700469174683,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8kIdOc0ysz",
                "forum": "N0isTh3rml",
                "replyto": "seDWZWJ5jL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission595/Reviewer_YS2Q"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission595/Reviewer_YS2Q"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your response"
                    },
                    "comment": {
                        "value": "It looks like my initial understanding of the paper is correct, and I still think that most of the discussion concerning layout should be simplified as, in the end, simple `networkx` routines are used. Written as is, the paper is very misleading, and its contribution is weak."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission595/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700581606807,
                "cdate": 1700581606807,
                "tmdate": 1700581606807,
                "mdate": 1700581606807,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]