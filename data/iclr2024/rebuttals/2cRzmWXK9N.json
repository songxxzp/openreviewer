[
    {
        "title": "Beyond Reverse KL: Generalizing Direct Preference Optimization with Diverse Divergence Constraints"
    },
    {
        "review": {
            "id": "31H1oER32G",
            "forum": "2cRzmWXK9N",
            "replyto": "2cRzmWXK9N",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3139/Reviewer_sLFR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3139/Reviewer_sLFR"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a general method that utilizes diverse divergence constraints and direct preference optimization to achieve alignment with human preferences. This paper shows that the reward can be reparameterized using the policy model and a reference model. Namely, it is possible to solve the RLHF problem via a supervised learning approach under a broad class of divergence constraints. The divergence regularization can be used to trade-off between accuracy and diversity. The experiment results show that the proposed method is more stable and efficient in optimization than the RL method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "> This paper investigates the AI alignment performance under the constraint of many popular divergences and analyzes the effectiveness of these regularizations. This introduces greater flexibility for the fine-tuning process using human preference.\n\n> This paper establishes the relationship between the reward function and the optimal policy. It proves that the language model can be optimized using the policy model and the reference model with divergence constraints. This method is a supervised learning approach and sounds more stable than RLHF.\n\n> The empirical results show that the balance between the alignment accuracy and the diversity can be adjusted using different divergence regularizations. In addition, the proposed framework shows comparable (and, in some cases, better) performance and greater divergence efficiency than PPO-based methods."
                },
                "weaknesses": {
                    "value": "> The novelty of the method is limited. DPO uses the reverse KL regularization constraint to achieve the mapping between the reward model and the optimal policy. The proposed method only extends the reverse KL regularization into a broad class of commonly used divergences.\n\n> There is still room to innovate on the theoretical findings. The derivations in the method section take up much space. However, they are mainly from DPO - the difference is that the authors introduce the Lagrange multiplier for the constrained (but in practice, we do not need to deal with these constraints because they are naturally satisfied) and use Df rather than DKL."
                },
                "questions": {
                    "value": "> What are the assumptions and limitations of f-DPO, and how do they affect its applicability?\n\n> What is the advantage of using different divergences compared to tuning DPO\u2019s beta hyperparameter to balance the accuracy and diversity?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3139/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698636804037,
            "cdate": 1698636804037,
            "tmdate": 1699636260911,
            "mdate": 1699636260911,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "9VKhoybpxe",
            "forum": "2cRzmWXK9N",
            "replyto": "2cRzmWXK9N",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3139/Reviewer_AH62"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3139/Reviewer_AH62"
            ],
            "content": {
                "summary": {
                    "value": "This paper takes a look at recent RLHF works that use policy gradient methods in conjunction with divergence-to-the-initial-model regularizations, and through some derivations show that they all belong in a similar family of methods. These equivalences lead the authors to propose a generalized form of DPO, a purely supervised objective, which is derived by combining the general f-divergence based family of methods with a Bradley-Terry type reward.\n\nThe authors show that using the proposed method on RLHF datasets allows for faster training on the reward and better calibrated models than PPO counterparts, and show that the choice of f-divergence induces interesting trade offs."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is well written, and in my book is good science: generalizing from several methods, taking care to do proper evaluation of the performance _and_ the behaviors that may lead to the performance, as well as an evaluation of the different trade offs offered by the method.\n\nI'm not sure I'll be the best judge of impact but this seems like a set of valuable insights into the space of RLHF methods."
                },
                "weaknesses": {
                    "value": "I don't have much bad to say about the paper, although I am wondering if there could not be a more complete set of baselines (see below)."
                },
                "questions": {
                    "value": "It feels like there's a missing baseline (although this may be a criticism of RLHF more than of this specific work), which is an off-policy offline RL baseline, such as CQL [1] and recent variants. As the authors correctly point out, training a model with (on-policy) policy gradient methods such as PPO require rollouts, whereas purely supervised methods such as that proposed in the paper do not. This creates an evaluation mismatch though. It may be the the real difference between f-DPO and PPO is not the different use of divergence per-se but rather the pure use of offline data. As far as I can tell the present work does not account for that.\n\n[1] Conservative Q-Learning for Offline Reinforcement Learning, Aviral Kumar, Aurick Zhou, George Tucker, Sergey Levine, 2020"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3139/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698800196339,
            "cdate": 1698800196339,
            "tmdate": 1699636260835,
            "mdate": 1699636260835,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "Up2XxUX5uK",
            "forum": "2cRzmWXK9N",
            "replyto": "2cRzmWXK9N",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3139/Reviewer_GUwk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3139/Reviewer_GUwk"
            ],
            "content": {
                "summary": {
                    "value": "This paper generalises the direct preference optimisation (DPO) framework to a broad class of divergences, i.e., f-divergence. With the similar ideas of DPO, f-DPO uses the reward model, BT model, and the KKT conditions to deal with the constraints. Experiments demonstrates that the proposed f-DPO outperforms the two PPO-based baselines."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "DPO is an elegant framework for RLHF. This paper is an important extension of DPO to a more general class. The writing of the paper is clear and the experiments are sufficient."
                },
                "weaknesses": {
                    "value": "I think one issue is that how to select the f-divergence. Given that there are many options of the divergence, how to select the best one. Other questions can be found in the questions section."
                },
                "questions": {
                    "value": "There are several questions I want the authors to address during the rebuttal:\n1. how to select the f-divergence? The proposed framework is general, that is great. However, when applying to the real scenarios, how to select the best f-divergence? From the experiments, the FKL and RKL can be the options. But more discussion can be included. Is it possible to optimise the f-divergence simultaneously with the policy?\n\n2. One observation is that RKL will harm the diversity and FKL can do better on the diversity. Is there any explanation about this? I think instead of showing the experiments, a more empirical or theoretical analysis would be helpful for us to understand the necessities of f-divergence. \n\nI generally think this is a good paper. So not many questions are raised."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3139/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698997117798,
            "cdate": 1698997117798,
            "tmdate": 1699636260741,
            "mdate": 1699636260741,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "Rgz3tVA857",
            "forum": "2cRzmWXK9N",
            "replyto": "2cRzmWXK9N",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3139/Reviewer_Lpy5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3139/Reviewer_Lpy5"
            ],
            "content": {
                "summary": {
                    "value": "This paper extends the Direct Preference Optimization (DPO) algorithm used for training LLMs with RLHF to handle different $f$-divergences between distributions, which have differing tradeoffs between alignment and diversity. The current DPO approach optimizes the reverse KL divergence between the learned and reference model, however the reverse KL is widely known to induce mode-collapsing behavior, which explains the loss of diversity with increased alignment. This paper gives a closed form solution for a variety of $f$ divergence, including reverse KL, forward KL, Jenson-Shannon and different $\\alpha$-divergences. Each of these have different tradeoffs between mode-seeking and mode-covering behaviors (reverse KL is the most mode-seeking, forward KL mode-covering). They evaluate language models fine-tuned with these different divergences across 3 datasets (IMDB, Anthropic HH and MT-Bench). They find that training with the different divergences indeed induce different tradeoffs between reward maximization and diverse generations. The existing DPO algorithm, which uses reverse KL, maximizes the reward but is the least diverse; forward KL is the most diverse but lowest reward, and different $\\alpha$ divergences are in the middle. The experiments also show that for the same divergence, $f$-DPO consistently outperforms the PPO analog in terms of generation quality as evaluated by GPT4. Finally, a nice experiment which is in the appendix shows that GPT4 also judges DPO with some diversity (like using the JSD) over DPO with reverse KL, which is consistent with the intuition that somewhat diverse text is more realistic. \n\nOverall, I think this is a nice paper that should be accepted. It proposes a clean, theoretically grounded and computationally efficient solution for handling the tradeoff between alignment and diversity for finetuning LLMs with RL, which I think will be of broad interest. The paper is also very nicely written. While the experiments are on a somewhat small scale in terms of LLM and datasets size, I think they are sufficient to confirm the behavior predicted by the theory, namely that optimizing the different divergences induces different tradeoffs between alignment and diversity, and these trends are consistent across the different experiments. This is sufficient for a first paper, and I expect that the community will extend these evaluations to larger models and datasets."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "- Simple, effective and potentially very useful method for inducing differing tradeoffs between alignment and diversity when finetuning LLM with preference models. I think this is timely and will be of broad interest to the community working on fine-tuning LLMs with RL. \n- Well-written paper\n- Nicely designed experiment section"
                },
                "weaknesses": {
                    "value": "- Experiments are on a somewhat small scale with older LLMs - ideally it would have been nice to see experiments with more modern LLMs like LLaMA-7B. However, I think the method is principled enough and the trends in the experiments robust enough that I am optimistic the results will transfer to larger models."
                },
                "questions": {
                    "value": "My main suggestion for improving the paper would be evaluating the approach on more modern LLMs like LLaMA. I understand these experiments can be computationally expensive though, and I don't think they are essential for the paper to be accepted."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3139/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699236887177,
            "cdate": 1699236887177,
            "tmdate": 1699636260662,
            "mdate": 1699636260662,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]