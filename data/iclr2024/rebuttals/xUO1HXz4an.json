[
    {
        "title": "Negative Label Guided OOD Detection with Pretrained Vision-Language Models"
    },
    {
        "review": {
            "id": "eSDr0YK1j1",
            "forum": "xUO1HXz4an",
            "replyto": "xUO1HXz4an",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5140/Reviewer_dUt1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5140/Reviewer_dUt1"
            ],
            "content": {
                "summary": {
                    "value": "The field of out-of-distribution (OOD) detection seeks to recognize samples originating from unknown classes, thereby ensuring the trustworthiness of models when confronted with unanticipated inputs. While there exists a vast body of research delving into OOD detection within the visual modality, vision-language models (VLMs) stand out by synergizing both textual and visual data for a range of multi-modal tasks. However, there's a noticeable gap, with only a few OOD detection techniques capitalizing on the textual modality. In this study, the authors introduce a groundbreaking post hoc OOD detection technique, termed NegLabel, which harnesses a plethora of negative labels sourced from expansive corpus databases. They meticulously craft an innovative scheme, wherein the OOD score seamlessly collaborates with these negative labels. A detailed theoretical scrutiny aids in unraveling the intricate workings of these negative labels. Rigorous experimentation underscores that their NegLabel approach not only sets new standards in OOD detection benchmarks but also exhibits a laudable adaptability across diverse VLM architectures. Moreover, the NegLabel technique showcases exemplary resilience when faced with various domain shifts."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1 The research topic is very novel and important. There are a few papers focusing on this field. After reviewing this paper and relevant literature, it can be found that this paper moves a solid step to detect OOD samples when VLMs are available. \n\n2 This paper identifies an important issue of relevant literature: how to select the negative words to improve the OOD detection power. Correspondingly, this paper proposes a novel method to address this important issue, which is a significant contribution in my view. The method's effectiveness is also verified by the extensive experiment.\n\n3 The claims of this paper obtains strong support via extensive experiments that are based on various zero-shot OOD detection benchmarks and multiple VLM architectures.\n\n4 The theoretical understanding starts from the data modelling, which is an elegant way to build up an understanding from the scratch. I am glad to see how the score is built up via a well-motivated data modelling method. The use of the probability tools is very interesting in the field, as we can directly see the distribution."
                },
                "weaknesses": {
                    "value": "1 In Eq. (5), the cosine similarity is selected. Although the reason is given in the paper, it is still interesting to see if we can have other choices.\n\n2 I enjoy reading how you move from basic data modelling to a more realistic data modelling in B.3. However, it would be better to point out why the contents in B.3 is more general, which can increase the interests of readers for understanding the proposed score.\n\n3 In Table 2, all methods can obtain good results (AUROC>90% or FPR95<10% in the most cases). I actually do not see the necessarity to put that table in the main content. It would be better to move some interesting parts in Appendix to the main content.\n\n4 What will happen if we select all different words from the ID classes?"
                },
                "questions": {
                    "value": "1 In Table 2, all methods can obtain good results (AUROC>90% or FPR95<10% in the most cases). I actually do not see the necessarity to put that table in the main content. Can we move it to appendix?\n\n2 What will happen if we select all different words from the ID classes?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5140/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5140/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5140/Reviewer_dUt1"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5140/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698613443578,
            "cdate": 1698613443578,
            "tmdate": 1700621351039,
            "mdate": 1700621351039,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "T97jXWkNpj",
                "forum": "xUO1HXz4an",
                "replyto": "eSDr0YK1j1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5140/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5140/Authors"
                ],
                "content": {
                    "comment": {
                        "value": ">**W1:** In Eq. (5), the cosine similarity is selected. Although the reason is given in the paper, it is still interesting to see if we can have other choices.\n\n**A1:** Thanks for your suggestions! We tried using L1 distance and KL divergence to measure the similarity between images and negative labels, and the results are shown below. As the CLIP-like VLM models are trained under cosine similarity supervision, the features are naturally measured in the cosine space. \n\nObserving the experimental results, it can be seen that when using L1 distance and KL divergence as metrics, there is a significant drop in performance on SUN, Places, and Textures datasets, while the impact on iNaturalist dataset is relatively small. This is because our method (based on cosine similarity) achieves a 99.49 AUROC on iNaturalist, almost completely distinguishing between ID and OOD data. Therefore, even when using metrics such as L1 and KL divergence that are not suitable for cosine space, there is still a significant difference between ID samples and OOD samples from iNaturalist. For more discussions on iNaturalist, please refer to our response to reviewer 9gus in Reply **A1**.\n\n|       Choice      | iNaturalist |          |    SUN    |           |   Places  |           |  Textures |           |  Average  |           |\n|:-----------------:|:-----------:|:--------:|:---------:|:---------:|:---------:|:---------:|:---------:|:---------:|:---------:|:---------:|\n|                   |    AUROC\u2191   |  FPR95\u2193  |   AUROC\u2191  |   FPR95\u2193  |   AUROC\u2191  |   FPR95\u2193  |   AUROC\u2191  |   FPR95\u2193  |   AUROC\u2191  |   FPR95\u2193  |\n| Cosine (baseline) |  **99.49**  | **1.91** | **95.49** | **20.53** | **91.64** | **35.59** | **90.22** | **43.56** | **94.21** | **25.40** |\n|   KL Divergence   |    98.19    |   9.19   |   78.75   |   85.47   |   68.61   |   90.47   |   73.84   |   90.07   |   79.85   |   68.80   |\n|    L1 Distance    |    97.34    |   11.95  |   80.28   |   79.39   |   69.29   |   89.14   |   66.57   |   86.51   |   78.37   |   66.75   |\n\n>**W2:** I enjoy reading how you move from basic data modelling to a more realistic data modelling in B.3. However, it would be better to point out why the contents in B.3 is more general, which can increase the interests of readers for understanding the proposed score.\n\n**A2:** Thanks for your comments! In the theoretical analysis of the main text, to simplify the theoretical model and improve the readability of the article, we assume that the probability of a sample $x$ matching with each negative label is $p$. Therefore, for $M$ negative labels, we consider the number of matches between an image $x$ and them to follow a binomial distribution $B(M, p)$. However, due to the large number of negative labels, covering a wide semantic space, their affinity with the sample $x$ is variable.\n\nSpecifically, in Appendix B.3, we assume that the probability of an image $x$ matching with a negative label is $p_i$, where different negative labels have different probabilities. This better reflects the real-world scenario, and thus we consider the number of matches between an image $x$ and them to follow a Poisson binomial distribution.\n\nWe will provide more details in the main text to help readers better understand the general case in B.3.\n\n>**W3:** In Table 2, all methods can obtain good results (AUROC>90% or FPR95<10% in the most cases). I actually do not see the necessarity to put that table in the main content. It would be better to move some interesting parts in Appendix to the main content.\n\n\n**A3:** Thanks for your suggestions! We will reorganize our paper in the updated version."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5140/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700405594018,
                "cdate": 1700405594018,
                "tmdate": 1700405594018,
                "mdate": 1700405594018,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PRtcPWlxS2",
                "forum": "xUO1HXz4an",
                "replyto": "zyiTNsirQR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5140/Reviewer_dUt1"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5140/Reviewer_dUt1"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the detailed response and clarification of the authors. My concerns have been addressed. Overall, I think it is a good paper and I will raise my score to support this paper."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5140/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700621334772,
                "cdate": 1700621334772,
                "tmdate": 1700621334772,
                "mdate": 1700621334772,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "KaPTt5Iouf",
            "forum": "xUO1HXz4an",
            "replyto": "xUO1HXz4an",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5140/Reviewer_9gus"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5140/Reviewer_9gus"
            ],
            "content": {
                "summary": {
                    "value": "In the traditional OOD detection, the text information of labels will be abandoned, which actually loses some information regarding labels. With the assist of VLMs, this kind of information might be useful for OOD detection. The authors present a study aligning with this interesting and new research direction: OOD detection with VLMs. The existing methods are quite on the early stage, and this paper challenges these methods and proposes a new OOD detection method consisting of a negative label selection method and a new score function. Experiments are solid and form a solid contribution to OOD detection with VLMs."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "+ Experiments are solid and cover many aspects, which can address many in-the-mind issues\nautomatically. It is enjoyable to the whole experimental section, which also contains some interesting insights directly from the experiments conducted in this paper.\n\n+ The idea to design the new score function is straightforward after demonstrating an intuitive motivation. The motivation of this paper is supported by some evidence instead of \"wording\", which appreciated. \n\n+ The presentation is generally good. The flow is great to follow, and necessary analysis is easy to follow as well."
                },
                "weaknesses": {
                    "value": "- OOD detection methods have different performance on different datasets. On some datasets, the performance is extremely good. Can we see the difference among these datasets? Why can OOD detection be easily addressed on some datasets?\n\n- Especially for results based on CLIP-B/16 with various ID datasets, the performance is too high. More explanation is need. \n\n- Figures might be better to illustrate the final performance in Section 4. \n\n- What is the relationship between Bernoulli distribution and binomial distribution? This is a well-known result in statistics?\n\n- Why is B.3 more general than things demonstrated in main context? \n\n- Please keep the naming strategy consistent in Algorithm 1 and other algorithms."
                },
                "questions": {
                    "value": "In general, this paper addresses a significant problem and makes a solid contribution to this field. However, please answer questions listed in the Weakness:\n\n- OOD detection methods have different performance on different datasets. On some datasets, the performance is extremely good for all methods. Can we see the difference among these datasets? Why can OOD detection be easily addressed on some datasets?\n\n- What is the relationship between Bernoulli distribution and binomial distribution? This is a well-known result in statistics?\n\n- Why is B.3 more general than things demonstrated in main context?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5140/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698629618596,
            "cdate": 1698629618596,
            "tmdate": 1699636507478,
            "mdate": 1699636507478,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ellNJuSORM",
                "forum": "xUO1HXz4an",
                "replyto": "KaPTt5Iouf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5140/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5140/Authors"
                ],
                "content": {
                    "comment": {
                        "value": ">**W1:** OOD detection methods have different performance on different datasets. On some datasets, the performance is extremely good. Can we see the difference among these datasets? Why can OOD detection be easily addressed on some datasets?\n\n\n**A1:** Thanks for your comments! We think that the performance differences mainly stem from the characteristics of the OOD datasets themselves. In  Figure 4 and Figure 5 of the Appendix, we provide a few sample images from the ID and OOD datasets for reference. The ID dataset ImageNet contains a large number of categories related to animals and food, while iNaturalist consists of images of natural plants. SUN and Places datasets contain images of natural landscapes. Therefore, compared to other OOD datasets, iNaturalist has the largest differences in category terms compared to ImageNet, such as animals vs. plants. Therefore, introducing negative labels can significantly improve the OOD detection performance. On the other hand, SUN and Places datasets often contain multiple elements from the natural world beyond their annotated ground truth, making them more prone to confusion with the ID data.\n\nFurthermore, recent research trends have also shown that achieving high performance on iNaturalist is quite common. For example, ASH [1] and NNGuide [2] both achieve over 97% AUROC on iNaturalist, while GEN [3] achieves 99.13% AUROC.\n\n>**W2:** Especially for results based on CLIP-B/16 with various ID datasets, the performance is too high. More explanation is need.\n\n**A2:** Thanks for your suggestions! We follow MCM to conduct experiments on the large-scale benchmark in Table 1 and small-scale benchmarks in Table 2. The performance of MCM in Table 2 is very high and our NegLabel outperforms MCM. Our opinion about the high performance is consistent to MCM, i.e., the small-scale ID datasets have less challenges for VLMs.\n\n>**W3:** Figures might be better to illustrate the final performance in Section 4.\n\n**A3:** Thanks for your suggestions! We will replace some tables with figures in Section 4 in the updated paper. \n\n>**W4:** What is the relationship between Bernoulli distribution and binomial distribution? This is a well-known result in statistics?\n\n**A4:** The Bernoulli distribution is a special case of the binomial distribution, where n = 1. Symbolically, X ~ B(1, p) has the same meaning as X ~ Bernoulli(p). Conversely, any binomial distribution, B(n, p), is the distribution of the sum of n independent Bernoulli trials, Bernoulli(p), each with the same probability p.[4]\n\n\n\n>**W5:** Why is B.3 more general than things demonstrated in main context?\n\n**A5:** Thanks for your comments! In the theoretical analysis of the main text, to simplify the theoretical model and improve the readability of the article, we assume that the probability of a sample $x$ matching with each negative label is $p$. Therefore, for $M$ negative labels, we consider the number of matches between an image $x$ and them to follow a binomial distribution $B(M, p)$. However, due to the large number of negative labels, covering a wide semantic space, their affinity with the sample $x$ is variable.\n\nSpecifically, in Appendix B.3, we assume that the probability of an image $x$ matching with a negative label is $p_i$, where different negative labels have different probabilities. This better reflects the real-world scenario, and thus we consider the number of matches between an image $x$ and them to follow a Poisson binomial distribution.\n\nWe will provide more details in the main text to help readers better understand the general case in B.3.\n\n>**W6:** Please keep the naming strategy consistent in Algorithm 1 and other algorithms.\n\n**A6:** Thanks for your suggestions! We will revise the algorithms to keep the naming strategy consistent in the updated paper.\n\n**Reference**\n\n[1] Djurisic, Andrija, et al. \"Extremely Simple Activation Shaping for Out-of-Distribution Detection.\"\u00a0The Eleventh International Conference on Learning Representations. 2022.\n\n[2] Park, Jaewoo, Yoon Gyo Jung, and Andrew Beng Jin Teoh. \"Nearest Neighbor Guidance for Out-of-Distribution Detection.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023.\n\n[3] Liu, Xixi, Yaroslava Lochman, and Christopher Zach. \"GEN: Pushing the Limits of Softmax-Based Out-of-Distribution Detection.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\n\n[4] https://en.wikipedia.org/wiki/Binomial_distribution#Bernoulli_distribution"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5140/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700405401623,
                "cdate": 1700405401623,
                "tmdate": 1700405401623,
                "mdate": 1700405401623,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PmbEm6tRPQ",
                "forum": "xUO1HXz4an",
                "replyto": "ellNJuSORM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5140/Reviewer_9gus"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5140/Reviewer_9gus"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the reponse."
                    },
                    "comment": {
                        "value": "Thanks for the detailed response from the authors. My main concerns have been addressed and will keep my initial score."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5140/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700662546939,
                "cdate": 1700662546939,
                "tmdate": 1700662546939,
                "mdate": 1700662546939,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "M36BQvuzTV",
            "forum": "xUO1HXz4an",
            "replyto": "xUO1HXz4an",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5140/Reviewer_JPmP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5140/Reviewer_JPmP"
            ],
            "content": {
                "summary": {
                    "value": "This paper is a pioneer work to study the OOD detection problem under VLMs. Compared to state-of-the-art papers in this frontier, this paper addresses several issues that SOTA methods do not cover. These issues are justified well in this paper, and the corresponding explanation is reasonable and solid. Then, a OOD-word selection method and a corresponding score function is proposed to address the observed issues. The proposed method makes sense and works well in the extensive experiments."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1 As a pioneer study in this field, I find that this paper is easy to follow and demonstrates issues of SOTA methods very clearly. Thus, in my point of view, this paper has a great potential to motivate more relevant work in this important field: OOD detection with VLMs/Zero-shot OOD detection.\n\n2 I am convinced that the proposed work has a solid contribution by the extensive experiments conducted by the authors. The experiments cover many scenarios, which is a solid evidence that the proposed method has its own contribution to this field.\n\n3 This paper also benefits from the theoretical understanding of the proposed score function. The analysis is easy to follow and looks totally new to me."
                },
                "weaknesses": {
                    "value": "1 Figure 1 is clear and important. However, it makes me think about one question: in a statistical view, what should the right subfigure be?\n\n2 In Algorithm 1, the comments are too long. It would be better to say the key functionality of this part.\n\n3 Why does grouping strategy appear in Section 3.2 instead of 3.1? It seems that Grouping Strategy is a more advanced way to pre-process negative labels?\n\n4 Can the authors explain why is the case considered in B.3 more general? More explanation can make readers know your analysis better."
                },
                "questions": {
                    "value": "1 Figure 1 is clear and important. However, it makes me think about one question: in a statistical view, what should the right subfigure be?\n\n2 Why does grouping strategy appear in Section 3.2 instead of 3.1? It seems that Grouping Strategy is a more advanced way to pre-process negative labels?\n\n3 Can the authors explain why is the case considered in B.3 more general? More explanation can make readers know your analysis better."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5140/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698630664090,
            "cdate": 1698630664090,
            "tmdate": 1699636507393,
            "mdate": 1699636507393,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0zGFulkjGy",
                "forum": "xUO1HXz4an",
                "replyto": "M36BQvuzTV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5140/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5140/Authors"
                ],
                "content": {
                    "comment": {
                        "value": ">**W1:** Figure 1 is clear and important. However, it makes me think about one question: in a statistical view, what should the right subfigure be?\n\n\n**A1:** Thanks for the comments! The actual subgraph would be very difficult to visualize, as the x-axis would contain 1,000 (ID labels) + 10,000 (negative labels) elements. We have provided the visualization results of the top 5 ID labels and negative labels similarities in Figures 4 and 5 in the appendix.\n\n\n>**W2:** In Algorithm 1, the comments are too long. It would be better to say the key functionality of this part.\n\n**A2:** Thanks for your suggestions! We will revise the comments in Algorithm 1.\n\n>**W3:** Why does grouping strategy appear in Section 3.2 instead of 3.1? It seems that Grouping Strategy is a more advanced way to pre-process negative labels?\n\n**A3:** Thanks for this question. Here we detail our proposed pipeline of Negative-Label-based OOD detection: corpus -> NegMining -> Grouping -> NegLabel Score. Specifically, our method requires first selecting appropriate negative labels through NegMining (as shown in Section 3.1), and then designing a NegLabel score using these negative labels (as shown in Section 3.2 part 1). The Grouping strategy is an optimization of the NegLabel score. By grouping the selected negative labels and calculating the NegLabel score for each group, the average of all groups is taken as the final OOD score. Therefore, the Grouping Strategy is not a more advanced way to preprocess negative labels. It does not involve the preprocessing process of negative labels. It is suitable to be placed in Section 3.2 part 2 as it optimizes and enhances the OOD score.\n\nTo better understand the grouping strategy, we provide a toy example. Let's assume for an ID image, there is a relatively high response in the ID labels, but coincidentally, there is also a relatively high response in the negative labels, with both responses being of comparable magnitude. Without grouping, after softmax function, the probabilities for ID and negative labels might both be around 0.5, resulting in a score of 0.5.\n\nHowever, if we divide the negative labels into 10 groups, this misclassification would be included in one of the groups at most. The score for this group would be 0.5, while the rest of the groups would be close to 1.0. Thus, the final score would be calculated as 1 * 0.9 + 0.5 * 0.1 = 0.95. This illustrates how the Grouping Strategy helps mitigate the impact of such misclassifications.\n\n>**W4:** Can the authors explain why is the case considered in B.3 more general? More explanation can make readers know your analysis better.\n\n**A4:** Thanks for your comments! In the theoretical analysis of the main text, to simplify the theoretical model and improve the readability of the article, we assume that the probability of a sample $x$ matching with each negative label is $p$. Therefore, for $M$ negative labels, we consider the number of matches between an image $x$ and them to follow a binomial distribution $B(M, p)$. However, due to the large number of negative labels, covering a wide semantic space, their affinity with the sample $x$ is variable.\n\nSpecifically, in Appendix B.3, we assume that the probability of an image $x$ matching with a negative label is $p_i$, where different negative labels have different probabilities. This better reflects the real-world scenario, and thus we consider the number of matches between an image $x$ and them to follow a Poisson binomial distribution.\n\nWe will provide more details in the main text to help readers better understand the general case in B.3."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5140/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700405317093,
                "cdate": 1700405317093,
                "tmdate": 1700405317093,
                "mdate": 1700405317093,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "xRKHi28pC7",
            "forum": "xUO1HXz4an",
            "replyto": "xUO1HXz4an",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5140/Reviewer_RPtB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5140/Reviewer_RPtB"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a novel method for zero-shot out-of-distribution (OOD) detection, leveraging the capabilities of vision-language models (VLMs). The primary objective is to enhance OOD detection by incorporating textual information. NegLabel introduces a large number of negative labels, extending the label space to distinguish OOD samples more effectively. The method uses a novel scheme for the OOD score, taking into account affinities between images and both ID labels and negative labels. The paper provides a theoretical analysis, justifying the use of negative labels."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- **Well-written and technically sound paper** The paper is well-structured and its concepts and contributions are clearly presented.\n\n- **Quality illustrations**\n\n- **Extensive experiments** The paper conducts extensive experiments, including OOD detection, hard OOD detection, and robustness to domain shifts. \n\n- **Large and consistent gains compared to several SOTA baselines** The paper's results show substantial improvements over several state-of-the-art (SOTA) baselines. \n\n- **Intuitive justification and theoretical analysis provided** The paper offers an intuitive justification for the proposed method and includes theoretical analysis to explain the mechanism of negative labels.\n\n- **Extensive ablations**"
                },
                "weaknesses": {
                    "value": "- **Assumption about CLIP's latent space** The assumption that ID image and corresponding ID label embeddings are close in CLIP's latent space is actually false. The contrastive learning strategy of CLIP is based on cosine similarity and thus textual and visual embeddings are actually on different manifolds see (Weixin Liang et al. 2022) for more details. Using ID labels as proxies for the ID images may thus produce unexpected results. In my understanding, the negative labels are compared to the images using cosine similarity, and thus having them close to ID textual embeddings still makes sense but the authors should be cautious here.\n\n- **Comparison with related work** The method is close in spirit to CLIPN (Wang et al. 2023) as well as ZOC (Esmaeilpour et al 2022). The former fine-tunes CLIP to incorporate negative prompts to assess the probability of a concept not being present in the image. An extensive comparison with CLIPN would be overkill but some discussion about it would be welcome.  The second also performs zero-shot OOD using maximum softmax probability on the labels extended with the object detected in the images. Comparisons with ZOC are presented in the supplementary but would better belong in the main paper IMO as it is one of the few recent zero-shot OOD detection baselines. To be fair this comparison should be provided with the same set-up as in Table 2.\n\n- **Misleading baseline presentation** Even if the MSP, ODIN, or Energy Logit baseline are generally used on models fine-tuned on a downstream task, one could also use these detectors in a zero-shot setting. Classifying them in purely zero-shot can be misleading eg. MCM can be seen as zero-shot MSP on CLIP's output probabilities.\n\n- **Short Related Work** The related work section is relatively short, especially for such a prolific field as OOD detection. I don't think it is really detrimental to the paper but a discussion on the sense of OOD detection for foundation models trained on vast and various data would be welcome. \n\nI am willing to improve my rating depending on the author's rebuttal.\n\n**References** \n\nWang, H., Li, Y., Yao, H., & Li, X. (2023). CLIPN for Zero-Shot OOD Detection: Teaching CLIP to Say No, ICCV 2023.\n\nEsmaeilpour, S., Liu, B., Robertson, E., & Shu, L. (2022). Zero-Shot Out-of-Distribution Detection Based on the Pre-trained Model CLIP. Proceedings of the AAAI Conference on Artificial Intelligence, 36(6), Article 6.\n\nLiang, W., Zhang, Y., Kwon, Y., Yeung, S., & Zou, J. (2022, May 16). Mind the Gap: Understanding the Modality Gap in Multi-modal Contrastive Representation Learning. Advances in Neural Information Processing Systems."
                },
                "questions": {
                    "value": "- I wonder why the authors did not add the simple and strong baseline Mahaloanobis (Lee et al. 2018) /SSD (Sehwag et al. 2022).\n- No ablations are provided on $n_g$. How do the authors set this parameter and what is its impact?\n- Would the hard OOD detection task be better qualified as Open-Set Recognition?\n- Why the comparison with ZOC is performed using ImageNet-200 and not ImageNet-1k as in Table 2? A Homogeneous comparison setup would greatly enhance the paper quality. \n\n**References**\n\nLee, K., Lee, K., Lee, H., & Shin, J. (2018). A Simple Unified Framework for Detecting Out-of-Distribution Samples and Adversarial Attacks. Advances in Neural Information Processing Systems, 31. \n\nSehwag, V., Chiang, M., & Mittal, P. (2022, February 10). SSD: A Unified Framework for Self-Supervised Outlier Detection. International Conference on Learning Representations."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5140/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5140/Reviewer_RPtB",
                        "ICLR.cc/2024/Conference/Submission5140/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5140/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699018938750,
            "cdate": 1699018938750,
            "tmdate": 1700738334976,
            "mdate": 1700738334976,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "SfP4iL6lsh",
                "forum": "xUO1HXz4an",
                "replyto": "xRKHi28pC7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5140/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5140/Authors"
                ],
                "content": {
                    "comment": {
                        "value": ">**W1:** Assumption about CLIP's latent space The assumption that ID image and corresponding ID label embeddings are close in CLIP's latent space is actually false. The contrastive learning strategy of CLIP is based on cosine similarity and thus textual and visual embeddings are actually on different manifolds see (Weixin Liang et al. 2022) for more details. Using ID labels as proxies for the ID images may thus produce unexpected results. In my understanding, the negative labels are compared to the images using cosine similarity, and thus having them close to ID textual embeddings still makes sense but the authors should be cautious here.\n\n**A1:** Thank you for this valuable comment. We apologize for the inappropriate description. As mentioned in the question, images and text belong to different manifolds in the embedding space of CLIP, so they cannot be considered *close*. We will revise this description in the last paragraph of Section 3.1.\n\nBesides, we would reiterate our OOD detection setting based on CLIP-like VLM models. We only work with a pre-trained CLIP-like model and a specific set of class labels or names for a zero-shot classification task. And the ID images are not available under this zero-shot setting. Hence\uff0cusing ID labels as the proxies of ID images for negative label selection is intuitive, and it is experimentally verified to be effective in our method. We believe that not relying on the accessibility of ID images allows our method to be used in a wider range of scenarios and is more consistent with the working conditions of CLIP-like models."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5140/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700405034351,
                "cdate": 1700405034351,
                "tmdate": 1700405034351,
                "mdate": 1700405034351,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XwDNi3tuui",
                "forum": "xUO1HXz4an",
                "replyto": "jhgmj9B7U8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5140/Reviewer_RPtB"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5140/Reviewer_RPtB"
                ],
                "content": {
                    "comment": {
                        "value": "I would like to thank the authors for their extended answers. Most of my concerns have been addressed.\n\nConcerning the first point, I agree that using only OOD concepts is a strength of the method. However, I do not see it as a novelty as it was already performed in MCM or Fort et al, 2021. I am still not completely satisfied with the formulation as in my sense the prompts are not used as proxies of the ID images in the paper (and that is a good thing). It makes sense to search for OOD concepts based on the ID ones **but** it does not make sense to see these OOD concepts as pseudo-OOD images. \n\nMoreover, upon rereading, Fort et al, 2021, I see that the last section is the same as the proposed NegLabel score. It seems that the novelty of NegLabel rather relies on the mining algorithm and the grouping strategy than the OOD scorer itself which is not explicit in the contributions. With this in mind, comparisons with this baseline would have been appreciated."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5140/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700668597895,
                "cdate": 1700668597895,
                "tmdate": 1700668597895,
                "mdate": 1700668597895,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2gTdN6SluW",
                "forum": "xUO1HXz4an",
                "replyto": "UYGGqnkaSp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5140/Reviewer_RPtB"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5140/Reviewer_RPtB"
                ],
                "content": {
                    "comment": {
                        "value": "I am satisfied with the authors' rebuttal and I am willing to increase my rating."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5140/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700738303693,
                "cdate": 1700738303693,
                "tmdate": 1700738303693,
                "mdate": 1700738303693,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]