[
    {
        "title": "Generative Marginalization Models"
    },
    {
        "review": {
            "id": "GjxXPQBgH6",
            "forum": "rUH2EDpToF",
            "replyto": "rUH2EDpToF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7731/Reviewer_kkqm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7731/Reviewer_kkqm"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents Marginalization Models (MAMs) as a novel approach to generative modelling for high-dimensional discrete data. MAMs offer scalability, tractable likelihood calculations, and efficient evaluation of marginal probabilities. They support any-order generative models and excel in energy-based training. Experimental results demonstrate MAMs' effectiveness across diverse data types, emphasizing their significant speedup in marginal probability evaluation and their capacity to handle high-dimensional problems, surpassing previous methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper introduces a novel family of generative models that offer both computationally feasible marginalisation and scalability for generating high-dimensional discrete data."
                },
                "weaknesses": {
                    "value": "My primary concern centres around the aspect of marginalization self-consistency. I'm sceptical about whether the soft constraint presented in equation 7 can effectively ensure self-consistent marginalization. The paper lacks reports on real-world applications that involve marginal evaluation. Is it possible to validate the effectiveness of the marginal constraint in some toy examples, like training the model on a predefined  Gaussian distribution with a known marginal density and directly testing it using the mean squared error?"
                },
                "questions": {
                    "value": "- To implement the self-consistency constraint outlined in equation 7, you must specify a distribution $q(x)$ for subsampling. How does the choice of the $q$ distribution impact the training process? Is $q(x)$ set to be the data distribution, or can it be any arbitrary distribution?\n- When it comes to inference, do you employ equation 6 for sampling, or do you utilize the conditional distribution $p_\\phi(x_{\\sigma(d)} | x_{\\sigma(<d)})$? What\u2019s the difference between these two schemes?\n- Point 2) in section 4.3 appears unclear. To train an order-agnostic AutoRegressive Model within the EBM framework, it seems plausible to employ the reinforce gradient estimation method outlined in equation 9. By replacing $\\log p_\\phi (x)$ with the Monte Carlo estimation of $\\mathbb{E}_\\sigma \\log p_\\phi(x|\\sigma)$, one could potentially achieve this. Therefore, the statement \"ARMs cannot be trained with the expected DKL objective over all orderings simultaneously\" is somewhat unclear to me.\n- In section 4.3, point 3), I hold a different perspective regarding the efficiency of MAMs compared to ARMs, especially in high-dimensional scenarios. While it's true that in ARM-Full, you do require D feed-forward runs for gradient computation, in MAMs, you also necessitate Gibbs sampling to generate samples from the model. Even if you employ block-wise Gibbs sampling, it still demands multiple steps to guarantee MCMC chain convergence. Hence, I doubt that MAMs also face challenges in high-dimensional problems.\n- Auto-regressive diffusion models have demonstrated strong performance on the CIFAR-10 dataset. Is it conceivable that MAM could also yield promising results on CIFAR-10?\n- Could you offer additional examples to illustrate why we should be concerned with any-order AutoRegressive models? Do any-order ARMs exhibit superior performance compared to their order-specified counterparts?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7731/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698615079749,
            "cdate": 1698615079749,
            "tmdate": 1699636943320,
            "mdate": 1699636943320,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "OSDINkxD30",
                "forum": "rUH2EDpToF",
                "replyto": "GjxXPQBgH6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7731/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7731/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your comprehensive review and valuable feedback on our paper. We really appreciate your time and efforts dedicated. We are glad to hear that you found the idea to be novel. We are pleased to address your concerns and questions to better elucidate the contributions and capabilities of MaMs.\n\n- **Why Any-Order auto-regressive models, v.s. fixed order?**\n    - The test NLL achieved by AO-ARM are often just slightly worse than fixed-order ARM, since it is solving a harder problem. But they are interesting and useful due to the flexibility in sampling. For problems without natural ordering, such as molecules, proteins, materials, AO-ARMs allow full control of the generation ordering, and arbitrary conditional generation that fixed-order ARM cannot. See EvoDiff [1] for a recent example in controllable protein design. We have also provided an experiment on molecule string conditional generation (**Figure 7, 24, 25**). Even for tasks such as languages with an ordering, AO-ARMs allow us to do in-fillings and re-edits on existing text, which is useful for iteratively improving and more customizable than current language models.\n    - Access to marginals further increase the flexibility in generation, such as sampling multiple variables at one step (see **general response C1.2 Experiment 2**). Marginals also allow for much better scalability in evaluating marginal likelihood, which is crucial for test time inference of $\\log p$ (such as whether a new protein sequence is better in $\\log p$ than its counterpart) and scalable training in energy-based setting.\n- **Concerns about marginalization self-consistency**\n    \n    Please refer to **general response C1** for details**.** Thanks for your suggestions. We conducted extra experiments to validate self-consistency on a synthetic problem with known ground truth PMF. We also measured self-consistency on real-world problems (see C1.2). \n    \n- **Choice of $q$ for sampling self-consistency**\n    \n    Please refer to **general response C2.** \n    \n- **Sampling with conditionals $p_\\phi$ v.s. marginals in eq.(6)**\n    - Please refer to **general response C1.2 Experiment 2.** We conducted extra ablation studies of the two sampling approaches. We have also experimented sampling with different number of variables at one step using marginals.\n    - In **general response C1.1**, we additionally provided a synthetic problem that purely learns marginals and sample using marginals.\n- **Why not train AO-ARM with MC estimation of the gradient in energy-based training?**\n    \n    Please refer to **general response C3.** \n    \n- **Block-wise Gibbs sampling still demand multiple steps to guarantee MCMC chain convergence.**\n    \n    Please refer to **general response C4.** \n    \n- **Promising results on CIFAR-10?**\n    \n    We followed your suggestion and conducted experiments. Indeed MaM can yield promising results on CIFAR-10. Please refer to **general response C5** for details.\n    \n\nWe hope this addresses your concerns and answers your questions, and we\u2019re happy to follow up on any questions or concerns in more detail. \n\n[1] Alamdari, Sarah, Nitya Thakkar, Rianne van den Berg, Alex Xijie Lu, Nicolo Fusi, Ava Pardis Amini, and Kevin K. Yang. \"Protein generation with evolutionary diffusion: sequence is all you need.\"\u00a0*bioRxiv*\u00a0(2023): 2023-09."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7731/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700259705275,
                "cdate": 1700259705275,
                "tmdate": 1700259705275,
                "mdate": 1700259705275,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GtIp9gvGIe",
                "forum": "rUH2EDpToF",
                "replyto": "GjxXPQBgH6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7731/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7731/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer kkqm,\n\nThanks again for your comprehensive review and valuable feedback. In our previous response, we have conducted additional experiments (on both predefined distribution and real-world distribution) following your suggestions to address your concerns centered around \u201cmarginalization self-consistency\u201d. We have also answered the questions raised in your review. In light of our response, we would appreciate if you would consider re-evaluating our submission. Please let us know if there are further questions we can address before the rebuttal period ends."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7731/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700583209509,
                "cdate": 1700583209509,
                "tmdate": 1700583266275,
                "mdate": 1700583266275,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TclIbbUg3H",
                "forum": "rUH2EDpToF",
                "replyto": "GtIp9gvGIe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7731/Reviewer_kkqm"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7731/Reviewer_kkqm"
                ],
                "content": {
                    "title": {
                        "value": "Response to the authors"
                    },
                    "comment": {
                        "value": "Thanks for your great effort in responding and revising. Could you show me what the generated image on CIFAR-10 looks like?"
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7731/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700585547146,
                "cdate": 1700585547146,
                "tmdate": 1700585547146,
                "mdate": 1700585547146,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RtXho6D1tt",
                "forum": "rUH2EDpToF",
                "replyto": "4GzT9v4xyi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7731/Reviewer_kkqm"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7731/Reviewer_kkqm"
                ],
                "content": {
                    "title": {
                        "value": "Response to the authors"
                    },
                    "comment": {
                        "value": "Thanks for your effort in adding the generated image on CIFAR-10. The results look good. Overall, the paper looks good to me in terms of the idea of ensuring marginalisation constraint in sequential generative models. However, as pointed out by reviewers sWKU and G1Dh, I am still concerned about the effectiveness of such soft constraints. This concern is not raised by the experimental results, but by the less of theoretical guarantee of MaM compared to probabilistic circuits. In this regard, I decided to keep my score unchanged and lean towards accepting the paper."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7731/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700598676136,
                "cdate": 1700598676136,
                "tmdate": 1700598676136,
                "mdate": 1700598676136,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RE7kyNHvyJ",
                "forum": "rUH2EDpToF",
                "replyto": "GjxXPQBgH6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7731/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7731/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for acknowledging that the idea of ensuring marginalization constraint is interesting and supported by experiments. We appreciate your feedback and being supportive of accepting our paper. \n\nWe would also hope to highlight that as a NN-based approximate inference model, our paper's focus is on how modeling marginals unlock new capabilities beyond previous NN-based methods, while maintaining strong generative modeling performance. The new capabilities include: \n- Scalable training under energy-based setting. This was not possible for ARMs due to reasons explained in Section 4.3. The optimizaion of marginalization self-consistency in this setting shares similar spirit with minimizing Bellman residue in actor-critic based methods for reinforcement learning. Although theoretically the equality is not guaranteed, but it enables rewards (energies in this case) to be propagated and make training scalable on high-dimensional problems.\n- Significantly faster marginal inference for comparing $\\log p$'s\n- Flexible sampling of multiple variables in one step using marginals\n\nall due to directly modeling the marginals. And those capabilities *do not require marginal estimates to be exactly self-consistent*, as shown in the experiments. As discussed in our response with reviewer sWKU, there is a trade-off between exact marginalization and NN-based soft marginalization. We think it is equally worth exploring in both directions.\nDesigning neural networks that inherently satisfy the self-consistency would be a very interesting yet challenging research problem, however it falls beyond the scope of this paper.\n\nThank you again for your valuable feedbacks and suggestions that help improve our paper."
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7731/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700600978380,
                "cdate": 1700600978380,
                "tmdate": 1700621747855,
                "mdate": 1700621747855,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "yHbHjMsrMS",
            "forum": "rUH2EDpToF",
            "replyto": "rUH2EDpToF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7731/Reviewer_CcDR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7731/Reviewer_CcDR"
            ],
            "content": {
                "summary": {
                    "value": "The paper develops marginalization models (MAMs) for modelling all marginals and conditionals for discrete data. It introduces a consistency loss that is combined with either a maximum likelihood objective or an energy-based objective. MAMs require only a single forward pass for computing likelihoods in energy-based learning setups, in contrast to autoregressive models (ARMs). On MLE tasks, the method leads to similar NLL as any-order ARM, but with much smaller marginal inference time. Similarly, for energy-based learning task, the suggested approach performs similar to ARM, but with significant speedup."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is generally well written and easy to follow. It addresses an important question. \n\nThe idea to introduce a scalable optimisation term that encourages marginalisation self-consistency is new as far as I am aware. \n\nThe approach seems to be applicable broadly (I am not sure how well this extends to non-discrete data) and is illustrated on challenging problems. Various experiments on both maximum likelihood training illustrates that it performs comparable with any-order ARMs, while being significantly faster."
                },
                "weaknesses": {
                    "value": "In Section 4.3 2), the authors argue that if the model is not perfectly self-consistent, this poses an issue for ARMs for energy-based setups. As MAMs will likewise not be perfectly self-consistent, why is this not an issue for them? In particular, it is not clear to me that the Gibbs sampling procedure then leads to samples from $p_{\\theta}$. Are you adjusting via importance sampling in the experiments?\n\nThe paper often assumes that neural networks are universal approximators. It has not become clear why this is a practical assumption for the used architectures as the number of marginal constraints scales badly with K and D. Does this consistency loss actually go to zero in the experiments?"
                },
                "questions": {
                    "value": "Is there a reason why you choose the squared difference of the log densities to be matched due to marginalisation constraints over samples from q? If these distributions should coincide, why not use a divergence between them such as their KL? \n\nCan you clarify when the \u2018generative\u2019 conditional eq. (6) is used vs. the learnable conditionals with parameter $\\phi$?\n\nCan you clarify why a high correlation with AO-ARM-E is a sensible evaluation measure?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7731/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698634021854,
            "cdate": 1698634021854,
            "tmdate": 1699636943203,
            "mdate": 1699636943203,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GdYfbl90DP",
                "forum": "rUH2EDpToF",
                "replyto": "yHbHjMsrMS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7731/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7731/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your thoughtful review and constructive feedback on our paper. We are glad to hear that you found the paper well-written and the idea of introducing a scalable optimization term for marginalization self-consistency novel. We address below the concerns and questions you have raised.\n\n- **Concerns about training with persistent Gibbs sampling**\n    \n    Please refer to **general response C4**. \n    \n- **Assumption of neural nets as universal approximators for self-consistency**\n    - Your concern about the practicality of the assumption that neural networks are universal approximators is valid. We make this assumption based on literature and empirical evidence that deep neural networks *can approximate a wide range of functions if given a scalable training objective* (for example the most recent successes include modeling conditional distribution in LLMs, learning score functions in diffusion model). In synthetic example with $D=32$, $K=2$, we observe that self-consistency are almost perfectly enforced with a MLP. Please refer to **general response C1.1.**  In real-world problems, the consistency loss does not reach absolute zero but approaches a small value. Please check **general response C1.2** on how well the self-consistency are enforced and how neural network capacity is required for learning marginals.\n    - To clarify, the assumption is only required for the two-stage efficient training to be theoretically sound. If we drop the assumption, we can still train by using a regularized objective, i.e. $\\min_{\\theta,\\phi} - \\mathbb E_{ {x} \\sim p_{\\text{data}}} \\mathbb E_{\\sigma \\sim \\mathcal U\\left(S_D\\right)} \\sum_{d=1}^D \\log p_\\phi \\left( x_{\\sigma(d)} \\mid x_{ \\sigma(<d)} \\right) + \\lambda \\mathbb E_{ x \\sim q(x)} \\mathbb E_{\\sigma \\sim \\mathcal U\\left(S_D\\right)} \\mathbb E_{d \\sim \\mathcal U (1, \\cdots, D)} \\left( \\log [p_\\theta \\left( x_{\\sigma(<d)}\\right) p_\\phi\\left( x_{\\sigma(d)} \\mid x_{\\sigma(<d)}\\right) ]-\\log p_\\theta\\left( x_{\\sigma(\\leq d)}\\right)\\right)^2$This training procedure should achieve the same performance, as long as the first term is approximated with conditionals instead of $\\log p_\\theta$.\n- **Why** **Choosing squared distance of $\\log p$ instead of KL divergence for self-consistency**\n    \n    We choose squared distance of $\\log p$ so that $q(x)$ can be flexibly specified to reflect the distribution to perform marginal inference on. Please refer to **general response C2** on how $q(x)$ can be set. KL divergence is a good suggestion. The idea of using KL divergence is intriguing. However it is relatively not straightforward to define what is $p$ and $p^\\prime$ in $D_\\text{KL}(p\\parallel p^\\prime)$, since both $p$ and $p^\\prime$ involve marginals $p_\\theta$, the gradient might not be scalable to approximate. If we fix $p^\\prime$ to be $p_\\theta\\left( x_{\\sigma(\\leq d)}\\right)$but detach the gradient and set $p$ to be $p_\\theta\\left( x_{\\sigma(<d)}\\right) p_\\phi\\left( x_{\\sigma(d)} \\mid x_{\\sigma(<d)}\\right)$. Then one can show that the REINFORCE gradient of $D_\\text{KL}(p\\parallel p^\\prime)$ falls under a special case of the gradient of our current squared distance loss when $q$ is set to $p_\\theta$. (Check [1] for a similar argument on connection of squared loss of $\\log p$ and KL divergence.)\n    \n- **Using \u2018generative\u2019 conditionals (based on marginals) v.s. using learnable conditionals**\n    - Please refer to **general response C1.2 Experiment 2.** We conducted extra ablation studies of the two sampling approaches. We have also experimented sampling with different number of variables at one step using marginals.\n    - In **general response C1.1**, we additionally provided a synthetic problem that purely learns marginals and sample using marginals.\n- **Why measuring** $\\log p$ **correlation with that from AO-ARM-E**\n    - *Why compare with AO-ARM-E*: In real-world applications, we don\u2019t have a ground truth PMF to compare to, hence we want to pick the best possible marginal likelihood estimate we can get. AO-ARM-E models the distribution $p_\\theta$ the best in terms of attaining the best test NLL. Therefore we regard its marginal estimate to be the best quality to compare with.\n    - *Why measuring correlations*: The correlation is sensible as when we perform marginal inference, it is either using it to compare likelihoods of two samples, i.e. $\\log p(x)$ v.s. $\\log p(x^\\prime)$, or getting normalized conditionals such as $p(x_\\mathcal{V} | x_\\mathcal{U})  = \\frac{p([x_\\mathcal{V}, x_\\mathcal{U}])}{\\sum_{x_{\\mathcal{U}}}p([x_\\mathcal{V}, x_\\mathcal{U}])}$. High correlations means these questions can be answered correctly.\n\nWe hope this addresses your concerns and answers your questions, and we\u2019re happy to follow up on any questions or concerns in more detail. \n\n[1] Nikolay Malkin, Salem Lahlou, Tristan Deleu, Xu Ji, Edward Hu, Katie Everett, Dinghuai\nZhang, and Yoshua Bengio. Gflownets and variational inference. arXiv preprint\narXiv:2210.00580, 2022."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7731/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700259620614,
                "cdate": 1700259620614,
                "tmdate": 1700259758069,
                "mdate": 1700259758069,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pc69t80hsJ",
                "forum": "rUH2EDpToF",
                "replyto": "GdYfbl90DP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7731/Reviewer_CcDR"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7731/Reviewer_CcDR"
                ],
                "content": {
                    "title": {
                        "value": "Response to rebuttal"
                    },
                    "comment": {
                        "value": "Thank you for responses that have largely addressed my questions. I intend to keep my weak accept score."
                    }
                },
                "number": 25,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7731/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700705004138,
                "cdate": 1700705004138,
                "tmdate": 1700705004138,
                "mdate": 1700705004138,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "BWFUDfxBAt",
            "forum": "rUH2EDpToF",
            "replyto": "rUH2EDpToF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7731/Reviewer_sWKU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7731/Reviewer_sWKU"
            ],
            "content": {
                "summary": {
                    "value": "Authors propose Marginalization Models (MaMs), generative models for discrete data allowing (approximate) marginal inference. They key idea is to minimize a penalty term aimed at approximately satisfying the marginalization self-consistency constraint (Eq. 5). The model is evaluated on binary images, text, and molecules."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "None."
                },
                "weaknesses": {
                    "value": "- The paper presents false claims, and the whole modelling framework is not theoretically solid/supported\n- By evaluating what authors call \"augumented vector representation\" we are not really summing-out the unobserved variables, i.e. we are not performing marginalisation. Rather, we are computing a simple p(x) where x belongs to an augmented state space, and therefore there's no guarantee that this satisfy what authors call \"marginalization self-consistency\".\n- The novelty of the work boils down to the constraint in the loss function (cf. Eq. 9)\n- The model is not agnostic to variable orderings and, as such, cannot deal with efficient and *exact* marginalization (see first equation in section 4.2).\n- As far as I understood, there's no guarantee that the model satisfies the marginalization self-consistency constraint (Eq. 5), and therefore the model compares to any other dealing with approximate marginal inference, such as [1] [2].\n- Sampling is still sequential, as standard autoregressive models. This is weird, as a model allowing for proper marginal inference should not have sequential sampling.\n- In general, there should be much more focus and discussion on Probabilistic Circuits, as they satisfy the self-consistency constraint by design, with no need for a penalty term, resulting in exact marginal inference (which is the goal of this paper). Furthermore, PCs are not limited on working with discrete variables only, but can handle heterogeneous data. PCs allow one-shot (conditional) sampling, without relaying on a variable ordering as in MaMs. In short, I believe PCs should be treated as the main competitor of this work, but this is not the case.\n\n[1] Shih, Andy, Dorsa Sadigh, and Stefano Ermon. \"Training and Inference on Any-Order Autoregressive Models the Right Way.\" arXiv preprint arXiv:2205.13554 (2022).\n\n[2] Strauss, Ryan, and Junier B. Oliva. \"Arbitrary conditional distributions with energy.\" Advances in Neural Information Processing Systems 34 (2021): 752-763."
                },
                "questions": {
                    "value": "- INTRO: why marginal evaluations in transformer should be O(D)? I would say that O(D) is the evaluation cost of a fully observed sample,\nnot for partially observed samples (i.e. marginals). Indeed, transformers, as any standard autoregressive models, cannot perform arbitrary marginalisation.\n\n- INTRO: why EBMs should be limited in fixed-order generative modeling? I do not agree with this claim\n\n- I do not agree with most of what is said in paragraph \"Energy-based training\", e.g. Why do authors say that in this setting there are no data available?\nAFAIK, we do not have access to $f$, rather we model $f$, an unnormalized density/PMF.\nWhen mentioning EBMs I think about [1] and the immense literature deriving from it.\n\n[1] LeCun, Yann, et al. \"A tutorial on energy-based learning.\" Predicting structured data 1.0 (2006).\n\n\n- What MaMs provide that PCs don't?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7731/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7731/Reviewer_sWKU",
                        "ICLR.cc/2024/Conference/Submission7731/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7731/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698662267485,
            "cdate": 1698662267485,
            "tmdate": 1700389549283,
            "mdate": 1700389549283,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "VZ0eyJKpk2",
                "forum": "rUH2EDpToF",
                "replyto": "BWFUDfxBAt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7731/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7731/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate your thorough review and insightful comments on our manuscript. Your feedback is invaluable in enhancing the clarity and impact of our work. Below, we address each of your concerns to clarify misunderstandings and reinforce the contributions of our model.\n\n- **Marginalization Self-Consistency**\n    - **Theoretical foundation:**\n        \n        We understand your concern regarding the theoretical solidity of MaMs. The foundation of MaMs is built on probabilistic principles, i.e. that the model should adhere to the marginalization self-consistency. In training, we make use of neural nets as powerful function approximator to learn marginals that are self-consistent.\n    - **Investigating self-consistency empirically:** \n\n      We have also investigated in depth on how well the marginalization self-consistency is enforced in MaMs and how the soft-consistencies are very useful in practice. Please refer to **general response C1.1** for details.\n    - **Compare with PCs**:\n        \n      Probabilistic circuits are very powerful and promising approaches that exhibit great properties such as fast and exact marginalization through smart design of the model\u2019s structure and operations. We fully acknowledge this and have discussed in details in related work.\n         - **neural approximate inference models v.s. PCs:**\n             - As you have pointed out, our model falls under the category of neural generative models that perform approximate inference, and is more comparable with approaches such as AO-ARM [3], Mast-tuned ARM[1] and Arbitrary conditional energy models (ACE)[2].\n             - Compared to PCs, the neural approximate inference approaches do not have the exact marginalization properties but have better flexibility in modeling complex distributions. This has been shown in the generative models literature and in our paper. We have included more detailed discussion on this (in **Appendix A.4**) about trade-offs between exact marginalization v.s. approximate marginalization.\n        - In experiments, we compare with **both** **neural models** that perform approximate inference and **PCs** that perform exact inference (**Table 1**). PCs have great performance in marginal inference time and is the only one that performs exact marginal inference. In terms of generative modeling performance, there remains a gap between PCs and NN-based methods. On larger problems such as Text8 and molecules, we focus on NN-based approaches, since we do not find PC models designed for text or molecules.\n            - We have also included MAC [1] following your suggestions. (See **general response C5** for details.) [2] focus on continuous data and mixed data, hence we do not directly compare. We also compared with SOTA discrete generative model such as AO-ARM [1] and GFlowNets.\n        - Last, we want to clarify that the perspective of this paper is to propose a neural-network based generative model based on learning marginals. MaMs unlock new capabilities beyond previous NN-based methods, while maintaining strong generative modeling performance. The new capabilities include: 1) significantly faster marginal $\\log p$ inference, 2) scalable training under energy-based setting, and 3) flexible sampling of multiple variables in one step using marginals, all due to directly modeling the marginals."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7731/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700259047606,
                "cdate": 1700259047606,
                "tmdate": 1700260367940,
                "mdate": 1700260367940,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "plmecwdYr4",
                "forum": "rUH2EDpToF",
                "replyto": "BWFUDfxBAt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7731/Reviewer_sWKU"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7731/Reviewer_sWKU"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your detailed rebuttal. Given the clarifications in the current state of the paper I'm increasing my score from 3 to 5 (although I feel a 4 to better reflect my position). Given the approximate nature of the method proposed and on its limitation to work with discrete data, part of my concerns remain. \n\nI would just like some more clarifications about the number of discrete state $K$. Can you elaborate a bit more the paragraph \"Scalable learning of marginals with conditionals\"? Could your proposed solution be enough to test MaMs on the (non-binary) MNIST dataset? If so, could you report results in bpd? The literature is filled of results for MNIST, and as such would help contextualize better the capabilities of your model.\n\nFinally, note that computing marginals could be very important for neural compression [A]. If MaM allows accurate approximation of marginals, why not testing how well it would perform in neural compression tasks? \n\n[A] Anji Liu, Stephan Mandt, and Guy Van den Broeck. Lossless compression with probabilistic circuits. In International Conference on Learning Representations, 2022"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7731/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700389341974,
                "cdate": 1700389341974,
                "tmdate": 1700389507673,
                "mdate": 1700389507673,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZGN3I5j6Fr",
                "forum": "rUH2EDpToF",
                "replyto": "BWFUDfxBAt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7731/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7731/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your reply and valuable suggestions. We appreciate your acknowledgement of our clarifications. We are happy to clarify on the questions you raised.\n\n- **Number of discrete state $K$**\n    \n    We conducted an extra experiment on MNIST ($K=256$) with the approach proposed in Section 4.1, Claim 1 (with the scalable learning objective in (7) proposed when $K$ is large). MaM achieve a test NLL of $0.577$ bpd (using the conditionals), which is better than most SOTA such as Locally Masked PixelCNN ($0.65$). The marginal $\\log p$ from MaM have a spearman correlation $0.940$ and a pearson correlation $0.941$ (as compared to $\\log p$ from conditionals). We have also conducted an experiment on CIFAR-10 ($D=3\\times 32 \\times 32$, $K=256$) in this case. MaMs achieve competitive test NLL and (fast) marginal estimates as well, please refer to **general response C4**. \n    \n- **Neural compression**\n    - Thanks for the great suggestion and the great reference. Neural compression is indeed an important use-case of exact likelihood models such as PCs, ARMs and Normalization Flows. We tested neural compression with MaM on MNIST-Binary, and the **actual bpd** is $0.154$ which is close to the (theoretical) **test NLL** $0.146$. We will include this result in our updated manuscript.\n    - Since compression algorithms such as rANS require a sequence of PMF and CDF over the variables, MaM will incur similar amount of compute time as autoregressive models, i.e. compute PMF and CDF along every dimension. Hence we did not focus on this task as it is not when MaM should be used instead of models such as ARMs and Flow models. However, by utilizing ideas in MADE [1], it might be possible to use just one NN forward pass to get all the PMF and CDF needed for neural compression. But again there is a trade-off in terms of model expressiveness v.s. efficiency. This is also related to how PCs with structures [A] enable $\\mathcal O(\\log D)$ evaluations. \n- **Approximate v.s. exact**\n    \n    We appreciate the feedback that the method is limited by its approximate nature. We would like to point out that there is a constant trade-off of approximate and exact. By utilizing the approximation power of NNs, we have to give up some nice theoretical properties. In this case, the marginals are soft-consistent instead of being exactly consistent. We also empirically measured how well the self-consistencies are preserved and how the marginals learned can be useful in practice. MaMs unlock **three new capabilities** \u2014 1) significantly faster marginal $\\log p$ inference, 2) scalable training under energy-based setting, and 3) flexible sampling of multiple variables in one step using marginals, all due to directly modeling the marginals \u2014  **beyond previous NN-based methods**, while maintaining strong generative modeling performance. We think our work proposed a new idea and provided evidence that learning approximate marginals are worth exploring, in parallel to how PCs with tractate likelihood properties are worth exploring as well. At the moment, they might be best suited for different tasks where marginal inference are required. But there is more hope that by further exploring those ideas, powerful generative models with fast exact likelihood can become a reality. \n  \nWe are happy to follow up on any further questions or concerns you might have.\n\n[1] Papamakarios, George, Theo Pavlakou, and Iain Murray. \"Masked autoregressive flow for density estimation.\"\u00a0*Advances in neural information processing systems*\u00a030 (2017)."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7731/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700513984859,
                "cdate": 1700513984859,
                "tmdate": 1700514209099,
                "mdate": 1700514209099,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "UOTAwqKbn5",
            "forum": "rUH2EDpToF",
            "replyto": "rUH2EDpToF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7731/Reviewer_G1Dh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7731/Reviewer_G1Dh"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces Generative Marginalization Models (MAMs), which are a new type of generative model for high-dimensional discrete data. MAMs address the limitations of existing methods by explicitly modeling all derived marginal distributions, allowing for scalable and flexible generative modeling. MAMs can rapidly evaluate any marginal probability with a single forward pass of a neural network, without requiring accurate marginal inference. MAMs also support scalable training of generative models with arbitrary orderings of high-dimensional problems in an energy-based training setting. The effectiveness of MAMs is demonstrated on various discrete data distributions, including binary images, language, physical systems, and molecules. MAMs significantly speed up the evaluation of marginal probabilities, enabling the modeling of arbitrary orderings of high-dimensional problems that were not achievable with conventional methods in energy-based training tasks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The authors introduce a new family of generative models called Marginalization Models (MAMs) that have tractable likelihoods and offer scalable and flexible generative modeling.\n  - MAMs allow for fast evaluation of arbitrary marginal probabilities with a single forward pass of the neural network, overcoming a limitation of autoregressive models.\n  - The proposed model supports scalable training of any-order generative models for high-dimensional problems under the energy-based training setting.\n  - MAMs achieve significant speedup in evaluating marginal probabilities.\n- The effectiveness of the proposed model is demonstrated on various discrete data distributions, including binary images, language, physical systems, and molecules.\n- The authors identify an interesting connection between generative marginalization models and GFlowNets, showing their equivalence under certain conditions in A.3 in the appendix."
                },
                "weaknesses": {
                    "value": "- When I read the introduction, I expected to find a way to rigorously guarantee self-consistency constraints, but in fact I was somewhat underwhelmed because the actual way is only to impose soft constraints during optimization.\n- The experimental results are not so good. In many experiments, the evaluation with NLL is slightly worse than the baseline, which does not fully demonstrate the superiority of the MAM.\n- The effectiveness of two-stage training has not been adequately explained. At least, there should be an experimental comparison with the case without the two-stage training."
                },
                "questions": {
                    "value": "- I do not understand why two-stage training is effective. Do the authors have any hypothesis on it?\n- What is the definition of the marginal inference time in Tables 2, 3, and 4?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7731/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699083438488,
            "cdate": 1699083438488,
            "tmdate": 1699636942982,
            "mdate": 1699636942982,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "oSFLI78fHH",
                "forum": "rUH2EDpToF",
                "replyto": "UOTAwqKbn5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7731/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7731/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the time and efforts devoted to reviewing our paper. We appreciate the valuable feedbacks and hope to clarify on the questions and concerns. We have conducted extra experiments to support our clarifications. We have also incorporated the valuable feedbacks into the paper.\n\n- \u201c**Self-consistency is only enforced softly\u201d**\n    - Please refer to **general response C1.**\n- **\u201cDefinition of marginal inference time\u201d**\n    \n    We are happy to make clarifications. Marginal inference here means given a $x_\\mathcal{S}$ (for example $x_\\mathcal{S}= (0,1,1,1,1,1,?,?,?)$) evaluate the marginal probability under the learned generative model, i.e. $p_\\theta(x_\\mathcal{S})$. With MaM, we use the marginal network $\\theta$ to evaluate $p_\\theta(x_\\mathcal{S})$ with a single NN forward pass. As with AO-ARM, it requires evaluating a sequence of conditionals (NN forward passes) to get the marginal of $p_\\theta(x_\\mathcal{S})$. Those marginals are compared to the marginal from the best likelihood estimation model (AO-ARM-E) by measuring Spearman\u2019s and pearson correlations.\n    \n- **\u201cExperiment results are not good\u201d**\n    \n    We believe this is a misunderstanding on how methods should be compared to each other. The main merit of MaMs is in unlocking new capabilities beyond previous methods, while maintaining strong generative modeling performance. The new capabilities include: 1) significantly faster marginal $\\log p$ inference, 2) scalable training under energy-based setting, and 3) flexible sampling of multiple variables in one step using marginals, all due to directly modeling the marginals. \n    \n    - **Comparable generative modeling performance**: In MLE setting, the fair comparison of test NLL will be MaM with AO-ARM-Single, since that is how data is generated (using a single random ordering). Since both methods have learnable conditionals, they achieve the same NLL. The AO-ARM-Ensemble model uses an ensemble of orderings for evaluating a better quality marginal likelihood, which is used only as a reference. Because in reality, AO-ARM-Ensemble cannot be used for generation. We are sorry for the confusion.\n    - **Significantly faster marginal** $\\log p$ **inference** (see last column of Table 1,2,3,4) as compared with all other methods while maintaining its generative modeling performance. Probabilistic circuits (PCs) are also fast in marignal evaluation but their generative modeling capability is not as good as NN-based approaches.\n    - **Training with scalability on high-dimensional problems under EB setting:** For example, Ising-model with $900$ dimensions and molecule strings with $500$ dimensions that other approaches fail to scale to.\n    - **Flexible sampling with marginals**: In **general response C1.2**, we provided additional experiments illustrating the power of marginal estimations in sampling with various block variable sizes that other SOTA methods such as AO-ARM cannot perform.\n\n- **\u201cTwo-stage training explanation\u201d**\n    \n    Thanks for the suggestion. We are happy to include more explanation. \n    \n    - The basic intuition is that the optimal conditionals can be obtained by doing Stage 1 just by maximizing the likelihood. And this is shown by proof of Proposition 1 in Appendix A.1. In practice, we find it most important to use the expectation over conditionals $\\mathbb E_{ x \\sim p_{\\text {data}}} \\mathbb E_{\\sigma \\sim \\mathcal U \\left(S_D\\right)} \\sum_{d=1}^D \\log p_\\phi\\left(x_{\\sigma(d)} \\mid x_{\\sigma(<d)}\\right)$ for the $\\mathbb E_{ x \\sim p_{\\text {data}}} \\log p_\\theta( x )$ part of the loss in eq.(8). Otherwise if marginal network is used for this part, we empirically find maximizing likelihood pushes $\\log p_\\theta( x)$ to high values that break the self-consistency. We propose two-stage optimization since it is theoretically principled and this is more GPU memory efficient (than optimizing $\\phi,\\theta$ jointly). If there is enough compute, it is possible to train Stage 1 objective and Stage 2 objective jointly, i.e. $\\min_{ \\theta,\\phi} - \\mathbb E_{ x \\sim p_{\\text {data }}} \\mathbb E_{\\sigma \\sim \\mathcal U \\left(S_D\\right)} \\sum_{d=1}^D \\log p_\\phi\\left(x_{ \\sigma (d)} \\mid x_{\\sigma(<d)}\\right) + \\lambda \\mathbb E_{x \\sim q(x)} \\mathbb E_{\\sigma \\sim \\mathcal U\\left(S_D\\right)} \\mathbb E_{d \\sim \\mathcal U(1, \\cdots, D)}\\left(\\log \\left[p_\\theta\\left(x_{\\sigma(<d)}\\right) p_\\phi\\left(x_{\\sigma(d)} \\mid x_{\\sigma(<d)}\\right)\\right]-\\log p_\\theta\\left(x_{\\sigma(\\leq d)}\\right)\\right)^2$\n    - In **general response C1.1**, we also tested just training MaM marginals (no conditionals are learned) on a synthetic problem. In that simple case, training with a regularized objective is totally fine and results in self-consistent marginals.\n\nWe hope this addresses your concerns and answers your questions, and we\u2019re happy to follow up on any questions or concerns in more detail."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7731/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700258666455,
                "cdate": 1700258666455,
                "tmdate": 1700258666455,
                "mdate": 1700258666455,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "XKVmF6uhtV",
            "forum": "rUH2EDpToF",
            "replyto": "rUH2EDpToF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7731/Reviewer_2RBk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7731/Reviewer_2RBk"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces Marginalization Models (MAMs), a novel class of generative models that brings scalability and flexibility to generative modeling while allowing tractable estimation of the marginal likelihood for any subset of multivariate random variables. MAMs can accommodate both maximum likelihood training and energy-based training, making them exceptionally versatile, particularly when dealing with discrete random variables, such as molecules, or random variables defined by an energy function. Extensive experimental results show the superior efficiency of MAMs compared to autoregressive models."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Originality & Significance: The paper proposes a new type of generative models MAMs. It can handle both maximum likelihood training and energy-based training, which is quite promising. The ability to handle such diverse training methods strengthens the significance of this work.\n\nQuality: The work is well-motivated and logically compact. The claim in Sec. 4.1 and discussion in Sec. 4.3 are sound.\n\nClarity: The paper is generally well-written and easy to follow. Figures are very illustrative."
                },
                "weaknesses": {
                    "value": "One issue with this paper is that it mainly uses small, simple datasets for its experiments. While the results look good with these small datasets, I'm not sure how well the model would work with larger, more complex real-world data. Real-world data is often much bigger and more complicated, presenting a lot of variables and intricacies. For instance, the number of possible orderings scales fractionally with data dimension To really understand if this approach is useful in practice, it would be helpful to test it on bigger and more diverse datasets.\n\nMinor:\n- typo in page 4: where $K$ is the number of discrete values $x_d$ can take -> ... discrete values $x_{\\sigma(d)}$ can take"
                },
                "questions": {
                    "value": "- >In this paper, we focus on the sampling procedure that generates one variable at a time, but marginalization models can also facilitate sampling multiple variables at a time in a similar fashion.\n\n    How can MAMs generate multiple variables simultaneously? It appears that MAMs are limited to generating variables sequentially, as indicated in Equation (5) and (6).\n\n- How does the order in which variables are sampled impact the quality of the samples?\n\n- How does different $q$ (e.g., uniform v.s. $p_{\\\\mathrm{data}}$) affect the learning of MAMs?\n\n- I'm a bit confused about the sample generation process with MAMs. In Section 3, it mentions using the normalized conditional (Equation (6)) for sampling, but in Section 6.1, it's said that the conditional model $p_{\\\\phi}$ is used to generate data. Can you clarify this process?\n\n- Given the two-stage training approach for MLE, it seems that the learned conditional model $p_{\\phi}$ is independent of the marginalization model $p_{\\\\theta}$. As mentioned on page 5, $p_{\\\\theta}$ is described as distilling marginals from conditionals. How does the conditional model, as an AO-ARM model, compare to the AO-ARM baselines?\n\n- In the experiments, how are the NLL (bpd) values estimated for MAM? Are these values the outputs of $p_{\\theta}$ or the logarithmic products of $p_{\\\\theta}$?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7731/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699178012031,
            "cdate": 1699178012031,
            "tmdate": 1699636942867,
            "mdate": 1699636942867,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "HB7zzcRCex",
                "forum": "rUH2EDpToF",
                "replyto": "XKVmF6uhtV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7731/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7731/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the constructive feedback and questions. We appreciate your time in reading our paper in details and catching typos. We have conducted extra experiments to support our clarifications. We have also incorporated the valuable feedbacks into the paper.\n\n- **\u201cSmall, simple datasets for evaluation\u201d**\n    - Datasets like MNIST Binary are relatively small. We study on them first since this is a new method and we want to understand it better. Text8 and Molecular Sets (MOSES) are relatively big and widely used for testing generative models, see [1][2] for example. Text8 has 100M characters and sequence is of length $D=250$. MOSES contain ~2M molecular structures, and the sequence length is $D=55$ (for SMILES) or $D=57$ (for SELFIES). We have also included an experiment CIFAR-10 **in general response C5**, which has much higher dimensions ($D=3 \\times 32 \\times 32, K=256$) than MNIST Binary ($D=28 \\times 28, K=2$). MaMs are able to learn marginals effectively and use them for marginal inference.\n- **Sampling and evaluation with MaMs**\n    \n    We apologize for not including all the details in our paper. \n    \n    - \u201c**How can MaMs generate multiple variables simultaneously?**\u201d: For generating multiple variables simultaneously, in **general response C1.2** we provide more details and conducted experiments to show this. The idea is in similar spirit to eq.(6): evaluate marginals on all possible outcomes over combinations of multiple variables and get the normalized sampling probability, i.e. $p_\\theta( x_{s_i} | x_{s(<i)}) = \\frac{p_\\theta([ x_{s_i}, x_{s(<i)}])}{\\sum\\nolimits_{ x_{s_i}} p_\\theta([ x_{s_i}, x_{s(<i)}])}$, where $s_i$ is the set of multiple variables to sample at this step.\n    - \u201c**How is sampling/generation and evaluation done with MaM?**\u201d:\n        - We can use either the conditional or the marginal for generation and evaluation. The sampling and evaluation of test NLL in the original submission is using conditionals. We have additionally included sampling and evaluation using marginals with different number of variables at one step.  Please refer to **general response C1.2** for details.\n        - The test NLL is evaluated with products of conditionals $p_\\phi$ or $p_\\theta$ in eq.(6),. The numbers from the output $p_\\theta(x)$ on a complete $x$ are not used since they are not exactly normalized probabilities. In experiments (see **general response C1.2 Experiment 1**), we evaluate the quality of $p_\\theta$, and show they are highly correlated with the actual $\\log p$ from conditionals.\n        - In **general response C1.1**, we additionally included a synthetic task that purely learns marginals and samples using marginals.\n\n- **\u201cHow does the order in which variables are sampled impact the quality of the samples?\u201d**\n    - We do not observe much difference, since MaMs are trained by sampling with arbitrary orderings. Same applies to AO-ARMs. For problems with natural ordering, such as language or maybe images, it is observed that training on a fixed ordering gives better results than training on any-orderings, see [1]. But that approach loses the flexibility of generating in any-order.\n- **\u201cHow does different q (e.g., uniform v.s. p_data) affect the learning of MAMs?\u201d**\n    - Please refer to **general response C2.**\n- **How does the conditional model, as an AO-ARM model, compare to the AO-ARM baselines?**\n    - They perform relatively the same since both are trained with the same objective $\\mathbb E_{ x \\sim p_{\\text {data }}} \\mathbb E_{\\sigma \\sim \\mathcal U \\left(S_D\\right)} \\sum_{d=1}^D \\log p_\\phi\\left(x_{\\sigma(d)} \\mid {x}_{\\sigma(<d)}\\right)$, except MaM additionally learn marginals.\n\nWe hope this addresses your concerns and answers your questions, and we\u2019re happy to follow up on any questions or concerns in more detail.\n\n[1] Emiel Hoogeboom, Alexey A. Gritsenko, Jasmijn Bastings, Ben Poole, Rianne van den Berg,\nand Tim Salimans. Autoregressive diffusion models. In 10th International Conference on\nLearning Representations, 2022\n\n[2] Shih, Andy, Dorsa Sadigh, and Stefano Ermon. \"Training and Inference on Any-Order Autoregressive Models the Right Way.\" arXiv preprint arXiv:2205.13554 (2022)."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7731/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700258401037,
                "cdate": 1700258401037,
                "tmdate": 1700584015968,
                "mdate": 1700584015968,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]