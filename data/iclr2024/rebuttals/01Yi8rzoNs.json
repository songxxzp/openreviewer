[
    {
        "title": "Visual Chain of Thought: Bridging Logical Gaps with Multimodal Infillings"
    },
    {
        "review": {
            "id": "I8JGPeI8Ot",
            "forum": "01Yi8rzoNs",
            "replyto": "01Yi8rzoNs",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4191/Reviewer_E16R"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4191/Reviewer_E16R"
            ],
            "content": {
                "summary": {
                    "value": "This manuscript presents Visual Chain-of-Thought (VCoT) which extends chain-of-thought prompting (CoT) in NLP literature to vision-and-language tasks. The paper considers the situation where the sequential data contains logical gaps. The core idea is to fill in the gaps by generating synthetic vision-and-language data (i.e., multimodal infillings). \n\nVCoT first extracts the global information of the sequential data, multipoint foveation (MPF), by using several pre-trained models, such as StableDiffusion, CLIP, and GPT-3.5. Then, the proposed approach recursively generates the multimodal infillings (i.e., text-visual pairs) based on the original sequential data and MPF. In each step of generation, VCoT selects the most consistent multimodal infillings among candidates after generating multiple candidates. Such a generation process is repeated a fixed number of times. \n\nThe paper conducts experiments regarding the quality of the generated multimodal infillings through human evaluation and visualization."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "(S1) Generating intermediate data to bridge the logical gaps is definitely a novel direction.\n\n(S2) The work conducts a human evaluation to assess the generated infilling\u2019s quality."
                },
                "weaknesses": {
                    "value": "(W1) The generated multimodal infillings show marginal improvements compared with the simple baselines (Chain-of-Thought and Chain-of-Images). As can be seen in Table 2, VCoT has a roughly 30% chance of winning on average. It indicates that the proposed method does not dominate the baselines in downstream tasks (i.e., WikiHow and Visual Storytelling). Even worse, the gap between VCoT and the method that does not use any infillings (No Infilling) also seems marginal, although VCoT leverages several strong pre-trained models (e.g., GPT-3.5, CLIP, StableDiffusion, and OFA) with massive computation. \n\n\n(W2) The paper mainly focused on checking the quality of the generated multimodal infillings. It would be great if the paper presented the impact of VCoT in existing evaluation protocols at VIST and WikiHow tasks.\n\n\n(W3) There are several components in VCoT, but the paper does not present any ablation studies. So, I can\u2019t check the individual contribution of the proposed components. For example,\n- VCoT generates multipoint foveation (MPF) only with text (i.e., w/o image captions)\n- VCoT generates multimodal infillings without MPF (i.e., w/o MPF) \n- VCoT generates a single multimodal infilling, not multiple (i.e., five) candidates (i.e., w/o consistency-driven selector)\n     \n\n(W4) There are too many typos and inconsistent notations in the manuscript as below. I request the authors to review the entire paper thoroughly.\n- In Section 3, the main text describes that the judgment function j takes vision and language data, but the function takes the text in Eq. 2.  \n- In Section 3, textual references of Equation 3 and 4 should be Equation 5 and 6, respectively.\n- In Equation 6, \u201cEq. 6 holds\u201d should be \u201cEq. 5 holds\u201d\n- In Section 4, textual references of Equation 5 should be Equation 7."
                },
                "questions": {
                    "value": "Please see the weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4191/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4191/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4191/Reviewer_E16R"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4191/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698119906329,
            "cdate": 1698119906329,
            "tmdate": 1699636385415,
            "mdate": 1699636385415,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "DaK2EvGn8t",
            "forum": "01Yi8rzoNs",
            "replyto": "01Yi8rzoNs",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4191/Reviewer_BRKH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4191/Reviewer_BRKH"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses the problem of visual chain-of-thought, which is formulated as an infilling task that constructs intermediate states based on information from their adjacent states. It takes advantage of a large language model (GPT 3.5) and a diffusion model to generate textual descriptions and images, and adaptively selects candidate text/image by evaluating their novelty and consistency w.r.t. adjacent states. Experimental results show that the proposed approach is able to outperform single-modal chain-of-thought methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+ This paper addresses an important problem of extending chain-of-thought logical reasoning from single-modal to multi-modal domain.\n\n+ The method takes into account the relationship between the intermediate states and neighbors (i.e., novelty and consistency), showing enhanced performance in generating coherent text and images.\n\n+ Large-scale human evaluation demonstrates the advantages of the proposed approach."
                },
                "weaknesses": {
                    "value": "- There is a lack of ablation study on the proposed method, making it difficult to estimate the effectiveness of individual components. For instance, while the recursive generation seems to be an important step, the paper does not provide any quantitative/qualitative evaluation of how such a process is advantageous to its counterpart (e.g., iterative). Looking at Figure 7, the text generated by the proposed method does not appear to be very novel compared to the one for the baseline.\n\n- While it could be unfair to directly compare the proposed zero-shot method with supervised approaches, it would still be reasonable to conduct evaluation on the ground truth annotations. Otherwise, it is unclear whether or not the method truly formulates the intermediate logical procedure for problem-solving.\n\n- Despite the superiority of the proposed method reported in Table 1, it appears that there is no clear advantage when considering different approaches independently (i.e., Figure 6).\n\n- The paper only considers a simplified setting of chain-of-thought, where the goal is to generate a single intermediate step based on its two adjacent neighbors. On the other hand, for complicated problems, multiple logical steps are typically required to form the chain-of-thought. How would the proposed approach generalize to these scenarios? More importantly, it is also unclear whether or not it can address out-of-domain reasoning problems, considering its heavy reliance on a pretrained large language model for inferring the intermediate steps."
                },
                "questions": {
                    "value": "(1) What is the performance of the method when evaluating with ground truth annotations? \n\n(2) Can the method handle a longer chain-of-thought? \n\n(3) Please provide additional results to support the effectiveness of the recursive algorithm.\n\n(4) Does the method work on visual reasoning tasks that have a stronger emphasis on logical reasoning?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4191/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698778289193,
            "cdate": 1698778289193,
            "tmdate": 1699636385334,
            "mdate": 1699636385334,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "jqOhCpmbl6",
            "forum": "01Yi8rzoNs",
            "replyto": "01Yi8rzoNs",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4191/Reviewer_a5oz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4191/Reviewer_a5oz"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a method named visual chain-of-thought to infill the logical gaps inside multimodal sequential data. Specifically, this method first generates a global text focus and then recursively generates infilling texts and images by prompting ChatGPT and Stable Diffusion. In experiments, they evaluate their method by humans and show it can outperform the baselines."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The idea of visual chain-of-thought matches the task of multimodal infilling well. \n2. The paper includes a lot of qualitative results and human evaluations against the baseline, which looks promising."
                },
                "weaknesses": {
                    "value": "1. Lack of verification on downstream tasks. Although the authors mention a lot about downstream evaluation in the paper, it mainly focuses on directly evaluating the quality of the generated image/text in the downstream dataset.s However, a more important dimension, IMO, is whether the infilled image/caption is useful for downstream tasks. For example, when adding the generated data as additional training data for visual storytelling and wiki-how, if the performance of the SOTA model  (retrieval/generation) can be further boosted or not? \n2. About the recursive approach to generate the infillings. It's claimed in the paper a lot of advantages of `recursive approach` vs `iterative approach` about dynamics, such as `A recursive approach makes it easier to dynamically determine when infillings are not beneficial to the task and when to stop generating them in contrast to an iterative approach`. However, in Sec. 4.4, the authors admit that a fixed depth is the best in practice, which eliminates the advantages claimed before, especially about dynamically determining/stopping.  \n3. I don't quite get why the recursive approach is called `novelty-driven`. To me, there is nothing emphasizing adding additional novel content in the recursive algorithm.\n4. There are many hyperparameters in the paper, such as the number of text candidates, number of image candidates, number of few-shot examples. However, no ablation experiments are conducted to validate them.\n5. No ablation on the effectiveness of core components of the method, eg, multipoint foveation, novelty/consistency-driven infilling.  \n6. How are the generated multimodal infillings compared with ground-truth quality? It's the upper-bound of this task and should be evaluated too."
                },
                "questions": {
                    "value": "1. The details of COT and COI parallel baseline are missing. \n2. What does the prompt of multipoint foveation look like?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4191/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698781344355,
            "cdate": 1698781344355,
            "tmdate": 1699636385256,
            "mdate": 1699636385256,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "6gUanWgq6H",
            "forum": "01Yi8rzoNs",
            "replyto": "01Yi8rzoNs",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4191/Reviewer_dqPG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4191/Reviewer_dqPG"
            ],
            "content": {
                "summary": {
                    "value": "The paper extends the paradigm of CoT from reasoning in language-only QA tasks to multi-modal (vision-language) data tasks that are complex and involve creativity. \n\nThe proposed method VCoT (Visual Chain-of-Thought) introduces synthetic infillings in temporal sequences that aid in solving downstream tasks. The main goal is to fill \u201creasoning gaps\u201d that exist in temporal sequences. \n\nVCoT is evaluated on two datasets \u2014 Visual Storytelling (Vist) and WikiHow. Humans find that VCoT outperforms text-only-CoT and vision-only-CoT baselines."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Chain-of-Thought based reasoning is now a well-established technique in Question-Answering and reasoning tasks in NLP. The proposed novel multi-modal Visual Chain-of-thought approach demonstrates improved performance on variations of downstream storytelling (VIST) and sequential reasoning task (WikiHow) tasks / datasets. The overall results are evaluated via human studies along multiple relevant dimensions of image and text qualities. \n\nThe two main constraints of novelty and consistency are reasonable \u2014 it prevents redundancy and ensures high relatedness of the generated image with the other images in the sequence. \n\nImages provide additional information that is absent in the textual modality. The synthetically generated Chain-of-Thought text and corresponding images serve as a different source of creative details for downstream tasks like storytelling."
                },
                "weaknesses": {
                    "value": "One of the paper\u2019s major motivations for VCoT is that it provides a human-interpretable insight into the AI system\u2019s reasoning ability. However it\u2019s not clear in what way, and which particular model\u2019s predictions are explained by the proposed, synthetically generated VCoT images.  \n\nSome details of the approach are unclear from the paper. \n\n- E.g., in Eq. (2), the argmax is computed over CLIP similarity scores between the current predicted text t_i, and what other element?\n- The notation in the Equation series (1-5) is also confusing. g appears to be a generative model that predicts text t_i in Eq. (1), and image v_i in Eq. (3). However, an argmax is computed over g(.) in Eq. (4), as if it were a score function.\n- What exactly is the scoring function, using which is  v_i^{best} chosen in Eq. (4) ?\n- It appears that the Equations being referenced in the text of Sec. 3 and Sec. 4.2 might be incorrect.\n\nUnfortunately, the current version of Fig. 2 doesn\u2019t clarify things either. The different blocks in the current color-scheme appear too similar to each other, and distinguishing between them is difficult for this reader. \n\nDuring task unification (of text-only WikiHow data), individual images are generated given individual text instructions. A number of candidate images for the time-step i are generated, and CLIP embedding similarity between the images and the corresponding text t_i is computed. The most relevant image is chosen. While this provides a sequence of images, there is no constraint that the images themselves, are \u201ccoherent\u201d (related to each other). This could result in a sequence of different, apparently unrelated images despite the text being coherent and related to the corresponding images. \n\nIt would be good if it were possible to scale the evaluation to a larger dataset. Currently, evaluation is only based on human feedback. However, this approach only scales at a significant cost. If the authors present an automated method ot evaluate the different aspects of the model, that might be a good contribution that makes the paper stronger. \n\nThe overall approach is rather involved, with multiple blocks / processes that generate the final result. However, the influence of the individual blocks, e.g., \u201cmulti-point foveation\u201d, \u201ctask unification\u201d (+ captioning), etc., are not evaluated or discussed in detail.\n\n\nMinor comments:\n\n- Fig. 3 (left) typo -- (v_i\u2019\u2019, t_i\u2019\u2019) --> (v_i', t_i')? \n- The colors of the different blocks in the architecture diagram in Fig. 2 are too similar to each other."
                },
                "questions": {
                    "value": "1. Are there any ablation studies regarding the influence of the different parts of the overall system?\n\n2. Are there any possible automatic metrics that can be used to (at least approximately) evaluate performance of VCoT? \n\n3. Algorithm 1 seems to describe details wrt \"consistency\". Could you please also explain in detail, the algorithm design that ensures \"novelty\"? \n\n4. A key advantage of VCoT is to reduce logical gaps that are present, e.g., in VIST samples. Is there ny way to quantify the semantic concept of \"logical gap\" with a metric? If automatic metrics are insufficient, then even a metric based on (multiple, averaged) human scores might be useful."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4191/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699060026867,
            "cdate": 1699060026867,
            "tmdate": 1699636385192,
            "mdate": 1699636385192,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]