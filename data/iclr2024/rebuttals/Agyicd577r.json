[
    {
        "title": "BatchPrompt: Accomplish more with less"
    },
    {
        "review": {
            "id": "iLcwXqwqR9",
            "forum": "Agyicd577r",
            "replyto": "Agyicd577r",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission360/Reviewer_Nbfh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission360/Reviewer_Nbfh"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes an efficient prompting technique, BatchPrompt, which batches the input samples into a single prompt to improve the token utilization. While simply batching samples leads to a significant performance drop, this paper introduces Batch Permutation and Ensembling (BPE) and Self-reflection-guided Early Stopping (SEAS) to maintain the generation quality. BPE permutes the data order in each batch and uses majority voting to get the final prediction. SEAS allows early stopping of voting when LLM is confident about the sample. Experiments on some language understanding tasks show BPE+SEAS boosts BatchPrompt performance to be competitive with single-data prompting while using far fewer tokens and API calls."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The idea of BatchPrompt is simple and practical. Using ensemble and early stopping techniques BPE and SEAS to improve performance is novel to me.\n- The paper is clearly written and easy to follow.\n- The work focuses on the important problem of improving the prompting efficiency of LLM inference."
                },
                "weaknesses": {
                    "value": "- The proposed method adds some hyperparameters like batch size and voting rounds for configuration. More analysis could be provided on computational efficiency tradeoffs and how to determine the good hyperparameters\n- The experiments are conducted on language understanding tasks. It would be helpful to evaluate the method on more diverse tasks, e.g. reasoning, knowledge-intensive QA, and creative writing."
                },
                "questions": {
                    "value": "- How to determine good hyperparameters like batch size and voting rounds for BatchPrompt? More analysis could be provided on computational efficiency tradeoffs.\n- It seems that gpt-3.5-turbo suffers from performance degradation when using BatchPrompt, while gpt-4 does not. Is this caused by the model scale? I believe it is helpful to add an analysis of BatchPrompt on the LLaMA series with different model sizes.\n- It would be helpful evaluate the method on more diversed tasks, e.g. reasoning, knowledge-intensive, creative writing tasks. Does the type or difficulty of the instruction affect the performance of BatchPrompt?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission360/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission360/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission360/Reviewer_Nbfh"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission360/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698672517538,
            "cdate": 1698672517538,
            "tmdate": 1699635963214,
            "mdate": 1699635963214,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "AQ62d6CNcn",
                "forum": "Agyicd577r",
                "replyto": "iLcwXqwqR9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission360/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission360/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer Nbfh"
                    },
                    "comment": {
                        "value": "### [Q1]\nThank you. First, we set the batch size 32 and voting rounds 5 as default. We provide the detailed token usage against voting rounds/batch size in Fig. 4, as well as in our \u201cgeneral response to reviewer PJJX, pc53, Nbfh\u201d, with detailed numbers. Detailed explanation for choosing batch size and voting round in our response to R3-Q2. \n\nIn general, hyperparameter choices can be important. The ablation on batch size and voting rounds are intended to illustrate this. While there is no absolute strategy guaranteed to work on all tasks or in all domains, \u201cbatch size 32 and voting rounds 5\u201d is a good default setting, and a short calibration is possible for any task. This process is similar to fine-tuning the parameter of neural networks. We cannot assume the best batch size/epoches when training a neural network at the very begining, but need a validation process to achieve this. To find the best parameters for a specific task, we can use a small validation dataset with ~100 randomly chosen data to validate. For example, a user can start from batch size 16 and 1 voting rounds, and iteratively increase batch size or voting rounds or both, until performance stabilizes, and according to priority of cost-savings and accuracy. The optimal parameters still depend on users\u2019 needs and priorities.\n\n### [Q2]\nThanks to the reviewer for the kind suggestion. GPT-3.5-turbo indeed suffers more from performance degradation compared with gpt-4, and it is caused by the model scale. BatchPrompt performance improves as LLM performance improves.. Also, it has been the trend that more advanced LLMs allow more tokens (32k for Gpt-4, 8k for GPT-3.5-Turbo), which also makes larger batches possible. In this way, every update that increases the available context window of LLMs adds to the leverage now possible with BatchPrompt. \n\nBenchmarking with Llama is a good idea, and will make a good addition to our revision. We agree with your hunch, that a larger more generally-performant model will improve BatchPrompt performance.\n\n### [Q3]\nWe also evaluate the method in GSM8K, which is an arithmetic reasoning task. Also, the task presented in  our \u201cgeneral response to reviewer PJJX, pc53, Nbfh\u201d. \nBatchPrompt should generally be applicable to all close-ended tasks, e.g., reasoning, translation, summarization, etc., while might be difficult to be applied to tasks with open-ended solutions/answers, e.g., text generation, creative writing, etc.\nOn difficulty of instruction, we agree that a difficult-to-answer, or ambiguously-worded instruction could hamper the performance. Finding the best instruction should be important, while might not be the major task for BatchPrompt. We will leave this to future work."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission360/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700174999713,
                "cdate": 1700174999713,
                "tmdate": 1700253972350,
                "mdate": 1700253972350,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Szd7rw2sZI",
            "forum": "Agyicd577r",
            "replyto": "Agyicd577r",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission360/Reviewer_pc53"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission360/Reviewer_pc53"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a method for batching prompts.  Larger batch size generally improve throughput, but degrade performance.  This paper introduced some suggestions (voting rounds and SEAS) to reduce the performance degradation."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper advocates the use of batching for prompting, and may be successful in setting a new trend in that direction."
                },
                "weaknesses": {
                    "value": "I worry about running so many experiments.  The plots in Figure 3 suggest that there are patterns to the results, but even so, if we run lots and lots of experiments and report the best values, the best value could be the result of randomness.\n\nOn the other hand, to make the case for trends, we may need to run even more experiments over more benchmarks, models, batch sizes and so on.\n\nIt would be nice to fit some kind of smooth regression to the results to help with interpretation.  Can you say how performance depends on batch size, voting rounds and model?  An ANOVA would help address concerns above with running so many experiments."
                },
                "questions": {
                    "value": "Can you say more clearly up front that large batches improve throughput, but would degrade performance.  To address performance, you introduce voting rounds and SEAS.  This should also be stated clearly in the conclusions.\n\nThe discussion of the results should address the comments above about interpretation.  The ablation studies show that voting rounds are effective.  But it is hard to see the relation between batch size and performance.  It looks like batch size still reduces performance, even with voting rounds and SEAS.  Is that right?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission360/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission360/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission360/Reviewer_pc53"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission360/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698685039246,
            "cdate": 1698685039246,
            "tmdate": 1699635963120,
            "mdate": 1699635963120,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "mxstQCJK2d",
                "forum": "Agyicd577r",
                "replyto": "Szd7rw2sZI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission360/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission360/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer pc53"
                    },
                    "comment": {
                        "value": "### [Q1]\nGood suggestion. We experimentally find that large batches improve throughput while degrading performance, which can be mitigated through BPE+SEAS. We will clarify this in the intro and conclusion in the next version.\n\n### [Q2]\nThank you for these comments. \n\u201cThe ablation studies show that voting rounds are effective. But it is hard to see the relation between batch size and performance.\u201d\n\u201cCan you say how performance depends on batch size, voting rounds and model? An ANOVA would help address concerns above with running so many experiments.\u201d\n\nIn general, we see that performance improves as number of voting rounds increases - this is natural, as we expect that with more samples for each data point, we converge on the true result (marginalizing out the effect of batch position). We understand that [on their own] larger batch sizes tend to decrease performance \u2014 an effect that is strongly mitigated by multiple voting rounds. Therefore, the optimal batch size is that which achieves the efficiency target, paired with a number of voting rounds sufficient to keep performance above a threshold. We empirically found that batch size 32 was the best choice across tasks. Equivalently, a practitioner could set the number of voting rounds to 5 by default (after which we empirically saw diminishing returns), and continue boosting batch size as long as performance remains high.\n\nFor more details on degradation due to large batch size, we refer to [2307.03172] Lost in the Middle: How Language Models Use Long Contexts (arxiv.org). Due especially to this effect, we find that BPE is an important component to building efficient systems that still perform well."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission360/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700180723174,
                "cdate": 1700180723174,
                "tmdate": 1700253938804,
                "mdate": 1700253938804,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "uRDowjKCav",
            "forum": "Agyicd577r",
            "replyto": "Agyicd577r",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission360/Reviewer_7rFs"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission360/Reviewer_7rFs"
            ],
            "content": {
                "summary": {
                    "value": "For NLP tasks where each data point for inference is not necessarily lengthy, the token count for instructions and few-shot examples in the prompt may be considerably larger than that of the data point, resulting in lower token-resource utilization. This paper try to alleviate the preceding problem by batching multiple data points into a single prompt, a prompting strategy we refer to as \u201cBatchPrompt\u201d. This strategy increases the \u201cdensity\u201d of data points, which in turn leads to improved token utilization, which shows promising fulture."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.\tBatchprompt could highly improve token-resource utilization\n2.\tBPE could effectively It can effectively reduce the error rate caused by the different position in a batch.\n3.\tSEAS could effectively reduce the amount of unnecessary calculations"
                },
                "weaknesses": {
                    "value": "1.\tIt seems that each item in the new batch (with only one prompt) could not be computed parallelly as original. Whether it will increase the time cost? It might be better to add time and flops metrics in the experiments.\n2.\tI think the \u201cbatchprompt\u201d could be used in both training and test phases, right?\n3.\tIn BPE, the weight for confidence is directly 1. What about to generate the weights scores directly by the LLM without whether confident?"
                },
                "questions": {
                    "value": "Please see the weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission360/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698980757306,
            "cdate": 1698980757306,
            "tmdate": 1699635963047,
            "mdate": 1699635963047,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "g1Bn2z0EuZ",
                "forum": "Agyicd577r",
                "replyto": "uRDowjKCav",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission360/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission360/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer 7rFs"
                    },
                    "comment": {
                        "value": "## [Q1]\nThank you for the question. As our motivation is frugality for LLMs, we initially did notconsider the time cost. We use OpenAI API, and inference time for GPT can depend on number of concurrent users, which is not stable. A statistic can be found below: https://gptforwork.com/tools/openai-api-and-other-llm-apis-response-time-tracker . A detailed comparison of response time against tokens:\n\nhttps://community.openai.com/t/gpt-3-5-and-gpt-4-api-response-time-measurements-fyi/237394 . Also, as we are directly using pre-trained LLMs (gpt-3.5-turbo, gpt-4), we might not be able to calculate an accurate number of flops. \n\nIn the link above, we see that response time is in direct proportion to number of completion tokens. Since we use many fewer tokens compared with SinglePrompt (see token comparison in Table 2-4, Fig. 4), we will use proportionally less time. \n\n## [Q2]\nVery interesting idea. We did not consider BatchPrompt for training efficiency, only for robust inference. This could be a nice area of future research.\n\n## [Q3]\nRight. Weights can be generated by the LLM. We simplify to 1 and \\alpha, for sake of exposition. We can also generate a score directly. Please refer to Appendix D, where we have a description:\n\n```\n[Conf-Description]: \u2019You not only need to generate The label/answer, but also your confidence. If you are confident in your output class, append a \u201d(confident)\u201d at the end of the label; else, append a \u201d(not confident)\u201d.\u2019 \n```\n\nHere, we could  change this description to:\n\n```\n[Conf-Description]: \u2019You not only need to generate The label/answer, but also your confidence. Please append a confidence score between 0 and 1 at the end of the label to represent your confidence in the result you generate.\n```\n\nThen we can multiply the score with the corresponding result, and select the result with maximum score as final output in the majority voting phase. \n\nWe experimented with allowing a range of confidence values, but the results did notchange. Therefore we chose the binary representation. We will explain this in the next paper version."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission360/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700084879261,
                "cdate": 1700084879261,
                "tmdate": 1700085899229,
                "mdate": 1700085899229,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1zHz9hxnoO",
            "forum": "Agyicd577r",
            "replyto": "Agyicd577r",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission360/Reviewer_PJJX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission360/Reviewer_PJJX"
            ],
            "content": {
                "summary": {
                    "value": "The paper describes a method to improve resource utilization by increasing the 'density' of user query tokens using batching the queries. User queries exhibit lower token utilization compared to the system prompts and/or few shot examples that goes with the query. Authors point out that this is not cost efficient and the 'batchprompt' method requires less LLM calls and better user query token utilization (saving the overall numbers of tokens in a batch which in effect is more cost-efficient). \n\nBatch prompting makes the LLM generation task n times harder for batch size n since the LLM needs to generate n outputs corresponding the n packed queries. Authors conduct experiments and show that this significantly degrades performance, and the order of the packed queries also significantly impact the performance. \n\nAuthors develop a batch permutation and ensembling method (to utilize voting from repeated permutations) - this slightly increases the token count and increases the LLM calls (still much less compared to single prompt inference) however improves the performance. Further improvement is realized with 'self reflection guided early stopping (SEAS) scheme) where the generator is also asked to provide the confidence of the result and using rules, the system stops the voting procedure early.\n\nAuthors have performed experiments on 3 datasets (yes/no question answering, entailment, paraphrase detection) and shown that with a batch size of 32, and using BPE and SEAS. the accuracies on 3 datasets do not degrade (improve slightly)."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Authors propose a robust method that uses larger batch size, more voting rounds (eg. 5+) and a self-reflection guided early stopping approach.\n\n- The early stopping method also uses a pruning strategy to prune away confident predictions leaving fewer/harder samples for later\nrounds. In the process, the harder samples might also become easier to predict, due to smaller effective batch size in later rounds. \n\n- via experiments, authors show that voting is most successful when the baseline performance is strong (for example, gpt4 vs. gpt3.5)"
                },
                "weaknesses": {
                    "value": "- Authors chose small number of tasks (only 3 simple tasks (yes/no QnA, paraphrase detection and entailment detection) -> these tasks may be too easy for gpt3.5 and gpt4 systems \n\n- Results are shown using few experiments (~300 dataset queries each for the 3 datasets); typically a validation on more tasks and more datasets would have helped get a more confident understanding of the approach. \n\n- this is a nice applied research paper with good results and a principled approach for improving cost efficiency, however there are many variables to unpack (quality and length of tasks, mixing different types of instructions, performance on novel datasets not seen by the LLMs, solving position bias discrepancy via BPE with more experiments and results, role of prompt variations on the results, etc)"
                },
                "questions": {
                    "value": "- All tasks are very short answer type tasks, using tasks that generate longer answers might be very hard to experiment using the batchprompt approach. Couldn't see a discussion on this topic in the paper. Thoughts?\n\n- it is not clear how this system would be used with all its advantages in a deployment scenario - batching real world prompts with very different instructions might have unpredictable behavior, any thoughts?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission360/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698983813806,
            "cdate": 1698983813806,
            "tmdate": 1699635962985,
            "mdate": 1699635962985,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ayIU9agXkF",
                "forum": "Agyicd577r",
                "replyto": "1zHz9hxnoO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission360/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission360/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer PJJX"
                    },
                    "comment": {
                        "value": "## [Q1] \nSorry for the lack of clarity. BatchPrompt can be used for all close-ended tasks. Task answers need not be short, but must have the same format in each voting round, in order to determine agreement. This is required for our ensemble (majority voting) step. We achieve this goal through task specification. \n\nPlease note the prompt in Appendix D. For all three tasks in the paper, we write the following description at the end our task specification:\n\n```\nBelow are the outputs you need to generate. \u201dX\u201d can be \u20190\u2019 or \u20191\u2019. \nLabel for Input 0: [class X] [Place-Holder-Conf] \nLabel for Input 1: [class X] [Place-Holder-Conf] ......\n\nPlease make sure each generated label is in format of [class X].\nPlease make sure to generate [BATCH-SIZE] labels. You may include other additional sections here.\n```\n\nThe motivation for writing this is to encourage the model to map answers of varied formats  to a binary label, and to unify the format of the answer to make majority voting possible.\n\nHowever, what if the answer cannot be binary label?\n \nPlease note the work in Appendix C. We provide a general experimental result on GSM8K, the arithmetic reasoning task, whose answers cannot be a binary label. The answer to the arithmetic reasoning task can be long chain-of-thought answers. That said, we can still use the task specification to map the answer to a specific number/format, enabling a majority vote. In Appendix D, we provide prompts for GSM8K: \n\n```\nPlease make sure to write your a series of intermediate reasoning steps. Please make sure the final sentence is \u201dThe answer is xxx.\u201d, and the answer should be a number. Please make sure to generate [BATCH-SIZE] labels each time. \n```\n\nWe add this at the end of each task specification to make answer formats consistent in each voting round, and make ensembling possible.\n\nWe recognize that for open-ended tasks like text generation, it is not obvious how BatchPrompt could be applied. BatchPrompt requires an \u201cequality\u201d function to enable majority voting. Suppose two open-ended responses are considered to \u201csay the same thing\u201d. By providing this kind of semantic equality function, BatchPrompt can be extended to this task. We leave this as future work.\n\n## [Q2]\nIf we understand correctly, the question is about batching different tasks into each model call, e.g. \n\n```\n\u201cInstruction #1, Question #1, Instruction #2, Question #2, \u2026\u201d\n```\n\nversus\n\n```\n\u201cInstruction #1, Question A, Question B, Question C, \u2026\u201d\n```\n\nIn the former case, we lose the token-saving benefit from sharing instructions, but still get other robustness and cost-saving benefits from permutation, ensembling, and early stopping. BatchPrompt is designed for processing large volumes of data where the LLM performs a specific function, e.g. \u201clabel sentiment of all emails in an organization\u201d, or \u201cmap each request to one of k functions/tools/teams\u201d. We expect LLMs will take many such roles in software systems. \n\n## [Answer to ~300 dataset queries each for the 3 datasets]\nThank you for bringing this up - we will add clarification.. Each benchmark dataset is already divided into train/validation/test by Hugging Face, and we directly use the validation data in our experiments. We do not, however, use the full validation set for two reasons: (1) Quota constrains our ability to test, and (2) Data containing sensitive content cannot be used with gpt-3.5-turbo/gpt-4.  Therefore, we use gpt-3.5-turbo to filter out sensitive content, and randomly choose 320 samples for Boolq, QQP, RTE.\n\n## [Answer to small number of tasks]\nOne more task, on arithmetic reasoning, can be found in Appendix D. We provide three representative tasks in the main paper, but emphasize that BatchPrompt can be used in any task with close-ended answer, as mentioned in the answer to Q1.\n\nTo increase confidence, we will add the following results on more datasets, with long answers, and where all validation (>300) data is used. This can be found above, as in our \"general response to reviewer PJJX, pc53, Nbfh\".\n\n## [Answer to these tasks may be too easy]\nThe major motivation for BatchPromptis frugality, while maintaining base-level accuracy (i.e. \u201cSinglePrompt\u201d), for tasks of any difficulty. We do not seek to improve the accuracy of each task, but keep performance stable, while saving energy and money via fewer tokens and LLM calls.  That said, permutation and ensembling might be even more useful in cases of hard tasks, where repeated samples lower the risk of picking an incorrect answer. This related topic has been studied in: [2203.11171] Self-Consistency Improves Chain of Thought Reasoning in Language Models (arxiv.org)\n\n## [Answer to many variables to unpack]\nThank you for the detailed comment and feedback. We indeed have many  parameters/variables, but one easy way to use BatchPromptis to simply use BatchPrompt+BPE+SEAS with default parameters, e.g., set batch size to 32, maximum voting time to 7, and LLMs temperature to 0."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission360/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700084091827,
                "cdate": 1700084091827,
                "tmdate": 1700086192108,
                "mdate": 1700086192108,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]