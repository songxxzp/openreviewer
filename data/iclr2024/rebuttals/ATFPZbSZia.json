[
    {
        "title": "Grouplane: End-to-End 3D Lane Detection with Channel-Wise Grouping"
    },
    {
        "review": {
            "id": "cfLdLKcVJP",
            "forum": "ATFPZbSZia",
            "replyto": "ATFPZbSZia",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1158/Reviewer_5Kts"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1158/Reviewer_5Kts"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a novel convolutional neural network for end-to-end 3D lane detection. First, the approach splits BEV features into vertical and horizontal groups along the channel dimension. Then, group convolution is applied to both vertical and horizontal groups to extract further features. During the training phase, the authors propose a SOM strategy to match ground truth and predictions. Finally, 3D lanes are detected by performing row-wise classification. Notably, this method achieves state-of-the-art performance in the 3D lane detection task on 3 benchmarks, OpenLane, Once-3DLanes, and OpenLane-Huawei."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Idea seems fundamentally sound.\n2. Spliting BEV feature for vertical and horizontal lane detection would be valuable, espeically when model is deployed on an edge device.\n3. Paper is well written and very easy to read."
                },
                "weaknesses": {
                    "value": "1. Simply dividing each group into N outputs limited the max output lane number of the model.\n2. SOM strategy is simple yet effect, details are not well explained or even missing, eg. the matching cost definition."
                },
                "questions": {
                    "value": "1. It is unclear what will happen when both the vertical and horizontal heads match the ground truth (GT). The paper does not provide a clear explanation or analysis of this scenario.\n2. The paper does not address how to ensure stable predictions. It raises concerns about the possibility of different heads predicting the same lane at different time and one of them is not the optimal prediction. \n3. Can not find the matching cost definition or loss functions in Section 3.4."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1158/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1158/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1158/Reviewer_5Kts"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1158/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698048980599,
            "cdate": 1698048980599,
            "tmdate": 1699636042245,
            "mdate": 1699636042245,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "DoFe9QVbjj",
                "forum": "ATFPZbSZia",
                "replyto": "cfLdLKcVJP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1158/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1158/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 5Kts"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the effort and valuable feedback.\n\n**Q1: Simply dividing each group into N outputs limited the max output lane number of the model.**\n\nA1: Firstly, it is important to note that the number of lanes on real roads is typically limited. In most cases, there are at most tens of lanes on a road. Therefore, having an upper bound on the prediction number does not significantly impact the practical deployment of GroupLane in real-world applications.\n\n&ensp;&ensp; Secondly, it is worth mentioning that almost all current lane detectors also have an upper bound on the prediction number. For example, the maximum prediction number of DETR is determined by the number of queries, while anchor-based detectors have a maximum prediction number based on the total number of anchors.\n\n&ensp;&ensp; In comparison to other algorithms, GroupLane offers several advantages, including being more lightweight, faster, and demonstrating better precision. Consequently, it is relatively straightforward to increase the maximum prediction number of GroupLane beyond that of other detectors if necessary.\n\n&ensp;&ensp; Considering the aforementioned points, we firmly believe that having a maximum prediction number does not undermine the superiority of GroupLane. The model's performance and efficiency, combined with the practical limitations of lane numbers on real roads, make GroupLane a highly effective solution for lane detection tasks.\n\n**Q2: SOM strategy is simple yet effect, details are not well explained or even missing, eg. \nthe matching cost definition.**\n\nA2: We apologize for the oversight in not providing sufficient details on the SOM strategy in the paper. We have provided a more comprehensive explanation of the matching cost definition and other relevant details in Appendix Section A.4 of the paper, which has been highlighted in cyan. We do not present it here because it is too long and the reply box has a character limit. We hope the reply can address the concern of the Reviewer well.\n\n**Q3: It is unclear what will happen when both the vertical and horizontal heads match the ground truth (GT). The paper does not provide a clear explanation or analysis of this scenario.**\n\nA3: It seems that there are some misunderstanding about our method. In fact, the single-win one-to-one matching strategy employed in our approach explicitly matches one GT with only one prediction from the vertical and horizontal heads (details can refer to Section 3.3 in the paper). This ensures that the scenario where both heads match the same GT does not exist in our framework.\n\n\n**Q4: The paper does not address how to ensure stable predictions. It raises concerns about the possibility of different heads predicting the same lane at different time and one of them is not the optimal prediction.**\n\nA4: The issue of unstable predictions is not a concern in GroupLane. The matching process employed in GroupLane is based on the same principles as the DETR algorithm [1]. The one-to-one matching algorithm ensures that only the predictions that are most similar to the ground truth (GT) are matched. Any predictions that do not match any GT are treated as background.\n\nIf a matched prediction for a lane is not of sufficient quality, the loss function will penalize the prediction and encourage the model to generate a better one. This mechanism ensures that the algorithm remains stable by continuously refining and improving the predictions.\n\n[1] Carion N, Massa F, Synnaeve G, et al. End-to-end object detection with transformers[C]//European conference on computer vision.  2020: 213-229."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1158/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700234386583,
                "cdate": 1700234386583,
                "tmdate": 1700234386583,
                "mdate": 1700234386583,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "KGTq1kHzYP",
            "forum": "ATFPZbSZia",
            "replyto": "ATFPZbSZia",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1158/Reviewer_dGoq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1158/Reviewer_dGoq"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a end-to-end 3D lane detection from a single image. The proposed model is based on technical contributions: (1) a splitting strategy that build several groups of features to represent a line, (2) two groups of heads to recognize, in the bird-eye-view (BEV), horizontal and vertical lines. The resulting model is evaluated on three public benchmarks and outperform existing models."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is nicely written and easy to read. The main contribution, to my point of view, consists in splitting the BEV features into two groups of candidates: horizontal candidates and vertical candidates. Each group has 6 heads to predict existence confidence, visibility, category, row-wise classification index, x-axis offset, and z-axis offset. Since the proposed model splits the group of candidates in horizontal and vertical, the authors proposed an adapted technic called single-win one-to-one matching (SOM) to match each candidate with the training labels."
                },
                "weaknesses": {
                    "value": "In the experimental part, GROUPLANE is evaluated on three datasets. The selected baseline model is PersFormer (described as the best published model). Can you give details on this choice? Regarding the benchmark webpage, it seems that the best 2022 model is 58% F1 score and that PersFormer is currently ranked 9. The resulting figure 2 is not fair and should be changed with new models.\nMoreover, can you add the two following references (ranked 1 and 2) from ICCV2023: \nLATR: 3D Lane Detection from Monocular Images with Transformer\nPETRv2: A Unified Framework for 3D Perception from Multi-Camera Images\n\nIn the ablation study, the authors compare the Horitontal/Vertical grouping strategy with only a vertical strategy. The proposed strategy increases about 5% the F1 score. It should be interesting to give information on the horizontal/vertical ratio of lines of the dataset. Moreover, it could be interesting to split the results into vertical/horizontal lines."
                },
                "questions": {
                    "value": "Can you give details on the choice of PersFormer as the baseline for figure 2 and table 3?\nCan you give information on the horizontal/vertical ratio of lines of the dataset used for table 6?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1158/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698653564844,
            "cdate": 1698653564844,
            "tmdate": 1699636042167,
            "mdate": 1699636042167,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "g1ksRTBgYk",
                "forum": "ATFPZbSZia",
                "replyto": "KGTq1kHzYP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1158/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1158/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer dGoq"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the effort and valuable feedback.\n\n**Q1: In the experimental part, GROUPLANE is evaluated on three datasets. The selected baseline model is PersFormer (described as the best published model). Can you give details on this choice? Regarding the benchmark webpage, it seems that the best 2022 model is 58% F1 score and that PersFormer is currently ranked 9. The resulting figure 2 is not fair and should be changed with new models. Moreover, can you add the two following references (ranked 1 and 2) from ICCV2023: LATR: 3D Lane Detection from Monocular Images with Transformer PETRv2: A Unified Framework for 3D Perception from Multi-Camera Images.**\n\nA1: We apologize for the oversight in not considering more recent publications, as this work was completed before the ICCV2023 conference. We appreciate you bringing this to our attention. We compare our method with PersFormer in Figure 2 of the paper for two reasons: (1) PersFormer is the published SOTA when we completed this work. (2) Both GroupLane and PersFormer produce BEV feature explicitly, while the current SOTA method LATR is not BEV based.\n\n In the revised version of our paper, we have made the necessary updates. We remove all sentences that claim PersFormer is the published SOTA, and add the reason why we choose PersFormer to compare in Figure 2 to Appendix A.7. The results of PETRv2 and LATR are added to Table 1 (Page 6) of the paper for performance comparison. The references you mentioned are also added (page 2 and Page 3). The updated content is marked in blue.\n\n**Q2: In the ablation study, the authors compare the Horitontal/Vertical grouping strategy with only a vertical strategy. The proposed strategy increases about 5% the F1 score. It should be interesting to give information on the horizontal/vertical ratio of lines of the dataset. Moreover, it could be interesting to split the results into vertical/horizontal lines.**\n\nA2: We apologize for the lack of clarity in the paper. Here, we provided the following details:\n\nIn the OpenLaneV2 dataset, approximately 71% of lane instances are vertical lanes, while the remaining 29% are horizontal lanes.\n\nFollowing your suggestion, we conducted separate evaluations for horizontal and vertical lane detection performances in GroupLane. For the evaluation of horizontal lane detection, we removed the vertical lane labels and predictions. Similarly, for the evaluation of vertical lane detection, we removed the horizontal lane labels and predictions. The results are presented below:\n\n\n| H-SOM | Test Mode | F1 Score| Recall | Precision | DET-L |\n| :-: | :-: | :-: | :-: | :-:| :-:|\n| No | Vertical | 43.29% | 40.92% | 45.96% | 23.49% |\n| No | Horizontal | 19.71% | 16.07% | 25.47% | 1.01% |\n| No | Overall | 31.00% | 27.51% | 35.51% | 9.34% |\n| Yes | Vertical | 43.10% | 40.66% | 45.86% | 22.01% |\n| Yes | Horizontal | 30.11% | 27.81%  | 32.82% | 10.94% |\n| Yes | Overall | 35.82% | 32.67% | 39.64% | 17.07% |\n\nAs observed from the results, the performance of horizontal lane detection is higher compared to that of vertical lanes. This difference can be attributed to the fact that the dataset contains a larger number of vertical lanes compared to horizontal lanes. The imbalanced distribution of lane types in the dataset influences the performance metrics.\n\nWe have added this experiment analysis to the paper, which is presented in Appendix A.6 and marked in blue."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1158/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700234326291,
                "cdate": 1700234326291,
                "tmdate": 1700234326291,
                "mdate": 1700234326291,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VrVSPZnWmb",
                "forum": "ATFPZbSZia",
                "replyto": "bJO2rAaUj7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1158/Reviewer_dGoq"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1158/Reviewer_dGoq"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for the answers and additional experiments.  \nI think there is a mistake in the answer :\n\"As observed from the results, the performance of horizontal lane detection is higher compared to that of vertical lanes\"\nshould be \n\"As observed from the results, the performance of VERTICAL lane detection is higher compared to that of HORIZONTAL lanes\"\n\nRegarding PersFormer as SOTA, I appreciate that the sentences have been removed. When comparing your contribution to SOTA on a public dataset with an open competition, you should mention the exact date of the comparison. Moreover, you should mention bot the ranking of your model in the overall models and the ranking of your model in the same class of models (BEV based solutions). \n\nBest"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1158/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700726374319,
                "cdate": 1700726374319,
                "tmdate": 1700726374319,
                "mdate": 1700726374319,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "EvzhV2lSAH",
            "forum": "ATFPZbSZia",
            "replyto": "ATFPZbSZia",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1158/Reviewer_RkdN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1158/Reviewer_RkdN"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces anchor-based 3D lane detection utilizing channel-wise grouping features. Additionally, the authors propose a single-win one-to-one matching method that associates a grid belonging to vertical or horizontal lanes. The detection heads predict the existence, visibility, row index, lane category, and offset of lane points to grid centers. The paper provides extensive experimental results, demonstrating high performance on various lane detection benchmarks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Provide test results on various datasets and achieved high performances."
                },
                "weaknesses": {
                    "value": "- The ultra-fast deep lane detection method has already introduced a hybrid anchor-based lane detection that predicts row-and-column anchors corresponding to lanes.\n- It is interesting to note that Table 1 and Table 2 exhibit inconsistent results when using different backbone models. It would be nicer if the authors further investigated this issue.\n\nZ. Qin, P. Zhang and X. Li, \"Ultra Fast Deep Lane Detection With Hybrid Anchor Driven Ordinal Classification,\" in IEEE TPAMI, 2022."
                },
                "questions": {
                    "value": "."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1158/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1158/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1158/Reviewer_RkdN"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1158/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698777213140,
            "cdate": 1698777213140,
            "tmdate": 1699636042077,
            "mdate": 1699636042077,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "45p8xLgBFf",
                "forum": "ATFPZbSZia",
                "replyto": "EvzhV2lSAH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1158/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1158/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer RkdN"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the effort and valuable feedback."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1158/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699689960689,
                "cdate": 1699689960689,
                "tmdate": 1699863294930,
                "mdate": 1699863294930,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HgIEx0KdEg",
                "forum": "ATFPZbSZia",
                "replyto": "EvzhV2lSAH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1158/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1158/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer RkdN"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the effort and valuable feedback.\n\n**Q1: The ultra-fast deep lane detection method has already introduced a hybrid anchor-based lane\ndetection that predicts row-and-column anchors corresponding to lanes.**\n\nA1: Please kindly note that we had cited the method of UFLD as a reference (in the 2nd page: Qin et al., 2020). To address you concern, we give a deep comparison here.\n\n* Though UFLD proposes row-wise classification lane detection, its application is limited to 2D lane detection, which is significantly different from that in 3D lane detection.\n\n* To successfully apply the row-wise classification lane detection to 3D lane detection, we have conducted multiple improvements:\n\n &ensp;&ensp; (1) First of all, the row-wise classification lane detection method is conducted on the BEV plane, instead of the camera plane (adopted by UFLD). This is because we consider the geometry of lanes is simpler in the BEV plane and thus easier to learn as shown in Figure 6 of the paper.\n\n&ensp;&ensp; (2) Secondly, we build two groups of heads, one for horizontal lanes detection and the other for vertical lanes detection. \n\n&ensp;&ensp; (3) Thirdly, a novel one-to-one matching algorithm named single-win one-to-one matching (SOM) is developed to match predictions with targets during training for computing loss.  \n    \n&ensp;&ensp; As a result, although UFLD behaves poorly compared with other counterparts in the 2D lane detection domain, our GroupLane achieves strong performance.\n\n&ensp;&ensp; The above discussion has been included in our paper (page 2 and page 3, marked in brown).\n\n**Q2: It is interesting to note that Table 1 and Table 2 exhibit inconsistent results when using\ndifferent backbone models. It would be nicer if the authors further investigated this issue.**\n\nWe thank the Reviewer for the suggestion. We have conducted experimental results and analysis as suggested in the follows to address your concern.\n\n* In the paper, we hypothesize the reason that using ResNet18 behaves better than ResNet50 in the Once-3DLanes dataset is Once-3DLanes does not require classifying lanes and only localization is needed. By contrast, OpenLane requires both classification and localization. Therefore, the main point causing the inconsistency between Table 1 and Table 2 is that **increasing backbone parameter volume primarily benefits classification rather than localization**. To verify this speculation, we remove the classification information in OpenLane. Specifically, we change all the lane category labels in OpenLane as *zero*. Since all lanes share the same class ID, no classification is needed for the detector. In this experimental setting, we validate the performances of GroupLane taking different backbones and the results are as follows.\n\n| Backbone | ResNet18 | ResNet50 | ConvNext-Base |\n| :-: | :-: | :-: | :-: |\n| F1 Score |  71.8%  | 71.7% | 71.2% |\n\n&ensp;&ensp;&ensp; According to the results in the table, a similar performance degradation appearing in OpenLane after removing classification. Thus, our hypothesis holds.\n\n* In addition, We hypothesize the deeper reason is that: with more parameters in a backbone, better classification capability is obtained from the ImageNet pre-training, which is also a classification task. However, it does not boost the localization ability.\n\nThe analysis on this issue has also been updated to the Appendix A.5 of the paper and is marked in brown."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1158/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700234274966,
                "cdate": 1700234274966,
                "tmdate": 1700234274966,
                "mdate": 1700234274966,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Up92VUMWHD",
                "forum": "ATFPZbSZia",
                "replyto": "SyuMx4vpjZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1158/Reviewer_RkdN"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1158/Reviewer_RkdN"
                ],
                "content": {
                    "comment": {
                        "value": "I read the authors' feedback and the additional experiments clear the uncertainties."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1158/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700714510002,
                "cdate": 1700714510002,
                "tmdate": 1700714510002,
                "mdate": 1700714510002,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "q6kZREP8F4",
                "forum": "ATFPZbSZia",
                "replyto": "EvzhV2lSAH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1158/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1158/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to the Reviewer RkdN"
                    },
                    "comment": {
                        "value": "Dear Reviewer RkdN,\n\nThank you for your valuable feedback on our paper. We are grateful for the time and effort you have put into reviewing our work. We have carefully considered all of your concerns and have made significant revisions to address them.\n\nAs presented in the paper, this work proposes a 3D lane detector achieving very promising performance and efficiency. Besides, this is the first time of deploying row-wise classification in 3D lane detection. For the first time, a fully CNN based detector can realize end-to-end detection like DETR. This is also the first time that the detecting horizontal lane problem is considered in lane detection.\n\nWe kindly ask if there are any remaining concerns that need to be addressed in order to improve the rating of our paper. We believe that the revisions we have made have significantly improved the quality and contribution of our work. Thank you once again for your time and effort in reviewing our work. We look forward to hearing back from you.\n\nBest regards,\n\nThe Authors"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1158/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700715953576,
                "cdate": 1700715953576,
                "tmdate": 1700716520771,
                "mdate": 1700716520771,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]