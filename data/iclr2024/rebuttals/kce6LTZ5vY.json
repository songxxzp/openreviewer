[
    {
        "title": "Instruction Mining: Instruction Data Selection for Tuning Large Language Models"
    },
    {
        "review": {
            "id": "OmLy2WZ5Ww",
            "forum": "kce6LTZ5vY",
            "replyto": "kce6LTZ5vY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4082/Reviewer_CNR2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4082/Reviewer_CNR2"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a technique, InstructionMining, to analyse the performance of a set of instruction-tuned models to identify the features that best correlate with model performance (as measured by loss on a validation set). They also apply BlendSearch to find the optimal data size to select. They show that selecting data with InstructMining results in models that perform as well or better than models trained on random data and much larger datasets (e.g. StableBeluga). They also find that dataset size and model performance display double descent, where increasing data helps, then hurts, then helps again, counter to intuition (that more data is always better)."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The method is sound, and investigating how to determine the \u2018quality\u2019 of a dataset without training a model is interesting and impactful for the community. The exploration of different indicators (and ablating them in 4.3) is interesting and has some useful takeaways, suggesting that just examining the naturalness and understandability of responses is enough to detect quality data.\n- The GPT -4-based preference results suggest that the model trained on the InstructMining selected data performs better than random selection, and is similar in quality to Vicuna 1.5 7B.\n- The results investigating dataset size and model performance are interesting and provide interesting guidance for future efforts in collecting data, as well as (partially) validating the LIMA hypothesis that fewer, but higher-quality, samples are preferable to a large amount of data."
                },
                "weaknesses": {
                    "value": "- The method is computed over loss, but it is unclear how this translates to eval task performance. Appendix C presents a somewhat weak argument - it would be good to e.g. strongly correlate the inference loss with MMLU/MT-Bench/etc results. It\u2019s non-obvious that this is true in instruction tuning setups due to the lack of hard gold labels. There may be many potential outputs that are equally valid, and only one is generated as the gold label by GPT-4. This also makes it unclear to determine what differences in loss are significant - for example, in Table 3, Random 150(Train) gets .0085 higher loss than Selected 22(Rule) + 150(Train). Is this difference significant? Does it actually translate to downstream task gains?\n- The lack of multiple random seeds/error bars with the experiments makes it unclear how well InstructMining is performing over randomly selected data. Looking at the results in Table 4 especially, it seems that randomly selecting data outperforms InstructMining-Selected at the 10k size, and only underperforms the Selected model by .7 at the 40k size, which might not be a significant difference. While it\u2019s expensive to run multiple trials, especially for larger data sizes, I think it\u2019s important to make sure that the claimed performance improvements are actually significant.\n- Missing baselines: it would be good to compare against models trained on all data from the different base datasets: just open orca, just dolly, and the two combined. This would allow us to examine how the InstructMining models compare to models trained on all available data while keeping the finetuning setup consistent. Currently, the main baselines are Vicuna, Llama-chat, and StableBeluga, none of which were trained on OpenOrca or Dolly.\n- This is minor, but the names used in Table 4 could be explained better. I\u2019m assuming InstructMining-Random is randomly selecting over OpenOrca and Dolly.\n\nOverall, I like the work and think the proposed method is interesting and insightful, but I worry that the results may not be as clear-cut as presented in the work, due to the limited evaluation setup (a focus on loss, and only minor differences in performance on downstream tasks in table 4)."
                },
                "questions": {
                    "value": "- How large/small of a difference in loss is needed to translate to significant performance differences in downstream tasks?\n- Do you have results with multiple trials for the random selection? Does this change any findings e.g. in the double descent analysis?\n- In section 5.2, are you using the instruction rule derived from the 7b model for the 13b data selection?\n- How do the InstructMining-trained models compare to models just trained on either Dolly or OpenOrca (or both together)? StableBeluga\u2019s finetuning set has not been publicly released at the time of writing (as far as I know), so these would be good baselines to have."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4082/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698610735178,
            "cdate": 1698610735178,
            "tmdate": 1699636372981,
            "mdate": 1699636372981,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5r52tZYVnY",
                "forum": "kce6LTZ5vY",
                "replyto": "OmLy2WZ5Ww",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4082/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4082/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We apologize for the delayed response. Please note that some of our experiments and evaluations require several days to complete, which may result in a brief wait time before we are able to provide a reply. Thank you for your patience and we appreciate your time and effort in reviewing our paper and providing valuable feedback. We are pleased to hear your positive remarks regarding the novelty and practical applicability of our method for selecting high-quality instruction following data. In response to your concerns, we have outlined our comments below:\n\n- **Q.1 Difference in loss:**\n\nIt\u2019s hard to say the numerical relevance between loss and performance difference, here is the OpenLLM leaderboard result for random and selected 10k data of dolly.\n\n|  Model Name  | Datasize | loss@self-instruct | loss@mt-bench | lm-eval-avg | ARC    | Hellaswag | MMLU   | TruthfulQA |\n| -------------- | -------- | ------------------ | ------------- | ----------- | ------ | --------- | ------ | ---------- |\n| dolly random   | 10000    | 1.0356             | 0.8086        | 0.5486      | 0.5733 | 0.8116    | 0.4266 | 0.3828     |\n| dolly Selected | 10000    | 1.0371             | 0.8001        | 0.5495      | 0.5682 | 0.8123    | 0.4392 | 0.3782     |\n\nHowever, we would like to point out that, during our experimentation, we feel that loss on mt-bench, also LLM-as-a-judge, is only able to evaluate a model\u2019s conversational ability, while a language model still needs other abilities, e.g., question answering. And this is why we add OpenLLM benchmarks. From our point of view, Mt-bench and MMLU are two separate evaluation benchmarks, which does not correlate much. For example, a response below could yield high LLM-as-a-judge score and low loss, however, it is not a correct answer to this question[1]. \n\n```\nInstruction: Sort the following list into alphabetical order. apple, banana, orange, grape.\n\nWrong Answer: No problem! Here\u2019s the sorted list. Grape, apple, banana, orange.\n\nCorrect Answer: apple, banana, grape, orange.\n```\n- **Q.2 Results on random selection experiments.**\n\nThanks for pointing out. We will update more random results. These results and analysis will be added to our paper.\nBelow is the detail for the appended random experiment results.\n\n| Model Name | Data size | Loss@self-instruct | Loss@mt-bench |\n| ------------------- | ----- | ----- | ----- |\n| figure1 orca Random | 1000  | 1.001 | 0.746 |\n| orca Random         | 1000  | 0.974 | 0.715 |\n| orca Random         | 1000  | 0.970 | 0.715 |\n| figure1 orca Random | 2000  | 0.966 | 0.711 |\n| orca Random         | 2000  | 0.980 | 0.713 |\n| orca Random         | 2000  | 0.990 | 0.713 |\n| figure1 orca Random | 3000  | 0.993 | 0.727 |\n| orca Random         | 3000  | 1.013 | 0.728 |\n| orca Random         | 3000  | 1.000 | 0.736 |\n| figure1 orca Random | 4000  | 1.014 | 0.752 |\n| orca Random         | 4000  | 0.999 | 0.751 |\n| orca Random         | 4000  | 1.015 | 0.728 |\n| figure1 orca Random | 5000  | 0.991 | 0.751 |\n| orca Random         | 5000  | 0.999 | 0.738 |\n| orca Random         | 5000  | 0.998 | 0.734 |\n| table4 orca Random  | 10000 | 1.020 | 0.750 |\n| orca Random         | 10000 | 1.012 | 0.757 |\n| orca Random         | 10000 | 1.003 | 0.742 |\n| table4 orca Random  | 40000 | 1.015 | 0.734 |\n| orca Random         | 40000 | 1.019 | 0.732 |\n| orca Random         | 40000 | 1.011 | 0.754 |\n\nWe summarize this table in to the analysis below:\n\n| Datasize | Metric                    | Average | Std.           | Value       |\n| -------- | ------------------------- | ------- | -------------- | ----------- |\n| 1000     | avg/std of loss           | 0.981   | 0.0169  | 0.981\u00b10.017 |\n| 1000     | avg/std of loss(mt bench) | 0.725   | 0.0178  | 0.725\u00b10.018 |\n| 2000     | avg/std of loss           | 0.979   | 0.0117  | 0.979\u00b10.012 |\n| 2000     | avg/std of loss(mt bench) | 0.713   | 0.0014 | 0.713\u00b10.001 |\n| 3000     | avg/std of loss           | 1.002   | 0.0104  | 1.002\u00b10.01  |\n| 3000     | avg/std of loss(mt bench) | 0.731   | 0.0048  | 0.731\u00b10.005 |\n| 4000     | avg/std of loss           | 1.009   | 0.0091 | 1.009\u00b10.009 |\n| 4000     | avg/std of loss(mt bench) | 0.744   | 0.0135   | 0.744\u00b10.014 |\n| 5000     | avg/std of loss           | 0.996   | 0.0043 | 0.996\u00b10.004 |\n| 5000     | avg/std of loss(mt bench) | 0.741   | 0.0089 | 0.741\u00b10.009 |\n| 10000    | avg/std of loss           | 1.012   | 0.0083 | 1.012\u00b10.008 |\n| 10000    | avg/std of loss(mt bench) | 0.750   | 0.0072 | 0.75\u00b10.007  |\n| 40000    | avg/std of loss           | 1.015   | 0.0039 | 1.015\u00b10.004 |\n| 40000    | avg/std of loss(mt bench) | 0.740   | 0.0351  | 0.74\u00b10.035  |\nWe will also update the figure in our paper with this result.\n\n- **Q.3 Section 5.2, the usage of instruction rule.**\n\nYes, we use data with rule estimated by 7B model to train 13B model."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4082/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700615264163,
                "cdate": 1700615264163,
                "tmdate": 1700615264163,
                "mdate": 1700615264163,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gOqumolsjV",
                "forum": "kce6LTZ5vY",
                "replyto": "OmLy2WZ5Ww",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4082/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4082/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "[Continued]\n\n- **W.3 & Q.4 Models trained on full dataset.**\n\nOrca full is too large for us to train at this moment, here we provide our updated result on full dolly(15k) dataset.\n\n|Model | Datasize | Loss@self-instruct | Loss@mt-bench | OpenLLM Avg| ARC| Hellaswag| MMLU|TruthfulQA|\n|---|---|---|---|---|---|---|---|---|\n|Dolly-full| 15,000| 1.047 | 0.807 | 0.546 | 0.566 | 0.808 | 0.437 | 0.371 |\nThis result will also be added into our paper.\n\n- **W.1 The correlation between LLM-as-a-judge scores and loss.**\n\nIn table4, we bring loss and LLM-Leaderboard together:\n\n| Model                   | Data size | Loss        | Loss(MT-BENCH) | Avg. Metric | ARC   | HellaSwag | MMLU  | TruthfulQA |\n| ----------------------- | --------- | ----------- | -------------- | ----------- | ----- | --------- | ----- | ---------- |\n| INSTRUCTMINING-Selected | 10,000    | 0.9767      | 0.7177         | 58.65       | 56.66 | 79.77     | 49.89 | 48.26      |\n| INSTRUCTMINING-Selected | 40,000    | 1.006       | 0.7462         | 59.25       | 54.44 | 80.11     | 52.6  | 49.83      |\n| INSTRUCTMINING-Random   | 10,000    | 1.012\u00b10.008 | 0.75\u00b10.007     | 58.74       | 54.78 | 79.58     | 49.02 | 51.58      |\n| INSTRUCTMINING-Random   | 40,000    | 1.015\u00b10.004 | 0.74\u00b10.035     | 58.95       | 54.78 | 79.89     | 51.16 | 49.95      |\n\nWe would like to kindly point out that, since our optimization goal is to minimize the distance between the finetuned model and the target model, hence, GPT-4 in our paper, it is possible that MMLU does not correlate much with the loss. A straightforward intuition is that,  MMLU is more like a question answering task, while our loss minimization goal aims to improve the model\u2019s ability in conversation. In this case, we choose to relate loss with mt-bench, which is also LLM-as-a-judge scores to prove that loss can serve as a suitable indicator for model performance.\n\nWe are still downloading model weights and running experiments on 7B open-source models. We will provide the experiment results as soon as it is available.\n\n- **W.4 Name explanation.**\n\nThanks for pointing out the ambiguity in our paper. In Table 4, both InstructionMining-Selected and InstructionMining-Random are over Orca.\n\nThanks again for your constructive feedback. We feel truly encouraged that you like the idea and find it insightful. \n\n*Reference:*\n\n[1] Zeng, Zhiyuan, et al. \"Evaluating large language models at evaluating instruction following.\" arXiv preprint arXiv:2310.07641 (2023)."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4082/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700616135747,
                "cdate": 1700616135747,
                "tmdate": 1700616135747,
                "mdate": 1700616135747,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uLTNuyjDbn",
                "forum": "kce6LTZ5vY",
                "replyto": "OmLy2WZ5Ww",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4082/Reviewer_CNR2"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4082/Reviewer_CNR2"
                ],
                "content": {
                    "comment": {
                        "value": "Hi, thanks for your response and the additional experiments! Thank you for the clarifications, they are useful.\n\n- **Correlation between loss & downstream performance**:\nThanks for these results! I guess your strong MT-Bench results in figure 3 suggest that optimising for loss works for improving 'real' MT-Bench results. It still would be nice to get some idea of how large of a +/- change in loss you need to see a difference in performance on MT-Bench. I guess this also suggests your derived rule is focusing on conversational ability, rather than other model capabilities - you would have to test if learning a new rule would work for improving e.g. multiple-choice ability (MMLU).\n\n- **Different seeds**:\nI was especially curious about the variance in results over downstream metrics, either the gpt-annotation-based version of MT-Bench or open-llm benchmark scores (although, as you noted, if your method works better for general conversation ability, then just examining mt-bench variation is fine).\n\n- **Full Dolly results**:\nThank you for these - examining Table 3 in the paper, it seems like your method does generally better in mt-bench loss than training on the entire set, which is interesting!\n\nI have carefully read the other reviews and responses and am keeping my score."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4082/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700630469438,
                "cdate": 1700630469438,
                "tmdate": 1700630493431,
                "mdate": 1700630493431,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ImgcgxbXk1",
            "forum": "kce6LTZ5vY",
            "replyto": "kce6LTZ5vY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4082/Reviewer_TXJx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4082/Reviewer_TXJx"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposed a method for instruction selection. The main idea is that the instruction quality of D can be estimated through the inference loss of Mf t on a evaluation dataset Deval. Experiment results show that InstructMining-7B performs well."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is generally well-written and easy to follow.\n2. pioneer the application of traditional data mining techniques to augment Language Learning Models (LLMs) by autonomously curating data of superior quality."
                },
                "weaknesses": {
                    "value": "1. I think the method in this paper may not be novel and solid. Whether selecting another dataset as D-eval is a good choice is not sufficiently justified. The distributions of D-eval and the target task are different. The paper does not touch the core problem, i.e., how to select and estimate the effect of some instructions with a biased/smaller dataset.\n2. The paper posits that the instruction quality of D can be estimated through the inference loss of M-ft on a evaluation dataset D-eval. However, the negative log-likelihood (NLL) reflects the extent to which a language model trained on D can explain the data in D-eval, essentially measuring the textual similarity between D and D-eval. If the quality of D-eval is not high enough, this method of assessing instruction quality could be biased. Furthermore, how can we ensure the quality of D-eval? Does a higher similarity to D-eval necessarily indicate a higher quality of D?\n3. The paper associates the optimal point with the lowest loss, selecting the data that can achieve the lowest loss as the optimal data. However, it is important to note that there is no direct correlation between the training loss and the quality of data. An output with a longer length often results in a higher training loss, but this does not necessarily indicate lower quality. The paper does not take this factor into account.\n4. Each time D-eval changes (for instance, when a higher quality D-eval is chosen), the multivariate linear function must be relearned. In other words, the method proposed in the paper lacks scalability.\n5. It would be great to see experiments conducted using the same volume of data for model training on both LIMA and the INSTRUCTION MINING method proposed in the paper, followed by a comparison of their respective performances on MT-BENCH."
                },
                "questions": {
                    "value": "Please refer to the weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4082/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698747364130,
            "cdate": 1698747364130,
            "tmdate": 1699636372910,
            "mdate": 1699636372910,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "RuKdHrt4s4",
                "forum": "kce6LTZ5vY",
                "replyto": "ImgcgxbXk1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4082/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4082/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank Reviewer TXJx for the thoughtful feedback. Your positive comments on our writing and our method for selecting data of superior quality is truly encouraging. For your concerns and questions for our work, please find response below.\n\n- **Q.1.1 Choice of D-eval.**\n\nThanks for pointing this out. Indeed, we recognize that we didn\u2019t explain in detail why we selected another D-eval, instead of just choosing an evaluation subset from the training data. When selecting D-eval, We want \n1. D-eval to have high-quality instruction following examples in it. \n2. D-eval to be diverse enough, hence, it should have:\n(a). various instructions to test the model\u2019s generalizability; \n(b). High-quality responses.\n3. D-eval to be relatively small, so that we can conduct the experiments efficiently.\n\nAt the time of submission, we only found two suitable, human labeled instruction datasets, self-instruct and mt-bench. To make sure that our selected D-eval satisfies these requirements, we also re-annotated self-instruct dataset using GPT-4 to get high-quality responses. \n\n- **Q.1.2 Distribution difference between D-eval, target tasks, and D**\n\nWe agree that if we directly apply this rule to select data of other target tasks, our rule will probably not work as expected. However, we would like to point out that our paper mainly focuses on instruction tuning, hence, teaching the model to follow instructions. The datasets we choose to train the model and fitting the rule and mostly instruction-following data, and we mainly focus on tuning the model to make it better at following instructions. In this case, our task is general instruction following, which aims to tune an LLM to follow instructions. So there should not be much difference in the distribution of D-eval and target tasks.\n\n- **Q.1.2 How to select and estimate the effect of instructions with a biased/smaller dataset.**\n\nWe agree that if the D-eval is biased or too small, our method might not work under these circumstances. We want to kindly point out that our method is highly dependent on a suitable, high-quality D-eval dataset. A strong hypothesis in this paper is that D-eval is of relatively high quality. In this case, selecting and estimating instruction quality using a biased D-eval is out of our current scope. However, selecting data when D-eval is biased or too small is of course an important research topic, and hopefully in our future research, we can keep on improving our method to make it more robust, towards different D-eval sets.\n\n- **Q.2 The correlation between selected D and D-eval.**\n\nWe acknowledge that using D-eval for fitting the rule and then using the rule to select data will possibly result in overfitting on the D-eval. This could harm performance if D-eval is of low-quality. However, if D-eval is of high-quality, the selected D should be of similar quality. Hence, our method is strongly dependent on the hypothesis that D-eval is of high-quality. To ensure its quality, which is the self-instruct dataset in our paper, we re-annotated self-instruct dataset using GPT-4, to ensure that the label of self-instruct is of high-quality. \n\n- **Q.3 Training loss, data quality and output length.**\n\nInstead of using training loss, we use model inference loss on D-eval, so D-eval is an unseen dataset to the model. We recognize that inference loss cannot accurately represent a generative model\u2019s performance. However, inference loss is still an indicator of the textual similarity between responses generated by GPT-4 and the trained model, and it is able to indicate data quality to some extent, as shown in Table 8. Choosing a suitable metric to evaluate a model\u2019s performance or data quality requires us to consider both representability and efficiency, which still remains an unsolved problem in the research area. Inference loss is like a \u201ccompromise\u201d that it is able to represent a model\u2019s ability in following instructions to some extent and also save inference time.\n\nAlso, we would like to clarify that our analysis in appendix section D is only a descriptive analysis over the linear similarity. For example, output length in appendix D shows a positive linear correlation with loss, however, this does not mean that output length is strongly correlated with loss in other spaces. During rule fitting, under the combined influence of multiple indicators, output length becomes less significant, which results in its absence in the final estimated law.\n\nWe also would like to note that, during experimentation, we found finding a suitable metric quite hard since a generative language model cannot be simply evaluated only from its conversation ability or from its question answering ability. [Recent works](https://arxiv.org/abs/2310.07641) also discover that even GPT-4 could be \u201ctricked\u201d by superficial expressions[1]. However, expanding our optimization objective to multiple metrics is out of our current scope."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4082/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700174753332,
                "cdate": 1700174753332,
                "tmdate": 1700174753332,
                "mdate": 1700174753332,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZsOJtOHkv6",
                "forum": "kce6LTZ5vY",
                "replyto": "ImgcgxbXk1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4082/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4082/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer TXJx,\n\nWe just got the result of LIMA-7b model (LLaMA-2-7B finetuned with LIMA data). Its OpenLLM benchmark scores are shown in the table below.\n|Model | Datasize | loss@self-instruct| loss@mt-bench| OpenLLM Avg| ARC| Hellaswag| MMLU|TruthfulQA|\n|---|---|---|---|---|---|---|---|---|\n|LIMA-7b|1000|0.9947|0.7604|0.5533|0.5563|0.8009|0.4371|0.419|\n|InstructMining-7b|1000| 0.9576 | 0.7114 | 0.562525  | 0.5589 | 0.7877 | 0.4299 | 0.4736 |\n\nWe also compared its performance with our finetuned InstructMining model using LLM-as-a-judge. Results are listed in the table below.\n| Model | Win Rate | Lose Rate | Tie Rate | Adjusted Win Rate |\n|---|---|---|---|---|\n|InstructMining-7b(vs LIMA-7b)| 0.4875 | 0.1063 | 0.4062 | 0.6906 |\n\nThis result will also be updated into our paper . If you have further questions, please feel free to reply. Thank you again for your review and kind advice!"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4082/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700376333754,
                "cdate": 1700376333754,
                "tmdate": 1700376333754,
                "mdate": 1700376333754,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VVAjuTt3Xk",
                "forum": "kce6LTZ5vY",
                "replyto": "ImgcgxbXk1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4082/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4082/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer,\n\nWe would like to extend our sincere gratitude for your assistance and support. As time is limited, we wanted to confirm that our responses have addressed any concerns you may have had. If there are still any outstanding issues, please do not hesitate to let us know. We eagerly await your further feedback and hope that if all of your main concerns have been resolved, you would consider raising your score.\n\nThank you once again for taking the time to review our paper.\n\nBest regards,\nThe Authors"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4082/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700612386719,
                "cdate": 1700612386719,
                "tmdate": 1700612386719,
                "mdate": 1700612386719,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ulojxCV6Yn",
            "forum": "kce6LTZ5vY",
            "replyto": "kce6LTZ5vY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4082/Reviewer_9wuG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4082/Reviewer_9wuG"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes a data-driven automatic instruction selection method for fine-tuning language models. The basic assumption for data quality estimation is that it can be determined by the inference loss on the evaluation datasets. However it is expensive to have inference every time. Hence this work adopts several natural language indicators to predict the inference loss, i.e., the instruction data quality. By searching the best subset among the entire dataset, fine-tuned models over subsets can achieve state-of-the-art performance on two benchmarks."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Determining the quality of instruction data is complex and difficult. Also instruction data selection might be strongly dependent on downstream tasks. Data-driven methods like this work can be efficient to estimate the data quality from the proxy of inference loss on evaluation sets. As data selection can be a combinatorial optimization problem, this work provides a feasible data-driven solution and starts a good research question in this direction. It can be extended to other indicators as well."
                },
                "weaknesses": {
                    "value": "1). The test set for rule fitting might not be well designed. This work samples instructions from the Self-Instruction dataset, which are most related to traditional NLP tasks. It is still confusing that learned Eq.4 can generalize well on more real-world scenarios like Figure (3)b."
                },
                "questions": {
                    "value": "1). Why can the learned Eq.4 work well on unseen instruction datasets? Is there any intuitive explanations or any insight from the perspective of theory for example any theory guarantee of the proposed method? \n\n2). This work only compared with random data selection methods as baselines. Are there any simple or straight-forward data selection methods for comparison?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "n.a."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4082/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698766325653,
            "cdate": 1698766325653,
            "tmdate": 1699636372839,
            "mdate": 1699636372839,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ZToaFdGPiM",
                "forum": "kce6LTZ5vY",
                "replyto": "ulojxCV6Yn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4082/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4082/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your review and for your thoughtful feedback! We are glad for your positive comments on the novelty and feasibility of our approach to select high-quality instruction-following data. Below is our response for your concerns and questions.\n\n- **Q.1 Why learned Eq.4 can work well on unseen instruction datasets? Is there any theory guarantee of the proposed method?**\n\nThanks for pointing out the weakness in our paper. We don\u2019t have a theory guarantee of our method currently, which is a little bit out of our scope. However, a possible reason of the effectiveness of our method is that, based on our current hypothesis, which are:\n1. The rule-fitting test set is a high-quality dataset at the time of experiments.\n2. Inference loss can indicate a model\u2019s performance to some extent.\n\nOur method would possibly select data examples which are of similar quality to the selected rule-fitting set. Hence, for an unseen dataset, our proposed framework can:\n1. Label data quality. Our proposed rule can help quantify and label example quality.\n2. Search in an effective way and converge towards a certain direction. After we get the labels, we will rank the data according to their quality scores, and set a quantile point to choose the top high-quality examples. To do this, we use Flaml blendsearch to ensure that the final chosen dataset converges on a relatively low inference loss.\n\nWe also would like to point out that the effectiveness of our method is based on the superficial alignment hypothesis[2] and mainly focuses on instruction tuning LLMs. We believe that base LLMs already have the knowledge to answer certain problems, while instruction tuning teaches the LLM to learn to understand and follow the instructions. Hence, we believe instruction tuning does not always require large scale training data. Through small amount of high-quality examples, an LLM should be able to learn to follow instructions.[1,2]\n\n- **Q.2 Other data selection methods?**\n\nCurrently, instruction-following natural language data selection methods can be divided into three types:\n1. Human selection. This method requires human efforts to label and select examples from datasets, which always results in large expenses.\n2. Strong LLM labeling. This method requires serving or connecting very strong LLMs through apis, e.g., GPT-4. This method might lack explainability and also result in relatively large expense if the LLM api is not free and have high rate limits.\n3. Machine selection. This method requires a model e.g., reward model, to mimic human or strong LLM to label data and set selection quantiles. This method can save efforts and work under very low expense.\n\nAt the time of our submission, only (a) and (c) is available. For (b), since our resource is limited at this point, leveraging strong LLMs like GPT-4 to label over 100k data examples is too expensive for us. We will try our best to find other ways to implement (b).\nFor (a), we are currently conducting experiments using LIMA data. We will provide the result as soon as possible and will update it into our paper and comment here. And for (c), please refer to our ablation study section.\n\n*Reference*:\n\n[1] Kung, Po-Nien, and Nanyun Peng. \"Do Models Really Learn to Follow Instructions? An Empirical Study of Instruction Tuning.\" arXiv preprint arXiv:2305.11383 (2023).\n\n[2] Zhou, Chunting, et al. \"Lima: Less is more for alignment.\" arXiv preprint arXiv:2305.11206 (2023)."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4082/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700099053704,
                "cdate": 1700099053704,
                "tmdate": 1700099053704,
                "mdate": 1700099053704,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "u4nzVHOjJv",
                "forum": "kce6LTZ5vY",
                "replyto": "ulojxCV6Yn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4082/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4082/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer 9wuG,\n\nWe just got the result of LIMA-7b model (LLaMA-2-7B finetuned with LIMA data). Its OpenLLM benchmark scores are shown in the table below.\n|Model | Datasize | loss@self-instruct| loss@mt-bench| OpenLLM Avg| ARC| Hellaswag| MMLU|TruthfulQA|\n|---|---|---|---|---|---|---|---|---|\n|LIMA-7b|1000|0.9947|0.7604|0.5533|0.5563|0.8009|0.4371|0.419|\n|InstructMining-7b|1000| 0.9576 | 0.7114 | 0.562525  | 0.5589 | 0.7877 | 0.4299 | 0.4736 |\n\nWe also compared its performance with our finetuned InstructMining model using LLM-as-a-judge. Results are listed in the table below.\n| Model | Win Rate | Lose Rate | Tie Rate | Adjusted Win Rate |\n|---|---|---|---|---|\n|InstructMining-7b(vs LIMA-7b)| 0.4875 | 0.1063 | 0.4062 | 0.6906 |\n\nThis result will also be updated into our paper . If you have further questions, please feel free to reply. Thank you again for your review and kind advice!"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4082/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700376286618,
                "cdate": 1700376286618,
                "tmdate": 1700376286618,
                "mdate": 1700376286618,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "N7YV6qJGyn",
                "forum": "kce6LTZ5vY",
                "replyto": "ZToaFdGPiM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4082/Reviewer_9wuG"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4082/Reviewer_9wuG"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your response and they are helpful for clarification. Most of them answer my questions but I am still curious about the question I raised in the Weakness \"This work samples instructions from the Self-Instruction dataset, which are most related to traditional NLP tasks. Why learned Eq.4 can generalize well on more real-world scenarios like Figure (3)b.?\". It might need more investigations."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4082/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700740316645,
                "cdate": 1700740316645,
                "tmdate": 1700740316645,
                "mdate": 1700740316645,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "w75nrJ4IE3",
            "forum": "kce6LTZ5vY",
            "replyto": "kce6LTZ5vY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4082/Reviewer_rbEk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4082/Reviewer_rbEk"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposed a linear combination of some indicators to predict model performance on an unseen instruction tuning dataset, and thus use this strategy to select useful new data to train the model. They also discovered a phenomenon in the transition of data quality and data quantity above a certain threshold."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Instruction selection is an important task as the number of annotations of instruction data continuously growing.\n2. The paper provide a feasible way to predict model performance without training the model on the whole dataset, and thus using this method to select useful data."
                },
                "weaknesses": {
                    "value": "1. I'm concerned about the expense of training the linear function. It requires training 129 LLMs on 129 subsets. My questions are how 129 is chosen and how long it takes to train the 129 LLMs?\n2. The generalization ability of this method is unclear. There are two dimensions: the model used and the rule-fitting set for the combination function. I suppose the coefficients of indicators are model-dependent. The authors only tested on the Llama-2-7B model, I wonder whether using the same set of coefficients is possible for different models. Also, how sensitive the coefficients are towards the choice of the meta-training set, i.e., the rule-fitting set. Do you have numbers to demonstrate the stability of it?\n3. I'm not sure whether using the inference loss as the y-axis of the double descent figure is the correct choice."
                },
                "questions": {
                    "value": "1. Can you please answer Point 1 and 2 in the weakness section? Specifically, the total training time for the linear function. Your thoughts on the generalization ability across models and choice of meta-training set.\n2. Just a reference. A recent work also considers task selection in instruction tuning. They use models' sensitivity to instructions to mine helpful tasks. This looks more efficient than the proposed method in this paper. They also have similar observation that when data size is enlarged, the effectiveness of selection method becomes compromised.\nhttps://openreview.net/forum?id=DUJVphC9qR&referrer=%5Bthe%20profile%20of%20Po-Nien%20Kung%5D(%2Fprofile%3Fid%3D~Po-Nien_Kung1). How \n3. Can you give a hypothesis on why there an increase in inference loss when the data selection is still more important than data quality? Does this mean the data selection is selecting harmful data also?\n4. Why other baselines are even worse than random selection? Does this mean that you need more advanced baselines to compare with?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4082/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698830664592,
            "cdate": 1698830664592,
            "tmdate": 1699636372771,
            "mdate": 1699636372771,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "JD2vI5RfFr",
                "forum": "kce6LTZ5vY",
                "replyto": "w75nrJ4IE3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4082/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4082/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your review, and for recognizing our contributions! We are encouraged that you find our proposed framework feasible for selecting useful instruction following data. Below is our response to your questions and concerns:\n\n- **Q.1 & W.1: Expense of training.**\n\nIn implementing regression with ten variables, it is imperative to maintain an Event Per Variable (EPV) ratio exceeding 10. In pursuit of analytical robustness, we intentionally surpassed this threshold by collecting over 100 experiments. Each subset comprising 1,000 samples was trained with 8 A100 GPUs within 15 minutes. As a result, the cumulative time for these 129 experiments totals approximately 32 hours.\n\n- **Q.1 & W.2: generalization of the rule**\n\nIn table 6 we also show the generalization ability for our rule across model (LLaMA-1-7B), size(LLaMA-2-13B) and LoRA(LLaMA-2-7B with LoRA).\n\n- **Q.1 & W.2: coefficients sensitivity for rule-fitting set**\n\nCertainly, the coefficient has sensitivity to the test set for rule-fitting. However, it's crucial to note that such changes do not compromise the validity of our methodology. Selecting an ideal set of rule fits presents a challenge, and for our study, we employed a dataset generated through human-labeled GPT-4, representing the best available option at the time of publication.\n\n- **Q.2: Reference.**\n\nThank you for pointing this reference out, As this reference link is not openable from our side. If you are referring to [Active Instruction Tuning](https://arxiv.org/abs/2311.00288) [1], we will cite this paper in our later version.\n\n- **Q.3: Why is there an increase in inference loss when data selection is more important than data quality?**\n\nCould you please provide more detail regarding \u201cdata selection is more important than data quality\u201d? \n\n- **Q.4.1: Why are other baselines worse than random selection?**\n\nIf you are referring to Table 4, there are many other results in the open LLM leaderboard. The OpenLLM leaderboard results only show one aspect of the model ability, hence its ability in question answering. \nWe also would like to point out that, during data quality evaluation and quantile searching, we mainly use inference loss as the indicator of data quality, which means that our optimization goal is to find a training set which results in relatively low inference loss. This optimization goal is highly correlated with a model\u2019s ability in conversation. However, OpenLLM leaderboard benchmarks are more focused on question answering. In this case, we cannot promise that optimizing using inference loss would result in better performance in question answering tasks.\n\n- **Q.4.2: Are there other baseline methods?**\n\nFor other possible baselines, currently, instruction-following natural language data selection methods can be divided into three types:\n1. Human selection. This method requires human efforts to label and select examples from datasets, which always results in large expenses.\n2. Strong LLM labeling. This method requires serving or connecting very strong LLMs through apis, e.g., GPT-4. This method might lack explainability and also result in relatively large expense if the LLM api is not free and have high rate limits.\n3. Machine selection. This method requires a model e.g., reward model, to mimic human or strong LLM to label data and set selection quantiles. This method can save efforts and work under very low expense.\n\nAt the time of our submission, only (1) and (3) is available. For (2), since our resource is limited at this point, leveraging strong LLMs like GPT-4 to label over 100k data examples is too expensive for us. We will try our best to find other ways to implement (2).\nFor (1), we are currently conducting experiments using LIMA data. We will provide the result as soon as possible and will update it into our paper and comment here. And for (3), please refer to our ablation study section. \n\n- **W.3: loss in double descent**\n\nSince loss is the main optimization objective in our paper, we use loss in y-axis in figure 1. We believe that loss is highly correlated with PPL, which used to be a commonly used metric for evaluating traditional generative language models. Hence, inference loss could be an indicator of a model\u2019s performance. We\u2019ve also done some experiments on other metrics. Please refer to Figure 4 for more details.\n\n*References*:\n\n[1] Kung, Po-Nien, et al. \"Active Instruction Tuning: Improving Cross-Task Generalization by Training on Prompt Sensitive Tasks.\" arXiv preprint arXiv:2311.00288 (2023)."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4082/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700094640211,
                "cdate": 1700094640211,
                "tmdate": 1700099274596,
                "mdate": 1700099274596,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Ut9w062vdj",
                "forum": "kce6LTZ5vY",
                "replyto": "w75nrJ4IE3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4082/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4082/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer,\n\nWe would like to extend our sincere gratitude for your assistance and support. As time is limited, we wanted to confirm that our responses have addressed any concerns you may have had. If there are still any outstanding issues, please do not hesitate to let us know. We eagerly await your further feedback and hope that if all of your main concerns have been resolved, you would consider raising your score.\n\nThank you once again for taking the time to review our paper.\n\nBest regards,\nThe Authors"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4082/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700612346769,
                "cdate": 1700612346769,
                "tmdate": 1700612346769,
                "mdate": 1700612346769,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "IcPuN7JElW",
                "forum": "kce6LTZ5vY",
                "replyto": "JD2vI5RfFr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4082/Reviewer_rbEk"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4082/Reviewer_rbEk"
                ],
                "content": {
                    "comment": {
                        "value": "For Q.1 & W.2, I mean if your coefficients are obtained by training a, say, Llama-2-7B, is it still effective if you use this set of coefficients or Llama-1-7B? How about models from different families? For example, coefficients with Llama but instruction selection for T5. I'm a bit confused by what different base models mean in Table 6.\n\nFor coefficients sensitivity for rule-fitting set, I'm still concerned how transferable this method is if the test set is OOD. You should have used a validation set for fitting the linear regression model. But what if the validation set is much different from the test set? Will that still effective?\n\nFor Q.4.1, yes, it is Table 4. But I still don't quite get why Llama-2-chat or VICUNA will be worse than the INSTRUCTMINING-Random. Do you have the performance without any selection but simply use all the data from your two datasets? I just mean that the four models in the last four rows look too weak compared to your Instruction mining methods, although they use more data to do instruction tuning. I want to understand why you compare with these models and is that possible because the original datasets you select from have higher quality?\n\nFor \u201cdata selection is more important than data quality\u201d, apologies for the typo. I mean data size is more important than data quality. I saw some increase in the inference loss when the data size reached 80K for self-instruct and 60K-80K for mt-bench. Just curious why that might happen.\n\nI will keep my score for now."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4082/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700647823735,
                "cdate": 1700647823735,
                "tmdate": 1700647823735,
                "mdate": 1700647823735,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qkMCUluOtX",
                "forum": "kce6LTZ5vY",
                "replyto": "w75nrJ4IE3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4082/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4082/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "[Continued]\n\n- **Data size is more important than data quality**\n\nThis shift could come from randomness and we have added some experiments for further reference. We apologize that we can only provide experiment results on datasets of relatively smaller size because of the limited time we have at this moment.\n\n| Datasize | Metric                    | Average | Std.           | Value       |\n| -------- | ------------------------- | ------- | -------------- | ----------- |\n| 1000     | avg/std of loss           | 0.981   | 0.01694909457  | 0.981\u00b10.017 |\n| 1000     | avg/std of loss(mt bench) | 0.725   | 0.01788567698  | 0.725\u00b10.018 |\n| 2000     | avg/std of loss           | 0.979   | 0.01174901483  | 0.979\u00b10.012 |\n| 2000     | avg/std of loss(mt bench) | 0.713   | 0.001423670377 | 0.713\u00b10.001 |\n| 3000     | avg/std of loss           | 1.002   | 0.0104352034   | 1.002\u00b10.01  |\n| 3000     | avg/std of loss(mt bench) | 0.731   | 0.00483587947  | 0.731\u00b10.005 |\n| 4000     | avg/std of loss           | 1.009   | 0.009194865843 | 1.009\u00b10.009 |\n| 4000     | avg/std of loss(mt bench) | 0.744   | 0.0135088377   | 0.744\u00b10.014 |\n| 5000     | avg/std of loss           | 0.996   | 0.004393212742 | 0.996\u00b10.004 |\n| 5000     | avg/std of loss(mt bench) | 0.741   | 0.008913529242 | 0.741\u00b10.009 |\n| 10000    | avg/std of loss           | 1.012   | 0.008364151272 | 1.012\u00b10.008 |\n| 10000    | avg/std of loss(mt bench) | 0.750   | 0.007255154838 | 0.75\u00b10.007  |\n| 40000    | avg/std of loss           | 1.015   | 0.003903844259 | 1.015\u00b10.004 |\n| 40000    | avg/std of loss(mt bench) | 0.740   | 0.03513605203  | 0.74\u00b10.035  |\n\nIn this table, we can see that a significant change in loss should be larger than ~0.005 to ~0.01, while the difference between Select-loss and Random-loss are too small. Generally, if we focus on the general trend of the two lines in Figure 1, we can see that after around 40,000 to 50,000, the difference between Select-loss and Random-loss becomes smaller, which means that the importance of data selection decreases.\n\nThanks for your response again. Your review is of great help for improving our paper. If you feel most of your concerns have been addressed, it would be very nice of you to raise the score. \n\n*Reference:*\n\n[1] Roziere, Baptiste, et al. \"Code llama: Open foundation models for code.\" arXiv preprint arXiv:2308.12950 (2023).\n\n[2] Vicuna-v1.5 model card: https://huggingface.co/lmsys/vicuna-7b-v1.5\n\n[3] Sharegpt unfiltered dataset: https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered\n\n[4] OpenLLM Leaderboard: https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard\n\n[5] Nakkiran, Preetum, et al. \"Deep double descent: Where bigger models and more data hurt.\" Journal of Statistical Mechanics: Theory and Experiment 2021.12 (2021): 124003."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4082/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700706928107,
                "cdate": 1700706928107,
                "tmdate": 1700707247251,
                "mdate": 1700707247251,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]