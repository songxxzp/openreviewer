[
    {
        "title": "Knowledge Distillation Based on Transformed Teacher Matching"
    },
    {
        "review": {
            "id": "k1uorNj0LV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission760/Reviewer_fD3s"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission760/Reviewer_fD3s"
            ],
            "forum": "MJ3K7uDGGl",
            "replyto": "MJ3K7uDGGl",
            "content": {
                "summary": {
                    "value": "This paper proposes a new variant of knowledge distillation called Transformed Teacher Matching (TTM) that drops temperature scaling on the student side and introduces an inherent regularization term. The paper shows that TTM leads to better generalization and achieves state-of-the-art accuracy performance. The paper also introduces a weighted version of TTM called Weighted Transformed Teacher Matching (WTTM) that enhances the student's capability to match the teacher's power transformed probability distribution. The experiments conducted in the paper demonstrate the effectiveness of TTM and WTTM on various datasets and architectures."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper introduces a new variant of knowledge distillation that drops temperature scaling on the student side and introduces an inherent regularization term. This approach is motivated by recent works and is a departure from conventional knowledge distillation. The paper also introduces a weighted version of TTM that enhances the student's capability to match the teacher's power transformed probability distribution. These contributions are novel and have not been explored in previous works.\n\n2. The paper is well-written and presents a clear and concise description of the proposed methods. The authors provide a thorough analysis of the experimental results and compare their approach with state-of-the-art methods. The experiments are conducted on various datasets and architectures, which demonstrates the effectiveness and robustness of the proposed methods.\n\n3. The proposed methods have the potential to improve the performance of knowledge distillation and have practical applications in various domains. The paper demonstrates that TTM and WTTM achieve state-of-the-art accuracy performance on various datasets and architectures. The inherent regularization term in TTM also provides a new perspective on knowledge distillation and has the potential to inspire further research in this area. Overall, the paper makes a significant contribution to the field of knowledge distillation"
                },
                "weaknesses": {
                    "value": "1. The paper could benefit from addressing the lack of novelty by acknowledging that techniques such as R\u00b4enyi or f divergence, temperature scaling, and logits normalization have already been widely used in knowledge distillation. For example, Information Theoretic Representation Distillation (BMVC) employed R\u00b4enyi divergence for standard distillation, and AlphaNet (ICML2021) utilized the f divergence to distill different sub-networks. Moreover, this method is likely already considered in the distiller's search work (KD-Zero: Evolving Knowledge Distiller for Any Teacher-Student Pairs, NeurIPS-2023). \n\n2. To strengthen the paper's findings, it is important to validate the proposed method on downstream tasks such as object detection and segmentation. Including evaluation results on these tasks will demonstrate the practical effectiveness and applicability of the proposed method. Additionally, providing more examples and visualizations will enhance the readers' understanding of how the method works and its impact on the learning process.\n\n3. Furthermore, it is essential to incorporate a thorough discussion of relevant KD-related studies, including Self-Regulated Feature Learning via Teacher-free Feature Distillation (ECCV2022), NORM: Knowledge Distillation via N-to-One Representation Matching (ICLR2023), Shadow Knowledge Distillation: Bridging Offline and Online Knowledge Transfer (NIPS2022), DisWOT: Student Architecture Search for Distillation Without Training (CVPR2023), and Automated Knowledge Distillation via Monte Carlo Tree Search (ICCV2023). This discussion will help position the proposed approach within the existing literature, establish connections, and provide valuable insights for potential comparisons."
                },
                "questions": {
                    "value": "The only concern to me is the novelty of the work and I hope the authors could discuss some of the related work I mentioned in the revised version.\n\n\n---------------------------------\n\nThe author's response addressed my concerns well, so I'm improving my score to acceptance, thanks!"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "no"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission760/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission760/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission760/Reviewer_fD3s"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission760/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697168579535,
            "cdate": 1697168579535,
            "tmdate": 1700288810015,
            "mdate": 1700288810015,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "pqOffyVxyx",
                "forum": "MJ3K7uDGGl",
                "replyto": "k1uorNj0LV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission760/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission760/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer fD3s (Part 1)"
                    },
                    "comment": {
                        "value": "We thank you very much for taking time to review our paper and provide valuable feedbacks. To address your major concern about the novelty of our work, we have discussed the related works you mentioned in the revised version of our paper (see appendix A.7). Please find our point-by-point responses to your comments as follows.\n\n> Comment 1: The paper could benefit from addressing the lack of novelty by acknowledging that techniques such as R\u00b4enyi or f divergence, temperature scaling, and logits normalization have already been widely used in knowledge distillation. For example, Information Theoretic Representation Distillation (BMVC) employed R\u00b4enyi divergence for standard distillation, and AlphaNet (ICML2021) utilized the f divergence to distill different sub-networks. Moreover, this method is likely already considered in the distiller's search work (KD-Zero: Evolving Knowledge Distiller for Any Teacher-Student Pairs, NeurIPS-2023).\n\n**Response**: One of the major contributions of our work is showing that dropping the temperature on the student side leads to a better version of KD, dubbed as TTM in the paper. In our work, R\u00b4enyi entropy is not something deliberately involved by us to build our own method. Instead, it naturally emerges when we remove the temperature on the student side, which explains why dropping the temperature on the student side is beneficial. Therefore, although similar concepts appear in both our work and the works you listed, the context is totally different. \n\nTo further alleviate your concern, we next show that how our work is different from the works you mentioned one by one as follows: (1) Information Theoretic Representation Distillation is a feature-based distillation method, while our proposed method is logits-based, with only minimal modifications to the original KD. (2) AlphaNet utilizes $\\alpha$-divergence in distillation instead of the conventional KL divergence. However, we stick to the KL divergence used in the original KD. (3) We acknowledge that KD-Zero does search for distillers in a pretty wide searching space. However, according to Table 1 in the paper of KD-Zero, we firmly believe that the adaptive weighting term $U$ proposed in our work is not considered in the searching space of KD-Zero.\n\nActually, based on our literature review, the most similar work to us is Self-Distillation as Instance-Specific Label Smoothing (NIPS 2020), and we have already carefully addressed this work in Subsection 2.3.\n\nBased on the above discussion, we have full confidence in the novelty of our work.\n\n> Comment 2: To strengthen the paper's findings, it is important to validate the proposed method on downstream tasks such as object detection and segmentation. Including evaluation results on these tasks will demonstrate the practical effectiveness and applicability of the proposed method. Additionally, providing more examples and visualizations will enhance the readers' understanding of how the method works and its impact on the learning process.\n\n**Response**: The major contribution of this paper is to show that it is better off to drop the temperature T from the student side. This is well supported by both theoretical analysis and empirical results. This is a new understanding about KD and, together with the statistical perspective of KD, offers a new explanation of why KD helps. On top of this contribution, we further introduce a sample-adaptive coefficient to KD, yielding the method called WTTM. While TTM can be regarded as a correction of KD, WTTM is new and achieves the state-of-the-art performance. In addition, WTTM has essentially the same computation complexity as KD does. It can be applied wherever KD is applicable.  In the literature, KD has already been widely used on downstream tasks such as object detection and segmentation. So, there\u2019s no reason to believe our proposed methods are not applicable to them. Of course, how much gain TTM and WTTM would offer in comparison with KD and other existing distillers on those tasks needs to be found out in future work.\n\nAs for examples and visualizations, please refer to Figure 1 in our paper, where we visualize the regularization effect of our proposed objectives compared to vanilla KD, and Figure 2, where we visualize the behavior of the average $D(p^t_T||q)$ for TTM and WTTM throughout the learning process, which helps explain why WTTM performs better than TTM."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission760/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700288086951,
                "cdate": 1700288086951,
                "tmdate": 1700288086951,
                "mdate": 1700288086951,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Y40TBvWyZr",
                "forum": "MJ3K7uDGGl",
                "replyto": "k1uorNj0LV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission760/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission760/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer fD3s (Part 2)"
                    },
                    "comment": {
                        "value": "> Comment 3: Furthermore, it is essential to incorporate a thorough discussion of relevant KD-related studies, including Self-Regulated Feature Learning via Teacher-free Feature Distillation (ECCV2022), NORM: Knowledge Distillation via N-to-One Representation Matching (ICLR2023), Shadow Knowledge Distillation: Bridging Offline and Online Knowledge Transfer (NIPS2022), DisWOT: Student Architecture Search for Distillation Without Training (CVPR2023), and Automated Knowledge Distillation via Monte Carlo Tree Search (ICCV2023). This discussion will help position the proposed approach within the existing literature, establish connections, and provide valuable insights for potential comparisons.\n\n**Response**: Please refer to appendix A.7 of our revised paper, where we cite all these papers you mentioned and provide a thorough discussion of relevant KD-related studies."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission760/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700288268198,
                "cdate": 1700288268198,
                "tmdate": 1700288268198,
                "mdate": 1700288268198,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Z3ujhSeZeq",
                "forum": "MJ3K7uDGGl",
                "replyto": "pqOffyVxyx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission760/Reviewer_fD3s"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission760/Reviewer_fD3s"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the response."
                    },
                    "comment": {
                        "value": "The author's response addressed my concerns well, so I'm improving my score to acceptance, thanks!"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission760/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700288871210,
                "cdate": 1700288871210,
                "tmdate": 1700288871210,
                "mdate": 1700288871210,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "NtJdqPheLy",
            "forum": "MJ3K7uDGGl",
            "replyto": "MJ3K7uDGGl",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission760/Reviewer_2Z26"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission760/Reviewer_2Z26"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates the temperature for Knowledge Distillation, proposing Transformed Teacher Matching (TTM), which drops the temperature scaling on the student side. TTM has an inherent Renyi entropy term in its objective function, and this regularization leads to better performance with KD."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The method that rethinking KD via temperature scaling is interesting.\n2. The final TTM does not introduce extra hyper-parameters. Also, the training speed keeps the same.\n3. The results on various datasets and models prove its effectiveness."
                },
                "weaknesses": {
                    "value": "1. Some references and comparisons are missing:\n\n    [1] Knowledge distillation from a stronger teacher.\n\n    [2] From Knowledge Distillation to Self-Knowledge Distillation: A Unified Approach with Normalized Loss and Customized Soft Labels.\n\n    [3] Curriculum Temperature for Knowledge Distillation.\n\n    [4] VanillaKD: Revisit the Power of Vanilla Knowledge Distillation from Small Scale to Large Scale.\n2. When temperature=1, is TTM the same as the original KD? In some papers, the temperature on ImageNet is actually 1.0.\n3. Could TTM still achieve better performance for larger models (e.g. DeiT-T or DeiT-S)?  VanillaKD shows under strong training settings, the original KD also performs well."
                },
                "questions": {
                    "value": "above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission760/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission760/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission760/Reviewer_2Z26"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission760/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698653687199,
            "cdate": 1698653687199,
            "tmdate": 1700709055181,
            "mdate": 1700709055181,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "tyZ9K7jr1p",
                "forum": "MJ3K7uDGGl",
                "replyto": "NtJdqPheLy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission760/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission760/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 2Z26"
                    },
                    "comment": {
                        "value": "We thank you very much for taking time to review our paper and provide valuable feedbacks. We have addressed the papers you mentioned in the revised version of our paper (see appendix A.7). Also, we managed to generate some preliminary result for DeiT-T to address your concern. Below, please find our point-by-point responses to your comments.\n\n> Comment 1: Some references and comparisons are missing \u2026\n\n**Response**: Please refer to appendix A.7 in our revised paper, where we include the 4 papers you mentioned as references. As for comparison, in the revised version, we include the first 3 papers you mentioned as benchmarks in Table 5, and our method is still achieving outstanding performance. However, there are no overlapping experimental settings between our work and VanillaKD to form a fair comparison, and it's infeasible for us to follow the settings in VanillaKD to generate some new results in such limited time based on our computational resources, so we didn't include VanillaKD into comparison.\n\n> Comment 2: When temperature=1, is TTM the same as the original KD? In some papers, the temperature on ImageNet is actually 1.0.\n\n**Response**: Yes, TTM (and WTTM) is the same as the original KD when temperature=1, and we know that the commonly used temperature on ImageNet is 1.0 for the original KD. However, with temperature dropped on the student side, the effect of the temperature in distillation is changed, so it's reasonable for TTM (and WTTM) to have a different optimal temperature from the original KD. According to our experiments, the optimal $\\gamma$ for our method is around 0.8 on ImageNet (see Table 10 in our revised paper), which is equivalent to temperature=1.25. Consequently, our method is different from the original KD on ImageNet.\n\n> Comment 3: Could TTM still achieve better performance for larger models (e.g. DeiT-T or DeiT-S)? VanillaKD shows under strong training settings, the original KD also performs well.\n\n**Response**: Both TTM and WTTM have essentially the same computation complexity as KD does. They can be applied wherever KD is applicable. In the literature, KD has already been widely used in improving the performance of larger models such as the transformer models you mentioned. So, there\u2019s no reason why TTM and WTTM cannot.\n\nIn the interest of time and also due to our limited computational resources, we tried our best in the last a couple of days and managed to generate one result for DeiT-T on ImageNet without tuning any hyperparameters. The result is shown in the table below.\n\n|Teacher|Student|KD|WTTM|\n|:---:|:---:|:---:|:---:|\n|RegNetY-16GF (82.9) |DeiT-T (72.2)|72.2|72.7|\n\nFollowing the training strategy of the original DeiT paper [1], KD is not able to improve the Top-1 accuracy (%) performance of DeiT-T on ImageNet (72.2%), while WTTM can improve the performance to 72.7%. This small example at least shows the potential of WTTM for larger models.\n\nNote that the training strategy in [1] is also quite strong, including AdamW optimizer, cosine LR decay, label smoothing, stochastic depth, repeated augmentation, Rand Augment, mixup, cutmix, random erasing and so on, with carefully tuned hyperparameters for each ingredient. Therefore, we believe that our method is still better than the original KD even in strong training settings.\n\n[1] Touvron et al., \"Training data-efficient image transformers & distillation through attention\", ICML 2021."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission760/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700287798668,
                "cdate": 1700287798668,
                "tmdate": 1700287798668,
                "mdate": 1700287798668,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "eafWHhYWNR",
                "forum": "MJ3K7uDGGl",
                "replyto": "tyZ9K7jr1p",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission760/Reviewer_2Z26"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission760/Reviewer_2Z26"
                ],
                "content": {
                    "comment": {
                        "value": "I am still concerned about the perfromance of larger models. In Tab.11 of paper NKD, KD can improve the performance of DeiT actually. Could you provide the performance with the proposed WTTM? It may need about two days to train a DeiT-Tiny."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission760/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700447525885,
                "cdate": 1700447525885,
                "tmdate": 1700447525885,
                "mdate": 1700447525885,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "m3RhFxPShZ",
                "forum": "MJ3K7uDGGl",
                "replyto": "NtJdqPheLy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission760/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission760/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer 2Z26,\n\nThanks again for reviewing our paper. Since the discussion period is going to end soon, we are eager to know if you are satisfied with our previous responses. If no, please kindly tell us your remaining concerns and hopefully we can address them before the deadline. If yes, we wonder if it\u2019s possible for you to raise the score. In any case, we would be extremely grateful to hear from you.\n\nThanks."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission760/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700682016721,
                "cdate": 1700682016721,
                "tmdate": 1700682016721,
                "mdate": 1700682016721,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0VLyFBizkx",
                "forum": "MJ3K7uDGGl",
                "replyto": "m3RhFxPShZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission760/Reviewer_2Z26"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission760/Reviewer_2Z26"
                ],
                "content": {
                    "comment": {
                        "value": "I suggest to add the results for DeiT. I decide to increase my score."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission760/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700709006840,
                "cdate": 1700709006840,
                "tmdate": 1700709006840,
                "mdate": 1700709006840,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ypOw6Us1KF",
            "forum": "MJ3K7uDGGl",
            "replyto": "MJ3K7uDGGl",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission760/Reviewer_DiN5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission760/Reviewer_DiN5"
            ],
            "content": {
                "summary": {
                    "value": "The paper systematically studies a variant of KD without temperature scaling on the student side, dubbed TTM. Temperature scaling is crucial in knowledge distillation (KD). This paper introduces transformed teacher matching (TTM), a variant of KD that omits temperature scaling on the student side. TTM includes an inherent regularization term and produces better generalization compared to the original KD. Weighted TTM (WTTM) further enhances the student's ability to match the teacher's probability distribution, achieving state-of-the-art accuracy."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Fruitful discussion about related works to engage the readers.\n- Theoretical derivation from KD to the proposed TTM."
                },
                "weaknesses": {
                    "value": "The results are completely dependent on the list T and \u03b2 values of all experiments (see Table 8 and 9), which makes the method impractical. Furthermore, the optimal value may even vary from task to task, dataset to dataset and backbone to backbone. These are my main concerns. Based on the marginal gain compared to the baselines, these empirical results actually weaken the claimed contribution."
                },
                "questions": {
                    "value": "see above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission760/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698743835822,
            "cdate": 1698743835822,
            "tmdate": 1699636003258,
            "mdate": 1699636003258,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qHdA18MssB",
                "forum": "MJ3K7uDGGl",
                "replyto": "ypOw6Us1KF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission760/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission760/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer DiN5"
                    },
                    "comment": {
                        "value": "We thank you very much for taking time to review our paper and provide valuable feedbacks. Please find our response to your comment below.\n\n> Comment: The results are completely dependent on the list T and \u03b2 values of all experiments (see Table 8 and 9), which makes the method impractical. Furthermore, the optimal value may even vary from task to task, dataset to dataset and backbone to backbone. These are my main concerns. Based on the marginal gain compared to the baselines, these empirical results actually weaken the claimed contribution.\n\n**Response**: We appreciate your concerns mentioned above and will address them from four different perspectives.\n\n- First, the major contribution of this paper is to show that it is better off to drop the temperature T from the student side. This is well supported by both theoretical analysis and empirical results. Regardless of how large or small the gain is, the point is that removing the temperature from the student side leads to a better distiller with more regularization. This is a new understanding about KD and, together with the statistical perspective of KD, offers a new explanation of why KD helps. On top of this contribution, we further introduce a sample-adaptive coefficient to KD, yielding the method called WTTM. While TTM can be regarded as a correction of KD, WTTM is new and achieves the state-of-the-art performance.\n- Second, as for practicality, WTTM has essentially the same computation complexity as KD does. It can be applied wherever KD is applicable. Third, yes, for results in Table 1, hyperparameters are tuned according to Tables 8 and 9. However, this is done for the purpose of fair comparison and benchmarking since some benchmark method (such as DKD [1]) also has its hyperparameters tuned from task to task, dataset to dataset and backbone to backbone. In addition, if one examines Tables 8 and 9 carefully, it can be seen that hyperparameters of WTTM do not fluctuate much from one teacher-student pair to another in most cases. This implies that even with fixed hyperparameters, WTTM would perform well in general for all tested teacher-student pairs.\n- Fourth, to further confirm the point mentioned above, the table below shows the results (Top-1 accuracy, %) of KD and WTTM on CIFAR-100 with $T=4$ and $\\beta=36/\\bar{U}$ fixed for all teacher-student pairs, the same as how we generate Figure 1 in our paper. Note that these parameters are optimized for KD, but not for WTTM. From the table below, it is clear that WTTM still outperforms KD by a large margin in most cases.\n\n||KD|WTTM|\n|:---:|:---:|:---:|\n|WRN-40-2 &#8594; WRN-16-2|74.92|75.88|\n|WRN-40-2 &#8594; WRN-40-1|73.54|74.17|\n|resnet56 &#8594; resnet20|70.66|71.52|\n|resnet110 &#8594; resnet20|70.67|71.18|\n|resnet110 &#8594; resnet32|73.08|73.49|\n|resnet32x4 &#8594; resnet8x4|73.33|74.97|\n|vgg13 &#8594; vgg8|72.98|73.66|\n|vgg13 &#8594; MobileNetV2|67.37|68.12|\n|ResNet50 &#8594; MobileNetV2|67.35|68.59|\n|ResNet50 &#8594; vgg8|73.81|74.13|\n|resnet32x4 &#8594; ShuffleNetV1|74.07|73.45|\n|resnet32x4 &#8594; ShuffleNetV2|74.45|76.13|\n|WRN-40-2 &#8594; ShuffleNetV1|74.83|74.81|\n\n[1] Zhao et al., \"Decoupled knowledge distillation\", CVPR 2022."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission760/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700287567920,
                "cdate": 1700287567920,
                "tmdate": 1700287567920,
                "mdate": 1700287567920,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "AiiuIfnt4b",
                "forum": "MJ3K7uDGGl",
                "replyto": "ypOw6Us1KF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission760/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission760/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer DiN5,\n\nThanks again for reviewing our paper. Since the discussion period is going to end soon, we are eager to know if you are satisfied with our previous response. If no, please kindly tell us your remaining concerns and hopefully we can address them before the deadline. If yes, we wonder if it\u2019s possible for you to raise the score. In any case, we would be extremely grateful to hear from you.\n\nThanks."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission760/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700634876881,
                "cdate": 1700634876881,
                "tmdate": 1700634876881,
                "mdate": 1700634876881,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "bYuBD4MNLm",
            "forum": "MJ3K7uDGGl",
            "replyto": "MJ3K7uDGGl",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission760/Reviewer_LeDY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission760/Reviewer_LeDY"
            ],
            "content": {
                "summary": {
                    "value": "The paper systematically analyzed the effect of dropping the temperature scaling on the student side in knowledge distillation (KD). The theoretical analysis shows that such a transformation leads to a general KD loss and a Renyi entropy regularization that improves the generalization of the student. Further, To further enhance student\u2019s capability to match teacher\u2019s power transformed probability distribution, the paper introduces a sample-adaptive coefficient to the method. Experiments are conducted to validate the effectiveness of both modules. Experiments are evaluated with different model architectures and teacher quality."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "I think overall the paper provides new findings to understand the role of temperature in knowledge distillation. And the evaluation experiments are extensive.\n\n1. The theoretical derivation and analysis for the general KD, Renyi entropy, and transformed teacher matching is precise and solid.\n\n2. Extensive experiments confirm the theoretical analysis and show the effectiveness of each proposed module."
                },
                "weaknesses": {
                    "value": "1. It's better to provide a detailed summary and comparison of the latest related works.\n\n2. It's also more convincing to show results on transformer models such as ViT."
                },
                "questions": {
                    "value": "Please see the weakness part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission760/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698856026956,
            "cdate": 1698856026956,
            "tmdate": 1699636003191,
            "mdate": 1699636003191,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zcSUQDuZk5",
                "forum": "MJ3K7uDGGl",
                "replyto": "bYuBD4MNLm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission760/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission760/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer LeDY"
                    },
                    "comment": {
                        "value": "We thank you very much for taking time to review our paper and provide valuable feedbacks. We have added a subsection in our paper (see appendix A.7) to summarize some latest related works about KD, and compared our results with more latest works in the result section (see Table 5) as well. Also, we managed to generate some preliminary result on a transformer model. Below, please find our point-by-point responses to your comments.\n\n> Comment 1: It's better to provide a detailed summary and comparison of the latest related works.\n\n**Response**: Please refer to appendix A.7 in our revised paper, where we provide a summary of the latest related works. As for comparison, in the revised version, we include more benchmarks in Table 5 including many works published last year and this year, which are DKD [1], DIST [2], KD++ [3], NKD [4], CTKD [5], and KD-Zero [6], and our method is still achieving outstanding performance.\n\n[1] Zhao et al., \"Decoupled knowledge distillation\", CVPR 2022.  \n[2] Huang et al., \"Knowledge distillation from a stronger teacher\", NIPS 2022.  \n[3] Wang et al., \"Improving knowledge distillation via regularizing feature norm and direction\", Under review of ICLR 2024.  \n[4] Yang et al., \"From Knowledge Distillation to Self-Knowledge Distillation: A Unified Approach with Normalized Loss and Customized Soft Labels\", ICCV 2023.  \n[5] Li et al., \"Curriculum Temperature for Knowledge Distillation\", AAAI 2023.  \n[6] Li et al., \"KD-Zero: Evolving Knowledge Distiller for Any Teacher-Student Pairs\", NIPS 2023.\n\n> Comment 2: It's also more convincing to show results on transformer models such as ViT.\n\n**Response**: Both TTM and WTTM have essentially the same computation complexity as KD does. They can be applied wherever KD is applicable. In the literature, KD has already been widely used in improving the performance of transformer models. So, there\u2019s no reason why TTM and WTTM cannot.\n\nIn the interest of time and also due to our limited computational resources, we tried our best in the last a couple of days and managed to generate one result for one of the smallest transformer models, namely DeiT-tiny, on ImageNet without tuning any hyperparameters. The result is shown in the table below.\n\n|Teacher|Student|KD|WTTM|\n|:---:|:---:|:---:|:---:|\n|RegNetY-16GF (82.9) |DeiT-tiny (72.2)|72.2|72.7|\n\nFollowing the training strategy of the original DeiT paper [7], KD is not able to improve the Top-1 accuracy (%) performance of DeiT-tiny on ImageNet (72.2%), while WTTM can improve the performance to 72.7%. This small example at least shows the potential of WTTM for transformer models.\n\n[7] Touvron et al., \"Training data-efficient image transformers & distillation through attention\", ICML 2021."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission760/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700287424715,
                "cdate": 1700287424715,
                "tmdate": 1700287424715,
                "mdate": 1700287424715,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VzrpHgYIMh",
                "forum": "MJ3K7uDGGl",
                "replyto": "bYuBD4MNLm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission760/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission760/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer LeDY,\n\nThanks again for reviewing our paper. We have more result about transformer models. Please refer to our latest response to Reviewer 2Z26. \n\nSince the discussion period is going to end soon, we are eager to know if you are satisfied with our previous response. If no, please kindly tell us your remaining concerns and hopefully we can address them before the deadline. If yes, we wonder if it\u2019s possible for you to raise the score. In any case, we would be extremely grateful to hear from you.\n\nThanks."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission760/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700634782603,
                "cdate": 1700634782603,
                "tmdate": 1700634782603,
                "mdate": 1700634782603,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]