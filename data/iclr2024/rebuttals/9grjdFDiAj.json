[
    {
        "title": "Probabilistic Stability of Stochastic Gradient Descent"
    },
    {
        "review": {
            "id": "HcFRW9Ucwi",
            "forum": "9grjdFDiAj",
            "replyto": "9grjdFDiAj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1548/Reviewer_PopN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1548/Reviewer_PopN"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes to study the stability of SGD from a probabilistic viewpoint. The argument is that the existing stability analysis based on the convergence as measured by the moment is not sufficient to explain the dynamic behavior of SGD. The main results show that the probabilistic stability of SGD in high-dimension is equivalent to a condition on the sign of the Lyapunov exponent of the SGD dynamics. The derived results provide a new perspective to understand why SGD selects solutions with good generalization from an enormous number of possible solutions."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper proposes a different perspective to understand the dynamic behavior of SGD, which differs from the existing explanation that SGD prefers flatter solutions. The paper proposes probabilistic stability as a weaker condition than the moment stability. Furthermore, the connection of the probabilistic stability to the Lynapunov exponents is interesting."
                },
                "weaknesses": {
                    "value": "It seems the analysis is not quite rigorous, and I found several gaps in the theoretical analysis.\n\nThe Lyapunov exponent seems to be a conservative parameter as it needs to take the maximum over all initializations. It seems that this quantity cannot fully illustrate the behavior of SGD since the initialization also has a large impact on the behavior of the algorithm, which cannot be explained by the Lyapunov exponent."
                },
                "questions": {
                    "value": "Above Eq (16), the paper shows that algorithmic stability holds if $\\lambda=1/x_i^2$. Then, Eq (16) says that the largest stable learning rate is $1/x_{min}^2$. Should the largest stable learning rate be $1/x_{max}^2$ since we need the algorithm to be stable for any chosen example, and therefore should choose the smallest step size?\n\nAbove Eq (37), the paper shows that $m=tE_x[\\log|1-\\lambda h_t|]$. However, this identity only holds if $h_1=h_2=\\ldots=h_t$. In this case, the matrix $\\hat{H}(x)$ should remain the same over the optimization process. This seems to be a strong requirement. \n\nI cannot see how the argument below Eq (39) holds. That is, how to get the convergence by the law of large numbers?\n\nIn Eq (46), the paper uses the identity $E[\\hat{H}_t\\theta_t]=E[\\hat{H}_t]E[\\theta_t]$. This identity holds if $\\hat{H}_t$ and $\\theta_t$ are independent. This also seems to be a restrictive condition.\n\nTheorem 3 requires $X_i$ to be independent random matrices. The paper applies Theorem 3 to get Eq (54), which requires $\\hat{H}_i$ to be independent. However, it seems that these matrices are not independent, and therefore the Theorem 3 cannot be applied?\n\nI cannot see how Eq (52) holds. In particular, how this identity holds with a small O notation.\n\nTypos:\nEq (6): $\\lambda^3$ should be $O(\\lambda^3)$\nBelow Eq (10): \", This\" should be \", this\"\nProposition 2: \"but $L_p$ stable\" should be \"\"but not $L_p$ stable\"\"?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1548/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698114517768,
            "cdate": 1698114517768,
            "tmdate": 1699636083219,
            "mdate": 1699636083219,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GGcukplbfC",
                "forum": "9grjdFDiAj",
                "replyto": "HcFRW9Ucwi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1548/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1548/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply part 1"
                    },
                    "comment": {
                        "value": "Thank you for the detailed and constructive feedback. We answer both the weaknesses and questions below.\n\n\nWeaknesses:\n\n**1. It seems the analysis is not quite rigorous, and I found several gaps in the theoretical analysis.**\n\nThanks for the criticism. We address the concerns below and have clarified these points in the revision. In short, we believe that most of the criticisms are due to misunderstanding of the definitions or of the problem setting.\n\n**2. The Lyapunov exponent seems to be a conservative parameter as it needs to take the maximum over all initializations. It seems that this quantity cannot fully illustrate the behavior of SGD since the initialization also has a large impact on the behavior of the algorithm, which cannot be explained by the Lyapunov exponent.**\n\nThank you for the criticism. The Lyapunov exponent is actually more versatile than what we have focused on studying in the manuscript, and it is easy to extend the framework to study the effect of initialization. What we have called the \"Lyapunov exponent\" is more technically called the \"largest Lyapunov exponent.\" More broadly, the Lyapunov exponent is defined with respect to a specific initialization. If the dynamical variable is $d$-dimensional, a famous result proves that the Lyapunov exponent takes on at most $d$ distinctive values. Therefore, an important future direction is to study the effect of initialization on the Lyapunov exponent and this is not a limitation of the proposed theoretical framework. Also, see footnote 5 in our manuscript for this point.\n\n\nQuestions:\n\n**1. Above Eq (16), the paper shows that algorithmic stability holds if $\\lambda = 1/x_i^2$. Then, Eq (16) says that the largest stable learning rate is $1/x_\\text{min}^2$. Should the largest stable learning rate be $1/x_\\text{max}^2$ since we need the algorithm to be stable for any chosen example, and therefore should choose the smallest step size?**\n\nThanks for this question. While this point might be surprising to some readers, we point out that, it holds true that for any $i$, if $\\lambda = 1/x_i^2$, then the algorithm is globally stable, meaning that running SGD converges to the global minimum in probability. \n\nWhen $\\lambda=1/x_{min}$, it might appear unlikely that SGD is stable because it is unstable for the rest of the data points. However, in this very intriguing example, SGD being stable only for a single sample is sufficient to prevent its divergence and guarantee a probabilistic convergence to the global minimum (and this demonstrates the power of the probabilistic stability).\n\n\n\n**2. Above Eq (37), the paper shows that $m=tE_x[\\log|1-\\lambda h_t|]$. However, this identity only holds if $h_1=h_2=\\ldots=h_t$. In this case, the matrix $\\hat{H}(x)$ should remain the same over the optimization process. This seems to be a strong requirement.**\n\nThank you for the comment. This is a misunderstanding. $m$ is an auxiliary constant, which we defined as the expected value. Therefore, there is no requirement or assumption here. It is just a definition we used to simplify the proof. The purpose of introducing $m$ is to define $z_t$ as a zero-mean random variable. \n\n\n**3. I cannot see how the argument below Eq (39) holds. That is, how to get the convergence by the law of large numbers?**\n\nIt follows from the fact that $z_t$ is the sum of independent random variables, and so $z_t / t$ is the sample average of the random variable $\\log|1 - \\lambda h_t| - \\mathbb{E}[\\log|1 - \\lambda h_t|]$. This random variable has zero mean and is identically distributed across $t$. As we always approximate $h$ using the Hessian at the stationary point, there is no dependence of $h$ on $\\theta_t$. As a consequence, $h_t$ are independent and so does $\\log|1 - \\lambda h_t| - \\mathbb{E}[\\log|1 - \\lambda h_t|$. Thus, the law of large numbers applies. The RHS can be obtained taking the limit of $t\\to\\infty$, and no law of large numbers is required."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1548/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700664666855,
                "cdate": 1700664666855,
                "tmdate": 1700664666855,
                "mdate": 1700664666855,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "k8gEEKm1sl",
            "forum": "9grjdFDiAj",
            "replyto": "9grjdFDiAj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1548/Reviewer_My7s"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1548/Reviewer_My7s"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces probabilistic stability as a new notion for analyzing the dynamics of SGD around critical points.  Specifically, the proposed notion is used to characterize different learning phases of SGD such as correct learning, incorrect learning, convergence to low-rank saddles and unstable learning phase. In particular, the authors provide many insights into the convergence to saddle points based on their probabilistic stability notion."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Previous approaches may be insufficient to characterize the dynamics of SGD near saddle points while the proposed probabilistic stability in this paper overcomes this limitation."
                },
                "weaknesses": {
                    "value": "The analysis in this work is restricted to the assumption that initialization point is near a given stationary point. While I acknowledge that such a limitation is not unique to this paper, it deviates from real-world scenarios. Numerous observations have indicated that the early phase of learning substantially impacts the ultimate generalization performance of deep neural networks. Consequently, centering on the dynamics around stationary points may fall short in elucidating the success of deep learning.\n\nIn addition, this paper is poorly written and it needs substantial improvement."
                },
                "questions": {
                    "value": "Major concerns:\n\n1. A recent study [1] shows that neural networks trained by gradient-based methods may not necessarily converge to stationary points. In other words, the gradients (norm) may not even vanish when these networks exhibit satisfactory performance. This observation raises questions about the applicability of probabilistic stability in understanding deep learning.\n\n[1] Jingzhao Zhang, et al. \"Neural network weights do not converge to stationary points: An invariant measure perspective.\" ICML 2022.\n\n2. Is the notion of Linear stability as defined in Wu et al. (2018) equivalent to Definition 2 in this paper? Or, is one notion weaker than the other?\n\n3. On Page 3, in the text beneath the caption of Figure 1, the statement \"This means that Eq. (1) can be seen as an effective description for only a small subset of all parameters ...\" lacks clarity in its flow. Could you provide further elaboration on this?\n\n4. On Page 6, in the fifth-to-last line, you mention, \"The theory also shows that if we fix the learning rate and noise level, increasing the batch size makes it more and more difficult to converge to the low-rank solution...\". However, in Proposition 3, it's not  clear to me how the batch size explicitly factors into this observation. Could you elaborate more on the role of batch size in this context?\n\n5. Second line on Page 4: \"If $\\mathbb{E}[h]>0$, the condition is always violated\". When $\\mathbb{E}[h(x)]>0$, the RHS of Eq.(6) $<0$, doesn't it make the condition in Eq.(5) be valid? Why the condition is violated? Is it a typo or do I misunderstand anything?\n\nMinor comments:\n\n1. Notations are inconsistent in this paper. For example, $n$ serves as the representation of a norm type in the first paragraph on Page 2, where it is a scalar. Later in Section 3.1, $n$ is used as a fixed unit vector. In addition, there is a disparity in the definition of the loss function between Eq.(2) and Eq.(12), as they take different inputs.\n\n2. The paragraph after Definition 1: \"A sequence $z_t$ converges\" $\\Longrightarrow$ \"A sequence $\\theta_t$ converges\"\n\n3. The sentence after Eq.(2): \"The dynamics of $w$\", and the sentence before Eq.(10): \"the following condition implies that $w$...\", what is $w$ here? It is not defined.\n\n4. In the capitation of Figure 1: \"Phase **I** corresponds to\" $\\Longrightarrow$ \"Phase **Ia** corresponds to\"\n\n5. Paragraph after Eq.(7): \"The denominator is the Hessian ... signal in the gradient. The denominator is the strength ...\", do you mean the numerator for the first \"denominator\"?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1548/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698537294723,
            "cdate": 1698537294723,
            "tmdate": 1699636083133,
            "mdate": 1699636083133,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BO6vD0VuPi",
                "forum": "9grjdFDiAj",
                "replyto": "k8gEEKm1sl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1548/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1548/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply part 1"
                    },
                    "comment": {
                        "value": "Thank you for the detailed and constructive feedback. We answer both the weaknesses and questions below.\n\n\nWeaknesses:\n\n**1. The analysis in this work is restricted to the assumption that initialization point is near a given stationary point. While I acknowledge that such a limitation is not unique to this paper, it deviates from real-world scenarios. Numerous observations have indicated that the early phase of learning substantially impacts the ultimate generalization performance of deep neural networks. Consequently, centering on the dynamics around stationary points may fall short in elucidating the success of deep learning.**\n\nThanks for this criticism. This is a very important point that we want to clarify. We would like to point out that our theoretical analysis is fully compatible with the observation that the weights do not converge to stationary points but to stationary distributions. We have clarified this point in this revision. See our answer to Question 1 below.\n\n\n**2. In addition, this paper is poorly written and it needs substantial improvement.**\n\nThanks for this criticism. We have now fully updated the manuscript to incorporate the feedback from all the reviewers.\n\nQuestions:\nMajor concerns:\n\n**1. A recent study [1] shows that neural networks trained by gradient-based methods may not necessarily converge to stationary points. In other words, the gradients (norm) may not even vanish when these networks exhibit satisfactory performance. This observation raises questions about the applicability of probabilistic stability in understanding deep learning.\n[1] Jingzhao Zhang, et al. \"Neural network weights do not converge to stationary points: An invariant measure perspective.\" ICML 2022.**\n\nThank you for the criticism and this interesting reference. First of all, we would like to point out that Ref. [1] leaves the possibility open for models to converge to stationary points. For example, Ref. [1] states clearly in section A-1 that \u201cthe study of overparametrization and convergenece to stationary point may still be true in many cases.\u201d Thus, the analysis in this paper is likely applicable for the smaller models that are able to overfit the data. \n\nSecondly, our analysis can still apply when the model converges to a distribution, but not to a stationary point. The reason is that it is fully possible (and quite common) for SGD to converge to a point in some subspace, and at the same time converge to a nontrivial distribution overall. As a simple example to illustrate our point, let us consider a linear regression problem trained with SGD, with the per-batch loss: $l(w)=(w^Tx -y)^2$. Suppose $x$ takes a low-rank structure such that with probability one, $x$ is orthogonal to a vector $n$. Then, one can show that with probability one, $n^T \\nabla_w l(w) =0$. Namely, in the subspace specified by $n$, the model parameters stay at a single point during training (and thus its series converges to a point in all common notions of stability). In contrast, under common assumptions on the learning rate and data distribution, the dynamics of $w$ in the subspaces orthogonal to $n$ do not converge to a point but to a distribution. Another more advanced example is the phenomenon of the loss of plasticity, common in continual learning. Here, more and more neurons become dead (incoming and outgoing weights becoming zero gradients once and for all) as the training proceeds. In these tasks, the model never converges to fixed points, yet a subset of the parameters do converge to stationary points (for example, see https://arxiv.org/abs/2303.07507).\n\nThe reason for this is that the gradient covariance matrix of SGD in deep learning is highly structured and contains many degenerate directions. We have now included a numerical example of matrix factorization to show this: the overall gradient norm does not converge to zero, while the projection of the gradient norm in a subspace does converge to zero -- therefore, in this given subspace, SGD has converged to a stationary point. See Section A5 for the experiment. We have also added these references and discussion to the main text."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1548/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700664211869,
                "cdate": 1700664211869,
                "tmdate": 1700664211869,
                "mdate": 1700664211869,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "awMmcjadZk",
            "forum": "9grjdFDiAj",
            "replyto": "9grjdFDiAj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1548/Reviewer_bHzM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1548/Reviewer_bHzM"
            ],
            "content": {
                "summary": {
                    "value": "The authors investigate the behavior of SGD through the lens of probabilistic stability: convergence in probability to certain critical points. Dynamics are considered for quadratic losses (justified through local approximation about a critical point), with particular attention given to the effect of saddle points. Necessary and sufficient conditions are provided for probabilistic stability, first for rank-1 quadratic forms, and then for the general case. Probabilistic stability is compared with norm-based stability to highlight the inadequacy of norm-based approaches. Two synthetic examples are considered, and phase diagrams are presented to illustrate different behaviors according to the scale of the noise, and the step size. The phases are delineated as:\n\n- Ia. Probabilistic stability to correct solution\n- Ib. Stability in norm to correct solution\n- II. Probabilistic stability to incorrect solution\n- III. Convergence to low-rank saddle point\n- IV. Completely unstable\n\nUniversal behavior is implied when experiments on ResNet18 with CIFAR10 also display similar phases. Finally, selection of solutions is considered for SGD with a two-layer network w/ Swish activation, highlighting once again that norm-based convergence is an ineffective criterion."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Linearised dynamics are effective for investigating the late stages of training; well-suited for studying stability. \n- Convergence in probability is a much better condition to study than norm-based convergence; this is shown theoretically and empirically.\n- The examples are relatively simple, but are effective at demonstrating the phase transitions.\n- Universality of the phase diagrams even with real examples is fascinating"
                },
                "weaknesses": {
                    "value": "- Studying dynamics of SGD using Lyapunov-type criteria is hardly new, so the theoretical contributions here are particularly limited in their novelty. These conditions for ergodicity of random linear recurrence relations (which immediately imply probabilistic stability as stated here) are already well known [1]. Such conditions have been considered for stochastic optimizers in the ML literature as well [2,3].\n- The presentation of the phase diagrams is less than ideal, especially since this is perhaps the key contribution of the paper. Figures are confusing as presented: at the very least, they should be closer to where they are described in text, and empirical results should be compared more directly with theoretical findings. Terms are introduced here that do not appear to be explained. \n- Minor: a few typos throughout, e.g. eqn 6 missing an O in front of the cubic term and under eqn 10. \n\n[1] Diaconis, P., & Freedman, D. (1999). Iterated random functions. SIAM Review, 41(1), 45-76.\n\n[2] Gurbuzbalaban, M., Simsekli, U., & Zhu, L. (2021). The heavy-tail phenomenon in SGD. In International Conference on Machine Learning (pp. 3964-3975). PMLR.\n\n[3] Hodgkinson, L., & Mahoney, M. (2021). Multiplicative noise and heavy tails in stochastic optimization. In International Conference on Machine Learning (pp. 4262-4274). PMLR."
                },
                "questions": {
                    "value": "- Is sparsity/density the fraction of zero/non-zero elements in the matrix? This is my best guess, but it is surprising that there would be so many \"zero\" elements here; is there some larger cutoff which determines whether an element is \"zero\", or is something else considered here?\n- Is Figure 1 comparable to any other empirical examples? If not, what is the purpose of this figure?\n- Am I to interpret Figures 4 (right) and 5 (right) as displaying the same behavior as Figure 2 (right)? Can the theoretical prediction be overlaid here too?\n- Why is there an arrow from A to B in Figure 6? The text suggests that initialized at B, SGD jumps to C. \n- Is Figure 7 converging to Figure 2 as $N \\to \\infty$?\n- I assume convergence to the low-rank saddle should correlate with poor performance?\n- Can you outline the phases in the main text? These need to be displayed front and center. It is very confusing to have to refer to a figure legend towards the top of the paper for this. \n- Can you put the SGD solution selection part (including Figure 6) near Section 3.2? Otherwise, the purpose of this section is lost on first read."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1548/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698760713499,
            "cdate": 1698760713499,
            "tmdate": 1699636083054,
            "mdate": 1699636083054,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kh9Qia4O7w",
                "forum": "9grjdFDiAj",
                "replyto": "awMmcjadZk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1548/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1548/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply part 1"
                    },
                    "comment": {
                        "value": "Thank you for the detailed and constructive feedback. We answer both the weaknesses and questions below.\n\n\nWeaknesses:\n\n**1. Studying dynamics of SGD using Lyapunov-type criteria is hardly new, so the theoretical contributions here are particularly limited in their novelty. These conditions for ergodicity of random linear recurrence relations (which immediately imply probabilistic stability as stated here) are already well known [1]. Such conditions have been considered for stochastic optimizers in the ML literature as well [2,3].**\n\nThank you for pointing this out and for providing the references. We believe this criticism is a misunderstanding of the contribution of our work. First of all, let us clarify that we did not claim to be the first to use the Lyapunov-type criteria to study SGD. We have updated the manuscript to clarify this point. \n\nOur main contribution is to study the stability of saddle points in a neural network landscape -- although the analysis can also be extended to study local minima, this is not the emphasis of our work and so its discussion is only presented in the manuscript. The most important contribution of our work is to show both empirically and theoretically that it is both common and possible for SGD to converge to constrained saddle points -- and a crucial theoretical insight is that this type of convergence can ONLY be studied within the framework of probabilistic stability. We would like to emphasize the second point more: we did not just apply probabilistic stability or Lyapunov-type analysis to these saddles, but we also proved that this is the only correct notion, which is established by proposition 2. Neither of these contributions appear in [2,3] and do not follow trivially from any result in [1].\n\nThat being said, these references do relate to ours. However, compared to [2] and [3] and as explained above, our work has a different context and emphasis. Both [2] and [3] focus on studying the dynamics of SGD and its ergodicity near a local minimum. In contrast, the focus of our work is the attractivity and/or repulsiveness of the saddles. Thus, the findings in these papers do not apply immediately to the case we are interested in, that is, the linear stability near the saddle points.\n\nWe have cited these works and added discussion regarding their relation to our work.\n\n[1] Diaconis, P., & Freedman, D. (1999). Iterated random functions. SIAM Review, 41(1), 45-76.\n\n[2] Gurbuzbalaban, M., Simsekli, U., & Zhu, L. (2021). The heavy-tail phenomenon in SGD. In International Conference on Machine Learning (pp. 3964-3975). PMLR.\n\n[3] Hodgkinson, L., & Mahoney, M. (2021). Multiplicative noise and heavy tails in stochastic optimization. In International Conference on Machine Learning (pp. 4262-4274). PMLR.\n\n\n**2. The presentation of the phase diagrams is less than ideal, especially since this is perhaps the key contribution of the paper. Figures are confusing as presented: at the very least, they should be closer to where they are described in text, and empirical results should be compared more directly with theoretical findings. Terms are introduced here that do not appear to be explained.**\n\nThanks for this comment. We have improved the readability of the figures and clarified their connections to the main text. Also, see our answers to the questions below. \n\nThat being said, while the phase diagrams are indeed a key contribution of our work, we stress that it is not the only crucial contribution we have. As we pointed out in the answer to Weakness 1, the first (and perhaps more important) contribution of our work is to show that the saddle points in deep learning have to be studied within the framework of probabilistic stability -- here, proposition 2 plays a key role as it proves that no other existing stability definition can be used to analyze the attractivity of these saddles.\n\n\n**3. Minor: a few typos throughout, e.g. eqn 6 missing an O in front of the cubic term and under eqn 10.**\n\nThanks for pointing this out. We have fixed this typo in the revision."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1548/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700663482393,
                "cdate": 1700663482393,
                "tmdate": 1700663482393,
                "mdate": 1700663482393,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "y90dQI5oWf",
            "forum": "9grjdFDiAj",
            "replyto": "9grjdFDiAj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1548/Reviewer_Mfzw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1548/Reviewer_Mfzw"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a new stability notion, i.e., probabilistic stability, to study the stability of the SGD learning algorithm. The goal of proposing the new stability notion is to explain why deep learning models trained with SGD generalize well. The paper also revisits some variance-based stability notions and illustrates that those stability notions cannot explain the convergence of SGD."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The problem studied in this paper, i.e., explaining deep learning phenomena using a new stability notion, is valuable and interesting.\n2. The paper tackles this problem from a different angle, such as characterizing SGD dynamics from control theory."
                },
                "weaknesses": {
                    "value": "1. Literature on the variance-based stability notion is not adequately discussed in this paper. The paper presents the definition of the variance-based stability notion in Definition 2, but it is unclear what the current results are regarding this type of stability. Additionally, the reference for this stability notion cannot be found in this paper. It would be beneficial to include more discussions regarding the related work.\n2. The clarity of the paper can be improved. It is difficult to follow and extract the key points of each section that the paper wants to deliver. It is also hard to connect each section. For example, Section 3.1 shows that rank-1 dynamics are solvable, but how this connects with probabilistic stability is unclear. Section 3.2 jumps to the point that variance-based stability is insufficient, and it is hard to connect these two sections.\n3. The technical soundness and significance can be improved. Section 3 show that the linearized dynamics of SGD converges with probability under certain conditions, but it is difficult to establish a connection between this result and how it explains the generalization of SGD in deep learning. Section 4 discusses different phases of SGD learning, but it is unclear how these phases relate to the stability notions proposed in the paper and how they explain the generalization of SGD."
                },
                "questions": {
                    "value": "1. Can we understand that the probabilistic stability defined in this paper is more like a convergence guarantee?\n2. What is the literature on variance-based stability?\n3. In Definition 1 and 2, it would be great to clarify whether \u03b8* is fixed or a random variable. After Definition 1, there is a typo in the convergence in probability, \"< \u03b5\" should be revised to \"> \u03b5.\"\n4. Some typos:\n    1. After equation (2): \"The dynamics of w thus obeys Eq. (1).\" Here, w is not defined.\n    2. Before equation (10): \"Then, the following condition implies that w \u2192 p 0:\". Also, w is not defined.\n\n\n5. Figure 1: It would be great to explain the x-axis and y-axis in the caption (the same in Figure 2). Also, what are w_t and u_t? How are the phases (different colors) calculated based on values on the x-axis and y-axis?\n6. Figure 3: It would be great to explain how the color map is calculated and how convergence is calculated."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1548/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698797917270,
            "cdate": 1698797917270,
            "tmdate": 1699636082974,
            "mdate": 1699636082974,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "hw7i5WIr1W",
                "forum": "9grjdFDiAj",
                "replyto": "y90dQI5oWf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1548/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1548/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply part 1"
                    },
                    "comment": {
                        "value": "Thank you for the detailed and constructive feedback. We answer both the weaknesses and questions below.\n\n\nWeaknesses:\n\n**1. Literature on the variance-based stability notion is not adequately discussed in this paper. The paper presents the definition of the variance-based stability notion in Definition 2, but it is unclear what the current results are regarding this type of stability. Additionally, the reference for this stability notion cannot be found in this paper. It would be beneficial to include more discussions regarding the related work.**\n\nThank you for the detailed comments. We believe that your main misunderstanding comes from our insufficient discussion of why the notion of stability is important.\n\nIn the context of deep learning, the main purpose for both the variance-based stability (previous works) and probabilistic stability (this work) is to understand which class of solutions or stationary points of the loss function is preferred by the SGD-based training. The most well-known result of the variance-based stability is that at a large learning rate or small batch-size, SGD prefers flat local minima because sharp minima are unstable, and, thus, repulsive, and so SGD will eventually escape sharp minima because of unstable oscillation and fluctuation (for example, see Wu et al. (2018)). However, a major limitation of the variance-based stability is that it cannot be applied to understand the stability of saddle points -- which is the main topic of our work.\n\nWe have added references to the variance-based stability and clarified these points. See the discussion below definition 2.\n\n\n\n**2. The clarity of the paper can be improved. It is difficult to follow and extract the key points of each section that the paper wants to deliver. It is also hard to connect each section. For example, Section 3.1 shows that rank-1 dynamics are solvable, but how this connects with probabilistic stability is unclear. Section 3.2 jumps to the point that variance-based stability is insufficient, and it is hard to connect these two sections.**\n\nThank you for the comment. Please first see our answer to weakness 1 and note that the main focus of our work is this: why and when are saddle points attractive under SGD. It is to serve this purpose that the manuscript is organized.\n\nWhile there has been various empirical evidence that saddle points are attractive, there has not been any theory to show and understand how and why SGD can converge to these saddle points in deep learning. Our goal is to fill this important theoretical gap. Section 3 solves rank-1 dynamics under the probabilistic stability -- the main message here is that saddle points can indeed become attractive under the probabilistic stability under many choices of hyperparameter (large learning rate, for example).\n\nThe next question is whether we have any other theoretical tool to understand the attractivity of saddles under SGD, and this is the purpose of Section 3.2. Section 3.2 shows that perhaps surprisingly, no notions of stability based on the statistical moments can help us understand the attractivity of saddles. This establishes the probabilistic stability as something quite unique -- because one must study the probabilistic stability to understand the attractivity of the saddles.\n\nIn summary, sections 3.1 and 3.2 together establish that probabilistic stability is especially suited and perhaps the ONLY tool for studying why the saddle points are attractive in deep learning."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1548/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700662919180,
                "cdate": 1700662919180,
                "tmdate": 1700662919180,
                "mdate": 1700662919180,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]