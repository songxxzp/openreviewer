[
    {
        "title": "The Role of Forgetting in Fine-Tuning Reinforcement Learning Models"
    },
    {
        "review": {
            "id": "Hh7kgch9si",
            "forum": "FFvCjbhpDq",
            "replyto": "FFvCjbhpDq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7158/Reviewer_5ZRC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7158/Reviewer_5ZRC"
            ],
            "content": {
                "summary": {
                    "value": "This work investigates catastrophic forgetting in fine-tuning pre-trained reinforcement learning (RL) policies on subsequent tasks sequentially in a stationary environment while data distribution shifts. It first shows how fine-tuned policies would deteriorate in performance for previous tasks. Then, the paper identifies two conditions in which forgetting occurs, namely, state coverage gap and imperfect cloning gap. Experimentally, the work further shows how existing knowledge retention methods like elastic weight consolidation (EWC) mitigate forgetting during the fine-tuning process."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This is an important research problem for both the understanding of deep RL training and potential practical deployments. We have seen extensive studies on fine-tuning of supervised learning. The same aspect in RL is relatively less studied. As deep RL moves towards large-scale pretraining, understanding the best practices of fine-tuning with downstream tasks is crucial.\n2. The paper shows strong empirical analysis in understanding the problem, accompanied with extensive experimental results. I find the identification of the two conditions to be informative to researchers of this subfield\n3. The paper in general is clearly written with key results elaborately explained.\n4. Experimental results are comprehensively displayed. I particularly find figure 4 to be intuitive and helpful in visualizing the forgetting phenomenon."
                },
                "weaknesses": {
                    "value": "1. The choice of benchmarking algorithms for knowledge retention, although somewhat representative of existing methods, does not quite match with state-of-the-art approaches. Newer methods like [1], if added, can strengthen the conclusions of the paper.\n2. It is unclear to me how is this setting different from continual/lifelong RL\n3. [Minor] the term \u2018realistic RL algorithms\u2019 is confusing \n\n[1] Ben-Iwhiwhu, E., Nath, S., Pilly, P. K., Kolouri, S., & Soltoggio, A. (2022). Lifelong reinforcement learning with modulating masks. arXiv preprint arXiv:2212.11110."
                },
                "questions": {
                    "value": "1. How is non-stationary enironment different from data shifts in stationary environment? Is it not the same underlying data shift problem?\n2. What if we pretrain \u2018CLOSE\u2019 states first instead? Do we see better forward transfer?\n3. Can the authors provide their views on why pre-trained models (counterintuitively) do not seem to exhibit any signs of positive transfer? Existing methods do seem insufficient for RL to leverage pretraining\n4. Why is EWC missing in some of the subsequent experiments?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7158/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698698677420,
            "cdate": 1698698677420,
            "tmdate": 1699636848034,
            "mdate": 1699636848034,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "M3jcBG63xA",
                "forum": "FFvCjbhpDq",
                "replyto": "Hh7kgch9si",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7158/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7158/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 5ZRC"
                    },
                    "comment": {
                        "value": "We thank the Reviewer for the kind and insightful comments about our work. During the rebuttal period we performed additional experiments and improved the manuscript. In the general answer we list introduced changes and provide a clarification of the differences for the CL setup. We hope it is sufficient, please let us know if you have any more questions or comments.\n\n> How is a non-stationary environment different from data shifts in a stationary environment? Is it not the same underlying data shift problem?\n\nOn a high level view, the problem which we study can be framed as a \u2018data shift\u2019 problem. The difference from the standard online RL setup is the \u2018direction of changes\u2019 the training experiences FAR states only after seeing CLOSE ones. In the pre-training/fine-tuning scenario considered in this work, it might be otherwise, and, as we argue (see the General response), this will be more likely as researchers progress with building foundational models in RL. In this work, we highlight that this revered order may cause challenges. We hope this answers your question, please let us know, if you have any suggestions or further questions. This bit is quite challenging to describe in an unambiguous way.\n\n\n> Newer methods like [1], if added, can strengthen the conclusions of the paper.\n> Why is EWC missing in some of the subsequent experiments?\n\nSince our main goal is to show and analyze the forgetting problem in fine-tuning RL models, we decided to use relatively simple methods to avoid multiple overlapping effects. We discovered behavioral cloning to be better and more robust than L2 or EWC, so we decided to use it. For the same reason, we do not check more sophisticated continual learning methods on our benchmark. We believe that incorporating more sophisticated CL methods to the fine-tuning problem is an important next step, which we now mention in the Limitations section.\n\n> What if we pretrain \u2018CLOSE\u2019 states first instead? Do we see better forward transfer? \n\nWe thank the Reviewer for this interesting ablation study. To test it, we run additional experiments in the robotic domain, see Appendix D, paragraph \u201cOther sequences\u201d. In short, we do not observe forgetting and there is no significant gap between vanilla fine-tuning and knowledge retention methods. This is further evidence that the forgetting caused by the state visitation gap is at the heart of the problem of transfer.\n\n> Can the authors provide their views on why pre-trained models (counterintuitively) do not seem to exhibit any signs of positive transfer? Existing methods do seem insufficient for RL to leverage pretraining\n\nPre-trained models exhibit some positive transfer, which, however, varies substantially unless forgetting is under control. Our work is one of the first initial steps towards understanding this issue and building its taxonomy (see also [2] for an interesting benchmark of transfer in CL). Additionally, in the experiment suggested above by the Reviewer, we show that without the state visitation gap we can see a significant transfer even without any knowledge retention methods. At the same time, we do agree with the Reviewer that further research is needed in order to use pre-trained knowledge more efficiently. \n\n \n> [Minor] the term \u2018realistic RL algorithms\u2019 is confusing\n\nWe agree that the term is not precise and we get rid of it in the revised version of the paper. Thanks for pointing this out.\n\n[1] Ben-Iwhiwhu, E., Nath, S., Pilly, P. K., Kolouri, S., & Soltoggio, A. (2022). Lifelong reinforcement learning with modulating masks. arXiv preprint arXiv:2212.11110. \\\n[2] Wolczyk et al, Disentangling Transfer in Continual Reinforcement Learning, NeurIPS\u201922."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7158/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700073839966,
                "cdate": 1700073839966,
                "tmdate": 1700073839966,
                "mdate": 1700073839966,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "k1sOxmMDat",
                "forum": "FFvCjbhpDq",
                "replyto": "Hh7kgch9si",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7158/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7158/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 5ZRC30, revision 2"
                    },
                    "comment": {
                        "value": "Following your suggestion, we have conducted additional experiments incorporating EWC in Montezuma\u2019s Revenge. While we observed that EWC provides some benefit in mitigating forgetting, we found that behavioral cloning remains a more effective and robust approach for addressing the forgetting problem in our specific context. The results and a detailed analysis of these experiments are now included in our revised manuscript (figure 5, 6 and 7). We believe this comparison adds valuable insights to the discussion of different methods for addressing forgetting in fine-tuning RL models."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7158/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700507886349,
                "cdate": 1700507886349,
                "tmdate": 1700507886349,
                "mdate": 1700507886349,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "AR3TLEYuPM",
                "forum": "FFvCjbhpDq",
                "replyto": "k1sOxmMDat",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7158/Reviewer_5ZRC"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7158/Reviewer_5ZRC"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you very much for addressing my questions together with the additional experiments and discussion. My rating of this work remains to be positive. It is an informative and detailed contribution to this research area."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7158/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700560862807,
                "cdate": 1700560862807,
                "tmdate": 1700560862807,
                "mdate": 1700560862807,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "aqwaOeg5UQ",
            "forum": "FFvCjbhpDq",
            "replyto": "FFvCjbhpDq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7158/Reviewer_bshN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7158/Reviewer_bshN"
            ],
            "content": {
                "summary": {
                    "value": "This is an experimental paper that studies the forgetting issue in finetuning pre-trained models with RL. The paper focuses on two special cases of the problem: state coverage gap and imperfect cloning gap. To study the two problems respectively, the paper compares several existing methods in Meta-World, Montezuma's Revenge, and NetHack. Results shows that RL with behavior cloning on the pre-training dataset outperforms other methods, maintaining the pre-trained capabilities better during RL."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. Forgetting of the previously learned skills is a problem worth studying in RL. \n\n2. The paper refines this problem into two cases and conducts appropriate experimental evaluation."
                },
                "weaknesses": {
                    "value": "1. As an experimental paper studying forgetting, it lacks evaluation on many related methods. The paper only evaluates two kinds of  methods: parameter regularization and behavior cloning. But there exists many other methods addressing the forgetting issue in the literature of continual RL and finetuning with RL, like using offline RL over previous data [1], adding KL-divergence loss to the pre-trained policy on the online data [2], and a lot of methods in sharing representations and structures [3].\n\n2. The experimental results do not provide different insights into the two problems. All the results demonstrate that Finetuning+BC outperforms other methods and the vanilla Finetuning method suffers from forgetting on the FAR states. But beyond that, the results lack further analysis of the two problems and do not reflect the significance of dividing the forgetting problem into the two types.\n\n3. The paper has no novel contributions in methods and techniques. It also cannot provide insights in how to better address forgetting in the future work.\n\n[1] Modular Lifelong Reinforcement Learning via Neural Composition.\n\n[2] Video PreTraining (VPT): Learning to Act by Watching Unlabeled Online Videos.\n\n[3] Towards continual reinforcement learning: A review and perspectives."
                },
                "questions": {
                    "value": "1. Can the experimental results provide different insights into the two problems? In addition to these two problems, does the problem of forgetting include other cases?\n\n2. Based on the experimental results, are there any insights in improving the existing methods or further addressing the forgetting problem?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7158/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698754355693,
            "cdate": 1698754355693,
            "tmdate": 1699636847907,
            "mdate": 1699636847907,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wwBF8Evpq1",
                "forum": "FFvCjbhpDq",
                "replyto": "aqwaOeg5UQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7158/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7158/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer bshN"
                    },
                    "comment": {
                        "value": "We thank the Reviewer for their insightful review and useful comments. We note that in the revised version we provide multiple clarifications and new experiments, listed in the general answer. We would be happy to address more questions and suggestions.\n\n> As an experimental paper studying forgetting, it lacks evaluation on many related methods\n\nWe thank the reviewer for this suggestion. Since our main goal is to show and analyze the forgetting problem in fine-tuning RL models, as we discuss in General response, we decided to use relatively simple tools to mitigate it. We discovered behavioral cloning to be a better performing and a more robust approach than L2 or EWC, so we decided to use it. For the same reason, we do not check more sophisticated continual learning methods on our benchmark, even though we believe they might also lead to amazing results. However, we agree with the Reviewer that testing other approaches would be a great idea for further studies.\n\nWe thank the reviewer for providing valuable references. Authors of [1] use batch RL in order to maintain performance on the previous tasks. We take advantage of the fact that SAC is an off-policy method that has been a basis for multiple offline RL algorithms [4, 5] and we are currently running experiments where we keep the data from the previous tasks in the buffer. We will update the experiments once they are done. At the same time, we would like to point out that this approach would not be viable in the case of NetHack, where we pre-train the model using behavioral cloning and as such we do not have a critic network to clone.\n\nVPT [2] includes a different type of distillation, but they do not specify what data exactly they are using, e.g. are the trajectories samples from the student or the teacher policy. In order to better understand this problem, we perform further studies of two variations of distillation: (a) where the trajectories are obtained from the student (i.e. known as kickstarting in the literature [6]) (b) and where the targets are obtained from the expert dataset that the pre-trained policy aimed to clone rather than from the pre-trained policy itself. Surprisingly, we find that both of these approaches perform worse than our standard strategy. The full results and analysis are shown in Appendix F, paragraph \u201cDifferent approaches to distillation\u201d.\n\nFinally, although we believe methods relying on sharing representations and structures [3] could bring substantial improvements to the studied problem, to the best of our knowledge most approaches in this family require clear task separation. In the RL fine-tuning setting it might not be trivial to partition the state space into FAR and CLOSE states, and as such applying these methods would be difficult. However, we believe finding efficient ways to apply such approaches to be an important future research direction. We agree with the reviewer that incorporating a wider variety of CL methods to the fine-tuning problem would be useful, and we add this point to the Limitations section in the revised manuscript.\n\n> Can the experimental results provide different insights into the two problems?\n\nWe use the two problems to highlight natural circumstances in which forgetting of pre-trained capabilities occurs. That is, when the state spaces of the upstream and downstream task overlap but are not identical [7, 8, 9], and when the model is pretrained on offline data and finetuned online [10, 11, 12]. \n\nThere are subtle differences between these two settings. In particular, with the imperfect cloning gap, it is far more difficult to assess in which states we should minimize forgetting. As such, one should be careful about where to apply distillation techniques, as we show through new experiments in Appendix F. \n\n> In addition to these two problems, does the problem of forgetting include other cases?\n\nAnother natural case is the related set of tasks that share the same state space but differ in the reward functions. \n\n> Based on the experimental results, are there any insights in improving the existing methods or further addressing the forgetting problem?\n\nAn important conclusion (see Appendix F) is that it is probably beneficial to use regularization techniques which \u2018strength\u2019 depends on the state and the phase of training. Yes, in the future work we provide multiple open research directions. \n\n[4] Conservative q-learning for offline reinforcement learning \\\n[5] Uncertainty-Based Offline Reinforcement Learning with Diversified Q-Ensemble \\\n[6] Kickstarting Deep Reinforcement Learning \\\n[7] Probing Transfer in Deep Reinforcement Learning without Task Engineering \\\n[8] Progressive neural networks. \\\n[9] Actor-mimic: Deep multitask and transfer reinforcement learning \\\n[10] Video PreTraining (VPT): Learning to Act by Watching Unlabeled Online Videos \\\n[11] Accelerating Online Reinforcement Learning with Offline Datasets \\\n[12] Adaptive Policy Learning for Offline-to-Online Reinforcement Learning \\"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7158/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700073771976,
                "cdate": 1700073771976,
                "tmdate": 1700073771976,
                "mdate": 1700073771976,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "61JFH35lVh",
                "forum": "FFvCjbhpDq",
                "replyto": "aqwaOeg5UQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7158/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7158/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer bshN31, revision 2"
                    },
                    "comment": {
                        "value": "Following your suggestion, we implemented and evaluated an additional method on Continual World; Episodic Memory (EM), in which we populate SAC\u2019s replay buffer with trajectories from the pre-training tasks. We observe that EM works slightly better than EWC, but worse than behavioral cloning. \n\nAdditionally, we run experiments with EWC on Montezuma\u2019s Revenge. The results show that EWC provides improvements, but is not as good as Behavioral Cloning. Details of the experiments are presented in Figure 5.\n\nWe believe that overall new experiments and analyses considerably strengthen our paper. We, thus, gently ask you to consider raising the score or pointing out remaining deficiencies."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7158/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700507847334,
                "cdate": 1700507847334,
                "tmdate": 1700507847334,
                "mdate": 1700507847334,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "X1vFgAJU2b",
            "forum": "FFvCjbhpDq",
            "replyto": "FFvCjbhpDq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7158/Reviewer_BuFP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7158/Reviewer_BuFP"
            ],
            "content": {
                "summary": {
                    "value": "Summary:\n\nThis paper studies fine-tuning in RL, and specifically the issue of forgetting and potential mitigation strategies. They demonstrate in several settings (simulated robot manipulation, Montezuma's Revenge and NetHack) that if a policy is pretrained on some part of the state space which is far from the initial state distribution, the knowledge is often forgotten and there are little to no improvements over training from scratch. They furthermore investigate different knowledge retention strategies (such as L2 penalties between pretrained and fine-tuned policy weights, possibly weighted by fisher information, as well as simple BC regularization on the pretraining data). They find that BC regularization helps the most, and can help prevent forgetting the behaviors encoded in the pretrained policy."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "- The paper's main takeaway message, that adding BC regularization helps avoid forgetting previous behaviors during fine-tuning, is well supported by the experiments. This is demonstrated in 3 environment, including continuous control (MetaWorld), a pixel-based Atari game (Montezuma's Revenge) and a procedurally generated, long-horizon game with complex dynamics (NetHack). \n\n- The paper does a nice job with their analysis and visualizations illustrating the forgetting behavior."
                },
                "weaknesses": {
                    "value": "- The main takeaway, which is essentially that co-training on the old tasks prevents forgetting when learning a new task, is pretty unsurprising and has been demonstrated before in previous works in continual learning both for the supervised case and the RL case. It's not clear what the contribution of this work adds. \n- An obvious downside of co-training on previous tasks is that the memory requirement increases linearly with the number of tasks and the computation increases quadratically - this is not adequately discussed. \n- It would have been nice to include result for the L2 and EWC on Montezuma and NetHack."
                },
                "questions": {
                    "value": "Some suggestions on the writing:\n\n- In the intro, it would be helpful to give a bit more details on the \"knowledge retention techniques\" used to mitigate the forgetting problems. Currently, the ready does not have much idea on the methodological aspects going into the paper. \n\n- Example in 2-state MDP: the notation here is confusing. Both $theta$ and $f_\\theta$ are used before being defined. Please add the definitions in the main text."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7158/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699121669647,
            "cdate": 1699121669647,
            "tmdate": 1699636847788,
            "mdate": 1699636847788,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KXgGrbEUpd",
                "forum": "FFvCjbhpDq",
                "replyto": "X1vFgAJU2b",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7158/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7158/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer BuFP"
                    },
                    "comment": {
                        "value": "We would like to thank the Reviewer for the kind comments about the paper\u2019s experimental value, and for the interesting suggestions. We address them below. Additionally, we conducted new experiments, please see the general response for a full list.\n\n\n> The main takeaway, which is essentially that co-training on the old tasks prevents forgetting when learning a new task, is pretty unsurprising and has been demonstrated before in previous works in continual learning both for the supervised case and the RL case. It's not clear what the contribution of this work adds.\n\nWe respectfully disagree with the Reviewer regarding the value of the main takeaway. As detailed in the general answer, we consider the conceptualization of 'forgetting of pre-trained capabilities' to be non-obvious and important. We study a new perspective. More precisely, we are not aware of any work comprehensively showing that forgetting is detrimental to transfer in RL fine-tuning scenarios. Please let us know if you know any; we would be happy to include and discuss them.\n\n> An obvious downside of co-training on previous tasks is that the memory requirement increases linearly with the number of tasks and the computation increases quadratically - this is not adequately discussed.\n\nWe thank the Reviewer for raising this important issue. To address this we:\n- include additional discussion in the Limitations section,\n- provide additional experiments in Appendix D, which indicate that as little as 100 samples is sufficient to observe a significant improvement over fine-tuning. We typically use 10000 transitions sampled out of 5M steps for training, which amounts to roughly 3.5MB of memory. We argue that, compared to the current hardware capabilities, this is negligible.\n\n> It would have been nice to include results for the L2 and EWC on Montezuma and NetHack.\n\nIn the robotic experiments, we discovered that behavioral cloning performs better and is more robust than L2 or EWC, and we anticipated the same for the other environments. However, we consider designing and benchmarking other methods to be a very important direction of future work, which is indicated in the Limitations section.\n\n> In the intro, it would be helpful to give a bit more details on the \"knowledge retention techniques\u201d\n> Example in 2-state MDP: the notation here is confusing. \n\nWe thank the Reviewer for these suggestions. In the revised version of the manuscript we added a short introduction to knowledge retention techniques and we clarified the notation in the MDP examples. Please let us know if you have any further comments."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7158/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700073640426,
                "cdate": 1700073640426,
                "tmdate": 1700073640426,
                "mdate": 1700073640426,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "48yVieaBqM",
                "forum": "FFvCjbhpDq",
                "replyto": "X1vFgAJU2b",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7158/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7158/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer BuFP04, revision 2"
                    },
                    "comment": {
                        "value": "We are happy to announce new results. \n\nFollowing your suggestion, we ran additional experiments with EWC on Montezuma\u2019s Revenge. Our findings indicate that while EWC provides improvements, it still fails short compared to performance achieved by Behavioral Cloning. Detailed results and analyses of these experiments are presented in Figure 5 the revised manuscript. \n\nThank you once again for your constructive feedback, which improves our work. We believe that our paper improved considerably during the rebuttal period. We, thus, gently ask you to consider raising the score, or providing us with further questions."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7158/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700507799833,
                "cdate": 1700507799833,
                "tmdate": 1700507799833,
                "mdate": 1700507799833,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "EhQYXXxdfR",
            "forum": "FFvCjbhpDq",
            "replyto": "FFvCjbhpDq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7158/Reviewer_dWac"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7158/Reviewer_dWac"
            ],
            "content": {
                "summary": {
                    "value": "This paper examines finetuning of pretrained RL agents in a single environment. Two problematic mechanisms are identified. A state coverage gap occurs when the agent is pretrained on a part of the state space but, in the fintetuning phase, has to first learn a policy on a different part. Then, the policy on the first part of the state space is lost during finetuning and must be relearned. The second, the imperfect cloning gap, occurs when the agent is pretrained through imitation learning. As the policy is finetuned, the performance on states later in trajectories also degrades. \nThe use of behavioru cloning on states from the first task and other forgetting mitigation techniques are shown to solve these issues. A variety of environments are considered including toy tasks, metaworld and Nethack to demonstrate the problem and the utility of the solutions."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- There are extensive experiments on a variety of environments. The sequence of metaworld tasks was an interesting custom addition. \n\n- The identified problem could be relevant in a variety of practical settings. The imperfect cloning gap seems to be particularly applicable since we may often want to start with imitation learning from previous policies if possible. \n\n- There was sufficient detail in the text to understand the experiments and the figures were clear in general."
                },
                "weaknesses": {
                    "value": "- The clarity of certain sections could be improved with more details. For example, for the initial toy example, it would help if the main text explained the motivation behind the MDPs design a little more: why that particular choice of transitions and rewards was made. Also, $f_0$ is not described in the main text and it looks like subfigures b) and c) are interchanged for this example.\n\n- The main proposed solution, behaviour cloning from the pretrained policy, seems to be somewhat limited. Behaviour cloning is inherently limited by the quality of the pretrained policy. The experiments show that it's possible to retain the pretrained policy's performance but not exceed it.\n\n- While novelty is difficult to judge, it seems like the identified problematic phenomena are facets of catastrophic forgetting i.e. the idea that neural networks will forget on certain parts of the input space after being trained on others. In this view, it's not too surprising that pretraining on one part of the state space will lead to a detoriation of performance in another. I can appreciate that there's value in demonstrating this in an RL setting though."
                },
                "questions": {
                    "value": "- Are the benefits of pretraining purely from learning a good policy? Are there benefits due to the representations learned in the pretraining phase? \n\n- Have you experimented with the agent only learning a decent, but not great, policy on the FAR tasks? Could we expect to surpass the performance of the pretrained policy? Using behaviour cloning would seem to be limited by the pretrained policy.\n\n- Have you considered off-policy methods? It seems like there may be an advantage since these methods could simply keep around samples (or trajectories) from previous tasks in the replay buffer to learn from without having to necessarily imitate the previous behaviours.\n\n- In the robotic sequence task, have you tried pretraining on the second and third tasks? Are there any learning benefits for the 4th task if you do so? \n\n- In Montezuma's revenge, how far is the agent able to reach without pretraining? Does it get past room 7 consistently? It would be nice to see the overall learning curves of the agent that has been pretrained vs. the one that has not.\n\n- Nethack levels are generated procedurally. How are the sequence of levels chosen for these experiments? When the agent is reinitialized, is it to a fixed level with the same seed?\n\n- For the Nethack experiment, Fig.6, how come the learning curve for finetuning only matches that of the original agent after 2 billion steps? It looks like, if training continued further, the pretrained agent would even do worse.\n\n- The Sokoban results (fig.7) seem to be fairly poor for both agents since they can only fill less than 1.5 pits on average---not close to a solution. Is this to be expected?\n\nMinor points:\n- I would consider moving some more of the results from the appendix to the main text since it looks like there's still space remaining.\n\n- In Fig. 4, I would consider changing the text \"pre-trained optimal policy\" to \"pre-trained expert policy\" since we don't necessarily have the optimal policy in those environments."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7158/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7158/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7158/Reviewer_dWac"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7158/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699158908238,
            "cdate": 1699158908238,
            "tmdate": 1700687189346,
            "mdate": 1700687189346,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "iQ6hIyEjRn",
                "forum": "FFvCjbhpDq",
                "replyto": "EhQYXXxdfR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7158/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7158/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer dWac"
                    },
                    "comment": {
                        "value": "We thank the Reviewer for a meticulous review and the many useful suggestions. We are grateful for them and we are happy to accept more.\n\nAs to the novelty, we indeed focus on the RL setting and we consider the value of our work to lay in the conceptualization of the influence of forgetting on transfer. Please see the general answer for more details. We also note that many new experiments have been added in the revised version. \n\n\n> The clarity of certain sections could be improved with more details.\n\nThank you for bringing up the issue of clarity of the MDP examples. We have made improvements in the revised version. We also enhanced the clarity of the paper in other sections. All changes are highlighted in blue. Please let us know if you have any more suggestions.\n\n> Are the benefits of pretraining purely from learning a good policy? Are there benefits due to the representations learned in the pretraining phase?\n\nWe thank the reviewer for these questions \u2013 the issue of disentangling the impact of the policy from the impact of the representation is an interesting one. Although there are several studies on the impact of representation learning on transfer in supervised learning [1, 2], the same question in RL remains relatively understudied. We run new experiments to better understand this problem. Please see Appendix D, paragraph \u201cImpact of representation vs policy on transfer\u201d, where we describe experiments with resetting the last layer of the networks before starting the training. As such, the policy at the beginning is random even on the tasks known from pre-training, but has features relevant to solving the downstream tasks. The results suggest that in fine-tuning the benefits come from transferring both policy and representations, and for EWC in particular the representation transfer is more prominent.\n\n\n> Have you experimented with the agent only learning a decent, but not great, policy on the FAR tasks? Could we expect to surpass the performance of the pretrained policy? Using behaviour cloning would seem to be limited by the pretrained policy.\n\nWe agree this is a relevant concern. However, in our experimental results, in many cases fine-tuning with knowledge retention outperforms the level of the pre-trained model. \n\nFor example, on NetHack, the average return of a pre-trained model is 1000, which is outperformed by the BC-regularized fine-tuning agent by a factor of 3.5x, with the same pattern holding for deeper levels (aka FAR states); see Figure 6. On Montezuma\u2019s Revenge, we observe that in terms of completing rooms present in pre-training, PPO-BC manages to outperform the pre-trained model in some cases, see Appendix D, Figure 19. In the robotic manipulation tasks, fine-tuning does not outperform the pre-trained policies since the latter are close to optimal.\n\nWe speculate that outperforming the pre-trained policy is due to the fact that regularization is a \u2018soft bias\u2019, which can be controlled with hyperparameters (e.g., the KL coefficient), expert data, or the policy network size. As such, this bias can be overwritten in the fine-tuning optimization. Although we found it rather easy to tune, there are also rare negative cases (we detail one of them in Appendix F, paragraph \u201cDifferent approaches to distillation\u201d). How to efficiently control this 'soft bias' throughout training is an interesting research question, and we included it in the Limitations section.\n\n> Have you considered off-policy methods? It seems like there may be an advantage since these methods could simply keep around samples (or trajectories) from previous tasks in the replay buffer to learn from without having to necessarily imitate the previous behaviours.\n\nWe agree with the Reviewer that off-policy methods seem well-suited to deal with the forgetting problem. However, on-policy approaches are often better adjusted to settings that require processing a high number of trajectories to facilitate learning: Montezuma's Revenge and NetHack are examples of such environments. As such, we focus on knowledge retention methods that can be used in a wider variety of problems. At the same time, we agree that this is an interesting research direction and we are currently conducting such experiments on the RoboticSequence benchmark with Soft Actor-Critic. We will update the manuscript once they are ready."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7158/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700073440569,
                "cdate": 1700073440569,
                "tmdate": 1700073440569,
                "mdate": 1700073440569,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DLWVdE7MWg",
                "forum": "FFvCjbhpDq",
                "replyto": "EhQYXXxdfR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7158/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7158/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer dWac, pt. 2"
                    },
                    "comment": {
                        "value": "> In the robotic sequence task, have you tried pretraining on the second and third tasks? Are there any learning benefits for the 4th task if you do so?\n\nWe appreciate the suggestion for an interesting experiment. We conduct such a study and present the results in Appendix D, paragraph \u201cOther sequences\u201d, i.e. we pre-train the model on the second and third goal from the downstream environment. The relative performance of all methods is the same as in our original ordering, however, we observe that EWC almost matches the score of BC. We note that in terms of asymptotic performance on the 4th task knowledge retention methods perform much better than vanilla fine-tuning, which in turn is better than a model trained from scratch.\n\n> In the Montezuma's Revenge, how far is the agent able to reach without pretraining? Does it get past room 7 consistently? It would be nice to see the overall learning curves of the agent that has been pretrained vs. the one that has not.\n\nThe agent is able to reach FAR states without pre-training, however only in the later stages of training. We include the visualization of time spent in different rooms across training time in Appendix E. It takes the agent learned from scratch more than 3x longer to start entering further rooms when compared with the fine-tuned one. \n\n> Nethack levels are generated procedurally. How are the sequence of levels chosen for these experiments? When the agent is reinitialized, is it to a fixed level with the same seed?\n\nIn terms of training, we run the experiments on the standard NLE environment without any modifications: the levels in each episode are generated randomly. The FAR states in this scenario are not any specific levels but rather related to challenges in the later parts of the game. In this vein, the dungeon level N in Figure 7 denotes these states, which are encountered after passing the first N-1 level. These are stochastic, in practice, we collect a sample of 200 states. We thank the reviewer for this comment and we modify the manuscript to include a further explanation of these issues in Section 4 as well as Appendix B.2.\n\n> For the Nethack experiment, Fig.6, how come the learning curve for finetuning only matches that of the original agent after 2 billion steps? It looks like, if training continued further, the pretrained agent would even do worse.\n\nFine-tuning (solid green line) starts with the performance of the pre-trained agent (dashed blue horizontal line). After 2B steps an agent trained from scratch (solid orange line) matches the performance of the fine-tuned agent around the reward value of 2500. Our preliminary results suggest that the performance of these two plateaus at that level. This is consistent with our main takeaway: knowledge transfer from a pre-trained model is greatly hindered due to forgetting of capabilities on the \u2018FAR\u2019 states.\n\n\n> The Sokoban results (fig.7) seem to be fairly poor for both agents since they can only fill less than 1.5 pits on average---not close to a solution. Is this to be expected?\n\nYes, this is expected. The original Sokoban (i.e., moving boulders) itself is a complex game, save the fact that the Nethack agent still needs to fight monsters. In Appendix F, we added a more detailed explanation of this matter. \n\n> In Fig. 4, I would consider changing the text \"pre-trained optimal policy\" to \"pre-trained expert policy\" since we don't necessarily have the optimal policy in those environments.\n\nThank you for the suggestion, we corrected it in the revised version of the manuscript.\n\n[1] Neyshabur, B., Sedghi, H., & Zhang, C. (2020). What is being transferred in transfer learning?. Advances in neural information processing systems, 33, 512-523. \\\n[2] Kornblith, S., Chen, T., Lee, H., & Norouzi, M. (2021). Why do better loss functions lead to less transferable features?. Advances in Neural Information Processing Systems, 34, 28648-28662."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7158/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700073548958,
                "cdate": 1700073548958,
                "tmdate": 1700073548958,
                "mdate": 1700073548958,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mPSD85cF4G",
                "forum": "FFvCjbhpDq",
                "replyto": "EhQYXXxdfR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7158/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7158/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer dWac05, revision 2"
                    },
                    "comment": {
                        "value": "We are happy to announce new results. \n\nFollowing your suggestion, we implemented and evaluated an additional method on Continual World; Episodic Memory (EM), in which we populate SAC\u2019s replay buffer with trajectories from the pre-training tasks. We observe that EM works slightly better than EWC, but worse than behavioral cloning.\n\nAdditionally, we run experiments with EWC on Montezuma\u2019s Revenge. The results show that EWC provides improvements, but is not as good as Behavioral Cloning. Details of the experiments are presented in Figures 5.\n\nPlease let us know if these experiments and comments address your concerns. Otherwise we ask you to consider raising the score."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7158/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700507752169,
                "cdate": 1700507752169,
                "tmdate": 1700507752169,
                "mdate": 1700507752169,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4B9tNx0mrN",
                "forum": "FFvCjbhpDq",
                "replyto": "EhQYXXxdfR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7158/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7158/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Additionally, in the latest revision, we include experiments with EWC on NetHack and we run an experiment to answer the Reviewer\u2019s question about what would happen if we kept training a randomly initialized model (APPO) for a larger number of steps. Indeed, results presented in Appendix F, paragraph \u201cTraining from scratch with more steps\u201d, show that a model trained from scratch manages to outperform pure fine-tuning. However, after some time the training stagnates and in the end falls significantly short of fine-tuning with additional knowledge retention (e.g. APPO-BC)."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7158/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700591598247,
                "cdate": 1700591598247,
                "tmdate": 1700591598247,
                "mdate": 1700591598247,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xObCKcBY22",
                "forum": "FFvCjbhpDq",
                "replyto": "4B9tNx0mrN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7158/Reviewer_dWac"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7158/Reviewer_dWac"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the clarifications and additions to the paper. \nI am willing to revise my score upwards."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7158/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700687162120,
                "cdate": 1700687162120,
                "tmdate": 1700687162120,
                "mdate": 1700687162120,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]