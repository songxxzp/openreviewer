[
    {
        "title": "In-context Curriculum for Mathematical Reasoning in Small Language Models"
    },
    {
        "review": {
            "id": "WUESriaJ8x",
            "forum": "kcKfkJiKw5",
            "replyto": "kcKfkJiKw5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8651/Reviewer_LFaS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8651/Reviewer_LFaS"
            ],
            "content": {
                "summary": {
                    "value": "Context is crucial for the adaptability of language models. This adaptability can be enhanced without changing model parameters through In-context Learning (ICL), which incorporates demonstrations and instructions as input. This paper aims to investigate the possibility of harnessing emergent abilities, like mathematical reasoning, in smaller models. The Chain-of-Thought (COT) reasoning is highlighted as an effective method for larger models, but its applicability to smaller models is questioned. The paper introduces the In-context Curriculum (ICC) prompt, which resembles curriculum learning. Experiments are based on the GSM8K dataset, with knowledge distillation from larger models (like GPT 3.5) forming the baseline."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper focuses on an intriguing and timely problem: how to harness the emergent abilities typical of Large Language Models (LLMs) in more resource-efficient Small Language Models (SLMs). If that was feasible, and if the paper had proposed an effective way to achieve that goal, it would be a significant contribution."
                },
                "weaknesses": {
                    "value": "The proposed method is not at the technical level/depth that we expect from an ICLR paper.  The method simply uses some out-of-distribution datasets to strengeth the performance of in-context learning. There is nothing fundamentally new in this idea. Plus it is very unclear how these methods generalize to other problem domains."
                },
                "questions": {
                    "value": "To provide a significant research contribution, the paper will need to be extended in some major ways:\n- make the problem more general -- not so specific to the GSM8K dataset and the other datasets being used. \n- increase the depth and novelty of the proposed method. In its current formulation, the proposed method does not seem any more sophisticated than standard prompt engineering, often described in blogs and other online resources.\n- make sure that the experimental results are reproducible by others."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8651/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8651/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8651/Reviewer_LFaS"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8651/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698518523352,
            "cdate": 1698518523352,
            "tmdate": 1699637084058,
            "mdate": 1699637084058,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "acXCpnlfBZ",
            "forum": "kcKfkJiKw5",
            "replyto": "kcKfkJiKw5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8651/Reviewer_VGtM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8651/Reviewer_VGtM"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a prompting method for smaller language models for mathematical reasoning. The authors propose and compare two methods, one based on a static prompt that contains examples of increasing complexity from other datasets (ICCR), and a second one based on dynamically selected examples (ICCKA). Experiments performed on GSM8k show mixed results."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The paper tackles an interesting and current problem, of making small language models perform complex multi-step reasoning, that is well situated in the context of prior work (e.g. Tiny Stories, and Fu et al's ICML 2023 paper that is cited here).\n\nThe idea of building a \"curriculum\" of in-context examples might be new, as far as I'm aware. I also haven't seen exploring fine-tuning and in-context example selection together."
                },
                "weaknesses": {
                    "value": "The presentation of the method is a bit vague. I had to re-read several times, and I'm not sure I understand what exactly happens in ICCR and ICCKA. For ICCR, there seems to be some dataset weighing mechanism that is not fully described. For ICCKA, the example selection method has little detail in Section 3.3.\n\nThe evaluation in the paper is extremely narrow: only T5 models (fine-tuning only T5 base, while prompting the larger T5 models), and only on GSM8k. Moreover, the effects obtained on GSM8k are also rather small (1-2 absolute percent in the best cases, with mixed results everywhere). This makes it quite hard to tell whether even these gains are in fact caused by the intuitions given in the paper (e.g., the idealized idea of a curriculum), or random low-level effects from the combination of T5, GSM8k and the prompts used.\n\nThere is very limited discussion on what qualitatively happened in the experiments. Given the small absolute results, it might indeed be hard to find examples that illustrate \"why\" the method might help in some cases.\n\nOverall, I unfortunately do not know what to take away from the paper, and I'm not convinced most readers in ICLR would take away a clear message either from the ideas or from the results here. Since the results were marginal and the idea is quite specific, I'm not sure what generalizes here."
                },
                "questions": {
                    "value": "- For ICCR, what exactly is the weighing of each dataset? You mention that MathQA has the least importance -- what importance is given to it, and why, since it seems like it has the most relevant CoT examples?\n- For ICCKA, the paper mentions that it selects \"favoring examples that utilize the same operator\". Is this an explicit heuristic, or is it just an observation of what empirically tends to happen?\n- What sentence embedding model was used to select examples?\n- Are there any representative examples showing how the behavior of T5 changes with your prompt? These would be useful to include in the Appendix, which is currently empty.\n- What are the most comparable results from Fu et al, 2023? It would help to have them side-by-side in the paper. Note that their paper used code-davinci-002 as the teacher model, which seems to not be the case here."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8651/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8651/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8651/Reviewer_VGtM"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8651/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698619735993,
            "cdate": 1698619735993,
            "tmdate": 1699637083937,
            "mdate": 1699637083937,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "Z7y5h4DKej",
            "forum": "kcKfkJiKw5",
            "replyto": "kcKfkJiKw5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8651/Reviewer_Rowt"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8651/Reviewer_Rowt"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new way to design in-context examples, particularly for mathematical reasoning using small language models. By varying the complexity of demonstrations (from a simple single reasoning step to more complex multi-step reasoning examples), the proposed method aims to provide a comprehensive demonstration and knowledge to the small LLMs. Empirical evaluations show that the proposed in-context examples improve the performance of small language models on mathematical reasoning tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The idea is elegant and reasonable. There have been many studies on in-context example selections to improve the performance of language models. In comparison, this paper proposes a new perspective of in-context example selection: to include comprehensive different in-context examples that cover different kinds of knowledge (from simple to complex)."
                },
                "weaknesses": {
                    "value": "1. **Limited applicable scenarios.** It is a good idea to decompose in-context examples into different complexity levels to provide more comprehensive knowledge for small language models. However, the proposed method only covers mathematical reasoning tasks, where it is relatively easy to decompose in-context examples into multiple complexity levels. It would be important to discuss how to extend the proposed method to other tasks.\n2. **Only applicable to small language models.** The proposed method aims to provide comprehensive knowledge of small language models to adapt them for mathematical reasoning tasks. However, considering the LLMs already have enough domain-specific knowledge in terms of math, the proposed method might be ineffective with LLMs. A better way to demonstrate the effectiveness of the proposed method would be testing LLMs on a specific domain where even LLMs are not good at. In that case, we can better test if providing more comprehensive domain-specific knowledge can help language models finish the task.\n3. **The performance improvement is not significant.** Intuitively the idea should work considering more comprehensive knowledge is provided. However, according to the numbers in Table 1-3, the performance improvement is actually very limited. \n\nIn summary, the idea is elegant and promising, while the applicable scenarios and evaluation results indicate that this paper still need further revision to make it more effective."
                },
                "questions": {
                    "value": "Please refer to the weakness above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8651/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698875401968,
            "cdate": 1698875401968,
            "tmdate": 1699637083836,
            "mdate": 1699637083836,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "86uv5wnAHf",
            "forum": "kcKfkJiKw5",
            "replyto": "kcKfkJiKw5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8651/Reviewer_QtAE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8651/Reviewer_QtAE"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes using a curriculum-style prompt to improve the performance of small language models (SLMs) on mathematical reasoning tasks. ICC mirrors a curriculum learning strategy that starts with simple single-step COT examples and then increases complexity towards some multi-step examples. Their experiments on the GSM8K math reasoning dataset show ICCR outperforms the baseline COT prompt method when specializing in a 250M parameter SLM."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper introduces a sensible curriculum-based prompting strategy to address the limitations of SLMs for complex reasoning. \n2. The authors compare multiple prompt designs including integration of external datasets and knowledge augmentation. Their experiments thoroughly evaluate specialization and in-context learning scenarios across model sizes. \n3. The authors provide useful analysis and ablation studies to understand key factors in prompt design."
                },
                "weaknesses": {
                    "value": "1. ICC prompt variations are heuristic designs lacking a formalization or learning component.\n2. Evaluates on a single mathematical reasoning dataset (GSM8K) - unclear if findings generalize.\n3. The knowledge augmentation (ICCKA) strategy appears ineffective but the reasons are not fully explored.\n4. Qualitative analysis of differences in mistakes between small and large LMs could further inform prompt design."
                },
                "questions": {
                    "value": "1. Is there a way to make the ICC prompting more adaptive or learned as opposed to pre-defined heuristic curricula?\n2. Have you considered evaluating prompt designs on broader mathematical reasoning datasets besides GSM8K?\n3. Could you provide examples of mistakes commonly made by SLMs vs LLMs to better understand differences in their reasoning?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8651/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698956481217,
            "cdate": 1698956481217,
            "tmdate": 1699637083736,
            "mdate": 1699637083736,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "cSRKnwjxYm",
            "forum": "kcKfkJiKw5",
            "replyto": "kcKfkJiKw5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8651/Reviewer_QfrS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8651/Reviewer_QfrS"
            ],
            "content": {
                "summary": {
                    "value": "This paper examines the problem of performing mathematical reasoning in small language models.\nThe paper proposes a type of prompt that includes a curriculum (ICCR prompt), combining prompts from tasks related to but different from the target task, ordered by their complexity.\nEvaluation is based on the GSM8K dataset. While some LLMs have achieved accuracies of over 90%, small LMs such as FLAN lag substantially even with finetuning/specialization (depending on size, in the 10-30% range).\nThe prompting method proposed in the current paper boosts the after-specialization performance of Flan-T5-Base (up to 16.2%, compared to the baseline of 14.3%).\nThe paper finds in an ablation study that some aspects of the prompt design are useful.\nFinally, the paper examines the usefulness of different prompts in pure ICL (no specialization); here, interestingly the curriculum design shows no benefit."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. provides an interesting strategy crossing prompting with curriculum learning, leading to gains in the mathematical reasoning ability of a specialized small language model."
                },
                "weaknesses": {
                    "value": "1. the key proposed method, described as the main contribution (at the beginning of Section 3), is only evaluated on one model (Flan-T5-Base) and dataset (GSM8K). While computational budget considerations may make it desirable to focus on the Flan-T5-Base model, evaluation on other target tasks may be a way of strengthening the generalizability of the conclusions.\n\n2. the submission is unclear on a set of implementation points, some of them important, as described under \"Questions\".\n\n3. While ICCR is framed as the key contribution, better after-specialization accuracy is in fact achieved by specializing on AsDiv IC and then using baseline prompts for inference (Table 2). On the one hand, this confirms the point that specializing on OOD prompts can be beneficial. On the other hand, it contradicts the claim that having a curriculum of different tasks, arranged in some order, is beneficial."
                },
                "questions": {
                    "value": "1. Can the authors provide the full exact ChatGPT prompt (Section 3.1) in the appendix?\n\n2. Section 3.2 \"getting the highest priority and then SVAMP\" -- how exactly is the different priority for each dataset specified? Also, how does this interact with the constraint that of \"inclusion of at least one example from the MatHhA dataset\". And how are the detailed choices motivated? Exact specification will be key, as ICCR is the key novel prompting strategy.\n\n3. Section 3.3 \"predominant operator\" -- does this refer to the most frequent one?\n\n4. Section 3.3 how exactly are the \"most pertinent problems\" identified? What embedding similarity metric, and what are the implementation details used for approximate nearest neirhbor search?\n\n5. ICCKA prompts are derived based on both query and answer key. How does this work in inference, where ICCKA prompts can also be used (third row of Table 1)? Presumably the answer key should not be used in creating prompts at inference time?\n\n6. The paper concludes from the inferior performance of ICCR-Random order that the simple-to-complex ordering of the curriculum is important. However, an alternative explanation is that ICCR-Random leads to inferior results because the ordering of the tasks in the prompt varies randomly between different prompts. What would performance be if some random ordering is chosen, and then kept constant across all prompts used in specialization?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8651/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8651/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8651/Reviewer_QfrS"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8651/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699059866597,
            "cdate": 1699059866597,
            "tmdate": 1699637083608,
            "mdate": 1699637083608,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]