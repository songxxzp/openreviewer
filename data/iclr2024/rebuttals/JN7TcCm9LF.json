[
    {
        "title": "Koopman-based generalization bound: New aspect for full-rank weights"
    },
    {
        "review": {
            "id": "bo7ljLaIhm",
            "forum": "JN7TcCm9LF",
            "replyto": "JN7TcCm9LF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission165/Reviewer_ebFP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission165/Reviewer_ebFP"
            ],
            "content": {
                "summary": {
                    "value": "This paper proves generalization bounds for deep neural networks by establishing that they belong to the RHKS of sobolev spaces of a given order: in the most basic result (theorem 1 and proposition 5), a very concrete bound is shown which under the assumption that the transformations associated to the weights in each layer are invertible. The bound scales as the product of the $s$th power of the operator norms of the weights divided by the square root of their determinant. There is also a factor  $\\|K_{\\sigma}\\|_H$ which depends on the activation function. Furthermore, follow-up theorems extend the results to the situation where the weight matrices are not invertible but only injective: in this case, a similar bound holds with the determinant of the weight replaced by the square root of the determinant of $W^{adj} W$, but at the cost of a product of factors $G_j$, which depend on the isotropy of the networks' function. Finally, further generalizations are provided which completely circumvent the need for injectivity by considering an augmented version of the network where each layer outputs a copy of its input (in addition to outputting the usual output). It is also shown that the bounds can be combined with existing bounds by employing one method for some of the layers and another for the remaining layers. Experiments demonstrate that imposing regularization inspired from the bounds provided improves performance."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This is an **extremely interesting direction** which, to the best of my knowledge, is underexplored. The results presented in this paper have the potential to be of great interest to the community for further research, since it approaches the problem from an entirely different perspective: instead of controlling the function class capacity through norms of the weights or number of parameters, properties of the learned functions in terms of their smoothness over the inputs are used instead. This may really be a key to more a more satisfying approach to generalization bounds in the overparametrized setting.\n\n\nGiven the great potential this work has, I am quite disappointed that the treatment offered doesn't more thoroughly study in a reader-friendly way the concrete implications in terms of lack of architectural dependencies for concrete networks. Fortunately, ICLR allows authors to upload a new pdf with very substantial revisions, so I am looking forward to that and may increase my score to 8 if my doubts are all thoroughly resolved."
                },
                "weaknesses": {
                    "value": "The writing is quite crisp and abstract, sometimes to the detriment of precision. I think the results make sense in that bounds for the norms of the neural network are indeed given, but the interpretation in terms of asymptotics and the lack of dependence on the number of parameters don't fully make sense due to the presence of obscure quantities with unclear asymptotic behavior. \n\n To be honest, I am **not completely convinced of the correctness** of the final conclusions from a mathematical standpoint. In particular, the notations used by the authors rely a lot on Sobolev norms which are then absorbed into the O notations as if they were constants, and this process is not performed nearly carefully enough to ensure that no additional dependency on the number of parameters appears.  Here are a few examples where there are imprecisions which must be very thoroughly cleared up during the rebuttal for me to keep my score or increase it: \n\n1 (may be fixable): Proposition 5 includes factors of $\\|K_{\\sigma}\\|$. According to the authors, such factors can be controlled by Proposition 1, which seems to be the justification for absorbing those factors into the $O$ notation in equation (1). I can sort of believe that the final conclusion, but note that the proof given is at best sloppy: each term in the sum over multi-indices with components living in a set of cardinality equal to the layer width is bounded individually by a term $C_{\\beta,\\gamma,\\delta}$, thus, this analysis cannot be used directly to obtain a result truly of the order claimed in equation (1), since the use of this proposition would introduce **an additional dependency on the width of the network**. Now, I agree that this is probably avoidable by using a **component-wise** activation function, which would make most of the terms cancel in the sum,  but this assumption is not even clearly stated, nor is it used anywhere. There are many missing details for the results to truly qualify as applicable to concrete neural networks.\n\n\n2. (more serious) I am not convinced by the applicability of the result in the case of injective but non bijective maps: the bound contains the factors $G_j$, which depend on the \"the isotropy of $f_j$\", with $f_j$ only definable via the composition of the network's functions. It is absolutely unclear to what extent these quantities can be considered as constants. Even in the extremely hypothetical case where $G_j\\leq G$ for some absolute constant $G$, there is still an exponential dependence in the depth of the network which is not explicitly written in the $O$ notation. In addition, it doesn't seem to be the case that the $G_j$ can be controlled properly either. In page 6, below lemma 8, the authors attempt to reassure the readers by giving an example where $G_j$ can be bounded by $(4/\\pi)^{dim(R(W_j)^\\top)/4}$. Note that even in that case, **this term introduces intractable, exponential dependence** in the architectural parameters $L$ (the depth) and $dim(R(W_j)^\\top)$ (which is closely related to the width). \n\n\n3. Similarly, the argument in appendix B is a bit vague and certainly appears to introduce dimensional dependence. \n\n\n\n\n\n\n\n==============Minor comments (maths)=======\n\n\nIn general, it would be nice to make the paper more reader-friendly by adding more lines in the calculations for readers not familiar with the techniques used. For instance, it would be nice to remind the readers in a separate theorem of the Faa di Bruno formula mentioned on page 13 at the beginning of the proof of Proposition 1. \n\n\nFor instance, in the proof of Theorem 2 on page 14, the fact that the $s_i$s are Rademacher variables is not even explicitly stated. \n\nThe implication of Assumption 2 should be explained in terms of concrete assumptions about the network and the inputs to the network. I know that later theorems rely on a concrete formula for $p(\\omega)$ which appears to make this assumption hold trivially, but this absolutely should be explained explicitly. It also seems like not choosing the $p(\\cdot)$ earlier on \n\nThe spaces $R(W_j)$, which refer to the column spaces of $W_j$, should also be defined somewhere. The same applies to the non-standard notation $W^{-\\star}$, which refers to the inverse of the adjoint of $W^{*}$. It also seems a little strange to use conjugate transpose notation (without introduction or justification) when all the weights are presumably real). \n\nIn addition, some of the basic notation for Fourier analysis and the relevant inner products absolutely should be included. Note that various authors use different constants in the definition of the Fourier transform, and it is not clearly stated in this paper which convention is used. I deduced from the first line of the proof of Lemma 3 on page 13 the convention used is the one where there is no constant factor in the definition of the Fourier transform. Similarly, the second equality on the same line takes some time to digest without additional details. \n\nSimilarly, the fact that the Sobolev norms are both equal to sums over all multi-indices of the relevant derivatives, and to the RKHS norm defined at the bottom of page 3 should be explained in much more detail. \n\nAlso, the main paper and the appendix are too heavily reliant on each other, it is better practice to make the appendix fully mathematically self-contained, which would imply at a minimum the following reminders to the readers: the definition of $J\\sigma^{-1}$ on page 13, a reminder of the definitions of $F_{inj}$ on page 14 in the proof of Theorem 6, a better separation go Proposition 11 from proposition 10 on page 16 (since it actually refers to a completely different setting), and the definition of $G_j$ at the beginning of page 15. \n\n\n\n\n\n============minor typos/ language=======\n\n\nFourth line of the introduction: \"a large number of parameters make the complexity\" should be \"a large number of parameters makes the complexity\" \n\nat the beginning of remark 1, it would be better to write \" Let $g$ be a smooth function which doesn't decay at infinity (e.g., a sigmoid), ...\" \n\nIn the beginning of Section 4.3, \"as a result $h\\circ W_j$ does not contained in...\" should be \"as a result $h\\circ W_j$ is not contained in...\"\n\nSection 4.3.2 \"We only need...., not whole $W_j$\" should be \"We only need...., not the whole of  $W_j$\"\n\n\nJust before the second equation in Section 4.4 \" set of all functions which has\" ==> \"set of all functions which have\" \n\nPage 9 \"we constructed a network and learned it\" ===> \"trained\" \n\n\nTop of Page 18 in Appendix B, there shouldn't be a capital letter at  \"then By\""
                },
                "questions": {
                    "value": "What is the norm of the bump functions $\\phi_j$ assumed to satisfy in Proposition 10? Are they constant, or equal to 1? How can this be achieved without additional dependence on the ambient dimension at all? Could you provide a concrete example? \n\nCould you address the points mentioned in the weaknesses, especially the control over the quantity $G_j$ (from lemma 8)and avoiding dimensional dependence when summing over multi-indices in Sobolev norms? \n\nCould you write a complete and detailed version of your result for a fully concrete example where the loss function is either the cross entropy loss or the square loss and the activation is component-wise and fully explicit (e.g. the smooth version of leaky relu), without using any undefined quantity such as $G_j$, $f_j$ or even $\\phi_j$ (you can choose a concrete $\\phi_j$ if necessary, and show that the corresponding factors in the bound are bounded by the absolute constants)?  How can you concretely control the quantity $G_j$ in the case where we have a two-layer neural network with a very wide hidden layer?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission165/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698598421084,
            "cdate": 1698598421084,
            "tmdate": 1699635942039,
            "mdate": 1699635942039,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "OZW3ILTEMe",
                "forum": "JN7TcCm9LF",
                "replyto": "bo7ljLaIhm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission165/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission165/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your very constructive comments.\nAs you pointed out, some of our results are not properly stated, or we need more clear explanations about the factors that appeared in our results.\nAnalyzing them for completely general cases is challenging, but we tried to give examples and make readers easily understand as much as possible.\nAlthough our analysis may have room to improve, we emphasize that our main contribution is introducing an operator theoretic approach to shed light on why networks with high-rank weight matrices generalize well.\nOur results provide a new direction for analyzing generalization of networks.\nBelow, we address each comment.\nWe revised the paper based on your comments.\nThe revised parts are colored in red.\n\n**Norm of the bump function**\n\nAs you pointed out, the $O$ notation in Proposition 10 was improperly used in the original version.\nWe updated Proposition 10 and its proof without using the $O$ notation.\nThe norm of the bump function depends on the support of the function and parameters in the function.\nFor example, we can use the bump function $\\psi$ defined in Appendix D.\nUnfortunately, upper bounding the norm of this bump function analytically is challenging.\nHowever, we have the following observation.\nIf the parameters $a$ and $b$ are large, then the support of $\\psi$ becomes large, which makes the $L^2$-norm of $\\psi$ large, and the Sobolev norm of $\\psi$ also becomes large.\nIf $a-b$ is small, then $\\vert \\psi(x)-\\psi(y)\\vert/\\Vert x-y\\Vert$ for $\\Vert x\\Vert^2=a$ and $y=(b/a)x$ becomes large.\nThus, the $L^2$-norms of the derivatives of $\\psi$ are expected to be large, and the Sobolev norm of $\\psi$ also becomes large if $a-b$ is small.\nPlease note that in Proposition 10, we only need the norm of $\\psi_j$ on $ker(W_j)$.\nThus, it depends only on $r_j=dim(ker(W_j))$, but $r_j$ is not directly related to $d_j$ and if $W_j$ is high-rank, $r_j$ is small.\nMoreover, combining with the factor $G_j$, the norm of $\\psi_j$ can be canceled.\nPlease see Remark 9 and Appendix F for details.\n\nWe can also apply the above bump function to Proposition 9, which involves $\\Vert \\tilde{g}\\Vert=\\Vert \\psi\\Vert\\,\\Vert g\\Vert$.\nIn this case, according to the original version of Subsection 4.3.1, the above $d$ should be $d=\\sum_{j=0}^{L-1}d_j$, and the norm of $\\psi$ depends on $d$.\nHowever, we can alleviate the dependency on the width $d_j$ by modifying the definition of $\\tilde{W}\\_j$ using the projection $P_j$ onto $ker(W_j)$ .\nPlease see the modified version of Subsection 4.3.1 and Appendix I.\nWe define $\\tilde{g}$ in the same manner as that in Subsection 4.3.1.\nThe norm of $\\psi$ depends on $\\sum_{k=0}^{L-1}r_k$, but $r_j=\\mathrm{dim}(ker(W_j))$ is not directly related to $d_j$.\nIf $r_j$ is small, then $\\sum_{k=0}^{L-1}r_k$ is also small.\n\nWe emphasize that our bound focuses on the case where the weight matrices are high-rank.\nOur bound is effective in the case where $dim(ker(W_j))$ is small.\nOur fundamental result is Theorem 2 for invertible weight matrices.\nThe norm of the bump function and the factor $G_j$ measure how much the situation is different from the case of invertible weight matrices.\nWe also emphasize that, as we stated in Subsection 4.4, we can combine our bound with existing bounds.\nWe can use existing bounds for low-rank weight matrices to obtain a tighter bound.\n\n**Factor $G_j$**\n\nWe found that we can replace $G_j$ in Theorem 6 by a smaller value by considering $H_{p_{j-1}}(\\mathcal{R}(W_j))$ instead of $H_{p_{j}}(\\mathcal{R}(W_j))$.\nWe remark that since we assume $W_j$ is injective, $dim(\\mathcal{R}(W_j))=d_{j-1}$ and $dim(\\mathcal{R}(W_j)^{\\perp})=d_j-d_{j-1}$.\nPlease see the revised version of our paper for the details of the modification of the proof.\n\nThe modified version of $G_j$ does not seriously affect the bound.\nIndeed, in the case of $f_j(x)=\\mathrm{e}^{-c\\Vert x\\Vert^2}$, we can evaluate $G_j$.\n$G_j$ becomes small as $c$ becomes large and $s_j$ and $d_j$ becomes large. \nPlease see Remark 5 and Appendix C for details."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission165/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700379812672,
                "cdate": 1700379812672,
                "tmdate": 1700390759228,
                "mdate": 1700390759228,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bRotWnRA7i",
                "forum": "JN7TcCm9LF",
                "replyto": "OZW3ILTEMe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission165/Reviewer_ebFP"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission165/Reviewer_ebFP"
                ],
                "content": {
                    "title": {
                        "value": "Rotational symmetry"
                    },
                    "comment": {
                        "value": "Dear Authors, \n\n\nFirst of all, thank you so much for the detailed revision. I am working my way through the details as best as possible.  I especially appreciate the fact that you have added additional details in some of the mathematical derivations as I have requested. \n\nI do have a couple more questions, the first one of which concerns the $G_j$ and the additional section Appendix H with the two layer example.  Apologies if I misunderstood something, please kindly provide more explanations. \n\nI appreciate the discussion of the calculation of $G_j$ and the admission that \"your, the main goal of this paper is to investigate how the property of the weight matrices affects generalization\" (without regards to existing dependence on architectural parameters). This is an **enormous restriction** that severely restricts the applicability of your results, but if you had been able to completely circumvent this difficulty, the work would have been nothing short of exceptional. \n\n\n\n\nSo my **first (most important) question is**: in remark 5 (appendix C), at the second equality in the calculation of $G_j$, you are using the rotational symmetry of the Gaussian function $f_j$. **But I don't see how this could possibly hold when there are several layers.** Could you explain a bit more whether this step holds in general without having to assume something about $f_j$ (which is a little bit like cheating) and instead relying only on a definition of $g$? \n\n \n\n**Section question**: In Section H where you consider the case of a two layer neural network (example 3), the same thing bothers me a bit: is your network 2-layer or 1-layer (assuming we consider $g$ as the loss function)? There seems to be some confusion as to the role of $\\tilde{f}$ versus $g$.\n\nMy final question in this post relates to example 4: I am very happy that your conclusion is that there is dependence on the input dimension (but not on the hidden dimension). **This is exactly what I would expect at an intuitive level and what makes your approach unique! **\n\nCould you write down the detailed proof using the \"peeling technique\"? The current version still looks more like a sketch. Also, what is $g$ in this case? Will the result hold with the square loss? How about a classification context with the cross entropy or a margin based loss?"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission165/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700549175980,
                "cdate": 1700549175980,
                "tmdate": 1700549175980,
                "mdate": 1700549175980,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jEFLDlFDEx",
                "forum": "JN7TcCm9LF",
                "replyto": "bo7ljLaIhm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission165/Reviewer_ebFP"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission165/Reviewer_ebFP"
                ],
                "content": {
                    "title": {
                        "value": "Sobolev norms"
                    },
                    "comment": {
                        "value": "Thanks for adding the \"note\" under example 1. It really will help improve the readability of the paper. In addition, could you also provide a publicly available reference with a specific page and theorem number?"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission165/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700549303236,
                "cdate": 1700549303236,
                "tmdate": 1700549303236,
                "mdate": 1700549303236,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TSngkyppx8",
                "forum": "JN7TcCm9LF",
                "replyto": "bo7ljLaIhm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission165/Reviewer_ebFP"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission165/Reviewer_ebFP"
                ],
                "content": {
                    "title": {
                        "value": "RELU"
                    },
                    "comment": {
                        "value": "Dear Authors, \n\nI have one more open ended question. \n\nIt seems that a lot of the issues that make the results hard to improve in terms of dimensional dependence comes from the application of the Faa di Bruno formula for the higher derivatives. Thanks for your detailed explanation! It took me a while but thanks to your help I finally understand that even if the activation function is elementwise, we still have  a problem with dimensional dependence. However, I am wondering if this can be fixed by using RELU: in know that we can no longer apply the Faa di Bruno formula at fact value, but it seems like smoothness at downstream layers will counteract the lack of smoothness of RELU near zero: for instance, if f is sufficiently smooth, f(RELU (x)) should become smooth as well. In addition, this would make the cross terms cancel. Can you try this out or explain why it wouldn't work?"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission165/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700549773297,
                "cdate": 1700549773297,
                "tmdate": 1700549773297,
                "mdate": 1700549773297,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "YnNhrJt8wB",
            "forum": "JN7TcCm9LF",
            "replyto": "JN7TcCm9LF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission165/Reviewer_K7E7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission165/Reviewer_K7E7"
            ],
            "content": {
                "summary": {
                    "value": "This paper establishes a new generalization bound for neural networks based on the Koopman operator. To be specific, the authors first represent the network by the product of Koopman operators. Then, new upper bounds of Rademacher complexity are derived for invertible, injective, and non-injective weight matrices, respectively. Furthermore, the Koopman-based bound is combined with other generalization bounds such that both high and low layers can be sharply bounded. Finally, numerical results validate the effectiveness of the regularization induced by the Koopman-based bound, and the different behaviors of singular values of the weight matrix for each layer are also observed."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "The proposed generalization bound is sharp and fills the theoretical gap. Specifically, benefiting from the denominator induced by the Koopman operator, the generalization bound can be sharp when the condition number of the weight matrix is small. What\u2019s more, if the weight matrices are orthogonal, the bound reduces to 1 and is independent of the width of the network. This result explains the generalization ability of neural networks when the weight matrices are full-rank.  By contrast, existing results either depend on the $(p, q)$ matrix norm, which scales by the order of $d^{1/p}$ for a $d \\times d$ matrix, or become loose when faced with high-rank weight matrices.\n- The authors validate the proposed bound with the help of numerical results. On one hand, experimental results on both regression and classification validate the proposed generalization bound. On the other hand, the different behaviors of singular values of the weight matrix for each layer are also observed for AlexNet on the CIFAR dataset.\n- This paper is well organized, which makes it easy to understand."
                },
                "weaknesses": {
                    "value": "- The authors mainly consider the neural networks with dense layers. I wonder whether these theoretical results can generalize well to neural networks with other structures such as convolution. A simple explanation is recommended.\n- The experimental results on MNIST validate the effectiveness of the induced regularization term. Can it boost model performance on datasets with larger scales such as CIFAR?\n- Besides, there are some typos. For example, \n\t- In the introduction part, \"depth of the network.Another approach\" should be \"depth of the network. Another approach\". \n\t- In the introduction part, the third paragraph has an extra indent.\n\t- In Table 1, a larger line spacing is recommended."
                },
                "questions": {
                    "value": "Please refer to Weakness"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "nan"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission165/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698631688780,
            "cdate": 1698631688780,
            "tmdate": 1699635941967,
            "mdate": 1699635941967,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zSpTh8EEvt",
                "forum": "JN7TcCm9LF",
                "replyto": "YnNhrJt8wB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission165/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission165/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your very constructive comments.\nWe addressed your comments as follows.\nWe also revised the paper based on your comments.\nThe revised parts are colored in red.\n\n**Generalization to other structures**\n\nWe can generalize our results to convolutional layers by regarding the convolution as the action of a matrix.\nFor the convolution $\\sum_{i=1}^n\\sum_{j=1}^mf_{k-i,l-j}x_{i,j}$ with a convolutional filter $F=[f_{i,j}]$, we can construct a tensor $W_{i,j,k,l}=f_{k-i,l-j}$.\nIf $i$ or $j$ is out of the bound of the index of the filter, then we set $f_{i,j}=0$.\nWe can combine the indices $(i,j)$ and $(k,l)$ and obtain a matrix that represents the convolution.\nWe added the explanation of this in Appendix J.4.\nPlease see Appendix J.4 for more details.\nGeneralizing our results to other structures, such as pooling layer, is future work.\n\n**Experiment with CIFAR-10**\n\nWe conducted an additional experiment with AlexNet and CIFAR-10 to observe the generalization property with a regularization based on our result.\nPlease see Appendix J.4 for more details.\nWe observed that without the regularization, although the train loss becomes small, the test loss becomes large as the iteration proceeds.\nOn the other hand, with the regularization, the train loss becomes small, and the test loss does not become so large as the case without the regularization.\n\n**Other comments**\n\nThank you for your comments.\nWe revised the paper based on your comments."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission165/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700370938745,
                "cdate": 1700370938745,
                "tmdate": 1700381658791,
                "mdate": 1700381658791,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "lZtZR64jBB",
            "forum": "JN7TcCm9LF",
            "replyto": "JN7TcCm9LF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission165/Reviewer_i9ex"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission165/Reviewer_i9ex"
            ],
            "content": {
                "summary": {
                    "value": "This paper provide an operator-theoretic approach to analyzing networks. They proposed a novel bound for generalization of neural networks using Koopman operators and Rademacher complexity, which reveals a new aspect of neural networks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.\tThis paper proposed a new complexity bound that involves both the norm and determinant of the weight matrices. This bound is particularly useful when the condition numbers of the weight matrices are small. \n2.\tIt provides a new perspective on why networks with high-rank weights generalize well. By combining our bound with existing bounds, we can obtain a more comprehensive description of the role of each layer in the network. \n3.\tThis paper presented an operator-theoretic approach to analyzing networks, using Koopman operators to derive the determinant term in our bound. This approach offers a new way to analyze the generalization performance of neural networks."
                },
                "weaknesses": {
                    "value": "This paper gives the generalization error bound of neural networks from a novel perspective which sounds very interesting and introduces new tools to generalization analysis. But since I'm not familiar with dynamic-based Koopman operators, I have some concerns that I'd like to see answered by the author.\n\n1. As the author said, Efficient learning algorithms have been proposed by describing the learning dynamics of the parameters of neural networks by Koopman operators. It seems that the author represents the composition structure of neural networks using Koopman operators, and then uses the complexity method to give an upper bound. My question is, dynamics sound algorithm-related, while complexity is algorithm-independent, so what's the point of using Koopman operators here.\n\n2. Looking at the entire proof, the conclusion of this paper seems to be highly related to the hypothesis of RKHS. My intuitive feeling is that the conclusion of this paper mainly comes from the RKHS assumption, and whether the neural network is abstracted into Koopman operators has little relevance. I hope the author can explain the relationship between Koopman operators, RKHS and the final conclusion and give an idea of what kind of these techniques/assumptions play a role in the proofs."
                },
                "questions": {
                    "value": "See above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission165/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699190091027,
            "cdate": 1699190091027,
            "tmdate": 1699635941897,
            "mdate": 1699635941897,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "02MzA90L2k",
                "forum": "JN7TcCm9LF",
                "replyto": "lZtZR64jBB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission165/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission165/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your very constructive comments.\nWe addressed your comments as follows.\n\n**Reason for applying Koopman operators to generalization bound**\n\nIn this study, the Koopman operators are not really related to dynamical systems.\nWe use them to describe the composition structure of neural networks rather than connecting them to dynamical systems.\nBy using Koopman operators, we can represent a neural network $f$ with the product of the Koopman operators: $f=K_{W_1}K_{b_1}K_{\\sigma_1}\\cdots K_{W_L}K_{b_L}g$, where $W_j$, $b_j$, and $\\sigma_j$ are the action of the weight matrix, bias, and activation function at the $j$th layer.\nIn addition, $g$ is the final nonlinear transformation.\nWe can bound $\\Vert f\\Vert$ by the product $\\Vert K_{W_1}\\Vert \\Vert K_{b_1}\\Vert \\Vert K_{\\sigma_1}\\Vert \\cdots \\Vert K_{W_L}\\Vert \\Vert K_{b_L}\\Vert\\Vert g\\Vert$.\nBy virtue of the representation with Koopman operators, instead of analyzing the composition structure of the neural network directly, we only need to evaluate the norm of each Koopman operator.\n\nAs for the connection with algorithms, our bound provides a way of regularization (Please see Figure 1 (b) in Section 5).\nHowever, as of now, our analysis is independent of the existing analysis of learning dynamics with Koopman operators.\nCombining our results with the analysis of learning dynamics is an interesting direction for future work.\n\n**Relationship among Koopman operators, RKHS, and final conclusion**\n\nThe derivation of our bound depends on the assumption of RKHS, but it also strongly depends on the application of the Koopman operators.\nBy the assumption of RKHS, evaluating the Rademacher complexity is reduced to evaluating $\\Vert f\\Vert$.\nAs we also stated above, by virtue of the Koopman operators, we can bound $\\Vert f\\Vert$ using the norm of Koopman operators.\nAn important feature of our bound is that it involves the determinant of the weight matrix in the denominator.\nThis determinant factor appears by bounding the norm of the Koopman operator.\nWe didn't have this type of factor in existing results, which means the norm of the Koopman operator gives us a new perspective on the generalization of neural networks.\nBy the effect of the determinant factor, our bound becomes small if the smallest singular value of the weight matrix is large.\nThe direction of this result is completely different from that of existing results that focus on low-rank weight matrices or weight matrices with small singular values. \nOur result reveals why networks with full or high-rank weight matrices generalize well."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission165/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700363054562,
                "cdate": 1700363054562,
                "tmdate": 1700363631419,
                "mdate": 1700363631419,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]