[
    {
        "title": "Setting the Record Straight on Transformer Oversmoothing"
    },
    {
        "review": {
            "id": "QHqOXvcYWp",
            "forum": "OCx7dp58H1",
            "replyto": "OCx7dp58H1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8455/Reviewer_yd6A"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8455/Reviewer_yd6A"
            ],
            "content": {
                "summary": {
                    "value": "The paper analyses the oversmoothing effect in Transformers. They find that Transformers are not inherently low-pass filters but oversmoothing depends on the eigenspectrum of the update equations. To prove this, the authors analyse a simplified Transformer architecture and incorporate the spectrum of attention and weight matrices. The authors relate their findings to existing pretrained architectures. They finally propose a reparametrization of the Transformer weights that ensures that oversmoothinng does not occur. The implications of such a reparametrization can be detrimental, as preliminary experiments show."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper builds on the established and successful framework of Wang et al, that analyses the degree to which a function is a low-pass filter. Doing so:\n1. They give new insights on the eigenspectrum of both attention and weight matrices.\n2. They find that, surprisingly, the transformer updates do not always lead to a low-pass filter.\n3. Motivated by their findings, they propose a new parametrization for the linear layers following the self-attention computation. This leads to a change in the dominating eigenvalues. The authors showcase how these changes can lead to superior performance under cases of severe corruption or for low-data regimes."
                },
                "weaknesses": {
                    "value": "1. Overall I find the main contributions/results hard to digest. Most of the results seem to be extensions from Wang et al.\n2. Oversmoothing is primarily a problem that prohibits/hampers training at initialization (see e.g. Noci et al.). The authors make the same observation, while also noting that trained models exhibit different behavior. In that sense, it would make more sense to show if/how their new parametrization enables training in severe cases of oversmoothing, as e.g. when the networks become much deeper. Although I do find the applications of low-data and corruption interesting, their motivation is less clear. It could be interesting to analyse what is happening during the early phase of training.\n3. Oversmoothing is less of a problem in ViT for image classification, compared to other scenarios. In image classification, labels are sparse -- 1 per sequence of tokens -- and some oversmoothing in deeper layers is expected -- in fact modern ViT just use the mean activations of the last layer to make predictions [1]. In that sense, a task in vision or language that requires a higher rank output, requiring a different prediction per token, could be more desirable. \n4. The authors analyse a simplified Transformer architecture, making some non-trivial assumptions along the way. It is not clear how these findings generalize to a more general scenario. In more detail:\n- They analyze a 1-head attention layer.\n- They assume the same attention and weights are repeated across layers.\n- They ignore the existence of LayerNorm.\n- Attention weights change depending on the data.\n\nMost notably, Pre-LN architectures [2] have been shown to effectively counteract some of the oversmoothing in Transformers. In the experiment sections, ViTs used by the authors seem to include LN as far as I can tell. The authors should make this clear.\n\n[1] Zhai, Xiaohua, et al. \"Scaling vision transformers.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.\n\n[2] Xiong, Ruibin, et al. \"On layer normalization in the transformer architecture.\" International Conference on Machine Learning. PMLR, 2020."
                },
                "questions": {
                    "value": "1. Can you comment on how your findings will change in the presence of different weights per layer and in the presence of pre-ln layers?\n2. If oversmoothing is the problem, what about other ways to mitigate it? I am talking about scaling the residuals (Noci et al, [1, 2]) or initializing the attention layers differently, e.g. [3] or different initializations per layer, e.g. [4]. There is a long list of proposed techniques in the literature from the signal propagation perspective. Since you are proposing a new parametrization, it makes sense to compare what you are achieving compares to what they are trying to achieve. Ensuring that the Transformer is not a low-pass filter, does not necessarily mean that any of the meaningful signal is preserved or that feature learning can take place.\n3. Before section 5, should the superscript $^+$ model be initialized as $\\text{diag}(\\Lambda_H) = + (\\psi^2)$?\n4. Can you comment on the stability of your new parametrization? Especially what (if any) are the differences in the early stage of training. \n\n[1] Noci, Lorenzo, et al. \"The shaped transformer: Attention models in the infinite depth-and-width limit.\" arXiv preprint arXiv:2306.17759 (2023).\n\n[2] He, Bobby, et al. \"Deep transformers without shortcuts: Modifying self-attention for faithful signal propagation.\" arXiv preprint arXiv:2302.10322 (2023).\n\n[3] Trockman, Asher, and J. Zico Kolter. \"Mimetic Initialization of Self-Attention Layers.\" arXiv preprint arXiv:2305.09828 (2023).\n\n[4] Zhang, Hongyi, Yann N. Dauphin, and Tengyu Ma. \"Fixup initialization: Residual learning without normalization.\" arXiv preprint arXiv:1901.09321 (2019)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8455/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697543445005,
            "cdate": 1697543445005,
            "tmdate": 1699637055214,
            "mdate": 1699637055214,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "TlLp5MlOSm",
                "forum": "OCx7dp58H1",
                "replyto": "QHqOXvcYWp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8455/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8455/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer yd6A (Part 1)"
                    },
                    "comment": {
                        "value": "Dear Reviewer yd6A,\n\nThank you so much for your time and many comments. We respond to each question below.\n\n> [..Weaknesses:..]\n\n> [..1. Overall I find the main contributions/results hard to digest. Most of the results seem to be extensions from Wang et al...]\n\nThank you for asking about this. We completely agree that Wang et al., 2022 provides big inspiration for this work. Their analysis is extremely insightful and our goal is to further extend this analysis to uncover the relationship between the eigenspectrum of the weights and the suppression (or not) of high frequencies when the weights H and residual connection are taken into account. This turns out to be very important as it gives us tools to control the eigenspectrum of the weights in order to induce smoothing or sharpening.\n\nBeyond the nice work of Wang et al., 2022, we were also heavily inspired by the work of [Dong et al., Attention is not all you need: Pure attention loses rank doubly exponentially with depth (ICML 2021)] who show that feature representations in Transformers can rank collapse to rank 1, but that there exist infinitely many weights that do not rank collapse with the residual connection. We extend this to show exactly which weights do not rank collapse: only when multiple eigenvalues $(1 + \\lambda^H_j \\lambda^A_i)$ simultaneously have equivalent dominant magnitudes. However, this is a rare case: because $\\mathbf{A}$ and $\\mathbf{H}$ are learned, it is unlikely the magnitude $|1 + \\lambda^H_j \\lambda^A_i|$ of any two eigenvalues will be identical.\n\nWe were also influenced by the extensive analysis of [Park & Kim How do vision transformers work? (ICLR 2022)]: we base our experiment setup off of theirs. Finally, our inspiration to look deeper into how the eigenspectrum can be connected to smoothing, while including the weights and residual connection is also inspired by the related line of work on oversmoothing in graph neural networks e.g., [Di Giovanni et al., Understanding convolution on graphs via energies. (TMLR 2023)].\n\n> [..2. show if/how their new parametrization enables training in severe cases of oversmoothing, as e.g. when the networks become much deeper...]\n\nThank you for raising this point, we agree. To answer this we have added an experiment where we double the depth of the ViT-Ti models on CIFAR100. We have also added a parameterization variant where the first $L/2$ layers sharpen (as in ViT-Ti$^-$) and the remaining $L/2$ layers are left normal (as in ViT-Ti), which we call ViT-Ti*. Finally we have added a comparison FeatScale by Wang et al., 2022. All results are averaged over 3 trials with standard deviations (we bold the best result in each column and any method whose mean plus standard deviation is greater than or equal to mean of the best result):\n\n[new] Increased Depth (Table 3)\n\n| Depth       \t| 12       \t| 24       \t|\n|----------------|--------------|--------------|\n| ViT-Ti     \t| 66.78 \u00b1 0.1  | 67.22 \u00b1 0.1  |\n| ViT-Ti + FeatScale | 66.57 \u00b1 0.5  | 67.71 \u00b1 0.5  |\n| ViT-Ti$^+$ \t| 64.34 \u00b1 0.3  | 65.65 \u00b1 0.3 |\n| ViT-Ti$^-$ \t| 66.62 \u00b1 0.3  | 66.76 \u00b1 0.3  |\n| ViT-Ti$^*$ \t| **67.84 \u00b1 0.6**  | **69.05 \u00b1 0.1**  |\n\nHere ViT-Ti$^+$ and ViT-Ti$^-$ both underperform ViT-Ti, and we suspect this is because ViT-T$^-$ sharpens too much, amplifying noise in the data (whereas ViT-Ti$^+$ oversmooths the data). On the other hand ViT-Ti* significantly outperforms all other models not just for larger depths in Table 3, but also in corruption robustness (Table 4), and data efficiency (Table 2). This suggests that both oversmoothing and oversharpening are detrimental to performance, and that a careful balance is key to an accurate model. We have updated the paper to describe these results and look forward to investigating this further in future work.\n\n> [..3. ...a task in vision or language that requires a higher rank output, requiring a different prediction per token, could be more desirable...]\n\nThis is a very good point. We are interested to try out the approach in a more label-dense vision task such as segmentation, where oversmoothing poses much larger problems than image classification as you suggest.\n\n(response continues below)"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8455/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700353518627,
                "cdate": 1700353518627,
                "tmdate": 1700353518627,
                "mdate": 1700353518627,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ffsmLLlJ1P",
                "forum": "OCx7dp58H1",
                "replyto": "QHqOXvcYWp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8455/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8455/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer yd6A (Part 2)"
                    },
                    "comment": {
                        "value": "> [..4. ...The authors analyse a simplified Transformer architecture...Pre-LN architectures [2] have been shown to effectively counteract some of the oversmoothing...] \n\n> [..Questions:..]\n\n> [..1. ...how your findings will change in the presence of different weights per layer and in the presence of pre-ln layers?...]\n\nWe were also interested in understanding how our theoretical results generalize to a more general scenario with changing parameters and Pre-LN architectures. To test this, in our experiments we allow both attention matrices $\\mathbf{A}$ and weights $\\mathbf{H}$ to change every layer, and we use the same architecture as ViT-Ti (a Pre-LN architecture). We were excited to see that even though these models don\u2019t follow the assumptions of our theoretical results, we can still use them to control the smoothing/sharpening behavior of these models. Thank you for brining this up. We have clarified these details in the paper.\n\n> [..2. If oversmoothing is the problem, what about other ways to mitigate it? I am talking about scaling the residuals (Noci et al, [1, 2]) or initializing the attention layers differently, e.g. [3] or different initializations per layer, e.g. [4]...]\n\nThanks for bringing this up. One of the strengths of our approach is that it is complementary to these approaches (scaling residuals [1 ,2], different initializations [3, 4]) and so we can apply them at the same time as our approach. Because of this we opted to compare against an approach that isn\u2019t complementary, specifically FeatScale by Wang et al., 2022. Alongside the above increased depth experiment we also compare its data efficiency and corruption robustness below:\n\nData Efficiency (Table 2)\n\n| Data Kept          \t| 100%     \t| 50%      \t| 10%      \t|\n|------------------|--------------|--------------|--------------|\n| ViT-Ti       \t| 66.78 \u00b1 0.1  | 56.38 \u00b1 0.5  | 32.47 \u00b1 0.9  |\n| ViT-Ti + FeatScale | 66.57 \u00b1 0.5  | 57.08 \u00b1 0.3  | 32.50 \u00b1 0.4  |\n| ViT-Ti$^+$   \t| 64.34 \u00b1 0.3  | 55.21 \u00b1 0.5  | 28.67 \u00b1 0.5  |\n| ViT-Ti$^-$   \t| 66.62 \u00b1 0.3  | 56.58 \u00b1 0.3  | **33.99 \u00b1 0.4**  |\n| ViT-Ti$^*$   \t| **67.84 \u00b1 0.6**  | **57.78 \u00b1 0.4**  | **33.64 \u00b1 0.5**  |\n\n\nCorruption Robustness (Table 4)\n\n| Corruption Intensity | 0          \t| 1          \t| 2          \t| 3          \t| 4          \t| 5          \t|\n|----------------------|----------------|----------------|----------------|----------------|----------------|----------------|\n| ViT-Ti           \t| 66.78 \u00b1 0.1\t| 61.59 \u00b1 0.2\t| 57.75 \u00b1 0.2\t| 53.12 \u00b1 0.1\t| 49.32 \u00b1 0.1\t| 39.17 \u00b1 0.2\t|\n| ViT-Ti + Featscale   | 66.57 \u00b1 0.5\t| 61.25 \u00b1 0.5\t| 57.36 \u00b1 0.6\t| 52.64 \u00b1 0.6\t| 48.87 \u00b1 0.5\t| 38.77 \u00b1 0.4\t|\n| ViT-Ti$^+$       \t| 64.34 \u00b1 0.3\t| 58.36 \u00b1 0.4\t| 53.83 \u00b1 0.5\t| 49.03 \u00b1 0.6\t| 45.15 \u00b1 0.6\t| 35.86 \u00b1 0.6\t|\n| ViT-Ti$^-$       \t| 66.62 \u00b1 0.3\t| 61.62 \u00b1 0.2\t| 58.00 \u00b1 0.3\t| 53.56 \u00b1 0.2\t| 49.70 \u00b1 0.3\t| 40.07 \u00b1 0.2\t|\n| ViT-Ti$^*$       \t| **67.84 \u00b1 0.6**\t| **62.49 \u00b1 0.5**\t| **58.82 \u00b1 0.7**\t| **54.32 \u00b1 0.7**\t| **50.40 \u00b1 0.6**\t| **40.38 \u00b1 0.7**\t|\n\t\n> [..3. Before section 5, should the superscript $\\mbox{}^+$ model be initialized as $\\mbox{diag}(\\Lambda_H) = +(\\psi^2)$?..]\n\nThis is correct! Thank you for catching this typo!\n\n> [..4 . Can you comment on the stability of your new parametrization? Especially what (if any) are the differences in the early stage of training..]\n\nThis is also interesting! To better understand what is happening early in training we have added a plot of HFC/LFC for the final layer against training iterations in Figure 4 in Section 5. Interestingly, we see that the last layers of both ViT-Ti$^*$ and ViT-Ti have the same filtering properties from the beginning to the end of training, despite the fact that the first $L/2$ layers of ViT-Ti$^*$ are sharpening. Further at the beginning of training, the last layer of ViT-Ti$^-$ becomes less sharp, but by the end of training it becomes sharper than at initialization. Thank you for asking about this and for these citations, we will add them to the paper.\n\nThank you again for your time. Let us know if you have any other questions and we will respond as soon as possible."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8455/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700353932512,
                "cdate": 1700353932512,
                "tmdate": 1700353932512,
                "mdate": 1700353932512,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "sy2p4mNP6A",
                "forum": "OCx7dp58H1",
                "replyto": "ffsmLLlJ1P",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8455/Reviewer_yd6A"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8455/Reviewer_yd6A"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you very much for the clarifications and the additional results. I am replying using the same order as you did.\n\n> Contributions of the paper\n\nI appreciate your honesty and the more precise list of inspiration for your work. I summarize your contribution as \"We extend this to show exactly which weights do not rank collapse\". This is indeed interesting, although Theorem 1 as I said is not easily parsable. I am not arguing that every theorem should be, but you can perhaps think of a different way to formulate it.\n\n> Other ways to mitigate oversmoothing.\n\nAlthough I do find some merit in the \"ViT-Ti*\" model, it seems more like you are overfitting this single classification dataset. I do not believe that you should necessarily get the best results, but instead focus on cases where training is significantly obscured, as when going even deeper. If not such cases exist, this damages the motivation of your study.\n\n> label, dense task\n\n> generalization to the more comple Transformer architecture\n\nStill seems like an important question to me\n\n> Other baselines\n\nI do not fully agree that all these methods are complementary. Sure, you can apply them in parallel. But if you just want to avoid oversmoothing, one of them is enough.\n\n> Stability of new parametrization\n\nAppreciate the new results. It seems that there are many unanswered questions.\n\nAll in all, I appreciate your commitment. I believe there are many unanswered questions. The main takeaway message is nonetheless clear message: not all weights cause rank collapse."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8455/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700513159833,
                "cdate": 1700513159833,
                "tmdate": 1700513159833,
                "mdate": 1700513159833,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hNTfbAWAxR",
                "forum": "OCx7dp58H1",
                "replyto": "mlh4TiOspu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8455/Reviewer_yd6A"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8455/Reviewer_yd6A"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the additional details. I think my concerned are mostly covered with either additional explanations or promises for future experiments.\n\n> [..focus on cases where training is significantly obscured...]\n\nAs you point out, for up to 24 layers, ViT-Ti$^{-}$ and ViT-Ti have negligible difference in performance in Table 3, so I don't see how this proves your point. Oversmoothing might be bad at the beginning of training, as proven by Dong et al, Noci et al, but further during training is hard to say anything. I would thus focus more on initialization.\n\n> [..generalization to the more complete Transformer architecture..]\n\nMy point was about the theory and not the empirical results. \n\n> [...methods are complementary...]\n\nAlthough pre-LN helps with oversmoothing, I never suggested this as a baseline. Especially I proposed \"I am talking about scaling the residuals (Noci et al, [1, 2]) or initializing the attention layers differently, e.g. [3] or different initializations per layer, e.g. [4]\"."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8455/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700601352300,
                "cdate": 1700601352300,
                "tmdate": 1700601352300,
                "mdate": 1700601352300,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HDyz3IHJUE",
                "forum": "OCx7dp58H1",
                "replyto": "QHqOXvcYWp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8455/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8455/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Comment to Reviewer yd6A"
                    },
                    "comment": {
                        "value": "Dear Reviewer yd6A,\n\nThank you again for your time on the paper. Do you have any extra questions about the paper or our response?\n\nLet us know and we will try to response as soon as possible before the discussion period ends on November 22.\n\nThank you,\n\nThe authors"
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8455/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700648943943,
                "cdate": 1700648943943,
                "tmdate": 1700648943943,
                "mdate": 1700648943943,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Poco2LXNuQ",
            "forum": "OCx7dp58H1",
            "replyto": "OCx7dp58H1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8455/Reviewer_rqLo"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8455/Reviewer_rqLo"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the oversmoothing behavior in deep transformer networks. The authors' main focus is in showing under what conditions popular transformer architectures are amenable to oversmoothing and how to mitigate it. The authors mainly analyze residual plus self attention layer using its eigenspectrum. They first show that one eigenvalue will dominate the rest based on phases of H and eigenvalues of A. Depending on the dominating eigenvalue, the representation of the network will converge either to a single vector or a rank one matrix. Theorem-3 further shows that oversmoothing is not inevitable and residual connections and H can counteract it. Finally, the authors show a sufficient condition for counteracting oversmoothing via constraining the eigenspectrum of H to [-1, 0). Experimental results show that proposed solution is effective in counteracting oversmoothing and existing networks do not necessarily oversmooth."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is easy to follow with necessary details given to understand the theory. It extends the available theory and proposes a new perspective on reparameterization. Experimental results support the proposed theory and preventive measures on oversmoothing."
                },
                "weaknesses": {
                    "value": "While the theory extends previous work in some aspects, it is not clear if this difference is significant. Other preventive measures such as AttnScale/FeatScale from Wang et al. needs comparison and more discussion. Several details are also missing from the paper.\n\n1. Wang et al also examines residual blocks and FFN in addition to the self-attention layer. While they mention the inevitability of high frequency components to be suppressed, their analysis suggest that using residual connections might prevent it from collapsing to zero. In fact, Eq (6) in Theorem-3 suggests that it is possible to prevent decaying of high-frequency components by constraining the $||W_V||_2$ properly so that it is a non-contractive mapping. I think a more detailed comparison to Wang et al is needed to highlight how your analysis differs from low-pass filtering aspect and how your preventive measures are different from constraining $||W_V||_2$ or AttnScale/FeatScale that Wang et al applies.\n\n2. While Figure-2 shows that asymmetry degrades with more epochs, is 0.7 small enough to suggest symmetry, given that it starts at 0.95? What about outliers in the off-diagonal entries? Additionally, can you connect it to the condition number?\n\n3. Can you clarify more on how do you update $\\Theta$ in $QR(\\Theta)=[V_H, R]$? Do you do QR decomposition after every gradient step or do you use $\\Theta R^{-1}=V_H$ where you backpropagate gradients directly to $\\Theta$?\n\n4. How does the HFC/LFC evolve in training steps? \n\n5. At the end of Section 4, page 6, why do you have $V_H^{-1}$ after clip?\n\n6. The same page, last paragraph, I think it should be $diag(\\Lambda_H)=\\Psi^2$ -- no negative sign.\n\n7. Please define Q in the main statement of Theorem-2.\n\n8. Page 6, \"||\" is missing in definition of asymmetry.\n\n9. Please describe the metrics for Table-2 and Table-3."
                },
                "questions": {
                    "value": "Please see above for more details.\n\n1. Can you provide a more detailed comparison with Wang et al? Including more baselines.\n\n2. How should I interpret the asymmetry metric? How does it relate to condition number?\n\n3. Can you give more details on updating QR decomposition?\n\n4. How does HFC/LFC evolve during training?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8455/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698790851919,
            "cdate": 1698790851919,
            "tmdate": 1699637055084,
            "mdate": 1699637055084,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "dsD05ALoVq",
                "forum": "OCx7dp58H1",
                "replyto": "Poco2LXNuQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8455/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8455/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer rqLo (Part 1)"
                    },
                    "comment": {
                        "value": "Dear Reviewer rqLo,\n\nThank you so much for your time and detailed response. We respond to each question below.\n\n> [..Weaknesses:..]\n\n> [..1. ...a more detailed comparison to Wang et al is needed..]\n\nThanks for this! We agree that it would be enlightening to compare with Wang et al., 2022 and so we have implemented their FeatScale method. Here are the new accuracy results. We have added another a new experiment where we double the model depth (Table 3) and a parameterization variant where the first $L/2$ layers sharpen (as in ViT-Ti$^-$) and the remaining $L/2$ layers are left normal (as in ViT-Ti), which we call ViT-Ti*. All results are averaged over 3 trials with standard deviations (we bold the best result in each column and any method whose mean plus standard deviation is greater than or equal to mean of the best result):\n\nData Efficiency (Table 2)\n\n| Data Kept          \t| 100%     \t| 50%      \t| 10%      \t|\n|------------------|--------------|--------------|--------------|\n| ViT-Ti       \t| 66.78 \u00b1 0.1  | 56.38 \u00b1 0.5  | 32.47 \u00b1 0.9  |\n| ViT-Ti + FeatScale | 66.57 \u00b1 0.5  | 57.08 \u00b1 0.3  | 32.50 \u00b1 0.4  |\n| ViT-Ti$^+$   \t| 64.34 \u00b1 0.3  | 55.21 \u00b1 0.5  | 28.67 \u00b1 0.5  |\n| ViT-Ti$^-$   \t| 66.62 \u00b1 0.3  | 56.58 \u00b1 0.3  | **33.99 \u00b1 0.4**  |\n| ViT-Ti$^*$   \t| **67.84 \u00b1 0.6**  | **57.78 \u00b1 0.4**  | **33.64 \u00b1 0.5**  |\n\n\n[new] Increased Depth (Table 3)\n\n| Depth       \t| 12       \t| 24       \t|\n|----------------|--------------|--------------|\n| ViT-Ti     \t| 66.78 \u00b1 0.1  | 67.22 \u00b1 0.1  |\n| ViT-Ti + FeatScale | 66.57 \u00b1 0.5  | 67.71 \u00b1 0.5  |\n| ViT-Ti$^+$ \t| 64.34 \u00b1 0.3  | 65.65 \u00b1 0.3 |\n| ViT-Ti$^-$ \t| 66.62 \u00b1 0.3  | 66.76 \u00b1 0.3  |\n| ViT-Ti$^*$ \t| **67.84 \u00b1 0.6**  | **69.05 \u00b1 0.1**  |\n\n\nCorruption Robustness (Table 4)\n\n| Corruption Intensity | 0          \t| 1          \t| 2          \t| 3          \t| 4          \t| 5          \t|\n|----------------------|----------------|----------------|----------------|----------------|----------------|----------------|\n| ViT-Ti           \t| 66.78 \u00b1 0.1\t| 61.59 \u00b1 0.2\t| 57.75 \u00b1 0.2\t| 53.12 \u00b1 0.1\t| 49.32 \u00b1 0.1\t| 39.17 \u00b1 0.2\t|\n| ViT-Ti + Featscale   | 66.57 \u00b1 0.5\t| 61.25 \u00b1 0.5\t| 57.36 \u00b1 0.6\t| 52.64 \u00b1 0.6\t| 48.87 \u00b1 0.5\t| 38.77 \u00b1 0.4\t|\n| ViT-Ti$^+$       \t| 64.34 \u00b1 0.3\t| 58.36 \u00b1 0.4\t| 53.83 \u00b1 0.5\t| 49.03 \u00b1 0.6\t| 45.15 \u00b1 0.6\t| 35.86 \u00b1 0.6\t|\n| ViT-Ti$^-$       \t| 66.62 \u00b1 0.3\t| 61.62 \u00b1 0.2\t| 58.00 \u00b1 0.3\t| 53.56 \u00b1 0.2\t| 49.70 \u00b1 0.3\t| 40.07 \u00b1 0.2\t|\n| ViT-Ti$^*$       \t| **67.84 \u00b1 0.6**\t| **62.49 \u00b1 0.5**\t| **58.82 \u00b1 0.7**\t| **54.32 \u00b1 0.7**\t| **50.40 \u00b1 0.6**\t| **40.38 \u00b1 0.7**\t|\n\nWe have added these results to the paper. We suspect the reason that FeatScale underperforms is because it is still motivated assuming high frequencies are always suppressed, specifically they (Wang et al., 2022) say \u201cAccording to our analysis in Section 2.2, MSA module will indiscriminately suppress high-frequency signals, which leads to severe information loss. Even though residual connection can retrieve lost information through the skip path, the high-frequency portion will be inevitably diluted (Theorem1). To this end, we propose another scaling technique that operates on feature maps, named Feature Scaling (FeatScale).\u201d\nWe agree with you that their analysis suggests that residual connections might prevent high frequencies from collapsing to zero, but Wang et al., 2022 disagree: \u201cAlthough multi-head, FFN, and skip connection all help preserve the high-frequency signals, none would change the fact that MSA block as a whole only possesses the representational power of low-pass filters\u201d. \nWe believe the reason they argue this is because when they analyze the update equations that include residual connections (Proposition 5) they do not account for the eigenspectrum of those equations. Our goal with this work is to set the record straight with a new analysis that uncovers the relationship between the eigenspectrum of the weights and the suppression (or not) of high frequencies in the update equations. \n\n> [..2. is 0.7 small enough to suggest symmetry, given that it starts at 0.95?..]\n\nThank you for bringing this up. It\u2019s true that Figure 2 does not suggest perfect symmetry. Our original motivation for using a symmetric parameterization was inspired by [Hu et al., 2019. Exploring weight symmetry in deep neural networks.] who show that weight symmetry in NNs does not affect their universal approximation capabilities. However, we agree that it would be interesting to test a non-symmetric parameterization. Based on your suggestion we have implemented a new parameterization for $\\mathbf{H}$ as $\\mathbf{H} = \\mathbf{V}_H \\Lambda \\mathbf{V}_H^{-1}$. Specifically, we take gradients to $\\mathbf{V}_H$ in the backwards pass, and compute $\\mathbf{H}$ using the above equation in the forwards pass. We use the suffix \u201c-NS\u201d for the non-symmetric parameterizations. Here are the new accuracy results: \n\n(response continues below)"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8455/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700349758475,
                "cdate": 1700349758475,
                "tmdate": 1700353971063,
                "mdate": 1700353971063,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "AYwMsSchce",
                "forum": "OCx7dp58H1",
                "replyto": "Poco2LXNuQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8455/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8455/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer rqLo (Part 2)"
                    },
                    "comment": {
                        "value": "Data Efficiency (Table 2)\n\n| Data Kept          \t| 100%     \t| 50%      \t| 10%      \t|\n|------------------|--------------|--------------|--------------|\n| ViT-Ti       \t| 66.78 \u00b1 0.1  | 56.38 \u00b1 0.5  | 32.47 \u00b1 0.9  |\n| ViT-Ti + FeatScale | 66.57 \u00b1 0.5  | 57.08 \u00b1 0.3  | 32.50 \u00b1 0.4  |\n| ViT-Ti$^+$   \t| 64.34 \u00b1 0.3  | 55.21 \u00b1 0.5  | 28.67 \u00b1 0.5  |\n| ViT-Ti$^-$   \t| 66.62 \u00b1 0.3  | 56.58 \u00b1 0.3  | **33.99 \u00b1 0.4**  |\n| ViT-Ti$^*$   \t| 67.84 \u00b1 0.6  | **57.78 \u00b1 0.4**  | **33.64 \u00b1 0.5**  |\n| ViT-Ti-NS$^-$\t| 67.80 \u00b1 0.2  | 57.54 \u00b1 0.1  | **33.78 \u00b1 0.3**  |\n| ViT-Ti-NS$^*$\t| **68.53 \u00b1 0.6**  | **57.67 \u00b1 0.3**  | 33.75 \u00b1 0.2  |\n\n\n[new] Increased Depth (Table 3)\n\n| Depth       \t| 12       \t| 24       \t|\n|----------------|--------------|--------------|\n| ViT-Ti     \t| 66.78 \u00b1 0.1  | 67.22 \u00b1 0.1  |\n| ViT-Ti + FeatScale | 66.57 \u00b1 0.5  | 67.71 \u00b1 0.5  |\n| ViT-Ti$^+$ \t| 64.34 \u00b1 0.3  | 65.65 \u00b1 0.3 |\n| ViT-Ti$^-$ \t| 66.62 \u00b1 0.3  | 66.76 \u00b1 0.3  |\n| ViT-Ti$^*$ \t| 67.84 \u00b1 0.6  | 69.05 \u00b1 0.1  |\n| ViT-Ti-NS$^-$  | 67.80 \u00b1 0.2  | 67.09 \u00b1 0.1  |\n| ViT-Ti-NS$^*$  | **68.53 \u00b1 0.6**  | **69.64 \u00b1 0.3**  |\n\n\nCorruption Robustness (Table 4)\n\n| Corruption Intensity | 0          \t| 1          \t| 2          \t| 3          \t| 4          \t| 5          \t|\n|----------------------|----------------|----------------|----------------|----------------|----------------|----------------|\n| ViT-Ti           \t| 66.78 \u00b1 0.1\t| 61.59 \u00b1 0.2\t| 57.75 \u00b1 0.2\t| 53.12 \u00b1 0.1\t| 49.32 \u00b1 0.1\t| 39.17 \u00b1 0.2\t|\n| ViT-Ti + Featscale   | 66.57 \u00b1 0.5\t| 61.25 \u00b1 0.5\t| 57.36 \u00b1 0.6\t| 52.64 \u00b1 0.6\t| 48.87 \u00b1 0.5\t| 38.77 \u00b1 0.4\t|\n| ViT-Ti$^+$       \t| 64.34 \u00b1 0.3\t| 58.36 \u00b1 0.4\t| 53.83 \u00b1 0.5\t| 49.03 \u00b1 0.6\t| 45.15 \u00b1 0.6\t| 35.86 \u00b1 0.6\t|\n| ViT-Ti$^-$       \t| 66.62 \u00b1 0.3\t| 61.62 \u00b1 0.2\t| 58.00 \u00b1 0.3\t| 53.56 \u00b1 0.2\t| 49.70 \u00b1 0.3\t| 40.07 \u00b1 0.2\t|\n| ViT-Ti$^*$       \t| 67.84 \u00b1 0.6\t| 62.49 \u00b1 0.5\t| 58.82 \u00b1 0.7\t| 54.32 \u00b1 0.7\t| 50.40 \u00b1 0.6\t| 40.38 \u00b1 0.7\t|\n| ViT-Ti-NS$^-$    \t| 67.80 \u00b1 0.2\t| **63.03 \u00b1 0.2**\t| **59.65 \u00b1 0.1**\t| **55.05 \u00b1 0.3**\t| **51.30 \u00b1 0.1**\t| **41.33 \u00b1 0.1**\t|\n| ViT-Ti-NS$^*$    \t| **68.53 \u00b1 0.6**\t| 63.01 \u00b1 0.1\t| **59.44 \u00b1 0.3**\t| **54.90 \u00b1 0.4**\t| **51.12 \u00b1 0.4**\t| 40.90 \u00b1 0.4\t|\n\nWe have added these results to the paper. The non-symmetric parameterization does improve upon the symmetric parameterization in some cases (particularly for increased depth and corrupted data). We appreciate your suggestion to investigate this.\n\n> [..Additionally, can you connect it to the condition number?...How should I interpret the asymmetry metric? How does it relate to condition number?..]\n\nThis is an interesting question. To investigate this we computed the average condition number for all H matrices used in ViT-Ti for every epoch in training on CIFAR100. We have added the results to Appendix C. The condition number fluctuates during training but gradually increases throughout training. We do not know of a connection between the condition number and the asymmetry of H, but agree it would be an interesting direction for future work. \n\n> [..3. how do you update $\\Theta$ in $QR(\\Theta) = [V_H, R]$?...Can you give more details on updating QR decomposition?..]\n\nWe backpropagate through the QR decomposition, which requires a QR decomposition for each gradient step. This is because we require $V_H$ to be orthogonal. If we instead parameterized $V_H = \\Theta R^{-1}$ and used gradient descent to update $\\Theta$ then this could break the orthogonality of $V_H$. Thank you for this, we will add this detail to make this clearer in the paper.\n\n> [..4. How does the HFC/LFC evolve in training steps?..]\n\nGood question. To answer this we plot the HFC/LFC of the final layer for every epoch during training for the best performing symmetric models (ViT-Ti$^-$ and ViT-Ti$^*$), as well as ViT-Ti, and present the results in Figure 4 in Section 5. Interestingly, we see that the last layers of both ViT-Ti$^*$ and ViT-Ti have the same filtering properties throughout training, despite the fact that the first $L/2$ layers of ViT-Ti$^*$ are sharpening. Further at the beginning of training, the last layer of ViT-Ti$^-$ becomes less sharp, but by the end of training it becomes sharper than at initialization. Thank you for this suggestion!\n\n> [..5. why do you have $V_H^{-1}$ after clip?..]\n\n> [..6. I think it should be $\\mbox{diag}(\\Lambda_H) = \\Phi^2$..]\n\n> [..8. \"||\" is missing in definition of asymmetry..]\n\nThank you for catching these! There should be no $V_H^{-1}$, no negative sign, and additional ||.\n\n> [..7. Please define $Q$ in the main statement of Theorem-2..]\n\nGood call, we have moved the definition from the proof to the theorem statement.\n\n> [..9. Please describe the metrics for Table-2 and Table-3..]\n\nSorry, these are test accuracies, thank you for asking!\n\nThank you again for your time. Let us know if you have any other questions and we will respond as soon as possible."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8455/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700350742519,
                "cdate": 1700350742519,
                "tmdate": 1700354049245,
                "mdate": 1700354049245,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ls1dytVQC3",
                "forum": "OCx7dp58H1",
                "replyto": "Poco2LXNuQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8455/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8455/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Comment to Reviewer rqLo"
                    },
                    "comment": {
                        "value": "Dear Reviewer rqLo,\n\nThank you again for your time on the paper. Do you have any extra questions about the paper or our response? \n\nLet us know and we will try to response as soon as possible before the discussion period ends on November 22.\n\nThank you,\n\nThe authors"
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8455/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700648845145,
                "cdate": 1700648845145,
                "tmdate": 1700648845145,
                "mdate": 1700648845145,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "D63623EbV6",
            "forum": "OCx7dp58H1",
            "replyto": "OCx7dp58H1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8455/Reviewer_m7iA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8455/Reviewer_m7iA"
            ],
            "content": {
                "summary": {
                    "value": "This paper delves into the issue of oversmoothing in Transformers, demonstrating that it is not a pervasive problem, as Transformers do not always act as low-pass filters. The authors provide a theoretical analysis to delineate the conditions under which oversmoothing occurs and when it does not. Drawing insights from these findings, they introduce a novel reparameterization technique designed to mitigate oversmoothing. In addition, the authors conducted experiments with the Vision Transformer (VIT) and showcased the effectiveness of their proposed approach in addressing the oversmoothing issue, enabling VIT models to achieve greater depth and better robustness."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The authors undertake a comprehensive investigation, encompassing both theoretical and empirical analyses, to gain insights into the oversmoothing phenomenon within Transformers. They give clear explanations regarding the underlying reasons for oversmoothing.\n\n2.  A novel reparameterization technique is introduced as a solution to mitigate the oversmoothing problem.\n\n3. Empirical evaluations validate the efficacy of the proposed approach in addressing oversmoothing, showcasing its ability to deepen Transformers and enhance robustness in Transformer models."
                },
                "weaknesses": {
                    "value": "The empirical evaluations in this study are exclusively conducted on computer vision tasks. However, there is an expectation for a broader and more diverse range of tasks, including but not limited to natural language processing (NLP) and multimodal tasks, to provide a more comprehensive evaluation of the proposed approach."
                },
                "questions": {
                    "value": "Can the proposed approach effectively mitigate oversmoothing even when employing an exceptionally high number of layers? \nAdditionally, what is the performance impact of this approach on NLP tasks?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8455/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698798358572,
            "cdate": 1698798358572,
            "tmdate": 1699637054945,
            "mdate": 1699637054945,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fB5fj2oWrA",
                "forum": "OCx7dp58H1",
                "replyto": "D63623EbV6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8455/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8455/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer m7iA"
                    },
                    "comment": {
                        "value": "Dear Reviewer m7iA,\n\nThank you so much for your time and feedback. We respond to each question below.\n\n> [..Weaknesses:..]\n\n> [..an expectation for a broader and more diverse range of tasks...what is the performance impact of this approach on NLP tasks?..]\n\nWe definitely aim to investigate this in future work. In large part, most research we are aware of on Transformer oversmoothing divides into either vision-focused or NLP-focused work, both of which are usually entirely empirical (with the rare exceptions of Dong et al., 2021 and Wang et al., 2022). Because our theory extends the theoretical work of Dong et al., 2021 and Wang et al., 2022 who restrict their focus to vision Transformers, we originally opted to focus solely on vision as well. \n\nThis said, we agree that it would strengthen the paper to extend the evaluation to another domain. We are currently running an NLP experiment that should be ready on Monday. We will get back to you with these results as soon as possible!\n\n> [..Questions..]\n\n> [..Can the proposed approach effectively mitigate oversmoothing even when employing an exceptionally high number of layers?..]\n\nThis is a good question! To answer this we have added an experiment where we increase the depth of the ViT-Ti models on CIFAR100. We have also added a parameterization variant where the first L/2 layers sharpen (as in ViT-Ti$^-$) and the remaining $L/2$ layers are left normal (as in ViT-Ti), which we call ViT-Ti*. Finally we have added a comparison FeatScale by Wang et al., 2022. All results are averaged over 3 trials with standard deviations (we bold the best result in each column and any method whose mean plus standard deviation is greater than or equal to mean of the best result):\n\n[new] Increased Depth (Table 3)\n\n| Depth       \t| 12       \t| 24       \t|\n|----------------|--------------|--------------|\n| ViT-Ti     \t| 66.78 \u00b1 0.1  | 67.22 \u00b1 0.1  |\n| ViT-Ti + FeatScale | 66.57 \u00b1 0.5  | 67.71 \u00b1 0.5  |\n| ViT-Ti$^+$ \t| 64.34 \u00b1 0.3  | 65.65 \u00b1 0.3 |\n| ViT-Ti$^-$ \t| 66.62 \u00b1 0.3  | 66.76 \u00b1 0.3  |\n| ViT-Ti$^*$ \t| **67.84 \u00b1 0.6**  | **69.05 \u00b1 0.1**  |\n\nHere ViT-Ti$^+$ and ViT-Ti$^-$ both underperform ViT-Ti, and we suspect this is because ViT-Ti$^-$ sharpens too much, amplifying noise in the data (whereas ViT-Ti$^+$ oversmooths the data). On the other hand ViT-Ti* significantly outperforms all other models not just for larger depths in Table 3, but also in corruption robustness (Table 4), and data efficiency (Table 2). This suggests that both oversmoothing and oversharpening are detrimental to performance, and that a careful balance is key to an accurate model. We have updated the paper to describe these results and look forward to investigating this further in future work.\n\nThank you again for your time. Let us know if you have any other questions and we will respond as soon as possible."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8455/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700348666427,
                "cdate": 1700348666427,
                "tmdate": 1700354084811,
                "mdate": 1700354084811,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nZDf63FKeK",
                "forum": "OCx7dp58H1",
                "replyto": "D63623EbV6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8455/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8455/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> [..an expectation for a broader and more diverse range of tasks...what is the performance impact of this approach on NLP tasks?..]\n\nWe are still running NLP experiments and hope to have it by tomorrow"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8455/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700601922947,
                "cdate": 1700601922947,
                "tmdate": 1700601922947,
                "mdate": 1700601922947,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BtLqrnIkZz",
                "forum": "OCx7dp58H1",
                "replyto": "D63623EbV6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8455/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8455/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Comment to Reviewer m7iA"
                    },
                    "comment": {
                        "value": "Dear Reviewer m7iA,\n\nThank you again for your time on the paper. Do you have any extra questions about the paper or our response? \n\nLet us know and we will try to response as soon as possible before the discussion period ends on November 22.\n\nThank you,\n\nThe authors"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8455/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700648801166,
                "cdate": 1700648801166,
                "tmdate": 1700648801166,
                "mdate": 1700648801166,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ZwAdSuRJVq",
            "forum": "OCx7dp58H1",
            "replyto": "OCx7dp58H1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8455/Reviewer_QJ8g"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8455/Reviewer_QJ8g"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates how Transformers oversmooth through the lens of low/high-pass filters and introduces a reparameterization of the product of the output and value projection matrices to avoid oversmoothing. The paper presents conditions in which Transformers are not low-pass filters using analysis of the domination of the eigenvalues of the attention matrix and how at the infinite-layer limit, the feature vectors converge to different solutions based on the domination of these eigenvalues."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper provides a good analysis of when and how to avoid over-smoothing in Transformers is an important problem since Transformers in practice have many layers.\n\n2. The paper is well-motivated."
                },
                "weaknesses": {
                    "value": "1. The proposed method for avoiding oversmoothing is based on the claim that H becomes more symmetric as the training progresses. However, it is not confirmed that H eventually becomes a symmetric matrix. Therefore, parameterizing H as a symmetric matrix might restrict the expressive power of the Transformer model as a whole.\n\n2. There is no definite answer to why the ViT-Ti^- version improves robustness and data efficiency. The intuition provided by the authors is not enough to answer this because robustness and data efficiency are more nuanced than simply choosing higher-frequency features.\n\n3. The writing of the paper is not polished, which creates confusion in key details of the paper. For example, given the redundancy in the cases in Theorem 1, maybe Theorem 1 could be rewritten to make the main result easier to read.\n\n4. In Table 1, the authors show the distribution of the dominating eigenvalues but do not show whether the features oversmooth when \\lambda_1^A dominates. This must be verified because the Theorems presented only work when the attention matrix A is fixed across layers, which does not hold for all practical settings. Therefore, showing the distribution of the dominating eigenvalues without showing how they affect the final features is vacuous."
                },
                "questions": {
                    "value": "1. The precise definition of dominating eigenvalues should be given to make Theorem 1 easier to read.\n\n2. \u201cTo show a case where over-smoothing is guaranteed, we also define a model diag(\u039bH) := \u2212(\u03c8^2), which we refer to using the superscript +.\u201d I think the authors mean diag(\u039bH) := +(\u03c8^2).\n\nMinor Comments that did not affect the score:\n\n1. It would be good if the authors could compare the runtime of their ViT-Ti^+ with other methods.\n\n2. The authors showed the distribution of dominating eigenvalues of their - version on CIFAR10, but not on ImageNet. It would be interesting to see if the distribution of the dominating eigenvalues is still the same on ImageNet."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8455/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698829332443,
            "cdate": 1698829332443,
            "tmdate": 1699637054807,
            "mdate": 1699637054807,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "OepGWKiBHM",
                "forum": "OCx7dp58H1",
                "replyto": "ZwAdSuRJVq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8455/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8455/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer QJ8g (Part 1)"
                    },
                    "comment": {
                        "value": "Dear Reviewer QJ8g,\n\nThank you so much for your time and questions. We respond to each of them below.\n\n> [..Weaknesses:..]\n\n> [..1. ...parameterizing H as a symmetric matrix might restrict the expressive power..]\n\nThanks for this comment. Our original motivation for using a symmetric parameterization was inspired by [Hu et al., 2019. Exploring weight symmetry in deep neural networks.] who show that weight symmetry in NNs does not affect their universal approximation capabilities. However, we agree that it would be interesting to test a non-symmetric parameterization. Based on your suggestion we have implemented a new parameterization for $\\mathbf{H}$ as $\\mathbf{H} = \\mathbf{V}_H \\Lambda \\mathbf{V}_H^{-1}$. Specifically, we take gradients to $\\mathbf{V}_H$ in the backwards pass, and compute $\\mathbf{H}$ using the above equation in the forwards pass. For both the symmetric and non-symmetric versions we test out 2 variants: 1. (-) i.e., the sharpening model: $\\mbox{diag}(\\Lambda_H ) := \u2212(\\psi^2)$ and a new variant 2. (*) where the first L/2 layers are parameterized as the sharpening model above, and the remaining L/2 layers are left normal (as in ViT-Ti). We use the suffix \u201c-NS\u201d for the non-symmetric parameterizations. We have also included a new baseline, FeatScale [Wang et al., 2022], as well as a new experiment where we double the depth of the network. Here are the new accuracy results, averaged over 3 trials with standard deviations (we bold the best result in each column and any method whose mean plus standard deviation is greater than or equal to mean of the best result): \n\nData Efficiency (Table 2)\n\n| Data Kept          \t| 100%     \t| 50%      \t| 10%      \t|\n|------------------|--------------|--------------|--------------|\n| ViT-Ti       \t| 66.78 \u00b1 0.1  | 56.38 \u00b1 0.5  | 32.47 \u00b1 0.9  |\n| ViT-Ti + FeatScale | 66.57 \u00b1 0.5  | 57.08 \u00b1 0.3  | 32.50 \u00b1 0.4  |\n| ViT-Ti$^+$   \t| 64.34 \u00b1 0.3  | 55.21 \u00b1 0.5  | 28.67 \u00b1 0.5  |\n| ViT-Ti$^-$   \t| 66.62 \u00b1 0.3  | 56.58 \u00b1 0.3  | **33.99 \u00b1 0.4**  |\n| ViT-Ti$^*$   \t| 67.84 \u00b1 0.6  | **57.78 \u00b1 0.4**  | **33.64 \u00b1 0.5**  |\n| ViT-Ti-NS$^-$\t| 67.80 \u00b1 0.2  | 57.54 \u00b1 0.1  | **33.78 \u00b1 0.3**  |\n| ViT-Ti-NS$^*$\t| **68.53 \u00b1 0.6**  | **57.67 \u00b1 0.3**  | 33.75 \u00b1 0.2  |\n\n\n[new] Increased Depth (Table 3)\n\n| Depth       \t| 12       \t| 24       \t|\n|----------------|--------------|--------------|\n| ViT-Ti     \t| 66.78 \u00b1 0.1  | 67.22 \u00b1 0.1  |\n| ViT-Ti + FeatScale | 66.57 \u00b1 0.5  | 67.71 \u00b1 0.5  |\n| ViT-Ti$^+$ \t| 64.34 \u00b1 0.3  | 65.65 \u00b1 0.3 |\n| ViT-Ti$^-$ \t| 66.62 \u00b1 0.3  | 66.76 \u00b1 0.3  |\n| ViT-Ti$^*$ \t| 67.84 \u00b1 0.6  | 69.05 \u00b1 0.1  |\n| ViT-Ti-NS$^-$  | 67.80 \u00b1 0.2  | 67.09 \u00b1 0.1  |\n| ViT-Ti-NS$^*$  | **68.53 \u00b1 0.6**  | **69.64 \u00b1 0.3**  |\n\n\nCorruption Robustness (Table 4)\n\n| Corruption Intensity | 0          \t| 1          \t| 2          \t| 3          \t| 4          \t| 5          \t|\n|----------------------|----------------|----------------|----------------|----------------|----------------|----------------|\n| ViT-Ti           \t| 66.78 \u00b1 0.1\t| 61.59 \u00b1 0.2\t| 57.75 \u00b1 0.2\t| 53.12 \u00b1 0.1\t| 49.32 \u00b1 0.1\t| 39.17 \u00b1 0.2\t|\n| ViT-Ti + Featscale   | 66.57 \u00b1 0.5\t| 61.25 \u00b1 0.5\t| 57.36 \u00b1 0.6\t| 52.64 \u00b1 0.6\t| 48.87 \u00b1 0.5\t| 38.77 \u00b1 0.4\t|\n| ViT-Ti$^+$       \t| 64.34 \u00b1 0.3\t| 58.36 \u00b1 0.4\t| 53.83 \u00b1 0.5\t| 49.03 \u00b1 0.6\t| 45.15 \u00b1 0.6\t| 35.86 \u00b1 0.6\t|\n| ViT-Ti$^-$       \t| 66.62 \u00b1 0.3\t| 61.62 \u00b1 0.2\t| 58.00 \u00b1 0.3\t| 53.56 \u00b1 0.2\t| 49.70 \u00b1 0.3\t| 40.07 \u00b1 0.2\t|\n| ViT-Ti$^*$       \t| 67.84 \u00b1 0.6\t| 62.49 \u00b1 0.5\t| 58.82 \u00b1 0.7\t| 54.32 \u00b1 0.7\t| 50.40 \u00b1 0.6\t| 40.38 \u00b1 0.7\t|\n| ViT-Ti-NS$^-$    \t| 67.80 \u00b1 0.2\t| **63.03 \u00b1 0.2**\t| **59.65 \u00b1 0.1**\t| **55.05 \u00b1 0.3**\t| **51.30 \u00b1 0.1**\t| **41.33 \u00b1 0.1**\t|\n| ViT-Ti-NS$^*$    \t| **68.53 \u00b1 0.6**\t| 63.01 \u00b1 0.1\t| **59.44 \u00b1 0.3**\t| **54.90 \u00b1 0.4**\t| **51.12 \u00b1 0.4**\t| 40.90 \u00b1 0.4\t|\n\nWe have added these results to the paper. The non-symmetric parameterization does improve upon the symmetric parameterization in some cases (particularly for increased depth and corrupted data). We appreciate your suggestion to investigate this.\n\n\n> [..2. ...robustness and data efficiency...]\n\nThanks. This seems to be a very deep question. We originally suspected a link between sharpening and data efficiency because of the eigenvalue distribution of the Data Efficient Image Transformer (DeiT-B/16) in Table 1: compared to all other baseline models, it has the highest distribution of sharpening eigenvalues. This is what led us to test data efficiency. We also hypothesized that it may help with input corruption robustness as many of the corruption techniques in Hendrycks & Dietterich, 2019 blur or reduce the resolution of the image: any further smoothing would likely make classification even harder. As far as we are aware we are the first to investigate the relationship between sharpening and data efficiency and corruption robustness. We agree this is interesting to further investigate. If you have any suggestions for additional experiments we will try to implement them as quickly as possible.\n\n(response continues below)"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8455/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700346807079,
                "cdate": 1700346807079,
                "tmdate": 1700354150589,
                "mdate": 1700354150589,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mcph4HTU9E",
                "forum": "OCx7dp58H1",
                "replyto": "ZwAdSuRJVq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8455/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8455/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer QJ8g (Part 2)"
                    },
                    "comment": {
                        "value": "> [..3. ...redundancy in the cases in Theorem 1..]\n\nThank you for pointing out this redundancy! We have fixed this.\n\n> [..4. ...the authors show the distribution of the dominating eigenvalues but do not show whether the features oversmooth when $\\lambda_1^A$ dominates...]\n\nWe believe there is a small confusion here. For ViT-Ti$^+$ we have that (a) Figure 3 shows that the features do oversmooth more and more, and (b) Table 1 shows that $\\lambda_n^A$ dominates. On the other hand, for ViT-Ti$^-$ we have that (a) Figure 3 indicates that the features do not oversmooth, and (b) Table 1 shows that $\\lambda_1^A$ dominates. This shows that even when the attention matrix $\\mathbf{A}$ is not fixed across layers the results agree with the Theorem statements. Thank you for letting us clarify this, we will add these details to the text to make this clearer.\n\n> [..Questions..]\n\n> [..1. The precise definition of dominating eigenvalues should be given...]\n\nWe agree. We have pulled this definition out of the proof and placed it into the main paper.\n\n> [..2.  I think the authors mean $\\mbox{diag}(\\Lambda_H ) := +(\\psi^2)$..]\n\nThank you for catching this! We have fixed this.\n\n> [..Minor Comments..]\n\n> [..1. compare the runtime of their ViT-Ti$^+$ with other methods...]\n\nThanks. The training time of ViT-Ti versus our original proposed methods is as follows (averaged over 3 runs):\n|     |  Training time |\n|----------|---------------|\n| ViT-Ti  |   5h 34min |\n| ViT-Ti$^+$|  6h 03min |\n| ViT-Ti$^-$ |  6h 03min |\n\nThe throughput of each method (in images/second, averaged over 10 runs) is:\n\n|     |  Throughput |\n|----------|---------------|\n| ViT-Ti  |   1683.0 |\n| ViT-Ti$^+$|  1642.6 |\n| ViT-Ti$^-$ |  1639.0 |\n\nWe will add these timings to the text.\n\n> [..2. The authors showed the distribution of dominating eigenvalues of their - version on CIFAR10, but not on ImageNet...]\n\nWe think there is a small confusion. The dominating eigenvalue distribution of our $\\mbox{}^-$ version is always 0%: $(1 + \\lambda^H_j \\lambda^A_n)$ and 100%: $(1 +  \\lambda^H_j \\lambda^A_1)$, regardless of the data, as we control the eigenvalues via bounding $\\Lambda^H$ in our parameterization. We will clarify this in the paper.\n\nThank you again for your time. Let us know if you have any other questions and we will respond as soon as possible."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8455/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700347299607,
                "cdate": 1700347299607,
                "tmdate": 1700354118807,
                "mdate": 1700354118807,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bwhw50JUdc",
                "forum": "OCx7dp58H1",
                "replyto": "ZwAdSuRJVq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8455/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8455/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Comment to Reviewer QJ8g"
                    },
                    "comment": {
                        "value": "Dear Reviewer QJ8g,\n\nThank you again for your time on the paper. Do you have any extra questions about the paper or our response? \n\nLet us know and we will try to response as soon as possible before the discussion period ends on November 22.\n\nThank you,\n\nThe authors"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8455/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700648746427,
                "cdate": 1700648746427,
                "tmdate": 1700648746427,
                "mdate": 1700648746427,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]