[
    {
        "title": "How Large Language Models Implement Chain-of-Thought?"
    },
    {
        "review": {
            "id": "VFHXBGFenw",
            "forum": "b2XfOm3RJa",
            "replyto": "b2XfOm3RJa",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4909/Reviewer_Cn8C"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4909/Reviewer_Cn8C"
            ],
            "content": {
                "summary": {
                    "value": "This paper makes an important contribution to our understanding of LLMs and how they work. Using reference datasets, this paper looks at the behavior of the model under perturbations of the input data to determine paths and attention heads that are used as parts of reasoning processes. The specific perturbations they use focus on constructing counterfactual examples."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "The biggest strength of this paper is the question they are asking. While many authors are focused on improving model performance on reasoning tasks, this paper focuses on understanding the internals of the model and advancing the science of the models themselves. The paper was also very strong in its data processing and creation, which was a novel and original reuse of existing data\n\n- Originality: There are two particularly original aspects to this paper. First, the use of reasoning datasets to provide counterfactual examples for models is an original and useful idea. Second \n\n- Quality: Tests were well thought out and explained, datasets were relevant and well chosen. The use of ablation/knockout methods to really focus and prove claims about model performance was particularly nice. \n\n- Clarity: By addressing a hard, technical topic, this paper did not set itself up for success on clarity, however the paper is well written with no major flaws in style or content. \n\n- Significance: As previously stated, the significance of this paper is that it is advancing the science of how LLMs work, rather than improve their performance while punting on the basic understanding of how they work."
                },
                "weaknesses": {
                    "value": "There are two basic weaknesses of this paper. First is clarity, which, as mentioned above, this is an area that it is difficult to be clear in because of the technical nature of the content.  Second, is the generality of the claims they make.\n\nRegarding the generality of the claims, the weakness of this comes from using limited data and reference models. This paper makes claims about LLMs at large, based on two example LLMs. I would like to know why these two models are representative for LLMs at large and why results from these two models are expected to generalize. Even better would be some claims about what classes of models these results are expected to apply to."
                },
                "questions": {
                    "value": "Suggestions:\n\n- Please fix the citation on page 3 for HuggingFace.\n\n- I would also like to see a specific discussion section that pulls together all the results into a summary of what you learned. Right now, that content is spread across a lot of the experimental section, so I'd suggest consolidating it under its own section and highlighting the valuable lessons learned."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "10: strong accept, should be highlighted at the conference"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4909/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698180204129,
            "cdate": 1698180204129,
            "tmdate": 1699636476026,
            "mdate": 1699636476026,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "hn3Z8OwVWB",
                "forum": "b2XfOm3RJa",
                "replyto": "VFHXBGFenw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4909/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4909/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Cn8C"
                    },
                    "comment": {
                        "value": "We would like to thank the reviewer for taking the time to review our work. We appreciate that you find our paper **ask a good question**.  According to your valuable comments, we provide detailed feedback. Please find the special responses to your comments below.\n\n**Q1**: Clarity issue\n> There are two basic weaknesses of this paper. First is clarity, which, as mentioned above, this is an area that it is difficult to be clear in because of the technical nature of the content.\n\n***Ans for Q1):***\nThanks for pointing out the clarity issue. \n- Following your kind suggestion, we have made adjustments to the content of the **Method** section in the main text (highlighted in blue). \n- We have redrawn the framework diagram (Figure 1) of our method and included it in the main text. We would appreciate it if you could find some time to review our revised version.\n\n**Q2**: Generality issue\n> Second, is the generality of the claims they make. Regarding the generality of the claims, the weakness of this comes from using limited data and reference models. This paper makes claims about LLMs at large, based on two example LLMs. I would like to know why these two models are representative for LLMs at large and why results from these two models are expected to generalize. Even better would be some claims about what classes of models these results are expected to apply to.\n\n***Ans for Q2):***\nThanks for your constructive comments. \n- The reason we chose these two models (LLaMA2-7B and Qwen-7B) has two folds: \n  - i) The distinguishing feature of large language models compared to earlier language models is their emergent ability, which starts to emerge with 7B parameters. \n  - ii) The model architectures of large language models are predominantly transformers and text data. The main difference between different large language models lies in their training data. LLaMA2-7B [1] is primarily trained on English corpora, while Qwen-7B [1] is trained on both Chinese and English corpora. Both models [1,2] have shown good performance on NLP tasks. Therefore, we chose LLaMA2-7B as a representative of large English language models and Qwen-7B as a representative of large Chinese-English language models.\n\n> ***Reference***\n>\n> [1] Llama 2: Open foundation and fine-tuned chat models. In Arxiv 2023. https://arxiv.org/abs/2307.09288\n> \n> [2] Qwen technical report. https://qianwen-res.oss-cn-beijing.aliyuncs.com/QWEN_TECHNICAL_REPORT.pdf\n\n**Q3**: Typo issue\n> Please fix the citation on page 3 for HuggingFace.\n\n***Ans for Q3):***\nThanks for your suggestion and careful review. We have fixed this typo problem.\n\n**Q4**: More discussion\n> I would also like to see a specific discussion section that pulls together all the results into a summary of what you learned. Right now, that content is spread across a lot of the experimental section, so I'd suggest consolidating it under its own section and highlighting the valuable lessons learned.\n\n***Ans for Q4):***\nThanks for your careful review and constructive suggestions. In Section 5, we summarized the valuable lessons learned from experiments as follows:\n\n*Moreover, through additional analysis, we discover interesting patterns from the two of the identified key heads specifically. Additionally, through extensive ablation studies on various CE templates and metrics to assess the importance of the patched heads, we find that in-context learning is an effective method to construct REs/CEs for path patching, which can control the behavior of LLMs. The control over LLMs can be strengthened by increasing the few-shot examples in the prompt. Focusing on the probability of the tokens related to a specific task/behavior is a useful method to deal with the sparse causal effect problem.*"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4909/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700199392296,
                "cdate": 1700199392296,
                "tmdate": 1700199392296,
                "mdate": 1700199392296,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ywC4ioI0UH",
                "forum": "b2XfOm3RJa",
                "replyto": "VFHXBGFenw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4909/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4909/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Welcome for more discussions (Cn8C)"
                    },
                    "comment": {
                        "value": "Dear reviewer #Cn8C,\n\nThanks for your valuable time in reviewing and constructive comments, according to which we have tried our best to answer the questions and carefully revise the paper. Here is a **summary of our response** for your convenience:\n\n- (1) **Clarity issue**: Thanks for the kind suggestion, we have made adjustments to the **Method** section and redrawn the framework diagram (**Figure 1**).\n- (2) **Generality issue**: The reason we chose LLaMA2-7B and Qwen-7B has two folds, i) The distinguishing feature, emergent ability, starts to emerge with 7B parameters. ii) The main difference between different LLMs is the training data. LLaMA2-7B is primarily trained on English corpora, while Qwen-7B is trained on both Chinese and English corpora. Both models have shown good performance on NLP tasks. Therefore, we chose LLaMA2-7B as a representative of large English language models and Qwen-7B as a representative of large Chinese-English language models.\n- (3) **Typo issue**: Thanks for your suggestion and careful review. We have fixed this typo problem.\n- (4) **More discussion**: Thanks for your careful review and constructive suggestions. In Section 5 of our revised paper, we summarized the valuable lessons learned from experiments.\n\nWe humbly hope our response has addressed your concerns. If you have any additional concerns or comments that we may have missed in our responses, we would be most grateful for any further feedback from you to help us further enhance our work.\n\n\nBest regards\n\nAuthors of #4909"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4909/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700389770125,
                "cdate": 1700389770125,
                "tmdate": 1700389943678,
                "mdate": 1700389943678,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SI3beQRJpR",
                "forum": "b2XfOm3RJa",
                "replyto": "VFHXBGFenw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4909/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4909/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Window for discussion and revision is closing"
                    },
                    "comment": {
                        "value": "Dear Reviewer #Cn8C,\n\nThanks a lot for your time in reviewing and insightful comments, according to which we have carefully revised the paper to answer the questions. We sincerely understand you\u2019re busy. But since the discussion due is approaching, would you mind checking the response and revision to confirm where you have any further questions?\n\nWe are looking forward to your reply and are happy to answer your further questions.\n\nBest regards\n\nAuthors of #4909"
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4909/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700549207247,
                "cdate": 1700549207247,
                "tmdate": 1700549207247,
                "mdate": 1700549207247,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZvrtJt5NAx",
                "forum": "b2XfOm3RJa",
                "replyto": "VFHXBGFenw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4909/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4909/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Window for discussion is closing in 21 hours"
                    },
                    "comment": {
                        "value": "Dear Reviewer Cn8C,\n\nThanks very much for your great efforts in reviewing and valuable comments. The author's discussion will end in the last 21 hours. At this final moment, we sincerely appreciate it if you could check our responses including **Response to Reviewer Cn8C**. And our response to other Reviewers may also provide you with more information.\n\nIf you have any further concerns, we will instantly respond to you at this final moment. Your support for a novel discovery in its earlier stage is very important and may inspire more new findings in LLMs interpretability.\n\nBest regards and thanks,\n\nAuthors of #4909"
                    }
                },
                "number": 25,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4909/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700623899690,
                "cdate": 1700623899690,
                "tmdate": 1700624011228,
                "mdate": 1700624011228,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "B5IJ6ADwWN",
            "forum": "b2XfOm3RJa",
            "replyto": "b2XfOm3RJa",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4909/Reviewer_6hxZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4909/Reviewer_6hxZ"
            ],
            "content": {
                "summary": {
                    "value": "The paper investigates how chain-of-thought (CoT) prompting enhances reasoning capabilities in large language models (LLMs). The key findings are:\nAdjusting the few-shot examples in a CoT prompt is an effective way to generate paired inputs that elicit or suppress reasoning behaviour for analysis. Only a small fraction of attention heads, concentrated in middle and upper layers, are critical for reasoning tasks. Ablating them significantly harms performance. The authors show that some heads focus on the final answer while others attend to intermediate reasoning steps, corresponding to the two stages of CoT."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper:\n- Provides novel insights into CoT reasoning through attention-head analysis. \n- Links model components to reasoning subtasks."
                },
                "weaknesses": {
                    "value": "This paper:\n- Focuses only on textual reasoning tasks, not more general capabilities.\n- Limited to analyzing attention heads, does not cover other components like MLPs.\n- Does not modify training to directly improve reasoning abilities."
                },
                "questions": {
                    "value": "Do you think these findings would transfer to more open-ended generative tasks beyond QA?\n\nDid you consider any changes to the model architecture or training process to improve reasoning?\n\nCould your analysis approach help detect if a model is just memorizing vs. logically reasoning?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4909/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4909/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4909/Reviewer_6hxZ"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4909/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698735449659,
            "cdate": 1698735449659,
            "tmdate": 1699636475945,
            "mdate": 1699636475945,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "R7xOmiZY9u",
                "forum": "b2XfOm3RJa",
                "replyto": "B5IJ6ADwWN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4909/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4909/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 6hxZ [part 1/2]"
                    },
                    "comment": {
                        "value": "We would like to thank the reviewer for taking the time to review our work. We appreciate that you find our paper provides novel insights into LLMs' CoT reasoning. According to your valuable comments, we provide detailed feedback. Please find the special responses to your comments below.\n\n**Q1**: General capabilities issues:\n> \"This paper Focuses only on textual reasoning tasks, not more general capabilities.\"\n\n***Ans for Q1):*** \nWe agree with your perspective that when exploring reasoning ability, it is important to conduct some general investigations. \n- In our study, we followed the work of [1,2,3] and explored the interpretability of the CoT reasoning ability of LLMs using the path patching method they proposed. Our work differs from theirs in that we explore the interpretability of more complex textual reasoning capabilities on a larger language model. \n- As for reasoning abilities in other modalities, that will be part of our future work. We hope that our exploration of the textual modality can inspire research on the interpretability of reasoning abilities in other modalities.\n\n**Q2**:  Not cover MLPs:\n> \"This paper limited to analyzing attention heads, does not cover other components like MLPs.\"\n\n***Ans for Q2):***\nThanks for your constructive comments. \n- In this work, we focus on the attention heads because heads can better represent semantic information compared to neurons in MLP layers, consistent with previous work [1]. \n- Inspired by your insightful comments, we would like to note that heads serve as input units for MLP. If we can categorize identified heads under different MLPs, we would indirectly identify important MLPs, which may align with your insightful comments. \n- We sincerely appreciate your insight, and in our future work, we will directly explore MLPs rather than analyzing them indirectly through attention heads.\n\n\n\n**Q3**: Model improvement issues:\n> This paper does not modify training to directly improve reasoning abilities.\n\n***Ans for Q3):***\nThanks for your insightful comments. \n\n- We mainly follow previous works on LLM interpretability [1,2,3] to locate key heads/units, paving the way for finetuning LLMs. \n- The direction of using identified heads to improve training is indeed promising. However, this exciting direction remains largely under-explored. In this context, we hope our work can contribute to the community in the mentioned promising direction, i.e., interpret-then-finetune.\n- We really appreciate your insightful suggestion to explore how to fine-tune attention heads to enhance model reasoning ability. We will leave it as our future work.\n\n> ***Reference***\n>\n> [1] Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small. In ICLR 2023.\n> \n> [2] Localizing Model Behavior With Path Patching. In ArXiv 2023.\n> \n> [3] How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model. In ArXiv 2023.\n\n**Q4**: Generalization issues:\n> Q: Do you think these findings would transfer to more open-ended generative tasks beyond QA?\n\n***Ans for Q4):***\nThanks for the inspiring comments. \n\n- We will explore the interpretability of model performance in open-ended generative tasks in future work.\n- We believe that our work can be transferred to more open-ended generative tasks. A possible approach is to perturb the outputs of attention heads and allow the model to generate content freely. By evaluating the changes in the quality of the generated content, we can assess whether a head is crucial for completing the open-ended generative task.\n- However, it is challenging to i) design dynamic counterfactual examples $x_c$ to perform causal intervention and ii) evaluate the quality changes in generated content. Thanks for the inspiring question, we believe exploring these challenges is exciting.\n\n**Q5**: Model improvement issues:\n> Q: Did you consider any changes to the model architecture or training process to improve reasoning?\n\n***Ans for Q5):***\nThanks for pointing out the promising direction. \n\n- We agree with your point that identifying key heads and making modifications is a very promising direction. However, this may involve the field of model editing/modifying, which is a promising direction to explore.\n- We would like to note that we mainly focus on how to interpret the reasoning ability of large language models through head localization. Aligning with your insightful question, interpreting models paves the way for model re-training for finetuning to accurately improve model performance, i.e., on a specific task."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4909/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700199282423,
                "cdate": 1700199282423,
                "tmdate": 1700327181253,
                "mdate": 1700327181253,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "AeKzcotCMG",
                "forum": "b2XfOm3RJa",
                "replyto": "B5IJ6ADwWN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4909/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4909/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 6hxZ [part 2/2]"
                    },
                    "comment": {
                        "value": "**Q6**: memorizing vs. reasoning:\n> Q: Could your analysis approach help detect if a model is just memorizing vs. logically reasoning?\n\n***Ans for Q6):***\nIt is an insightful question! Thanks for your kind guidance aiming to make our work more solid. \n- Model memorization is a classical/traditional field in deep learning while exploring a model's memorization capabilities is inherently challenging. In the context of large models, there are two challenges: i) determining whether the model's behavior is due to memorization, generalization, or reasoning is quite a challenging task, and ii) under the conditions of large models, we can access the model weights but not the training data, making it even more difficult to determine whether the model is memorizing or reasoning without access to the data.\n\nWe have added a discussion about the interesting direction in our revised paper (Appendix F)."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4909/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700199316739,
                "cdate": 1700199316739,
                "tmdate": 1700327216832,
                "mdate": 1700327216832,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "phu9qDqK2u",
                "forum": "b2XfOm3RJa",
                "replyto": "B5IJ6ADwWN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4909/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4909/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Welcome for more discussions (6hxz)"
                    },
                    "comment": {
                        "value": "Dear reviewer #6hxz,\n\nThanks for your valuable time in reviewing and constructive comments, according to which we have tried our best to answer the questions and carefully revise the paper. Here is a **summary of our response** for your convenience:\n\n- (1) **Focuses only on textual reasoning tasks**: Thanks for the insightful comments, it is important to conduct some general investigations. our work mainly follows the representative work of LLMs interpretability that uses path patching to locate the key component for a specific behavior in LLMs. Exploring reasoning abilities in other modalities will be our future work.\n- (2) **Not cover MLP**: Thanks for your constructive comments. We focus on the attention heads because heads can better represent semantics, which is consistent with previous work. The importance of MLP can be indirectly categorized using the key heads we identified. We will directly explore MLPs in our future work.\n- (3) **Does not modify training**: We mainly follow previous works of LLM interpretability to locate key heads/units. The direction of using identified heads to improve training is promising but largely under-explored. We will leave it as our future work.\n- (4) **Transfer to more open-ended generative tasks**: Thanks for the inspiring comments. We believe that our work can be transferred to more open-ended generative tasks. But there are two challenges: i) design the dynamic $x_c$. ii) evaluate the generated content. In our future work, we will explore the interpretability of the model on generative tasks.\n- (5) **Change the model architecture or training process**: We agree with your point that identifying key heads and making modifications is a very promising direction, which involves the field of model editing and is a promising direction for future work.\n- (6) **Memorizing vs. logically reasoning**: It is an insightful question! Exploring a model\u2019s memorization capabilities is inherently a challenging task. However, since we can not access the model training data, it is even more difficult to determine whether the model is memorizing or reasoning.\n\nWe humbly hope our response has addressed your concerns. If you have any additional concerns or comments that we may have missed in our responses, we would be most grateful for any further feedback from you to help us further enhance our work.\n\nBest regards\n\nAuthors of #4909"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4909/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700389746427,
                "cdate": 1700389746427,
                "tmdate": 1700389818903,
                "mdate": 1700389818903,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OQDC9W5wCG",
                "forum": "b2XfOm3RJa",
                "replyto": "B5IJ6ADwWN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4909/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4909/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Window for discussion and revision is closing"
                    },
                    "comment": {
                        "value": "Dear Reviewer #6hxZ,\n\nThanks a lot for your time in reviewing and insightful comments, according to which we have carefully revised the paper to answer the questions. We sincerely understand you\u2019re busy. But since the discussion due is approaching, would you mind checking the response and revision to confirm where you have any further questions?\n\nWe are looking forward to your reply and are happy to answer your further questions.\n\nBest regards\n\nAuthors of #4909"
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4909/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700549167658,
                "cdate": 1700549167658,
                "tmdate": 1700549167658,
                "mdate": 1700549167658,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qBxz5s4kHD",
                "forum": "b2XfOm3RJa",
                "replyto": "AeKzcotCMG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4909/Reviewer_6hxZ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4909/Reviewer_6hxZ"
                ],
                "content": {
                    "title": {
                        "value": "Respond to authors"
                    },
                    "comment": {
                        "value": "I thank the authors for responding to my initial comments. I will maintain my score."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4909/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700556890333,
                "cdate": 1700556890333,
                "tmdate": 1700556890333,
                "mdate": 1700556890333,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ftjGwQ1LGB",
            "forum": "b2XfOm3RJa",
            "replyto": "b2XfOm3RJa",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4909/Reviewer_bDfK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4909/Reviewer_bDfK"
            ],
            "content": {
                "summary": {
                    "value": "This paper aims to explain the CoT reasoning ability of LLMs by identifying \"important\" attention heads that \"contribute the most\" to the predictions. Specifically, the paper first constructs counterfactual samples for every referential few-shot CoT sample by replacing the CoT texts in the few-shot examples with random texts generated by ChatGPT. The paper then adopts a method developed in prior work to assign an importance score for every attention head in the LLM. \n\nThe authors discover that only a small fraction of the attention heads are important to the CoT task. They also discover that attention heads have different roles: some are responsible for verifying the answer and some are used to synthesize the step-by-step behavior of CoT."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Understanding the behavior of LLMs is an important topic that could lead to more robust and trustworthy deep-learning models. This paper focuses on demystifying the chain of thought behavior, which is a practically useful and widely studied phenomenon of LLMs. The paper focuses mainly on identifying important attention heads, which could lead to a better understanding of the attention mechanism employed by Transformer models. Interesting observations regarding the attention patterns and different roles of every attention head have been made."
                },
                "weaknesses": {
                    "value": "My primary concern is that the methodology used in the paper is not tailored to understanding CoT behaviors. Specifically, the method used to identify \"important\" attention heads is adopted from prior work (Wang et al. (2023) cited in the paper). On the method side, the only task-specific design is how to construct the counterfactual sample $x_c$ given a reference sample $x_r$, which is done by replacing the CoT part in the few-shot example prompt with some randomly generated text (by ChatGPT). It would need more justification why the important attention heads identified by $x_c$ generated in this way contribute to the CoT behavior since (i) it is possible that a (simple) adversarial change in the prompt could significantly decrease the accuracy; (ii) in $x_c$, since the CoT reasoning demonstrations are removed, the LLM would by default not using CoT, which explains the drop in the accuracy; (iii) it would be nice to design the counterfactual example in some other ways, e.g., add incorrect (but still relevant) CoT demonstrations.\n\nAdditionally, the score/importance of every attention head is scored by the accuracy drop when substituting its output with the corresponding activations generated by the counterfactual examples. Since $x_r$ and $x_c$ could differ significantly, the paper fails to justify whether replacing the activations directly will have some deteriorating effects on the LLM, since it completely \"block\" the information flow. This makes it harder to justify the conclusions made in the paper."
                },
                "questions": {
                    "value": "The proposed method does not seem to have specific designs for understanding CoT.\n\nWill directly replacing the activations of certain attention heads have deteriorating effects on the LLM?\n\nPlease refer to the weakness section for more information."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4909/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4909/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4909/Reviewer_bDfK"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4909/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698741287526,
            "cdate": 1698741287526,
            "tmdate": 1699636475864,
            "mdate": 1699636475864,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Z9rhHrBp5G",
                "forum": "b2XfOm3RJa",
                "replyto": "ftjGwQ1LGB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4909/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4909/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer bDfK [part 1/3]"
                    },
                    "comment": {
                        "value": "We would like to thank the reviewer for taking the time to review our work. We appreciate that you find our paper practically **useful and interesting**. According to your valuable comments, we provide detailed feedback. Please find the special responses to your comments below.\n\n\n**Q1**: Methodology issues:\n> \"My primary concern is that the methodology used in the paper is not tailored to understanding CoT behaviors. Specifically, the method used to identify \"important\" attention heads is adopted from prior work (Wang et al. (2023) cited in the paper). On the method side, the only task-specific design is how to construct the counterfactual sample $x_c$ given a reference sample $x_r$, which is done by replacing the CoT part in the few-shot example prompt with some randomly generated text (by ChatGPT). \n> \n> It would need more justification why the important attention heads identified by generated in this way contribute to the CoT behavior since (i) it is possible that a (simple) adversarial change in the prompt could significantly decrease the accuracy; (ii) in $x_c$, since the CoT reasoning demonstrations are removed, the LLM would by default not using CoT, which explains the drop in the accuracy; (iii) it would be nice to design the counterfactual example in some other ways, e.g., add incorrect (but still relevant) CoT demonstrations.\"\n\n***Ans for Q1.1):*** \nThanks for highlighting the potentially confusing issue. We apologize for the misunderstanding, which may result from the inadequate explanation of **path patching**. In response to your comments, we have added corresponding explanations to our work.\n\n- Path patching is a method used to study causal relationships and identify key modules in language models through causal interventions [1,2,3]. The causal interventions are implemented through the design of reference data and counterfactual data $x_r, x_c$, where the key distinction lies in whether they can elicit the model's specific abilities/behaviors. Our focus is on the CoT reasoning ability of large language models (LLMs). In this context, the difference between $x_r, x_c$ lies in whether they can trigger the model's reasoning ability. We believe it aligns with your comments that our $x_r$ triggers the model's CoT ability, and $x_c$ does not. Designing appropriate pairs is challenging and promising for interpreting LLMs.\n- According to path patching, the causal relationship between key heads and the model's CoT ability can be investigated by designing appropriate $x_r, x_c$. The importance of data construction is explicitly stated in path patching [1,2,3], and this crucial aspect is also reflected in our study. Poorly designed $x_c$ may inadvertently trigger the model's reasoning ability, making it difficult to locate the model's CoT reasoning ability, as discussed in our ablation study in Appendix E. \n- The construction of $x_c$ is based on our proposed method that leverages in-context learning for interpretation, which effectively controls the model's reasoning ability while not affecting its other capabilities. As shown in Table 2, when we replace the reasoning process in the few-shot example with irrelevant sentences, the model no longer outputs the reasoning process but still produces the answer.\n\n> ***Reference***\n>\n> [1] Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small. In ICLR 2023.\n> \n> [2] Localizing Model Behavior With Path Patching. In ArXiv 2023.\n> \n> [3] How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model. In ArXiv 2023."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4909/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700194783094,
                "cdate": 1700194783094,
                "tmdate": 1700194783094,
                "mdate": 1700194783094,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Xntr2wdsh2",
                "forum": "b2XfOm3RJa",
                "replyto": "ftjGwQ1LGB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4909/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4909/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer bDfK [part 3/3]"
                    },
                    "comment": {
                        "value": "**Q3**: Method issue:\n> \"The proposed method does not seem to have specific designs for understanding CoT.\"\n\n***Ans for Q3):***\nWe apologize for the misunderstanding. Accordingly, we would like to highlight the novelty and contribution as follows. \n\n- We have added more explanations (in Section 3.1) to highlight the challenges and two key approaches to address the challenges, aiming to avoid similar issues. Specifically, we propose to leverage in-context learning to construct $x_r, x_c$ (in Section 3.2), and propose a novel metric to evaluate the causal effect, i.e., the proposed Word-of-Interest (WoI) Norm (in Section 3.3). The challenges and our proposed methods in Section 3.1 are shown below, and we believe these changes would make our paper more clear.\n<font color=blue style=\"font-family: 'Times New Roman';\">In this work, we aim to localize the key attention head within the model responsible for CoT reasoning. However, we are faced with two primary challenges: i) The complexity arises from the diverse range of abilities that LLMs possess to achieve CoT reasoning, including numerical computation, knowledge retrieval, and logical reasoning, making it challenging to design reference examples (REs) that are accurately paired with counterfactual examples (CEs) differing only in their ability to trigger the CoT reasoning behavior of LLMs. ii) Additionally, the extensive potential word in the LLM's output results in a sparsity of causal effects, as the words directly related to CoT reasoning are significantly limited in number. To overcome these challenges, we propose an innovative approach for constructing paired REs and CEs, accompanied by a word-of-interest (WoI) normalization method aimed at addressing the issue of sparse causal effects.</font>\n\n- We would like to highlight that i) it is challenging to interpret the CoT reasoning ability, accordingly, we propose to leverage path patching to address the challenge; ii) it is challenging to directly employ path patching for the task, because it is unclear how to construct $x_r, x_c$ to realize path patching, accordingly, we propose an in-context learning approach and a novel approach to realize it; iii) it is challenging to locate key heads even using path patching, accordingly, we propose a novel normalization approach to address the challenge. We believe these explorations are specific designs for understanding CoT.   \n\n- Inspired by your valuable comments, we have conducted numerous experiments in data construction and found that utilizing in-context learning is an effective method. Please refer to Section 4.4 for more details about our other attempts to design $x_c$ based on in-context learning. In addition to the in-context learning method, we also attempted to use the zero-shot CoT prompt to design $x_r, x_c$. However, the heads identified through this method are not the key heads for CoT reasoning since knocking out these heads shows little effect on the models' reasoning ability. Detailed results can be found in Appendix E. \n\n\n**Q4**: Worry about the interference with the attention heads:\n> \"Will directly replacing the activations of certain attention heads have deteriorating effects on the LLM?\"\n\n***Ans for Q4):***\nThank you for your valuable question. \n- To validate your viewpoint, we have performed random perturbations on the heads, as shown in **Figure 3**. The random perturbations on random-selected heads have almost no impact on the CoT reasoning capability of the model while perturbing the key heads significantly affects the model's performance. Please refer to **Section 4.2 Validation of key heads** for more detailed information. \n- Inspired by your question, we plan to include the results of model performance on common NLP evaluation tasks (such as MMLU [1]) when knocking out the key heads. We will add it to our work when we complete the experiments.\n\n> ***Reference***\n>\n> [1] Measuring Massive Multitask Language Understanding. In ICLR 2021."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4909/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700195153289,
                "cdate": 1700195153289,
                "tmdate": 1700296327853,
                "mdate": 1700296327853,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7DLz1uY1Xj",
                "forum": "b2XfOm3RJa",
                "replyto": "ftjGwQ1LGB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4909/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4909/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Welcome for more discussions (bDfK)"
                    },
                    "comment": {
                        "value": "Dear reviewer #bDfK,\n\nThanks for your valuable time in reviewing and constructive comments, according to which we have tried our best to answer the questions and carefully revise the paper. Here is a **summary of our response** for your convenience:\n\n- (1) **Methodology issues**: Thanks for highlighting the potentially confusing issue. we have explained our **methodology** in ***Answer for Q1.1*** and listed some representative papers of LLM interpretability. We have explained the relationship between adversarial change and causal intervention in ***Answer for Q1.2*** for better understanding. Furthermore, according to your suggestion about $x_c$ design, extensive experiments about $x_r, x_c$ construction are discussed in **Section 4.4** and **Appendix E**.\n- (2) **Deteriorating effects on LLMs**: Following your constructive advice, a further explanation about path patching and a more comprehensible method framework diagram have been supplemented in the revised paper. In **Section 4.2**, we have verified that replacing the activations directly will not have deteriorating effects on the LLMs.\n- (3) **No specific designs for understanding CoT**: Apologies for the misunderstanding, more explanations about the challenges and our innovative methods are highlighted and added to **Section 3.1**. Inspired by your valuable comments, numerous experiments in data construction have been further conducted and we find that utilizing in-context learning is an effective method.\n- (4) **Worry about the interference with the attention heads**: Thank you for your valuable question. We have performed random perturbations on the heads. The random perturbations have almost no impact on the model on CoT reasoning while perturbing the key heads significantly affects the model\u2019s performance. (See **Section 4.2  Validation of key heads)\n\nWe humbly hope our response has addressed your concerns. If you have any additional concerns or comments that we may have missed in our responses, we would be most grateful for any further feedback from you to help us further enhance our work.\n\n\nBest regards\n\nAuthors of #4909"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4909/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700389714318,
                "cdate": 1700389714318,
                "tmdate": 1700389864117,
                "mdate": 1700389864117,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lEJc2HNxAB",
                "forum": "b2XfOm3RJa",
                "replyto": "ftjGwQ1LGB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4909/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4909/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Window for discussion and revision is closing"
                    },
                    "comment": {
                        "value": "Dear Reviewer #bDfK,\n\nThanks a lot for your time in reviewing and insightful comments, according to which we have carefully revised the paper to answer the questions. We sincerely understand you\u2019re busy. But since the discussion due is approaching, would you mind checking the response and revision to confirm where you have any further questions?\n\nWe are looking forward to your reply and are happy to answer your further questions.\n\nBest regards\n\nAuthors of #4909"
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4909/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700549129628,
                "cdate": 1700549129628,
                "tmdate": 1700549129628,
                "mdate": 1700549129628,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "f0KBS3It8z",
                "forum": "b2XfOm3RJa",
                "replyto": "ftjGwQ1LGB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4909/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4909/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Window for discussion is closing in last 21 hours"
                    },
                    "comment": {
                        "value": "Dear Reviewer bDfK,\n\nThanks very much for your great efforts in reviewing and valuable comments. The author's discussion will end in the last 21 hours. At this final moment, we sincerely appreciate it if you could check our responses including **Response to Reviewer bDfK [part 1/3]**, **Response to Reviewer bDfK [part 2/3]**, and **Response to Reviewer bDfK [part 3/3]**. And our response to other Reviewers may also provide you with more information.\n\nIf you have any further concerns, we will instantly respond to you at this final moment. And we sincerely appreciate that you could consider improving your score if there is no further concern. Your support for a novel discovery in its earlier stage is very important and may inspire more new findings in LLMs interpretability.\n\nBest regards and thanks,\n\nAuthors of #4909"
                    }
                },
                "number": 24,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4909/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700623810554,
                "cdate": 1700623810554,
                "tmdate": 1700623920684,
                "mdate": 1700623920684,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "e0spcU3ujK",
                "forum": "b2XfOm3RJa",
                "replyto": "ftjGwQ1LGB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4909/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4909/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Window for responsing and draft updating is closing"
                    },
                    "comment": {
                        "value": "Dear Reviewer #bDfK,\n\nThanks a lot for your time in reviewing and reading our response and the revision. Thanks very much for your valuable comments. We sincerely understand you\u2019re busy. But as the window for responding and paper revision is closing, would you mind checking our response (a brief summary, and details) and confirming whether you have any further questions? We look forward to answering more questions from you.\n\nBest regards and thanks,\n\nAuthors of #4909"
                    }
                },
                "number": 26,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4909/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700711773589,
                "cdate": 1700711773589,
                "tmdate": 1700711773589,
                "mdate": 1700711773589,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "cJG20XUIbV",
            "forum": "b2XfOm3RJa",
            "replyto": "b2XfOm3RJa",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4909/Reviewer_wEdy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4909/Reviewer_wEdy"
            ],
            "content": {
                "summary": {
                    "value": "The reasoning processes of the LLaMA2-7B and Qwen-7B models are interpreted using the path patching method (initially introduced in [1], which is an interoperability method rooted in causal intervention), in the context of using few-shots prompts. The evaluation is grounded on three benchmarks: StrategyQA, AQuA, and CSQA (which have all been introduced in previous publications). They find that only a small number of attention heads are responsible for reasoning, and they also find that they are located in specific locations in the model's architecture.\n\nThis represents a solid effort to interpret large language models using the path patching method to date and is the first paper where the chain-of-thoughts method is interpreted using the path patching method. \n\n[1] https://openreview.net/pdf?id=NpsVSN6o4ul"
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Carrying out this piece of research requires handling several different technical aspects (carefully constructing the datasets to allow counterfactual evaluation, applying the path patching method, and evaluating the effects of knocking out different attention heads), which the authors seem to largely have done well, including paying attention to subtle issues like the choice of right metric (section 4.4.2).\n\nTheir findings highlight interesting behaviors that the attention heads display, which is summarized in section 4.3, for example: \"_Analogically,\nthe function of Head 18.30 and 13.16 corresponds to the two stages of the chain-of-thought (CoT) process: firstly think step-by-step to get intermediate thoughts, then answer the question based on these thoughts._\"         \n\nI think such research will become more widely spread in the future in order to understand the reasoning processes of attention-based models better, and as such, it is very timely work."
                },
                "weaknesses": {
                    "value": "- the presentation is at times unclear (see the questions section). I found it quite hard to read and had to spend some time understanding their methodology\n- the literature review section could be more comprehensive: a number of other articles on interpretability rooted in causal interventions exist on models of similar sizes, such as [2,3], the latter also using a 7B model but not being cited. I would recommend the authors contrast the existing approaches in the \"_Related Work_\" section or an appendix to that section, such as from [3] -but there are also other articles- with their own and comment upon similarities and differences so that the reader is well-informed of how their methods compete with existing ones.\n- the improvements are nice but somewhat incremental since a single in-context learning technique is analyzed on just two models.\n- their methodology might also be improved by the use of diagrams to show in a single glance all the relevant information\n- it's somewhat strange that Appendix B (\"_Reference Data Examples_\") is empty; why include it in that case?\n\n\n[2] https://arxiv.org/pdf/2305.00586.pdf     \n[3] https://arxiv.org/pdf/2305.08809.pdf"
                },
                "questions": {
                    "value": "Tables 1, 2 / 4, 5 are unclear and suffer from a number of issues:\n- Table 1, 2: A reader might at first be confused whether what is shown is the complete few-shot that is supplied to the model that is to be tested (e.g., LLaMA2) to facilitate in-context learning - or if what follows after the \"A\" is the answer of LLaMA2/Qwen?\nThat the former is correct transpires indirectly only from the text: \"_The outputs of LLaMA-7B and Qwen-7B on the counterfactual data are shown in Table 4 and 5, respectively._\" I would recommend adding at least a caption here, explaining more clearly what can be seen, without having to look in the text for the meaning of the table.\n- In Table 1,2, the last question is \"_Can Reiki be stored in a bottle?_\", which led me to believe that this is the question the model should have answered (once with the correct in-context learning text and once with the modified/colored text as indicated from table 2).     \nBut a look in the appendix at Table 4 suddenly reveals a different last question, \"_Would a rabbi worship martyrs Ranavalona I killed?_\" (all else being the same), which I found confusing. Can the authors explain this?\n- minor formatting: \"Q\" and \"A\" from Table 1,2 are set in bold but not in Table 4,5, which does not aid readability.\n- since these tables seem an essential part of the paper, as the capture the methodology, it is tiring to go back and forth between Tables 1,2 and the Appendix; I would propose to move all Tables to the main body to aid readability.\n\nIn the section \"Counterfactual data generation\" you say: \"_x_c is generated by partially editing x_r with the purpose of altering the model\u2019s reasoning effects. In order to suppress the reasoning capability of the model while maintaining equal lengths for counterfactual data and   reference data, we replaced the evidence in x_r with irrelevant sentences describing natural scenery, which were randomly generated\nusing ChatGPT-3.5._\"\nNo mention of a specific dataset is being made. Shouldn't the change that you made account for the specific structure of the dataset? I am guessing if some dataset actually deals with natural scenery, then inserting random natural scenery descriptions might not achieve the desired counterfactual effect. Some statement should be made here that what is randomly included matches the dataset."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "(not applicable)"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4909/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4909/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4909/Reviewer_wEdy"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4909/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698773486636,
            "cdate": 1698773486636,
            "tmdate": 1699636475792,
            "mdate": 1699636475792,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "n3DxwrX6yE",
                "forum": "b2XfOm3RJa",
                "replyto": "cJG20XUIbV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4909/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4909/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer wEdy [part 1/3]"
                    },
                    "comment": {
                        "value": "We would like to thank the reviewer for taking the time to review our work. We appreciate that you find our paper is **a solid effort** to interpret LLMs and our method is carefully designed. According to your valuable comments, we provide detailed feedback. Please find the special responses to your comments below.\n\n\n**Q1**: Presentation issues:\n> \"The presentation is at times unclear (see the questions section). I found it quite hard to read and had to spend some time understanding their methodology.\"\n\n***Ans for Q1):*** \nInspired by your valuable comments, we have made the following improvements/explanations:\n\n**a)** We have adjusted the misleading Table 1,2/4,5, and the specific modifications will be explained in detail in ***Ans for Q6)***.\n\n**b)** We have redrawn the methodology framework diagram (Figure 1) and included it in the main text instead of the appendix.\n\n**c)** We have made adjustments to the content of the **Method** section in the main text and highlighted the specific changes in blue. We would appreciate it if you could find some time to review our revised version paper, which incorporates these changes.\n\nSpecifically, in order to enhance the clarity of the methodology, we have reorganized the **Method** section. We now start by introducing the core framework of the path patching method and have added an introduction to path patching in Sec.3.1:\n\n\n*Path patching is a method for localizing the key components of LLMs for accomplishing specific tasks or behaviors. The term \"behavior\" is defined within the context of input-output pairs on a specific dataset, thereby effectively reflecting the model's specific capabilities. To assess the significance of a model component for a particular behavior, a causal intervention is implemented to interfere with the behavior. Assuming that the component holds importance, the causal effect on the model's specific ability should be notable. The causal intervention applied to the model component is achieved through the creation of appropriate reference examples (REs) and counterfactual examples (CEs), which solely vary in their capacity to trigger the model's specific behavior.*\n\nand supplement the challenges of interpreting LLMs' CoT reasoning ability using path patching in Sec.3.1:\n\n*In this study, we aim to localize the key attention head within the model responsible for CoT reasoning. However, we are faced with two primary challenges: i) The complexity arises from the diverse range of abilities that LLMs possess to achieve CoT reasoning, including numerical computation, knowledge retrieval, and logical reasoning, making it challenging to design reference examples (REs) that are accurately paired with counterfactual examples (CEs) differing only in their ability to trigger the CoT reasoning behavior of LLMs. ii) Additionally, the extensive potential word in the LLMs' output results in a sparsity of causal effects, as the words directly related to CoT reasoning are significantly limited in number. To overcome these challenges, we propose an innovative approach for constructing paired REs and CEs, accompanied by a word-of-interest (WoI) normalization method aimed at addressing the issue of sparse causal effects.*\n\nThis sets the stage for the subsequent discussion of our method's innovations, namely the $x_r, x_c$ construction, and causal effect evaluation metric design."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4909/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700193027114,
                "cdate": 1700193027114,
                "tmdate": 1700193055854,
                "mdate": 1700193055854,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "EBupeh5wSw",
                "forum": "b2XfOm3RJa",
                "replyto": "cJG20XUIbV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4909/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4909/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Welcome for more discussions (wEdy)"
                    },
                    "comment": {
                        "value": "Dear reviewer #wEdy,\n\nThanks for your valuable time in reviewing and constructive comments, according to which we have tried our best to answer the questions and carefully revise the paper. Here is a **summary of our response** for your convenience:\n\n- (1) **Unclear presentation**: Following you constructive comments, we have reorganized our **method section** and provided additional explanation about path patching. \n- (2) **Literature issues**: Following your constructive comments, we have discussed related works based on causal abstraction. And we also add these discussions into our revision to enhance our work.\n- (3) **A single in-context learning technique is analyzed**: We agree with your valuable suggestions, we have supplemented the results of experiments that used zero-shot CoT to construct $x_r,x_c$, and experiments that used other in-context learning methods to construct $x_c$. In addition, an interesting experiment is conducted (please see our ***Answer for Q3***).\n- (4) **Improve the methodology and empty Appendix B**:  Thanks for your valuable advice, we have redrawn our figure of method overview and moved the figure to Section 2.\n- (5) **empty Appendix B**: Thanks for your careful review, we have fixed the typo.\n- (6) **Table issues**: For our misleading Table 1,2/3,4, we have adjusted the content of the tables and added the caption to help better understand our data.\n- (7) **No mention of a specific dataset**: According to your valuable device, we have provided more explanations to illustrate the relationship between the generated irrelevant sentences and the CoT reasoning datasets we used.\n\nWe humbly hope our response has addressed your concerns. If you have any additional concerns or comments that we may have missed in our responses, we would be most grateful for any further feedback from you to help us further enhance our work.\n\n\nBest regards\n\nAuthors of #4909"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4909/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700389672077,
                "cdate": 1700389672077,
                "tmdate": 1700389908816,
                "mdate": 1700389908816,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Y2bf7gPhjk",
                "forum": "b2XfOm3RJa",
                "replyto": "cJG20XUIbV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4909/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4909/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Window for discussion and revision is closing"
                    },
                    "comment": {
                        "value": "Dear Reviewer #wEdy,\n\nThanks a lot for your time in reviewing and insightful comments, according to which we have carefully revised the paper to answer the questions. We sincerely understand you\u2019re busy. But since the discussion due is approaching, would you mind checking the response and revision to confirm where you have any further questions?\n\nWe are looking forward to your reply and are happy to answer your further questions.\n\nBest regards\n\nAuthors of #4909"
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4909/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700549083376,
                "cdate": 1700549083376,
                "tmdate": 1700549083376,
                "mdate": 1700549083376,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4b2oKAfBKI",
                "forum": "b2XfOm3RJa",
                "replyto": "cJG20XUIbV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4909/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4909/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Window for discussion is closing in last 21 hours"
                    },
                    "comment": {
                        "value": "Dear Reviewer wEdy,\n\nThanks very much for your great efforts in reviewing and valuable comments. The author's discussion will end in the last 21 hours. At this final moment, we sincerely appreciate it if you could check our responses including **Response to Reviewer wEdy [part 1/3]**, **Response to Reviewer wEdy [part 2/3]**, and **Response to Reviewer wEdy [part 3/3]**. And our response to other Reviewers may also provide you with more information.\n\nIf you have any further concerns, we will instantly respond to you at this final moment. And we sincerely appreciate that you could consider improving your score if there is no further concern. Your support for a novel discovery in its earlier stage is very important and may inspire more new findings in LLMs interpretability.\n\nBest regards and thanks,\n\nAuthors of #4909"
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4909/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700623736956,
                "cdate": 1700623736956,
                "tmdate": 1700623757233,
                "mdate": 1700623757233,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "r51B6w11NM",
                "forum": "b2XfOm3RJa",
                "replyto": "cJG20XUIbV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4909/Reviewer_wEdy"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4909/Reviewer_wEdy"
                ],
                "content": {
                    "title": {
                        "value": "Reviewer answer"
                    },
                    "comment": {
                        "value": "I thank the authors for their thorough effort in clarifying all points and presenting a significantly updated revision of their paper. \n\nSome points: Figure 1 looks much clearer now, but I would advise the authors to use slightly darker colors from green and red, since text such as \"Hydrogen is the first element and has an atomic number of one. To square a number, you multiply it by itself. The Spice Girls has five members\" is hard to read.             \nI appreciated the inclusion of Figure 6 related to the knockout method, which clarifies further their methodology (I think this figure is new; I cannot access the original paper version to compare under \"Revisions\").\nAlso, a number of improvements have been made throughout the paper, and new experiments have been carried out.\n\nAll in all, I am satisfied that this is a substantially improved version of the paper. It would now be a solid 7 (from an original 6); unfortunately, ICLR does not allow scores of 7 (only 6 and 8), so I am keeping my score unchanged."
                    }
                },
                "number": 27,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4909/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700733416130,
                "cdate": 1700733416130,
                "tmdate": 1700733581552,
                "mdate": 1700733581552,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]