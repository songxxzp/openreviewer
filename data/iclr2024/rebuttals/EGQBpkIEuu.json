[
    {
        "title": "Revisiting Data Augmentation in Deep Reinforcement Learning"
    },
    {
        "review": {
            "id": "qJ4wa2Mafw",
            "forum": "EGQBpkIEuu",
            "replyto": "EGQBpkIEuu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5345/Reviewer_pcif"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5345/Reviewer_pcif"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents an analysis of various data augmentation techniques applied in image-based deep reinforcement learning.  \nThe authors classify these techniques into two primary categories: implicit and explicit data augmentation where implicit DA consists of directly applying transformations on the observations during training, oppositely to explicit DA where transformations are applied on samples used for an auxiliary regularization term.  \nFollowing this classification, the authors then put forth a generalized formulation for integrating DA within actor-critic methods, covering both the previously mentioned implicit and explicit DA techniques. One of the paper's most salient contributions lies in the analysis of how DA impacts the variance associated with the actor and critic losses.  \nThe insights drawn from this analysis guide the authors towards designing a principled data augmentation strategy.  \nA notable aspect of their proposed method is the integration of a computer vision regularization called tangent prop into the learning objective for the critic. This modification is suggested to potentially improve the stability and efficiency of the learning process.  \nThe experimental segment of the paper includes tests on the DeepMind control suite benchmark to evaluate the proposed data augmentation scheme. Additionally, the authors test the generalization capabilities of their method using the dmcontrol-generalization-benchmark."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "**Unified Framework:** A significant strength of this paper is its development of a unified framework that facilitates the direct application of data augmentation in RL. This holistic approach serves to streamline various data augmentation techniques within the context of reinforcement learning.\n\n **Analysis on KL regularization direction:** The paper offers a commendable examination regarding the optimal direction for applying the KL regularization term. While the theoretical insights are appreciated, an empirical evaluation would have added further depth to this assessment.\n\n **Variance Analysis in SAC:** The comprehensive analysis of the variance associated with the different terms used in SAC when DA is applied is insightful and sparks intriguing questions that could guide future research in the domain.\n\n**Tangent Prop regularization:** Although the tangent prop regularization has its roots in the computer vision world and might not be a groundbreaking addition to RL, its introduction in this context is promising. The authors successfully empirically demonstrate its potential benefits."
                },
                "weaknesses": {
                    "value": "**Oversimplified Unified Framework:** While the proposed unified framework is a step forward, it seems overly simplified. Specifically, the representation of implicit data augmentation as solely reliant on KL regularization seems limiting. Other methods like SODA [1] and SGQN [2] incorporate data augmentation using auxiliary objectives, drawing parallels with techniques in self-supervised learning. Such nuances should have been addressed for a more comprehensive overview.\n\n**Excessive Reference to Appendix:** The frequent deference to the appendix, especially in sections 5.2 and 6, detracts from the paper's flow. Readers are forced into continuous back-and-forth toggling, making the narrative harder to follow.\n\n**Unverified Claims:** The assertion that introducing image transformations in the target doesn't lead to significant variance seems unsupported, particularly when considering table 5 in the appendix. Such claims should ideally be substantiated with empirical evidence.\n\n**Comparison with RAD and SVEA:** In the original paper, SVEA, which does not employ data augmentation on the target, significantly outperforms RAD. A comment or analysis from the authors on this discrepancy would have been informative.\n\n**Unclear Table Presentation:** Table 4 in the appendix is a bit perplexing. The logic behind measuring variance in relation to augmentations not employed during SVEA training (such as overlay, randomconv, rotation, or blur) is unclear. Additionally, the specific \"DA\" used here is not explicitly mentioned, leading to further confusion.\n\n**Novelty and Contribution:** A major point of contention is the actual novelty and impact of the paper's contributions. Its principal contribution seems to revolve around the unified framework and the introduction of tangent prop in RL. Yet, the proposed principled algorithm appears to be a minor extension of DrQ by merely incorporating a KL term. Based on figures 1 and 2, the tangible benefits seem largely attributed to the tangent prop, emphasizing its crucial role. While integrating this regularization in RL is commendable, its novelty in the broader context appears somewhat limited.\n \n**Minor comments**\n Bolding in Tables: The rationale behind the use of bolding in tables (specifically tables 4 and 5) needs clarification. \nLimitations are located in the appendix these and should be explicitly referenced in the main body.\nMissing legend in figure 7, what is the green line?"
                },
                "questions": {
                    "value": "* Based on the data from Table 2, RAD+ exhibits lower variance on the critic loss compared to DrQ. Yet, it underperforms DrQ in terms of overall results. Given that both methods utilize the SAC algorithm, this seems to counter the claim made by the authors that reducing critic loss variance leads to more stabilized training. Can the authors provide insights or explanations for this observation?\n\n* My understanding is that the only discernible difference between the authors' method and DrQ+KL is the introduction of the tangent prop regularization. Drawing from Table 1, it appears that this regularization is the primary factor that reduces the critic loss variance, rather than the authors' generic algorithm. Could the authors comment on the role and impact of tangent prop in this context?\n\n* The authors assert that employing random convolution in SVEA doesn't induce significant variance. However, the empirical results from Table 5 seem to suggest otherwise. Could the authors clarify?\n\n* What drives the intuition that the KL term assists in reducing the variance of the target critic?\n\nFor a more comprehensive analysis, would it be feasible for the authors to include SVEA in the experiments of the initial experimental setup?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5345/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5345/Reviewer_pcif",
                        "ICLR.cc/2024/Conference/Submission5345/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5345/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698507957467,
            "cdate": 1698507957467,
            "tmdate": 1700639084493,
            "mdate": 1700639084493,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "49waWqXufH",
                "forum": "EGQBpkIEuu",
                "replyto": "qJ4wa2Mafw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5345/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5345/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official reply to Reviewer pcif [1]"
                    },
                    "comment": {
                        "value": "Thank you for noting the generality of our proposition and providing constructive suggestions.\nBelow, we provide our answers to the weaknesses and questions.\n1. **W1:** *Oversimplified Unified Framework*\n    \nPlease check point 3 in our Meta Reply.\n    \n2. **W2:** *Excessive Reference to Appendix.*\n    \nWe thank the reviewer to have read our paper so carefully.\nWe admit that it is distracting to frequently check the appendix and we apologize for this.\nAs the reviewer may understand, due to the page limit, all the information cannot be included in the main text.\nWe tried to maintain the coherence of our presentation as much as possible. The references to the appendix mainly provides the theoretical proofs, further empirical evidence, additional experiments, and other minor details.\nWe believe that they can be skipped in a first reading.\n    \n3. **W3:** *Unverified Claims.*\n    \nNote that the expectation of the target is actually very large compared to the variance of the target. \nAt the end of the training (steps 500K), the mean of the target is more than 100 while the variance of the target is just about 1 for both cases of using complex image transformations and not. \nFrom Table 5, although we can observe a small increase in the variance of the target due to  using complex image transformation, it is still reasonable to state that this variance increase is not the dominant issue for the performance drop. \n    \nWe thank the reviewer for raising this point.\nWe updated Table 5 to include the mean of the target.\n    \n4. **W4:** *Comparison with RAD and SVEA*\n    \nBefore we compare RAD and SVEA, we need to clarify that SVEA actually uses random shift in calculating the targets and only avoids those complex image transformations in calculating the targets.\nAlthough it is removed from their public repository in the github commit (https://github.com/nicklashansen/dmcontrol-generalization-benchmark/commit/35a1eed87f72d3c9881c425fd83604c756849e56), we checked with the authors by email to confirm that the results in the paper were generated by applying random shift in calculating the targets.\n\nNow, if we compare the generalization ability evaluated in color-hard background, we should observe that RAD (random shift) $<$ RAD (random conv) $<$ SVEA (random conv).\nThe first inequality is decided by using random conv as the image transformation.\nThe second inequality is caused by avoiding using random conv in calculating the target.\n    \n5. **W5:** *Unclear Table Presentation*\n    \nThank you for raising this point. \nWe apologize for the confusion and clarified the formatting of Table 4.\nIn this table, each column corresponds to SVEA trained with one specified image transformation (DA) and each row corresponds to the cosine similarity recorded at 100k steps between the features of augmented states.\nHere, the cosine similarities between latent features of randomly shifted images are used as the baseline for comparisons.\nBy comparing elements in one column, we can observe a smaller cosine similarity between the augmented features when training SVEA with complex image transformation which indicates the invariance with respect to complex image transformation is harder to learn.\n    \n6. **W6:** *Novelty and Contribution*\n    \nPlease check point 1 in our Meta Reply.\n\n7. **W7:** *Minor comments Bolding in Tables.*\n    \nThank you for your constructive suggestions. \nWe initially used bold to highlight the values that are commented in the text.\nWe understand this formatting may lead to some confusion. \nWe have modified our paper according to the suggestions:\n- For a clear presentation, we removed bolding in Tables 1, 4, and 5 and added more explanations in the captions.\n- We included more discussions in the Limitation section and explicitly referred to it in the Conclusion section in the main text.\n- We added the missing legends for Figure 8."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5345/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700051133282,
                "cdate": 1700051133282,
                "tmdate": 1700051133282,
                "mdate": 1700051133282,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "eYT6DDChs4",
                "forum": "EGQBpkIEuu",
                "replyto": "qJ4wa2Mafw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5345/Reviewer_pcif"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5345/Reviewer_pcif"
                ],
                "content": {
                    "comment": {
                        "value": "I acknowledge the author's responses to my questions. Given this information and the reviews provided by my colleagues, I am willing to raise my score from 5 to 6."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5345/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700639005613,
                "cdate": 1700639005613,
                "tmdate": 1700639027338,
                "mdate": 1700639027338,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "DYYZ2hcImE",
            "forum": "EGQBpkIEuu",
            "replyto": "EGQBpkIEuu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5345/Reviewer_9PSY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5345/Reviewer_9PSY"
            ],
            "content": {
                "summary": {
                    "value": "This paper delves into the realm of data augmentation methods in Deep Reinforcement Learning (DRL), conducting a thorough analysis of existing techniques. The authors provide a theoretical framework to understand and compare these methods and propose a new regularization term, tangent prop, to enhance invariance. Their contributions lie in offering a theoretical understanding of data augmentation methods in DRL, suggesting how to use data augmentation in a more theoretically-driven manner, and introducing a novel regularization approach. Through extensive experiments, the authors validate their theoretical propositions."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Originality:\nIt conducts a comprehensive analysis of existing data augmentation methods, initially based on the author's introduction of assumptions regarding uncertainty in reinforcement learning and existing image-based online Deep Reinforcement Learning (DRL) data augmentation techniques. The paper establishes an integrated AC framework incorporating data augmentation. Within this framework, the mainstream data augmentation methods are analyzed and categorized into explicit and implicit regularization techniques. Qualitative and quantitative analyses of these augmentation methods are performed. Building upon these two types of regularization, the paper introduces a novel regularization technique \u2013 tangent prop \u2013 providing theoretical support for addressing uncertainty in the Critic component.\n\nQuality and Clarity:\nI think the quality of the paper is very high. Each proposition and hypothesis presented in the paper is accompanied by corresponding formulas, and the appendix contains detailed derivations of these formulas. Additionally, I find the paper to be very logically structured.\nFirst, the paper explains the process of the Data-Augmented Off-policy Actor-Critic Scheme and introduces the key hypotheses regarding the uncertainty in Q-values and policy \u03c0. The subsequent proofs and derivations are based on these definitions. The paper then elaborates on how explicit and implicit uncertainties are defined within the A-C framework and how they affect the Loss function of Actor and Critic. After providing the theoretical background, the paper proposes its own generic algorithm and provides detailed derivations and proofs.\n\nFurthermore, the paper analyzes the effects of applying different image transformations when calculating target Q-values, as well as the empirical actor/critic losses estimated under data augmentation. In implicit regularization, the author, unlike the previous SVEA method, incorporates KL divergence into the training process for policy. The paper explains and proves that, under the premise of Critic invariance, introducing KL divergence helps the model better learn the invariance of the Actor. Finally, the paper introduces the innovative concept of Tangent Propagation to further demonstrate how this newly introduced additional regularization term promotes Critic invariance.\n\nThe logic throughout the main body of the paper is very clear. It systematically proves its hypotheses about uncertainty and effectively addresses the initial problems posed in the paper.\n\nSignificance:\nThe paper holds significant value for the DRL research community. By offering a theoretical framework and introducing a novel regularization approach, the paper addresses a key challenge in DRL, enhancing the understanding and application of data augmentation techniques. The experimental validation across various environments supports the significance of the proposed methods. The findings are likely to influence future research directions, providing valuable insights for researchers and practitioners in the field."
                },
                "weaknesses": {
                    "value": "The theoretical derivation in the paper is very thorough. However, I believe the experimental section of the paper is somewhat lacking. It compares the performance with the previous statistically trained model, SVEA, providing detailed experimental data and theoretical analysis. Nevertheless, there is a lack of in-depth analysis of the shortcomings of the previous algorithms and the advantages of the proposed algorithm. Moreover, I think the algorithm should be further compared with more methods and applied in various domains to validate its generalizability.\n\nThe entire article is dedicated to theoretically proving the reliability of its algorithm, and it has obtained favorable experimental results. However, throughout the entire text, there is no discussion about the shortcomings and limitations of the algorithm, nor is there any mention of how to extend the algorithm or areas that need further exploration in the future.\n\nThe article discusses how different complex image augmentation techniques have varying impacts on the invariance of target Q-values. The paper proposes using cosine similarity measured from encoder outputs for this evaluation. It is mentioned in the article that techniques such as random addition or Gaussian blur easily achieve invariance through these transformations because they result in high cosine similarity. When using these types of image transformations in computing target values, it does not affect performance. In contrast, image transformations like random convolution or random rotation are relatively challenging to achieve invariance during the training process, as they result in lower cosine similarity. The solution proposed in the paper is to enforce this invariance by performing more updates at each training step. However, the paper does not address whether there might be potential overfitting issues under these specific conditions and how to handle them."
                },
                "questions": {
                    "value": "Refer to the weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5345/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698560216791,
            "cdate": 1698560216791,
            "tmdate": 1699636537752,
            "mdate": 1699636537752,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ReBDLudaXC",
                "forum": "EGQBpkIEuu",
                "replyto": "DYYZ2hcImE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5345/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5345/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official reply to Reviewer 9PSY"
                    },
                    "comment": {
                        "value": "Thank you for noting the quality of our work especially in originality and significance. \nWe are happy that you recognize our paper being logically clear.\nBelow, we provide an answer to the weaknesses.\n\n1. **W1:** *However, I believe the experimental section of the paper is somewhat lacking.*\n    \nTo the best of our knowledge, most recently-proposed techniques for improving generalization ability are actually orthogonal to our analysis and could be combined with our  proposed algorithm.\nHowever, if we have missed any  relevant work, we would appreciate if you could provide some specific references and we would try our best to add some additional comparative experimental results in our final version.\nAs for the limitations of our proposed algorithm, please check point 2 in our Meta Reply.\n    \n2. **W2:** *However, throughout the entire text, there is no discussion about the shortcomings and limitations of the algorithm, nor is there any mention of how to extend the algorithm or areas that need further exploration in the future.*\n    \nPlease check point 2 in our Meta Reply.\n    \n3. **W3:** *However, the paper does not address whether there might be potential overfitting issues under these specific conditions and how to handle them.*\n    \nWe did not observe this overfitting issue when we updated more.\nTo verify this point, we ran some additional experiments to provide further evidence for it.\nFirst, we can see that the training scores are similar to the evaluation scores when we update more, as shown in the figure (evaluation_training_score_for_reviewer_9PSY.png) in our new supplementary material.\nWe also ran the experiments of training the agent with more iterations when more updates are applied per iteration and the result is shown in the figure (More_iterations_and_more_updates_for_reviewer_9PSY.jpeg) in our new supplementary material. \nIn this experiment, we trained the agent with more iterations and more updates per iterations while we did not observe an overfitting issue.\n\nPlease let us know if you have any other comments or concerns!"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5345/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700050662029,
                "cdate": 1700050662029,
                "tmdate": 1700289183882,
                "mdate": 1700289183882,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Tvcc5cTZ5S",
            "forum": "EGQBpkIEuu",
            "replyto": "EGQBpkIEuu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5345/Reviewer_BeKW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5345/Reviewer_BeKW"
            ],
            "content": {
                "summary": {
                    "value": "This paper theoretically analyzes and compares the impact of current data augmentation techniques on Q-target and actor/critic loss in visual reinforcement learning. The regularization term \"tangent prop\" proposed in computer vision is also applied to reinforcement learning to learn invariant."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The theoretical analysis of the paper is sufficient, which is difficult to see in many similar works. At the same time, the experimental data are also considerable."
                },
                "weaknesses": {
                    "value": "1) The work in this paper seems to be equivalent to adding an explicit regularization to implicit regularization, is it equivalent to DrQ combined with DrAC in terms of functionality?\n2) How the proposed algorithm addresses the initial question \"Although they empirically demonstrate the effectiveness of data augmentation for improving sample efficiency or generalization, which technique should be preferred is not always clear.\". The main idea of the paper is not analyzed."
                },
                "questions": {
                    "value": "1) Please check that the two terms in the KL divergence in Eq. 8, and there seems to be an ambiguity with Eq. 4 and Eq. 9.\n2) How to prove that the distributions of $\\nu$ and $\\mu$ in Eq. 11 and Eq. 12 are the distributions of $\\hat{\\nu}$ and $\\hat{\\mu}$ satisfying Lemma 1?\n3) How to show that \"even using complex image transformations such as random convolution in the target\". We did not find an experiment in the paper that verifies this conclusion.\n4) In the experimental part, the grid search for $ \\alpha_{KL}$ and $ \\alpha_{tp}$ is too sparse, and the final choice of 0.1 leads to curiosity about the results within [0,0.1]. A similar situation occurs with the SVEA comparison experiment of 0.5 selected from {0.1, 0.5}.\n5) I'm curious about the method of determining $ \\alpha_i$, which seems to be missing the reason(s) in the paper. Does it have a specific value for each augmentation in the set, or does it choose an average weight?\n6) In Figure 2, the batch size of DrQ reproduced in the results is 256 instead of 512 in the original DrQ. The scores of DrQ will decrease under some environments when using a smaller batch size. Compared with the official results of DrQ (https://github.com/denisyarats/drq), some results of DrQ shown in Figure 9 are lower. Such as ball_in_catch , walker_walk, walk_run. Therefore, to make a fair comparison, the author should completely use the hyperparameter settings in DrQ to reproduce, or use the scores in DrQ."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5345/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5345/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5345/Reviewer_BeKW"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5345/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698652650455,
            "cdate": 1698652650455,
            "tmdate": 1699636537664,
            "mdate": 1699636537664,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jb3jrDB828",
                "forum": "EGQBpkIEuu",
                "replyto": "Tvcc5cTZ5S",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5345/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5345/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official reply to Reviewer BeKW [1]"
                    },
                    "comment": {
                        "value": "Thank you for summarizing our work and highlighting that our theoretical analysis is rare and valuable compared to many similar work.\nBelow, we provide our answer to the weaknesses and questions.\n\n1. **W1:** *The work in this paper seems to be equivalent to adding an explicit regularization to implicit regularization, is it equivalent to DrQ combined with DrAC in terms of functionality?*\n\nPlease check point 1 in our Meta Reply.\n    \nRoughly speaking, our method follows the idea from DrQ to apply invariant image transformations in both the training of the actor and critic, complements it with a KL regularization term in the actor loss and introduces a new regularization term called tangent prop in the critic loss.\nHowever, there are three key differences between our method and the simple combination of DrQ and DrAC:\n- Our method introduces a new regularization term called tangent prop which is novel in DRL.\n- The KL regularization term in our method is not exactly the KL regularization term used in DrAC. \nThe KL regularization term $D_{KL}[\\pi_\\theta(\\cdot|s),\\pi_\\theta(\\cdot|f_\\mu(s))]$ used in DrAC only augments the state on one side while the KL regularization term $D_{KL}[\\pi_\\theta(\\cdot|f_\\eta(s),\\pi_\\theta(\\cdot|f_\\mu(s))]$ in our method augments the states on both sides.\nThe regularization term in our method is somehow equivalent to using an averaged policy as the target, which may be preferred.\nThe detailed analysis is presented at the end of Section 4.2 and the experimental comparisons between these two KL regularizations are shown in Figure 2. The two lines which are respectively labeled as KL\\_fix\\_target and KL correspond to the KL from DrAC and the KL in our method.\n- Our method is compatible with using more than one type of image transformation in the training which is required to achieve state-of-the-art performance in both sample efficiency and generalization ability. \nMoreover, we also provide an analysis and suggest a criterion for the usage of complex image transformations.\n\n2. **W2:** *How the proposed algorithm addresses the initial question \"Although they empirically demonstrate the effectiveness of data augmentation for improving sample efficiency or generalization, which technique should be preferred is not always clear.\".*\n    \nThe goal of this work was to analyze existing methods and make recommendations on how to use data augmentations in a more principled way in DRL.\nWe start our analysis by connecting the implicit with explicit regularization and indicate some preferences about them. \nFor example, the actor loss in implicit regularization can only enforce the invariance in the actor under the condition that the invariance in the critic has been learned.\nThis motivates our inclusion of an explicit KL regularization term in the final actor loss of our method.\nNote that we justify most of our recommendations in Section 5.2.\nFor example, we recommend applying complex image transformations in calculating the target Q values and provide a criterion for judging if an image transformation should be considered as complex or not.\nAll in all, we tried to judge different techniques with our theoretical analysis and validate our propositions empirically.\n\n3. **Q1:** *Please check that the two terms in the KL divergence in Eq. 8, and there seems to be an ambiguity with Eq. 4 and Eq. 9.*\n    \nThere is actually no ambiguity here. \nThe KL term in Equation 8 is different from the KL terms in Equation 4 and Equation 9, as listed below:\n- Only the state in one side of the KL term in Equation 8 is augmented while the states in both sides of the KL term in Equation 4/Equation 9 are augmented.\nThe differences between them are explained in our answer 1(b) to W1 above.\n- The directions of the KL regularization terms are different.\nIn Equation 8, the stop gradient is attached to the $\\theta$ on the right hand side.\nConsidering that KL is asymmetric, it is necessary to discuss the difference which is presented in the second paragraph below Proposition 4.1.\nIn conclusion, we found that using the stop gradient in the first term is better and including an explicit KL term in the actor loss is preferred.\n    \n4. **Q2:** *How to prove that the distributions of $\\nu$ and $\\mu$ in Eq. 11 and Eq. 12 are the distributions of $\\hat\\nu$ and $\\hat\\mu$ satisfying Lemma 1?*\n\n    Note that in Eqns. 11 and 12, the distributions of $\\nu$ and $\\mu$ are not restricted. They are usually set to be uniform distributions, but other distributions could be chosen depending on the application domain.\n    \n    Lemma 1 simply states that for any distributions $\\nu$, implicit regularization in the critic with specifically chosen $\\hat\\nu$ and $\\hat\\mu$ (see Proof of Lemma 1 for details) is equivalent to explicit regularization in the critic with $\\nu$."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5345/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700049981948,
                "cdate": 1700049981948,
                "tmdate": 1700050252283,
                "mdate": 1700050252283,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zFUgLnmHtj",
                "forum": "EGQBpkIEuu",
                "replyto": "Tvcc5cTZ5S",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5345/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5345/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official reply to Reviewer BeKW [2]"
                    },
                    "comment": {
                        "value": "5. **Q3:** *How to show that \"even using complex image transformations such as random convolution in the target\". We did not find an experiment in the paper that verifies this conclusion.*\n    \nWe apologize for the ambiguity here. The experimental results for this point are shown in Appendix F.  \nWe modified our paper to explicitly indicate that all observations about using complex image transformations in calculating the targets are presented in Appendix F.\nMoreover, we clarified the descriptions of Tables 4 and 5.\n    \n6. **Q4:** *In the experimental part, the grid search for $\\alpha_{KL}$ and $\\alpha_{tp}$\n     is too sparse, and the final choice of 0.1 leads to curiosity about the results within [0,0.1]. A similar situation occurs with the SVEA comparison experiment of 0.5 selected from {0.1, 0.5}.*\n    \nConsidering that our main goal was to provide some justifications and recommendations on techniques used in data augmentation methods in DRL, we did not focus on getting the best possible score on this DMControl benchmark.\nHowever, we can indeed perform a more thorough grid search.\nDue to the time constraint in this rebuttal phase, we will update the results in the final version.\n    \n7. **Q5:** *I'm curious about the method of determining $\\alpha_i$, which seems to be missing the reason(s) in the paper. Does it have a specific value for each augmentation in the set, or does it choose an average weight?*\n    \nWe simply use an average weight for different image transformations, as it was done in SVEA.\n    \n8. **Q6:** *In Figure 2, the batch size of DrQ reproduced in the results is 256 instead\n    of 512 in the original DrQ.*\n    \nWe ran the experiments of DrQ and our method in the environments listed in the question with batch size of 512:\n|  Envs  | DrQ(batch_size=256) | DrQ(batch_size=512) | Ours(batch_size=256) | Ours(batch_size=512)\n|-----------------|-------------------|----------------|----------------|----------------|\n| Ball in cup catch | 947.38+-41.76  |962.08+-19.54 |974.06+-5.09|969.96+-8.30\n| Walker walk | 591.25+-433.38 | 740.67+-343.23 |925.62+-35.03 |903.11+-56.54\n| Walker run | 378.29+-184.58 | 350.71+-210.66 |496.06+-26.33 |569.17+-63.75\n\nWe can see that\n1. Our method is insensitive to the choice of batch size.\n2. Without retuning the hyperparameter, our method always matches or outperforms DrQ in these environments with these two choices of batch size.\n3. Considering the modest computational resources we have and the huge increase in computational cost due to usage of batch size = 512, it is fair to compare sample efficiency of all the methods with the batch size of 256.\n\nPlease let us know if you have any other comments or concerns!"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5345/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700050059005,
                "cdate": 1700050059005,
                "tmdate": 1700667156669,
                "mdate": 1700667156669,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PwCryD1Ewo",
                "forum": "EGQBpkIEuu",
                "replyto": "kl4TTZcc6p",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5345/Reviewer_BeKW"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5345/Reviewer_BeKW"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the author's responses to my comments. Based on the comments of all reviewers and the author's reply, I decided to keep my score."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5345/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700720714614,
                "cdate": 1700720714614,
                "tmdate": 1700720714614,
                "mdate": 1700720714614,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "s7vSCVYzqY",
            "forum": "EGQBpkIEuu",
            "replyto": "EGQBpkIEuu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5345/Reviewer_GnUL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5345/Reviewer_GnUL"
            ],
            "content": {
                "summary": {
                    "value": "The paper revisits state-of-the-art data augmentation methods in Deep Reinforcement Learning (DRL). It offers a theoretical analysis to understand and compare different existing methods, providing recommendations on how to exploit data augmentation in a more theoretically-motivated way. The paper introduces a novel regularization term called tangent prop and validates the propositions by evaluating the method in DeepMind control tasks. The experimental results are to be released after publication, and the paper also includes theoretical proofs to support the proposed methods and analysis. However, it is important to note that the paper is highly technical and may be challenging for readers without a strong background in DRL and computer vision to understand. Additionally, it focuses on image-based DRL and does not address the ethical implications of using data augmentation techniques in DRL."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.  The paper provides a comprehensive analysis of existing data augmentation techniques in DRL and offers recommendations on how to use them more effectively.\n2.  The authors introduce a novel regularization term called tangent prop and demonstrate its state-of-the-art performance in various environments.\n3.  The experimental results are presented in a clear and concise manner, and the code with comments on how to reproduce the results will be released after publication.\n4.  The paper provides theoretical proofs to support the proposed methods and analysis."
                },
                "weaknesses": {
                    "value": "1. More insights into the limitations and potential failures of the proposed method should be discussed. This would provide a more balanced perspective and help readers better understand the practical considerations when applying the proposed approach.\n\n2. Further analysis and comparisons with a wider range of existing techniques should be conducted to showcase the advantages and limitations of the proposed method in different scenarios. This would provide a more comprehensive view of its effectiveness and contribution to the field."
                },
                "questions": {
                    "value": "NA"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5345/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699450254202,
            "cdate": 1699450254202,
            "tmdate": 1699636537559,
            "mdate": 1699636537559,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "T3OX8wpWZe",
                "forum": "EGQBpkIEuu",
                "replyto": "s7vSCVYzqY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5345/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5345/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official reply to Reviewer GnUL"
                    },
                    "comment": {
                        "value": "Thank you for indicating our contributions of comprehensive theoretical analysis and concise experimental validations.\nBelow, we provide an answer to the points corresponding to the weaknesses.\n\n\n1. **W1:** *More insights into the limitations and potential failures of the proposed method should be discussed.*\n    \nPlease check point 2 in our Meta Reply.\n\n2. **W2:** *Further analysis and comparisons with a wider range of existing techniques should be conducted to showcase the advantages and limitations of the proposed method in different scenarios.*\n\nPlease check point 3 in our Meta Reply.\n\nPlease let us know if you have any other comments or concerns!"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5345/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700049400451,
                "cdate": 1700049400451,
                "tmdate": 1700049400451,
                "mdate": 1700049400451,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]