[
    {
        "title": "Translating Labels to Solve Annotation Mismatches Across Object Detection Datasets"
    },
    {
        "review": {
            "id": "YyGob2HXfk",
            "forum": "ChHx5ORqF0",
            "replyto": "ChHx5ORqF0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission334/Reviewer_2kAn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission334/Reviewer_2kAn"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors study the problem of addressing bounding-box annotation mismatches across different datasets. For this end, they propose training a \"translator\" network that converts the bounding box annotations in the source dataset into the target dataset."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ Annotation mismatch across datasets is a severe problem in OD.\n+ Good results."
                },
                "weaknesses": {
                    "value": "1. I had problem following the scope of the paper.\n\n1.1. The paper introduces a very general alignment setting in Introduction + Figure 1, which is not matched with what is really performed. I had to make several loops between Method & Introduction + Figure 1 to understand what is going on.\n\n1.2. \"1) We consider scenarios where the class label spaces are matched but the annotation protocols are different, therefore, leading to annotation mismatches (see Section 3). 2) We optimize the target performances, while multi-dataset detection optimizes the average performances of all datasets considered.\" => The authors should provide convincing arguments + results on why this narrowed-down scope of the mismatch problem is significant.\n\n1.3. \"In this work, we assume that (1) the class labels are the same between the source and the target labeling functions and (2) the source labeling function either detects or over-detects all the objects specified in the target labeling function.\" => Again, the validity of these assumptions should be justified.\n\n1.4. \"We optimize the target performances, while multi-dataset detection optimizes the average performances of all datasets considered.\" => Why is it not better to have a multi-dataset version?\n\n1.5. \"label translation\" is misleading. With the general coverage in the Introduction, I expected language translation to be performed in the method. \"Recall from our main assumption that label translation does not require generating a correct class label, but only determining whether the object should be labeled as well as the corresponding bounding box.\" => I have difficulty calling this translation.\n\n1.6. Section 3: It would be worthwile to extend this section with analyses. As such, it is rather weak.\n\n2. Experimental evaluation is limited. It is not clear why YOLOv3 is chosen, compared to its newer versions or other one-stage detectors?\n\n3. Most importantly, as bbox localization changes through the proposed method, the paper should quantify localization directly and evaluate performance with such a measure. I would recommend the localization term of the Localization Recall Precision metric.\n\n\nMinor comments:\n- Fig 1 left: Cyclist => rider for the MVD dataset.\n- Introduction: \"This work characterizes four types of annotation mismatches,\" => It would be nice to briefly summarize here what these four types are.\n- Figure 3: The figure gives a feeling that the 'cyclist' label belongs to both datasets, which makes it difficult to understand what is being translated into what.\n\n**After the rebuttal**\n\nThe authors have been able to partially address my concerns about the significance of the taxonomy and the experimental evaluation. For taxonomy to be considered a contribution, I expect more analyses & justifications. The experiments with a SOTA model should also be more comprehensive as YOLOv8-nano is hardly a SOTA model. However, the authors did the best of what could be done in a short amount of time and I find the extended discussion on the taxonomy and the new results promising. Therefore, I would like to increase my original recommendation."
                },
                "questions": {
                    "value": "See Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission334/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission334/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission334/Reviewer_2kAn"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission334/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698053809444,
            "cdate": 1698053809444,
            "tmdate": 1700734038228,
            "mdate": 1700734038228,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "3NHXqCjZum",
                "forum": "ChHx5ORqF0",
                "replyto": "YyGob2HXfk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission334/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission334/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 2kAn"
                    },
                    "comment": {
                        "value": "$\\textbf{Dear Reviewer 2kAn}$\n\nThank you for your positive comments regarding the good results presented in the paper.\n\nIn the following, we answer the reviewer's concern with the scope of the paper:\n1. **Confusion with the introduction and Figure 1**: We introduce the label translation problem as a general and prevalent type of mismatches among object detection datasets. While resolving arbitrary label mismatches is extremely non-trivial, we consider our streamlined version of addressing specifically bounding box mismatches as the necessary first step in this novel line of research. Fortunately, our metric of concern (target performance) provides strong evidence that solving the simpler problem remains effective. We would appreciate if the reviewer could expand on any points that they feel we haven't addressed.\n2. **Narrowing down the scope of the mismatch problem**: \n    - We argue that optimizing only the target dataset performance is not narrowing down from multi-dataset object detection. Instead, they have distinct values depending on the application. Optimizing target dataset performance is particularly important when there is a specific target domain of interest, such as in Sim2Real. The performance on simulated data is relatively unimportant and only serves the purpose of improving target performance on real-world data.\n    - While we narrowed down the mismatch problem to a shared class label space, LGPL can handle the setting when two datasets have only partially overlapped categories, by translating the overlapped categories and leaving the other as is. \n3. **Assumptions in Section 4.1**: The assumptions are made based on the observation from seven datasets surveyed in Section 3. \n    - To further justify the second point \u201cthe source labeling function either detects or over-detects all the objects specified in the target labeling function\u201d, we manually collect the gold translated labels and perform TIDE analysis to understand the types of translation required (See the details in Section 5.1 and Section A.3). The results in Table 9 show the low value of \u201cMissed error\u201d and high value of \u201cBackground error\u201d, which supports the need to fix background errors.\n4. **A multi-dataset version**: As pointed out in the second point (`Narrowing down the scope of the mismatch problem`), performing multi-dataset object detection has distinct application values that differ from our goals in this work.\n5. **The term \"label translation\"**: We thank the reviewer for the thoughtful concerns. The confusion with language translation is unfortunate, and we are keen to hear suggestions on how this might be mitigated in future.\n6. **The role of Section 3**: Section 3 introduces the concept of annotation mismatches and categorizes them into four common types. While it is challenging to quantitatively measure how severe annotation mismatches are between each pair of datasets, we manually collect gold translation labels and quantify the TIDE errors in Section 5.1 and Appendix A.3.\n\n    \n\n\n**Empirical evaluation**\n\nThe reviewer asked why we chose YOLOv3 instead of newer variants or one-stage detectors. In fact, we specifically choose three different types of object detectors: two-stage object detector (Faster-RCNN), one-stage object detector (YOLOv3), and transformer-based object detector (Def-DETR). \n\nWe choose YOLOv3 out of three YOLO variants in MMdetection due to its stable training. But in any case, we feel that we have adequately demonstrated the robustness of our technique to different object detectors. If the reviewer would like to see the performance on another detector, please kindly provide us with a reference and we will aim to incorporate these results into a later version.\n\nWe would also want to emphasize that we have tested on four different scenarios with three different architectures as well as ablations with model-centric alternatives such as domain adaptation and foundation model alternatives such as SAM.\n\n**Suggestions of LRP metric**\n\nWe appreciate the reviewer's great suggestion of the LRP metric and we will include the LRP metric in our camera-ready version.\n\n**Minor comments**\n\nWe appreciate the feedback and have revised the paper submission accordingly."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission334/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700084001347,
                "cdate": 1700084001347,
                "tmdate": 1700084001347,
                "mdate": 1700084001347,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "p7MyiwkRmf",
                "forum": "ChHx5ORqF0",
                "replyto": "3NHXqCjZum",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission334/Reviewer_2kAn"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission334/Reviewer_2kAn"
                ],
                "content": {
                    "title": {
                        "value": "Re: Response to Reviewer 2kAn"
                    },
                    "comment": {
                        "value": "Dear authors,\n\nThank you for the detailed response. \n\n* Label translation: I would recommend \"label transfer\" as an alternative. \"Label translation\" is also inappropriate from a bbox transformation point of view in that you are not simply translating the boxes, you are performing a transformation on them.\n\n* If you can resolve this ambiguity, it will be easier to follow the paper, without making multiple loops between the Method and Introduction sections.\n\n* \"See the details in Section 5.1 and Section A.3\" => Please make these cross-references in places where you are stating your assumptions. Without these, a reader cannot follow the validity of your assumptions.\n\n* Difficulty of justifying the taxonomy: I understand that this might be difficult. As it is, without concrete analyses and justifications, it is not possible to see the extent/severeness of these problems discussed in Section 3. As such, I find the taxonomy contribution claim of the paper not substantiated.\n\n* YOLO v3 results: Your current results suggest that the improvements with strong detectors (Def-DETR and Faster R-CNN) are less. Therefore, one wonders how your method would have performed with a stronger/newer one-stage detector. I suspect that the improvement will be less. I don't find the use of MMdetection nor stable training a good excuse. People, including me, do use other frameworks and can train newer detectors in a stable manner.\n\n* I also would like to see a SOTA comparison. Def-DETR and Faster R-CNN are strong detectors, but they are not SOTA. If bbox translation does not yield SOTA results (improvements with a SOTA method), then it is not clear why it would be needed. \n\nThank you,\nBest"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission334/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700137682930,
                "cdate": 1700137682930,
                "tmdate": 1700137682930,
                "mdate": 1700137682930,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dnHfw9UGvX",
                "forum": "ChHx5ORqF0",
                "replyto": "K17JySuUeV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission334/Reviewer_2kAn"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission334/Reviewer_2kAn"
                ],
                "content": {
                    "title": {
                        "value": "Re: Official Comment by Authors"
                    },
                    "comment": {
                        "value": "Dear authors,\n\nThank you for the comprehensive and detailed response. Your response does partially address my concerns about the taxonomy and the evaluation with the SOTA models. You did the best of what could be done in a short amount of time, thank you. I will increase my recommendation.\n\nBest"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission334/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700733903821,
                "cdate": 1700733903821,
                "tmdate": 1700733903821,
                "mdate": 1700733903821,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "llKtyDRqPI",
            "forum": "ChHx5ORqF0",
            "replyto": "ChHx5ORqF0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission334/Reviewer_MRBF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission334/Reviewer_MRBF"
            ],
            "content": {
                "summary": {
                    "value": "This paper attempts to tackle annotation mismatches in multi-dataset training for object detectors. It points out that different datasets may have different definitions or annotation protocols to the same category, leading to annotation mismatches. Four kinds of annotation mismatches are highlighted, i.e. Class semantics, Annotation instructions, Human-machine misalignment, Cross-modality labels. To address those mismatches, it proposes to train a label translator to translate annotations of source datasets to the target datasets. The label translator follows the design of two-stage detector (e.g. faster rcnn) with a RPN and RoI head. During the training of the translator, the RPN is trained with source datasets and generates source-style boxes. The RoI head converts the generated source-style boxes to the target-style. During inference, the RoI head converts ground-truth annotations of the source dataset (both boxes and class labels) to the target-style. Experiments shows that the proposed label translator improves various detectors on certain pairs of datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The task of annotation mismatches for the same category is interesting. Based on my knowledge, studies of multiple dataset training seldom explore annotation issues for the same category.\n\n2. It's interesting to leverage RoI head to covert annotations of the source datasets to the target datasets.\n\n3. The proposed translator achieves good performance on various detectors and datasets.\n\n4. The paper is well written and easy to follow."
                },
                "weaknesses": {
                    "value": "1. The assumption is too strong that the source and the target datasets have the same categories. In multi-dataset training, it is a common case that different datasets not only have overlapped categories but also have unique categories. I believe it's more important to handle unique categories that are annotated in one dataset but not in the others. That's because unannotated objects in the other datasets will be regarded as background by mistake. It's better to have a solution to handle both overlapped and unique categories.\n\n2. RoI head is able to refine various region proposals to the target-style boxes. Is it necessary to train a RPN as box generator that generates source-style boxes? It's better to compare the proposed solution with a only-RPN solution. That is, train a RPN (probably together with the two-stage architecture) with the target dataset and use it on different source datasets so that we don't need to train RPNs for different pairs of source-target datasets.\n\n3. I'm not sure how to use the label translator to handle target datasets with more than one categories. Based on the experiments, it seems that the label translator only handles one category ('cyclist' in  Synscapes, Cityscapes, MVD, nuImages and Waymo). Probably, with the increase of categories, the label translator may introduce the noisy pseudo labels and negatively impact the performance.\n\n4. Evaluations on widely used object detection datasets are missing. How about COCO, Objects365, and OpenImages? It seems that the experiments only include detection datasets for driving scenarios.\n\n5. Based on Sect. 5.3, translation-mAP is not strongly correlated with downstream-mAP, and visually analyzing the translated labels is insufficient to judge label translation. Why is translation-mAP reported?\n\n6. The proposed label translation seem to require training between every source and target datasets, which is costly to scale up."
                },
                "questions": {
                    "value": "See Weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission334/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698443961899,
            "cdate": 1698443961899,
            "tmdate": 1699635960237,
            "mdate": 1699635960237,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "IcTwlB9luz",
                "forum": "ChHx5ORqF0",
                "replyto": "llKtyDRqPI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission334/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission334/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer MRBF"
                    },
                    "comment": {
                        "value": "$\\textbf{Dear Reviewer MRBF}$\n\nThank you for your positive comments regarding the interesting research problem, the proposed usage of RoI head, and the good performance on various detectors and datasets.\n\n**Handling unique categories that are in one dataset but not in the others**\n\n\nThis is a great point. Indeed, LGPL can handle the setting when two datasets have only partially overlapped categories, by translating the overlapped categories and leaving the others as is.\n\nTo specifically translate unique classes, we note that LGPL is model/training-agnostic and can be applied with any other data augmentation or training algorithm. For example, if the target dataset has unique categories, we can use any off-the-shelf semi-supervised learning to learn the unique categories. If the source dataset has some unique categories, we can just drop those labels.\n\n\n**Necessity of training a RPN that generates source-style boxes**\n\nWe interpret the reviewer's suggestion as: \"Using the RPN trained on the target dataset so that the training/inference is not tied with any source dataset.\"\n\nWe highlight that the pseudolabeling baseline is exactly training the RPN together with RoI on the target dataset. However, as shown in Table 1, pseudolabeling under-performs in the label translation problems, often with worse performance than \"No translation\".\n\n\n**Handle target datasets with more than one category**\n\n\nWe apologize for the confusion. In fact, most of our experiments handled multiple categories, i.e., nuScenes $\\rightarrow$ nuImages (10 classes), Synscapes $\\rightarrow$ Cityscapes (7 classes), and Internal-Dataset $\\rightarrow$ nuImages$^\\dagger$ (3 classes). We provide the dataset statistics in Table 6 in the Appendix and the full class names in Section A.2. We have clarified in Section 5.1 \n\n\n**Evaluation on other detection datasets**\n\nWe expect our strategy to work on other common detection data sets like COCO and Object365 since our method and problem are not restricted to driving data. We focus on driving data sets because this is the real-world motivation of our work (e.g., see our Internal-Dataset) where the practice involves combining multiple data sets from different vendors [1] or mixing synthetic data [2]. We also emphasize that we considered four scenarios from seven different data sets and arrived at consistent results.\n\n\n\n**The role of translation-mAP**\n\n\nWe absolutely agree with your assessment on the effectiveness of translation-mAP, which reflects the false intuition that visual inspection of translated labels might be sufficient to assess label quality. We believe it is important to disprove the false notion and emphasize the importance of downstream training when evaluating label quality.\n\n\n**Label translator is source-target specific**\n\n\n\nWe agree that a universal label translator that would not require retraining on the source and the target datasets would be impactful and impressive. However, we emphasize that we are the first one that formally introduce the label translation problem. We feel it is necessary for this work to streamline the problem setup. Furthermore, implementing a universal label translator is non-trivial (e.g., off-the-shelf foundation models are not always effective as we show in Table 5). \nWe hope to extend this framework to a universal translator in future work.\n\n\n\n\n[1] Yan Wang et al., Train in Germany, Test in The USA: Making 3D Object Detectors Generalize, Conference on Computer Vision and Pattern Recognition , 2020.\n\n[2] Viraj Uday Prabhu et al., Bridging the sim2real gap with CARE: Supervised detection adaptation with conditional alignment and reweighting. Transactions on Machine Learning Research,2023."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission334/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700084215703,
                "cdate": 1700084215703,
                "tmdate": 1700084215703,
                "mdate": 1700084215703,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bn9lSgh537",
                "forum": "ChHx5ORqF0",
                "replyto": "oTDcQQUi0L",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission334/Reviewer_MRBF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission334/Reviewer_MRBF"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your response"
                    },
                    "comment": {
                        "value": "I thank the authors for their response. I feel most of my concerns have been addressed, but I still encourage the authors to add results on common object detection benchmarks (e.g., COCO), which might demonstrate the value of the proposed method in more general cases. Overall, I still remain positive about this paper, but given the weakness raised by the other reviewers (e.g., limited insights) and the missing results aforementioned, I'd like to keep my initial rating."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission334/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700738682448,
                "cdate": 1700738682448,
                "tmdate": 1700738682448,
                "mdate": 1700738682448,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "RdP06upq0B",
            "forum": "ChHx5ORqF0",
            "replyto": "ChHx5ORqF0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission334/Reviewer_XHr3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission334/Reviewer_XHr3"
            ],
            "content": {
                "summary": {
                    "value": "This paper fuscous on object detection and tries to address the annotation mismatch issue among different datasets. \nThis paper first formally defines the label translation problem and proposes a taxonomy that characterizes the annotation mismatches across object detection datasets. In addition, this paper  introduces a simple yet effective label-guided pseudo-labeling (LGPL) approach. The proposed LGPL method extends the concept of pseudo-labeling by leveraging source dataset bounding boxes and class information for label translation. Comprehensive experiments and analysis on four translation scenarios across seven datasets validate the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ This paper is well organized and written. The overall motivation is clear and convincing.\n+ The research problem defined in this paper is interesting and practical.\n+ Promising results are achieved compared to several baselines."
                },
                "weaknesses": {
                    "value": "- Although the introduced research problem is interesting. The technical contributions of this paper is somewhat limited. It would be better to further highlight the novelty and technical contributions.\n- In Figure 4, the proposed method is compared to an unsupervised domain adaptive object detection method and an unsupervised image domain adaptive method. In my understanding, the two domain adaptive methods don\u2019t have any labels in the target domain. It is not strange the proposed method achieves superior performance, as it uses more annotations. I\u2019m wondering why the proposed method is not compared to some weakly supervised cross-domain object detection methods, such as \"H2FA R-CNN: Holistic and Hierarchical Feature Alignment for Cross-Domain Weakly Supervised Object Detection, CVPR\u201922\", where all the image-level annotations are available.\n- The class-wise threshold $\\sigma_c$ is ad hoc. How to choose the value? Is it sensitive to the final results?"
                },
                "questions": {
                    "value": "Please refer the Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission334/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission334/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission334/Reviewer_XHr3"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission334/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698807500140,
            "cdate": 1698807500140,
            "tmdate": 1699635960169,
            "mdate": 1699635960169,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rEfzjpdb82",
                "forum": "ChHx5ORqF0",
                "replyto": "RdP06upq0B",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission334/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission334/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer XHr3"
                    },
                    "comment": {
                        "value": "$\\textbf{Dear Reviewer XHr3}$\n\n\nThank you for the positive comments regarding the practical importance of our problem, the presentation of our work, and the promising results achieved compared to the baselines.\n\n\n**Novelty and technical contributions**\n\nWe revised the paper to clarify our novelty and the contributions in the Introduction and the Conclusion. \n\n\nTo clarify our novelty and technical contributions:\n\n1. **Novelty**: We are the first to introduce and formalize the label translation problem, to the best of our knowledge. We demonstrate that this problem is prevalent with an extensive study of several object detection datasets, by categorizing a set of four common types of annotation mismatches. \n2. **Technical/Methodology**: We propose a simple but effective label translation algorithm, which, although motivated by pseudolabeling, expands upon this naive baseline. Unlike pseudolabeling, LGPL effectively leverages source dataset bounding boxes and class labels during label translation. \n3. **Empirical**: We extensively validate our approach on seven datasets and four source-target combinations. Our approach *consistently* outperforms all the baselines in every setting. Importantly, we compare against strategies like supervised domain adaptation [1], demonstrating that our data-centric approach is a new alternative to the standard method for addressing image/label mismatch.\n\n\n**Comparison with supervised domain adaptive object detection method [1]**\n\nWe apologize for the confusion on this experiment. In Figure 4, we indeed compare the proposed approach with supervised domain adaptation (SDA) [1]. Prior art [1] adopts popular unsupervised domain adaptation (UDA) approaches to the supervised settings, where both the source and the target labels are accessible. We adopt two variants S-DANN and S-CycConf from [1]. Note that we prefix the supervised version with \"S\". \n\nSDA approaches proposed in [1] directly align instance features given the source and the target labels, while cross-domain weakly supervised object detection (CD-WSOD) either aligns instance-level features via pseudolabels [2,3] or aligns image-level features [4]. We choose SDA as our baselines since SDA avoids noisy pseudolabels and is able to align instance-level features between the source and the target domain effectively.\n\nWe appreciate the reviewer's careful review. We have fixed the submission to clarify the comparison in Figure 4 and added a brief background to the supervised domain adaptation in Section C.1.\n\n\n\n**Details of choosing class-conditional thresholds $\\sigma_c$**\n\nOur revision clarifies this point in Appendix D.1.\n\nWe choose $\\sigma_c$ by quantizing the confidence score [5] for each class and applying it to all the label translators. Annotations falling into the last bin or with a confidence score lower than $0.001$ are dropped. In this way, the classes that are more challenging get the lower thresholds and vice versa. We leave further investigation of the class-conditional threshold to future research.\n\nLet $v_i$ be the value of the $i^{th}$ bin. We performs sensitivity test w.r.t. $\\sigma_c$ on nuScenes $\\rightarrow$ nuImages:\n\n| Label translator                  | Faster-RCNN Downstream-mAP |\n|-------------------|----------------|\n| LGPL with $\\sigma_c = v_1$  | 42.6 $\\pm$ 0.1               |\n| LGPL with $\\sigma_c = v_2$  | 42.02               |\n| LGPL with $\\sigma_c = v_5$  | 42.05               |\n\nFrom the experiments above, the choices of the class-conditional threshold do have impacts on the downstream-mAP but all of them still outperform the baselines.\n\n\n[1] Viraj Uday Prabhu et al., Bridging the sim2real gap with CARE: Supervised detection adaptation with conditional alignment and reweighting. Transactions on Machine Learning Research,2023. \n\n[2] Naoto Inoue et al., Cross-Domain Weakly-Supervised Object Detection through Progressive Domain Adaptation, CVPR2018\n\n[3] Shenxiong Ouyang et al., Pseudo-Label Generation-Evaluation Framework For Cross Domain Weakly Supervised Object Detection, ICIP 2021\n\n[4] Yunqiu Xu et al., H2FA R-CNN: Holistic and Hierarchical Feature Alignment for Cross-domain Weakly Supervised Object Detection, CVPR2022\n\n\n[5] Herbert A. Sturges, The choice of a class interval, Journal of the American Statistical Association 1926"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission334/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700101960858,
                "cdate": 1700101960858,
                "tmdate": 1700101960858,
                "mdate": 1700101960858,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TBpljUK5kA",
                "forum": "ChHx5ORqF0",
                "replyto": "rEfzjpdb82",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission334/Reviewer_XHr3"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission334/Reviewer_XHr3"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the authors\u2019 efforts in rebuttal. \n\nThe authors clarify most of my concerns in the experiment setup and design choice (e.g., supervised domain adaptive object detection and $\\sigma_c$). \n\nDespite the novelty regarding research problem formulation, I remain apprehensive about the overall technical contributions of this work. The proposed method is appreciated for its simplicity and effectiveness, but I feel the proposed method brings limited insights. Furthermore, as other reviewer pointed out, the research problem investigated is based on some strong hypotheses that limit its application. \n\nAfter carefully reading the response and other reviewers' comments, I would like to keep my rating."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission334/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700452984000,
                "cdate": 1700452984000,
                "tmdate": 1700452984000,
                "mdate": 1700452984000,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]