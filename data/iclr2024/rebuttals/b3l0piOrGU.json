[
    {
        "title": "Representation Deficiency in Masked Language Modeling"
    },
    {
        "review": {
            "id": "U3BwvqKOay",
            "forum": "b3l0piOrGU",
            "replyto": "b3l0piOrGU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3908/Reviewer_HZEv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3908/Reviewer_HZEv"
            ],
            "content": {
                "summary": {
                    "value": "The main claim of this paper is that the `[mask]` token takes some dimensions of the representation and that this may raise the risk of overfitting or result in a waste of model capacity. They first provide some empirical results showing that the `[mask]` tokens indeed cause the model to generate lower-rank representations. Theoretically, the authors also show that\n\n- The representation of the `[mask]` has high rank at the last layer. (lemma 2.1)\n- The vector space of the real tokens (non-mask tokens) representation at some layer does not include the vector space for the `[mask]` token, so the representation of real tokens can not be full-rank. (theorem 2.2)\n\nThese empirical and theoretical analyses motivate them to propose a encoder-decoder-based pretraining approach, MAE-LM, where the encoder\u2019s input does not contain `[mask]` tokens. Their empirical results show that MAE-LM outperform MLM on the GLUE and SQuAD benchmarks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. They provide empirical evidence supporting their claim.\n2. Their proposed MAE-LM outperforms MLM.\n3. They conduct comprehensive ablation studies."
                },
                "weaknesses": {
                    "value": "My main concerns are about the theoretical arguments in this paper.\n\n## Main concern 1: The connection between pretraining and fine-tuning\n\nIn the introduction, the paper claims that \n>  Those dimensions exclusively used for [MASK] tokens have not been pretrained to represent real tokens, and will have to be either trained from scratch on downstream data, raising the risk of overfitting (Hendrycks et al., 2019; Kumar et al., 2022), or become unused, resulting in a waste of model capacity.\n\nI think the interplay between pretraining and fine-tuning is very complicated and not fully understood yet. Thus I don\u2019t think this argument is substantiated. The authors cite Hendrycks et al. (2019) and Kumar et al. (2022) but I am not sure how these two works support this argument. Also, Figure (a) in this paper shows that inputs without a mask token have higher-rank representations. Doesn\u2019t it just indicate that the impact of having mask tokens during pretraining does not impact the rank of real tokens when mask tokens are not used?\n\n## Main concern 2: The unwritten assumptions of Theorem 2.2 \n\n***It seems that Theorem 2.2 is based on some unwritten assumptions***, e.g. the model needs to be an attention-only model without MLP and residual layers.\n\n## Main concern 3: The mismatch between the setup for Lemma 2.1 and the setup for Theorem 2.2\n\nFollowing the previous concern, Theorem 2.2 seems to be  based on some unwritten unrealistic assumptions, while Lemma 2.1 is based on the empirical results that (full) transformers can fit real-world high-rank distributions. If we look at the paper by Dong et al. (2021), we can find that under the assumption Theorem 2.2 is based on, the rank of the representation converges to 1. This implies that, under this assumption, Lemma 2.1 does not hold. Therefore, I think it\u2019s inappropriate to use Theorem 2.2 along with Lemma 2.1 to derive the conclusion of this paper.\n\n\n## Main concern 4: The weak implication of the theoretical results\n\nThe results only suggest that the representation of real tokens cannot be full-rank. It does not characterize how far the representation is from being full-rank. It is possible that the representation of the real tokens has rank $d - 1$. In this case, the so-called \u201crepresentation deficiency\u201d problem may not be a big issue.\n\n\n## Minor concern 1: The representation matrix of each example v.s. the whole corpus\n\nThe rank of the representation matrices discussed in Lemma 2.1 seems to be the representation matrix of the whole corpus. But in reality, we only feed in the model with a much shorter sequence of tokens, e.g., 512 tokens. In this case, the rank of the distribution of the masked tokens is at most 512 * 15% (in expectation), meaning that the mask tokens can\u2019t occupy too many dimensions. \n\nThis may not be a big issue, but I think the arguments starting from Theorem 2.2 need to be rewritten a little bit. For example, in Theorem 2.2, because it\u2019s about the sequence fed into a transformer model, the representation matrix should be of an example rather than of the whole corpus. Therefore, Lemma 2.1 can\u2019t be used directly.\n\n\n## Minor concern 2: The contribution of this work\n\nThis work has some similarities with the work by Dong et al. (2021):\n\n1. Dong et al. (2021) also plot the representation rank of transformer models.\n2. Theorem 2.2 in this paper is largely based on the proof from Dong et al. (2021).\n\nI think the authors should give credit to Dong et al. more explicitly."
                },
                "questions": {
                    "value": "I would probably recommend this paper more if the theoretical parts were stated differently (or removed)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3908/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697921832689,
            "cdate": 1697921832689,
            "tmdate": 1699636351115,
            "mdate": 1699636351115,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ZCpZxdm8MO",
                "forum": "b3l0piOrGU",
                "replyto": "U3BwvqKOay",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3908/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3908/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer HZEv"
                    },
                    "comment": {
                        "value": "Thank you very much for your thoughtful feedback! We address your raised points as follows.\n\n**The interplay between pretraining and fine-tuning is very complicated and not fully understood yet**: While we agree that theoretical understanding of the relationship between pretraining and fine-tuning is still lacking, the general consensus in the literature supports the pivotal role of pretraining in enhancing both the effectiveness (Kumar et al.) and robustness (Hendrycks et al.) upon fine-tuning. Therefore, analogous to why a non-pretrained model is inferior to a pretrained counterpart, the model dimensions that are not pretrained to represent real tokens (due to the deficiency issue caused by [MASK] tokens) will be less effective on downstream data. Furthermore, we have shown in Figure 5 (b) that the gap in effective rank persists in fine-tuning, confirming the lasting impact of pretraining.\n\n**Figure (a) in this paper shows that inputs without a mask token have higher-rank representations**: Sorry for the confusion, but our Figure 1 (a) shows the opposite \u2013 inputs without [MASK] suffer from lower-rank representations. \n\n**The unwritten assumptions of Theorem 2.2**: In our original submission, we explicitly stated the assumption and rationale for only analyzing the rank change induced by self-attention at the beginning of our proof in Appendix A. In the updated paper version, we have moved them to the main paper for better clarity. In summary, only the rank changes induced by self-attention reflect the extent of contextualization in token representations, and the effectiveness of text encoders is typically attributed to the contextualized representations. While MLPs/residual connections can also change the rank of representations, they do not provide each token with new information from other tokens. While we do not account for MLPs and residual connections, our analysis validates that the rank deficiency is caused by the self-attention mechanism, and in practice, MLPs and residual connections do not prevent the issue from happening. \n\n**The mismatch between the setup for Lemma 2.1 and the setup for Theorem 2.2**: We\u2019d like to clarify that Lemma 2.1 does not impose any architectural assumptions and it holds for any sequence modeling architecture. The empirical studies in Dong et al. (Figure 2 in their paper) are under a different setup than our Lemma 2.1: According to Section 4.1 of Dong et al., the pure attention architecture is not the one directly trained for the MLM task, whereas in our case the model is directly trained with MLM.\n\n**The weak implication of the theoretical results; it is possible that the representation of the real tokens has rank $d-1$**: Our primary goal is to utilize theoretical analysis to substantiate the presence of the representation deficiency issue. We have shown empirically that the representation deficiency issue results in a nontrivial gap in effective rank as large as ~50 dimensions, especially in deeper layers. Furthermore, we demonstrate that addressing this issue via our proposed MAE-LM method yields consistent performance improvements across various downstream tasks. Therefore, the identified issue is a crucial problem in MLM pretraining that warrants attention and remediation.\n\n**The representation matrix of each example v.s. the whole corpus**: Thanks for the question. We\u2019d like to clarify that while the analysis in Lemma 2.1 is conducted at the corpus level, the conclusion of Lemma 2.1 can extend seamlessly to individual sequences. Specifically, if the effective rank of [MASK] token increases throughout Transformer layers when considering the entire corpus, it logically follows that this phenomenon holds true by expectation for the majority, if not all, individual samples in the corpus.\n\n**The contribution of this work and Dong et al.**: While Dong et al. also plot the representation rank, their focus is not on investigating the issue related to [MASK] tokens in MLM pretraining. Regarding our theoretical analysis, we indeed draw inspiration from Dong et al., and we explicitly mentioned this in the full proof in our original submission (Appendix A). In the updated paper, we have added a paragraph at the beginning of our full proof in Appendix A to give Dong et al. more credits.\n\nThanks again for your thoughtful comments. We have incorporated the revision accordingly in the updated paper. Please let us know if there are any further questions."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3908/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700373308052,
                "cdate": 1700373308052,
                "tmdate": 1700679845288,
                "mdate": 1700679845288,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "CUgoWPhjzw",
            "forum": "b3l0piOrGU",
            "replyto": "b3l0piOrGU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3908/Reviewer_247c"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3908/Reviewer_247c"
            ],
            "content": {
                "summary": {
                    "value": "This paper identifies a significant discrepancy in Masked Language Modeling (MLM) pretraining, where model representations are skewed towards the [MASK] token, absent in downstream tasks. To address this, the authors introduce MAE-LM, a novel algorithm that enhances model efficiency and performance, outstripping robust baselines across various metrics."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper effectively pinpoints an important yet overlooked issue in BERT-like language models' (LMs) pretraining, specifically the representation mismatch due to [MASK] tokens. It then proceeds to look into this mismatch, attributing it to rank deficiency with robust empirical and theoretical backing in Section 2.2. This perspective is both insightful and novel.\n\n2. The proposed method, MAE-LM, addresses the identified mismatch issue without resorting to complex architectural alterations. This simplicity ensures the resulting pretrained model retains compatibility with existing BERT-like models, demonstrating the method's applicability.\n\n3. The paper's evaluation is very solid, adhering to standard practices with comprehensive benchmarking on GLUE and SQuAD, compared against multiple strong baselines. MAE-LM consistently outperforms these baselines across multiple scales\u2014base, base++, and large++\u2014sometimes even by substantial margins.\n\n4. Detailed analyses and studies confirm that the performance boost really stems from improved model dimension utilization, which is a direct result of addressing the rank deficiency issue.\n\n5. The writing and presentation of the paper are top-notch, mirroring the high quality of its content."
                },
                "weaknesses": {
                    "value": "1. Although the paper mentions in Footnote 2 that some MLM training settings (like Google's original BERT) keep 10% of [MASK] tokens as original, and that subsequent studies like Wettig et al. (2022) found this trick unnecessary, an exploration of the effective rank of models using this 80:10:10 technique, similar to Figure 1, would be beneficial.\n\n2. Missing reference: [this paper](https://proceedings.mlr.press/v97/gong19a.html) looked into attention patterns in pretrained BERT-like LMs, uncovering patterns related to low-rankness. This study could provide additional insights into the topic at hand."
                },
                "questions": {
                    "value": "Could you explore the effective rank of models using 80:10:10 masking technique (like Google's original BERT)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "10: strong accept, should be highlighted at the conference"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3908/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698629150103,
            "cdate": 1698629150103,
            "tmdate": 1699636351017,
            "mdate": 1699636351017,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jNvjxKXrQ8",
                "forum": "b3l0piOrGU",
                "replyto": "CUgoWPhjzw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3908/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3908/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 247c"
                    },
                    "comment": {
                        "value": "Thank you very much for your thoughtful feedback, and we appreciate your positive acknowledgment of our work! We address your raised points as follows.\n\n**Using 80:10:10 masking technique**: In our preliminary experiments, we found that the 80:10:10 masking technique did not provide notable performance gain compared to directly masking out 15% of the tokens, and thus we used the latter in our evaluations. That said, we agree that validating our findings under the 80:10:10 masking technique is beneficial. We have added the effective rank plot in Appendix E, Figure 6 of our updated paper. The results are largely similar to Figure 1, which confirms that randomly replacing a small percentage of [MASK] tokens with real ones does not effectively address the representation deficiency issue, as the ratio of [MASK] tokens in pretraining is still high.\n\n**Missing reference**: Thank you for pointing out the reference, and we have added the discussions of this work to both Section 5 and Appendix F. We agree that our findings in this paper are related to the paper you mentioned: The rank deficiency problem might have caused de-contextualization in token representations, which is reflected by the localized self-attention patterns as discovered in that study.\n\nThanks again for your review! Please let us know if you have any further questions."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3908/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700373172921,
                "cdate": 1700373172921,
                "tmdate": 1700373172921,
                "mdate": 1700373172921,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "3KVw3eMzTZ",
            "forum": "b3l0piOrGU",
            "replyto": "b3l0piOrGU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3908/Reviewer_sv4z"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3908/Reviewer_sv4z"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the following discrepancy: for masked language models (MLMs), the [MASK] token is only presented in pre-training but not in downstream fine-tuning. Based on both empirical and theoretical analysis, the authors show that the presence of [MASK] token in pretraining occupy some model dimensions even in downstream applications when [MASK] is not used, that is, not all model dimensions are leveraged to represent tokens other than [MASK]. To address this discrepancy, the authors propose MAE-LM, removing [MASK] tokens from MLM pre-training. Empirical evaluations show consistent improvement on the GLUE and SQuAD benchmarks compared to well known MLMs including BERT and RoBERTa."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "Being able to identify this discrepancy and provide a clear and in-depth analysis is the major strength of this paper. The proposed solution to this problem is simple and effective, providing insights for more sophisticated approaches.\n\nThis paper is overall very well presented; in particular, the theoretical analysis presented in this paper are easy to follow and immediately to-the-point; Lemma 2.1 shows that the rank of the matrices for [MASK] token representation increases as the layers go deeper, providing a nice explanation to the empirical observation; Theorem 2.2 shows that the embedding of some [MASK] tokens need to be orthogonal to that of real tokens, limiting the number of dimensions that can be used."
                },
                "weaknesses": {
                    "value": "It would be nice if the authors can expand a little bit more on the implications of this work on autoregressive LMs but I really do not see any major weakness in this paper. Though someone may argue that MAE-LM\u2019s improvements on benchmarks are relatively small compared to baselines, this is not a quite problem as the improvements shown are quite consistent; besides, one major contribution of this paper is really to identify the discrepancy between pre-training and downstream fine-tuning and to analyze its effect on MLM\u2019s expressiveness."
                },
                "questions": {
                    "value": "n/a"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3908/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699262956046,
            "cdate": 1699262956046,
            "tmdate": 1699636350954,
            "mdate": 1699636350954,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9lqN6u0gpH",
                "forum": "b3l0piOrGU",
                "replyto": "3KVw3eMzTZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3908/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3908/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer sv4z"
                    },
                    "comment": {
                        "value": "Thank you very much for your thoughtful feedback, and we appreciate your positive acknowledgment of our work! We discuss the implications of our work on autoregressive LMs as follows.\n\nWhile autoregressive LM pretraining generally does not introduce artificial symbols such as [MASK], our analyses can be extended to show that the representation deficiency issue can also arise in autoregressive pretraining when certain real tokens exist exclusively in the pretraining data but are either absent or occur infrequently in downstream data. Similar to the impact of [MASK] tokens, these tokens occupy dimensions during pretraining that may not be effectively utilized in downstream tasks. Consequently, it is desirable to maximize the vocabulary overlap between pretraining data and downstream data, which can be realized via pretraining data selection, training corpora pre-processing, and vocabulary pruning. We leave these explorations as future work, and we have added the discussions to Appendix F in the updated paper.\n\nThanks again for your review! Please let us know if you have any further questions."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3908/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700372884400,
                "cdate": 1700372884400,
                "tmdate": 1700372884400,
                "mdate": 1700372884400,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "m5lTDYCHeo",
                "forum": "b3l0piOrGU",
                "replyto": "9lqN6u0gpH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3908/Reviewer_sv4z"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3908/Reviewer_sv4z"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for providing your perspective on autoregressive models. My score remains unchanged."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3908/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700690577829,
                "cdate": 1700690577829,
                "tmdate": 1700690577829,
                "mdate": 1700690577829,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "PundcOXzXK",
            "forum": "b3l0piOrGU",
            "replyto": "b3l0piOrGU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3908/Reviewer_3FYT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3908/Reviewer_3FYT"
            ],
            "content": {
                "summary": {
                    "value": "This paper employs effective rank to analyze the representation deficiency caused by the [mask] token in Masked Language Models (MLM). Based on the analysis results, this paper proposes the MAE-LM model, which does not input [mask] in the encoder and supplements [mask]  in the shallow decoder. The MAE-LM model achieves satisfactory results in downstream tasks."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This work is the first study using effective rank to analyze the decrease in expressive power caused by the [mask] token, leading to two insightful theorems. These theorems may inspire future research in the field of MLM.\n\n2. This paper provides rigorous mathematical proofs supporting the analytical findings.\n\n3. The selected tasks for experimentation are representative, and the promising experimental results demonstrate a performance improvement of MAE-LM over classical MLM models."
                },
                "weaknesses": {
                    "value": "This paper follows the classic framework of analysis + improvement, but both core modules closely resemble existing work.\n\n1. In the analysis module, the use of effective rank to represent MLM's representative capacity overlaps with section 3.1 of ISOTROPY (https://openreview.net/pdf?id=xYGNO86OWDH), where effective rank is used to denote the isotropy of representations. The differences between these two approaches are minor.\n\n2. In the experimental module, the model structure used in this paper is identical to the one used in Mask Later (https://arxiv.org/pdf/2211.04898.pdf)."
                },
                "questions": {
                    "value": "The authors should explicitly define how this work differs from existing works, otherwise,  it appears as if the paper merely uses the theoretical tool of ISOTROPY to analyze why Mask Later is effective. Without a distinct contribution, the overall impact of this work seems limited."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3908/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3908/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3908/Reviewer_3FYT"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3908/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699437861546,
            "cdate": 1699437861546,
            "tmdate": 1700466871580,
            "mdate": 1700466871580,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2PMvgzY79l",
                "forum": "b3l0piOrGU",
                "replyto": "PundcOXzXK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3908/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3908/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 3FYT"
                    },
                    "comment": {
                        "value": "Thank you very much for your thoughtful feedback! We address your raised points as follows.\n\n**The use of effective rank to represent MLM's representative capacity overlaps with section 3.1 of ISOTROPY**: We\u2019d like to clarify that we follow the definition of effective rank in ISOTROPY (Cai et al.) to only compute the rank and empirically showcase the identified representation deficiency issue, and we have not used it in our theoretical analyses. Our major contributions are (1) investigating the discrepancy between pretraining and fine-tuning of MLM models caused by [MASK] tokens from the perspective of representation rank, and (2) demonstrating that a simple architectural modification in MLM\u2014excluding [MASK] tokens from the encoder\u2019s input\u2014can effectively address the representation deficiency and result in improved downstream performance. These contributions are orthogonal to Cai et al. We have updated our paper to provide clarifications in Section 2.2.\n\n**The model structure used in this paper is identical to the one used in Mask Later**: Compared to the 3ML approach proposed by Liao et al., our proposed MAE-LM shares the same masked autoencoder architecture. However, the fundamental motivations, focuses, results and insights of the two works are different: \n* _Focuses_: Liao et al. focus on an efficiency aspect, by excluding [MASK] tokens to reduce the sequence length. In contrast, our primary goal is to use principled empirical and theoretical analyses to illuminate an overlooked issue in MLM\u2014specifically, the impact of [MASK] tokens on the model's representation power. The MAE-LM framework is introduced as a simple solution to rectify the rank deficiency issue arising from [MASK] tokens, aiming for the understanding of its influence on the learned representations and the downstream task performance.\n* _Results_: In contrast to Liao et al.'s efficiency-driven emphasis, our experimental results substantiate that MAE-LM effectively mitigates the representation gap induced by [MASK] tokens, consistently outperforming standard MLM across various tasks and pretraining settings. To our knowledge, such a representation learning benefit of the masked autoencoder architecture has not been explored before.\n* _Insights_: Importantly, our contributions may extend beyond the specific model architecture and offer insights for future developments in MLM pretraining. For example, one may regularize the [MASK] token representations to reside in the same space of real token representations as another potential solution to the representation deficiency problem. Our analyses also underscore the importance of considering real tokens that exist solely in pretraining data, yet are absent or occur rarely in downstream data. Such tokens, akin to [MASK] tokens, have the potential to compromise model representation power.\n\nWe have incorporated a brief discussion in Section 3 of our revised paper to emphasize the distinctiveness of our contribution.\n\nThanks again for your review! Please let us know if you have any further questions."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3908/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700372827678,
                "cdate": 1700372827678,
                "tmdate": 1700372827678,
                "mdate": 1700372827678,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lDkdHvG1BM",
                "forum": "b3l0piOrGU",
                "replyto": "PundcOXzXK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3908/Reviewer_3FYT"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3908/Reviewer_3FYT"
                ],
                "content": {
                    "title": {
                        "value": "Re: Response to Reviewer 3FYT"
                    },
                    "comment": {
                        "value": "Thank you for your prompt response.\n\n1. **Analysis moudle**. After carefully reviewing your reply and Appendix A, I have reconsidered the theoretical analysis section of the paper. I recognize that both lemmas in the paper are non-trivial, and the proof methods used differ from ISOTROPY. So my concerns about the analysis module have been well addressed.\n\n2. **Experiment moudle.** Nevertheless, my concerns persist regarding the experiment module. Despite different motivations, Liao et al.'s results have already demonstrated that MAE-LM can achieve performance comparable to, or even superior to, traditional MLM (as shown in Table 3 of Liao et al.'s paper). In other words, we already hold a prior expectation that, regardless of the correctness of the theoretical analysis in this paper, the MAE-LM structure will work.\n\nHowever, I fully agree that the theoretical analysis of this paper \"may extend beyond the specific model architecture and offer insights for future developments in MLM pretraining.\" Consequently, I have raised the score to 6."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3908/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700467254120,
                "cdate": 1700467254120,
                "tmdate": 1700616555725,
                "mdate": 1700616555725,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]