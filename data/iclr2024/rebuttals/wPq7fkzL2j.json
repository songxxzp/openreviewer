[
    {
        "title": "Self-Paced Augmentations (SPAug) for Improving Model Robustness"
    },
    {
        "review": {
            "id": "ZXOHvWkzPS",
            "forum": "wPq7fkzL2j",
            "replyto": "wPq7fkzL2j",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission260/Reviewer_56By"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission260/Reviewer_56By"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces an adaptive data augmentation strategy for deep neural networks, focusing on enhancing model robustness and performance. By employing a self-paced augmentation method, the research dynamically adjusts the intensity of data augmentation based on individual sample characteristics. The paper demonstrates the effectiveness of this approach using various datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "(1) The point of the paper is very good.\n(2) Combining with multiple strategies to demonstrate performance improvement is also a great approach."
                },
                "weaknesses": {
                    "value": "(1) In Section 4.3, you mentioned that the learnable SPAug has a significant improvement effect on the performance of AugMix in processing corrupted data. However, in the above Table 2, compared to the optimal model, your performance improvement is very limited or there is no improvement at all. The superiority of the model is not sufficiently reflected.\n(2) You did not discuss the threshold \u03c4 in the subsequent experiments. It is not clear how you optimized the threshold. It feels like you are showing the best experimental results that you got separately with \u03c4=0.1 or \u03c4=0.2.\n(3) The last two models, after adding SPAug-Learnable, indeed improved on the corrupted dataset, but there was a loss on the clean dataset. I hope you can consider the adaptability issue between the specific data augmentation strategy and the data.\n(4) Introducing an adaptive learning data augmentation strategy increases the complexity of the model and may lead to extended training times and computational costs. While you mentioned that this method introduces minimal overhead, there is no specific experimental data in the article to support this claim."
                },
                "questions": {
                    "value": "Please refer to the weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission260/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698656704201,
            "cdate": 1698656704201,
            "tmdate": 1699635951740,
            "mdate": 1699635951740,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BdfrbbpMMT",
                "forum": "wPq7fkzL2j",
                "replyto": "ZXOHvWkzPS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission260/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission260/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 56By Q1"
                    },
                    "comment": {
                        "value": ">  In Section 4.3, you mentioned that the learnable SPAug has a significant improvement effect on the performance of AugMix in processing corrupted data. However, in the above Table 2, compared to the optimal model, your performance improvement is very limited or there is no improvement at all. The superiority of the model is not sufficiently reflected.\n\nThank you for your comment! After reviewing your feedback, we realized that the performance improvement of SPAug over AugMix may not have been adequately highlighted. The tables below provides a summary of the performance enhancements achieved by incorporating SPAug with the AugMix policy.\n\n| Dataset   | JSD? | AugMix          | AugMix+SPAug (Ours) | % Improvement |\n|-----------|------|-----------------|----------------------|--------------|\n| CIFAR-10  | No   | $14.0\\pm{0.4}$  | $13.0\\pm{0.2}$       | +1.0         |\n| CIFAR-10  | Yes  | $11.2\\pm{0.3}$  | $11.0\\pm{0.2}$       | +0.2         |\n\nTable 1: Comparison of SPAug+AugMix vs. AugMix in terms of mean corruption error on CIFAR-10-C with WRN-40-2 backbone.\n\n| Dataset     | JSD? | AugMix          | AugMix+SPAug (Ours) | % Improvement |\n|-------------|------|-----------------|----------------------|--------------|\n| CIFAR-100   | No   | $40.0\\pm{0.1}$  | $39.0\\pm{0.1}$       | +1.0         |\n| CIFAR-100   | Yes  | $36.1\\pm{0.1}$  | $35.0\\pm{0.1}$       | +1.1         |\n\nTable 2: Comparison of SPAug+AugMix vs. AugMix in terms of mean corruption error on CIFAR-100-C with WRN-40-2 backbone.\n\n| Dataset   | JSD? | AugMix          | AugMix+SPAug (Ours) | % Improvement |\n|-----------|------|-----------------|----------------------|--------------|\n| CIFAR-10  | No   | $10.7\\pm{0.2}$  | $10.0\\pm{0.1}$       | +0.7         |\n| CIFAR-10  | Yes  | $9.3\\pm{0.0}$   | $8.7\\pm{0.0}$        | +0.6         |\n\nTable 3: Comparison of SPAug+AugMix vs. AugMix in terms of mean corruption error on CIFAR-10-C with WRN-28-10 backbone.\n\n| Dataset     | JSD? | AugMix          | AugMix+SPAug (Ours) | % Improvement |\n|-------------|------|-----------------|----------------------|--------------|\n| CIFAR-100   | No   | $33.6\\pm{0.3}$  | $33.3\\pm{0.0}$       | +0.3         |\n| CIFAR-100   | Yes  | $31.9\\pm{0.2}$  | $31.3\\pm{0.2}$       | +0.6         |\n\nTable 4: Comparison of SPAug+AugMix vs. AugMix in terms of mean corruption error on CIFAR-100-C with WRN-28-10 backbone.\n\n\nAs demonstrated in the tables above, it is evident that coupling SPAug with AugMix consistently results in a significant improvement in performance on both CIFAR-10 and CIFAR-100, whether using the WRN-40-2 or WRN-28-10 backbone. This further underscores the benefit of employing instance-specific augmentation parameters over uniform augmentation parameters."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission260/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700697986018,
                "cdate": 1700697986018,
                "tmdate": 1700698018807,
                "mdate": 1700698018807,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Z8Sxhht9TY",
                "forum": "wPq7fkzL2j",
                "replyto": "ZXOHvWkzPS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission260/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission260/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> ntroducing an adaptive learning data augmentation strategy increases the complexity of the model and may lead to extended training times and computational costs. While you mentioned that this method introduces minimal overhead, there is no specific experimental data in the article to support this claim.\n\nThat is a valid question, and thank you for highlighting it. Since our method does not involve an expensive optimization process (such as the bi-level optimization procedure utilized in AugMax), we did not observe any significant slowdown compared to the baseline. To support our argument, we benchmarked the training time per epoch for standard training, AugMix, SPAug with AugMix policy, and AugMax on ImageNet with ResNet18, using a single NVIDIA A6000 GPU.\n\n| Method                | Time (sec/epoch) |\n|-----------------------|-------------------|\n| Standard              | 2669              |\n| AugMix                | 3622              |\n| **SPAug (w/ AugMix)**     | **3698**              |\n| AugMax                | 5264              |\n\nAs shown in the table above, introducing SPAug into a AugMix slows down the code only by 3698-3622  =  76 sec/epoch  which is very minimal computational overhead compared to augmentation policies that use complex optimization processes to determine instance-specific parameters, like AugMax, which can result in considerable computational overhead."
                    },
                    "title": {
                        "value": "Response to Reviewer 56By Q4"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission260/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700701151841,
                "cdate": 1700701151841,
                "tmdate": 1700701218769,
                "mdate": 1700701218769,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "eic1zo3TAh",
            "forum": "wPq7fkzL2j",
            "replyto": "wPq7fkzL2j",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission260/Reviewer_AiNq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission260/Reviewer_AiNq"
            ],
            "content": {
                "summary": {
                    "value": "The author proposed self-paced augmentations for training neural networks. In particular, it chooses the data augmentation strength dynamically according to the training statistics of training samples. The author conducts experiments on CIFAR10/CIFAR100 and demonstrates its effectiveness and robustness."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "i) The idea and the implementation are easy to follow, and the writing is clear to read.\n\nii) The author provides visualization and quantitative analysis to validate the effectiveness of the proposed method."
                },
                "weaknesses": {
                    "value": "i) The experiments are only conducted on some small datasets (e.g. CIFAR), it is not convincing without the experiment results on a large-scale dataset(e.g. ImageNet)\n\nii) The proposed augmentation, as illustrated in Eq.1, shares a similar formulation as two widely used augmentation methods: CutMix and Mixup. However, there is no comparison between the proposed method and CutMix/Mixup\n\niii) There are also some related works[a,b], which also adjust the augmentation strength according to other training statistics. They are not discussed and compared in experiments.\n\n[a] Universal Adaptive Data Augmentation\n[b] ADAPTIVE DATA AUGMENTATION FOR IMAGE CLASSIFICATION"
                },
                "questions": {
                    "value": "Refer to the weakness"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission260/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698657647731,
            "cdate": 1698657647731,
            "tmdate": 1699635951670,
            "mdate": 1699635951670,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "IY1LVwjOeA",
                "forum": "wPq7fkzL2j",
                "replyto": "eic1zo3TAh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission260/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission260/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer AiNq Q1 & Q2"
                    },
                    "comment": {
                        "value": "> The proposed augmentation, as illustrated in Eq.1, shares a similar formulation as two widely used augmentation methods: CutMix and Mixup. However, there is no comparison between the proposed method and CutMix/Mixup.\n\nThank you for the comment. Although the proposed approach may appear similar to CutMix and MixUp at first glance, they are fundamentally different from our SPAug. The primary distinction lies in the fact that CutMix and MixUp fall under a uniform augmentation policy. In other words, the hyperparameters for CutMix and MixUp are fixed for all samples in the dataset, resulting in uniform augmentation intensity applied to all samples. In contrast, SPAug introduces sample-dependent augmentation intensity during training, tailored to how each sample converges. Samples that converge rapidly (referred to as \"easy samples\") are encouraged to undergo higher levels of augmentation intensity, while those converging slowly (referred to as \"hard samples\") are encouraged to train with less synthetic augmentation applied. Moreover, instead of employing computationally expensive optimization processes such as reinforcement learning or bi-level optimization, we draw inspiration from the curriculum learning literature and propose to use each sample's training loss (cross-entropy loss) as a proxy measure to determine their ease or difficulty of convergence.\n\nHowever, in response to the reviewer's feedback, we have decided to compare our SPAug method with CutMix and MixUp to provide a more comprehensive analysis. Following your comment, we have conducted the comparison as shown in Table 1 and Table 2 (below). \n\n| Standard | CutOut | MixUp | CutMix | AA | AugMix | AugMix+SPAugLearnable |\n|----------|--------|-------|--------|----|--------|-----------------------|\n| 27.2     | 26.8   | 22.3  | 27.1   | 23.9 | 11.2   | 11.0                  |\n\nTable 1: Comparison of corrupted Test Error (C-Err.) of SPAug with AugMix policy on CIFAR-10-C with WRN-40-2.\n\n\n\n| Standard | CutOut | MixUp | CutMix | AA | AugMix | AugMix+SPAugLearnable |\n|----------|--------|-------|--------|----|--------|-----------------------|\n| 53.2     | 53.5   | 50.4  | 52.9   | 49.6 | 36.1   | 35.0                  |\n\nTable 2: Comparison of corrupted Test Error (C-Err.) of SPAug with AugMix policy on CIFAR-100-C with WRN-40-2.\n\nAs shown in Table 1 and Table 2, the augmentation policies CutOut, MixUp, and CutMix result in significantly higher corrupted test error rates (C-Err.) than Augmix+SPAugLearnable. This indicates that models trained with those augmentation policies have lower generalizability against common corruptions.\n\nIn regards to demonstrating results on ImageNet, we have done some preliminary comparisons in Table 3 of the main body. We will extend experiments on this dataset in future revisions."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission260/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700693732705,
                "cdate": 1700693732705,
                "tmdate": 1700693931047,
                "mdate": 1700693931047,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OYwJe9ScjT",
                "forum": "wPq7fkzL2j",
                "replyto": "eic1zo3TAh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission260/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission260/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer AiNq Q3"
                    },
                    "comment": {
                        "value": "Thank you for bringing our attention to these related works. However, we would like to clarify the differences between the proposed SPAug and two other related methods, Universal Adaptive Data Augmentation (UADA) [1] and Adaptive Data Augmentation [2].\n\nWhen comparing our work to UADA, the main distinction lies in the fact that, although UADA adjusts the augmentation parameters adaptively over iterations, it maintains uniform parameters for all data samples in the dataset. In other words, UADA's goal is to change (or adapt) the augmentation parameters based on the model's gradient information during training. In contrast, our objective is to have instance-dependent augmentation parameters that change based on how each sample converges during training. We will discuss UADA in the related work section, highlighting this distinction between SPAug and UADA. However, it's worth noting that UADA does not provide a comparison of their performance on corrupted test sets (such as CIFAR-10-C and CIFAR-100-C), which currently prevents us from making a direct comparison between UADA and SPAug.\n\nThe main intuition behind the Adaptive Data Augmentation framework [2] is to find a small transformation that results in the maximum classification loss, leading to training with worst-case data augmentation. However, their proposed worst-case augmentation framework only works with translational augmentations and cannot be directly extended to more complex augmentation operations, such as color transformations or mixing. Furthermore, they have validated their results on very small datasets, such as MNIST-500 and Small-NORB, which prevents us from making a direct comparison with their work. Nevertheless, we will discuss this work in the Related Work section since it represents one of the early attempts to demonstrate the importance of adaptive data augmentations."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission260/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700693795409,
                "cdate": 1700693795409,
                "tmdate": 1700693953724,
                "mdate": 1700693953724,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "F7vL8Lx2O3",
            "forum": "wPq7fkzL2j",
            "replyto": "wPq7fkzL2j",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission260/Reviewer_JB6n"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission260/Reviewer_JB6n"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces the used of a self-paced algorithm for training with data augmentation. Each sample is now a linear combination of a sample without transformations plus a sample with augmentations, where the blending factor is learned during training with a self-paced strategy based on the loss. In practice, during training, easy samples with low loss are augmented more than hard samples with high loss, that are more difficult to learn. This approach is supposed to improve results, especially on corrupted datasets. Results are presented for CIFAR 10/100 original and corrupted."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The idea of using self-paced learning for augmentations is new up to my knowledge and makes sense.\n- The method can be adapted to many kind of data augmentation by adding a few lines of code as shown in Algorithm 1."
                },
                "weaknesses": {
                    "value": "- The method is not compared directly with other reported results and the provided baselines seems to be weak, making results not accurate. For instance, in RA the error reported on CIFAR10/100 for Wide-ResNet 28-10 are respectively 2.7 and 16.7, while in the proposed paper are 3.3 and 19.1.\n- Authors state that all previous data augmentation models use augmentations that are not instance specific. However, there are papers, (eg. [1] or [2]) that learn instance specific augmentation through a neural network. Authors should cite the family of data augmentation methods based on transformations learned by a network and if possible compare with them.\n- Results are limited to CIFAR10/100. Results on a larger dataset as ImageNet should be provided.\n- In related work, there should be a part considering self-paced methods. There is a vast literature on such kind of approaches and even if it is not applied to data augmentation it is still relevant. Some approaches are cited during the presentation of the method, but I think that a more exhaustive presentation in related work is needed.\n- With large datasets, you need to store a large number of parameters, one per sample.\n\n[1] Miao et al., \"Learning Instance\u2013Specific Augmentations by Capturing Local Invariances\", ICML 2023\n\n[2] Benton et al. \"Learning invariances in neural networks\", NeurIPS 2020"
                },
                "questions": {
                    "value": "- Due to the stochastic nature of the augmentations, the corresponding loss for a given sample can fluctuate and introduce a certain noise on the selection of the easy/hard samples. Did you find any instabilities in the training due to this noise?\n- Why you train with a limited budget instead of training until convergence? This makes results not compareble with the state of the art. \n- What is the reason to propose the toy experiment in section 4.2. The following evaluations are also performed on the same dataset.\n- What is the value of the thresholds $\\tau$ for the experiments in Table 2?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission260/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698981686469,
            "cdate": 1698981686469,
            "tmdate": 1699635951605,
            "mdate": 1699635951605,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "HBAKO9OfRE",
                "forum": "wPq7fkzL2j",
                "replyto": "F7vL8Lx2O3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission260/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission260/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer JB6n Q1"
                    },
                    "comment": {
                        "value": "> Due to the stochastic nature of the augmentations, the corresponding loss for a given sample can fluctuate and introduce a certain noise on the selection of the easy/hard samples. Did you find any instabilities in the training due to this noise?\n\nThank you for your comment. No, we did not observe any training instabilities during the training. \n\nFor example, **Figure 7 in Supplementary Document** shows CE loss when training WRN-40-2 backbone with AugMix policy w/ and w/o SPAug. As can be seen, there is no training instabilities observed."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission260/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700692920756,
                "cdate": 1700692920756,
                "tmdate": 1700692920756,
                "mdate": 1700692920756,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "PpIH9X8TqT",
            "forum": "wPq7fkzL2j",
            "replyto": "wPq7fkzL2j",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission260/Reviewer_6RhX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission260/Reviewer_6RhX"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose an adaptive augmentation technique which dynamically adjust the augmentation intensity based on training statistics. The approach can be applied to existing augmentation framework, such as AugMix, RandomAugment and AutoAugment. The experimental results show that it can improve model robustness to image corruptions."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ Interesting idea to combine curriculum learning into data augmentation, controlling how models learn from augmented samples.\n+ The writing is well-structured and easy to read."
                },
                "weaknesses": {
                    "value": "__Missing comparisons with existing work__\nThe authors did not explain the difference of their approach from AugMax [1] framework, which combines augmented images with adversarially calculated weights.  Similarly, Hou et al. [2] adopted the idea of curriculum learning and applied it to decide when to augment data during training. But this is also not discussed by the authors. Other augmentation techniques being sota, PRIME [4] and TrivialAugment [5], are not compared with.\n\n__clarity__\nExplanation of equation (4) is not clear and in Fig. 3, the formula for hard sample should be: L_i - \\sigma(m_i)\nIt is unclear in the formula whether the cross-entropy loss should be calculated on the original images or the augmented images. If the cross-entropy loss is computed based on the augmented images, then the parameter m_i could be also updated through backpropagation, just like AugMax [1].\n\n__Non-comprehensive experimental results__\nThe authors only show results of SPAug combined with AugMix on ImageNet, while the results of it combined with AutoAugment and RandomAugment are not provided. The standard performance of models are not given in Table 2. The evaluation metrics in [3] used for benchmarking the robustness of models to image corruptions are not used. \nExperiments are mostly focused to small datasets, while ImageNet is only used for a single comparison. The claims and observations made on CIFAR10/100 are thus limited and cannot be generalized and compared with those in other papers that extensively experiments on ImageNet. Furthermore, only a single architecture is tested: how would this method perform with transformer training strategies?\n\n__Figures do not have enough explanations__\nThe meaning of x and y axes in Fig, 4 are not explained and  the figure itself is not easily readable. From the figure, I cannot interpret how the binary mapping function governs the extent of augmentations. In Fig. 5, the authors provide four augmented versions for one class, alongside the changes of m_i during training. However, the relationship between the augmented images and the m_i tendency is not explained. \n\n__Missing appendices__\nThe authors mention appendix and supplementary materials, but it is not given. \n\n[1] Wang et al., \u201cAugMax: Adversarial Composition of Random Augmentations for Robust Training\u201d, (2021)\n[2] Hou et al., \u201cWhen to Learn What: Model-Adaptive Data Augmentation Curriculum\u201d, (2023)\n[3] Hendrycks et al., \u201cBenchmarking Neural Network Robustness to Common Corruptions and Perturbations\u201d, (2019)\n[4] Modas et al., PRIME: A few primitives can boost robustness to common corruptions, (2022)\n[5] Muller et al. TrivialAugment: Tuning-free Yet State-of-the-Art Data Augmentation, (2021)\n\nMinor: Implementation python code directly pasted as pseudo code."
                },
                "questions": {
                    "value": "- How is the work different from [1,2] and what improvements have been made regarding them? This work is quite similar to AugMax[1] in the sense of combining augmented images with their original version using weights calculated by backpropagation.\n- In the experiments, how is the threshold \u03c4 that distinguishes easy samples from hard ones in the minibatch determined? Is it affected by the minibatch size? Is there any trade-off between them, considering needed computational resources?\n- In Tables 4 and 5, the results of models trained in the 100 and 200 epochs are given. However, the gained performance through more training epochs is not significant. For instance, in Table 4, the gained C-Err for SPAug-Learnable is on average 0.5, while this value for AA is 0.9. What is the reason for training more epochs to obtain trivial improvement using SPAug-Learnable? Besides, why is the baseline model WRN-28-10 having different C-Err in Table 2 and 4?\n- How does the method perform on ImageNet with other existing augmentation techniques, and with other architectures (e.g. Transformers)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission260/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission260/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission260/Reviewer_6RhX"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission260/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699118368304,
            "cdate": 1699118368304,
            "tmdate": 1699635951532,
            "mdate": 1699635951532,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5qWWxgQhI7",
                "forum": "wPq7fkzL2j",
                "replyto": "PpIH9X8TqT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission260/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission260/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 6RhX Q1"
                    },
                    "comment": {
                        "value": "We thank the reviewer for pointing out these interesting related works [1-2]. While we recognize that the high-level idea of AugMax, MADAug, and SPAug is to have instance specific augmentation parameters, our approach of addressing this problem fundamentally differs from others.\n\n*Answer to Q1*\n> How is the work different from [1,2] and what improvements have been made regarding them? This work is quite similar to AugMax[1] in the sense of combining augmented images with their original version using weights calculated by back-propagation.\n\nWhen considering AugMax alongside SPAug, we would like to emphasize the following differences:\n- In AugMax, these instance-specific parameters are determined in an adversarial manner. Hence, during training, each sample is trained with its \\textbf{maximum} augmentation level. In contrast, SPAug determines these parameters based on each sample's Cross-Entropy (CE) loss at the previous epoch.\n- Since AugMax determines the instance-specific parameters in an adversarial manner, for each sample, at each training iteration, they need to perform a potentially expensive adversarial attack, which adds computational overhead. In contrast, SPAug uses the sample's previous epoch loss as the proxy value, hence there is little to no computational overhead over standard training.\n- In addition to the above two main differences, AugMax introduces changes to the network architecture to boost its performance. Since, in AugMax, samples are trained with their maximum augmentations, it leads to a deviation of distribution between augmented and input data, which makes the training more challenging, as noted in the paper. To address this issue, they use a different normalization technique termed DuBIN, to disentangle the instance-wise feature heterogeneity of AugMax samples. Since SPAug controls augmentation intensity based on its previous loss values, the augmented views won't deviate much from the original training distribution as in AugMax. In contrast, our method (SPAug) does not require additional modifications to network architecture or the training setup and can be easily integrated to existing training pipelines. When we use the same basic WRN40-2 architecture (with regular 2D batchnorm) that we use in our paper, we find that on CIFAR-100 AugMax has 36.6\\% corrupted error (which is 1.6\\% worst than that of SPAug) . In the next revision, we will include this comparison.\n\nWhen considering the second work (MADAug), we want to draw the reviewer's attention to the fact that it was posted on arXiv on Sat, 9 Sep 2023, which is very close to the ICLR submission date and can be considered as a concurrent related work. Similar to SPAug, MADAug also proposes having instance-wise augmentation parameters, further demonstrating the recent trend in this direction. However, after carefully reviewing their approach, we have observed the following fundamental differences with our work:\n- They also acknowledge that at the beginning of training, samples are not well-fitted; hence applying the same level of augmentations at that phase reduces the final accuracy. Therefore, they propose to use a manually designed probability schedule $p(t) = \\tanh(t/\\tau)$, where $t$ denotes the current training epoch. In addition, MADAug assigns an augmentation probability $p$ and magnitude $\\lambda$ to each sample, determined by the policy network parameterized by $\\theta$, which takes the image features extracted from the task model parameterized by $w$. The bi-level optimization process consists of three steps:\n   1. One-step gradient descent on $w_t$ to achieve a closed-form surrogate $\\hat{w}$ of the lower-level problem solution.\n   2. Updating policy network parameters $\\theta_t$ by minimizing the validation loss computed by the meta-task model $\\hat{w}t$ on a mini-batch of the validation set.\n   3. Updating the main model parameters $w$ based on the parameter $\\theta_{t+1}$ of the policy model.\n\nDue to this bi-level optimization, MADAug requires 2 forward passes for each training minibatch as well as forward passes on the validation minibatch, hence taking approximately $1.5\\times$ longer training time per epoch compared to our method when training Wide-ResNet-28-10 on the CIFAR-10 dataset."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission260/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700690962734,
                "cdate": 1700690962734,
                "tmdate": 1700693105739,
                "mdate": 1700693105739,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jkZ9hVUK3u",
                "forum": "wPq7fkzL2j",
                "replyto": "PpIH9X8TqT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission260/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission260/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 6RhX Q2"
                    },
                    "comment": {
                        "value": "> In the experiments, how is the threshold $\\tau$ that distinguishes easy samples from hard ones in the minibatch determined? Is it affected by the minibatch size? Is there any trade-off between them, considering needed computational resources?\n\nWe thank the reviewer for this interesting question. As noted in the supplimantary material, the threshold $\\tau$ is determined like regular hyperparamter tuning. We find the best threshold $\\tau$ for a given dataset by performing hyperparamter seach over $\\tau$ values ranging from $0.0, 0.1, 0.25, 0.5, 0.75, 0.9, 1.0$. We did not change the batch-size values throughout the experiments and keep it same as the same batch size used in the standard training. \n\nFor example, Table 3 and 4 in **Supplementary Document** shows how the SPAug results varying for hyperparamter tuning on $\\tau$ for CIFAR-10 and CIFAR-100 datasets with AugMix. As shown in the tables, we observed that $\\tau = 0.75$ and $\\tau = 0.9$ works well and can be considered as a good rule of thumb."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission260/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700691684255,
                "cdate": 1700691684255,
                "tmdate": 1700693126744,
                "mdate": 1700693126744,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KDJj2D4Clw",
                "forum": "wPq7fkzL2j",
                "replyto": "PpIH9X8TqT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission260/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission260/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 6RhX Q3"
                    },
                    "comment": {
                        "value": "> In Tables 4 and 5, the results of models trained in the 100 and 200 epochs are given. However, the gained performance through more training epochs is not significant. For instance, in Table 4, the gained C-Err for SPAug-Learnable is on average 0.5, while this value for AA is 0.9. What is the reason for training more epochs to obtain trivial improvement using SPAug-Learnable? Besides, why is the baseline model WRN-28-10 having different C-Err in Table 2 and 4?\n\nThank you for the comment! The reason why we presented the results for 100 and 200 epochs is that we wanted to show that with SPAug, the network tends to converge pretty fast compared to having a uniform augmentation policy, as motivated in the introduction. You are absolutely correct in your assessment that with an additional 100 epochs of training, AA improves by +0.9\\%, whereas with SPAug, it improved by +0.5\\% because, with 100 epochs, it already converged to a better accuracy than AA. Another point we want to emphasize here is that usually when we incorporate an augmentation policy, it requires longer training time (like 200 epochs or more) to converge properly because with the uniform augmentation policy, some samples become much harder to fit, requiring longer training time. However, when we have instance-wise augmentation parameters, not all the samples undergo the same level of augmentation intensity, hence making the convergence easier and faster. We would like to thank the reviewer for this observation!\n\nRegarding your second point about why the baseline numbers in Table 2 and 4 are different, it's due to the stochastic nature of the experiments, and these are average values of two independent sets of 3 trials conducted for AugMix and AA. As we can see, each average accuracy has a non-zero standard deviation, hence the average value can fluctuate slightly. We can see that both average C-Err. values are within their standard deviation (for example, on CIFAR-10 with WRN-28-10 backbone: $23.5\\pm0.9$ vs. $23.3\\pm0.3$)."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission260/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700691726494,
                "cdate": 1700691726494,
                "tmdate": 1700693146551,
                "mdate": 1700693146551,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "m99b0nz8SU",
                "forum": "wPq7fkzL2j",
                "replyto": "PpIH9X8TqT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission260/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission260/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 6RhX Q4"
                    },
                    "comment": {
                        "value": "> How does the method perform on ImageNet with other existing augmentation techniques, and with other architectures (e.g. Transformers)?\n\nThank you for the comment. Following the previous works, we limited our backbones to ResNets and Wide-ResNet (WRN) architectures in the main body. We also included AllConvNet, DenseNet architectures as well in Table 2 of the **supplementary material**.) Similarly, following some of the literature, we have included ImageNet results in Table 3 on the ResNet-50 architecture.  However, we see the value of your comment, and we will try compare the results of SPAug with ViT architectures on ImageNet in the next revision, as it requires longer training time and some time to set up the experiments."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission260/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700691785873,
                "cdate": 1700691785873,
                "tmdate": 1700693164563,
                "mdate": 1700693164563,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]