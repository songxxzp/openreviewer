[
    {
        "title": "To the Cutoff... and Beyond? A Longitudinal Perspective on LLM Data Contamination"
    },
    {
        "review": {
            "id": "XRVbsbQxXL",
            "forum": "m2NVG4Htxs",
            "replyto": "m2NVG4Htxs",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission11/Reviewer_yfjE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission11/Reviewer_yfjE"
            ],
            "content": {
                "summary": {
                    "value": "The paper investigates data contamination of GPT-3.5-Turbo and GPT-4 with problems from Codeforces and Project Euler. It does so by analysing the passrates in relation to the GitHub presence and notes a positive correlation for data before the cutoff, but no significant correlation after this date."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Identifying data contamination is an important issue, especially for evaluation datasets that are often used to create rankings.  \n2. Including problem difficulty as an independent variable is an important step in isolating the confounding effect of item difficulty on pass rates.\n3. I appreciate the openness in referencing blog posts and tweets that anecdotally suggested possible contamination prior to this work"
                },
                "weaknesses": {
                    "value": "1. The methodology is only applied to GPT-3.5/GPT-4, where training details are unknown. In particular, as noted in footnote 1, OpenAI has admitted to using a small amount of data beyond the cutoff date. While I understand the choice of the GPT family as a commonly used model, it would have been better to verify the approach with fully open models where more training details are available (and more trustworthy).\n2. The methodology requires underlying datasets that are longitudinal in nature, i.e. release problems/individual tasks over time; this limits the applicability to sources other than Project Euler / Codeforces."
                },
                "questions": {
                    "value": "### Minor Comments\n* Particularly in section 2, some citations are formatted differently, with the author names outside the parentheses; in sequences of different citations, readability could be improved by using the same citation format as in section 1."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission11/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission11/Reviewer_yfjE",
                        "ICLR.cc/2024/Conference/Submission11/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission11/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697974261321,
            "cdate": 1697974261321,
            "tmdate": 1700680099572,
            "mdate": 1700680099572,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ZFi1oy8DGf",
                "forum": "m2NVG4Htxs",
                "replyto": "XRVbsbQxXL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission11/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission11/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer yfjE"
                    },
                    "comment": {
                        "value": "We thank you very much for your thorough and extensive review. We appreciate that you find our work tackles an important issue, while controlling for confounding variables. We reply to your questions below.\n\n**1. Additional LLMs.**\nThank you for the suggestion! We originally focused on GPT-3.5 and GPT-4 for a few critical reasons: (i) relevance to the large community, in and out of research, that rely on these models, (ii) these models\u2019 high performance on code generation tasks. With respect to this second point, we clarify that when a model\u2019s pass rate on these problems is prohibitively low, we are effectively unable to observe non-trivial trends or extract longitudinal insights\u2013and we have found this to be the case for some additional models we have tested. For example, GPT-3.5 achieved 27% pass rate before the cutoff and 13% after,  while GPT-4 achieved 37% pass rate before and 16% after. In contrast, the pass rates we obtained for Text-Davinci-002 with a comparable prompting strategy are less than 1% before *and* after\u2013yielding it impossible to conduct a substantive analysis featuring functional correctness. \n\nWe also are actively collecting data from some other models, including a modern open-source LLM, and plan on providing these results when the results are available soon (we aim to do so before the end of this discussion window). This discussion is also repeated in the new Appendix B.6.\n\n**2. Longitudinal datasets.**\n\nWe agree that our analysis requires longitudinal datasets; however, we anticipate that the popularity of such benchmarks will continue to rise, as releasing datasets over time is potentially the only foolproof method for avoiding data contamination. For example, creators of recent benchmarks such as [BIG-Bench](https://arxiv.org/abs/2206.04615) are consciously updating their benchmarks over time. Even other non-LLM benchmarks such as [OpenML Benchmarking Suites](https://arxiv.org/abs/1708.03731) are being updated over time, since there is a related push to ensure that the community [does not overfit](https://arxiv.org/abs/2112.01716) to any single set of benchmarks. We are optimistic that as staggered release of benchmark datasets becomes increasingly common and/or an accepted best practice, that the framework we present here can be more widely applied. \n\nFurthermore, we note that while our analysis can only be conducted on longitudinal datasets, its implications extend to any dataset which can be contained in a webscale training dataset; it leverages longitudinal structure in a test benchmark to reveal the massive contamination effect which can be observed on \u201cseen\u201d examples of any dataset.\n\n**3. Minor comments.**\nThank you for catching these formatting issues; they have now been fixed in the updated pdf.\n\nThank you once again, for your excellent points. Please let us know if you have any follow-up questions or further suggestions. We are still conducting new analyses and are excited to update you with their results. We would be happy to continue the discussion."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission11/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700174482528,
                "cdate": 1700174482528,
                "tmdate": 1700174482528,
                "mdate": 1700174482528,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QjzMfYlzfc",
                "forum": "m2NVG4Htxs",
                "replyto": "XRVbsbQxXL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission11/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission11/Authors"
                ],
                "content": {
                    "title": {
                        "value": "New Models"
                    },
                    "comment": {
                        "value": "Thank you again for your thoughtful review. We are writing with more experiments which we have conducted during the rebuttal period which address you concern *W1: Additional LLMs*. We liked your suggestion of adding more LLMs beyond the GPT-4, GPT-3.5, and Davinci models which were in our original manuscript. \n\nWe have now added the analysis of two more LLMs on Codeforces: [Codey/Code Bison](https://cloud.google.com/vertex-ai/docs/generative-ai/code/code-models-overview) (Google\u2019s code generation foundation model, released around the same time as PaLM 2), and [Code-Llama](https://huggingface.co/codellama). \n\nAs for Code Bison, we very interestingly observe the same behavior that we did for GPT-4 and GPT-3.5-Turbo. Specifically, after the training cutoff of Code Bison, we see that the GitHub presence metric is no longer a significant predictor of functional correctness for this model. code-bison@001\u2019s training cutoff, although not publicly known, is probably around February 2023, as this model was released [within weeks](https://cloud.google.com/vertex-ai/docs/generative-ai/learn/model-versioning) of text-bison@001 and chat-bison@001, which have [known training cutoffs of February 2023](https://cloud.google.com/vertex-ai/docs/generative-ai/learn/models). Accordingly, we conducted our analysis assuming this February 2023 cutoff. We have updated our manuscript to show this by updating Figures 2 and including all additional figures and tables in the appendix as with the GPT models. \n\nThe results from this additional model shows that the training cutoff itself is a moderating factor in the code generation completion performance. This conclusion is further bolstered by the almost 2 year separation between the GPT cutoff and the Code Bison cutoff and yet still producing the same qualitative finding.\n\nWe additionally worked with Code-Llama and attempted to the best of our abilities to get it to perform on these questions. Unfortunately, even on the largest and most capable model for our use case, 34b-Instruct, we could not get Code-Llama to output answers which yielded pass rates above 1% on Codeforces. This result follows that of our observation around Text-Davinci-002.\n\nWe hope that these additional experiments address your main concern about the limited number and models in our experiments. We now conduct analysis of 5 LLMs with various cutoff dates, and we observe signs of contamination for those models which yield pass rates above 1%. We look forward to answering any additional questions you may have, before the author response period closes tomorrow (end of day Nov 22nd). Thanks!"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission11/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700613840698,
                "cdate": 1700613840698,
                "tmdate": 1700613840698,
                "mdate": 1700613840698,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "D9rEeOBchE",
                "forum": "m2NVG4Htxs",
                "replyto": "QjzMfYlzfc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission11/Reviewer_yfjE"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission11/Reviewer_yfjE"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your comments and updated results. I will update my score accordingly.\n\nJust as a remark:\n\n> We additionally worked with Code-Llama and attempted to the best of our abilities to get it to perform on these questions. Unfortunately, even on the largest and most capable model for our use case, 34b-Instruct, we could not get Code-Llama to output answers which yielded pass rates above 1% on Codeforces. This result follows that of our observation around Text-Davinci-002.\n\nI suspect this may be an artifact of \"pass rate\" being a discontinuous metric (on a single sample) that cannot adequately measure *partial* progress. It would be interesting to look for a smoother metric. A simple choice might be to look at the perplexity of a/the correct solution, but I am not sure how well that would be calibrated."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission11/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700680081989,
                "cdate": 1700680081989,
                "tmdate": 1700680081989,
                "mdate": 1700680081989,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "gb4bzg4HXx",
            "forum": "m2NVG4Htxs",
            "replyto": "m2NVG4Htxs",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission11/Reviewer_EfiM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission11/Reviewer_EfiM"
            ],
            "content": {
                "summary": {
                    "value": "Assess whether GPT performance at coding (sometimes called program synthesis) was possibly affected by contamination of pretraining data using a naturally occurring experiment (i.e. comparing scores before and after the pretraining knowledge cutoff dates)."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Overall, I really liked this paper. I thought it was well motivated, clearly conceptualized, well executed and somewhat thorough. I have a small number of requested changes, and if the authors and I agree that the changes are sensible and if the authors agree to make the changes, I would be happy to increase my score."
                },
                "weaknesses": {
                    "value": "> Figure 1: Marginal Effects of Pass Rate Metric \n\nI think this is an amazing figure. 5 comments, ordered from minor to major:\n\n1. easy: Stacking log(Github Presence) and log(Difficulty) at the bottom makes reading the figure tricky. I might suggest moving log(Difficulty) to the right side.\n\n2. easy: GitHub is stylized \"GitHub\", not \"Github\"\n\n3. medium: Where is the equivalent plot for Project Euler? I might have missed this, but I cannot find it in the main text or appendix.\n\n4. hard: The pass rate is significantly lower for easy and medium problems, even for log(Github Presence) = 0. I understand that GitHub Presence is a proxy, but I would think that log(GitHub Presence) = 0 is our best guess for \"low or no contamination\", but there's still a 10-20% decrease in pass rate. Why? I can think of 2-4 possible answers: (a) GPT-4 genuinely becomes much worse after the knowledge cutoff; (b) GitHub presence is inadequate and/or misleading, (c) the distribution of Codeforce problems changed after GPT-4 was finished pretraining, or (d) something changed in how the pass rate is calculated on generated outputs. More explanations might also be possible. Is there some way for the authors to try to investigate the cause of this shift?\n\n5. hard: I was hoping for either a qualitative or quantitative analysis about what GPT-4 is outputting on Codeforces problems released after the cutoff, but I can't find even a single example of the raw generated outputs. Could the authors please provide some manual examples, even in the appendix, to convincingly demonstrate that GTP-4 is indeed outputting worse code? I want to rule out that silly possibilities (e.g., a shift in formatting) are affecting the results.\n\n> Table 1\n\nI personally find Tables are less effective at communicating than Figures. Since these are regression tables, could you possibly consider switching to a Forest plot of regression coefficients? Some random examples here:\n\n- https://www.researchgate.net/figure/Forest-plot-of-regression-coefficients-95-confidence-interval-for-the-association_fig1_331119872\n- https://www.researchgate.net/figure/Coefficient-plots-from-linear-regression-predicting-what-makes-an-interaction-meaningful_fig1_343608677 \n- http://www.strengejacke.de/sjPlot/reference/plot_models.html.\n\nTo make my suggestion as concrete as possible, using terminology from matplotlib & seaborn (assuming you're using Python, but I'm sure R could do this as well), I'm specifically thinking that your X axis should be the estimated parameters and confidence intervals, Y would be the covariates (i.e. Difficulty & GitHub presence), the Hue is either Before Cutoff or After Cutoff, and you have two side-by-side axes, one for GPT4 and the other for GPT3.5.\n\nI personally would prefer all regression tables to be visualized as such (Tables 1, 2, and those in the appendix)."
                },
                "questions": {
                    "value": "Not a question, but I want to note that:\n\n1. I like the use of Pass Rate in lieu of pass@1. I think that's a very sensible choice.\n\n2. I like the citation of Horace He's and Chris Cundy's tweets. Very good scholarship, even if Tweets aren't \"published\" in a traditional sense."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission11/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission11/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission11/Reviewer_EfiM"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission11/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698684490826,
            "cdate": 1698684490826,
            "tmdate": 1699635924418,
            "mdate": 1699635924418,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "AIrmfWxXs3",
                "forum": "m2NVG4Htxs",
                "replyto": "gb4bzg4HXx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission11/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission11/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer EfiM (1/2)"
                    },
                    "comment": {
                        "value": "We thank you very much for your thorough and enthusiastic review. We appreciate that you find our work well motivated, clearly conceptualized, and well-executed. We agree with all of your suggestions and have done our best to address them below in and in the updated manuscript. Please let us know if we have addressed your comments.\n\n**W1+2: Fig 1 minor changes.**\nThank you for pointing out these stylistic changes. We have now moved the legend and updated \u201cGithub\u201d -> \u201cGitHub\u201d.\n\n**W3: Fig 1 for Project Euler.**\nThank you; this plot can be found in Figure 17; the associated plot for GPT3.5 can be found in Figure 19. Here you\u2019ll see how the performance of these systems on Project Euler is much decreased on problems released before the training cutoffs, and entirely wiped out for problems released after the cutoff \u2014 neither GPT-4 or GPT-3.5 yield any pass rates above 0 for problems after the training cutoff. \n\n**W4: Pass rate when log(Github Presence)=0.**\nThis is a great insight! We believe that the main factor at play is b) - GitHub Presence is a strong indicator of availability in the training set, but it still underestimates the true availability in the training set. For example, *all* Codeforces problems show up on the internet in several places (e.g., [here](https://cf.kira924age.com/#/table/), [here in pdf](https://github.com/AliOsm/PDF-CodeForces-Problems)). Therefore, we believe this is a major reason for why the completion rate is higher before the cutoff even for problems with log(GitHub Presence)=0.  We comment on your other hypotheses as well.\n1. In terms of the models themselves, we used the exact same models when evaluating problems released before and after the cutoff date (in fact, the evaluations were performed in one large batch and later separated by date).\n2. As we state above, we believe that this is the main factor. We agree that \u201cGitHub presence\u201d may under-estimate the total presence of a Codeforces problem in the GPT training data. For example, *all* Codeforces problems show up on the internet in several places (e.g., [here](https://cf.kira924age.com/#/table/), [here in pdf](https://github.com/AliOsm/PDF-CodeForces-Problems)).\n3. It is challenging to fully rule out this possibility, but qualitatively, we cannot spot any change over time in the type of Codeforces problems released. It is possible that a difference in problems does cause a small change in pass rate after the cutoff, but we believe that the majority of the observed difference in pass rate can be attributed to (2).\n    1. To investigate, we conduct a set of additional experiments to assess whether the distribution over tags (only available for Codeforces) and/or difficulty level (available for Codeforces and Project Euler) changed in a statistically significant way during the post-cutoff period, relative to the pre-cutoff period. We present this analysis in its entirety, consisting of qualitative plots and  $\\chi^2$ tests, in Appendix B.7 of our updated submission pdf, and summarize key findings below:\n        - For **Codeforces**, we do not find any statistically significant difference in the distribution of normalized counts over problem tags during the pre-vs. post- period. We also do not find any statistically significant difference in the distribution over discretized difficulty scores during the pre-vs. Post-period. \n        - For **Project Euler**, problem tags are not publicly available, so we cannot perform tag analysis. We do not find any statistically significant difference in the distribution over discretized difficulty scores during the pre-vs. Post-period.\n        - These findings help to mitigate concerns that the drop-off in performance we observe might be attributable to significant changes in the distribution over tags (for Codeforces) and/or over difficulty levels during the post-cutoff period. \n\t\t\t\n4. We ran the same evaluation script for all problems, and only then separated by date: https://anonymous.4open.science/r/to-the-cutoff-review-253A/eval/chronological_evaluation/chronological_dataset.py (line 162).\n\n(1/2)"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission11/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700174322388,
                "cdate": 1700174322388,
                "tmdate": 1700174322388,
                "mdate": 1700174322388,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hd3M3a61gT",
                "forum": "m2NVG4Htxs",
                "replyto": "gb4bzg4HXx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission11/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission11/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer EfiM (2/2)"
                    },
                    "comment": {
                        "value": "**W5: GPT-generated outputs on problems released after the cutoff.**\n\nTo address this question, we would like to: (a) make a minor methodological clarification; and (b) note that we have added a new section to the appendix (B.8) that contains numerous examples of GPT-4 and GPT-3.5.-Turbo output (i.e., generated code) for Codeforces problems released before and after the GPT training cutoff. (We restrict our attention to Codeforces because our Project Euler analysis focused on the correctness of generated numerical solutions, rather than evaluating the outputs of generated code). \n\nWith respect to (a), we note for clarity that what we are trying to control for in our natural experiment are factors with the potential to influence an LLM\u2019s ability to produce a functionally or numerically correct solution to a problem (i.e., the problem\u2019s difficulty), and/or the likelihood that the LLM would have seen this problem during training (i.e., GitHub presence, since scraped GitHub repositories are part of the GPT training corpus). Thus, GPT need not produce *worse* code when we evaluate its ability to produce functionally correct solutions for problems released after the cutoff. Indeed, given that we perform all of our evaluations at a single point in time, the code-generating ability of each GPT instance is \u201cfixed\u201d--what is (potentially) changing is the artificially inflated performance benefit (or lack thereof) the model may demonstrate when it is evaluated on examples it *has* seen during training versus those it has not. \n\nWith respect to (b)---i.e., our new appendix section\u2014because the full description and generated output associated with a given problem can both be quite lengthy, we have constructed this new section by partitioning the Codeforces problems and along two dimensions: (1) release date pre- vs. post GPT cutoff; (2) discretized LLM functional correctness. With respect to (2), for a given LLM and problem, functional correctness is computed as the ratio of test cases that the LLM\u2019s generated code passes (see Section 4 for details). Thus, functional correctness takes values in [0,1.0]. We discretize via the following mapping:\n\n$\\lambda: x \\mapsto 0 | (x \\leq Q1) \\lor 1 | (Q1 < x \\leq Q2) \\lor 2 | (Q2 < x \\leq Q3) \\lor 3 | x > Q3$\n\nwhere $x$ represents a given problem's raw difficulty score, and Q1, Q2, and Q3 correspond to the first, second, and third quartiles, respectively. \n\nWe thus consider 8 subgroups per model (i.e., {pre, post} x {0,1,2,3}), and draw two examples uniformly at random from each subgroup. We present each generation along with corresponding problem title, ID, difficulty score, url, and the LLM\u2019s functional correctness score. Our intent here is to avoid cherry-picking of results, while facilitating the visualization of a representative set of generations. We also note that the generated code produced by each model for *every* Codeforces problem is available in the results file that we provide as part of our supplementary material. We view additional/comparative static and behavioral analyses of these outputs as a promising direction for future work.\n\n\n**Table 1.**\nThanks once again for this suggestion! We have now added these figures. We replaced Table 1 and Table 2 with their corresponding forest plots. Additionally, we have added the forest plots into the appendix for every regression coefficient table. These figures give new visual insights so the reader can quickly understand the relative relationship between the regression coefficients. For example, we can easily see that the coefficients for both Difficulty and GitHub presence get closer to 1 (the null effect) after the cutoff. \n\nThank you once again, for your excellent points. Please let us know if you have any follow-up questions or further suggestions. We are still conducting new analyses and are excited to update you with their results. We look forward to continuing the discussion.\n\n(2/2)"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission11/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700174376548,
                "cdate": 1700174376548,
                "tmdate": 1700174376548,
                "mdate": 1700174376548,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7nIlxdc76M",
                "forum": "m2NVG4Htxs",
                "replyto": "AIrmfWxXs3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission11/Reviewer_EfiM"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission11/Reviewer_EfiM"
                ],
                "content": {
                    "title": {
                        "value": "Response to Authors (1/2)"
                    },
                    "comment": {
                        "value": "Overall, I'm happy with your changes.\n\n> W1+2: Fig 1 minor changes.\n\nWonderful - thank you!!\n\n> W3: Fig 1 for Project Euler. Thank you; this plot can be found in Figure 17; the associated plot for GPT3.5 can be found in Figure 19\n\nI think the figure numbering might have changed between your answer and the revised manuscript. I see Figure 10 has a caption \"Figure 10: Marginal Effects of Pass Rate for GPT-4 on the Project Euler Dataset\" but a title of \"Functional Correctness Marginal Effects Plots for GPT\u22124 on Codeforces\". Is this figure for Euler or Codeforces?\n\n> W4: Pass rate when log(Github Presence)=0. This is a great insight! We believe that the main factor at play is b) - GitHub Presence is a strong indicator of availability in the training set, but it still underestimates the true availability in the training set.\n\nThis seems like a reasonable answer."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission11/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700439565435,
                "cdate": 1700439565435,
                "tmdate": 1700439565435,
                "mdate": 1700439565435,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TsTxMJs19C",
                "forum": "m2NVG4Htxs",
                "replyto": "gb4bzg4HXx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission11/Reviewer_EfiM"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission11/Reviewer_EfiM"
                ],
                "content": {
                    "title": {
                        "value": "Response to Authors (1/2)"
                    },
                    "comment": {
                        "value": "> Figure 2\n\nI think this looks very nice. Thank you!! Same with Figure 3.\n\n> Thus, GPT need not produce _worse_ code when we evaluate its ability to produce functionally correct solutions for problems released after the cutoff. \n\nI'm not sure I understand this point. We might be using different terminology. I agree that the evaluations are run at a single point in time, and I believe your evidence (e.g., Figure 1) that increasing GitHub presence is correlated with functional correctness. What I was trying to confirm is that generated code for problems released after cutoff is \"worse\" in the sense that we would all agree the generated code for problems released after cutoff pass fewer test cases than generated code for problems released before cutoff. My intention was to rule out that the evaluation process itself somehow (potentially unintentionally) affected the performance.\n\nFor example, suppose that before the cutoff, Codeforces required code to be submitted in format A (e.g., the main function should be called `main()`), but after the cutoff, Codeforces required code to be submitted in format B (e.g., the main function should be called `start()`). If so, I think it would be hardly surprising if a model pretrained on format A would perform less well when evaluated on format B, since the model wouldn't know that format A is deprecated and format B is appropriate?\n\nThis is what I meant by confirming that the code post-cutoff is \"wrong\". Perhaps \"less functionally useful\" is a better term. I want to know that the generated code for problems post-cutoff is functionally less useful than code for problems pre-cutoff. I'm not interested in things like formatting, variable name choice, etc. that we might consider when discussing code quality.\n\nCould you please clarify what you meant by \"GPT need not produce _worse_ code\" post-cutoff? How is GPT-4 scoring worse post-cutoff if it isn't producing worse code?\n\n\n\nI went to increase your score to 7, but apparently 7 is not an option for ICLR. If we can reach agreement on this topic, and I think we can, then I would be happy to bump you up to an 8."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission11/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700440307834,
                "cdate": 1700440307834,
                "tmdate": 1700440626223,
                "mdate": 1700440626223,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "M4jPZxVj0f",
            "forum": "m2NVG4Htxs",
            "replyto": "m2NVG4Htxs",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission11/Reviewer_XbnJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission11/Reviewer_XbnJ"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a detailed investigation into data contamination in large language models (LLMs), using GPT model training cutoffs to analyze benchmarks released over time. It examines two datasets, Codeforces and Project Euler, revealing clear patterns that suggest contamination based on the LLMs' pass rates correlated with benchmarks' GitHub popularity and release dates. The authors provide a comprehensive dataset, findings, and a framework for future analysis, promoting better practices for benchmark releases in the era of web-scale LLM training."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The idea to investigate data contamination in LLMs via cutoff datasets makes sense and is interesting, which guarantees that the testing data are not available in the training set of LLMs. And the findings are surprising, revealing that people should deal with the ability of LLMs more carefully. This study shows that LLMs are likely to have generalization problems as well as traditional ML models and deep neural networks. And I think this should raise the attention of ML researchers."
                },
                "weaknesses": {
                    "value": "I am not quite familiar with LLMs, and I only have one question about the design of cutoffs. What if a code problem released later is exactly similar as some problems that has already existed? And how to measure the data contamination problem is also important."
                },
                "questions": {
                    "value": "Please refer to Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission11/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698873037377,
            "cdate": 1698873037377,
            "tmdate": 1699635924329,
            "mdate": 1699635924329,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "WFd2jpAc4g",
                "forum": "m2NVG4Htxs",
                "replyto": "M4jPZxVj0f",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission11/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission11/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer XbnJ (1/2)"
                    },
                    "comment": {
                        "value": "We thank you for your thoughtful review. We appreciate that you find our longitudinal study interesting and that it should be raised to the attention of ML researchers. \n\n**1. Design of the cutoff: similar code problems.**\nThank you for the great question, and we understand that your question lies in the methodology of our analysis. \n\nRecall that we propose a method to measure contamination through a natural experiment where the training cutoff bifurcates an evaluation set. We appreciate that you agree with this methodological approach and our resulting analysis, and we agree that checking for duplicated questions before and after the cutoff is an important step. \n\nIn summary, based on your suggestion, we did find 56 exact duplicates, none of which straddle the cutoff (0.7% of the total set of problems). Community experts identified 8 similar questions in the Codeforces problems; only 5 straddled pre- and post-cutoff. The overall effect on our results should be minimal considering the large drop-off in performance after the cutoff. In the coming days, we will rerun our analysis and update you and the paper before the end of the rebuttal period. We explain more below.\n\nFor *textual duplicates*, we have confirmed that there are none in Project Euler. In Codeforces, we have identified a set of 40 unique problems (out of a total of more than 8,300) for which the dataset contains multiple (i.e., 2-3) copies. This can occur if a competition problem is later re-posted as part of a practice set. All duplicated tuples belong to the subset of problems released before the GPT training cut-off. As the focus of our analysis is on comparing performance on examples released before versus after the training cutoff, we would be concerned if a majority of these examples were \u201ccut-crossing\u201d, but do not find that to be the case. Before the discussion period concludes, we will re-run our analyses omitting the duplicates, and will provide you with updated results. We expect that the impact of removing 56 observations (corresponding to the duplicates of the aforementioned 40 problems) from the pre-cutoff subset which contains >6,000 observations will be minimal, and do not expect the qualitative nature of our conclusions to change. \n\nAs for *semantic duplicates*, it is certainly non-trivial to check for the existence of such problems. Thankfully, the Codeforces community has [discussed](https://codeforces.com/blog/entry/113016) this topic before, highlighting 8 near-duplicate pairs, of which 5 were pairs with one question on either side of the cutoff, and the other three were pairs with both questions were released before the cutoff. \n\n\nFor the sake of examining those 5 pairs which straddled the cutoff, we compare GPT-4 performance:\nPre-Cutoff Problem | Post-Cutoff Problem | Pre-Cutoff Problem Pass Rate | Post-Cutoff Problem Pass Rate\n|-|-|-|-|\n765_F | 1793_F | 0.50 | 1.00\n652_C | 1771_B | 0.38 | 0.00\n342_E | 1790_F | 1.00 | 0.50\n923_B | 1795_C | 0.17 | 0.00\n1462_C | 1714_C | 0.00 | 0.00\nMean before cutoff: 0.41, Mean after cutoff: 0.30\n\nWe also compare GPT-3.5-Turbo performance:\nPre-Cutoff Problem | Post-Cutoff Problem | Pre-Cutoff Problem Pass Rate | Post-Cutoff Problem Pass Rate\n|-|-|-|-|\n765_F | 1793_F | 0.50 | 1.00\n652_C | 1771_B | 0.00 | 0.00\n342_E | 1790_F | 0.00 | 0.50\n923_B | 1795_C | 0.33 | 0.00\n1462_C | 1714_C | 0.14 | 0.00\nMean before cutoff: 0.20, Mean after cutoff: 0.30\n\nA potential risk of including such semantically similar problem instances in our evaluation is that the performance on post-cutoff examples would be upwardly biased, since the model is actually seeing examples which are quite similar to those it has seen in training, despite our best efforts to control for such exposure via our cutoff-based partition of the dataset. The small size of this subset means this effect, were it to exist, would be relatively minimal, and it would be biased against, rather than in favor of, the conclusions we ultimately draw. As such, we do not feel that the presence of such examples undermines our analysis. We will provide an updated analysis in the coming days to confirm.\n\nFinally, we\u2019d like to acknowledge that we agree with your broader point -- i.e., the question of *how* to measure or detect when data contamination has occurred is important. We contend that our approach, which leverages the cutoff date as a source of naturally arising variation, should be viewed as a compelling complement to existing approaches, which often rely on the ability to manipulate the underlying dataset and/or synthetically introduce contamination and then measure the impact on downstream tasks. We are able to determine that contamination is likely to have occurred by evaluating performance leveraging a freely available feature of the dataset (i.e., each problem\u2019s release date, relative to the cutoff), without requiring any sort of post-facto intervention or manipulation. \n\n\n\n(1/2)"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission11/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700174150480,
                "cdate": 1700174150480,
                "tmdate": 1700174150480,
                "mdate": 1700174150480,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QcgjgTUZBl",
                "forum": "m2NVG4Htxs",
                "replyto": "M4jPZxVj0f",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission11/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission11/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer XbnJ (2/2)"
                    },
                    "comment": {
                        "value": "**2. Additional experimental updates**\nFinally, we would like to highlight a few additional experiments and updates we finished during the rebuttal period:\n1. We have rendered the coefficient tables as forest plots (suggested by EfiM) and supplemented the regression tables with them throughout. See Figures 2 and 3 in the main paper, and associated figures in the Appendix. This change is very helpful for the reader to quickly visualize the regression coefficients and see how after the coefficients become closer to 1 (or have no effect) after the cutoff. \n2. We have conducted new analyses to assess whether the drop-off in performance that we observe for problem examples released after the GPT training cutoff might be attributable to (potentially latent) covariate shifts. \n3. We added a new section B.6 to describe our experiments with open source LLMs; we also have plans to expand to other LLMs during the rebuttal period and are awaiting results.\n4. We added examples of generations from the LLMs in Section B.8.\n\n\nThank you once again, for your excellent questions. Please let us know if you have any follow-up questions or further suggestions. We would be happy to continue the discussion, and we will update you with our reanalysis soon.\n\n(2/2)"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission11/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700174196156,
                "cdate": 1700174196156,
                "tmdate": 1700174196156,
                "mdate": 1700174196156,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "P1VBi6DiNV",
                "forum": "m2NVG4Htxs",
                "replyto": "ufoSHwnvOs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission11/Reviewer_XbnJ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission11/Reviewer_XbnJ"
                ],
                "content": {
                    "title": {
                        "value": "Thank you"
                    },
                    "comment": {
                        "value": "Thanks for your rebuttal, and most of my concerns are addressed. Since I'm not an expert in this field, I would like to maintain my score."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission11/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700712186564,
                "cdate": 1700712186564,
                "tmdate": 1700712186564,
                "mdate": 1700712186564,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "mQVSo5ovS7",
            "forum": "m2NVG4Htxs",
            "replyto": "m2NVG4Htxs",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission11/Reviewer_VA65"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission11/Reviewer_VA65"
            ],
            "content": {
                "summary": {
                    "value": "The paper conducted longitudinal analysis of data contamination in large language models (LLMs), a problem where models are evaluated using data that they may have been trained on, thus overstating their capabilities.  The authors leveraged natural experiments provided by the training cutoff dates of models like GPT-3.5 and GPT-4 to study contamination. They analyzed Codeforces and Project Euler, websites that release code problems over time, and find evidence of contamination based on the pass rate of LLMs for problems released before their training cutoff dates. The study demonstrates statistically significant associations between a problem\u2019s presence on GitHub and LLM performance for pre-cutoff problems."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1: The analysis from longitudinal perspective is novel. \n2: The comprehensive experiments, large-scale dataset and code base provided by this work will definitely benefit the community of contamination analysis.\n3: This paper is well organized and easy to understand."
                },
                "weaknesses": {
                    "value": "1: The results are interesting but not that surprising. Many blogs or discussion in the community about Data Contamination has involved similar results.\n2: There is lack of depth analysis about how implicit contamination is possible. If some real examples can be extracted to show how this could happen, it will be much better.\n\nOverall, I do appreciate the effort to investigate the Data Contamination problem from longitudinal side and open-source data/codes. The experiments also show intriguing results. But I believe the contribution of this paper is not enough to be accepted by ICLR, for its limited scope and technical novelty. It's limited to Code datasets. And the only novelty is how to split the \"train\" and \"test\" set."
                },
                "questions": {
                    "value": "N/A"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission11/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission11/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission11/Reviewer_VA65"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission11/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699203471923,
            "cdate": 1699203471923,
            "tmdate": 1699635924234,
            "mdate": 1699635924234,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2AwqqIvl4C",
                "forum": "m2NVG4Htxs",
                "replyto": "mQVSo5ovS7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission11/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission11/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer VA65 (1/2)"
                    },
                    "comment": {
                        "value": "We thank you for taking time to review the manuscript and provide valuable feedback. We are glad to see that you find our work to be beneficial to the community of contamination analysis, our results intriguing, and our paper well-written. We reply to each of your questions below.\n\n**1. results interesting but not surprising.**\nThank you, we agree that the results are interesting. While we understand that the results may not be surprising to you, they are to other reviewers; XbnJ described \u201cthe findings are surprising, revealing that people should deal with the ability of LLMs more carefully\u201d. More importantly, our work is a non-trivial contribution to the community that presents the first scientifically rigorous confirmation of the phenomena that social media posts have speculated about via small-scale/ad hoc analyses. While many informal statements have been made about GPT-4 memorization, we show the first statistically significant differences in performance on problems released before and after the cutoff date. Furthermore, our work lays the groundwork for rigorous analysis of contamination and best practices in the age of LLMs trained on webscale data. Our methodology is becoming increasingly relevant as the community acknowledges the limitations of static benchmarks for LLM evaluation, and shifts toward dynamic/longitudinal benchmarks such as the ones we construct, open-source, and analyze here.\n\n**2. On how implicit contamination is possible.**\nTo begin, we\u2019d like to clarify our intention for including the words \u201cimplicit\u201d and \u201cexplicit\u201d in our abstract, specifically in reference to how a given problem might come to be included in a model\u2019s training corpus. Our intention here was to acknowledge that the process of collecting webscale data can lead to the inclusion (in the training set) of examples that were never *intentionally* selected for training\u2013for example, we consider the unintentional inclusion of BIG-bench examples in the GPT-4 training corpus ([OpenAI, 2023](https://arxiv.org/abs/2303.08774)). Through re-posting, removal of watermark information, indirect scraping, or other means, information intended to be omitted from LLM scraping is rarely truly safe. For clarity, we have modified our abstract text to refer to examples that are \u201cintentionally\u201d (explicitly) or \u201cunintentionally\u201d (implicitly) included in the training data. \n\nOn a separate note, we have newly added Appendix B.8 which includes a number of samples of generated outputs from GPT-4 and GPT-3.5-Turbo. These can be used to qualitatively examine their outputs across a range of pass rates on both datasets. \n\n**3. On scope and novelty.**\nThank you for your comments. We focus on code generation for a few key reasons: (i) it is a very popular use-case; (ii) most recent LLMs have code as a major part of their training data; (iii) unlike general online natural language text, GitHub has a uniform interface and is easily scrapable and cleanable, making it simultaneously easy for GitHub solutions to be added to train datasets as well as for us to assess GitHub presence; and (iv) code generation and problem solving datasets have objective correctness metrics (test cases) producing objective evaluations of open-ended generations that don\u2019t require another model or human in the loop. \n\nWe also note that dataset desiderata for the methodological approach we propose include: (i) problems must have been released over a sufficiently long time-horizon, such that it is possible to partition examples into pre- and post- GPT training cutoff subsets based on problem release date.  This restrictions precludes some other popular benchmarks which have been released in a single time-step (e.g. [HumanEval](https://github.com/openai/human-eval) or [MBPP](https://arxiv.org/pdf/2108.07732.pdf)). (ii) problems should consist of high-quality questions requiring non-trivial solution generation that admit objective evaluation functions/correctness measures.\nWe have thus focused our efforts on the non-trivial scraping and processing required to analyze Project Euler and Codeforces datasets in the way that we have. We also emphasize that our work introduces tools and paves the way for further rigorous study related to data contamination, which gives our work the potential to have high impact in the community.\n\nWe\u2019d also like to highlight that our paper should also be viewed as a methodological one. Specifically, we believe our work takes a novel view on data contamination estimation by employing a natural experiment \u2013 a concept borrowed from the economic literature. We use this concept effectively here and argue for its further use, particularly in experimenting, evaluating, and discovering the intricacies of LLMs. We argue that this evaluation methodology should be used in particular on closed-source models which refuse to reveal critical and important details about their development/training. We have updated our manuscript to highlight this point further.\n\n(1/2)"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission11/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700173950717,
                "cdate": 1700173950717,
                "tmdate": 1700173950717,
                "mdate": 1700173950717,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FwHY9yFTks",
                "forum": "m2NVG4Htxs",
                "replyto": "mQVSo5ovS7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission11/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission11/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer VA65 (2/2)"
                    },
                    "comment": {
                        "value": "Thank you very much, once again, for your helpful feedback. If you find our responses satisfying, we respectfully ask that you consider increasing your score. We are still conducting new analyses and are excited to update you with their results. We would be very happy to answer any follow-up or additional questions you have.\n\n(2/2)"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission11/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700174025495,
                "cdate": 1700174025495,
                "tmdate": 1700174025495,
                "mdate": 1700174025495,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]