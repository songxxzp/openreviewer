[
    {
        "title": "A Language-Agent Approach to Formal Theorem-Proving"
    },
    {
        "review": {
            "id": "NhVGVdgPTC",
            "forum": "XCMbagV0No",
            "replyto": "XCMbagV0No",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7710/Reviewer_4t7K"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7710/Reviewer_4t7K"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces COPRA, an approach to theorem proving that uses off-the-shelf, high-capacity LLM (GPT-4 in this case) as part of a policy that interacts with a proof environment. \nAt each step, the policy consumes a textual prompt by using the underlying proof assistant, or backtrack, or retrieve relevant lemmas and definitions from an external corpus. \nThe feedback from the execution is used toconstruct a new prompt for the policy, and the process reiterates.\nThe proposed approach is evaluated empirically."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1) The problem is rigorously formalised as a Markov decision process in reinforcement learning. \n\n2) The proposed approach compares favourably wrt the state of the art, but the differences between the different baselines make the comparison a bit opaque."
                },
                "weaknesses": {
                    "value": "1) The problem is formalised as an RL task, but then the authors say \"In this paper, we do not take on this problem. Instead, we consider a fixed policy - a wrapper around a pretrained LLM (GPT-4) that can learn in-context - and show that this policy can achieve a high reward\".\nSee question below.\n\n2) The structure of the framework proposed is not very clear. The algorithm in Fig. 3 is quite high-level. The calls and interactions with the LLMs are not discussed in much detail.\n\n3) on p. 9, the authors say: \"However, [COPRA] departs from these prior methods in using execution feedback and a more sophisticated search algorithm.\"\nIt is not clear to me what this more sophisticate search algorithm is, specifically why it is sophisticate."
                },
                "questions": {
                    "value": "1) Why defining the problem as an RL task, if this is not used in the methodology proposed?\n\n2) What is sophisticate about the search algorithm used in COPRA?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7710/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7710/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7710/Reviewer_4t7K"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7710/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698507453491,
            "cdate": 1698507453491,
            "tmdate": 1699636940028,
            "mdate": 1699636940028,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "PIgqrSHS0D",
                "forum": "XCMbagV0No",
                "replyto": "NhVGVdgPTC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7710/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7710/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your valuable feedback. We respond to your comments and questions below:\n   \n1. \u201cWhy define the problem as an RL task, if this is not used in the methodology proposed?\u201d\n\n**Ans.** Our approach is inspired by recent \u201cverbal RL\u201d approaches [1, 2] that use LLMs to perform control tasks (like game-playing) that were traditionally solved using RL. The big idea here is that unlike in traditional deep RL, there is no gradient-based policy update descent. Instead, RL-style exploration is combined with in-context learning: the agent interacts with the world, collects information, and then updates the LLM's prompt to elicit new behavior. \n\nThe MDP formulation associates our approach with this perspective. The MDP captures **interactions** between our LLM and the external world (the proof environment), which form the primary difference between our approach and prior LLM-based approaches to theorem-proving. The sentence \"Instead, we consider a fixed policy...\" captures this essential difference between our method and classical RL approaches where one updates the policy iteratively.  \n\n2. \u201cThe structure of the framework proposed is not very clear. The algorithm in Fig. 3 is quite high-level. The calls and interactions with the LLMs are not discussed in much detail.\u201d\n\n**Ans.** Figure 4 shows some of the interactions with the LLMs, we can add more details in the appendix. Please mention any specific detail you would like to see, and we would be happy to add that information to the paper.\n\n3. \"What is sophisticated about the search algorithm used in COPRA?\u201d\n\n**Ans.** The search algorithm maintains a stack and keeps track of previous failures, and current execution feedback. This information is then added back to the prompt helping the guidance model (GPT-4) to better direct the search. The algorithm also uses a partial order over goals (originally introduced in Proverbot) to ensure that tactics actually simplify the proofs. Most of the previous approaches tend to use a stateless best-first search strategy, but our search uses a form of stack-based DFS and is stateful (by backtracking when the model starts repeating the same failed steps for any given proof state). This not only optimizes the number of inference steps needed to find a proof but also lets us fail fast when the proof is too hard to find (see Tables 1 and 2 for comparison).\n\nReferences:\n\n[1] Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. arXiv preprint arXiv:2303.11366, 2023.\n\n[2] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. arXiv preprint arXiv:2305.16291, 2023."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7710/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699987233118,
                "cdate": 1699987233118,
                "tmdate": 1699987233118,
                "mdate": 1699987233118,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5Zcbk0sb6Z",
                "forum": "XCMbagV0No",
                "replyto": "PIgqrSHS0D",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7710/Reviewer_4t7K"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7710/Reviewer_4t7K"
                ],
                "content": {
                    "title": {
                        "value": "Follow up"
                    },
                    "comment": {
                        "value": "Thanks to the authors for their replies and sorry for low responsiveness.\n\n- MDPs: these are described in Sec. 2.2 and mentioned only 2 other times in the paper, one in the conclusions. So, the point is: what do you do with these MDPs that appear (practically) nowhere else in the paper?\n\n- Algorithm: I think this should appear in the main body of the paper, rather than in the appendix, as it is one of the main contributions of the paper. But please correct me if I am wrong. I am afraid it is not up to me to decide what should appear in the algorithm or the level of detail.  \n\n- Details: these are the details that it would be good to see in the description of the algorithm. I don't think these are mentioned anywhere in the paper right now, and therefore it is difficult to judge whether the algorithm is sophisticated or not."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7710/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700638024671,
                "cdate": 1700638024671,
                "tmdate": 1700638024671,
                "mdate": 1700638024671,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "QeMWSefAcK",
            "forum": "XCMbagV0No",
            "replyto": "XCMbagV0No",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7710/Reviewer_VHEv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7710/Reviewer_VHEv"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces COPRA, a language-agent framework that leverages the LLM GPT-4 for state-of-the-art performance in formal theorem-proving tasks. COPRA employs GPT-4 within a policy guiding a stateful backtracking search, where it selects proof tactics and retrieves relevant lemmas and definitions from an external database. The agent executes each tactic within a proof framework, using feedback to refine subsequent prompts, thus improving the decision-making process. Additionally, the system intelligently tracks search history to minimize errors and redundant queries to the language model. The experiments on two datasets verify the effectiveness of the proposed COPRA."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper is well organized with good language.\n2. The addressed problem is interesting because it is a practical application of LLM. \n3. The authors ensure the reproducibility of COPRA by providing detailed implementation details."
                },
                "weaknesses": {
                    "value": "1. The method 'Decomposing the Enigma' [1], released in May 2023, appears to outperform COPRA on the miniF2F dataset, with a pass rate of 45.5% compared to COPRA's 23.36% [1]. More notably, 'Decomposing the Enigma' [1] achieves this using only ChatGPT-3.5, which raises questions about COPRA's claim to being 'state of the art.' Furthermore, COPRA's performance falls short when compared to Proverbot on the CompCert dataset.\n\n2. It is unfair to compare the number of inferences made with REPROVER in Figure 5, as COPRA utilizes GPT-4 to prove theorems, while REPROVER employs a much smaller LLM. One query from GPT-4 is far more powerful than a single inference from REPROVER's LLM.\n\n3. The authors seem to overstate their claim of having 'the first LLM-agent approach' with 'state-of-the-art' performance.\n\n[1] Zhao, Xueliang, Wenda Li, and Lingpeng Kong. \"Decomposing the Enigma: Subgoal-based Demonstration Learning for Formal Theorem Proving.\" arXiv preprint arXiv:2305.16366 (2023)."
                },
                "questions": {
                    "value": "1. Can you explain the performance gap mentioned in point 1 of the weaknesses?\n\n2. Why does GPT-3.5 perform better than GPT-4 as indicated in Table 2? Does this suggest that there might be overfitting of prompts for different LLMs?\n\n3. How can it be verified that theorems are proved sufficiently?\n\n4. Why is COPRA claimed to be \"the first LLM-agent approach to formal theorem-proving\" when previous works like REPROVER and Decomposing the Enigma [1] might also be considered as LLM-agent approaches?\n\n5. The reference page should begin on a new page (page 10).\n\n6. What is the average API cost for COPRA per proven theorem? \n\n\n\n\n[1] Zhao, Xueliang, Wenda Li, and Lingpeng Kong. \"Decomposing the Enigma: Subgoal-based Demonstration Learning for Formal Theorem Proving.\" arXiv preprint arXiv:2305.16366 (2023)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7710/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698718940587,
            "cdate": 1698718940587,
            "tmdate": 1699636939895,
            "mdate": 1699636939895,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "me97hAmAyg",
                "forum": "XCMbagV0No",
                "replyto": "QeMWSefAcK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7710/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7710/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response (Part 1)"
                    },
                    "comment": {
                        "value": "Thank you very much for providing valuable feedback. We answer your questions below. Due to the 5000 characters limit, we split our response into multiple comments.\n\n1. Comparison with the method \u201cDecomposing the Enigma\u201d [1]. Comparison of COPRA's absolute performance with Proverbot on the CompCert dataset.\n\n**Ans.** \u201cDecomposing the Enigma\u201d [1] is based on Draft-Sketch-Proof (DSP) [4] style approaches, which uses informal proofs to find formal proofs.\nHaving access to informal proofs (human-written or LLM-generated) simplifies the problem of synthesizing the formal proof into more of a translation problem, and that is one of the reasons why DSP-like approaches perform well on miniF2F datasets. It is important to note that these numbers are not comparable across different languages, [1] uses Isabelle which has powerful automatic reasoning tools like Sledgehammer, unlike Lean. As we mentioned in the related work section of our paper, this idea is orthogonal to the ideas in our paper, and we will explore ways to combine it with our methods in future work. \n\nThat said, we also note that informal proofs are hard to get in real-world mathematical reasoning settings such as software verification like [CompCert](https://en.wikipedia.org/wiki/CompCert). Also, the approach prescribed in [1] starts with a manually written subgoal-based proof, and then subsequently refines it. By contrast, our approach is completely automatic. \n\nWe agree that COPRA doesn\u2019t outperform Proverbot on CompCert dataset in absolute numbers, but it uses much fewer inferences to find those proofs. It is quite possible that COPRA could do more proofs if we allowed more inference steps, but we are limited by our cost and API budget for GPT-4.\n\n2. \u201cIt is unfair to compare the number of inferences made with REPROVER in Figure 5, as COPRA utilizes GPT-4 to prove theorems, while REPROVER employs a much smaller LLM. One query from GPT-4 is far more powerful than a single inference from REPROVER's LLM.\u201d \n\n**Ans.** Our objective for comparing the number of inferences is as follows:\n\na. We want to test the hypothesis of whether one can match the performance of fine-tuned models using just in-context learning. There is a recent trend of using Language Agent [2, 3] for performing control tasks using in-context \u201cverbal RL\u201d. We wanted to explore this idea in the context of theorem proving. It is important to note that ReProver is finetuned on proof-state and tactic pair data, but our approach is completely in-context. \n\nb. The intention behind pass@k-inferences is to assess the speed of the proposed method and the effectiveness of the LLM or neural network to guide the proof search. To avoid any confusion we plan to rename this metric as pass@k-guidance budget, which is a reasonable metric as it does the right trade-off in accounting for the time taken to complete a proof and at the same time ignores very low-level hardware details. Even if ReProver uses a smaller model, it can make a lot more inferences in a given time. However, we observe that ReProver is slower in finding proofs even with the wall clock time. This means that the effectiveness of the guidance model is important in quickly finding the proof. We think that Coq/Lean ITP run time contributes more significantly when the guidance model is not effective search-wise (i.e. predicts wrong tactics). We can present an argument for ReProver as to why it is slower. We know that on average the time taken per inference (which includes time to run on ITP as well) is around 1.55 seconds for ReProver and 6.73 seconds for Copra (From Table 2). Even if we assume ReProver doesn\u2019t take any time (zero time) to run the inferences and spends all of 1.55 seconds running the tactic on ITP, then we can assume that about 5 seconds (6.73 - 1.55) is the average response time for GPT-4. However, we see that the number of inferences used by ReProver is about 46x higher on success. This interestingly shows up in the wall clock time too which is around 9x higher (~46x/5) for ReProver on success, so there is a tradeoff between the two, but the number of inferences dominates when the guidance model is not good. So, if the guidance model is good (it may be as big as GPT), we can empirically argue that asymptotically the search will converge to proof faster (given that it can be found using that guidance model).\n\nReferences:\n\n[1] Zhao, X., Li, W., & Kong, L. (2023). Decomposing the Enigma: Subgoal-based Demonstration Learning for Formal Theorem Proving. arXiv preprint arXiv:2305.16366.\n\n[2] Shinn, Noah, et al. \"Reflexion: Language agents with verbal reinforcement learning.\" Thirty-seventh Conference on Neural Information Processing Systems. 2023.\n\n[3] Wang, Guanzhi, et al. \"Voyager: An open-ended embodied agent with large language models.\" arXiv preprint arXiv:2305.16291 (2023).\n\n[4] Jiang, Albert Q., et al. \"Draft, sketch, and prove: Guiding formal theorem provers with informal proofs.\" arXiv preprint arXiv:2210.12283 (2022)."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7710/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699989249613,
                "cdate": 1699989249613,
                "tmdate": 1699989249613,
                "mdate": 1699989249613,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "MuErv7FUtq",
                "forum": "XCMbagV0No",
                "replyto": "QeMWSefAcK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7710/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7710/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response (Part 2)"
                    },
                    "comment": {
                        "value": "3. \u201cWhy does GPT-3.5 perform better than GPT-4 as indicated in Table 2? Does this suggest that there might be overfitting of prompts for different LLMs?\u201d\n\n**Ans.** GPT-3.5 doesn\u2019t perform better than GPT-4. Table 2 compares the average wall clock time taken to find proofs using various approaches. What we observe is that GPT-3.5 response time is usually faster than GPT-4 per inferences which makes sense because it is presumably a smaller model than GPT-4. However, Copra agent with GPT-4 still finds proofs faster than GPT-3.5 because the search guidance provided by GPT-4 is more effective.\n\n4. \u201cHow can it be verified that theorems are proved sufficiently?\u201d\n\n\n**Ans.** COPRA generates formal proofs. Formal proofs are machine-checkable and Interactive Theorem Provers (ITP) like Lean are used for the purpose of verifying the correctness of [formal proofs](https://en.wikipedia.org/wiki/Formal_proof). Often formal proofs are much more rigorous and pedantic than informal proofs.\n\n6. \u201cWhy is COPRA claimed to be \"the first LLM-agent approach to formal theorem-proving\" when previous works like REPROVER and Decomposing the Enigma [1] might also be considered as LLM-agent approaches?\u201d\n\n**Ans.** Proof agents, like other Language Agents, need to have more human-like interactions with the interactive theorem prover (ITP) and generate proof step-by-step along with using external knowledge and tools. In approaches like ReProver, Baldur, PACT, etc., the model is not trained to interact with ITP. Instead, it is trained to either generate the whole proof all at once or one proof step at a time. Also, there is no use of execution feedback, history of proof so far, or known failed proof steps.\n\nSpecifically, in the case of ReProver, there is no use of error feedback from ITP. Most of the current work uses a guidance model to carry out a search for completing the proof in a step-by-step manner. The guidance model is trained to predict the next step and the search is carried out using the best-first strategy. Some other methods generate the whole proof in one shot. Other approaches use informal proofs, for example in (Draft-Sketch-Proof) DSP [4].  While DSP might be using external knowledge (like generating informal proofs, hammer), there is no interaction with the ITP. The proof sketch is generated all at once and then other symbolic techniques (like hammers) are used to fill these holes. Also, in applications like software verification (e.g. our CompCert benchmark), the notion of informal proof is not well defined and there is no informal statement to begin with (which is required for approaches like DSP). \n\nLanguage Agents are used to perform control tasks (naturally framed using an MDP) through \u201cverbal RL\u201d [2, 3]. This is different from traditional deep RL because there is no gradient descent. Instead, the method synthesizes RL-style exploration with  in-context learning. Neither ReProver nor \u201cDecomposing the Enigma\u201d frame theorem-proving as a control/MDP task.\n\n7. \u201cThe authors seem to overstate their claim of having  'state-of-the-art' performance.\u201d\n\n**Ans.** Thank you for this point. We will make our claim of being state-of-the-art more nuanced, as we recognize that there are many versions of the state-of-the-art depending on how the problem is formulated (for example, whether the use of informal proofs is allowed or not).\n\n8. \u201cWhat is the average API cost for COPRA per proven theorem?\u201d\n\n**Ans.** It depends on the number of inferences needed, the model used, and the length of the proof. It is somewhere from `$0.75-$5` per theorem on miniF2F and CompCert datasets.\n\nReferences:\n\n[1] Zhao, Xueliang, Wenda Li, and Lingpeng Kong. \"Decomposing the Enigma: Subgoal-based Demonstration Learning for Formal Theorem Proving.\" arXiv preprint arXiv:2305.16366 (2023).\n\n[2] Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. arXiv preprint arXiv:2303.11366, 2023.\n\n[3] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. arXiv preprint arXiv:2305.16291, 2023.\n\n[4] Albert Q Jiang, Sean Welleck, Jin Peng Zhou, Wenda Li, Jiacheng Liu, Mateja Jamnik, Timothee\u00b4 Lacroix, Yuhuai Wu, and Guillaume Lample. Draft, sketch, and prove: Guiding formal theorem provers with informal proofs. arXiv preprint arXiv:2210.12283, 2022."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7710/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699990086829,
                "cdate": 1699990086829,
                "tmdate": 1699990110412,
                "mdate": 1699990110412,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "cy7XdRcYBp",
            "forum": "XCMbagV0No",
            "replyto": "XCMbagV0No",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7710/Reviewer_rua8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7710/Reviewer_rua8"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces COPRA, a language-agent approach that prompts a large language model (LLM), specifically GPT-4, for formal theorem-proving. COPRA enhances the theorem-proving process by employing a stateful backtracking  policy search using language model. In particular, during the search, the policy selects proof tactics and retrieves essential information such as lemmas and definitions from an external database. Execution feedback and historical search data are then prompted again for policy update.  The authors tested COPRA on benchmarks like miniF2F and Coq tasks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Formal theorem proving is a less explored application domain. This paper provides positive results for such an application."
                },
                "weaknesses": {
                    "value": "1. Novelty. It seems that the method is similar to retrieval-based LLM in the sense that the policy uses an external database. It would be great if the authors could compare with this line of works in detail, especially the ReProver paper. \n\n2. It seems that the proposed method does not significantly outperform ReProver. \n\n3. Ablation studies might be needed to understand the role played by the RL component."
                },
                "questions": {
                    "value": "1. Is there a difference between formal and informal theorem proving? It seems that there are some recent works that this work (LYRA: ORCHESTRATING DUAL CORRECTION IN AUTOMATED THEOREM PROVING) has a much higher score on miniF2F. \n\n2. What is the role played by the RL part? I understand that you formulate the problem as an MDP and then the language-agent essentially mimics an RL algorithm. What is this particular RL algorithm? How to handle exploration-exploitation tradeoff?\n\n3. Is RL really essential here? Can you replace it with other planning methods such as tree search, or even a close-loop planning method such as ReAct?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7710/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698733509436,
            "cdate": 1698733509436,
            "tmdate": 1699636939764,
            "mdate": 1699636939764,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4AWRWFl7io",
                "forum": "XCMbagV0No",
                "replyto": "cy7XdRcYBp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7710/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7710/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response (Part 1)"
                    },
                    "comment": {
                        "value": "Thank you for your valuable feedback. We address your main points below. Due to the 5000 characters limit, we split our response into multiple comments.\n\n1. Questions about novelty and differences from existing retrieval-based LLM approaches like ReProver.  \n\n**Ans.** Our approach is based on the new paradigm of Language Agents, where one uses LLMs to perform control tasks and interact with and external world. In particular, we were inspired by Reflexion [2], which uses language agents to solve natural language RL tasks, and Voyager [3], which uses language agents to play Minecraft. This approach is different from traditional deep RL because there is no gradient-based policy update. Instead, RL-style exploration is combined with in-context learning. \n\nIn the proof synthesis setting, an LLM agent needs to have human-like interactions with the underlying interactive theorem prover (ITP) and generate a proof step-by-step along with using external knowledge and tools. In approaches like ReProver, Baldur, PACT, etc., the model is not trained to interact with the ITP. The proof steps are either generated all at once or one at a time, but there is no use of execution feedback, history of proof so far, or known failed proof steps. \n\nSpecifically, in the case of ReProver, there is no use of error feedback from ITP. Most of the current work uses a guidance model to carry out a search for completing the proof in a step-by-step manner. The guidance model is trained to predict the next step and the search is carried out using the best-first strategy. Some other methods generate the whole proof in one shot. Other approaches use informal proofs, for example in Draft-Sketch-Proof (DSP) [4].  While DSP might be using external knowledge (like generating informal proofs, hammer), there is no interaction with the ITP. The proof sketch is generated all at once and then a powerful automatic reasoning tool (like hammer) is used to fill the holes. Also, it is important to note that in certain situations like software verification (e.g. CompCert) the notion of informal proof is not well defined and there is no informal statement to begin with (which is required for approaches like DSP). \n\n2. \u201cIt seems that the proposed method does not significantly outperform ReProver.\u201d\n\n**Ans.** It is true that the set of theorems that COPRA eventually discovers is not much larger than what Reprover discovers. However, COPRA  finds the proofs much faster than ReProver at least 9x faster in absolute wall clock time. COPRA also fails faster when it is not able to find the proofs. We weren't able to run COPRA on a very large number of timesteps because of the high cost of GPT-4 queries; however, we expect that doing so would improve COPRA's performance even more. \n\nMore generally, we believe that as foundation models get more and more capable and cheaper (think of OpenAI's recent announcement regarding GPT-4-turbo, which significantly boosts GPT-4's context window while slashing costs), the benefits of COPRA-like approaches will become even clearer. \n\n\n3. \u201cAblation studies might be needed to understand the role played by the RL component.\u201d\n\n**Ans.** Please let us know any specific ablations that you would want to see, and we would be happy to run them.\n\nReferences:\n\n[1] Zheng, Chuanyang, Haiming Wang, Enze Xie, Zhengying Liu, Jiankai Sun, Huajian Xin, Jianhao Shen, Zhenguo Li, and Yu Li. \"Lyra: Orchestrating Dual Correction in Automated Theorem Proving.\" arXiv preprint arXiv:2309.15806 (2023).\n\n[2] Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. arXiv preprint arXiv:2303.11366, 2023.\n\n[3] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. arXiv preprint arXiv:2305.16291, 2023.\n\n[4] Albert Q Jiang, Sean Welleck, Jin Peng Zhou, Wenda Li, Jiacheng Liu, Mateja Jamnik, Timothee\u00b4 Lacroix, Yuhuai Wu, and Guillaume Lample. Draft, sketch, and prove: Guiding formal theorem provers with informal proofs. arXiv preprint arXiv:2210.12283, 2022."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7710/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699992902687,
                "cdate": 1699992902687,
                "tmdate": 1699992902687,
                "mdate": 1699992902687,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JIObI0cUJo",
                "forum": "XCMbagV0No",
                "replyto": "S056in5Kr1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7710/Reviewer_rua8"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7710/Reviewer_rua8"
                ],
                "content": {
                    "comment": {
                        "value": "I would like to thank the authors for addressing my questions. \n\n**The role of RL** I am not convinced that RL is essential here. Of course, the problem can be written as an MDP -- the history is the state, the proof tactic is the action, reward is binary -- success or error. But this formulation does not touch upon the essence of RL -- in particular, it is unclear what kind of RL algorithm is implemented by in-context learning. This is different from existing works such as Tree of thoughts or RAP (Reasoning via Planning) which incorporated MCTS as part of the prompting strategy. In contrast, in this work, RL seems merely another way of saying the prompting is iterative -- the feedback is also added to the prompts. \n\nThus, I would suggest conducting additional experiments to compare with those methods that incorporate tree search. The rebuttal period is limited. But it would be great if the authors could further clarify the role played in RL. \n\n**formal vs informal proof** I would like to thank the authors for explaining the difference between these two proofs. Suppose we have a dataset of informal proofs, can we incorporate them into the prompt and use in COPRA?"
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7710/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700659826541,
                "cdate": 1700659826541,
                "tmdate": 1700659826541,
                "mdate": 1700659826541,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "GlrdMptHGs",
            "forum": "XCMbagV0No",
            "replyto": "XCMbagV0No",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7710/Reviewer_VNR5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7710/Reviewer_VNR5"
            ],
            "content": {
                "summary": {
                    "value": "The paper attacks the problem of tactic based theorem proving. Given a starting (goal,hypothesis) pair, we use *lean* to apply a *tactic* to it. This either yields a set of new goals, all of which need to be proved, or an error, or it directly proves the goal. The new set of goals are then handled recursively.\n\nThe paper proposes to choose the tactics by prompting an llm, and to embed this in a search algorithm which effectively does back tracking. The llm prompt includes *execution feedback* allowing the llm to improve on earlier failures. \n\nThe search is also guided by a way of pruning sets of (goal,hypothesis) pairs that are strictly harder to prove than existing ones.\n\nMore detailed comments:\n\nSection 1\n\nSome of the text in fig 1 is too small / blurry.\n\nSec 2.\n\nThe use of Sanchez-Stern's pruning method is cool.\n\nIt seems highly restrictive to only allow the model to choose tactics. Why not use prompting to ask the model the most promising partial prove to attack next?\n\nThe discussion of *Rewards* and *Histories and Policies* seems confusing and maybe erroneous. Detailed questions around that:\n- Why are both scalar rewards and text feedbacks formalised as part of the reward ? How does this compare to traditional RL / MDP setups? Why the departure from that?\n- Where is the scalar reward actually used? Apologies if I missed it but I couldn't quite see where the $r_i$ are used by the algorithm.\n- What is the point of the sentence \"A trajectory of a policy $\\pi$ is a history ... ? \n\nThe use of execution feedback is nice, and it is nice to see this idea brought into theorem proving. The literature for program synthesis and other areas could be mentioned apropos this.\n\nSeciton 3\n\nPlease fix the indentation of the pseudo-code in Fig. 3.\n\nFig. 4 was appreciated and seems helpful but it is slightly confusing in the current form ... Is this entire protocol repeated up to $k$ times, or is this all within one value of $j$ in your pseudo-code?\n\nSection 4\n\nEnd of page 5: including one shot prompting in copra dilutes what we can say about the method. Why not include an ablation where you only do the search, no one shot?\n\nI'm not sure about removing ReProver's retrieval mechanism, could this not be done similarly to them? The code is available, and the agent system mentions a Lemma repository, which should be crucial to their model.\n\nIf you could integrate the lemma/retriever for MiniF2F the results should be even stronger, not sure why this couldn't be done, the explanation was unclear since you can access ReProver's code for MiniF2F and get the set of relevant premises from there (even just using BM25).\n\nGeneral comment: the results look fishy. I suspect GPT was trained on these datasets, and this would make all of the absolute comparisons with other methods hard to interpret. Please convince me otherwise. The passage on page 7 with paragraph heading \"results\" also strongly indicates this.\n\nPass @ k inferences is not intuitive to me:\n-  why not use pass @ k tactic applications ? isn't this the main bottleneck? As it is structured, since each inference is restricted to a single response, they are essentially the same in this setup, but I think it's worth emphasising you are mostly restricted by the environment.\n- doesn't this make the comparisons unfair also because GPT is more expensive than the other models (i.e. your Fig 6 x-axis is not comparable)? in the pass @ k inferences, why not use wall clock time then ?\n\nLooking closely at Fig 6, why does proverbot have y value > 0 at x value = 0 ? \n\nGeneral comment: if we discount the absolute comparisons since GPT may have seen these datasets, how can we answer the research question \"does the search strategy work\". It seems like what might be missing is an answer to the questions \"how does running Copra with k = 1 repeatedly with i.i.d. sampling of the LLM compare to copra with k > 1, normalised for number of tactic applications?\". \n\nIn table 3:\n- how does the \"w/o backtracking\" work, precisely?\n- are these numbers comparable in terms of computational work? e.g. with number of tactic calls held constant?\n\nTypos\n\n- Page 7 typo (correlated [with the] number of correct proofs..)\n- Typo in results (if only 60 inferences [are] allowed)"
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "See the summary."
                },
                "weaknesses": {
                    "value": "See the summary."
                },
                "questions": {
                    "value": "Can you address the concerns? I like the paper and want to raise the score, but it feels like it might not be ready yet, if those concerns can't be reasonably addressed."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7710/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698995439378,
            "cdate": 1698995439378,
            "tmdate": 1699636939639,
            "mdate": 1699636939639,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "U3NwzkO2VZ",
                "forum": "XCMbagV0No",
                "replyto": "GlrdMptHGs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7710/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7710/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response (Part 1)"
                    },
                    "comment": {
                        "value": "Thank you for meticulously reviewing our paper and giving valuable feedback.  Due to the 5000 characters limit, we split our response into multiple comments. We respond to the important questions that you have raised below.\n\n1. \"It seems highly restrictive to only allow the model to choose tactics. Why not use prompting to ask the model the most promising partial proof to attack next?\"\n\n**Ans.** If we understand your point correctly, you would like us to use the LLM to not just select a tactic for proving an obligation, but also for selecting the best obligation to prove. In most cases, the LLM has a large enough context window to see all obligations, this way there is an implicit choice available for the LLM to choose which obligation to attack first. In order to prove a theorem, all obligations have to be proved; in most cases, it might not matter which obligation is proved first (except for cases like induction where we prove the base case first). \n\n2. Why are both scalar rewards and text feedback formalized as part of the reward? How does this compare to traditional RL / MDP setups? Why the departure from that? Where is the scalar reward actually used? What is the meaning of the sentence \u201cA trajectory of policy \u2026\u201d in Section 2.\n\n**Ans.** The scalar reward is there to incentivize correct proofs and disincentivize incorrect proofs within the MDP framework. (In principle, the scalar reward function can also be used to incentivize shorter proofs.) We agree that the use of text feedback as part of the reward is not standard in traditional RL. The justification of this idea is that just like reward functions in traditional RL, the text describes the effectiveness of an action at a given state. We borrow this idea from recent work on \u201cverbal RL\u201d [1], which uses LLM agents for solving certain natural language RL problems and uses text as a generalization of rewards. This line of work departs from traditional RL in not having explicit policy updates, but combining RL-style exploration with in-context learning. \n\nWe note, however, that the text feedback can also be modeled as part of the new environment state resulting from the action, as opposed to the reward signal. Such a model would make the reward function purely scalar just like in classical RL. If you think this is more natural, we would be happy to make this change. \n\nIn RL, the agent's goal is to maximize the expected reward aggregated over trajectories. This requires a definition of trajectories. The paragraph  \u201cA trajectory of policy ..,\u201d offers this definition.\n\n3. In the pseudo-code (Figure 3), is this entire protocol repeated up to k times, or is this all within one value of your j pseudo-code?\n\n**Ans.** Thank you for pointing this out, the \u2018k\u2019 mentioned in the algorithm is not the same \u2018k\u2019 as in pass@k-inference. All inference calls to LLMs even for fixing formatting errors are counted in pass@k-inferences. This is done to ensure that COPRA doesn\u2019t have any unfair advantage over other approaches when we cap the number of inferences allowed. You can assume that the \u2018k\u2019 in the pseudo-code is just a free hyperparameter, and has nothing to do with the overall inference cap. The inference cap is strictly enforced. For pass@k-inference, COPRA can never make more than \u2018k\u2019 API calls to the LLM including all calls for fixing the formatting errors, incomplete responses, etc.\n\n4. The use of one-shot prompting along with the agent interactions for finding proofs dilutes the capabilities of the agent.\n\n**Ans.**  Our reasoning here was that the goal is to find proofs quickly and cost-effectively. If GPT-4 can do it in one shot, the problem is not hard enough to justify the more expensive agent interaction. \n\n\nThat said, if you like, we can run the Copra agent on 26 theorems which GPT-4 could do in one shot. Even in the most unlikely case of Copra not proving any of the 26 theorems, we can still say that COPRA does at least about 3% more proofs than what GPT-4 could do alone (which is evident from the numbers in Table 1). \n\nReferences:\n\n[1] Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. arXiv preprint arXiv:2303.11366, 2023."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7710/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700000528381,
                "cdate": 1700000528381,
                "tmdate": 1700000528381,
                "mdate": 1700000528381,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Ym8i140S4j",
                "forum": "XCMbagV0No",
                "replyto": "GlrdMptHGs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7710/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7710/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Results from the additional experiments"
                    },
                    "comment": {
                        "value": "Dear Reviewer,\n\nWe ran experiments to address the concerns raised by you. We are excited to share some findings with you, which we believe demonstrate significant progress:\n1. **Enhancement in Problem Solving Capability using Retriever**: In response to the retriever disabled issue, we developed a BM25 retriever and re-ran our experiments. This gives us **27.45%** i.e. 67/244 success rate on miniF2F. This outperforms notable systems like ReProver, Llemma, PACT, and Lean GPT-f.\n\n2. **Addressing Data Leakage Concerns**: We undertook a thorough analysis of potential data leakage within the miniF2F dataset. Our findings indicate that of the 67 proofs our system generated, 68.65% are distinct from those mentioned in the miniF2F dataset or are cases where the dataset lacks a corresponding proof. Notably, the proofs matching with miniF2F are primarily simple ones, solvable with a single tactic like 'linarith', 'norm_num', or 'nlinarith'. Excluding these straightforward cases, we found that 92% of our generated proofs are unique compared to the miniF2F dataset.  Additionally, 25.37% of the proofs generated by our approach are not mentioned in the miniF2F dataset, compared to 22.9% of for the ReProver.\n\nWe appreciate the opportunity to run additional experiments and hopefully address your concerns regarding retriever and data leakage.\n\nBest regards,\n\nAuthors"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7710/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700601828666,
                "cdate": 1700601828666,
                "tmdate": 1700647971752,
                "mdate": 1700647971752,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]