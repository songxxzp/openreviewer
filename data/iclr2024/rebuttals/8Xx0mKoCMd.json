[
    {
        "title": "ExoViP: Step-by-step Verification and Exploration with Exoskeleton Modules for Compositional Visual Reasoning"
    },
    {
        "review": {
            "id": "9yI5IjInnQ",
            "forum": "8Xx0mKoCMd",
            "replyto": "8Xx0mKoCMd",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5514/Reviewer_rVcU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5514/Reviewer_rVcU"
            ],
            "content": {
                "summary": {
                    "value": "Compositional visual reasoning methods, which strive to convert a complex query into a structured composition of manageable visual tasks, have demonstrated considerable potential in intricate multimodal tasks such as visual question answering and language-guided image editing. This paper introduces EXOVIP, a plug-and-play method designed to rectify errors at both the planning and execution stages. Empirical results from two representative vision-language programming methods exhibit consistent enhancements across five compositional reasoning tasks on a standard benchmark."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1. EXOVIP designs a mixture of three sub-verifiers, including an image-text matching verifier, an image captioning verifier, and a visual\nquestion answering verifier, which seems to be reasonable.\n2. A reasoning trace tree based on the verification scores as well as the self-correctness score from LLMs is also built to find the best trace that has the highest score.\n3. To demonstrate the effectiveness of EXOVIP, this paper conducts experiments on two recent visual programming methods: self-defined programs, i.e., VISPROG and Python code programs, i.e., ViperGPT."
                },
                "weaknesses": {
                    "value": "1. Detailed information about different prompts is necessary for a comprehensive comparison\n\n2. The improvements on the GQA dataset appear to be minor when compared to those achieved by ViperGPT"
                },
                "questions": {
                    "value": "See weakness"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5514/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698635239839,
            "cdate": 1698635239839,
            "tmdate": 1699636564855,
            "mdate": 1699636564855,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "hrR5yCVeX9",
                "forum": "8Xx0mKoCMd",
                "replyto": "9yI5IjInnQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5514/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5514/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer rVcU"
                    },
                    "comment": {
                        "value": "Thank you for your meticulous review and acknowledgment of our method's effectiveness. We believe your support of our work will help us to make a timely contribution and broader impact on both Computer Vision and compositional reasoning community. We would like to provide further clarification to address your concerns.\n\n> W1: Detailed information about different prompts is necessary for a comprehensive comparison\n\n**A:** In our submission, we have demonstrated the exact prompt for self-correctness for all five tasks, Please refer to **Appendix G** for details. \nSpecifically, for example, for compositional question answering, the prompt goes like:\n`You are a ranker for a planner who use the candidate modules to answer a question: QUESTION, select the best solutions for answering the question \\n candidate modules include: LOC: detection, VQA: visual question answering, EVAL: use logic operation, RESULT: wrap up the final result, CROP/CROP_LEFTOF/CROP_RIGHTOF/CROP_FRONTOF/CROP_INFRONT/CROP_INFRONTOF/CROP_BEHIND/CROP_AHEAD/CROP_BELOW/CROP_ABOVE: crop the image \\n Current solutions: \\n 0 PLAN \\n 1 PLAN \\n If the modules in the solutions have better cause-and-effect relations, and more likely to answer the question, please rank it first. If you are unsure, please keep the original rank. Return sequence number of currently best solution, for example 0,1,2,3, DO NOT RETURN ANYTHING ELSE EXCEPT FOR NUMBERS SPLIT by ,`\n\nWe are willing to append more details if there is any further missed information.\n\n\n> W2: The improvements on the GQA dataset appear to be minor when compared to those achieved by ViperGPT\n\n**A:** Thank you for bringing this to our attention. We would like to highlight 4 aspects: \n\n- Our test set differs from that of ViperGPT. We used the same sampling strategy as in VisProg. As is claimed in **Section 5.1**, `we randomly sample 5 samples from the balanced val set and 20 samples from the testdev set of each question type. e.g. \u201cweatherVerify\u201d for judging the weather, \u201ctwoCmomon\u201d for judging common attributions of two objects. In summary, there are 102 question types and 2327 questions in our test set.`\n- Regarding the different test sets, we re-executed ViperGPT's experimental results on our test set using their official repository (https://github.com/cvlab-columbia/viper) without any modifications. The results indicate that VisProg+ExoViP (61.49%) improves the performance of ViperGPT (46.47%) by 5.02%.\n- Our insights differ from ViperGPT. While ViperGPT has transitioned to Python code for planning, thereby inherently addressing numerous planning errors, our module is designed to enhance existing compositional methods by tackling both planning and module errors.\n- As shown in Table 7, ExoViP can be generalized to improve ViperGPT by 1.37%."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5514/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699950126403,
                "cdate": 1699950126403,
                "tmdate": 1699950126403,
                "mdate": 1699950126403,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "KGQDqA55ML",
            "forum": "8Xx0mKoCMd",
            "replyto": "8Xx0mKoCMd",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5514/Reviewer_atdm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5514/Reviewer_atdm"
            ],
            "content": {
                "summary": {
                    "value": "This paper points out two major challenges when using Visual Programming, namely 1) module error and 2) planning error. The authors propose a plug-and-plan method, EXOVIP, which can self-correct the errors on the planning side and on the module execution side. More particularly, three verification module (e..g, Image-text matching, Image captioning, and VQA verifier) are proposed to verify each step. In the experiments, they show improvement of EXOVIP over BLIP2/InstructBLIP/VISPROG on GQA, and over Qwen-v1-chat-7b/VISPROG on RefCOCO, InstructPix2Pix/VISPROG on MagicBrush, etc."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The main idea is clear, and it's worth diving into this area to mitigate planning error and module error. The proposed verification score for prediction calibration is straightforward and show some improvements on several datasets.\n2. The paper are written clearly in general, though some small typos. The figures clearly present the intuition behind the proposed method."
                },
                "weaknesses": {
                    "value": "1. The verifier module is not carefully developed, CLIP for image-text matching is particularly not good at give correct similarity score, especially when the task are for compositional visual reasoning in this paper. In addition BLIP version1 (as cited in Section 4.1) for image caption/VQA is also not SOTA, and no ablation study is explored here to choose the best module for verifiers.\n2. The introduced verifiers can cost the whole system more computation time, however no efficiency info is added here, it should be more fair to consider this. Besides, the sub-verifiers for calibration introduce propagation error which is not handled, and it's not clear when the verification score is more reliable than the confidence score, while in the method, the verification score directly replace it.\n3. Very concerned with the compared baselines. e.g., many recent MLLMs achieve better results on GQA than the baselines used in Table 1, e.g., LLaVA get 63 on it. In Table 3, why is the reported referring expression comprehension results only on RefCOCO and RefCOCO+, why not including RefCOCOg, which is quite a standard comparison for this task. And why only a subset is validated, 2 samples per type from the test set is actually not enough. And the compared model and proposed model are only 30 IoU, which is far below the decent results in recent MLLM, which all receive 80+ (e.g., Shikra, Kosmos2, etc)\n4. In conclusion, the main concern is the baselines compared in each Table, very confusing, a lot SOTA models are removed out e.g., OFA-large is selected for NLVR, but not included for RefCOCO (which is actually reported in the origin paper). And the proposed method is not carefully developed or verified, as not sufficient ablation for each module is conducted, especially the choice for each verifier."
                },
                "questions": {
                    "value": "1. For the three proposed verifiers, the CLIP is used for image-text matching, and BLIP is used for image captioning and VQA, how these models are selected, is there any ablation experiments here? E.g., C_img is generated by BLIP, why not using SOTA caption model, at least BLIPv2? Is this a severe bottleneck for the verifier or even bring confusion when the verifier model make errors.\n2. It happens when the confidence score is more accurate than the verification score. However, seems in candidate prediction in Equation 6, is only using the averaged verification score to calibrate, while the confidence score is ignored.\n3. How is the propagation error being controlled? The verifier is proposed to mitigate the planning error and module error. How is the newly. introduced verifier module error being handled?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5514/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699581708396,
            "cdate": 1699581708396,
            "tmdate": 1699636564762,
            "mdate": 1699636564762,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7yzWrcrjkA",
                "forum": "8Xx0mKoCMd",
                "replyto": "KGQDqA55ML",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5514/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5514/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer atdm (1/3)"
                    },
                    "comment": {
                        "value": "Thank you for your careful review and insightful comments.  We have meticulously discussed the issues at hand and have attempted to dispel any misconceptions through our detailed responses. We trust that our comprehensive, point-by-point rebuttals have satisfactorily addressed all your concerns. Please let us know if there are further questions and we are delighted to have a deeper discussion.\n\n\n>W1: The verifier module is not carefully developed, CLIP for image-text matching is particularly not good at give correct similarity score, especially when the task are for compositional visual reasoning in this paper. In addition BLIP version1 (as cited in Section 4.1) for image caption/VQA is also not SOTA, and no ablation study is explored here to choose the best module for verifiers.\n\n>W3: And the proposed method is not carefully developed or verified, as not sufficient ablation for each module is conducted, especially the choice for each verifier.\n\n>Q1: For the three proposed verifiers, the CLIP is used for image-text matching, and BLIP is used for image captioning and VQA, how these models are selected, is there any ablation experiments here? E.g., C_img is generated by BLIP, why not using SOTA caption model, at least BLIPv2? Is this a severe bottleneck for the verifier or even bring confusion when the verifier model make errors.\n\n\n**A:** Sorry for any confusion. We have carefully considered the selection of models for each verifier module. Specifically,\n1. **Fair module comparisons.** We intentionally selected modules from VisProg's original modules. As noted in the implementation detail section of **Appendix E**, \"` In order to validate the effectiveness of our method and eliminate the benefits of external knowledge such as more advanced vision-language models which are trained on larger datasets. Both operation modules and verification modules are selected from the same candidate neural module sets.`\". \n2. **Trade-off between latency and performance.** As claimed in **Section 5.2.2**, \"`while Qwen-VL is built on a LLM with 7 billion parameters, which is trained on trillions of tokens from the corpus, our method assembles a team of experts whose collective parameters total less than 1 billion`\". Our goal is not only to achieve better performance with heavier and more models, but to deliver a carefully designed system that is both lightweight and easy to plug-and-play, offering superior performance with minimal additional computational expense. Specifically, when we substitute the QA model (blip-vqa-capfilt-large) with BLIPv2 (blip2-flan-t5-xl), we see a performance increase from 67.96 to 68.70 on NLVR. However, this comes with a *tenfold increase in parameters*.\n\n\n> W2: The introduced verifiers can cost the whole system more computation time, however no efficiency info is added here, it should be more fair to consider this. \n\n**A:** Thank you for pointing it out. We present the mean inference time on the GQA dataset. Generally, the temporal expenditure of our tree-based search method significantly surpasses that of VisProg. However, the majority of the time is consumed by the call of the OPENAI API, an issue we posit is intrinsic to analogous works [1,2,3]. When compared to Depth First Search/ Breadth First Search [1] or Monte Carlo Tree Search [2,3], we assert that our beam search-based method can achieve an optimal equilibrium between efficiency and effectiveness. We have added this part to **Appendix D** in our revised paper.\n\n| model   | Total Inference Time (s) |  Planning Time (s)   | Module Inference Time (s) |\n| ------- | -------------------- | --- | -------------------------- |\n| VisProg | 1.59                 |  1.10   | 0.49                       |\n| ExoViP  | 4.32                 |   3.64  | 0.68                       |\n\n\n\n\n>W2: It's not clear when the verification score is more reliable than the confidence score, while in the method, the verification score directly replace it.\n\n>Q2: It happens when the confidence score is more accurate than the verification score. However, seems in candidate prediction in Equation 6, is only using the averaged verification score to calibrate, while the confidence score is ignored. \n\n**A:** Sorry about the confusion. There might be severe misunderstandings regarding our method. we never used the verification score to replace the confidence score. In Equation 6, $p$ represents the confidence score, while $w$ denotes the normalized verification score, also known as the weight. Intuitively, the verification score, or weight, is used to scale (calibrate) the confidence score. We have not ignored the confidence score at any point."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5514/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699949822036,
                "cdate": 1699949822036,
                "tmdate": 1699949998608,
                "mdate": 1699949998608,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WtkEQC03an",
                "forum": "8Xx0mKoCMd",
                "replyto": "KGQDqA55ML",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5514/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5514/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer atdm (3/3)"
                    },
                    "comment": {
                        "value": ">W3: many recent MLLMs achieve better results on GQA than the baselines used in Table 1, e.g., LLaVA get 63 on it. \n\n**A:** Thank you for the references. We have included the results of LLava, as an upper bound of non-compositional models, in Table 1.\n\nAs claimed in the introduction, \"`despite their merits, current visual programming methods still suffer from challenges due to the failure of the LLM planning or the visual modules, lagging behind the performance\nof non-compositional models.`\", the focus of our work is to mitigate the performance gap between compositional models and non-compositional ones (e.g. LLaVA). Therefore, models like LLaVA can only be considered upper-bound and lead to unfair comparisons to our model.\nSpecifically, the result of 63.0 on GQA (\"`e.g., LLaVA get 63 on it`\") is from Llava (Llava-1.5-13B) [9]. However, this result is derived from instructions that were tuned on the GQA training corpus, making it incompatible with the zero-shot setting of our experiments. Additionally, the evaluation scripts for Llava come with an extra prompt, \"\\nAnswer the question using a single word or phrase.\". This particular prompt is not included in the scripts for other baselines.\n\n\n\n| Model | Accuracy on GQA | \n| -------- | -------- | \n| Llava-1.5-13b | 74.56 |\n| VISPROT     |  57.41    |\n| ExoViP     |   61.49   | \n\n\n\n>W3: In Table 3, why is the reported referring expression comprehension results only on RefCOCO and RefCOCO+, why not including RefCOCOg, which is quite a standard comparison for this task.\n\n**A:** Thank you for your suggestion. Due to limited resources and funds, we are unable to apply our method to all existing benchmarks. However, in response to your request, we will endeavor to report the results on the subset RefCOCOg, which is sampled by the same strategy as RefCOCO and RefCOCO+. \n\n| Model | IoU on RefCOCOg | \n| -------- | -------- | \n| Qwen-vl-chat-7b     |  18.53   |\n| VISPROG     |   14.83   | \n| ExoViP     |   15.82  | \n\n\n>W3:  And why only a subset is validated, 2 samples per type from the test set is actually not enough. \n\n**A:** As previously mentioned, our financial constraints limit us from affording a complete test set. However, the scale of our dataset is comparable to that of VISPROG. We also believe that implementing a balanced sampling strategy is of utmost importance.\n\n\n>W3: And the compared model and proposed model are only 30 IoU, which is far below the decent results in recent MLLM, which all receive 80+ (e.g., Shikra, Kosmos2, etc). \n\n\n**A:** It is worth noting that the testing dataset used to report evaluation results of `Shikra, Kosmos, etc.` is different from the evaluation dataset used in our work (following the sampling strategy as in VisProg; refer to **Sec. 5.1 GQA setup** for details). In order to stay aligned with the current advancements in MLLM, we have included some preprint models in our experiment, such as QWen-vl-7b-chat. To the best of our knowledge, this model is a state-of-the-art MLLM during our experimental period, and it outperforms Shikra in terms of results on RefCOCO. We have used the model weight from its official code repository to evaluate our dataset (URL: https://github.com/QwenLM/Qwen-VL): as reported in our paper, it achieves a score of 32.54 in the subset of RefCOCO&RefCOCO+. We are willing to include more references and compare with them if there are more advanced models. We have also updated all the evaluation scripts in the anonymous link provided in the submitted paper.\n\n\n>W3: In conclusion, the main concern is the baselines compared in each Table, very confusing, a lot SOTA models are removed out e.g., OFA-large is selected for NLVR, but not included for RefCOCO (which is actually reported in the origin paper).\n\n**A:** As previously stated, our objective in this work is not to achieve the SOTA performance on all tasks. We have selectively referenced parts of the SOTA models. Since Qwen-vl-7b significantly outperforms OFA-Large on the referring task, we did not report its results. \n\n\n\n\n\n\n[1] Tree of Thoughts: Deliberate Problem Solving with Large Language Models\n\n[2] Alphazero-like Tree-Search can Guide Large Language Model Decoding and Training\n\n[3] Language Agent Tree Search Unifies Reasoning Acting and Planning in Language Models\n\n[4] Mitigating Lies in Vision-Language Models\n\n[5] Improving Commonsense in Vision-Language Models via Knowledge Graph Riddles\n\n[6] A Multi-dimensional study on Bias in Vision-Language models\n\n[7] Multimodal Explanations by Predicting Counterfactuality in Videos\n\n[8] Towards Counterfactual Image Manipulation via CLIP\n\n[9] Improved Baselines with Visual Instruction Tuning"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5514/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699949983109,
                "cdate": 1699949983109,
                "tmdate": 1699954576311,
                "mdate": 1699954576311,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "J97UGWagvs",
            "forum": "8Xx0mKoCMd",
            "replyto": "8Xx0mKoCMd",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5514/Reviewer_2ndj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5514/Reviewer_2ndj"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a novel approach called \"exoskeleton\" verification modules designed for visual programming. These modules methodically validate the accuracy of vision module predictions. They are used to rectify errors in module predictions through calibration, and in planning, using tree searching techniques. This involves incorporating verification scores and the self-correction capability of large language models (LLMs). The approach is tested on five different tasks, demonstrating its efficacy. Notably, when applied to the VISPROG model in a compositional visual question answering task, the method achieved a notable 4% increase in accuracy."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper introduces an innovative concept in the field of visual programming - the \"exoskeleton\" verification modules. This unique approach effectively tackles both module and planning errors in compositional visual reasoning.\nThe robustness of the proposed methodology is evident through its application to two distinct models, yielding a notable 4% improvement in accuracy for compositional visual question answering tasks. Furthermore, the paper conducts a thorough preliminary study, demonstrating how individual components (such as self-correctness, beam search, and verification) contribute to enhanced performance.\nThe paper excels in clearly articulating its methodology. It provides detailed explanations on the use of exoskeleton modules for verification processes and the precise methods employed in calibrating module predictions.\nThe contributions of this paper are highly significant for the field of visual compositional reasoning. It addresses critical limitations in existing methodologies and offers a substantial improvement in prediction accuracy, marking a notable advancement in the field."
                },
                "weaknesses": {
                    "value": "The paper's experimental evaluation is somewhat limited, primarily focusing on VISPROG evaluation datasets. Expanding the range of datasets, especially including more varied and challenging ones, would significantly strengthen the demonstration of the method's generalizability. While the paper reports a 4% improvement on the GQA dataset, a broader dataset spectrum could provide a more comprehensive assessment of the method's effectiveness.\nThe paper lacks an in-depth analysis of its limitations and potential failure cases. For visual language grounding, natural language reasoning, and visual abstract reasoning tasks where the model fails to exceed the performance of SOTA models, no further analysis is provided apart from reporting the results. Although several failure cases are mentioned in the attachment, they are not thoroughly investigated or analyzed. A detailed examination of these cases would be invaluable for identifying areas for further refinement and development of the method.\nThe paper\u2019s approach appears to primarily involve an ensemble of multiple vision-language (VL) models for post-correction, which could be perceived more as an engineering endeavor based on the VISPROG work rather than a novel conceptual contribution. A more distinct delineation of the method's innovative aspects compared to existing VL models would be beneficial in highlighting its unique contributions to the field."
                },
                "questions": {
                    "value": "Generalizability Across Datasets:\nQuestion: Could you elaborate on the choice of primarily using VISPROG evaluation datasets?\nSuggestion: Consider applying your method to a wider array of datasets, especially those that are more diverse and challenging. This could help in better demonstrating the method's generalizability and effectiveness across different scenarios.\n\nAnalysis of Limitations and Failure Cases:\nQuestion: Why was there no detailed analysis provided for the failure cases mentioned in the attachment?\nSuggestion: It would be beneficial to include a thorough investigation and analysis of these failure cases. This analysis could offer insights into the method's limitations and guide future improvements.\n\nClarification on Novelty and Conceptual Contributions:\nQuestion: How does the proposed method differentiate itself, in terms of novelty, from simply being an ensemble of multiple VL models for post-correction?\nSuggestion: A more explicit explanation of the novel aspects of your approach would be helpful. Clarifying how your method advances beyond just combining existing VL models could strengthen the paper's claims of originality."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5514/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5514/Reviewer_2ndj",
                        "ICLR.cc/2024/Conference/Submission5514/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5514/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699584902451,
            "cdate": 1699584902451,
            "tmdate": 1700681991381,
            "mdate": 1700681991381,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Zb9bXxV1Vm",
                "forum": "8Xx0mKoCMd",
                "replyto": "J97UGWagvs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5514/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5514/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 2ndj (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for your insightful review and kind words regarding our innovative concept of verification modules. We appreciate your recognition of the **robustness of our method** and your acknowledgment of our **efforts to address critical limitations in existing methodologies**. We have carefully followed your suggestion to revise our paper. Please let us know if you have any more comments and we are willing to answer more details and further improve our work.\n\n>W: The paper's experimental evaluation is somewhat limited, primarily focusing on VISPROG evaluation datasets. Expanding the range of datasets, especially including more varied and challenging ones, would significantly strengthen the demonstration of the method's generalizability. While the paper reports a 4% improvement on the GQA dataset, a broader dataset spectrum could provide a more comprehensive assessment of the method's effectiveness. \n\n> Q1: Could you elaborate on the choice of primarily using VISPROG evaluation datasets? Suggestion: Consider applying your method to a wider array of datasets, especially those that are more diverse and challenging. This could help in better demonstrating the method's generalizability and effectiveness across different scenarios.\n\n**A:** Thank you for your kind advice. We choose similar tasks as in VisProg for fair comparison and better demonstration of the improvements. In addition, we have added an abstract reasoning task KILOGRAM, `a dataset featuring extensively annotated tangram\npuzzles that necessitate the model\u2019s comprehension of abstract tangram shapes and their subsequent\nclassification`. This is a challenging task ` designed\nto evaluate the model\u2019s capacity to abstractly generalize, utilizing visually ambiguous stimuli to do\nso`. In **Section 5.2.4**, we demonstrate `how our method effectively utilizes conceptual components to enhance abstract image understanding.` Furthermore, in **Appendix E**, we present the details of our implementation of abstract reasoning. \n\n\n>W: The paper lacks an in-depth analysis of its limitations and potential failure cases. For visual language grounding, natural language reasoning, and visual abstract reasoning tasks where the model fails to exceed the performance of SOTA models, no further analysis is provided apart from reporting the results. Although several failure cases are mentioned in the attachment, they are not thoroughly investigated or analyzed. A detailed examination of these cases would be invaluable for identifying areas for further refinement and development of the method. \n\n>Q2: Why was there no detailed analysis provided for the failure cases mentioned in the attachment? Suggestion: It would be beneficial to include a thorough investigation and analysis of these failure cases. This analysis could offer insights into the method's limitations and guide future improvements.\n\n**A:** Thank you for your valuable advice. We have reassessed the failure cases in GQA. Both the statistical results and qualitative results are now presented in **Appendix C** of our revised paper. Additionally, supplementing the analysis in **Section 5.2**, we analyze the failure cases of tasks that fail to exceed the performance of SOTA models.\n\nIn conclusion, We have the following observations: \n1. Regarding the sampled failure cases in GQA, we observed a reduction in module errors by 28.87% and a decrease in planning errors by 42.35%. Our method seems to be more effective in resolving planning errors. We believe this improvement is due to the incorporation of verification and self-correction mechanisms in our tree-based search algorithm. In essence, there is a need to enhance the verification process for candidate predictions.\n2. In the GQA task, our method has corrected 31.2% of the failure cases. However, when compared to the general improvements on the test set, we found that our method also introduced new issues. Therefore, we analyzed the failure cases specific to our method and discovered that it tends to introduce a planning trace made of the VQA modules, which often perform poorly. This observation has led us to consider introducing penalties for certain modules that perform poorly.\n3. Upon analyzing failure cases in visual language grounding, natural language visual reasoning, and visual abstract reasoning tasks, where the model falls short of outperforming SOTA models, we identified two attributes in these reasoning traces that could contribute to subpar performance: (a) the traces for visual abstract reasoning and visual language grounding are relatively short, making few improvements by our verification modules, and (b) the traces for natural language visual reasoning are primarily composed of VQA modules, which could potentially serve as bottlenecks."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5514/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699949226401,
                "cdate": 1699949226401,
                "tmdate": 1699949226401,
                "mdate": 1699949226401,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8mlN92FbVd",
                "forum": "8Xx0mKoCMd",
                "replyto": "J97UGWagvs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5514/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5514/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer 2ndj,\n\nWe thank you again for your valuable comments and suggestions to help us improve our paper in the aspects of presentation, experiments, analysis, and discussion. We also appreciate that you noticed our `innovative concept in the field of visual programming`, `the robustness of the proposed methodology is evident through its application to two distinct models, yielding a notable 4% improvement in accuracy for compositional visual question answering tasks`, `a thorough preliminary study, demonstrating how individual components contribute to enhanced performance`, `excels in clearly articulating its methodology`,  `detailed explanations`, `the contributions of this paper are **highly significant** for the field of visual compositional reasoning`,  ` **addresses critical limitations** in existing methodologies and offers a **substantial improvement** in prediction accuracy, marking a notable advancement in the field`. These compliments have encouraged us a lot and will help us to make a timely contribution and broader impact on both the Computer Vision and the compositional reasoning community. \n\n\nOur current revised paper has taken into account all your suggestions, please kindly let us know if you still have follow-up questions.\n\nThanks,\n\nAuthors"
                    },
                    "title": {
                        "value": "Looking forward to your feedback and discussion"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5514/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700024912805,
                "cdate": 1700024912805,
                "tmdate": 1700024951809,
                "mdate": 1700024951809,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "g1sdbpnVaY",
                "forum": "8Xx0mKoCMd",
                "replyto": "J97UGWagvs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5514/Reviewer_2ndj"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5514/Reviewer_2ndj"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment from Reviewer 2ndj"
                    },
                    "comment": {
                        "value": "Hi there,\n\nThanks for addressing all of my questions. I appreciate your efforts in conducting additional error analysis on the failure cases. It makes sense to me that extra errors would be introduced when aggregating the modules, and the performance could be bottlenecked by one particular one (in your case, seems to be the VQA model). I've updated the soundness score in recognition of your analysis."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5514/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700681975043,
                "cdate": 1700681975043,
                "tmdate": 1700681975043,
                "mdate": 1700681975043,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "LOyfjl8q4X",
            "forum": "8Xx0mKoCMd",
            "replyto": "8Xx0mKoCMd",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5514/Reviewer_Tiuv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5514/Reviewer_Tiuv"
            ],
            "content": {
                "summary": {
                    "value": "This work focuses on resolving two key classes of errors in existing visual programming approaches:\n1. Reasoning Error: Error in common sense reasoning regarding which operation to perform \n2. Module Error: Error by individual modules (like object detection) in their outputs.\n\nThey introduce ExoViP, which adds verification steps (Image-text matching, captioning and VQA verifier) for each module to detect errors before they propagate. To solve planning errors, they maintain multiple reasoning traces and use beam search to select the best sequence of neural modules. The authors evaluate the method on five visual composition reasoning tasks (Compositional image question answering, Referring Expression Segmentation, Natural Language Visual Reasoning, Abstract Reasoning, and Language guided Image Genereation)."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The authors propose a well-motivated idea to overcome module-based and planning-based errors in Visual Programming through the use of verification scores and trace reasoning. \n2. The method reuses existing modules for obtaining verification scores. \n3. The evaluation spans multiple common Visual Composition Reasoning problems with an analysis for each type of problem. \n4. The writing is good, and the paper is easy to follow"
                },
                "weaknesses": {
                    "value": "1. The initial analysis finds the percentage of errors caused by the two types of problems (module error and planning error). The developed method's intuition is to solve these two errors with the two mentioned strategies. However, the final analysis does not include the kind of errors this method actually ends up fixing and in what ratio. It would be beneficial to show the statistics that demonstrate what proportion of errors of each type were fixed by the proposed methods compared to VISPROG [1].  \n2. The method is not evaluated with any open-source Large Language Models (LLaMA, Vicuna, etc.) Restricting evaluation only to GPT-3.5-turbo reduces the generalizability of the approach.  \n3. The performance increment across tasks over VISPROG [1] is still incremental, considering the time cost of using beam search and multiple verification modules."
                },
                "questions": {
                    "value": "1. In section 4.2 - You sort the candidate neural modules according to verification scores. From what your paper says verification scores are obtained for different candidates generated by a single neural module. Are these verification scores for the neural modules themselves different or are they the verification score of the top candidate of each neural module? \n2. What is the inference latency for your approach compared to using VISPROG [1]? The use of beam search and multiple verification modules indicates a significant added overhead. \n3. Both ViperGPT [2] and VISPROG [1] implementations used GPT-3 as the model of choice. Will the advantages of your method transfer to GPT-3 and other Open LLMs as well? Or is GPT-3.5-turbo necessary in order to accomplish Self-Correction and Trace Reasoning?  \n4. Could you report an analysis about what percentage of module errors and planning errors were fixed by your method compared to VISPROG? \n\n[1] Tanmay Gupta and Aniruddha Kembhavi. Visual programming: Compositional visual reasoning without training. In Conference on Computer Vision and Pattern Recognition (CVPR), pp. 14953\u2013 14962, June 2023\n\n[2] Sur\\'is D\\'idac, Sachit Menon and Carl Vondrick, Sachit Menon, and Carl Vondrick. Vipergpt: Visual inference via python execution for reasoning. arXiv preprint arXiv:2303.08128, 2023."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5514/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5514/Reviewer_Tiuv",
                        "ICLR.cc/2024/Conference/Submission5514/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5514/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699590438835,
            "cdate": 1699590438835,
            "tmdate": 1700681099012,
            "mdate": 1700681099012,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "cCkTkP205w",
                "forum": "8Xx0mKoCMd",
                "replyto": "LOyfjl8q4X",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5514/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5514/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Tiuv (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for your constructive comments and acknowledgment of our well-motivated idea. We carefully address each of your concerns as follows.\n\n\n> W1: The initial analysis finds the percentage of errors caused by the two types of problems (module error and planning error). The developed method's intuition is to solve these two errors with the two mentioned strategies. However, the final analysis does not include the kind of errors this method actually ends up fixing and in what ratio. It would be beneficial to show the statistics that demonstrate what proportion of errors of each type were fixed by the proposed methods compared to VISPROG [1].\n\n> Q4: Could you report an analysis about what percentage of module errors and planning errors were fixed by your method compared to VISPROG?\n\n**A:** Thank you for the valuable suggestion. In our original submission, we have manually analyzed 100 randomly sampled failure cases on VisProg. We find that there are three typical reasons for the failures: a) vision module prediction error; b) LLM planning error; c) others. We demonstrate the statistics of the failure cases in **Appendix.C**. We provide illustrations of two representative failure cases resulting from our methodology in **Appendix I**.\n\nIn our revised paper, following the application of our proposed framework, we re-assessed the same cases. As shown in **Appendix C**, \"`we were pleased to discover a reduction in module errors by 28.87%, and a decrease in planning errors by 42.35%. Nevertheless, our current strategy was unable to rectify 69.8% of the errors. When juxtaposed with the data from Table 1, our method has enhanced VISPROG by 7.11%, which is lower than the improvement of the failure cases. This outcome suggests that our approach may give rise to novel challenges. We further demonstrate common errors of our method in Figure 10 and Figure 11, As highlighted in the study of failure cases, the majority of these instances originate from the VQA module.`\"\n\n> W2: The method is not evaluated with any open-source Large Language Models (LLaMA, Vicuna, etc.) Restricting evaluation only to GPT-3.5-turbo reduces the generalizability of the approach.\n\n> Q3: Both ViperGPT [2] and VISPROG [1] implementations used GPT-3 as the model of choice. Will the advantages of your method transfer to GPT-3 and other Open LLMs as well? Or is GPT-3.5-turbo necessary in order to accomplish Self-Correction and Trace Reasoning?\n\n**A:** Thank you for the helpful suggestion. In fact, both the official implementations of [ViperGPT](https://github.com/cvlab-columbia/viper) and [VISPROG](https://github.com/allenai/visprog) are supported by the GPT-3.5-turbo. In accordance with this line of research, all the experimental results reported herein are also built on the GPT-3.5-turbo model. In addition, we replaced the GPT-3.5-turbo with LLama2-chat-13b and obtained the following results. We are thrilled to discover that our method can yield significant improvements in open LLM. We have added these results to **Section 5.3** in our revised paper.\n\n\n| model | accuracy on GQA | \n| -------- | -------- |\n| VISPROG (GPT-3.5-turbo)     | 57.41     |\n| ExoViP (GPT-3.5-turbo)     | 61.49     |\n| VISPROG (Llama-2-13b-chat)    | 46.41     |\n| ExoViP (Llama-2-13b-chat)     | 54.45     |\n\n\n> W3: The performance increment across tasks over VISPROG [1] is still incremental, considering the time cost of using beam search and multiple verification modules.\n\n> Q2: What is the inference latency for your approach compared to using VISPROG [1]? The use of beam search and multiple verification modules indicates a significant added overhead.\n\n**A:** Thank you for the constructive advice. We present the mean inference time on the GQA dataset. Generally, the temporal expenditure of our tree-based search method significantly surpasses that of VisProg. However, the majority of the time is consumed by the call of the  OPENAI API, an issue we posit is intrinsic to analogous works [3,4,5]. When compared to Depth First Search/ Breadth First Search [3] or Monte Carlo Tree Search [4,5], we assert that our beam search-based method can achieve an optimal equilibrium between efficiency and effectiveness. We have added this part to **Appendix D** in our revised paper.\n\n| model   | Total Inference Time (s) |  Planning Time (s)   | Module Inference Time (s) |\n| ------- | -------------------- | --- | -------------------------- |\n| VisProg | 1.59                 |  1.10   | 0.49    |\n| ExoViP  | 4.32                 |   3.64  | 0.68         |\n\n\n\n\n\n\n[1] Visual Programming: Compositional Visual Reasoning Without Training\n\n[2] ViperGPT: Visual Inference via Python Execution for Reasoning\n\n[3] Tree of Thoughts: Deliberate Problem Solving with Large Language Models\n\n[4] Alphazero-like Tree-Search can Guide Large Language Model Decoding and Training\n\n[5] Language Agent Tree Search Unifies Reasoning Acting and Planning in Language Models"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5514/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699948898447,
                "cdate": 1699948898447,
                "tmdate": 1699948898447,
                "mdate": 1699948898447,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SbaObUhQ5G",
                "forum": "8Xx0mKoCMd",
                "replyto": "LOyfjl8q4X",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5514/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5514/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking forward to your feedback"
                    },
                    "comment": {
                        "value": "Dear Reviewer Tiuv,\n\nPlease kindly let us know if you still need further clarification. We will be more than happy to answer.\n\nWe thank you again for your valuable comments and suggestions to help us improve our paper. We have tried our best to address all your concerns. We have also substantially revised our paper by following other valuable suggestions from other reviewers. Please take some time to look at them and see if you have further questions.\n\nBest,\n\nAuthors"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5514/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700025435468,
                "cdate": 1700025435468,
                "tmdate": 1700112501866,
                "mdate": 1700112501866,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZTIPcNsSGH",
                "forum": "8Xx0mKoCMd",
                "replyto": "LOyfjl8q4X",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5514/Reviewer_Tiuv"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5514/Reviewer_Tiuv"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the clarifications and the detailed response. I believe adding error analysis of your method on the selected cases and results on open source LLMs warrants an improved soundness score (I have updated the soundness score accordingly). I will maintain my overall rating as positive feedback."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5514/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700681144317,
                "cdate": 1700681144317,
                "tmdate": 1700681144317,
                "mdate": 1700681144317,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]