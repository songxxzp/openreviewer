[
    {
        "title": "Revisiting Link Prediction: a data perspective"
    },
    {
        "review": {
            "id": "23VYGivakH",
            "forum": "8Ur2xmuw7w",
            "replyto": "8Ur2xmuw7w",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2113/Reviewer_cwp2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2113/Reviewer_cwp2"
            ],
            "content": {
                "summary": {
                    "value": "The paper analyzes link prediction across diverse datasets from a data-centric perspective. Specifically, it considers that a link can be generated through Local structural proximity (LSP), Global structural proximity (GSP), and Feature proximity (FP). The experiments show discrepancies in the top-ranked edges according to the selected features. The paper suddenly changes to the proposition of a model based on feature proximity, latent node pairwise distance, and some hyperparameters. The paper supports this model claiming that node pairs with a large number of common neighbors tend to have low feature proximity and vice versa, and that this phenomenon is given once the network has been already formed over time. Finally, the paper suggests considering the feature space and structural space by different models and combining them at the end."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper shows a very significant result and contribution, that LSP, GSP, and FP should be considered differently when they are used for link prediction. More details about this conclusion are given, and it will be important to clearly define the time in this conclusion. As it is mentioned in the paper, the claim is based on a given network, that has already made several connections among them. \n\nThe proposition of the new theoretical model is interesting but not so novel. I mean, while the combination of LSP, GSP, and FP has been largely considered, the novelty is the inclusion of these characteristics in a different factor (more important to FP when nodes have a large distance and vice versa)."
                },
                "weaknesses": {
                    "value": "Readability. As it is stated in the submission instructions \"reviewers are not required to read the appendix\". In this case, I did not read the appendix. For this reason, I could understand specific details from the paper. While I understand the main idea, there are details, such as the parameter r that I could not understand. Unfortunately, the paper uses the appendix to explain very important parts of the paper; for example: \"Limitation, broader impact, and future works are in Appendix G, H, and I\". So, if you can not reduce and contain the paper to 9 pages, I suggest you submit the paper to a journal, instead of a conference. Note, I usually read the appendices of the paper. They usually have important proof and complementary results, but in this case, it is being used as a main part of the paper. \n\nThe details of the theoretical models are not explained in the main paper. There are several details without any type of information, for example, the parameter r. It seems then rather than a parameter, r is a hyperparameter, but no details about this are given in the main paper. I have similar issues with the \\beta parameter/hyperparameter, which is described as a feature proximity parameter, but no details about its calculation or suggestion are given. \n\nI understand the main idea of the theoretical model and its importance in the paper, but this is not used in the experiment section of the main paper."
                },
                "questions": {
                    "value": "Can you make the paper readable by itself using 9 pages in length?\nThanks for changing the paper."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2113/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2113/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2113/Reviewer_cwp2"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2113/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697892962997,
            "cdate": 1697892962997,
            "tmdate": 1700772989077,
            "mdate": 1700772989077,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ewczNc5zqm",
                "forum": "8Ur2xmuw7w",
                "replyto": "23VYGivakH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2113/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2113/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer cwp2(1/3)"
                    },
                    "comment": {
                        "value": "**Response:** \nWe thank the reviewer for the concerns on the readability. We are also glad to see that the reviewer appreciates \"our paper showing a very significant result and contribution\". \n\nFor the readability issue, we worked hard to improve the writing to make the revision self-contained. **The reviewer can directly see our new updated revision that is better self-contained. If you still have any concerns, please let us know immediately, and we will make sure that the paper is self-contained with clear writing.** In the following part, we first summarize your questions, explain them in detail, and show our improvement in the revision. Besides, we carefully check all the appendix sections to avoid missing any important parts of the paper in the main content. If you still have any concerns about the readability, please let us know. We are eager to **make sure the paper is self-contained within the 9-page limitation to meet the standard for ICLR.** \n\n**Questions are summarized and retold as follows**: Questions are majorly three-fold: (1) The main paper lacks some important details about parameters $r$ and $\\beta$. (2) Limitation, broader impact, and future works are in Appendix G, H, and I\" (3) I understand the main idea of the theoretical model and its importance in the paper, but this is not used in the experiment section of the main paper. We then provide explanations and the corresponding revision to those three questions step by step. \n\n**Question1:** The main paper lacks some important details about the parameter $r$ and $\\beta$.\n\n**Response1:** Thanks for pointing out this issue. In the original paper, we put most detailed explanations of symbols and parameters in Section 3.2 when we first introduce the latent space model. However, we do not repeat the detailed meanings of those parameters or notify the reader where they can find the detailed explanations in each lemma and proposition. We have made the change in the modification accordingly to avoid potential confusion. \n\n**The detailed explanation of those parameters is as follows.**  We first clarify that all those parameters are only utilized for the theoretical analysis, corresponding to different intrinsic properties of datasets. They show no relevance in training a deep model process. Therefore, all those stuff in the theoretical analysis should be called parameters, not hyperparameters. \n\n**Detail explanation for parameter $r$ is as follows.**\n> $r_i$ is a connecting threshold parameter corresponding to node $i$.With $\\alpha = +\\infty$, $\\frac {1}{1+ e^ {\\alpha (d_{ij}-\\max\\{r_{i},r_{j}\\})}}=0$ if $d_{ij} > \\max\\{r_i, r_j\\}$, otherwise it equals to $1$. Therefore, a large $r_i$ indicates node $i$ is more likely to form edges, leading to a potentially larger degree. Nodes in the graph can be associated with different $r$ values, allowing us to model graphs with various degree distributions. Such flexibility enables our theoretical model to be applicable to more real-world graphs.\n\nThe detailed discussion of parameter $r$ can be found at the beginning of Section 3.2 theoretical model. \n\n**Detail explanation for parameter $\\beta$ is as follows.**\n> Feature perspective provides complementary information, considering two nodes with high feature proximity but located distantly in the latent space should also be potentially connected. In line with this, we introduce the feature proximity parameter $\\beta_{ij}$ in the latent space. A larger $\\beta_{ij}$ indicates more likely for node $i$ and $j$ to be connected. \n\nThe detailed discussion of parameter $\\beta$ in Section 3.2 can be found on, the 4th line of page 5, starting with **(ii)**. Moreover, $\\beta$ is a feature proximity parameter in the theoretical model which does not have a direct calculation formulation. It is just for the theoretical analysis. In the experiment part, we calculate the heuristic method, FH, corresponding to feature proximity $\\beta$ for calculation. Details in the paper are shown in Section 3.1.  \n\n**Our modification in the revision:** We carefully check whether there is a description each time we mention each parameter. We add a simple description **each time** we mention the parameter and let the reader see more details in the corresponding part in section 3.2 of the main paper. The newly added content is as follows\n> $r_i$ is a connecting threshold parameter corresponding to node $i$ while a large $r_i$ indicates node $i$ is more likely to form edges. A more detailed description of $r$ can be found under Eq.(1). $\\beta_{ij}$ is the feature proximity parameter between nodes $i$ and $j$ in the latent space while a large $\\beta_{ij}$ indicates large feature proximity. A more detailed description of $\\beta$ can be found in Section 3.2."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2113/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699937698535,
                "cdate": 1699937698535,
                "tmdate": 1699937698535,
                "mdate": 1699937698535,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "evZ0djyG7T",
                "forum": "8Ur2xmuw7w",
                "replyto": "23VYGivakH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2113/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2113/Authors"
                ],
                "content": {
                    "title": {
                        "value": "The last day remind for reviewer cwp2"
                    },
                    "comment": {
                        "value": "Dear reviewer cwp2, as today is the final day of our discussion, we appreciate the opportunity to engage with you. If you have any remaining questions or concerns, please don't hesitate to share them with us, we will be happy to respond. Thank you."
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2113/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700659795279,
                "cdate": 1700659795279,
                "tmdate": 1700688676262,
                "mdate": 1700688676262,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "99RIcWcRmR",
            "forum": "8Ur2xmuw7w",
            "replyto": "8Ur2xmuw7w",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2113/Reviewer_wtEd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2113/Reviewer_wtEd"
            ],
            "content": {
                "summary": {
                    "value": "This is a benchmark paper on link prediction. The authors experiment comprehensively and come to some interesting obervations. In the first part, the authors analyze the data perspective from three heuristics (1)Local structural proximity(LSP) (2)Global structural proximity(GSP) (3) Feature proximity(FP). Two natural questions are studied: how much does each help and whether they have overlappings. Then the authors introduce a latent space model and then theoretically demonstrate that the model reflects the effectiveness of LSP, GSP, and FP factors.  Then, based on the undetstandings of the heuristic groups, the authors provide practical guidelines from model and data perspective."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The writing is easy to follow and the paper presentation is clear\n2. The overall motivation is clear and the work is comprehensive. Both experiments and analysis look good."
                },
                "weaknesses": {
                    "value": "I'd thank the authors to elaborate more on my questions below."
                },
                "questions": {
                    "value": "1. What is the role of heterophily when calculating Feature proximity? Moreover, could you give an example in homophilic graph that FP and LSP are conflictive or FP and GSP are conflictive. I am asking this because I feel the conflict comes from the different link forming mechanism where FP is just not appropriate.\n2. I am not quite sure but I think NeurIPS D&B track and TMLR might be more appropriate venues."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2113/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698507033880,
            "cdate": 1698507033880,
            "tmdate": 1699636143880,
            "mdate": 1699636143880,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qAz4HT8E7m",
                "forum": "8Ur2xmuw7w",
                "replyto": "99RIcWcRmR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2113/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2113/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer wtEd"
                    },
                    "comment": {
                        "value": "**Question1:** What is the role of heterophily when calculating Feature proximity? Moreover, could you give an example in the homophilic graph that FP and LSP are conflictive or FP and GSP are conflictive. I am asking this because I feel the conflict comes from the different link-forming mechanisms where FP is just not appropriate\n\n\n**Response:** Thanks for your question about the role of heterophily when calculating feature proximity. We are not quite sure whether we exactly understand some details of your question. Therefore, we will first retell your question and answer it accordingly. If there exists any misunderstanding, please let us know, we are happy to solve all your valuable concerns and improve our paper accordingly. \n\nOur current understanding of your question is two-fold: (1) How to describe the heterophily with the feature proximity factor? and (2) Does the conflict only happen when low FP(heterophily) but high LSP? How can the conflict happen when high FP(homophily) but low LSP? Can you show an example?\n\n**R1:** Heterophily is the opposite to the well-known homophily concept in social science [1] with the principle \"similarity breeds connection\". More specifically, homophily indicates that nodes with larger feature similarity (high feature proximity). In contrast, heterophily indicates smaller feature similarity (small feature proximity). The ogbl-ddi dataset is the typical example where most node pairs are feature heterophilic as indicated in Figure 2. \n\n**R2:** No, the conflict can also happen in the homophily case with high FP(homophily) and low LSP. \n- We first clarify that there exists the case where node pairs with high FH but low LSP. The theoretical evidence and the empirical verification can be found in Lemma 2 and Figure 4, respectively. In Figure 4, we can observe that there is a large proportion of test node pairs that can be predicted correctly by heuristic-derived feature proximity (high FP) but predicted wrongly by heuristic derived local structural proximity (low LSP). \n- We then explain more on why there exists the case feature-homophilic pairs that FP and LSP are conflictive. **The key concept is, that link prediction is to predict the node pairs that aren't connected yet, excluding those node pairs that are already connected. When node pairs have both high FP and LSP, they are more likely to be with a link that exists already, rather than still unconnected for prediction.** We are then to predict those unconnected node pairs without both high FP and LSP. In such cases, high FP or high LSP individually is strong evidence of a potential link connected. Therefore, in a homophilic graph, it is likely that the unconnected node pairs with high FP and low LSP(conflictive) have a high probability of being connected.\n- We want to clarify that in the link prediction task, feature proximity and structural proximity are all important, rather than the feature proximity dominating this problem. Evidence can be found in proposition 1 and 3. The feature and structure are two perspectives that may not necessarily be aligned with each other. Similar thoughts can also be found in social network analysis[2].\n\n\n[1] McPherson, M., Smith-Lovin, L., & Cook, J. M. (2001). Birds of a feather: Homophily in social networks. Annual Review of Sociology.\n[2] Abebe, Rediet, et al. \"On the effect of triadic closure on network segregation.\" Proceedings of the 23rd ACM Conference on Economics and Computation. 2022.\n\n\n**Weakness2:** I am not quite sure but I think NeurIPS D&B track and TMLR might be more appropriate venues.\n\n**Response:** Thank you for suggesting alternative venues for our paper, including NeurIPS D&B track and TMLR. We appreciate and consider your suggestion according to our paper's focus and contributions. We agree that the NeurIPS D&B track and TMLR are prestigious venues with a strong focus on deep learning and machine learning research. **We still believe that our paper is also well-suited for ICLR.** The reason is that, despite our analysis being from a data perspective, we are not limited to empirical findings revolving on the new suitable datasets and benchmarking existing algorithms. Our paper focuses more on describing data with theoretical analyses derived from deep insight into the link prediction task.** Such understandings align with both the model perspective** (models to capture diverse pairwise data patterns, e.g., local structural patterns, the number of paths, and structural position) **and the data perspective** (data dominated by three factors: FP, LSP, and GSP). Our paper brings innovative understandings in the field of link prediction, a topic that aligns well with ICLR's focus on cutting-edge research in deep learning."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2113/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699937588443,
                "cdate": 1699937588443,
                "tmdate": 1699937588443,
                "mdate": 1699937588443,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mHjp1JZppY",
                "forum": "8Ur2xmuw7w",
                "replyto": "99RIcWcRmR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2113/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2113/Authors"
                ],
                "content": {
                    "title": {
                        "value": "The last day remind for reviewer wtEd"
                    },
                    "comment": {
                        "value": "Dear reviewer wtEd, as today is the final day of our discussion, we appreciate the opportunity to engage with you. If you have any remaining questions or concerns, please don't hesitate to share them with us, we will be happy to respond. Thank you."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2113/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700659769515,
                "cdate": 1700659769515,
                "tmdate": 1700688662389,
                "mdate": 1700688662389,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "EMRXyylcYE",
            "forum": "8Ur2xmuw7w",
            "replyto": "8Ur2xmuw7w",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2113/Reviewer_Rht2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2113/Reviewer_Rht2"
            ],
            "content": {
                "summary": {
                    "value": "This paper explores the principles of link prediction in graphs from a data-centric perspective. It identifies three critical factors for link prediction: local structural proximity, global structural proximity, and feature proximity. The paper reveals relationships among these factors and highlights an incompatibility between feature proximity and local structural proximity. This insight has implications for the performance of Graph Neural Networks for Link Prediction (GNN4LP) models. The study offers practical guidance for model design and benchmark dataset selection in the field of link prediction."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1.  The paper takes a unique data-centric approach to the problem of link prediction, shifting the focus from model design to understanding the underlying data factors. This perspective is essential for providing valuable insights into the link prediction task and guiding future research.\n2.  The paper identifies and empirically validates three critical factors for link prediction: local structural proximity, global structural proximity, and feature proximity. This systematic analysis enhances our understanding of what influences link formation in graphs, contributing to a more comprehensive approach to link prediction.\n3. The revelation of an incompatibility between feature proximity and local structural proximity in link prediction is a significant finding. It highlights a vulnerability in existing GNN4LP models and suggests that certain links, primarily driven by feature similarity, may be challenging to predict accurately. This insight provides researchers with a new perspective on model limitations and opportunities for improvement."
                },
                "weaknesses": {
                    "value": "1. The paper introduces a latent space model for link prediction and provides theoretical guarantees for the identified data factors. Nonetheless, why to use such model is still so clear and motivated\n2. Instead of hit metric, mrr is also very important for link prediction. It could be better if there is additional analysis on MRR\n3. The work somehow shows correlation with the network evolution. Can you add some detailed discussion with the network evolution especially the dynamic case?"
                },
                "questions": {
                    "value": "see the weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2113/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2113/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2113/Reviewer_Rht2"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2113/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698582953077,
            "cdate": 1698582953077,
            "tmdate": 1700492725145,
            "mdate": 1700492725145,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "tlXPkRM5V9",
                "forum": "8Ur2xmuw7w",
                "replyto": "EMRXyylcYE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2113/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2113/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer Rht2(1/2)"
                    },
                    "comment": {
                        "value": "**Weakness1:** The paper introduces a latent space model for link prediction and provides theoretical guarantees for the identified data factors. Nonetheless, why using such a model is still so clear and motivated?\n\n**Response:** Thanks for your great question about the motivation for the latent space model. The primary answer is that the latent space model is particularly suited for link prediction as it connects the likelihood of a link forming between two nodes to their proximity in a latent metric space. It is developed based on the most central principle in link prediction: Similarity breeds connection [6]. That is, there is a higher probability of forming a link if two nodes have similar characteristics. \n\nThe latent space model is a widely adopted **theoretical model** that helps design many real-world problems, especially for network analysis [1,2,3,4] and physics. Particularly, the latent space model in network analysis aims to locate network information in some latent space that can describe both local and global structures. The selected latent space can be Euclidean space [1], unit hypersphere space [5], and so on to describe different properties.\n\nWe then utilize the latent space model for link prediction, based on the important data factors, considering both feature and structure perspectives comprehensively. Each node is associated with a location in a D-dimensional latent space. The latent space for link prediction is typically utilized to describe node proximity mathematically, where nodes close in the latent space are likely to share particular characteristics. We can easily connect the distance in the latent space with the probability of forming a link. And the distance in the latent space can be approximated with different data factors. The theoretical model is utilized to mathematically formulate the graph data and link prediction task. Such mathematical description can help us conduct theoretical analysis and bring insights into the effectiveness of these data factors (Proposition 1,2,3) and the relationship between different factors (Lemma 1,2). \n\n[1] Peter D Hoff, Adrian E Raftery, and Mark S Handcock. Latent space approaches to social network analysis. Journal of the american Statistical association, 97(460):1090\u20131098, 2002.\n[2] Sewell, Daniel K., and Yuguo Chen. \"Latent space models for dynamic networks.\" Journal of the american statistical association 110.512 (2015): 1646-1657.\n[3] Smith, Anna L., Dena M. Asta, and Catherine A. Calder. \"The geometry of continuous latent space models for network data.\" Statistical science: a review journal of the Institute of Mathematical Statistics 34.3 (2019): 428.\n[4] Gormley, Isobel Claire, and Thomas Brendan Murphy. \"A latent space model for rank data.\" ICML Workshop on Statistical Network Analysis. Berlin, Heidelberg: Springer Berlin Heidelberg, 2006.\n[5] Jiang, Diqiong, et al. \"Sphere Face Model: A 3D morphable model with hypersphere manifold latent space using joint 2D/3D training.\" Computational Visual Media 9.2 (2023): 279-296.\n[6] McPherson, M., Smith-Lovin, L., & Cook, J. M. (2001). Birds of a feather: Homophily in social networks. Annual Review of Sociology.\n\n**Weakness2:** Instead of hit metric, mrr is also very important for link prediction. It could be better if there is additional analysis on MRR\n\n**Response**: We agree that MRR is also a very important metric. As such, we also add results with MRR. They are added to the revision in Appendix E: \"ADDITIONAL RESULTS IN MAIN ANALYSIS\". We observe that our existing conclusions are unchanged via the inclusion of MRR and in fact, are further verified."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2113/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699937510047,
                "cdate": 1699937510047,
                "tmdate": 1699937510047,
                "mdate": 1699937510047,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HRHdp4YUR5",
                "forum": "8Ur2xmuw7w",
                "replyto": "HTwrnGOsFu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2113/Reviewer_Rht2"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2113/Reviewer_Rht2"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the response"
                    },
                    "comment": {
                        "value": "After careful reconsideration and review of the feedback provided by other reviewers, I decide to raise the score accordingly and further champion for this paper. I believe this paper can inspire many future directions in the link prediction and network evolution domains."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2113/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700492704847,
                "cdate": 1700492704847,
                "tmdate": 1700492704847,
                "mdate": 1700492704847,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "zqytyfyWKc",
            "forum": "8Ur2xmuw7w",
            "replyto": "8Ur2xmuw7w",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2113/Reviewer_zuoh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2113/Reviewer_zuoh"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the reasons why GNNs largely fail to deliver on the task conventionally called \"link prediction\", which is the task of recovering, rather than \"predicting\", missing links in a graph, from a data-oriented perspective. It suggests than the task of link recovery depends on three factors, namely the local structral proxmimity of nodes, their global structural proximity, and their feature proximity, and examines how these three factors interrelate. The core finding of the study is that conventional GNNs for link prediction fail when the feature proximity factor becomes dominant. Given these insights, the paper offers advice on how one could select data for selecting benchmark data to evaluation GNNs for link recovery."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "Drawing from previous work, the paper suggests and reconfirms the importance of three key factors underlying the task of link recovery and proposes a latent space model that effectively embeds nodes in a D-dimensional space, which is used to analyze the relationships among the proposed data factors, revealing an underlying tension between feature proximity and local structural proximity, in other words, the fact that co-occurrence of both high feature similarity and high local structural similarity rarely happens. This is an interesting empirical finding."
                },
                "weaknesses": {
                    "value": "The core finding of the paper is the features should also be taken in consideration. This finding has been known to the community, especially when it comes to using embeddings for link recovery. The proposed latent space model looks suspiciously similar to a node embedding used for link recover. As such, it calls for comparison with state-of-the-art works on embeddings for that purpose, which examine pretty much the same issues as this paper. Such a comparison or even discussion of the relationship and underlying novelty is missing."
                },
                "questions": {
                    "value": "Is the proposed latent space different from a node embedding, and if so, why?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2113/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699262748485,
            "cdate": 1699262748485,
            "tmdate": 1699636143735,
            "mdate": 1699636143735,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6uxIZYBk3U",
                "forum": "8Ur2xmuw7w",
                "replyto": "zqytyfyWKc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2113/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2113/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer zuoh (1/2)"
                    },
                    "comment": {
                        "value": "**Weakness1:** The core finding of the paper is the features should also be taken into consideration. \n\n**R:** Thanks for your concerns about the core findings of our paper. We want to re-clarify the contribution of our paper. Our paper does not aim to design a new deep-learning model to achieve state-of-the-art performance in link prediction (recovery). Instead, we aim to understand when link prediction algorithms work on particular graphs and when they do not, according to important factors leading to the future link formulation. To achieve this goal, we propose a theoretical latent space model to formally understand and analyze different data factors that influence GNN's performance. By identifying important data factors and how they affect the link prediction problem together, we can provide guidance for future GNN model design and dataset selection. **Notably, taking features into consideration is not our core finding.** We add features into consideration just to provide **a more comprehensive understanding across all data factors** which can easily be applied to all graphs. Moreover, our paper majorly focuses on **GNN4LP models, the state-of-the-art models in link prediction**, rather than node embedding methods with modest performance[4]. \n\n\n**Our contribution is still significant in understanding link prediction on those graphs where node features are not available.** In particular, we first exhibit the effectiveness of the local structural factor (proposition 1) and local structural factor (proposition 2). Then we find their underlying relationship: global structural proximity only shows effectiveness when local structural proximity is deficient in Lemma 1. \n\n\n\n**Weakness2 & Question:** Is the proposed latent space different from a node embedding, and if so, why? What is the difference between the latent space from the node embedding? Can you make a comparison between the latent space model and node embedding models since the latent space model and node embedding models utilize latent space and embedding, respectively, for link recovery? \n\n**Response:** Thanks for your great question about the connection between node embedding models and latent space models. Our primary answer is that they can not be directly compared as the latent space model is a **theoretical model** while the node embedding model is a **concrete graph learning algorithm**. \n\n**On the one hand,** node embedding model is a **concrete graph learning algorithm**, similar to GNN4LP models. Node embedding models aim to learn node embedding to preserve important data properties for link prediction. Nonetheless, they are not the SOTA since GNN4LP can perform much better[4]. That is the reason why we focus on GNN4LP models. **On the other hand,** the latent space model is a **theoretical model**, which helps us understand the important data factors for the link prediction task and how different link prediction algorithms work. Hence, indeed, the latent space model can be used to understand the graph embedding algorithms as well as GNNs discussed in our paper. The theoretical latent space model cannot be directly applied and learned from data to achieve satisfying performance. It serves to better understand those algorithms applied for future collection. More details on the above explanations are as follows."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2113/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699937290102,
                "cdate": 1699937290102,
                "tmdate": 1699937290102,
                "mdate": 1699937290102,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JY60zf59Le",
                "forum": "8Ur2xmuw7w",
                "replyto": "zqytyfyWKc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2113/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2113/Authors"
                ],
                "content": {
                    "title": {
                        "value": "The last day remind for reviewer zuoh"
                    },
                    "comment": {
                        "value": "Dear reviewer zuoh, as today is the final day of our discussion, we appreciate the opportunity to engage with you. If you have any remaining questions or concerns, please don't hesitate to share them with us, we will be happy to respond. Thank you."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2113/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700659714132,
                "cdate": 1700659714132,
                "tmdate": 1700688634676,
                "mdate": 1700688634676,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]