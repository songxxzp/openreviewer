[
    {
        "title": "Un-Mixing Test-Time Normalization Statistics: Combatting Label Temporal Correlation"
    },
    {
        "review": {
            "id": "mOA5uHpAX2",
            "forum": "xyxU99Nutg",
            "replyto": "xyxU99Nutg",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission138/Reviewer_omE5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission138/Reviewer_omE5"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel test-time normalization layer to tackle the temporally correlated, distributionally shifted problems within the context of test-time adaptation, and boost the performance of test-time adaptation tasks involving various distribution shifts. The experiments demonstrate the method achieved SOTA."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The logic of this paper is clear and the performance is excellent.\nThe author argues that the assumption of test batches conforming to an i.i.d distribution can produce unstable and unreliable adaptations. This paper works on it and offers a reasonable approach.  \n2. Extensive experiments on three benchmarks and solid ablation studies.\n3. The paper is well organized and the idea is well presented."
                },
                "weaknesses": {
                    "value": "After reading the paper, it is clear that the method delivers improvements on TTA tasks. However, there are some questions remain unanswered:\n0. Some papers also explored the test-time non-iid setting, such as [a] and [b]. Compared with these papers, what's the contribution of the proposed setting and approach?\n1. Why the hyper-parameter \\alpha is set to be 0.5 in Equation (5) for implementation while initializing UNMIX-TNS components?\n2. In Section 2.2.3, the hyper-parameter \\lambda is not explained clearly.\n3. While Tables 1 and 2 include 11 methods for comparison, the experiments on the video dataset (Table 3) only include seven methods.\n4. Does the UNMIX-TNS update the same parameters with other TTA methods? And have the affine parameters in BN layers been changed within the optimization? Does the method require only single forward inference or multiple forward passes?\n5. The authors mentioned that \u201conly components that closely align with the instance-wise statistics undergo updates\u201d, but equations (12) and (13) update all K BN statistics components with different assignment probabilities. \n\n\n[a] Robust Test-Time Adaptation in Dynamic Scenarios, ICCV2023\n[b] Robust continual test-time adaptation: Instance-aware BN and prediction-balanced memory. NeurIPS2022"
                },
                "questions": {
                    "value": "Please refer to the weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission138/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission138/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission138/Reviewer_omE5"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission138/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698667023607,
            "cdate": 1698667023607,
            "tmdate": 1699635939256,
            "mdate": 1699635939256,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1mG4cV7MwI",
                "forum": "xyxU99Nutg",
                "replyto": "mOA5uHpAX2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission138/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission138/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your comment!"
                    },
                    "comment": {
                        "value": "We appreciate your constructive comments and suggestions, and they are exceedingly helpful for us in improving our paper. We have carefully incorporated them in the revised paper. In the following, your comments are first stated and then followed by our point-by-point responses.\n\n> Comparison w.r.t ([a] RoTTA, [b] NOTE).\n\nThank you for your comment. In the Introduction, we reference studies ([a] RoTTA, [b] NOTE) that address adaptation under non-i.i.d. data streams. NOTE introduces instance-aware BN (IABN), and RoTTA proposes a robust batch normalization (RBN) module, which uses global statistics to robustly normalize feature maps. However, as our results show, these normalization layers alone struggle in non-i.i.d test-time adaptation scenarios. Their effectiveness is partly due to their use of a memory bank for sampling category-balanced data, thereby simulating an i.i.d. test-time adaptation environment. In contrast, UnMix-TNS's core concept is to maintain both current and past statistics of online test features across its K different components. For each test sample, we refine the most closely aligned components, allowing for the estimation of unbiased BN statistics without relying on a memory bank. Our experiments demonstrate that UnMix-TNS outperforms RBN and IABN in non-i.i.d setups, even when integrated with RoTTA and NOTE, as shown in Tables 1 and 2. This highlights the unique contribution and robustness of our approach in these challenging adaptation scenarios.\n\n> Hyperparameter $\\alpha$.\n\nThank you for your query regarding the choice of setting the hyperparameter $\\alpha$. As mentioned in Appendix A.2 of our manuscript, we analyze the source variance within the context of UnMix-TNS. Our analysis indicates that the source variance is contributed by (1) the average variance of individual components, and (2) the variance of the means of individual components. By setting $\\alpha=0.5$, we ensure that these two contributions to the source variance are balanced and have equal weight. This balance is crucial for the effective functioning of UnMix-TNS.\n\n> Hyperparameter $\\lambda$.\n\nThank you for highlighting the need for a clearer explanation regarding the hyperparameter $\\lambda$ in Section 2.2.3 of our paper. Equations (12) and (13) are derived based on the principles of momentum batch normalization (MBN), and the hyperparameter $\\lambda$ is set according to the MBN guidelines proposed by Yong et al. (2020). The primary objective of MBN is to ensure a consistent noise level introduced by BN across varying batch sizes. This consistency is crucial to maintain equivalent noise levels in both small and large batch scenarios. The tuning of $\\lambda$ is integral to achieving this standardization based on the specific batch size employed. For a more comprehensive understanding of $\\lambda$ and its significance in our method, we have included an in-depth explanation in the newly added Appendix A.4 of the manuscript.\n\n> Method comparison: 11 in Tables 1 and 2, only 7 in video dataset (Table 3).\n\nThank you for your comment regarding the number of methods included in the comparisons for the video dataset (Table 3). We aimed for a fair and relevant evaluation by aligning our experimental setup with that of LAME (Boudiaf et al., 2022), selecting top-performing baselines for comparison. The methods listed in Tables 1 and 2, which are not included in Table 3, either necessitate extensive reimplementation for application to video datasets or are not directly applicable to such datasets. This decision was made to ensure the most meaningful and practical comparison within the constraints of our experimental setup.\n\n> Query on UNMIX-TNS: parameter updates, BN layer changes, and forward pass requirements.\n\nIn response to the reviewer's queries about the UnMix-TNS method, as a standalone method, UnMix-TNS is designed to update only its internal statistics and requires only a single forward pass. However, when integrated with other TTA methods, UnMix-TNS solely replaces the Batch Normalization (BN) layers and behaves like the replaced BN in the same manner implemented in those TTA methods, including parameter updates. For instance,  in a TTA method like TENT, the affine parameters of the BN layers are updated, and UnMix-TNS will follow this same update protocol. Conversely, in methods like LAME, the focus is on updating only the internal statistics of normalization layers. In summary, UnMix-TNS can either operate independently with a single forward pass or align with the update mechanisms of other TTA methods it is paired with.\n\n> Discrepancy in author's claim and Equations (12) \\& (13) on updating BN statistics in UNMIX-TNS.\n\nWe acknowledge that our initial phrasing may have been misleading. In practice, we update all K BN statistics components, each with varying assignment probabilities. We have updated the manuscript to clarify this point more accurately."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission138/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700493468298,
                "cdate": 1700493468298,
                "tmdate": 1700493468298,
                "mdate": 1700493468298,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RzJL4wJGx9",
                "forum": "xyxU99Nutg",
                "replyto": "1mG4cV7MwI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission138/Reviewer_omE5"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission138/Reviewer_omE5"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your response"
                    },
                    "comment": {
                        "value": "Thank you for your detailed answers to the other reviewers and me. I keep my originally positive score and look forward to other reviewers' comments."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission138/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700570080293,
                "cdate": 1700570080293,
                "tmdate": 1700570080293,
                "mdate": 1700570080293,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "auGxJrhucy",
            "forum": "xyxU99Nutg",
            "replyto": "xyxU99Nutg",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission138/Reviewer_yuas"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission138/Reviewer_yuas"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces UnMix-TNS, a new approach to address the challenges of test-time adaptation in neural networks under non-i.i.d. conditions, particularly focusing on data streams with temporal correlation. Traditional batch normalization (BN) techniques, fundamental in stabilizing neural network training, fall short under non-i.i.d. test scenarios, often encountered in real-world applications like autonomous driving. UnMix-TNS innovatively recalibrates instance-wise statistics within BN by mixing them with multiple unmixed components, simulating an i.i.d. environment. This method is adaptable to various state-of-the-art test-time adaptation strategies and seamlessly integrates with pre-trained architectures that include BN layers. The approach is empirically validated across diverse scenarios and shows promise in effectively handling data with temporal correlations. New datasets have also been introduced."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. **Innovative Approach to Test-Time Adaptation:** The introduction of UnMix-TNS recalibrates instance-wise statistics by mixing with multiple unmixed statistics components. It targets the challenge of non-i.i.d. test batch distributions, a limitation in current test-time adaptation methods.\n2. **Seamless Integration with Pre-Trained Models:** The ability of UnMix-TNS to integrate with existing pre-trained neural network architectures without significant modifications is also an advantage. This facilitates easier adoption in various applications.\n3. **Empirical Validation:** The paper provides empirical evidence of performance improvements across multiple benchmarks, including those with common corruption shifts and natural shifts. UnMix-TNS demonstrates robustness in diverse scenarios. The introduction of new datasets like ImageNet-VID-C and LaSOT-C for realistic domain shifts in frame-wise video classification adds value."
                },
                "weaknesses": {
                    "value": "1. **Lack of Theoretical Insight for Temporal Correlation Handling:** The abstract and introduction do not provide clear theoretical insights or explanations on how UnMix-TNS effectively deals with test data streams having temporal correlation. A deeper understanding of the underlying principles is crucial for assessing the robustness and reliability of the method.\n2. **Potential for Bias in Instance Selection:** The effectiveness of UnMix-TNS might be influenced by the selection of instances from incoming test batches for updating statistics. If the selection is not well-designed, it could introduce bias, affecting the generalization of the model."
                },
                "questions": {
                    "value": "- Could the authors provide more detailed theoretical insights or foundational principles on how UnMix-TNS adapts to test data streams with temporal correlation? Specifically, how does the method theoretically account for and mitigate issues that arise due to temporal dependencies in the data?\n- What is the theoretical rationale behind the 'unmixing' approach in handling temporally correlated data? How does this strategy compare with traditional methods in terms of theoretical robustness against temporal variations in data streams?\n- Could the authors elaborate on the criteria or algorithm used to select instances from incoming test batches for updating the UnMix-TNS statistics? How do the authors ensure that this selection process does not introduce bias, which might affect the model\u2019s generalization capability?\n- In situations where the test batches contain highly diverse or outlier instances, how does UnMix-TNS maintain the integrity of the normalization statistics? Is there a mechanism in place to detect and appropriately handle such anomalies?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission138/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698788153376,
            "cdate": 1698788153376,
            "tmdate": 1699635939182,
            "mdate": 1699635939182,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "q1bV2DokPG",
                "forum": "xyxU99Nutg",
                "replyto": "auGxJrhucy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission138/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission138/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your comment!"
                    },
                    "comment": {
                        "value": "Thank you for your constructive comments and suggestions; and they are exceedingly helpful for us in improving our paper. We have carefully incorporated them in the revised paper. In the following, your comments are first stated and then followed by our point-by-point responses.\n\n> Ambiguity regarding instance selection and potential bias. \n\nWe would like to clarify any confusion in the submitted paper regarding the potential for bias in instance selection. For a given batch of test feature-map instances, represented as $\\mathbf{z}\\in \\mathbb{R}^{B\\times C\\times H \\times W}$ (where $B$ is the number of instances, $C$ the number of channels, $H, W$ the width and height of an instance feature-map), **we do not perform any selection of the test features for updating the components statistics**. Instead, every individual feature map $z[b]$ $\\in \\mathbb{R}^{C\\times H \\times W}$ is soft-assigned to the $K$ components of UnMix-TNS. This assignment is based on the cosine similarity between the mean of each instance-wise feature map and the centers of the components (Equation 7). Based on this similarity given by $p_{b,k}$, we update the parameters $\\mu_k^t$, $\\sigma_k^t$ of all $K$ components (Equations 12 \\& 13 in the paper). We acknowledge the importance of this clarification.\n\n> More detailed theoretical insights to handling temporally correlated data, comparison with traditional methods and potential bias issue.\n\nWe appreciate your thoughtful review and valuable suggestion to provide a deeper theoretical understanding of how our model accounts for temporal correlation. In response, we have expanded our theoretical analysis. In our revised manuscript, we have added a dedicated section (A.3 in the Appendix) to address this topic comprehensively. We also compare UnMix-TNS with the standard test-time normalization method (TBN) in section A.3. Furthermore, we have highlighted why UnMix-TNS can help mitigate the bias introduced in the estimation of feature normalization statistics, which arises due to the time-correlated feature distribution.\n\n>  Dealing with highly diverse or outlier instances.\n\nWe appreciate the reviewer's concern about handling highly diverse or outlier instances in test batches. We want to emphasize that we adopt the same setting as other test-time normalization methods for a fair comparison by assuming that temporally correlated test batches are free from outliers, such as instances from novel or unknown categories. This is an important consideration, as the presence of outliers may result in inaccurate normalization statistics estimations in UnMix-TNS, an issue that is also applicable to other methods. \n\nFortunately, we are already planning to extend our work to include detecting and excluding anomalies during the test-time adaptation process. The envisioned approach involves leveraging an anomaly detection score, calculated based on the distance metric of test instances to the nearest component in the UnMix-TNS and will utilize an optimal threshold to identify outliers effectively. By identifying and pruning outlier instances using this strategy, we aim to prevent the adverse impact these instances might have on adapting the model. This will significantly improve the robustness and accuracy of UnMix-TNS in dealing with diverse test batches. We have outlined this planned improvement as a key focus for our future work in the revised version of our manuscript (conclusion section)."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission138/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700493158145,
                "cdate": 1700493158145,
                "tmdate": 1700493158145,
                "mdate": 1700493158145,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SPM9KyBfYl",
                "forum": "xyxU99Nutg",
                "replyto": "q1bV2DokPG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission138/Reviewer_yuas"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission138/Reviewer_yuas"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your comments"
                    },
                    "comment": {
                        "value": "My concerns have been addressed. Thank you very much.\n\nKind regards,\n\nReviewer yuas"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission138/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700598316505,
                "cdate": 1700598316505,
                "tmdate": 1700598316505,
                "mdate": 1700598316505,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "fr2MW9P5jr",
            "forum": "xyxU99Nutg",
            "replyto": "xyxU99Nutg",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission138/Reviewer_auPw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission138/Reviewer_auPw"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes Un-Mixing Test-Time Normalization Statistics (UnMix-TNS), a novel BatchNorm adaptation strategy under the assumption that the test distribution is a shifting mixture. Empirical experiments on multiple CV tasks indicate that UnMix-TNS can adapt to the temporal correlation of unlabeled test streams."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Novel idea!\n\nIntuitive toy example (i.e. Figure 1) illustrating the motivation of the proposed method.\n\nGood empirical performance, improving over baseline methods most of the time. Mixes well (no pun intended) with other BN adaptation methods like Tent and LAME, making them even better. Synergy!\n\nNegligible additional inference time cost (the authors should highlight it instead of burying it under a subsection titled \"Ablation Studies\")."
                },
                "weaknesses": {
                    "value": "UnMix-TNS seems tightly coupled to the convolutional layers, as it considers the channel dimension $C$ and all experiments are computer vision tasks. It is unclear whether it will work for other deep learning tasks, e.g. NLP.\n\nThe paper doesn't discuss how to choose $K$ and $\\lambda$, and the additional experiments in Appendix C only study the sensitivity to $K$.\n\nAlgorithm 1 in Appendix B.2 is both helpful and confusing: helpful because it describes the algorithm concisely; but confusing because the shapes don't add up. For example, `instance_mean` should have shape `(B, 1, C)`, otherwise you cannot get `hat_mean` of size `(B, K, C)` through broadcasting. It would be better to include a runnable Python snippet in the Appendix, even if that's slightly more verbose."
                },
                "questions": {
                    "value": "In Appendix B.2, it's unclear what `torch.var_mean(x, dim=[2, 3])` means. Does `x` have dimension `(batch, channel, height, weight)`?\n\nFrom my understanding, UnMix-TNS will fail when test samples gradually shift from one cluster to another, in which case the active $\\mu_k$ would follow the sample, causing two clusters to eventually overlap. How would you prevent that?\n\nNot really a question, but an interesting experiment is to set $K$ to the number of corruption types, and see if $p_{b,k}^t$ can recover the corruption type for each sample correctly. Maybe each cluster will correspond to the label instead of corruption. Who knows?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission138/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission138/Reviewer_auPw",
                        "ICLR.cc/2024/Conference/Submission138/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission138/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698824444382,
            "cdate": 1698824444382,
            "tmdate": 1700713462058,
            "mdate": 1700713462058,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "EhT1X7qw7A",
                "forum": "xyxU99Nutg",
                "replyto": "fr2MW9P5jr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission138/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission138/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your comment! (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for your constructive comments and suggestions; and they are exceedingly helpful for us in improving our paper. We have carefully incorporated them in the revised paper. In the following, your comments are first stated and then followed by our point-by-point responses. \n\n> Applicability of UnMix-TNS beyond vision tasks, e.g., NLP.\n\nThank you for your comment regarding the integration of UnMix-TNS with convolutional layers in computer vision tasks. Our work is specifically tailored to the batch normalization layer in image classification models. While the extension of UnMix-TNS to NLP tasks is an interesting concept, it falls outside the scope of this paper. This decision is guided by the prevalent use of layer normalization in NLP, which differs from our focus on batch normalization. In summary, exploring UnMix-TNS in NLP context would require a different approach, considering the distinct normalization techniques used in NLP.\n\n> Discussion on how to choose $K$ and $\\lambda$.\n\nThank you for pointing out the need for a clearer explanation of the selection of $K$ and $\\lambda$ in our paper. Regarding $K$, its choice is crucial, as emphasized in our future work. A single UnMix-TNS component can efficiently represent several similar classes (e.g., car, truck, bus), allowing for an accurate approximation of the test distribution with a lower $K$ value. This approach is particularly beneficial for datasets with distinct class similarities. However, in datasets with a large number of classes like CIFAR100-C, DomainNet-126, or ImageNet-C, aligning $K$ with the number of classes may lead to inefficiencies and an increased memory footprint. For instance, setting $K$ to 100 in CIFAR100-C tests led to reduced performance. The optimal $K$ varies with the testing scenario; larger $K$ values are necessary for mixed domains with diverse styles, while smaller $K$ values are sufficient in single or continual domains due to reduced variability. In summary, choosing $K$ in UnMix-TNS is about balancing class/domain complexity with model efficiency, and we plan to refine these guidelines further in our future research.\n\nConcerning $\\lambda$: Equations (12) and (13) are derived based on the principles of momentum batch normalization (MBN), and the hyper-parameter $\\lambda$ is set according to the MBN guidelines proposed by Yong et al. (2020). MBN aims to standardize the noise level introduced by Batch Normalization (BN) across different batch sizes, ensuring comparable noise levels between small and large batches. The tuning of $\\lambda$ is integral to achieving this standardization based on the specific batch size employed. For a more in-depth explanation, we have added new information in Appendix A.4 of our paper.\n\n> Algorithm 1 in Appendix B.2: Concise but confusing with shape mismatch.\n\nThank you for your valuable feedback regarding Algorithm 1 in Appendix B.2. We understand your concerns about the clarity of the shapes used in the algorithm and apologize for any confusion caused. To address this, we have revised Algorithm 1 to ensure the correct alignment of tensor shapes.\n\nIn the updated algorithm, we have introduced the **unsqueeze** operation to adjust the dimensions of various tensors appropriately. The **instance\\_mean** is now unsqueezed along dimension 1, altering its shape to (B, 1, C). This adjustment allows for the correct broadcasting when calculating the **hat\\_mean** of size (B, K, C). Additionally, we have applied the unsqueeze operation to self.components\\_means along dimension 0, resulting in a shape of (1, K, C). Furthermore, we have unsqueezed $p$ along the last dimension, changing its shape to (B, K, 1). By incorporating all the above steps, we can now correctly combine these tensors, ensuring the resultant **hat\\_mean** has the intended size of (B, K, C). We hope that these clarifications and the revised algorithm will make the implementation more straightforward and eliminate any confusion.\n\n> Unclear dimension explanation in Appendix B.2 for torch.var\\_mean(x, dim=[2, 3]).\n\nThank you for your query. We would like to clarify that in this context, the variable $x$ indeed represents a batch of intermediate features within a Convolutional Neural Network (CNN). As is typical with CNNs, the tensor $x$ possesses four dimensions: batch size, number of channels, height, and width. Therefore, when we call torch.var\\_mean(x, dim=[2, 3]), the function calculates the variance and mean across the height and width dimensions of the tensor $x$. This operation effectively computes the instance-wise statistics for each feature in the batch, considering the spatial dimensions (height and width) of the CNN's intermediate feature maps."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission138/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700492552502,
                "cdate": 1700492552502,
                "tmdate": 1700492552502,
                "mdate": 1700492552502,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mnc87MsWUe",
                "forum": "xyxU99Nutg",
                "replyto": "fr2MW9P5jr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission138/Reviewer_auPw"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission138/Reviewer_auPw"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks to the authors for the additional clarification and experiments. Overall, I think addressing temporally correlated test data is a realistic and interesting research direction, and this paper did a good job tackling it. I have increased my rating to 8. I look forward to your future work, hopefully with real-world applications.\n\nBefore going into details, I will reiterate my recommendation that you should put the phrase \"with negligible inference latency\" somewhere in the abstract, or maybe even do an experiment comparing the inference latency with other methods like Tent. UnMix-TNS looks very complex and use cases like autonomous driving care about inference latency a lot, so it's good to know it admits an efficient implementation.\n\n> To address this, UnMix-TNS is designed to update the statistics of the active component slowly, using an exponential moving average. This method of updating helps in mitigating the risk of rapid shifts in the active $\\mu_k$.\n\nI was worried about a gradual, instead of rapid, shift in feature representation. As you pointed out, the cluster mean $\\mu_k$ is pretty robust to rapid shifts thanks to the use of a moving average, but if the shift is gradual I'm not sure how a moving average helps. On the other hand, I should probably not worry too much about that, because we won't see a lot of samples sitting on the decision boundary in practice: those correspond to ambiguous samples that are information-theoretically hard to classify.\n\n> In line with your suggestion, we have conducted additional experiments on CIFAR10-C under mixed-domain scenarios.\n\nSorry for the confusion, but what I meant is \"To what extent does each instance's cluster membership correlate with its class label?\". However, your result is still helpful: I can infer that the answer is probably \"not much\", because each class can have multiple subclasses/domains, e.g. a dog in the snow may look more like a deer in the snow than a dog on the grass."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission138/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700712763255,
                "cdate": 1700712763255,
                "tmdate": 1700714146577,
                "mdate": 1700714146577,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "0WQtg24haE",
            "forum": "xyxU99Nutg",
            "replyto": "xyxU99Nutg",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission138/Reviewer_Yi31"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission138/Reviewer_Yi31"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies how to adapt the batch norm layers for test-time adaptation. Standard TBN method assumes that test samples are i.i.d. sampled from a single target distribution, while this assumption can be violated in real scenarios: samples can be drawn from multiple distributions and can be temporal correlated. The author propose a nuanced method called UnMix-TNS, which split the running statistics of each BN layer into K different components. For each testing sample, only the closest component will be refined, which makes the BN layer more stable and robust to changes in batch-wise label distribution. The author verify the proposed method over benchmarking datasets, several settings, a wide range of models, and compare to a variety of baselines."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Clear motivation: The author clearly summarizes key drawbacks of BN-based TTA methods. Figure 1 is intuitive and informative. \n2. Algorithm design: The algorithm is generally intuitive to me. Clearly if each component corresponds to a label, the algorithm is likely to be robust to the bias in label distribution. Also the author pay special attention on the initialization of UnMix-TNS components to preserve the statistical properties. \n3. Experiments: The proposed method is tested on a variety of datasets, models, evaluation protocols, and is compared to a wide range of related baselines."
                },
                "weaknesses": {
                    "value": "Major (These weaknesses or concerns significantly affect my understanding and decision regarding this paper)\n\n1. Multiple target domains: One of the claimed contribution is that UnMix-TNS has robustness when tested on continual domain and mixed domain. However, I am very confused which part of UniMix-TNS is designed for and beneficial to these multi-domain settings, especially the mixed domain setting. Considering that there is a new batch containing images from different domains, although different images may correspond to different UnMix-TNS components, they are finally normalized with the same mean and variance according to Eq (11). It seems like UnMix-TNS still treat multiple target domains as one single target domain. \n2. Unclear experimental setting and unsatisfactory performance: The author claimed that they follow the protocol outlined in (Lim et al., 2023). However, the performance for most of the baselines are significantly worse than the results in (Lim et al., 2023). Also, the proposed UnMix-TNS fails to outperform \u201cSource\u201d in mixed domains. I believe the comparison to baselines only makes sense if the test-time normalization is beneficial. (I presume that \u201cSource\u201d means no adaptation. Please correct me if I am wrong)\n3. If the temporal correlation mainly \n4. Choice of K. The appendix discusses the influence of K. However, it is still unclear how to choose K in practice. How is it related to the number of classes and number of domains? (Although the number of domains might not be exposed.)\n\nMinor (These minor weaknesses are not crucial but I believe fixing them will improve the quality of the paper) \n1. Figure 2 is pixelated. Please consider improving the dpi. \n2. The temporal correlation might be better explained in Section 2.1. Does it refer to correlation of feature, label, or domain? \n3. Page 5 after Equation (1), two exp have different font style. Also I recommend changing $\\sim$ to $\\approx$ since $\\sim$ usually means \u201cfollowing the distribution of\u201d."
                },
                "questions": {
                    "value": "Besides the major weaknesses, there are several minor questions: \n1. In Equation (5), it seems like all $\\mu_{k, c}^0$ for different $k$ distribute on the line of $\\mu_c + t \\sigma_c$. Is that intentional? Or it makes more sense if they do not have such low-rank structure? \n2. Equation (7). Are there any insight on using cosine similarity instead of L2 distance?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "I do not have any ethics concerns."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission138/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission138/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission138/Reviewer_Yi31"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission138/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699576807987,
            "cdate": 1699576807987,
            "tmdate": 1699635939026,
            "mdate": 1699635939026,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gZtWx9c6yI",
                "forum": "xyxU99Nutg",
                "replyto": "0WQtg24haE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission138/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission138/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your comment! (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for your constructive comments and suggestions. We have carefully addressed them. In the following, your comments are first stated and then followed by our point-by-point responses.\n\n> How does UnMix-TNS effectively handle mixed domain settings when its normalization suggests it treats multiple domains as a single one?\n\nThank you for your insightful comment. We agree that UnMix-TNS does not incorporate a specific module tailored for mixed domain settings. However, we want to shed light on the fact that UnMix-TNS shows robustness against temporal correlations (different values of Dirichlet coefficient $\\delta$) of the test images with respect to their inaccessible labels compared to other test time normalization schemes.\n\nThe core functionality of UnMix-TNS lies in its ability to retain current and past statistics of online test features through its $K$ components, functioning as a time-decaying memory. In cases where test features are label-correlated, UnMix-TNS effectively simulates an i.i.d. scenario. It accomplishes this by estimating the current mean and standard deviations from its $K$ components, each representing different groups of test features encountered previously.\n\nAdditionally, the gradual update of the $K$ components over time enables UnMix-TNS to adeptly handle continual domain settings, where domain shifts occur sequentially rather than randomly. This capability was demonstrated in our experiments, showing UnMix-TNS's effectiveness not only in single domain settings but also in continual domain scenarios. Therefore, while UnMix-TNS may not specifically target mixed domain settings, its underlying design principles and operational mechanics provide flexibility and adaptability across varying domain configurations, including mixed domain. Furthermore, as supported by our experiments, our method's adaptability and performance in mixed-domain scenarios can be further enhanced by adjusting the momentum hyperparameter $\\lambda$ and appropriately increasing the value of $K$ to encompass the style diversity in the mixed domain.\n\n Finally, our claim regarding the mixed domain scenario has been revised in the updated manuscript.\n\n> Unclear experimental setting, with UnMix-TNS not surpassing \"Source\" in mixed domains and baseline performances falling behind those in Lim et al. (2023)\n\nThank you for your comment and for highlighting these concerns. Firstly, we apologize for any confusion caused by the experimental protocol reference. We followed the protocol set out in Marsden et al., 2023, not Lim et al., 2023. This error has been corrected in our revised manuscript.\n\nRegarding the performance of baselines, it's important to note that Lim et al., 2023, primarily reported baseline performances in an i.i.d. setting. They only report the performance of their method for class imbalance for the single domain setting. Our approach with UnMix-TNS, while occasionally not surpassing the source model, does show consistent improvements over other test-time normalization schemes, particularly where these schemes may not have yielded significant enhancements. Additionally, it's important to recognize that our results are consistent with Lim et al., 2023's observation. They noted underperformance compared to the source model in class imbalance settings and indicated that relying solely on normalization techniques may not always lead to significant improvements in non-i.i.d settings.\n\nHowever, UnMix-TNS stands out by offering notable improvements to test-time adaptation methods. In some cases, it even helps these methods outperform the source model in different scenarios, including mixed domain. For instance, when integrating UnMix-TNS with ROID, we observed a significant performance leap over the source model, a feat not achieved with TBN as the normalization technique. It is noteworthy that UnMix-TNS outperforms the source model in the mixed domains scenario on DomainNet-126 and CIFAR10-C datasets. While the results for mixed domains have been reported with $K=128$ in the manuscript, increasing $K$ to $K=1024$ shows even more impressive results on CIFAR100-C. Specifically, the error rate on CIFAR100-C decreases to 45.8\\%, outperforming the source model by 0.7\\%.\n\nNevertheless, we can enhance the performance of UnMix-TNS in mixed domain adaptation scenarios by tuning the $\\lambda$ hyperparameter. As detailed in Appendix A.4, this adjustment, guided by momentum batch normalization (MBN) principles, ensures consistent noise levels across different batch sizes. $\\lambda$ is calculated as $\\lambda=1-(1-\\lambda_0)^\\frac{B}{B_0}$ where $\\lambda_0$ and $B_0$ represent the ideal values, and $B$ is the actual batch size. In mixed domain scenarios, with more varied styles and higher noise, we suggest a modified formula $\\lambda=1-(1-\\lambda_0)^\\frac{B}{B_0*15}$ considering $15$ as the number of corruptions. This adjustment lowered the error rate on CIFAR100-C to 45.9\\%, slightly outperforming the source model."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission138/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700491642964,
                "cdate": 1700491642964,
                "tmdate": 1700492027848,
                "mdate": 1700492027848,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ge8Fk9rvqU",
                "forum": "xyxU99Nutg",
                "replyto": "0WQtg24haE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission138/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission138/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your comment! (2/2)"
                    },
                    "comment": {
                        "value": "> If the temporal correlation mainly...\n\n It seems the point about temporal correlation was not completed. Could you please elaborate on this, so we can address it more effectively?\n\n> Choice of $K$\n\nWe appreciate your inquiry regarding the selection of $K$ in our UnMix-TNS framework. The determination of $K$ is a critical element, as we have previously mentioned in our discussion of future work. To begin, we highlight that a single UnMix-TNS component can sometimes effectively represent multiple similar classes (e.g., car, truck, bus). This means that even with a relatively low $K$ value, our model can approximate the mean and variance of the test distribution accurately. This approach is particularly beneficial when class similarities are pronounced. However, in scenarios with a large number of classes, such as CIFAR100-C, DomainNet-126, or ImageNet-C, setting $K$ equal to the number of classes can lead to inefficiencies. Specifically, it can result in an excessive memory footprint, which is impractical for many applications. For instance, in our experiments with CIFAR100-C, we observed that setting $K$ to 100 actually decreased performance.\n\nFurthermore, the optimal value of K varies depending on the test scenarios. In mixed domain scenarios, where there is a diverse range of styles in the test distribution, a larger $K$ is necessary to capture this diversity effectively. This contrasts with single domain scenarios, where the domain remains consistent, or continual domain scenarios, where the domain shifts sequentially. In these cases, a smaller $K$ may be sufficient due to the reduced variety in the test distribution.\n\nIn summary, the choice of $K$ in UnMix-TNS is a balance between the complexity of the classes and domains and the practical considerations of model efficiency. While a higher $K$ may offer more nuanced representation in diverse settings, it also comes with trade-offs in terms of computational and memory requirements. Our ongoing research aims to refine these guidelines further and provide more concrete recommendations in various application contexts.\n\n> Minor Weakness\n\nThank you for your feedback regarding minor weakness. We acknowledge the pixelation issue of Figure 2 and have improved the dpi to enhance its clarity in the revised manuscript. Also, in the revised manuscript (Section 2.1, second paragraph, Test-time adaptation under label temporal correlation), we have clarified that it specifically refers to the temporal correlation of labels.  We thank you for noting the font style inconsistency after Equation (1) on Page 5; we corrected this in the revised manuscript. Also, we appreciate your suggestion to replace $\\sim$ with $\\approx$ for clarity in denoting approximation rather than distribution. This change is implemented to enhance the precision of the expressions in the paper.\n\n> Equation (5)\n\nIn response to the reviewer's comment on Equation (5) regarding the distribution of $\\mu_{k,c}^0$, we would like to clarify that $\\mu_{k,c}^0$ is a scalar quantity representing the mean of the $k^{th}$ UnMix-TNS component for channel $c$, and $t$ is a scalar sampled from a Normal Distribution $\\mathcal{N}(0,1)$. Given this, the vector $\\mu_k$ does not lie on a line and has no low-rank structure.\n\n> Using cosine similarity instead of L2 distance?\n\nWe thank the reviewer's comment. The L2 distance is susceptible to the curse of dimensionality, and as the number of channels increases, the contrast between L2 distances ($b^{th}$ instance's statistics and the $K$ statistic components) tends to diminish. Consequently, it becomes necessary to decrease the temperature $\\tau$ when computing the assignment probability $p_{b,k}$ in Equation (10) to avoid updating all $K$ components with the same weight. Such a distance function, therefore, requires careful setting of the temperature for every layer. On the other hand, cosine similarity does not suffer from this problem and only requires fixing one unique and shared temperature for all layers. Thus, we have opted to retain this score."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission138/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700492009765,
                "cdate": 1700492009765,
                "tmdate": 1700492077153,
                "mdate": 1700492077153,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7SHaNmf3OL",
                "forum": "xyxU99Nutg",
                "replyto": "ge8Fk9rvqU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission138/Reviewer_Yi31"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission138/Reviewer_Yi31"
                ],
                "content": {
                    "title": {
                        "value": "Thanks!"
                    },
                    "comment": {
                        "value": "Thanks for your clarification! \n\n- Mixed domain: I appreciate your clarification and revision on the manuscript. I do believe this is an interesting topic for further research, which is beyond the scope of this paper. \n- Experimental setting: Thanks for your clarification! \n- Temporal correlation: I apologize for incompleted comments. I was wondering how temporal correlation is defined. Since it seems like it is mainly about temporal correlation on labels, I believe further comparison with Test-Time Prior Adaptation [1] methods will significantly improving the soundness of this paper. \n- Choice of $K$: Thanks for your discussion. I still believe that an empirical way to choose K will be beneficial. Just as you said, an impropriate choice of $K$ can decrease the performance. If we do not have any method to avoid that, the proposed algorithm will not be very practical. \n\nIn a butshell, thanks for your rebuttal! I apologize again for incomplete comments. In general, I think this is a paper with interesting idea and solid results. But I still believe the points above may contribute to the quality of this paper. I keep my score of 6 at this stage. Good luck!"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission138/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700608133116,
                "cdate": 1700608133116,
                "tmdate": 1700608133116,
                "mdate": 1700608133116,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wKIUD4hcI7",
                "forum": "xyxU99Nutg",
                "replyto": "0WQtg24haE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission138/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission138/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks!"
                    },
                    "comment": {
                        "value": "Dear Reviewer Yi31,\n\nThank you for your thoughtful comments. We appreciate your recognition of our revisions and value your suggestions for further enhancing our manuscript.\n\n**Mixed Domain**: We are pleased our clarification on the mixed domain aspect was well-received. \n\n**Experimental Setting**: We appreciate your acknowledgment of the clarification we provided on the experimental setting.\n\n**Temporal Correlation**: Regarding UnMix-TNS, our aim was to ensure robustness against temporal correlations (different values of Dirichlet coefficient $\\delta$) of the test images concerning their labels compared to other test time normalization schemes. If your reference to Test-Time Prior Adaptation methods pertains to label distribution shifts between source and target domain, this indeed offers a relevant yet distinct scenario from our current focus. We note your suggestion for further comparison in this area. **However, the specific Test-Time Prior Adaptation method you referred to was not cited**. It would be beneficial if you could provide more clarity on the approach. While we see the value in such a comparison, we must consider the scope and current focus of our manuscript to determine the feasibility of including this comparison. If not feasible at this stage, this will certainly be a consideration for our future work to enhance the robustness of our research.\n\n**Choice of K**: We acknowledge your concerns about empirically selecting K in our clustering algorithm. While our ablation studies and further investigation and discussion have touched upon this, we appreciate your suggestion for further refinement and will continue to explore this in future work to enhance our algorithm's practicality and applicability.\n\nThank you once again for your valuable input and for your acknowledgment of the interesting idea and solid results presented in our paper."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission138/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700652887804,
                "cdate": 1700652887804,
                "tmdate": 1700653176565,
                "mdate": 1700653176565,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aaHJRsAxVs",
                "forum": "xyxU99Nutg",
                "replyto": "wKIUD4hcI7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission138/Reviewer_Yi31"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission138/Reviewer_Yi31"
                ],
                "content": {
                    "title": {
                        "value": "Sorry for missing citation"
                    },
                    "comment": {
                        "value": "Here is the survey paper I mentioned. You may refer to section 6. This direction has been studied for decades, and I believe a comparison to these methods can improve the soundness of your paper. \n\n[1] Jian Liang, Ran He, Tieniu Tan. A Comprehensive Survey on Test-Time Adaptation under Distribution Shifts."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission138/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700671299464,
                "cdate": 1700671299464,
                "tmdate": 1700671299464,
                "mdate": 1700671299464,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WPunUS3BrS",
                "forum": "xyxU99Nutg",
                "replyto": "0WQtg24haE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission138/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission138/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Addressing New Comment on Prior Adaptation"
                    },
                    "comment": {
                        "value": "Thank you for directing our attention to the survey paper. We acknowledge the importance of comparing our work to the methods outlined in this comprehensive survey. However, implementing a direct comparison with these methods would require installing dependencies and running code from numerous online repositories, a task that is not feasible within the current discussion period's timeframe. Despite this, we have taken steps to demonstrate the versatility and robustness of our UnMix-TNS in the Test-Time Prior Adaptation scenario.  \n\nOne of our top-performing test-time adaptation baselines, ROID, incorporates an additional prior correction module during test-time (as detailed in Section 4.3 of their paper). This module is designed to adjust for prior shift based on the class distribution within a batch. We conducted an ablation study where we disabled their prior correction module (ROID-(Prior Correction)) and replaced its normalization layers with our proposed UnMix-TNS layer (ROID-(Prior Correction)+UnMix-TNS). The results, as shown in the table below (second row), indicate a significant performance gain over the ROID method without prior correction, even surpassing the performance of ROID with the prior correction module (ROID+(Prior Correction)) (third row). Furthermore, adding our normalization layer to the standard ROID (last row in the table) even further enhances the performance. These findings underscore the robustness of UnMix-TNS without the need for an additional prior correction module. \n\nThis compelling result motivates us to consider integrating UnMix-TNS into other related test-time adaptation methods in future work in scenarios slightly different but related to Test-Time Prior Adaptation.\n\nWe hope this additional insight and the efforts we have made in this direction positively influence your final rating and demonstrate our commitment to advancing the field of Test-Time Adaptation.\n\n[1] Robert A Marsden, Mario Dobler, and Bin Yang. Universal test-time adaptation through weight ensembling, diversity weighting, and prior correction. arXiv preprint arXiv:2306.00650, 2023.\n\n| METHOD      | GAUSS | SHOT | IMPUL. | DEFOC. | GLASS | MOTION | ZOOM | SNOW | FROST | FOG | BRIGH. | CONTR. | ELAST. | PIXEL | JPEG | AVG. |\n| ----------- | ----------- | ----------- | ----------- | ----------- | ----------- | ----------- | ----------- | ----------- | ----------- | ----------- | ----------- | ----------- | ----------- | ----------- | ----------- | ----------- |\nROID-(Prior Correction) | 77.4 | 76.9 | 80.0 | 73.6 | 80.2 | 73.8 | 73.8 | 75.3 | 73.9 | 73.0 | 71.6 | 73.2 | 77.3 | 76.4 | 78.6 | 75.7 |\n**+UnMix-TNS** | 39.5 | 37.9 | 47.6 | 21.3 | 44.1 | 20.8 | 19.9 | 24.8 | 24.7 | 21.9 | 13.1 | 18.9 | 33.0 | 32.5 | 37.8 | 29.2 |\nROID+(Prior Correction) | 75.6 | 74.9 | 78.7 | 70.7 | 79.2 | 71.1 | 71.1 | 72.7 | 71.2 | 70.0 | 68.5 | 70.2 | 75.7 | 74.2 | 76.8 | 73.4 |\n**+UnMix-TNS** | 24.4 | 22.5 | 32.3 | 8.4 | 27.9 | 7.7 | 7.6 | 10.3 | 11.2 | 9.4 | 3.9 | 7.3 | 17.1 | 17.7 | 21.7 | 15.3 |"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission138/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700680304708,
                "cdate": 1700680304708,
                "tmdate": 1700680420542,
                "mdate": 1700680420542,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]