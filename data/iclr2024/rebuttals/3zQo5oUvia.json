[
    {
        "title": "Retrieval-Based Reconstruction For Time-series Contrastive Learning"
    },
    {
        "review": {
            "id": "aT6h4JKWIk",
            "forum": "3zQo5oUvia",
            "replyto": "3zQo5oUvia",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7550/Reviewer_Hx4L"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7550/Reviewer_Hx4L"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a time-series contrastive learning framework that replaces the data augmentation module with a retrieval-based pair construction strategy. The idea sounds interesting and is proved to be effective on three time-series datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This work proposes a retrieval-based mask reconstruction strategy to help the model identify similar time series, which I think is a smart design.\n2. The authors show that using contiguous and intermittent masks during the training and evaluation respectively leads to the best performance. Such a result could bring some new insights to the time-series learning community.\n3. By constructing contrastive pairs retrieval, the proposed method does not rely on data augmentations, which could harm the pattern of signals, to perform contrastive learning. Experiments on three datasets demonstrate the effectiveness of the proposed method."
                },
                "weaknesses": {
                    "value": "1. Figure 4 shows that the diagonal pattern is worse on the PPG and ECG data compared with that on the HAR data. Some explanations need to be provided here to help readers understand the potential limitations of the method.\n2. During the contrastive learning stage, the positive counterpart is selected as the one most similar to the anchor. However, it is possible that there is more than one candidate that shares the same class label with the anchor. Would such false negative pairs influence the performance of contrastive learning? Have you tried other positive selecting strategies such as hard threshold?\n3. Typo: we uniformly at random sample -> we uniformly random sample"
                },
                "questions": {
                    "value": "Please refer to the weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7550/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697792595597,
            "cdate": 1697792595597,
            "tmdate": 1699636913522,
            "mdate": 1699636913522,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2ceLNxAeLF",
                "forum": "3zQo5oUvia",
                "replyto": "aT6h4JKWIk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7550/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7550/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Hx4L (1/1)"
                    },
                    "comment": {
                        "value": "Thank you for your kind words and discussion points. We have revised our text to capture our discussion on the different performance across datasets and the presence of false negatives. We have addressed each of your specific points below (with W = weakness and Q = question).\n\n\"_Figure 4 shows that the diagonal pattern is worse on the PPG and ECG data compared with that on the HAR data. Some explanations need to be provided here\"_ (W1):\n\n- This is a good point. The different classes within HAR are arguably more distinctive than the classes for ECG and PPG, which can be seen in each dataset's visualization in Fig. 10-12, and we added this detail in the Fig. 5 [formerly Fig. 4] caption. Appendix A.3 further details each of the distinctive domains that each dataset is capturing.\n\nFalse Negatives (Q2):\n\n- The idea that false negatives potentially being among the candidates is a great discussion point, and it is a fundamental issue within contrastive learning more broadly, rather than being specific to our REBAR method. For example, in augmentation-based approaches like SimCLR [1], they sample negative items from the batch, and the batch may contain items from the same downstream classification category as the anchor. However, they are able to still achieve strong performance. Similarly, our approach is still able to achieve the best representation learning performance across our baselines. In regards to the proposed thresholding method, we believe that this method would be ineffective. A threshold should not be designed to be a hyperparameter as it would be dependent on how each specific anchor's specific motifs compare to other general instances within the same class. However, learning a threshold as a function of the anchor would be non-obvious to do due to the lack of class label ground truth during contrastive learning. Addressing false negatives is an ongoing research area, and we are exploring other strategies as a part of future work. We add this discussion into the Appendix A.6.\n\nTypo (Q3):\n\n- Thank you for pointing this out, and we have fixed it in the text.\n\nCitations:\n\n[1] Chen, Ting, et al. \"A simple framework for contrastive learning of visual representations.\" _International conference on machine learning_. PMLR, 2020."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7550/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700584291575,
                "cdate": 1700584291575,
                "tmdate": 1700584291575,
                "mdate": 1700584291575,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "P02y0JRdJG",
            "forum": "3zQo5oUvia",
            "replyto": "3zQo5oUvia",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7550/Reviewer_mFnV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7550/Reviewer_mFnV"
            ],
            "content": {
                "summary": {
                    "value": "This article proposes a new perspective for determining positive and negative samples in time series contrastive learning. If one subsequence can be successfully reconstructed by retrieving information from another, it should form a positive pair. Based on this, the author trains a cross-attention module to reconstruct the masked query input subsequence from the key input subsequence. The subsequence with the lowest reconstruction error is labelled as positive, and the others are labelled as negative. Experiments show that the REBAR method of this article achieves state-of-the-art results in learning a class-discriminative embedding space."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The method proposed in this article is intuitive and easy to understand."
                },
                "weaknesses": {
                    "value": "1. Format issue: All formulas in this article are not numbered. The notations in the first formula on page 5 have no corresponding definition.\n\n2. The experimental volume of this paper is insufficient. As a new perspective in the field of time series contrastive learning, the author should validate the method on a wider dataset to demonstrate its universality for time series.\n\n3. The article lacks discussion and analysis of key parameters. Specifically, when training the cross-attention module, what impact will the length of subsequences and the proportion of random masking have on the reconstruction effect of key subsequences? Is the reconstruction effect of the cross-attention module directly related to downstream task performance? How to set the number of candidate subsequences (proportion of positive and negative samples) when obtaining Pos/Neg labels?"
                },
                "questions": {
                    "value": "I noticed that when applying the REBAR metric in contrastive learning, an anchor sequence and n candidate sequences are sampled randomly. Only the candidate sequence with the smallest reconstruction loss will be determined as a positive sample of the anchor sequence. Is there a situation where, for example, when sequence A is used as an anchor sequence, the candidate sequence with the smallest reconstruction loss is sequence B, and then A and B are mutually positive samples? However, when B is used as an anchor, the candidate sequence with the smallest reconstruction loss may be another sequence C. Therefore, B and C are positive samples. But if A is also in the candidate sequences, this method will divide A into negative samples of B. This leads to conflicting conclusions when the anchor sequence is different."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7550/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698676471740,
            "cdate": 1698676471740,
            "tmdate": 1699636913389,
            "mdate": 1699636913389,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "pLDmJZOUmT",
                "forum": "3zQo5oUvia",
                "replyto": "P02y0JRdJG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7550/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7550/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to mFnV (1/3)"
                    },
                    "comment": {
                        "value": "Thank you for the discussion points and suggestions. Based upon our discussion, we have significantly revised our paper to clarify the scope of our paper to be focused on time-series that can be specifically defined as a series of class-labeled subsequences, adding a new t-SNE visualization in Fig. 6 and more analysis to further expand experimental results, as well as adding a new Tbl. 3 that highlights an ablation study and hyperparameter sensitivity analysis of the REBAR model. We have addressed each of your specific points below (with W = weakness and Q = question).\n\nFormatting (W1):\n\n- Thank you. We have added numbers to all of our equations, clarified the notation, and added definitions throughout the Sec. 3 Methods and the rest of the paper. Our Sec. 3 has been completely revised and reorganized to have a better flow to improve our REBAR method's understandability.\n\nExperiment Scale (W2):\n\n- Thank you for this question. We have revised the paper to clarify the scope of our contribution: Our method targets longer (extended) time series that are themselves composed of class-labeled subsequences, such a single recording of HAR data composed of the different types of physical activities, as illustrated in the new Fig. 1. There are relatively few datasets that support this task. For example, TNC only included HAR, ECG, and a small simulated dataset. We note that our scope is well-aligned with many mobile health, embedded sensing, and IoT applications which involve continuous and passive data collection and produce very long recordings with few to no labels. This is an area where the datasets that are available for research purposes have lagged behind the driving applications. We note that datasets such as UEA and UCR contain pre-segmented class-labeled time series that are not suitable for evaluating our approach (our datasets have time-series that are 8,325,000 / 334,080 / 15,000 time points long, compared to the median length of 621 and 301 for UEA and UCR respectively). Please see revisions to Sec. 1. Introduction and Sec. 6 Conclusions.\n  - Using this type of long time-series data in our problem also allows us to benchmark sampling-based contrastive learning methods, such as TNC and our own REBAR method. These sampling-based methods exploit the time-series nature of our task. Because each subsequence is part of a longer time-series that is changing labels over time, we can sample positive and negative examples directly from the longer time-series, rather than attempting to create positive examples from augmentations.\n- With this being said, we have expanded upon our results section to increase our experimental scale to add a new Fully Supervised baseline in Tbl. 1, a new Fig. 6 t-SNE visualization, as well as a visualization of the various positive/negative pairs identified by REBAR for a given time-series in the new Fig. 7.\n  - The linear probe trained on our REBAR representation consistently achieved the strongest results, even beating the fully supervised model in PPG and HAR, achieving the same accuracies, but higher AUROC and AUPRC. This demonstrates our REBAR's methods strength in learning a representation that is better at handling class imbalance than the fully-supervised model.\n  - Our t-SNE visualizations in Fig. 6 further validate our findings regarding our REBAR method's clustering ability. In HAR, all methods except for REBAR group walk, walk up, and walk down together. In PPG, all of the methods have poor clustering, but REBAR seems to perform the best with clearer separation between classes and has fewer discontiguous regions for the same class. In ECG, REBAR continues to have the best intra-class clustering.\n  - Our positive/negative pair visualization in Fig. 7 visualizes the positive pairing from the list of candidates for a given anchor that was identified by our REBAR method, and an additional gallery of 15 more of this visualization is included in Appendix A.5. We see that even when there is no exact match of the anchor within the candidates, REBAR's motif-comparison retrieval and reconstruction is able to identify a positive example from the candidates that shares the same class as the anchor."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7550/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700584116245,
                "cdate": 1700584116245,
                "tmdate": 1700584116245,
                "mdate": 1700584116245,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "W3nPJI7M4o",
                "forum": "3zQo5oUvia",
                "replyto": "VqGPKqu7eL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7550/Reviewer_mFnV"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7550/Reviewer_mFnV"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the responses addressing the format issues and the detailed discussion on the hyperparameters. However, my concerns regarding the scale of the experiments remain, so I will keep my score as it is."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7550/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700662169787,
                "cdate": 1700662169787,
                "tmdate": 1700662169787,
                "mdate": 1700662169787,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "IsyAYWcv3p",
                "forum": "3zQo5oUvia",
                "replyto": "P02y0JRdJG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7550/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7550/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to expermiental scale"
                    },
                    "comment": {
                        "value": "Thank you for your feedback to our response. We would like to emphasize that with our clarified scope on \u201ctime-series that are defined as a series of class labeled subsequences\u201d, there are very few datasets that support this task. TNC only included HAR, ECG, as well a small simulated dataset designed specifically to assess their model's stationarity test feature [1]. Our work adds the PPG dataset for stress classification, to introduce an additional benchmark dataset for this type of time-series contrastive learning task. If you have any suggestions on additional datasets to include, we would be happy to include them in the next revision, but we believe that the three datasets we have included are very distinctive and allow us to assess the strength of each of the contrastive learning methods.\n\nHAR, PPG, and ECG each demonstrate distinctively different characteristics.  measuring acceleration, electrical activity of the heart, and blood volume change. The differences between the classes for a given dataset are all captured very differently. The activity labeled subsequences in HAR are primarily distinguished by amplitude, as seen in Fig. 10. Using PPG to identify stress is an ongoing research topic, but recent work has found that stress labeled subsequences in PPG are captured by wave shape [2]. The atrial fibrillation labeled subsequences in ECG are distinguished by irregular peak frequency. Therefore, although we only contain three datasets, their distinctive nature demonstrates the strength of our method.  \n\n[1] Tonekaboni, Sana, Danny Eytan, and Anna Goldenberg. \"Unsupervised representation learning for time series with temporal neighborhood coding.\" arXiv preprint arXiv:2106.00750 (2021).\n\n[2] Celka, Patrick, et al. \"Influence of mental stress on the pulse wave features of photoplethysmograms.\" Healthcare technology letters 7.1 (2020): 7-12.\n\n(edited to add TNC citation)"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7550/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700670854914,
                "cdate": 1700670854914,
                "tmdate": 1700672045817,
                "mdate": 1700672045817,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "vpypeO49Lx",
            "forum": "3zQo5oUvia",
            "replyto": "3zQo5oUvia",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7550/Reviewer_jiyG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7550/Reviewer_jiyG"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a novel method for constructing positive pairs for contrastive learning in time-series data. It presents experiments across three datasets to validate the approach.\n\n--- post rebuttal ---\n\nI appreciate the efforts made by the authors in addressing the concerns raised in my initial review. The manuscript has undergone significant changes, resulting in notable improvements in its quality. Considering these enhancements, I have revised my score from 3 to 6."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is motivated. The proposed method is grounded on a cogent hypothesis *\"if one time-series is useful for reconstructing another, then they likely share reconstruction information and thus should be drawn together within the embedding space as positive examples.\"*. Essentially, the author posits that time-series with similar semantics are capable of aiding in each other's reconstruction.\n\n2. An intriguing observation made in the paper is the difference in sparsity between cross-attention mechanisms when trained with *contiguous masks* versus *intermittent masks.* \n\n3. The author provides a comprehensive comparison, including many relevant baselines."
                },
                "weaknesses": {
                    "value": "1. The rationale for preferring a contiguous mask over an intermittent mask is presented but could be articulated with greater clarity to enhance its persuasiveness. Additionally, there seems to be some confusion regarding Figure 1. Clarification is needed as to whether the author implies that a) the contiguous mask is utilized during the training of REBAR, and b) the intermittent mask is employed when applying REBAR in contrastive learning. If this is the case, the reasons for using different masks in these contexts should be explicitly stated.\n\n2. The experimental scale appears somewhat limited. The paper does not specify the exact number of samples within the datasets, which seem to be on the smaller side. This limitation is accentuated when compared to previous works, such as TS2VEC [1], which utilized a much larger array of datasets, including 125 from the UCR archive and 29 from the UEA archive.\n\n3. The explanation of results requires expansion. For instance, the acronyms ARI and NMI in Table 2 are not defined within the context of the paper, leaving their significance unclear. Moreover, there is a notable difference in the results reported for the TNC on the HAR dataset between the original TNC paper [2] and this manuscript. In the origianl paper, it was reported AUPRC 0.94, Accuracy 88 while in this manuscript, it is reported AUPRC 0.98 and Accuracy 94. More information about the potential factors leading to these discrepancies would be beneficial for the reader's comprehension.\n\n\n* [1] Yue, Zhihan, et al. \"Ts2vec: Towards universal representation of time series.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 36. No. 8. 2022.\n\n* [2] Tonekaboni, Sana, Danny Eytan, and Anna Goldenberg. \"Unsupervised representation learning for time series with temporal neighborhood coding.\" arXiv preprint arXiv:2106.00750 (2021)."
                },
                "questions": {
                    "value": "In Table 3, the author evaluates the influence of different mask types on performance. It would be beneficial to clarify why the training stage favors a contiguous mask, while the evaluation stage shows a preference for an intermittent mask."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7550/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7550/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7550/Reviewer_jiyG"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7550/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699233216486,
            "cdate": 1699233216486,
            "tmdate": 1700689880106,
            "mdate": 1700689880106,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gx1kVBllv0",
                "forum": "3zQo5oUvia",
                "replyto": "vpypeO49Lx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7550/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7550/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to jiyG (1/3)"
                    },
                    "comment": {
                        "value": "Thank you for your great discussion points. Based your points, we have added 3 new figures (conv cross-attn in Fig. 3 to better explain our approach and masking procedure; t-SNE in Fig. 6 and pos pair examples in Fig. 7 to better explain our results) and a new ablation study+hyperparameter table into the main text, as well as substantially revising our paper to add more explanation of our method with a time-series motif framing, further model analysis and explanation to the results, and clarifying the scope of our paper to be focused on time-series that can be defined as series of class-labeled subsequences. We have addressed each of your specific points below (with W = weakness and Q = question).\n\nDifference in Masking (W1, Q1):\n\n- We replace the word \"evaluation\" with \"application\" here and in our text, so that it is more clear we are using the intermittent mask explicitly during application of REBAR in contrastive learning to identify positive/negative pairs. Our REBAR cross-attention model compares the motifs within a specific receptive field around a masked time-point in the query sequence with the motifs in the key sequence. Then, it retrieves the best matching motif from the key to be used for reconstruction of that specific masked time-point in the query, and this idea is illustrated in the new Fig. 1 and Fig. 3.\n- At training time, a contiguous mask is used so that the model learns to compare specific, potentially class-discriminative, motifs in the query with specific, potentially class-discriminative, motifs in the key, rather than comparing minor, transient, non-unique motifs. See Fig. 4a) and b) for attention weight visualizations.\n- At application time, a contiguous mask could be used, however, in doing so, only the motifs near the contiguously masked out region would be compared to the key. This is because when reconstructing a given masked time-point and another point contiguous to it, the receptive fields to be used to identify motifs to be compared to the key are heavily overlapping.\n  - An intermittent mask allows for many different motifs in the query, each of them captured in a receptive field around the many masked time-points dispersed throughout the signal, to be compared to the key during reconstruction. This allows for a higher coverage of the query in conducting such motif-similarity comparisons with the key. Therefore, during application, when we are testing each candidate as the key for a given anchor as the query, we would be able to identify the candidate which is most similar to the anchor as a whole.\n- We empirically justify this intuition. Table 4 in Appendix A.1.2 shows this combination of using a contiguous mask for training and an intermittent mask for application achieves the best performance in our validation experiment. Additionally, Figure 4b) demonstrates that the cross-attention model trained with a contiguous mask still maintains a sparse attention when evaluated on a transient mask. Therefore, the model is still reconstructing the query based on specific retrieved motifs from the key at each of the transiently masked out time-points.\n\n\"_The paper does not specify the exact number of samples within the datasets, which seem to be on the smaller side \u2026 UCR \u2026 UEA\"_ (Q2):\n\n- Thank you for the suggestion. We have added the exact number of samples within the datasets into our paper in Appendix A.2. Once we normalize our datasets by segmenting them into the subsequences that are used to train each of our self-supervised learning models, we have a total number of 76,590 / 6,914 / 1,305 sequences for ECG, HAR, and PPG, respectively. This is much larger than the datasets found in UCR and UEA: the median total dataset size in UCR is 687 and in UEA it is 621. In fact, if we look specifically at the 29 dataset subset of UEA that TS2Vec primarily presents in their paper, then this UEA subset only has a median dataset size of 412.5. Note that:\n  - 90 out of the 128 datasets in UCR and 123 out of the 183 datasets in UEA have 1,000 or less sequences.\n  - 8 out of the 128 total datasets in UCR and 14 out of the 183 total datasets in UEA have 100 or less sequences."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7550/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700583659235,
                "cdate": 1700583659235,
                "tmdate": 1700583659235,
                "mdate": 1700583659235,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hWzJl1zaJ6",
                "forum": "3zQo5oUvia",
                "replyto": "1uNRgGwsxB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7550/Reviewer_jiyG"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7550/Reviewer_jiyG"
                ],
                "content": {
                    "title": {
                        "value": "Response to Response (3/3)"
                    },
                    "comment": {
                        "value": "Thanks for the explaination. My issue on acronyms and difference in the results are addressed."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7550/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700689537696,
                "cdate": 1700689537696,
                "tmdate": 1700689537696,
                "mdate": 1700689537696,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dsjXzbkExu",
                "forum": "3zQo5oUvia",
                "replyto": "gx1kVBllv0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7550/Reviewer_jiyG"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7550/Reviewer_jiyG"
                ],
                "content": {
                    "title": {
                        "value": "Response to Response (1/3)"
                    },
                    "comment": {
                        "value": "I appreciate the author's effort on addressing my issue on \"Difference in Masking\". I am satisfied with the modification the author have made."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7550/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700689641175,
                "cdate": 1700689641175,
                "tmdate": 1700689641175,
                "mdate": 1700689641175,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ZoVJoHrfgv",
            "forum": "3zQo5oUvia",
            "replyto": "3zQo5oUvia",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7550/Reviewer_gCTH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7550/Reviewer_gCTH"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a novel approach called Retrieval-Based Reconstruction (REBAR) for self-supervised contrastive learning in time-series data. The REBAR method utilizes retrieval-based reconstruction to identify positive data pairs in time-series, leading to state-of-the-art performance on downstream tasks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Novel approach: The paper introduces a novel approach called Retrieval-Based Reconstruction (REBAR) for self-supervised contrastive learning in time-series data. This approach utilizes retrieval-based reconstruction to identify positive data pairs in time-series, which is a unique and effective way to address the challenges of creating positive pairs via augmentations in time-series data.\n\n2. State-of-the-art performance: The paper demonstrates that the REBAR method achieves state-of-the-art performance on downstream tasks across diverse modalities, including speech, motion, and physiological data.\n\n3. Comprehensive evaluation on two tasks including classification and cluster agreement."
                },
                "weaknesses": {
                    "value": "1. Lack of ablation studies: The paper does not include ablation studies to analyze the contribution of each component of the REBAR method. This makes it difficult to understand the relative importance of each component and how they interact with each other.\n\n2. Detailed explanation of the results. There is no detailed studies for table 1 and 2, e.g., visualizations of the learned embedding or positive/negative pairs. \n\n3. Limited discussion of hyperparameters: While the paper provides some details about the hyperparameters used in the experiments, it does not provide a comprehensive analysis of the sensitivity of the method to different hyperparameters. \n\n4. Without comparison with baselines, Figure 4 doesn't show any advantages of the proposed model since the diagonal pattern would be obvious for most of the baselines.\n\n5. Section 3.1, notations are used without clear definition"
                },
                "questions": {
                    "value": "1. Ablation study and hyperparameters selection.\n2. Include more visualizations or examples of the positive and negative pairs identified by the REBAR method\n3. \"During evaluation, we use an an intermittent mask\", explain the intuition why different masks are used in the training and evaluation\n4. How does the REBAR method perform on time-series data with different characteristics, such as varying lengths or noise levels?\n5. Provide more detailed explanations of the convolutional cross-attention architecture used in the REBAR method."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7550/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699240302505,
            "cdate": 1699240302505,
            "tmdate": 1699636913152,
            "mdate": 1699636913152,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "bqah6My11P",
                "forum": "3zQo5oUvia",
                "replyto": "ZoVJoHrfgv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7550/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7550/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to gCTH (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for your great suggestions. We have added 5 new figures (intuition in Fig. 1; conv cross-attn in Fig. 3; t-SNE in Fig. 6; pos pair in Fig. 7) and a new ablation study+hyperparameter table into the main text, as well as substantially revising our paper to capture your comments. We have addressed each of your points below (with W = weakness and Q = question).\n\nLack of Ablation Studies + Hyperparameter Analysis (W1, W3, Q1):\n\n- Thank you for this suggestion: we have added a new Table 3, which includes the results of our ablation study and a hyperparameter sensitivity analysis for our REBAR model.\n- Each of the model components and modifications contributes to REBAR's strong contrastive learning performance, especially the dilated convolutions (e.g. .061 drop in accuracy and a .263 drop in NMI). The exclusion of our reversible instance norm leads to a .008 drop in accuracy and a .127 drop in NMI, and the addition of an explicit positional embedding leads to a .006 drop in accuracy and .120 drop in NMI. As noted in Section 3.1, we intentionally keep the design of REBAR attention simple to emphasize the motif comparison within the cross-attention mechanism.\n- The REBAR model is fairly robust to hyperparameter tuning. When we modify the size of contiguous masks,  the size of the dilated conv's receptive field, or the number of epochs to train the REBAR cross-attention, the downstream linear probe and clusterability performance remains consistent.\n  - Increasing the mask size from 15 to 25 increases accuracy by .006 and increases NMI by .017. Decreasing the mask size from 15 to 5 decreases accuracy by .004 and decreases NMI by .017.\n  - Increasing the receptive field from 43 to 55 decreases accuracy by .016 and increases NMI by .037. Decreasing the receptive field from 43 to 31 decreases accuracy by .003 and increases NMI by .007\n  - Decreasing trained REBAR cross-attention epochs from 300 to 200 increases accuracy by .006 and increases NMI by .070. Decreasing trained REBAR cross-attention epochs from 300 to 100 increases accuracy by .006 and decreases NMI by .013.\n  - We note that these results should not imply that the REBAR model should not be trained. Tbl. 5 in the Appendix shows that if REBAR is only trained for 1 epoch, then accuracy drops by 0.010 and NMI drops by 0.251.\n\nMore Visualizations of Learned Embedding + Positive/Negative pairs. (W2, Q2)\n\n- Thank you for this idea. In the Sec. 5 Results section, we have added t-SNE visualizations of each model's encoding in the new Fig. 6, and a visualization of the various positive/negative pairs identified by REBAR for a given time-series in new Fig. 7, as well as an additional gallery of 15 more of this positive/negative pair visualization within Appendix A.5.\n- Our t-SNE visualizations in Fig. 6 further validate our findings regarding REBAR's ability to learn embeddings with superior class-discriminative properties. In HAR, all methods, except for REBAR, group \"walk\", \"walk up\", and \"walk down\" together. In PPG, all of the methods have poor clustering, but REBAR seems to perform the best with clearer separation between classes and fewer non-contiguous regions within the same class. In ECG, REBAR exhibits the best intra-class clustering.\n- In Fig. 7 positive/negative pair visualization, we illustrate the positive example identified by our REBAR method from the list of candidates, for a given anchor in each class. Even when there is no exact match for the anchor within the candidates, REBAR is still able to select an example that has the same class as the anchor.\n\n\"_Without comparison with baselines, Figure 4 [confusion matrices] \u2026\"_ (W4) :\n\n- We would like to clarify that the confusion matrices (now in Fig. 5) are designed to assess how REBAR's pseudo-distance function between two subsequences predicts mutual class membership between the two. This validates REBAR's effectiveness in learning a discriminative distance function, before contrastive learning takes place. Since the other baselines from the literature do not construct a pseudo-distance function, they can't be included in this analysis. We have clarified the text to make this more clear.\n\nNotation (W5):\n\n- Thank you: we have added notation definitions at the start of the Sec. 3 Methods section and clarified the notation further in 3.1 with more details."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7550/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700582120505,
                "cdate": 1700582120505,
                "tmdate": 1700582262764,
                "mdate": 1700582262764,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "NFAnlgjXoY",
                "forum": "3zQo5oUvia",
                "replyto": "ZoVJoHrfgv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7550/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7550/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to gCTH (2/2)"
                    },
                    "comment": {
                        "value": "\"_intuition why different masks are used in the training and evaluation [i.e. application]\"_ (Q3)\n\n- We replace the word \"evaluation\" with \"application\" here and in our text, so that it is more clear we are using the intermittent mask explicitly during application of REBAR in contrastive learning to identify positive/negative pairs. Our REBAR cross-attention model compares the motifs within a specific receptive field around a masked time-point in the query sequence with the motifs in the key sequence. Then, it retrieves the best matching motif from the key to be used for reconstruction of that specific masked time-point in the query, and this idea is illustrated in the new Fig. 1 and Fig. 3.\n- At training time, a contiguous mask is used so that the model learns to compare specific, potentially class-discriminative, motifs in the query with specific, potentially class-discriminative, motifs in the key, rather than comparing minor, transient, non-unique motifs. See Fig. 4a) and b) for attention weight visualizations.\n- At application time, a contiguous mask could be used, however, in doing so, only the motifs near the contiguously masked out region would be compared to the key. This is because when reconstructing a given masked time-point and another point contiguous to it, the receptive fields to be used to identify motifs to be compared to the key are heavily overlapping.\n  - An intermittent mask allows for many different motifs in the query, each of them captured in a receptive field around the many masked time-points dispersed throughout the signal, to be compared to the key during reconstruction. This allows for a higher coverage of the query in conducting such motif-similarity comparisons with the key. Therefore, during application, when we are testing each candidate as the key for a given anchor as the query, we would be able to identify the candidate which is most similar to the anchor as a whole.\n- We empirically justify this intuition. Table 4 in Appendix A.1.2 shows this combination of using a contiguous mask for training and an intermittent mask for application achieves the best performance in our validation experiment. Additionally, Figure 4b) demonstrates that the cross-attention model trained with a contiguous mask still maintains a sparse attention when evaluated on a transient mask. Therefore, the model is still reconstructing the query based on specific retrieved motifs from the key at each of the transiently masked out time-points.\n\nREBAR performance on Distinctive Time-series Domains (Q4)\n\n- We test our approach in three different time series domains which consist of signals with qualitatively very different properties, as illustrated in the new Fig. 10-12. The different classes within HAR are arguably more distinctive than the classes for ECG and PPG, although the different HAR categories of \"walk\", \"walk up\", and \"walk down\" require a more subtle distinction. This is reflected in our results, with our method performing the best on HAR. The fact that we achieve SOTA results in three very different data domains speaks to the effectiveness of our method. Regarding more systematic variations, we note that our datasets also contain substantial variation in signal lengths. The subsequence lengths are adapted to each of the signal domains and vary significantly in length, and the source signals that they are sampled from vary significantly as well. HAR uses 2.56s subsequences sampled from a 5 min time-series. PPG uses 60s subsequences sampled from an 87 min time-series. ECG uses 10s subsequences sampled from a 9.25 hour time-series.\n\nMore explanation of REBAR Convolutional Cross-Attention (Q5):\n\n- Thank you for this comment. In Sec. 1 Introduction, we have added the new time-series motif language to help better explain the intuition of our idea, and a new Fig. 1 has been added to illustrate the intuition. Sec. 3 Methods has been completely revised and re-organized to incorporate the motif language. The start of Sec. 3 has been modified to introduce the idea of using REBAR's reconstruction error as a distance metric earlier, motivating our approach. Sec. 3.1 now ties the mathematical formulation of cross-attention with our new Fig. 3 to explicitly demonstrate how convolutions aid the cross-attention in retrieving and comparing motifs. Training REBAR is now its own section, Sec. 3.2, with Applying REBAR is Sec. 3.3. In these two sections, we are able to ground the masking motivation more clearly with the motif language. With the ablation study and hyperparameter sensitivity results in Tbl. 3, we can clearly understand how each of the REBAR cross-attention components contribute to downstream performance, as well as how to train the model. Finally, in the Appendix A.1, we include extra details on modeling decisions, as well as a new Fig. 8 that illustrates the exact dilated convolution module used in REBAR."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7550/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700582160239,
                "cdate": 1700582160239,
                "tmdate": 1700582252079,
                "mdate": 1700582252079,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]